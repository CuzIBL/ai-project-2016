When Evolving Populations is Better than Coevolving Individuals: The Blind Mice 
                                                       Problem 

                                                   Thomas Miconi 
                                             34 Rue Desbordes-Valmore 
                                                 75016 Paris, France 
                                               thomas.miconi @free.fr 


                        Abstract                               difficulties and for the difference in behaviour with the sim•
                                                               pler genetic algorithm. 
     This paper is about the evolutionary design of 
     multi-agent systems. An important part of re•             2 (Some of the) Related work 
     cent research in this domain has been focusing 
     on collaborative revolutionary methods. We ex•            The simplest way to evolve a team of collaborating agents is 
     pose possible drawbacks of these methods, and             to have all agents be identical, that is, to have homogeneous 
     show that for a non-trivial problem called the            populations. These methods are not really different from in•
     "blind mice" problem, a classical GA approach in          dividual evolution, except at evaluation time: to evaluate a 
     which whole populations are evaluated, selected           given genotype, N agents are created out of this genotype in•
     and crossed together (with a few tweaks) finds an         stead of just one, and the resulting population is evaluated. 
     elegant and non-intuitive solution more efficiently       Then start again with a different genotype, etc. While being 
     than cooperative coevolution. The difference in ef•       very rigidly constrained, this method makes perfect sense in 
     ficiency grows with the number of agents within           situations where one does not need heterogeneity at all. This 
     the simulation. We propose an explanation for             method was used by [N.Zaera et «/., 1996) to evolve small 
     this poorer performance of cooperative coevolu•           groups of fish-like animats, controlled by neural networks, 
     tion, based on the intrinsic fragility of the evalua•     to perform extremely simple tasks (dispersion, aggregation, 
     tion process. This explanation is supported by the•       etc.). 
     oretical and experimental arguments.                        A similar method was used by [Luke, 1998] to evolve com•
                                                               petitive teams of soccer players for the Robocup competition 
                                                               [Kitano et a/., 1995]. The author used an adapted version of 
1 Introduction                                                 Genetic Programming [Koza, 19921. There was also an at•
Evolutionary algorithms are methods that apply the principles  tempt at introducing a limited degree of heterogeneity by de•
of darwinian evolution to the generation and adaptation of     composing teams into small sub-teams (defenders, attackers, 
artificial, logical entities (function parameters, rulesets, pro• etc.) and evolving different program trees for each such sub-
grams. ..). Their usability as a search technique has been sup• teams. However, because of the enormous search space (and 
ported both analytically (e.g. the Schema Theorem [Gold•       of the delays imposed by the Robocup server software), this 
berg, 1989] for genetic algorithms), and empirically by un•    approach proved intractable in practice: GP runs took days to 
countable applications. However, the overwhelming majority     produce meaningful results. Lack of time thus prevented the 
of these applications are about the generation of individuals. semi-heterogeneous teams from outperforming homogeneous 
   Collective evolution, that is, the generation and/or adapta• teams. 
tion of collaborating populations of agents, has attracted com•  A way to obtain some degree of heterogeneity is to have 
paratively less attention. There has been significant research only one population and make it change gradually over time, 
in this domain though, especially over the last decade. This   replacing some agents by others based on some evaluation 
research led to algorithms of ever-growing complexity. This    method. These new agents can be obtained by crossover or 
paper will first describe some of the work in that field, and  by duplication with mutation. If there is a way to evaluate the 
more particularily the principle of cooperative coevolution,   impact of one given individual, it is perfectly possible to per•
which seems to be the most popular type of method today.       form a simple genetic algorithm over the population. This is, 
  We then expose what we feel are possible drawbacks of co•    in essence, the idea behind classifier systems (iHolland and 
operative evolution, and propose a simpler way to adapt the    Reitman, 1978]), where a set of rules cooperate to control an 
canonical genetic algorithm to the generation of populations.  animat, and where individuals are evaluated after the animates 
We describe an experiment, based on the "blind mice" prob•     performance through a credit-sharing system. 
lem, and show that while an adapted genetic algorithm works      In the same vein, we proposed a simple scheme in (iMi-
pretty well with that problem, cooperative coevolution has     coni, 2001]), in which all agents were given an arbitrary in•
more difficulties. Finally, we give an explanation for these   dex, and agents of index K could only mate with agents whose 


MULTIAGENT SYSTEMS                                                                                                    647  indices fell within the [K-r; K+r] range. Evaluation occured  3 Two methods for evolving heterogeneous 
 simply by replacing one of the two parents by the offspring,      populations 
 then the second parent, and keeping the best of these two pop•
 ulations (with the possibility of discarding any changes if it 3.1 Cooperative coevolution 
 decreased the performance of the system). This simple algo•   Cooperative coevolution is quite an elaborate mechanism. In•
 rithm led to the emergence of sub-species that appeared, grow tuition indicates (and evidence confirms) that by focusing on 
 and shrank according to the needs of the population. An inter• each and every agent, it requires a huge number of evaluations 
 esting feature of this algorithm was its incrementality, which to converge towards a solution. This algorithm concentrates 
 allowed for long-term, adaptive evolution of the system.      on optimizing each individual agent in regard with the rest 
   Trying to obtain fully heterogeneous systems brings us to   of the population; it is rather different from the more holistic 
 another level of complexity, right into the realm of coopera• approach of traditional GAs, in which full genomes are ma•
 tive coevolution. While coevolution has been most frequently  nipulated, and the (co-)adaptation of genes emerges naturally 
 applied in a competitive way (by confronting individuals to   from selection, crossover and mutation - at least, in theory. 
 each other and using the result of this confrontation as an     Why would it not be possible to simply use traditional GAs 
 evaluation for individuals), it can also be used in a cooper• for the generation of multi-agent systems, regarding whole 
 ative way, in order to evolve sets of collaborating agents.   systems (not just each agent within them) as individuals? A 
   In cooperative coevolutionary iPotter and DeJong, 1994]     simple answer is that this approach leads to very large geno•
 methods, each agent within the system is actually taken from  types, since the genotype for an "individual" has to code for 
 a hidden subpopulation, or pool. To evaluate a given indi•    several agents instead of just one, and the resulting search 
 vidual, it is associated with a set of collaborators (one from space might become intractable for GAs. Cooperative coevo•
 each other pool) and the resulting population is evaluated as a lution can thus be seen as a simple way to decompose a big 
 whole. The resulting score is then attributed to the currently problem into several smaller ones, even though these smaller 
evaluated individual. Based on this evaluation method, the     problems are still strongly interlaced with each other. 
classical GA cycle (evaluate, select and reproduce) is applied   However cooperative coevolutionary algorithms seem to 
to each pool in turn, as many times as needed. In the first ver• have an important drawback: they basically evaluate each 
sion of the cooperative coevolutionary algorithm (CCGA-1)      agent by assessing its impact on the performance of the whole 
collaborators are chosen by taking the best individual from    system. The problem is that when the number of agents 
every pool. However, in the CCGA-2 version, evaluation is      within the system grows, the influence of one single agent 
refined by re-evalluating every agent with random collabora•   over the system's performance tends to decrease, thus possi•
tors, then taking the better score obtained between these two  bly making its assessment more difficult. This may become 
evaluations. The number of collaborators, the way these col•   troublesome when the problem has a stochastic component, 
laborators are chosen, the way the overall score is computed   as is the case in many simulations. In this case, evaluating 
(averaging the different scores, or taking the best score, or  the same population several times can lead to different re•
taking the worst score, etc.) are important parameters that    sults. The consequence of this may be more important than 
can influence the performance of the algorithms. The influ•    one might think, as we will see below. 
ence of these parameters has been studied to some extent by      But first, it might be interesting to see how classical genetic 
[Wiegand et ai, 2001], but this study applied only to simple   algorithms can be adapted to the evolution of populations, 
function optimization problems with only two variables.        and whether these population-oriented genetic algorithms can 
   As happens frequently with good ideas, cooperative coevo•   compete with cooperative coevolution. 
lution has been (re-)discovered a number of times under dif•
ferent names. Enforced subpopulations (ESP), for example,      3.2 Population-oriented genetic algorithms 
are exactly like cooperative coevolution, in which each agent  Genetic algorithms work by evaluating individuals, selecting 
is evaluated with only one set of collaborators: the best agents some of them according to their performance, crossing them 
from all other pools. In other words, ESP is the CCGA-1 al•    together and mutating them, and starting over again. is pos•
gorithm. While this method was initially devised for the evo•  sible to apply exactly the same method to whole multi-agent 
lution of neural networks [Gomez and Mikkulainen, 1997],       systems. We can evaluate populations, cross them together 
it was successfully applied to multi-agent evolution by [Yong  (thus creating new populations that inherit agents from both 
and Mikkulainen, 2001], who used it in a predator-prey sim•    parents), mutate them by changing one of their agents, etc. 
ulation. The algorithm managed to find efficient strategies      However, the fact is that multi-agent systems are not simple 
for predators, such as having two predators "chase" the prey   individuals. They do have an obvious level of decomposition 
while another one blocked it.                                  (the agent), and this can be exploited in several ways. 
  For some reason, the idea of simply using the standard ge•     The most obvious idea is that in order to cover the search 
netic algorithm to whole populations seems to have fallen      space efficiently, one must not only make new populations 
slightly out of fashion. The most probable reason is that it   out of existing agents, one must also create new agents. To 
is simply too obvious to be talked about. The second rea•      do this, we may introduce an inner crossover operator that 
son is that it does have intrinsic drawbacks, such as a more   allows us to cross two agents together. Thus, when creating a 
massive search space. The third one is that it requires a few  new population by importing agents from both parents, some 
modifications to be adapted to the evolution of populations.   of these imported agents would actually the result of an inner 
All these aspects are discussed in section 3.2.                crossover between agents from the parents. 


648                                                                                             MULTIAGENT SYSTEMS Figure 1: Normal crossover (top) can be "spiced up" with 
inner crossover between individual agents (bottom). This al•
lows for the creation of new agents, which is necessary to 
cover the search space efficiently 

   It is possible to make an analogy with traditional GAs: 
from the viewpoint of the whole population, these "crossed" 
agents have some similarity with bit-wise mutations in the            Cat 
standard genetic algorithm. They are part of the children's            Mouse 
genotype, yet they were not present in any of the parents' 
genotypes. However, these are not exactly random mutations,    Figure 2: The successful strategy. Mouse D attracts the cats, 
since the genetic material still comes from the parents' geno• mouse B plays a "balancing'* role, and other mice move to•
types. This suggests that at first sight, "inner crossover" rate gether in a tight flock 
should be slightly higher than the usual mutation rate in a 
classical GA (usually about 2%-5% for each bit).               problem is not a trivial task. Yet, as we will sec below, evo•
  Another possibility is to enhance traditional crossover by   lution managed to come up with an elaborate solution to this 
occasionally swapping agents between populations in the fi•    problem. 
nal offspring. This, too, could allow for a better covering of   In our experiments, the mice are controlled by simple feed•
the search space. However, we will not explore that possibil•  forward neural networks with 2 inputs, 2 outputs and 5 hidden 
ity in the present paper.                                      neurons. The two inputs are the coordinates of the center of 
                                                               gravity of the flock, that is, the sum of the X- (resp. Y-) co•
4 Application: The Blind Mice problem                          ordinates of all mice, divided by the number of mice. The 
4.1 Description of the experiment                              two outputs are two real numbers in the [-3.0; +3.0] range, 
                                                               indicating the horizontal and vertical speed of the mouse. All 
The experiment presented here is based on the "blind mice" 
                                                               weights are real numbers in the [-1.0; +1.0] range. All simu•
problem. A flock of mice, controlled by simple feed-forward 
                                                               lations use 4 cats. 
neural networks, have to escape a number of cats running af•
ter them in a toroidal world.                                  4.2 The evolved strategy 
  Now the "game" has three very simple rules: 
                                                               There seems to be an optimal strategy for this problem. This 
  1. The cats can see the mice and always run after the closest strategy emerged in all successful runs, sometimes with vari•
     mouse around.                                             ants. This strategy is described in Figure 2. No other strategy 
  2. The mice run faster than the cats.                        led to a really efficient behaviour. 
                                                                 Let us explain this strategy: as we can see, most mice are 
  3. The mice can not see the cats. Neither can they see each  aggregated together and move in a tight flock. This minimizes 
     other (they are "blind"). Their only input is a pair of   the probability that a "stray" cat might touch them, but it is 
     numbers: the X and Y coordinates of the center of the     not sufficient in itself to ensure a minimal catch rate. The 
     flock.                                                    really important behaviour is that of the mouse labelled D 
  When a cat touches a mouse, the cat is teleported to an•     (the "Dancer"). 
other, random location, and the population's "catch counter"     This mouse has a strange behaviour: it seems to revolve 
is increased; nothing else is changed, and the simulation is   around the rest of the flock, but not in a strictly circular fash•
not interrupted in any way.                                    ion. Instead, it constantly bounces around the flock, always 
  Let us consider these rules: they seem to make the prob•     staying at a respectable distance from it, and moving very fast 
lem extremely difficult for the mice. How is it possible to    along its path. The purpose of this behaviour becomes obvi•
escape predators that can see you, but that you can't sec?     ous when one sees the position of the cats: they are all fol•
Running around as fast as possible will just make them bump    lowing this "dancing" mouse, because it is simply the closest 
into any cat coming from the opposite direction. The same      to them, thus leaving the rest of the flock alone. 
is true for random movement strategies. Given the enormous       In other words, the purpose of this dancing behaviour is 
asymmetry of information between mice and cats, the sur•       simply to attract all the cats. The dancer moves very fast (so 
vival chances of the poor rodents appear to be desperately    that it cannot be touched by cats), but along a sinuous path, 
thin. Even for a human designer, finding a solution to this   so that: 


MULTIAGENT SYSTEMS                                                                                                   649    - It can "drag" cats more efficiently in the initial stage, 
     when cats and mice are at random position 
   - It never gets too far from the cats, which allows the cats 
     to follow it endlessly even though it runs much faster 
     than them. 
   The final touch of this strategy can be seen in the be•
haviour of the mouse labelled B (as "Balance"), even though 
it didn't appear in all successful runs. This mouse also re•
volves around the flock, but much closer to it. In some runs 
it moves around the flock in a circular fashion, in other runs 
it bounces around it, but it usually stays on the opposite side 
of the flock with respect to the dancer. We believe that this 
mouse has a ''balancing" role, in that it counterbalances the 
effect of the Dancer on the position of the center of gravity of 
the population, thus allowing the flock to be more stable. 
   Many variants appeared, such as having several dancers, 
or no balancer. But the essential traits of the strategy were 
consistent: aggregation of most mice, except for one or a few 
to attract the cats away from the flock. 
  Note that this strategy is very interdependent: each agent's 
performance is highly dependant on other agents' behaviour.    Figure 3: Performance of a population-adapted genetic algo•
This is even more true when you consider that in oider to      rithm, with 7 mice (top) and 15 mice (bottom). The y-axis 
behave that way, they must calculate their trajectories out of indicates the number of mice catched during this evaluation 
only one input: the position of the center of gravity of the  round, while the x-axis indicates the number of evaluations. 
whole flock, which is based on the position of all other agents. Both the fitness of the best population and the average fitness 
This fact plays a significant role in the results described be• of all populations are shown. 
low. 
                                                              collaborators can be used in turn in order to refine the evalua•
5 Experimental results                                        tion, and the final result can be calculated from these succes•
5.1 Experimental settings                                     sive evaluations in various ways (average, best, random...) 
                                                              However, we found that with this problem, increasing the 
We used two algorithms for this problem: a simple genetic     number of collaborators (and thus the number of evaluation 
algorithm, adapted with an inner crossover operator (as de•   rounds) brought absolutely nothing, and was even damag•
scribed above), and a full-featured cooperative revolution•   ing if the final score was anything else than the best score 
ary algorithm. Both methods were used with 7, then with 15    found. The most successful method was simply to evaluate 
mice. We used 100 populations of 7 (resp. 15) mice for the    each mouse by joining it with the best individual from each 
first algorithm, and 7 (resp. 15) pools of 100 mice for the sec• pool, exactly as in the enforced subpopulations algorithm. 
ond one. Each algorithm was run several times with different  This is not a surprising result, however, as we will explain 
random seeds.                                                 it below. 
  In the first algorithm, reproduction of populations occured 
through tournament selection and 1-point crossover at a rate 
                                                              5.2 Comparison of results 
of 60%. Every time two populations were thus crossed to•
gether, an inner crossover rate of 10% was applied, mean•     The first algorithm (simple genetic algorithm with two levels 
ing that each mouse in the offspring had a 10% chance to be   of crossover) proved remarkably efficient with this problem. 
the result of the crossing of the parents' corresponding mice. All runs led to the strategy described in section 4.2, whatever 
In the second algorithm, reproduction within each pool oc•    the number of mice within the simulation, although of course 
cured by tournament selection and 1-point crossover at a rate it took more time with 15 mice than with 7. Two typical runs, 
of 30%, which proved to be the most efficient. In both al•    with respectively 7 and 15 mice, are described in Figure 3. 
gorithms, mutation appeared only when crossing two mice          Cooperative coevolution led to different results. With 7 
together, by choosing a new random value for a connection     mice, in some runs, the algorithm failed to evolve any com•
weight with a 5% probability.                                 petitive behaviour. In other runs it managed to find the good 
  Note that in both method, we use a limited form of elitism, strategy, but took more evaluations than with the previous al•
in that the best individual from a given generation was pre•  gorithm. Many runs, however, achieved performance compa•
served in the next generation. This ensured better perfor•    rable with that of the simple genetic algorithm. With 15 mice, 
mance - and made the obtained results even more puzzling,     the success rate was much lower. Most runs did not converge 
as explained below.                                           after 10000 evaluations. Others converged, then suddenly di•
  Finally, cooperative coevolution specifies that each agent  verged quickly. All the runs, with 7 or 15 mice, exhibited a 
must be evaluated with a set of collaborators. Several sets of intriguing pattern of oscillation. 


650                                                                                             MULTIAGENT SYSTEMS                                                                may not be selected again, because the rest of the population 
                                                               has co-adapted towards a different state. However, it is pos•
                                                               sible that the this agent is selected again, thus resulting in a 
                                                               sharp increase in performance - until the next "error" in the 
                                                               evolutionary process. This seems to cause the up-and-down 
                                                               oscillations in the fitness curve. 
                                                                 There is little we can do about this problem. As we said, 
                                                               increasing the number of collaborators in a classical way (that 
                                                               is, by selecting random sets of collaborators) simply does not 
                                                               work, which is understandable: in this problem, the solution 
                                                               requires highly interdependent behaviours. An agent's per•
                                                               formance can only be good if other agents play their role. 
                                                               Evaluating each agent by fitting it in a random population is 
                                                               not likely to help reach this state. In a problem with many 
       0 2000 4000 6000 8000 1000012000140001600C 
                                                               strongly interdependant agents, adding random collaborators 
                                                               doesn't seem to be a helpful solution. * 
Figure 4: Performance of the best run for a cooperative co-      We could devise more elaborate schemes (e.g. using the 
evolutionary algorithm. The curves indicate the performance    second-best from each pool as a second set of evaluators) but 
of the best individual in the currently evaluated pool, and the that would not suppress the problem. It seems that in prob•
average fitness of individuals in this pool. Notice the brutal lems with a non-deterministic component, cooperative coevo•
variations in the curves.                                      lution is essentially fragile, or at least, more so than the stan•
                                                               dard genetic algorithm. 
  The fitness curve described in Figure 4 shows a good ex•
ample of this pattern. It is the result of the most successful 6 Conclusion 
run with 15 mice. The population seems to converge towards 
                                                               We have described possible pitfalls in cooperative coevolu•
a better behaviour, but then it suddenly diverges and seems to 
                                                               tion, and we have proposed a way to apply the canonical ge•
loose all that had been found. The pattern starts again a few 
                                                               netic algorithm to populations of agents. We have applied 
cycles later. 
                                                               both algorithms to a non-trivial problem, the blind mice prob•
  This is not what one could expect. In this algorithm, 
                                                               lem, that is both conceptually simple and non-trivial to solve. 
the population remains quite stable, being composed of the 
                                                               For this problem, the populations-oriented genetic algorithm 
best individual from each pool. Only one individual can be 
                                                               significantly outperformed cooperative coevolution. We pro•
changed at each cycle, and only for a better one, since the best 
                                                               posed an explanation for the difference in results, based on 
individual from previous generation is preserved. So how do 
                                                               the intrinsic fragility of cooperative coevolution in regard to 
we explain these brutal changes in fitness happening every 
                                                               excessive stochastic variations in the evaluation proces. 
now and then? 
                                                                 As a final note, an old French proverb goes: "Quand le 
  Our explanation is that this pattern is the result of coop•
                                                               chat n'est pas la, les souris dansent" (When the cat is away, 
erative revolution's main drawback: when the number of 
                                                               the mice dance). Artificial evolution has demonstrated that 
agents grow, and when the result of evaluation undergoes im•
                                                               the opposite can be true as well. 
portant stochastic variations, its performance is bound to de•
crease. 
  What happens is that at some point, the algorithm does find  References 
a "good" population, and keeps optimizing it by replacing      [Goldberg, 1989] David E. Goldberg. Genetic Algorithms in 
each agent by a better one in turn - as it should do. But then at Search, Optimization and Machine Learning. Addison-
some point, because of the stochastic variations in the evalu•   Wesley, 1989. 
ation process, one of these "good" agents is found to perform 
                                                               [Gomez and Mikkulainen, 1997] F. Gomez and R. Mikku-
not as well as another, non-optimal one. In classical evo•
                                                                 lainen. Incremental evolution of complex general behav•
lutionary algorithms, these occasional mistakes arc blurred 
                                                                 ior. Adaptive Behavior, 5:317-342, 1997. 
over a large number of trials and errors, and besides, one 
population's performance has no impact on another's. But       [Holland and Reitman, 1978] J. H. Holland and J. S. Reit-
in cooperative coevolution, this exceptional case is sufficient  man. Cognitive systems based on adaptive algorithms. In 
to wreck the behaviour of the whole population, because it 
can replace an essential agent (say, a Dancer) by a poorly        'Strangely enough, in [Potter and DeJong, 1994], random collab•
performing agent (say, a random wanderer).                     orators were introduced precisely for problems with strongly inter•
                                                               acting variables, such as the Roscnbrock function; however, for this 
  Then other agents from other pools are evaluated in turn,    kind of problem, this idea was only applied to two-variables func•
but since the conditions have changed dramatically, other      tions. In this case, choosing a random collaborator in addition to the 
agents are selected instead of the previous, quasi-optimal     better one may allow the algorithm to escape a local minima. But as 
ones. Thus, an error in the selection of one agent has an      the number of agents grows, it seems that evaluating one agent with 
influence over all other pools. When the evaluation cycle      a set of totally random collaborators could hardly bring any valuable 
comes back to the first pool, the previously essential agent   information. 


MULTIAGENT SYSTEMS                                                                                                   651 