   Kernel Carpentry for Online Regression using Randomly Varying Coefﬁcient
                                                Model

                Narayanan U Edakunni      ∗   Stefan Schaal   †   Sethu Vijayakumar     ∗†
               ∗ School of Informatics, University of Edinburgh, Edinburgh EH9 3JZ, UK
   † Department of Computer Science, University Southern California, Los Angeles, CA 90089, USA
              n.u.edakunni@sms.ed.ac.uk, sschaal@usc.edu, sethu.vijayakumar@ed.ac.uk

                    Abstract                          the model selection guarantees that Bayesian methods pro-
                                                      vide while retaining the ﬂexibility provided by nonparametric
    We present a Bayesian formulation of locally      localised learning.
    weighted learning (LWL) using the novel concept     One of the most attractive characteristics of LWPR-like lo-
    of a randomly varying coefﬁcient model. Based     calised learning schemes is its independent learning rules for
    on this, we propose a mechanism for multivariate
                                                      each individual local model, which combines or blends the
    non-linear regression using spatially localised lin- outputs only at the stage of prediction. In addition to avoid-
    ear models that learns completely independent of  ing negative interference [Schaal and Atkeson, 1998],this
    each other, uses only local information and adapts
                                                      property also allows asynchronous learning of local models
    the local model complexity in a data driven fashion. leading to improved efﬁciency. We preserve this property in
    We derive online updates for the model parameters our model by building a generative probabilistic model for
    based on variational Bayesian EM. The evaluation
                                                      each individual local model and derive corresponding learn-
    of the proposed algorithm against other state-of-
                                                      ing rules. We show that our novel formulation performs ro-
    the-art methods reveal the excellent, robust gener- bustly in estimating local model complexity, competes with
    alization performance beside surprisingly efﬁcient
                                                      the state-of-the-art methods in generalization capability, can
    time and space complexity properties. This paper,
                                                      be extended to learn in truly incremental fashion, i.e., without
    for the ﬁrst time, brings together the computational storing data and is surprisingly efﬁcient in both computational
    efﬁciency and the adaptability of ‘non-competitive’
                                                      complexity and space.
    locally weighted learning schemes and the mod-
    elling guarantees of the Bayesian formulation.
                                                      2   Randomly Varying Coefﬁcient model
1  Introduction                                       Modelling spatially localized linear models using a proba-
                                                      bilistic framework involves deriving a formulation that allows
Locally weighted projection regression (LWPR) [Vijayaku-
                                                      to model the ﬁt, in our case a linear ﬁt, and the bandwidth at
             ]
mar et al., 2005 is a prime example of recent developments a particular location in the input space. Each of these local
in the area of localised learning schemes that have resulted in models can then be combined to provide a prediction for a
powerful non-linear regression algorithms capable of operat-
                                                      novel data. Additionally, in order for the local models to be
ing in real-time, high dimensional, online learning scenarios.
                                                      independent, each of them should be capable of modelling
They have been proven to work on many real world appli- the entire data by learning the correct bandwidth that parti-
cations including, for e.g., supervised learning of sensorimo-
                                                      tions the data into two parts – one which corresponds to the
tor dynamics in multiple degree of freedom anthropomorphic
                                                      linear region of interest and the other which does not. In
             [                     ]
robotic systems Vijayakumar et al., 2002 .            this paper, we accomplish this by formulating a probabilis-
  All locally weighted schemes (including LWPR) have to
                                                      tic model called Randomly Varying Coefﬁcient(RVC) model
determine a region of validity of the local models, i.e., an
                                                      which builds upon the idea of a random coefﬁcient model
adaptive local distance metric, in a data driven fashion. This [Longford, 1993].
is usually achieved by minimising some sort of cross vali-
                                                        For a locally linear region centered around xc a generative
dation cost on the ﬁt using gradient descent methods. How- model for the data points can be written as:
ever, the initialization of the local complexity parameter or
                                                                               T
distance metric, the forgetting factor and the learning rates             yi = βi xi +                (1)
involved in the gradient method necessitate careful hand tun-
                                                                           T   T
ing of multiple open parameters in existing methods. This where xi ≡ [(xi − xc) , 1] represents the center sub-
                                                                                            (1)   (d+1) T
may not be trivially achieved in many real world problems tracted, bias augmented input vector, βi ≡ [βi ...βi ]
with limited prior domain knowledge. Also, there exists no represents the corresponding regression coefﬁcient and  ∼
proper probabilistic formulation of the local weighted learn- N (0,σ2) is the Gaussian mean zero noise with a standard de-
ing framework – a necessary development in order to exploit viation σ. The data is assumed to have been generated in an

                                                IJCAI-07
                                                   762                      Region of locality              higher conﬁdence over larger regions of the data. Therefore,
                                                      we use a Gamma regularizer prior over the bandwidth param-
                                                      eters such that it favors relatively small values of hj leading
                                                      to more localised models:
                             c
               b                                                        2
                          a                                            hj ∼ Gamma(aj ,bj )             (4)
                                                      We   shall further assign noninformative Normal prior
                                                      N (μ, S) for the parameter βˆ and a noninformative inverse
                     b                                Gamma prior with hyperparameters c and d for σ.Weas-
                                                      sume a uniform prior for the regularizer hyperparameters aj
                                                      and bj. Fig. 2 summarizes the resultant probabilistic model
                                                      for a single local model. In this model, one can marginalize
                                                      out the hidden variables βi to obtain
                                                                            Z
 Figure 1: Variation of prior with the location of the input ˆ                      T    2      ˆ
                                                       P (yi|β,σ,h1 ...hd+1)= P (yi|βi xi,σ )P (βi|β, C i)dβi

                                                                 ˆT    T        2
                                                        ⇒ yi ∼N(β   xi, xi C ixi + σ )
                                                                                                       (5)
                                                      It is interesting to note that the form of likelihood in Eq. (5)
                                                      corresponds to a heteroscedastic regression and will be used
                                                      in later sections for prediction. In the next section we deal
                                                      with computing the parameter updates and the resultant en-
                                                      semble posteriors in an efﬁcient manner.

                                                      3Learning
                                                      Our objective is to learn the posterior over the parameters βˆ,
                                                      hj, σ and to obtain point estimates for the hyperparameters –
                                                      aj, bj. The joint posterior is given by:
         Figure 2: The ‘local’ generative model                                      ˆ
                                                            ˆ                    P (y, β, h,σ,a, b,c,d,μ, S)
                                                        P (h, β,σ|y, a, b,c,d,μ, S)=
                                                                                    P (y, a, b,c,d,μ, S)
                                                                                                       (6)
IID fashion. Crucially, we allow the regression coefﬁcient to                              2    2   T
                                                      wherewehaveusedh   to denote the vector [h1 ...hd+1] and
be a random variable with a prior distribution given by:                              T                T
                                                      y denotes the training data [y1 ...yN ] , a ≡ [a1 ...ad+1]
                          ˆ                                              T
                   βi ∼N(β, C i)                (2)   and b ≡  [b1 ...bd+1] . However, the posterior over the
                                                      parameters is rendered intractable due to the difﬁculty in
where we have assumed that each βi is generated from a
                      βˆ                              evaluating the denominator of Eq. (6). This necessitates
Gaussian centered around with the conﬁdence being repre- the use of variational Bayesian EM to evaluate the posterior
sented by the covariance Ci. The covariance itself is deﬁned ˆ
                                                     P (h, β,σ|y, a, b,c,d,μ, S) and learn the regulariser hyper-
to be proportional to the distance of xi from the center. This
has the effect that for points that lie close to the center, the dis- parameters a and b.
                           ˆ
tribution of βi is peaked around β resulting in a linear region
around the center. This has been illustrated schematically in 3.1 Variational approximation
Fig. 1 where point c is the center of the local model: for a To learn the parameters of the model we can maximize the
point a that lies close to c we assign a prior that is fairly tight marginal log likelihood with respect to the parameters treat-
around the mean whereas for a point b that lies away from c ing βi as the hidden variables. The marginal log likelihood is
the prior is much broader. One can consider various distance given by:
functions to index the variation of the covariance matrix C.
                                                      L   =ln(y|a    b     μ S)
Here, we restrict ourselves to a diagonal version, each diago- ZP   , ,c,d, ,
nal element varying quadratically with x as:                                     ˆ
                                                          =ln    P (y, β1 ...βN , h, β,σ|a, b, μ, S,c,d)dβ1 ...dβN
                   T             2   T    2
   Ci(j, j)=((xi − xc) (xi − xc)+1)/hj = xi xi/hj (3)
                                                              h βˆ
                                                             d d "dσ
where hj is the bandwidth parameter of the kernel deﬁning      Z  Y
the extent of the locality along the -th dimension. This                           ˆ
                               j                          =ln        P (yi|β ,σ)P (β |β,h1,...hd+1)
choice of the kernel parametrization allows us to use a con-               i      i
                                                                   i                      #
jugate Gamma prior over hj. The higher values of hj im-      Y
ply lesser variation amongst the coefﬁcients βi and hence,         2        ˆ         2
                                                                P (hj |aj ,bj )P (β|μ, S)P (σ |c, d) dβ1 ...dβN
larger regions of linearity. Although the bandwidth modu-     j
lates the bias-variance tradeoff, an unconstrained likelihood
                                                                         ˆ
maximization will, in general favor large hj since it implies a dh1 ...dhd+1dβdσ                        (7)

                                                IJCAI-07
                                                   763                                                             X
                                                         ˜          −1    −1 −1
Using Jensen’s inequality, the objective function that lower S =( Ci + S  ) ,                       (16)
bounds L is given by:                                         i
      Z h                                                     X
                                                            ˜        −1      −1
                      ˆ  2                              μ˜ = S(  Ci  ν i + S μ)
 F =     Q(β1 ...βN , h, β,σ )                                                                        (17)
                                     #                         i
                        2
                     ˆ                                  ˜j = j +   2
      P (y, β1 ...βN , h, β,σ |a, b, μ, S,c,d)          a   a   N/                                    (18)
    ln                                 dβ1 ...dβN               X  h                       i
              (β    β  h  βˆ 2)                         ˜                     2         ˜       T
            Q   1 ... N , , ,σ                          bj = bj +   (νi,j − μ˜ i,j ) + Gi,jj + Sjj /(2xi xi) (19)
      ˆ   2                                                      i
   dhdβdσ
                                                (8)   Here, νi,j and μ˜ i,j denote the j-th element of the respective
                                         ˆ                               ˜
The  optimal  value  for  Q(β1 ...βN , h, β,σ) that   vectors and Gi,jj and Sjj denotes the j-th diagonal element.
makes the  bound  tight is given by the joint poste-
      (β     β   h βˆ |y)                                      c˜ = c + N/2                           (20)
rior P   1 ... N , , ,σ    but  since this posterior                  X  h                   i
                                                               ˜                T  2    T
is intractable, we make an approximation by assum-             d = d +    (yi − νi xi) + xi Gixi /2   (21)
ing that the posterior over the variables is indepen-                  i
                              (β    β   h βˆ  )=
dent and can be expressed as Q 1 ... N , , ,σ
                 2      ˆ      2                      We also need to learn the point estimates for the regulariser
   Q(βi|y)    Q(hj |y)Q(β|y)Q(σ |y).  This form  of
  i         j                                         hyperparameters aj and bj. Maximum likelihood value for
approximation is often called an ensemble variational ap- the hyperparameters aj and bj can be found by maximiz-
proximation, details of which can found in [Beal, 2003]. ing the bound Fapprox given by Eq. (9) with respect to these
Substituting the factorised approximation in Eq. (8) we get:
          X  h                                        hyperparameters keeping the posterior distributions Q ﬁxed.
                                                      Considering only the terms involving the hyperparameters:
 Fapprox =    ln P (yi|βi,σ)Q ,Q
                           βi  σ2
           i                                                        Z
                                              #                          2   ˜       2        2
          D                  E                                  E =   Q(hj |a˜j , bj )lnP (hj |aj ,bj )dhj
                  ˆ
        +  ln P (βi|β,h1 ...hd+1)
                              Qβ ,Qh ...Qh ,Q
                                i   1   d+1 βˆ                   E
         X   ˙           ¸     D           E          Maximising   with respect to the hyperparameters is equiv-
        +    ln  ( 2|   )     +  ln (βˆ|μ S)          alent to minimising the KL divergence between the distribu-
               P  hj aj ,bj Q      P    ,
                           hj               Q   (9)
          j                                  βˆ       tions Q and P . Since the posterior Q and prior P share the
                           X  D      E                same parametric form, KL divergence is minimised when the
         ˙      2   ¸
        + ln P (σ |c, d) −     ln Q                   parameters of these distributions match. This leads to the sim-
                     Q 2           β
                       σ            i Qβ
                         ﬁ  i  ﬂ        i             ple update rule for the hyperparameters given by:
         X   ˙     ¸
                                                                                     ˜
        −    ln  h     −  ln       −ln   2                             j =˜j    j = j
               Q  j Q       Q ˆ         Qσ  Q 2                         a    a ,b    b                (22)
                     hj       β              σ
          j                      Q ˆ
                                  β                                     μ S
where . denotes the expectation with respect to the dis- The hyperparameters , , c and d are initialised such that the
        Q                                             corresponding priors are non-informative. An initialisation of
tribution Q. The optimal values of the posterior probabili- μ = 0 S =10−3×I =10−3     =10−3
ties can be computed iteratively by maximizing the functional ,         , c      and d       ensures such a
Fapprox with respect to each individual posterior distribution condition. On the other hand the regulariser hyperparameters
keeping the other distributions ﬁxed akin to an EM procedure. a and b are initialised such that it encourages small h.A
Such a procedure can be shown to improve our factorised ap- value of a =1and a sufﬁciently large value for b ensures
proximation of the actual posterior in each iteration. Skipping such a bias. These are the settings used by RVC for all the
the derivation, such a procedure yields the following posterior evaluations carried out in Sec. 4.
distributions:

              Q(βi|y) ∼N(νi, Gi)               (10)   3.2  Prediction using the committee of local models
                 ˆ          ˜
               Q(β|y) ∼N(μ˜, S)                (11)   We have dealt so far with building a coherent probabilis-
               ( 2|y) ∼      (˜ ˜ )                   tic model for each local expert and have derived inference
              Q hj     Gamma  aj , bj          (12)   procedures to estimate the parameters of individual model.
                 2                 ˜
              Q(σ |y) ∼ Inv-Gamma(˜c, d)       (13)   Given the ensemble of trained local experts, in order to pre-
                                                      dict the response yq for a new query point xq,wetakethe
where                  ˙  ¸
            G  =(x xT    2 + C −1)−1                normalised product of the predictive distribution of each lo-
              i    i i / σ      i                     cal expert. This is close in spirit to the paradigm of Product
                                T
                        C i xix C i        (14)   of Experts [Hinton, 1999] and the Bayesian Committee Ma-
               = C −          i
                   i     2    T                             [         ]
                       σ  + x C i xi              chines Tresp, 2000 . The predictive distribution of each local
                              i                       expert is given by:
where the second part has been derived by making use of
                                           C   =              Z
the Sherman-Morrison  Woodbury  theorem. Here i                    ˆ        ˆ      2            ˆ  2
    (xT x    2     )      2                            P (yq|y)=   P (yq|β,σ,h)Q(β|y)Q(σ |y)Q(h|y)dhdβdσ
diag  i i/ hj  Q(h2) and σ   is the expectation with re-
                 j                                                                                    (23)
spect to Qσ2 . Furthermore,˙ ¸ using results from Eq. (14),   (  |βˆ h)
       ν = G  ( x    2 + C −1 μ˜ )                  where P  yq  ,σ,   has the form given by Eq. (5). We
        i    i yi i/ σ      i    i                                           ˆ
                            “         ”               can further integrate out β from Eq. (23), but cannot do
                 C i xi         T            (15)                2                                 2
         =                    i − x μ˜ + μ˜           thesameforσ    and h. Hence, we approximate Q(σ |y)
            ( 2 + xT C  x ) y i  i     i
             σ     i   i  i                           and Q(h|y) by a delta function at the mode which implies

                                                IJCAI-07
                                                   764   2
 (  |y) ≈   2        (h|y) ≈   2                      Algorithm 1 Training a local model
Q σ       δσmode and Q       δhmode . The ﬁnal predic-
tive distribution for the k-th local model is:         1: Initialise hyperparameters: Θ0 ≡{μ0, S0,c0,d0, a0, b0}.
             T       T ˜                   2
   yq,k ∼N(μ˜ xq,k, xq,k (Sk + C kh )xq,k + σmode)     2: for i =1to N do
                               mode                              x
where xq,k refers to the query point with the k-th center sub- 3: Input i, yi
tracted and augmented with bias. Blending the prediction of 4: repeat
                                                                                            ˜
different experts by taking their product and normalising it 5: Estimate posterior hyperparametersΘi using Θi and
results in a Normal distribution given by:
                         P                                    Eq. (14), (15) and Eqs. (24) - (29).
                                T                                                               a    b
           2               k αkμ˜ k xq,k 2   1         6:     Estimate values of the hyperparameters and of
 yq ∼N(μ, ζ ) where  μ =    P       ,ζ   = P     .
                              k αk           k αk             the regulariser prior using Eq. (22).
                                                       7:   until convergence of posteriors
Here, μ is a sum of the means of each individual expert            ˜
weighted by the conﬁdence expressed by each expert in its 8: Θi+1 = Θi
                 2
own prediction αk, ζ is the variance and αk is the precision 9: end for
of each expert:
  =1  (xT  (S˜ + C )x +  2)  C  =     {xT x    2 }
αk   /  q,k k    k  q,k σk ,  k   diag  q,k q,k/hj,k  3.4  Complexity analysis
3.3  Online updates                                   The time complexity of the algorithm is dominated by the
                                                                   G                               G
The iterative learning rules to estimate the posteriors over pa- computation of i in Eq. (14). The equations that use i are
rameters given the appropriate prior and the data, represented Eq. (27) and Eq. (29) and these can be rewritten to avoid ex-
                                                      plicit computation of Gi. Eq. (27) requires only the diagonal
by Eqs. (16)-(21), can be rewritten in the form of online up-   G                          ( )
dates by exploiting the Bayesian formalism. In a batch mode elements of i which can be computed in O d since
of posterior evaluation, we have                                                   2   2
                                                      Gi(j, j)=Ci(j, j)−(Ci(j, j)xi(j)) /(σ +γi) using Eq. (14)
                     YN
                   =    (       i) ×                             T
          posteriorN    likelihood prior0             where γi = xi Cixi which can also be computed in O(d) due
                      i                               to the fact that Ci is diagonal. On the other hand, Eq. (29) re-
                                                                           T
The same can be expressed as a set of online updates: quires the evaluation of xi Gixi which in turn can be written
   posterior = likelihoodi × priori; prior = posterior
          i                        i+1         i      down as:                     2
                                                                        xT G x =  σ γi
Therefore we can transform the batch updates that we had                 i  i i   2
derived earlier into online updates given by :                                   σ + γi
   ˜       −1    −1 −1                                                            ( )
   Si =(Ci  + Si )                           (24)   and can also be computed in O d . Furthermore, the ma-
       ˜     −1      −1                               trix inverses in Eq. (24) and Eq. (25) can also be computed
  μ˜ = Si(Ci  ν i + Si μ )                   (25)
    i                    i                            in O(d) due to the fact that Si and Ci are diagonal matri-
  a˜i,j = ai,j +1/2                            (26)
            h                        i                ces. Therefore the overall time complexity per online update
  ˜                    2         ˜        T           is O(dM) where d is the number of dimensions and M the
  bi,j = bi,j + (ν i,j − μ˜ i,j ) + Gi,jj + Si,jj /(2xi xi) (27)
                                                      number of local models. The algorithm doesn’t require any
   c˜i = ci +1/2                               (28)                                        (  )
           h                   i                      data points to be stored and hence, has a O M space com-
   ˜              T   2   T                           plexity for the sufﬁcient statistics stored in the local models.
   di = di + (yi − νi xi) + xi Gixi /2         (29)
                                                      The independence of the local models also means that the ef-
We repeat the above updates for a single data point {xi,yi} fective time complexity can be brought down to O(d) using
till the posteriors converge – here, Θ˜ represents the posterior M parallel processors. The time complexity for prediction is
of Θ.Forthe(i +1)-th point, we then use posterior of i-th O(dM) including the evaluation of mean and the conﬁdence
step as the prior as illustrated in Algorithm 1.      bounds. We can see from this analysis that the algorithm is
                                                      very efﬁcient with respect to time and space (in fact it matches
Addition/deletion of local models                     LWPR’s efﬁciency) and hence, is a strong candidate for situ-
The complexity of the learner is adapted by the addition and ations which require real time and online learning.
deletion of local models. When the predictive likelihood for a
new data point is sufﬁciently low then one can conclude that
the complexity of the learner needs to be increased by adding 4 Evaluation
a new local model. This leads to the simple heuristic for the In this section, we demonstrate the salient aspects of the RVC
addition of a local model wherein a local model is added at model by looking at some empirical test results, compare the
a data point when the predictive probability for the particular accuracy and robustness against state of the art methods and
training data is less than a ﬁxed threshold. The data point evaluate its performance on some benchmark datasets.
serves as the center for the added local model.         Fig. 3(a) shows the local linear ﬁts (at selected test points)
  When two local models have sufﬁcient overlap in the re- learned by RVC from noisy training data on a function with
gion they model, then one of them is redundant and can be varying spatial complexity. Such functions are extremely
pruned. The overlap between two local models can be deter- hard to learn since models with high bias tends to oversmooth
mined by the difference in the conﬁdence expressed in their the nonlinear regions while more complex models tend to
prediction for a common test point. The addition and deletion ﬁt the noise. One can see that the linear ﬁt roughly corre-
heuristics that have been used here is similar to the ones used sponds to the tangential line at the center of each local model
in [Schaal and Atkeson, 1998].                        as expected. A more signiﬁcant result is the adaptation of

                                                IJCAI-07
                                                   765   2
       Data points                     2                                  2
       True fn.                             Data points                        Data points
  1.5
       Linear fits                          True fn.       RVC                 True fn.        GP
                                      1.5   Learnt fn.                   1.5   Learnt fn.
   1                                        Confidence bounds                  Confidence bounds
 y
  0.5                                  1                                  1
                                    y                                  y

   0                                  0.5                                0.5
   3
                 x
                                       0                                  0
   2
  Bandwidth
   1                                 −0.5                               −0.5
    0    2     4    6     8    10       0    2     4     6     8     10    0     2    4     6     8     10
                 x                                    x                                  x
                 (a)                                 (b)                                (c)

Figure 3: (a)Local ﬁts and bandwidth adaptation. Fit and conﬁdence bounds learned by (b) RVC model and by (c) GP model.

the local bandwidth. The bottom section of Fig. 3(a) plots we have used RVC in the batch mode using the updates that
the converged locality measure computed as product of the we derived in Sec. 3(c.f. Eqs. 14 - 21). The subsequent eval-
bandwidth parameters along each input dimension - illustrat- uations in this section make use of the online updates derived
ing the ability to adapt the local complexity parameter in a in Sec. 3.3.
data driven manner. For this illustration, the local centers are
placed in a dense, uniform grid in input space. Using the same
                                                              100
target function, we compare the ﬁts and conﬁdence bounds           Motorcycle data: GP
learned by RVC and Gaussian Processes (GP) [Williams,
1998] in Fig. 3(b) and (c). It is important to note that we   50
have deliberately avoided using training data in [5.5,6.5] and
the conﬁdence bounds of RVC nicely reﬂect this.                0
                                                            y

                                                             −50
       100
            Motorcycle data: RVC
                                                             −100                  Data points
        50
                                                                                   Learnt fn.
                                                                                   Confidence bounds
                                                             −150
         0                                                      0    10   20    30   40   50    60
                                                                                x
     y

       −50
                                                      Figure 5: Fit and conﬁdence bounds for the motorcycle
                                                      dataset learned by the Gaussian Processes model
      −100                   Data points
                             Learnt fn.                 To compare the online learning characteristics, we trained
                             Confidence bounds
      −150                                            the three candidate algorithms on 500 data points from the
         0    10    20   30    40   50   60                                                 ∼N(0   0 052)
                          x                           sinc function corrupted with output noise:  , .  .
                                                      After each training data was presented to the learner, the er-
                                                      ror in learning was measured using a set of 1000 uniformly
Figure 4: Fit and conﬁdence bounds for the motorcycle distributed test points. The RVC model was allowed only a
dataset learned by the RVC model (local models were cen- single EM iteration for each data point to ensure a fair com-
tered at 20 uniformly distributed points along the input) parison with LWPR. The resulting error dynamics is shown in
                                                      Fig. 6(a). In this comparison, GP exhibits a sharply decreas-
  Our next experiment aims to illustrate the ability of RVC ing error curve which is not surprising considering that it is
to model heteroscedastic data (i.e., data with varying noise essentially a batch method and stores away all of the train-
levels). Fig. 4 illustrates the ﬁt and the conﬁdence inter- ing data for prediction. When we compare RVC with LWPR,
val learnt on the motorcycle impact data discussed in [Ras- we ﬁnd that RVC converges faster while using roughly sim-
mussen and Gharamani, 2000]. Notice that the conﬁdence ilar number of local models. This can be attributed to the
interval correctly adapts to the varying amount of noise in the Bayesian learning rules of RVC that estimates the posterior
data as compared to the conﬁdence interval learnt by a GP over parameters rather than point estimates. Since the poste-
with squared exponential kernel shown in Fig. 5. This abil- rior is a product of likelihood and prior, in the event of sparse
ity to model non-stationary functions is another advantage of data (as in the initial stages of online learning), the prior en-
RVC’s localised learning. In the evaluations presented so far, sures that the posterior distributions assigned to the parame-

                                                IJCAI-07
                                                   766