                              Bayesian Information Extraction Network 

                                          Leonid Peshkin and Avi Pfeffer 
                                                  Harvard University 
                                            Cambridge, MA 02138, USA 
                                         {pesha,avi}@eecs.harvard.edu 


                        Abstract                               tion and paste it into personal organizers. The goal of an IE 
                                                               system is to automatically identify target fields such as lo•
     Dynamic Bayesian networks (DBNs) offer an ele•
                                                               cation and topic of a seminar, date and starting time, ending 
     gant way to integrate various aspects of language 
                                                               time and speaker. Announcements come in many formats, 
     in one model. Many existing algorithms developed 
                                                               but usually follow some pattern. We often find a header with 
     for learning and inference in DBNs are applicable 
                                                               a gist in the form "PostedBy: john@host .domain; 
     to probabilistic language modeling. To demonstrate 
                                                               Who: Dr. Steals; When: 1 am;" and so forth. 
     the potential of DBNs for natural language process•
                                                               Also in the body of the message, the speaker usually precedes 
     ing, we employ a DBN in an information extraction 
                                                               both location and starting time, which in turn precedes end•
     task. We show how to assemble wealth of emerg•
                                                               ing time as in: MDr. Steals presents in Dean 
     ing linguistic instruments for shallow parsing, syn•
                                                               Hall at one am.'' The task is complicated since some 
     tactic and semantic tagging, morphological decom•
                                                               fields may be missing or may contain multiple values. 
     position, named entity recognition etc. in order to 
     incrementally build a robust information extraction         This kind of data falls into the so-called semi-structured 
     system. Our method outperforms previously pub•            text category. Instances obey certain structure and usually 
     lished results on an established benchmark domain.        contain information for most of the expected fields in some 
                                                               order. There are two other categories: free text and structured 
                                                               text. In structured text, the positions of the information fields 
1 Information Extraction                                       are fixed and values are limited to pre-defined set. Conse•
Information extraction (IE) is the task of filling in template quently, the IE systems focus on specifying the delimiters and 
information from previously unseen text which belongs to a     order associated with each field. At the opposite end lies the 
pre-defined domain. The resulting database is suited for for•  task of extracting information from free text which, although 
mal queries and filtering. IE systems generally work by de•    unstructured, is assumed to be grammatical. Here IE systems 
tecting patterns in the text that help identify significant in• rely more on syntactic, semantic and discourse knowledge in 
formation. Researchers have shown [Freitag and McCallum,       order to assemble relevant information potentially scattered 
1999; Ray and Craven, 2001] that a probabilistic approach      all over a large document. 
allows the construction of robust and well-performing sys•       IE algorithms face different challenges depending on the 
tems. However, the existing probabilistic systems are gen•     extraction targets and the kind of the text they are embedded 
erally based on Hidden Markov Models (HMMs). Due to            in. In some cases, the target is uniquely identifiable (single-
this relatively impoverished representation, they are unable   slot), while in others, the targets are linked together in multi-
to take advantage of the wide array of linguistic information  slot association frames. For example, a conference schedule 
used by many non-probabilistic IE systems. In addition, ex•    has several slots for related speaker, topic and time of the pre•
isting HMM-based systems model each target category sepa•      sentation, while a seminar announcement usually refers to a 
rately, failing to capture relational information, such as typ• unique event. Sometimes it is necessary to identify each word 
ical target order, or the fact that each element only belongs  in a target slot, while some benefit may be reaped from par•
to a single category. This paper shows how to incorporate a    tial identification of the target, such as labeling the beginning 
wide array of knowledge into a probabilistic IE system, based  or end of the slot separately. Many applications involve pro•
on dynamic Bayesian networks (DBN)—a rich probabilistic        cessing of domain-specific jargon like lnternetese—a style of 
representation that generalizes HMMs.                          writing prevalent in news groups, e-mail messages, bulletin 
   Let us illustrate IE by describing seminar announcements    boards and online chat rooms. Such documents do not follow 
which got established as one of the most popular bench•        a good grammar, spelling or literary style. Often these are 
mark domains in the field [Califf and Mooney, 1999; Freitag    more like a stream-of-consciousness ranting in which ascii-
and McCallum, 1999; Soderland, 1999; Roth and Yih, 2001;       art and pseudo-graphic sketches are used and emphasis is pro•
Ciravegna, 2001]. People receive dozens of seminar an•         vided by all-capitals, or using multiple exclamation signs. As 
nouncements weekly and need to manually extract informa•       we exemplify below, syntactic analysers easily fail on such 


INFORMATION EXTRACTION                                                                                               421 corpora.                                                       until no improvement can be made. Rules in RAPIER are for•
   Other examples of Hi application domains include job        mulated as lexical and semantic constraints and may include 
advertisements [Califf and Mooney, 1999] (RAPIER), ex•         PoS tags. WHISK [Soderland, 1999] uses constraints similar 
ecutive succession [Soderland, 1999] (WHISK), restaurant       to RAPIER, but its rules are formulated as regular expressions 
guides [Muslea et al ., 2001] (STALKER), biological pub•       with wild cards for intervening tokens. Thus, WHISK encodes 
lications [Ray and Craven, 2001] etc. Initial interest in      a relative, rather than absolute position of tokens with respect 
the subject was stimulated by ARPA\s Message Under•            to the target. This enables modeling long distance dependen•
standing Conferences (MUC) which put forth challenges          cies in the text. WHISK performs well on both single-slot and 
e.g. parsing newswire articles related to terrorism (sec e.g.  multi-slot extraction tasks. 
Mikheev [1998]). Below we briefly review various IE systems      Ciravcgna [2001] presents yet another rule induction 
and approaches which mostly originated from MUC compe•         method (LP)2. He considers several candidate features such 
titions.                                                       as lemma, lexical and semantic categories and capitalization 
   Successful IE involves identifying abstract patterns in the to form a set of rules for inserting tags into text. Unlike other 
way information is presented in text. Consequently, all pre•   approaches, (LP)2 generates separate rules targeting the be•
vious work necessarily relies on some set of textual fea•      ginning and ending of each slot. This allows for more flexi•
tures. The overwhelming majority of existing algorithms        bility in subjecting partially correct extractions to several re•
operate by building and pruning sets of induction rules de•    finement stages, also relying on rule induction to introduce 

fined on these features (SRV, RAPIER, WHISK, LP2). There       corrections. Emphasizing the relational aspect of the domain, 
are many features that are potentially helpful for extracting  Roth and Yoh [2001] developed a knowledge representation 
specific fields, e.g. there are tokens and delimiters that sig• language that enables efficient feature generation. They used 
nal the beginning and end of particular types of informa•      the features in a multi-class classifier SNOW-IE to obtain the 
tion. Consider an example in table 1 which shows how the       desired set of tags. The resulting method (SNOW-IE) works 
phrase ''Doctor Steals presents in Dean Hall                   in two stages: the first filters out the irrelevant parts of text, 
at one am." is represented through feature values. For         while the second identifies the relevant slots. 
example, the lemma "am" designates the end of a time field,      Freitag & McCallum [1999] use hidden Markov models 
while the semantic feature "Title" signals the speaker, and    (HMM). A separate HMM is used for each target slot. No pre•
the syntactic category NNP (proper noun) often corresponds     processing or features is used except for the token identity. 
to speaker or location. Since many researchers use the semi•   For each hidden state, there is a probability distribution over 
nar announcements domain as a testbed, we have chosen this     tokens encountered as slot-fillers in the training data. Weakly 
domain in order to have a good basis of comparison.            analogous to templates, hidden state transitions encode reg•
   One of the systems we compare to (specifically designed     ularities in the slot context. In particular prefix and suffix 
for single-slot problems) is SRV [Freitag, 1998]. It is built  states are used in addition to target and background slots to 
on three classifiers of text fragments. The first classifier is capture words frequently found in the neighborhood of tar•
a simple look-up table containing all correct slot-fillers en• gets. Ray&Craven [2001] make one step further by setting 
countered in the training set. The second one computes the     HMM hidden states in a product space of syntactic chunks 
estimated probability of finding the fragment tokens in a cor• and target tags to model the text structure. The success of the 
rect slot-filler. The last one uses constraints obtained by rule HMM-based approaches demonstrate the viability of proba•
induction over predicates like token identity, word length and bilistic methods for this domain. However, they do not take 
capitalization, and simple semantic features.                  advantage of the linguistic information used by the other ap•
   RAPIER [Califf and Mooney, 1999] is fully based on          proaches. Furthermore, they are limited by using a separate 
bottom-up rule induction on the target fragment and a few to•  HMM for each target slot, rather than extracting data in an 
kens from its neighborhood. The rules are templates specify•   integrated way. 
ing a list of surrounding items to be matched and potentially,   The main contribution of this paper is in demonstrating 
a maximal number of tokens for each slot. Rule generation      how to integrate various aspects of language in a single prob•
begins with the most specific rules matching a slot. Then      abilistic model, to incrementally build a robust information 
rules for identical slots are generalized via pair-wise merging, extraction system based on a Bayesian network. This sys-


422                                                                                        INFORMATION EXTRACTION tern overcomes the following dilemma. It is tempting to use    checking to catch misspelled words. This is done by inter•
a lot of linguistic features in order to account for multiple as• facing with the UNIX ispell utility. 
pects of text structure. However, deterministic rule induction 
                                                               Gazetteer 
approaches seem vulnerable to the performance of feature 
extractors in pre-processing steps. This presents a problem    Our original corpus contains about 11,000 different listems. 
since syntactic instruments that have been trained on highly-  This does not take into account tokens consisting of punc•
polished grammatical corpora, are particularly unreliable on   tuation characters, numbers and such. About 10% are 
weakly grammatical semi-structured text. Furthermore incor•    proper nouns. The question of building a vocabulary auto•
porating many features complicates the model which often       matically was previously addressed in IE literature(see e.g. 
has to be learned from sparse data, which harms performance    Riloff [1996]). We use the intersection of two sets. The first 
of classifier-based systems.                                   set consists of words encountered as part of target fields and 
                                                               in their neighborhood. The second set consists of words fre•
                                                               quently seen in the corpus. Aside from vocabulary there are 
2 Features                                                     two reserved values for Out-of-Vocabulary (OoV) words and 
Our approach is statistical, which generally speaking means    Not-a-Word (NaW). For example see blank slots in the lemma 
that learning corresponds to inferring frequencies of events.  row of Table 1. The first category encodes rare and unfa•
The statistics we collect originates in various sources. Some  miliar words, which are still identified as words according to 
statistics reflect regularities of the language itself, while oth• their part of speech. The second category is for mixed alpha-
ers correspond to the peculiarities of the domain. With this   numerical tokens, punctuation and symbolic tokens. 
in mind we design features which reflect both aspects. There   Syntactic Categories 
is no limitation on the possible set of features. Local fea•
                                                               We used LTChunk software from U.of Edinburgh NLP 
tures like part-of-speech, number of characters in the token, 
                                                               group [Mikheev et al, 1998]. It produces 47 PoS tags from 
capitalization and membership in syntactic phrase are quite 
                                                               UPenn TreeBank set [Marcus et ai, 1994]. We have clus•
customary in the in. In addition one could obtain such char•
                                                               tered these into 7 categories: cardinal numbers (CD), nouns 
acteristics of the word as imagibility, frequency of use, famil•
                                                               (NN), proper nouns (NNP), verbs (VB), punctuation (.), prepo•
iarity, or even predicates on numerical values. Since there is 
                                                               sition/conjunction (IN) and other (SYM). The choice of clus•
no need for features to be local, one might find useful includ•
                                                               ters seriously influences the performance, while keeping all 
ing frequency of a word in the training corpus or number of 
                                                               47 tags will lead to large CPTs and sparse data. 
occurrences in the document. Notice that the same set of fea•
tures would work for many domains. This includes semantic      Syntactic Chunking 
features along with orthographic and syntactic features.       Following Ray&Craven [2001], we obtain syntactic seg•
   Before we move on to presenting our system for proba•       ments (aka syntactic chunks) by running the Sundance sys•
bilistic reasoning, let us discuss in some detail notation and tem [Riloff, 1996] and flattening the output into four cate•
methods we used in preliminary data processing and feature     gories corresponding to noun phrase (NP), verb phrase (VP), 
extraction. To use the data efficiently, we need to factor the prepositional phrase (PP) and other (N/A). Table 1 shows a 
text into "orthogonal" features. Rather than working with      sample outcome. Note that both the part-of-speech tagger 

thousands of listems (generic words1) in the vocabulary, and   and the syntactic chunker easily get confused by non-standard 
combining their features, we compress the vocabulary by an     capitalization of a word "Presents" as shown by incorrect la•
order of magnitude by lemmatisation or stemming. Ortho•        bels in parenthesis. "Steals" is incorrectly identified as a verb, 
graphic and syntactic information is kept in feature variables whose subject is "Doctor" and object is "Presents". Remark•
with just a few values each.                                   ably, other state-of-the-art syntactic analysis tools [Charniak, 
                                                               1999; Ratnaparkhi, 1999] also failed on this problem. 
Tokenization 
Tokenization is the first step of textual data processing. A to• Capitalization and Length 
ken is a minimal part of text which is treated as a unit in sub• Simple features like capitalization and length of word are 
sequent steps. In our case tokenization mostly involves sepa•  used by many researchers (e.g. SRV [Freitag and McCallum, 
rating punctuation characters from words. This is particularly 1999]) Case representation process is straightforward except 
non-trivial for separating a period [Manning and Schutze,      for the choice of number of categories. We found useful intro•
2001] since it requires identifying sentence boundaries. Con•  ducing an extra category for words which contain both lower 
sider a sentence: Speaker: Dr. Steals, Chief                   and upper case letters (not counting the initial capital letter) 
Exec. of rich.com, worth $10.5 mil.                            which tend to be abbreviations. 

Lemmatisation                                                  Semantic Features 
                                                               There are several semantic features which play important role 
We have developed a simple lemmatiser which combines out•
                                                               in a variety of application domains. In particular, it is useful 
come of some standard lematisers and stemmers into a look•
                                                               to be able to recognize what could be a person's name, geo•
up table. Combined with lemmatisation is a step of spell 
                                                               graphic location, various parts of address, etc. For example, 
   'A word is a sequence of alphabetical characters, which has some we are using a list of secondary location identifiers provided 
meaning assigned to it. This would cover words found in general and by US postal service, which identifies as such words like hall, 
special vocabulary as well abbreviations, proper names and such. wing, floor and auditorium. We also use a list of 100000 most 


INFORMATION EXTRACTION                                                                                               423 popular names from US census bureau; the list is augmented              Last target 
by rank which helps to decide in favor of first or last name 
for cases like "Alexander". In general this task could be 
helped by using a hypernym feature of WordNet project [Fell-
baum, 1998]. The next section presents probabilistic model 
which makes use of the aforementioned feature variables. 

3 BIEN 
We convert the IH problem into a classification problem by 
assuming that each token in the document belongs to a target 
class corresponding to ether one of the target tags or the back•
ground (compare to Freitag [1999]). Furthermore, it seems 
important not to ignore the information about interdependen-
cies of target fields and document segments. To combine ad•
vantages of stochastic models with feature-based reasoning, 
we use a Bayesian network.                                            Figure 1: A schematic representation of BIEN. 
   A dynamic Bayesian network (DBN) is ideal for represent•
ing probabilistic information about these features. Just like a to improve performance. For example, one could learn in•
Bayesian network, it encodes interdependence among various     dependently the conditional vocabulary of email/newsgroup 
features. In addition, it incorporates the clement of time, like headers, or learn a probability of part-of-speech conditioned 
an HMM, so that time-dependent patterns ;,uch as common or•    on a word, to avoid dependence on external PoS taggers. Also 
ders of fields can be represented. All this is done in a compact prior knowledge about the domain and the language could be 
representation that can be learned from data. We refer to a re• set in the system this way. The fact that etime almost never 
cent dissertation [Murphy, 2002] for a good overview of all   precedes stime as well as the fact that speaker is never a verb 
aspects of Dynamic Bayesian Networks.                          could be encoded in a conditional probability table (CPT). In 
   Each document is considered to be a single stream of to•    large DBNs, exact inference algorithms are intractable, and 
kens. In our DBN, called the Bayesian Information Extraction   so a variety of approximate methods have been developed. 
Network (BIEN), the same structure is repeated for every in•  However, the number of hidden state variables in our model 
dex. Figure 1 presents the structure of BIEN. This structure   is small enough to allow exact algorithms to work. Indeed, 
contains state variables and feature variables. The most im•   all hidden nodes in our model are discrete variables which 
portant state variable, for our purposes, is "Tag" which corre• assume just a few values. "DocumentSegment" is binary in 
sponds to information we are trying to extract. This variable  {Header, Body) range; "LastTargct" has as many values as 
classifies each token according to its target information field, "Tag"—four per number of target fields plus one for the back•
or has the value "background" if the token does not belong    ground. 
to any field. "Last Target" is another hidden variable which 
reflects the order in which target information is found in the 4 Results 
document. This variable is our way of implementing a mem•
ory in a "memory-less" Markov model. Its value is determin-    Several researchers have reported results on the CMU sem•
istically defined by the last non-background value of "Tag"   inar announcements corpus, which we have chosen in order 
variable. Another hidden variable, "Document Segment", is     to have a good basis of comparison. The CMU seminar an•
introduced to account for differences in patterns between the nouncements corpus consists of 485 documents. Each an•
header and the main body of the document. The former is       nouncement contains some tags for target slots. On average 
close to the structured text format, while the latter to the free starting time appears twice per document, while location and 
text. "Document Segment" influences "Tag" and together        speaker 1.5 times, with up to 9 speaker slots and 4 loca•
these two influence the set of observable variables which rep• tion slots per document. Sometimes multiple instances of the 
resent features of the text discussed in section 2. Standard  same slot differ, e.g. speaker Dr. Steals also appears as 
inference algorithms for DBNs are similar to those for HMMS.  Joe Steals2. Ending time, speaker and location are miss•
In a DBN, some of the variables will typically be observed,   ing from 48%, 16% and 5% of documents correspondingly. 
while others will be hidden. The typical inference task is to In order to demonstrate our method, we have developed a web 
determine the probability distribution over the states of a hid• site which works with arbitrary seminar announcement and 
den variable over time, given time series data of the observed reveals some semantic tagging. We also make available a list 
variables. This is usually accomplished using the forward-    of errors in the original corpus, along with our new derivative 
backward algorithm. Alternatively, we might want to know      seminar announcement corpus3. 
the most likely sequence of hidden variables. This is accom•
                                                                 2Obtaining 100% performance on the original corpus is impos•
plished using the Viterbi algorithm. Learning the parameters  sible since some tags are misplaced and in general the corpus is not 
of a DBN from data is accomplished using the EM algorithm     marked uniformly—sometimes secondary occurrences are ignored. 
(see e.g. Murphy [2002]). Note that in principle, parts of 
                                                                 3The corpus and demo for this paper are available from 
the system could be trained separately on independent corpus  http://www.eecs.harvard.edu/~pesha/papers.html 


424                                                                                        INFORMATION EXTRACTION   Table 2: Fl performance measure for various IE systems. 

  The performance is calculated in the usual way, 
by precision and recall 
                         combined into F measure geo•
metrical average We report results using                      Figure 2: A learning curve for precision and recall with grow•
the same ten-fold cross validation test as other publi•       ing training sample size. 
cations concerning this data set [Roth and Yih, 2001; 
Ciravegna, 2001]. The data is split randomly into training 
and testing set. The reported results are averaged over five 
runs. Table 2 presents a comparison with numerous previous    slightly better than in Table 2 since we pushed the fraction of 
attempts at the CMU seminar corpus. The figures are taken     the training data to the maximum. Capitalization helps iden•
from Roth and Yih [2001 J. BIEN performs comparably to the    tify location and speaker, while losing it does not damage 
best system in each category, while notably outperforming     performance drastically. Although information is reflected in 
                                                              syntactic and semantic features, most names in documents do 
other systems in finding location. This is partly due to the 
                                                              not identify a speaker. One would hope to capture all relevant 
"LastTarget" variable. "LastTargct" variable turns out to be 
                                                              information by syntactic and semantic categories, however 
generally useful. Here is the learned conditional probability 
                                                              BIEN does not fare well without observing "Lemma". Losing 
table (CPT) for P(Targct\LastTaryet)  where the element 
                                    9                         the semantic feature seriously undermines performance in lo•
(/, J) corresponds to the probability to get target tag .7 after 
                                                              cation and speaker categories- - ability to recognize names is 
target tag / was seen. We learn that initial tag is stime or  rather valuable for many domains. 
speaker with 2:1 likelihood ratio; etime is naturally the most 
likely follower to stime and in turn forecasts location.        Reported figures are based on 80%-20% split of the cor•
                                                              pus. Increasing the size of training corpus did not dramati•
                                                              cally improve the performance in terms of F measure, as fur•
                                                              ther illustrated by Figure 2, which presents a learning curve— 
                                                              precision and recall averaged over all fields, as a function of 
                                                              training data fraction. Trained on a small sample, BIEN acts 
                                                              very conservatively rarely picking fields, therefore scoring 
                                                              high precision and poor recall. Having seen hundreds of tar•
                                                              get field instances and tens of thousands of negative samples, 
  Other variables turn out to be useless, e.g. the number of  BIEN learns to generalize, which leads to generous tagging 
characters does not add anything to the performance, and nei• i.e. lower precision and higher recall. 
ther does the initially introduced "SeenTag" variable which 
kept track of all tags seen up to the current position. Ta•     So far we provide results obtained on the original CMU 
ble 3 presents performance of BIEN with various individual    seminar announcements data, which is not very challeng•
features turned off. Note that figures for complete BIEN are  ing. Most documents contain the header section with all the 
                                                              target fields easily identifiable right after the corresponding 
                                                              key word. We have created a derivative dataset in which 
                                                              documents are stripped of headers and two extra fields are 
                                                              sought: date and topic. Indeed this corpus turned out to be 
                                                              more difficult, with our current set of features we obtain only 
                                                              64% performance on speaker and 68% performance on topic. 
                                                              Date does not present a challenge except for cases of regular 
                                                              weekly events or relative dates like "tomorrow". Admittedly, 
                                                              the bootstrapping test performance is not a guarantee of sys•
                                                              tems performance on novel data since preliminary processing, 
Table 3: Fl performance comparison across implementations     i.e. tokenization and gazetteering, as well as choice for PoS 
of BIEN with disabled features.                               tag set, lead to a strong bias towards the training corpus. 


INFORMATION EXTRACTION                                                                                              425 