           A Probabilistic Model of Redundancy in Information Extraction

                        Doug Downey, Oren Etzioni, and Stephen Soderland
                           Department of Computer Science and Engineering
                                       University of Washington
                                        Seattle, WA 98195-2350
                            {ddowney,etzioni,soderlan}@cs.washington.edu

                    Abstract                          obtained only once. Because the documents that “support”
                                                      the extraction are, by and large, independently authored, our
    Unsupervised Information Extraction (UIE) is the
                                                      conﬁdence in an extraction increases dramatically with the
    task of extracting knowledge from text without us-
                                                      number of supporting documents. But by how much? How
    ing hand-tagged training examples. A fundamen-
                                                      do we precisely quantify our conﬁdence in an extraction given
    tal problem for both UIE and supervised IE is as-
                                                      the available textual evidence?
    sessing the probability that extracted information
                                                        This paper introduces a combinatorial model that enables
    is correct. In massive corpora such as the Web,
                                                      us to determine the probability that an observed extraction
    the same extraction is found repeatedly in differ-
                                                      is correct. We validate the performance of the model empiri-
    ent documents. How does this redundancy impact
                                                      cally on the task of extracting information from the Web using
    the probability of correctness?
                                                      KNOWITALL.
    This paper introduces a combinatorial “balls-and-   Our contributions are as follows:
    urns” model that computes the impact of sample
    size, redundancy, and corroboration from multi-     1. A formal model that, unlike previous work, explicitly
    ple distinct extraction rules on the probability that models the impact of sample size, redundancy, and dif-
    an extraction is correct. We describe methods         ferent extraction rules on the probability that an extrac-
    for estimating the model’s parameters in practice     tion is correct. We analyze the conditions under which
    and demonstrate experimentally that for UIE the       the model is applicable, and provide intuitions about its
    model’s log likelihoods are 15 times better, on av-   behavior in practice.
    erage, than those obtained by Pointwise Mutual In-  2. Methods for estimating the model’s parameters in both
    formation (PMI) and the noisy-or model used in        the UIE and supervised IE tasks.
    previous work. For supervised IE, the model’s per-
    formance is comparable to that of Support Vector    3. Experiments that demonstrate the model’s improved per-
    Machines, and Logistic Regression.                    formance over the techniques used to assess extraction
                                                          probability in previous work. For UIE, our model is a
                                                          factor of 15 closer to the correct log likelihood than the
1  Introduction                                           noisy-or model used in previous work; the model is 20
Information Extraction (IE) is the task of automatically ex- times closer than KNOWITALL’s Pointwise Mutual In-
tracting knowledge from text. Unsupervised IE (UIE) is IE in formation (PMI) method [Etzioni et al., 2004], which
the absence of hand-tagged training data. Because UIE sys- is based on Turney’s PMI-IR algorithm [Turney, 2001].
tems do not require human intervention, they can recursively For supervised IE, our model achieves a 19% improve-
discover new relations, attributes, and instances in a rapid, ment in average log likelihood over the noisy-or model,
scalable manner as in KNOWITALL [Etzioni et al., 2004;    but is only marginally better than SVMs and logistic re-
2005].                                                    gression.
  A fundamental problem for both supervised IE and UIE  The remainder of the paper is organized as follows. Section
is assessing the probability that extracted information is cor- 2 introduces our abstract probabilistic model, and Section 3
rect. As explained in Section 5, previous IE work has used describes its implementation in practice. Section 4 reports
a variety of techniques to address this problem, but has on experimental results in four domains. Section 5 contrasts
yet to provide an adequate formal model of the impact of our model with previous work; the paper concludes with a
redundancy—repeatedly obtaining the same extraction from discussion of future work.
different documents—on the probability of correctness. Yet
in massive corpora such as the Web, redundancy is one of the
main sources of conﬁdence in extractions.             2   The Urns Model
  An extraction that is obtained from multiple, distinct doc- Our probabilistic model takes the form of a classic “balls-
uments is more likely to be a bona ﬁde extraction than one and-urns” model from combinatorics. We ﬁrst consider thesingle urn case, for simplicity, and then generalize to the full Note that these expressions include prior information about
multiple Urns Model used in our experiments. We refer to the the label x – for example, P (x ∈ C) is the prior probability
model simply as URNS.                                 that the string x is a target label, and P (num(x) = r|x ∈ C)
  We think of IE abstractly as a generative process that maps represents the probability that a target label x is repeated on r
text to extractions. Extractions repeat because distinct docu- balls in the urn. In general, integrating this prior information
ments may yield the same extraction. For example, the Web could be valuable for IE systems; however, in the analysis and
page containing “Scenic towns such as Yakima...” and the experiments that follow, we make the simplifying assumption
Web page containing “Washington towns such as Yakima...” of uniform priors, yielding the following simpliﬁed form:
both lead us to believe that Yakima is a correct extraction of Proposition 1
the relation City(x).
  Each extraction is modeled as a labeled ball in an urn. A P (x ∈ C|x appears k times in n draws) =
label represents either an instance of the target relation, or
                                                                            P           r k    r n−k
an error. The information extraction process is modeled as                    r∈num(C)( s ) (1 − s )
repeated draws from the urn, with replacement. Thus, in the               P              r0 k    r0 n−k
                                                                            r0∈num(C∪E)(  ) (1 −  )
above example, two balls are drawn from the urn, each with                               s       s
the label “Yakima”. The labels are instances of the relation 2.1 The Uniform Special Case
City(x). Each label may appear on a different number of For illustration, consider the simple case in which all labels
balls in the urn. Finally, there may be balls in the urn with from C are repeated on the same number of balls. That
error labels such as “California”, representing cases where is, num(c ) = R for all c ∈ C, and assume also that
the IE process generated an extraction that is not a member    i      C        i
                                                      num(ei) =  RE  for all ei ∈ E. While these assumptions
of the target relation.                               are unrealistic (in fact, we use a Zipf distribution for num(b)
  Formally, the parameters that characterize an urn are: in our experiments), they are a reasonable approximation for
  • C – the set of unique target labels; |C| is the number of the majority of labels, which lie on the ﬂat tail of the Zipf
    unique target labels in the urn.                  curve.
  • E – the set of unique error labels; |E| is the number of Deﬁne p to be the precision of the extraction process; that
    unique error labels in the urn.                   is, the probability that a given draw comes from the target
                                                      relation. In the uniform case, we have:
  • num(b) – the function giving the number of balls la-
                                                                              |C|R
    beled by b where b ∈ C ∪ E. num(B) is the multi-set               p =          C
    giving the number of balls for each label b ∈ B.                      |E|RE  + |C|RC
  Of course, IE systems do not have access to these param- The probability that a particular element of C appears in a
eters directly. The goal of an IE system is to discern which given draw is then p = p , and similarly p = 1−p .
of the labels it extracts are in fact elements of C, based on          C   |C|             E    |E|
repeated draws from the urn. Thus, the central question we Using a Poisson model to approximate the binomial from
are investigating is: given that a particular label x was ex- Proposition 1, we have:
tracted k times in a set of n draws from the urn, what is the P (x ∈ C|x appears k times in n draws) ≈
probability that x ∈ C?
                                                                                         1
  In deriving this probability formally below, we assume the                                          (2)
                                                                                   |E| p
IE system has access to multi-sets num(C) and num(E) giv-                               E k n(pC −pE )
                                                                                1 + |C| ( p ) e
ing the number of times the labels in C and E appear on balls                           C
in the urn. In our experiments, we provide methods that es- In practice, the extraction process is noisy but informative,
timate these multi-sets in both unsupervised and supervised so pC > pE. Notice that when this is true, Equation (2)
settings. We can express the probability that an element ex- shows that the odds that x ∈ C increase exponentially with
tracted k of n times is of the target relation as follows: the number of times k that x is extracted, but also decrease
  First, we have that                                 exponentially with the sample size n.
                                                        A few numerical examples illustrate the behavior of this
  P (x appears k times in n draws|x ∈ C) =            equation. The examples assume that the precision p is 0.9.
                                                          |C| = |E| = 2, 000              R   = 9 × R
    X  n  r k    r n−k                          Let                 . This means that C        E—
                  1 −       P (num(x) =  r|x ∈ C)     target balls are nine times as common in the urn as error balls.
         k   s        s
     r                                                Now, for k = 3 and n =  10, 000 we have P (x ∈ C) =
where s is the total number of balls in the urn, and the sum is 93.0%. Thus, we see that a small number of repetitions can
taken over possible repetition rates r.               yield high conﬁdence in an extraction. However, when the
  Then we can express the desired quantity using Bayes sample size increases so that n = 20, 000, and the other pa-
Rule:                                                 rameters are unchanged, then P (x ∈ C) drops to 19.6%. On
                                                      the other hand, if C balls repeat much more frequently than
  P (x ∈ C|x appears k times in n draws) =            E balls, say RC = 90 × RE (with |E| set to 20,000, so that
                                                      p remains unchanged), then P (x ∈ C) rises to 99.9%.
   P (x appears k times in n draws|x ∈ C)P (x ∈ C)
                                                (1)     The above examples enable us to illustrate the advantages
           P (x appears k times in n draws)           of URNS over the noisy-or model used in previous work [Linet al., 2003; Agichtein and Gravano, 2000]. The noisy-or
model assumes that each extraction is an independent asser-
tion, correct a fraction p of the time, that the extracted label is
“true.” The noisy-or model assigns the following probability
to extractions:
                                              k
   Pnoisy−or(x ∈ C|x appears k times) = 1 − (1 − p)
Therefore, the noisy-or model will assign the same
probability— 99.9%—in all three of the above examples.
Yet, as explained above, 99.9% is only correct in the case for
which n = 10, 000 and RC = 90 × RE. As the other two ex-
amples show, for different sample sizes or repetition rates, the
noisy-or model can be highly inaccurate. This is not surpris-
ing given that the noisy-or model ignores the sample size and
the repetition rates. Section 4 quantiﬁes the improvements Figure 1: Schematic illustration of the number of distinct
obtained by URNS in practice.                         labels in the C and E sets with repetition rate r. The “con-
                                                      fusion region” is shaded.
2.2  Applicability of the Urns Model
Under what conditions does our redundancy model provide For example, consider the  particular num(C)  and
accurate probability estimates? First, labels from the target num(E) learned (in the unsupervised setting) for the Film
set C must be repeated on more balls in the urn than labels relation in our experiments. For the sample size n = 134, 912
from the E set, as in Figure 1. The shaded region in Figure 1 used in the experiments, expected number of true positives is
represents the “confusion region” – some of the labels in this 26,133 and expected precision is 70.2%, which is close to
region will be classiﬁed incorrectly, even by the ideal classi- the actual observed true positives of 23,408 and precision of
ﬁer with inﬁnite data, because for these labels there simply 67.7%. Were we to increase the sample size to 1,000,000,
isn’t enough information to decide whether they belong to C we would expect that true positives would increase to 47,609,
or E. Thus, our model is effective when the confusion re- and precision to 84.0%. Thus, URNS and the above equations
gion is relatively small. Secondly, even for small confusion enable an IE system to intelligently choose its sample size
regions, the sample size n must be large enough to approxi- depending on precision and recall requirements and resource
mate the two distributions shown in Figure 1; otherwise the constraints, even in the absence of tagged training data.
probabilities output by the model will be inaccurate.
  An attractive feature of URNS is that it enables us to esti- 2.3 Multiple Urns
mate its expected recall and precision as a function of sample We now generalize our model to encompass multiple urns.
size. If the distributions in Figure 1 cross at the dotted line Information is often extracted using multiple, distinct mech-
shown then, given a sufﬁciently large sample size n, expected anisms – for example, an IE system might employ several
recall will be the fraction of the area under the C curve lying patterns for extracting city names, e.g. “cities including x”
to the right of the dotted line.                      and “x and other towns.” It is often the case that different pat-
  For a given sample size n, deﬁne τn to be the least number terns have different modes of failure, so extractions appearing
of appearances k at which an extraction is more likely to be across multiple patterns are generally more likely to be true
from the C set than the E set (given the distributions in Figure than those appearing for a single pattern. We can model this
1, τn can be computed using Proposition 1). Then we have: situation by introducing multiple urns where each urn repre-
                                                                                       1
  E[T rueP ositives] =                                sents a different extraction mechanism.
                                                        Thus, instead of n total extractions, we have a sample size
                       τn−1  
                 X      X    n  r k    r n−k      nm  for each urn m ∈ M, with the extraction x appearing
         |C| −                         1 −
                             k    s       s           km times. Let A(x, (k1, . . . , km), (n1, . . . , nm)) denote this
                        k=0
               r∈num(C)                               event. Further, let Am(x, k, n) be the event that label x ap-
where we deﬁne “true positives” to be the number of ex- pears k times in n draws from urn m, and assuming that the
tracted labels ci ∈ C for which the model assigns probability draws from each urn are independent, we have:
P (ci ∈ C) > 0.5.                                     Proposition 2
  The expected number of false positives is similarly:
                                                        P (x ∈ C|A(x, (k1, . . . , km), (n1, . . . , nm))) =
  E[F alseP ositives] =                                                 P     Q
                                                                                     P (Am(ci, km, nm))
                       τ −1                                               ci∈C  m∈M
                 X      Xn n  r k    r n−k                       P       Q
         |E| −                         1 −                               x∈C∪E   m∈M  P (Am(x, km, nm))
                             k    s       s
               r∈num(E) k=0
                                                        With multiple urns, the distributions of labels among balls
  The expected precision of the system can then be approxi-
                                                      in the urns are represented by multi-sets num (C) and
mated as:                                                                                       m
                        E[T rueP ositives]               1We may lump several mechanisms into a single urn if they tend
E[P recision] ≈
               E[F alseP ositives] + E[T rueP ositives] to behave similarly.numm(E). Expressing the correlation between numm(x)   In these expressions, i is the frequency rank of the extraction,
and numm0 (x) is an important modeling decision. Multiple assumed to be the same across all urns, and QC and QE are
urns are especially beneﬁcial when the repetition rates for ele- normalizing constants such that
ments of C are more strongly correlated across different urns
                                                                 X       −zC   X       −zE
than they are for elements of E—that is, when numm(x) and            QC i    =     QEi     = 1
numm0 (x) tend to be closer to each other for x ∈ C than for     ci∈C          ei∈E
x ∈ E. Fortunately, this turns out to be the case in prac-
tice. Section 3 describes our method for modeling multi-urn For a non-systematic error x which is not present in urn m,
correlation.                                          P (Am(x, km, nm)) is 1 if km = 0 and 0 otherwise. Substi-
                                                      tuting these expressions for P (Am(x, km, nm)) into Propo-
3  Implementation of the Urns Model                   sition 2 gives the ﬁnal form of our URNS model.
This section describes how we implement URNS for both UIE 3.1 Efﬁcient Computation
and supervised IE, and identiﬁes the assumptions made in
each case.                                            A feature of our implementation is that it allows for efﬁ-
  In order to compute probabilities for extractions, we need cient computation of probabilities. In general, computing
                                                                                                 C     E
a method for estimating num(C) and num(E). For the pur- the sum in Proposition 2 over the potentially large and
pose of estimating these sets from tagged or untagged data, sets would require signiﬁcant computation for each extrac-
                                                                                                 num(C)
we assume that num(C) and num(E)  are Zipf distributed, tion. However, given a ﬁxed number of urns, with
                                                      and num(E)  Zipf distributed, an integral approximation to
meaning that if ci is the ith most frequently repeated label in
                              −zC                     the sum in Proposition 2 (using a Poisson in place of the bi-
C, then num(ci) is proportional to i . We can then char-
acterize the num(C) and num(E) sets with ﬁve parameters: nomial in Equation 3) can be solved in closed form in terms
                                                      of incomplete Gamma functions. This closed form expres-
the set sizes |C| and |E|, the shape parameters zC and zE,
and the extraction precision p.                       sion can be evaluated quickly, and thus probabilities for ex-
  To model multiple urns, we consider different extraction tractions can be obtained efﬁciently. This solution leverages
                                                      our assumptions that size and shape parameters are identical
precisions pm for each urn, but make the simplifying assump-
tion that the size and shape parameters are the same for all across urns, and that relative frequencies are perfectly cor-
urns. As mentioned in Section 2, we expect repetition rate related. Finding efﬁcient techniques for computing proba-
correlation across urns to be higher for elements of the C set bilities under less stringent assumptions is an item of future
than for the E set. We model this correlation as follows: ﬁrst, work.
elements of the C set are assumed to come from the same 3.2 Parameter Estimation
location on the Zipf curve for all urns, that is, their relative
frequencies are perfectly correlated. Some elements of the In the event that a large sample of hand-tagged training ex-
E set are similar, and have the same relative frequency across amples is available for each target relation of interest, we
urns – these are the systematic errors. However, the rest of the can directly estimate each of the parameters of URNS. We
E set is made up of non-systematic errors, meaning that they use a population-based stochastic optimization technique to
appear for only one kind of extraction mechanism (for exam- identify parameter settings that maximize the conditional log
                                                                               2
ple, “Eastman Kodak” is extracted as an instance of Film likelihood of the training data. Once the parameters are set,
only in phrases involving the word “ﬁlm”, and not in those the model yields a probability for each extraction, given the
involving the word “movie.”). Formally, non-systematic er- number of times km it appears in each urn and the number of
rors are labels that are present in some urns and not in others. draws nm from each urn.
Each type of non-systematic error makes up some fraction of As argued in [Etzioni et al., 2005], IE systems cannot rely
the E set, and these fractions are the parameters of our cor- on hand-tagged training examples if they are to scale to ex-
relation model. Assuming this simple correlation model and tracting information on arbitrary relations that are not speci-
identical size and shape parameters across urns is too restric- ﬁed in advance. Implementing URNS for UIE requires a so-
tive in general— differences between extraction mechanisms lution to the challenging problem of estimating num(C) and
are often more complex. However, our assumptions allow us num(E) using untagged data. Let U be the multi-set consist-
to compute probabilities efﬁciently (as described below) and ing of the number of times each unique label was extracted;
do not appear to hurt performance signiﬁcantly in practice. |U| is the number of unique labels encountered, and the sam-
                                                                 P
  With this correlation model, if a label x is an element of C ple size n = u∈U u.
or a systematic error, it will be present in all urns. In terms of In order to learn num(C) and num(E) from untagged
Proposition 2, the probability that a label x appears km times data, we make the following assumptions:
in nm draws from m is:                                  • Because the number of different possible errors is nearly
                   n                                                                                 3
                      m         km          nm−km         unbounded, we assume that the error set is very large.
P (Am(x, km, nm)) =      (fm(x))  (1−fm(x))
                     km
                                                (3)      2Speciﬁcally, we use the Differential Evolution routine built into
where f (x) is the frequency of extraction x. That is, Mathematica 5.0.
      m                                                  3                            6
                         −zC                             In our experiments, we set |E| = 10 . A sensitivity analysis
       fm(ci)  =   pmQC  i   for ci ∈ C               showed that changing |E| by an order of magnitude, in either direc-
                              −zE
       fm(ei)  =   (1 − pm)QEi    for ei ∈ E          tion, resulted in only small changes to our results.  • We assume that both num(C) and num(E) are Zipf dis- methods experimentally, and lastly compare URNS with sev-
    tributed where the zE parameter is set to 1.      eral baseline methods in a supervised setting.
  • In our experience with KNOWITALL, we found that     We evaluated  our algorithms on extraction sets for
    while different extraction rules have differing precision, the relations City(x), Film(x), Country(x), and
    each rule’s precision is stable across different relations MayorOf(x,y), taken from experiments performed in [Et-
    [Etzioni et al., 2005]. URNS takes this precision as an zioni et al., 2005]. The sample size n was 64,581 for City,
    input. To demonstrate that URNS is not overly sensitive 134,912 for Film, 51,313 for Country and 46,129 for
    to this parameter, we chose a ﬁxed value (0.9) and used MayorOf. The extraction patterns were partitioned into
                                                4     urns based on the name they employed for their target re-
    it as the precision pm for all urns in our experiments.
                                                      lation (e.g. “country” or “nation”) and whether they were
  We then use Expectation Maximization (EM) over U in or- left-handed (e.g. “countries including x”) or right-handed
der to arrive at appropriate values for |C| and zC (these two (e.g. “x and other countries”). Each combination of rela-
quantities uniquely determine num(C) given our assump- tion name and handedness was treated as a separate urn, re-
tions). Our EM algorithm proceeds as follows:         sulting in four urns for each of City(x), Film(x), and
                                                                                                5
 1. Initialize |C| and zC to starting values.         Country(x), and two urns for MayorOf(x).   For each
 2. Repeat until convergence:                         relation, we tagged a sample of 1000 extracted labels, using
                                                      external knowledge bases (the Tipster Gazetteer for cities and
     (a) E-step Assign probabilities to each element of U the Internet Movie Database for ﬁlms) and manually tagging
        using Proposition (1).                        those instances not found in a knowledge base. In the UIE
     (b) M-step Set |C| and zC from U using the probabil- experiments, we evaluate our algorithms on all 1000 exam-
        ities assigned in the E-step (details below). ples, and in the supervised IE experiments we perform 10-
We obtain |C| and zC in the M-step by ﬁrst estimating the fold cross validation.
rank-frequency distribution for labels from C in the untagged
data. From the untagged data and the probabilities found in 4.1 UIE Experiments
the E-step, we can obtain E [k], the expected number of la-
                       C                              We compare URNS  against two other methods for unsuper-
        C                 k
bels from that were extracted times. We then round these vised information extraction. First, in the noisy-or model
fractional expected counts into a discrete rank-frequency dis- used in previous work, an extraction appearing k times is as-
tribution with a number of elements equal to the expected to- signed probability 1 − Q (1 − p )k, where p is the ex-
                     C                   P   E  [k]                        m∈M      m          m
tal number of labels from in the untagged data, k C . traction precision for urn m. We describe the second method
         z
We obtain C by ﬁtting a Zipf curve to this rank-frequency below.
distribution by linear regression on a log-log scale. Lastly,
            P
we set |C| =  k EC [k] + unseen, where we estimate the Pointwise Mutual Information
number of unseen labels of the C set using Good-Turing esti-
                                                      Our previous work on KNOWITALL used Pointwise Mutual
mation ([Gale and Sampson, 1995]). Speciﬁcally, we choose
                                                      Information (PMI) to obtain probability estimates for extrac-
unseen such that the probability mass of unseen labels is
                                                      tions [Etzioni et al., 2005]. Speciﬁcally, the PMI between
equal to the expected fraction of the draws from C that ex-
                                                      an extraction and a set of automatically generated discrimi-
tracted labels seen only once.
                                                      nator phrases (e.g., “movies such as x”) is computed from
  This unsupervised learning strategy proved effective for
                                                      Web search engine hit counts. These PMI scores are used
target relations of different sizes; for example, the number
                                                      as features in a Naive Bayes Classiﬁer (NBC) to produce a
of elements of the Country relation with non-negligible
                                                      probability estimate for the extraction. The NBC is trained
extraction probability was about two orders of magnitude
                                                      using a set of automatically bootstrapped seed instances. The
smaller than that of the Film and City relations.
                                                      positive seed instances are taken to be those having the high-
  Clearly, unsupervised learning relies on several strong as-
                                                      est PMI with the discriminator phrases after the bootstrapping
sumptions, though our sensitivity analysis has shown that the
                                                      process; the negative seeds are taken from the positive seeds
model’s performance is robust to some of them. In future
                                                      of other relations, as in other work (e.g., [Lin et al., 2003]).
work, we plan to perform a more comprehensive sensitivity
                                                                                [                ]
analysis of the model and also investigate its performance in Although PMI was shown in Etzioni et al., 2005 to order
a semi-supervised setting.                            extractions fairly well, it has two signiﬁcant shortcomings.
                                                      First, obtaining the hit counts needed to compute the PMI
4  Experimental Results                               scores is expensive, as it requires a large number of queries to

This section describes our experimental results under two set- 5Draws from URNS are intended to represent independent ex-
tings: unsupervised and supervised. We begin by describ- tractions. Because the same sentence can be duplicated across multi-
ing the two unsupervised methods used in previous work: the ple different Web documents, in these experiments we consider only
noisy-or model and PMI. We then compare URNS with these each unique sentence containing an extraction to be a draw from
                                                      URNS. In experiments with other possibilities, including counting
  4A sensitivity analysis showed that choosing a substantially the number of unique documents producing each extraction, or sim-
higher (0.95) or lower (0.80) value for pm still resulted in URNS ply counting every occurrence of each extraction, we found that per-
outperforming the noisy-or model by at least a factor of 8 and PMI formance differences between the various approaches were negligi-
by at least a factor of 10 in the experiments described in Section 4.1. ble for our task.