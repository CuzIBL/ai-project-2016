      A ﬂexible and robust similarity measure based on contextual probability

                       Hui Wang    ∗                             Werner Dubitzky
          School of Computing and Mathematics               School of Biomedical Science
            University of Ulster at Jordanstown           University of Ulster at Coleraine
                     Northern Ireland                             Northern Ireland
                   h.wang@ulster.ac.uk                        w.dubitzky@ulster.ac.uk

                    Abstract                          and similarity. This is evidenced by the large volume of AI
                                                      research in the areas of analogical reasoning and case-based
    Arguably, analogy is one of the most important
                                                      reasoning [Vosniadou and Ortony, 1989; Kolodner, 1993]. In
    aspects of intelligent reasoning. It has been hy-
                                                      the context of available background knowledge, analogy can
    pothesized that, given suitable background knowl-
                                                      be viewed as a logical inference process [Davies and Russell,
    edge, analogy can be viewed as a logical infer-
                                                      1987]. Another way of looking at analogy holds that similar-
    ence process. This study follows another school
                                                      ity can provide a probabilistic basis for inference, and that a
    of thought that argues that similarity can provide a
                                                      quantitative framework can be developed for the probability
    probabilistic basis for inference and analogy. Most
                                                      that an analogy is correct as a function of the degree of sim-
    similarity measures (which are frequently viewed
                                                      ilarity measured or observed [Russell, 1986]. This study is
    as being conceptually equivalent to distance mea-
                                                      concerned with the latter view of modeling analogy.
    sures) are restricted to either nominal or ordinal
    attributes, and some are conﬁned to classiﬁcation   Distance measures (or, equivalently, similarity measures)
    tasks. This paper proposes a ﬂexible similarity   are central to many areas related to AI, including reasoning
    measure that is task-independent and applies to   under uncertainty, knowledge-based systems, machine learn-
    both nominal and ordinal data in a conceptually   ing, pattern recognition, data mining, analogical, case-based,
    uniform way.  The proposed similarity measure     instance-based reasoning, and to other ﬁelds such as statis-
    is derived from a probability function and corre- tics, operations research and decision theory. A large number
    sponds to the intuition that if we consider all neigh- of distance metrics and measures are available and a particu-
    borhoods around a data point, the data points closer lar choice may have considerable implications for the success
    to this point should be included in more of these of the problem that is to be addressed.
    neighborhoods than more distant points. Experi-     Distance functions are broadly categorized into those that
    ments we have conducted to demonstrate the use-   can handle ordinal input data, nominal input data, and het-
    fulness of this measure indicate that it fares very erogeneous input data, consisting of both ordinal and nominal
                                                          2
    competitively with commonly used similarity mea-  data . Ordinal distance functions make use of the intrinsic to-
    sures.                                            tal ordering relation in the underlying attribute values, which
                                                      are either continuous (e.g. weight of an object) or discrete
                                                      (e.g. number of obstacles). A nominal or symbolic attribute
1  Introduction                                       is a discrete attribute whose values do not necessarily exhibit
Natural and artiﬁcial intelligent agents rely on different in- a total order relation. For example, an attribute representing
ference and reasoning strategies to achieve their goals and the role of crew member may have the values scientist, mis-
maintain their intended behavior. Arguably, two of the more sion specialist and commander. Using an ordinal distance
successful strategies are based on the notions of analogy 1 measurement on such values is meaningless.
  ∗Also LITA, Université de Metz, Ile du Saulcy, 57045 Metz Because the notion of ‘distance’ is intrinsically numeri-
Cedex, France.                                        cal, most available distance measures are deﬁned for data
  1Here we consider the subject of analogy in the following, rel- with ordinal attributes. However, distance measures that can
atively narrow, sense: objects are represented by feature vectors
rather than the typical graphs; we do not consider transfer of knowl- 2Here we consider two scales of measurement: ordinal and nom-
edge from one object to another; we do not consider cross-domain inal. In an ordinal scale, values (discrete or continuous) are ordered
knowledge but assume all objects share common set of attributes. whereas in a nominal scale, (discrete) values are unordered.process nominal attributes are required by many modern AI     10


algorithms. Some measures that handle nominal attributes do   9


exist. The most well known measure of this kind is the Value  8
Difference Metric (VDM) [Stanﬁll and Waltz, 1986]. It is de-                       b
                                                              7
ﬁned in terms of attribute values that are conditioned by pos-                    a
terior probabilities of a class. Hence, like some other distance 6

measures, the VDM is restricted to classiﬁcation tasks.       5                  t


  To cope with heterogeneous data, containing both nominal    4
and ordinal attributes, two approaches are commonly taken:
                                                              3
(1) The data is transformed into one of the two data types
that complies with the distance measure used, and (2) two     2
types of distance measures are combined and handle the data   1
separately in accordance with the data type (see, for example,
[Wilson and Martinez, 1997]). However, both approaches are          1  2  3  4   5  6  7  8  9  10
problematic when it comes to the interpretation of the results.
  The novel distance measure proposed in this study is called Figure 1: An illustrative example. Each of the 7 concentric
Neighborhood Counting Metric (NCM). It is derived from a circles represents a neighborhood of data point t deﬁned by
probability function and it can handle both nominal and or- some distance measure. Data point a is covered by 5 neigh-
dinal attributes in a conceptually uniform way. Intuitively, it borhoods whereas b by only 2. Geometrically, a is clearly
can be understood or interpreted as follows (see Figure 1): closer (more similar) to t than b.
If we consider all neighborhoods, N, around a data point,
t, then those data points closer to t should be included in
more of these neighborhoods, N, than points that are not
                                                      ming and the normalized Euclidean distance functions and
close to t. Usually neighborhoods are interpreted in terms
                                                      can therefore be used on heterogeneous data.
of distance. However, if we adopted this interpretation, we
                                                        The Value Difference Metric (VDM) [Stanﬁll and Waltz,
would deﬁne one distance function in terms of another. To
                                                      1986] is designed to handle nominal attributes in classiﬁca-
avoid this dilemma, we interpret neighborhood without dis-
                                                      tion tasks only. It uses pre-computed statistical properties
tance through the concept of hypertuples [Wang et al., 1999]
                                                      from available training data and can be interpreted in terms
for both nominal and ordinal attributes. As a result, our dis-
                                                      of probabilities. The Heterogeneous Value Difference Metric
tance measure applies to both ordinal and nominal attributes
                                                      (HVDM)  [Wilson and Martinez, 1997] combines the Euclid-
in a uniform way.
                                                      ean distance and the VDM. It is therefore able to handle het-
  The new NCM is conceptually simple, it is straightforward
                                                      erogeneous data, but because of its VDM heritage it is con-
to implement, and it has the added property that it is inde-
                                                      ﬁned to classiﬁcation tasks.
pendent of the underlying analytical or reasoning task (e.g.
                                                        The Interpolated Value Difference Metric (IVDM) [Wilson
classiﬁcation). The measure’s clear and unambiguous mean-
                                                      and Martinez, 1997] extends the VDM to scenarios involving
ing is deﬁned by the number of neighborhoods of a query that
                                                      ordinal attributes. In the learning phase, it employs a dis-
include or cover a given data point.
                                                      cretization step to collect statistics and determine the proba-
  Paper outline: Section 2 presents a short review of distance
                                                      bility for the discretized values (P in the VDM formula),
measures relevant to this study; it is followed by Sections 3                     a,x,c
                                                      but then retains the continuous values in the training data for
and 4, which present mathematical details of the new simi-
                                                      later use in the application or testing phase. The IVDM re-
larity measure. In Section 5 the empirical evaluation of the
                                                      quires a non-parametric probability density estimation to de-
method is presented and its results discussed. The paper con-
                                                      termine the probability values for each class. The Discretized
cludes with a summary and a brief discussion of future work.
                                                      Value Difference Metric (DVDM) is the same as the IVDM
2  A brief review of important distance               except that it avoids the retention of the original continuous
                                                      values but uses only the discretized values.
   functions                                            [Blanzieri and Ricci, 1999] introduced the Minimal Risk
Two of the most common distance functions are Euclid- Metric (MRM), a probability-based distance measure for
ean distance and the Hamming distance. The former is re- classiﬁcation and case-based reasoning. It minimizes the risk
stricted to ordinal and the latter is usually used for nomi- of misclassiﬁcation and depends on probability estimation
nal attributes. The Heterogeneous Euclidean-Overlap Metric techniques. As a result, the MRM exhibits a high compu-
(HEOM)  [Wilson and Martinez, 1997] combines the Ham- tational complexity and is task-dependent.3  A probability function                             fashion. While this may appear to be computationally expen-
Let V be a (non-empty) universe of discourse. A Σ-ﬁeld F sive, it is often possible to derive a simple formula for G.
on V is a collection of subsets of V, such that:        Depending on what F is and how neighborhoods are de-
                                                      ﬁned, we can use the G probability for different tasks. In
 1. V ∈ F ;                                           Section 4 we take F to be the set of all regions in a multi-
 2. If A ∈ F then A0 ∈ F , where A0 is the complement of A; dimensional space deﬁnable as hypertuples and our similarity
 3. If A,B,··· ∈ F , then A ∪ B ∪ ··· ∈ F .           measure is derived from this interpretation.
  A probability function P over V is a mapping from F to 3.1 Estimation of G
                                        [        ]
[0,1] satisfying the three axioms of probability Ash, 1972 . This section describes how G is estimated for a data point
If P is restricted to V, it is called a probability mass function from data samples. Let D be a sample of data drawn from V
(discrete) or probability density function (continuous). according to an unknown probability distribution P. Our aim
  Suppose we have a probability function P as deﬁned above. is to calculate Gˆ(t|D) for any t ∈ V.
For X ∈ F let f (X) be a non-negative measure of X satisfying To calculate G we need P, which can be estimated from
f (X1 ∪ X2) = f (X1) + f (X2) if X1 ∩ X2 = 0/. As an example, data assuming the principle of indifference as follows: For
we can take f (X) for the cardinality of X.           any E ∈ F ,
  Consider a function G : F → [0,1] such that, for X ∈ F ,
                                                                         Pˆ(E|D) = |E|/n
            G(X) =  ∑  P(E) f (X ∩ E)/K
                    E∈F                               where |E| is the number of elements in E and n is the number
                                                      of elements in D. Additionally we assume that f (X) = |X|
where K = ∑E∈F P(E) f (E). It can be easily shown that G is
                                                      for X ∈ F .
a probability function.                                 We then have,
  G(X) is calculated from all those E ∈ F that overlap with
                                                                Pˆ(E|D) × f (E ∩t)
X (i.e., f (X ∩ E) 6= 0). These E’s are relevant to X and serve Gˆ(t|D) = ∑                      deﬁnition
                                                              E∈F    K
as the contexts in which G(X) is induced. Each such E is
                                                                Pˆ(E|D) × |E ∩t|
called a neighborhood of X. In other words a neighborhood   = ∑                              speciﬁcation of f
                                                              E∈F    K
of X is an element E of F , i.e., a subset of V, such that E      Pˆ(E|D)
                                                            =  ∑                  assumption that t is in E so E ∩t = t
overlaps X.                                                   E∈F ,t∈E K
  In practice P is usually not known, but a sample of data    1
                                                            =        |E|      estimation of P by principle of indifference
                                                              nK  ∑
drawn from the data space according to P is commonly avail-     E∈F ,t∈E
                                                              1
able. P can be estimated from the sample by either paramet- =     cov(t,x)         expansion and then re-organisation
                                                              nK ∑
ric or non-parametric techniques. In some tasks (e.g., clas-    x∈D
siﬁcation) the point-wise probability is needed. When there where K is a normalisation factor, and cov(t,x) is the number
is insufﬁcient data, which is not uncommon, the point-wise of such E ∈ F that covers both t and x. We call this number
probability may be difﬁcult to estimate using non-parametric the cover of x with respect to t.
methods. However, it is likely that probability can be esti- To obtain cov(t,x), a straightforward approach would be
mated for some regions (contexts) in the data space based on to iterate over all E ∈ F and check if E covers both t and x.
the sample. Using the G probability function, we can estimate Clearly, such a process is undesirable because of its exponen-
the G probability for any single data point from knowledge tial complexity. In Section 4 we will present an efﬁcient way
of the P probabilities of various regions or contexts. This of calculating cov(t,x).
provides us with a formal way of inference about probability
under incomplete (hence uncertain) situations.        4   Measuring distance through counting
  To calculate the G probability for X ∈ F in practice, we This section considers an interpretation of neighborhood and
need to                                               derives a formula for calculating cov(t,x), which is then taken
 1. Find the set of all neighborhoods appropriate to the prob- as a measure of similarity or distance.
    lem at hand,
                                                      4.1  Neighborhood
 2. Estimate the P probability for every neighborhood,
                                                      Pursuing a non-distance-based conceptualization of neigh-
 3. Finally, estimate (calculate) the G probability for X ac- borhood, we interpret a neighborhood as a hypertuple [Wang
    cording to its deﬁnition.                         et al., 1999], and a neighbor as a data point (or tuple) cov-
In classiﬁcation tasks, we can further calculate the conditional ered by some neighborhood. So a neighborhood of a tuple is
G probability of a class given a single data point in a similar a hypertuple that covers the tuple.Hypertuples                                           to h is x ∈ V such that x ≤ h. By this deﬁnition any simple
Let R = {a1,a2,··· ,an} be a set of attributes, and dom(a) tuple has a neighborhood. At least the maximal hypertuple
be the domain of attribute a ∈ R. Furthermore let V def= in the extended data space is a neighborhood of any simple
                 def
 n                   n   dom(ai)                      tuple since the maximal hypertuple covers all simple tuples.
∏i=1 dom(ai) and L = ∏i=1 2   . V is the data space de-
ﬁned by R, and L an extended data space. A (given) data set For a query t ∈ V, not all hypertuples in ∏i Si are neigh-
is D ⊆ V – a sample of V.                             borhoods of t. For a hypertuple h to be a neighborhood of
  The attributes can be either ordinal or nominal. For sim- t we must have t(ai) ∈ h(ai) for all i. Therefore, to gen-
plicity, we assume the domain of any attribute is ﬁnite, but erate a neighborhood of t, we can take an si ∈ Si such that
the results are not limited to ﬁnite domains.         t(ai) ∈ si for all i, resulting in a hypertuple hs1,s2,··· ,sni.
                                                      If a is nominal, the number of s ∈ such that t(a ) ∈ s is
  If we write an element t ∈ V by hv1,v2,··· ,vni then vi ∈ i   ³    ´            i  Si           i   i
                                             dom(ai)    0   mi−1 mi−1     mi−1
dom(ai). If we write h ∈ L by hs1,s2,··· ,sni then si ∈ 2 Ni = ∑i=0 i =  2    since si is any subset of dom(ai)
or si ⊆ dom(ai).                                      that is the super set of t(ai). If ai is ordinal, this number
  An element of L is called a hypertuple, and an element of 0
                                                      is Ni = (max(ai) − t(ai) + 1) × (t(ai) − min(ai) + 1) since
V a simple tuple. The difference between the two is that a (max(ai) − t(ai) + 1) is the number of ordinal values above
ﬁeld within a simple tuple is a value while a ﬁeld within a t(ai), and (t(ai) − min(ai) + 1) is such a number below t(ai).
hypertuple is a set. If we interpret vi ∈ dom(ai) as a singleton Any pair of values from the two parts respectively forms an
set {vi}, then a simple tuple is a special hypertuple. Thus we interval.
can embed V into L, so V ⊆ L.                                                                           0
                                                        To summarize, the number of neighborhoods of t is ∏i Ni ,
  Consider two  hypertuples h1 and h2, where  h1 =    where
hs ,s ,··· ,s i and h = hs ,s ,··· ,s i. We say h is
 11  12    1n      2     21 22     2n          1      (1)
covered by h2 (or h2 covers h1), written h1 ≤ h2, if for    
                                                               m −1
i ∈ {1,2,··· ,n},                                           2 i  ,                     if ai is nominal
   (                                                     0
                                                       Ni =   (max(ai) −t(ai) + 1) × (t(ai) − min(ai) + 1),
    ∀x ∈ s1i,min(s2i) ≤ x ≤ max(s2i) if ai is ordinal       
                                                                                         if ai is ordinal.
    s1i ⊆ s2i                      if ai is nominal
                                                      Cover of simple tuples
Furthermore the sum of h and h , written by h + h def=
                      1      2            1    2      Under the above interpretation of neighborhood, we know ex-
hs1,s2,··· ,sni, is: for each i ∈ {1,2,··· ,n},
                                                     actly the number of all neighborhoods of a given simple tuple
    {x ∈ dom(ai) : min(s1i ∪ s2i) ≤ x ≤ max(s1i ∪ s2i)}, t. Here we present an efﬁcient way of calculating cov(t,x),
si =                if ai is ordinal                  the number of neighborhoods of t that cover x, which is
                                                    needed in Section 3.1.
      s1i ∪ s2i,    if ai is nominal
                                                        Consider two simple tuples t = ht1,t2,··· ,tni and x =
The product operation ∗ can be similarly deﬁned. It turns out
                                                      hx1,x2,··· ,xni. A neighborhood h of t covers t by deﬁni-
that hL,≤,+,∗i is a lattice.                          tion, i.e., t ≤ h. What we need to do is to check if h covers x
  For a simple tuple t, t(ai) represents the projection of t onto as well. In other words, we want to ﬁnd all hypertuples that
attribute ai. For a hypertuple h, h(ai) is similarly deﬁned. cover both t and x.
  A hypertuple can be generated by taking one subset from Eq.1 speciﬁes the number of all simple tuples that cover
each attribute. Let ai be an attribute, i = 1,2,··· ,n; Si be the t only. We take a similar approach here by looking at each
                                     00 def
set of all subsets of the domain of ai; and Ni = |Si|. Then a attribute and explore the number of subsets that can be used
hypertuple h is an element of ∏i Si. Therefore the number of to generate a hypertuple covering both t and x. Multiplying
                  00
all hypertuples is ∏i Ni .                            these numbers across all attributes gives rise to the number
                         dom(ai)      00   mi
  If ai is nominal, then Si = 2 and so Ni = 2 , where we require.
mi = |dom(ai)|. If ai is ordinal, then an element s ∈ Si cor- Consider attribute ai. If ai is ordinal, then the number of
responds to an interval [min(s),max(s)]. As a result, some intervals that can be used to generate a hypertuple covering
elements of Si may correspond to the same interval, and both xi and ti is as follows:
hence become equivalent. In general we have the follow-
ing number of distinctive intervals for an ordinal attribute: Ni = (max(ai)−max({xi,ti})+1)×(min({xi,ti})−min(ai)+1).
 00   mi−1           mi    mi(mi+1)
N  = ∑    (mi − j) = ∑  j =       .
 i    j=0            j=1      2                       If ai is nominal, the number of subsets for the same purpose
Neighborhoods as hypertuples                          is:                (
                                                                            m −1
For any simple tuple t, a neighborhood of t is taken to be a               2 i ,if xi = ti
                                                                    Ni =
hypertuple h such that t ≤ h; and a neighbor of t with respect             2mi−2,otherwiseRecall that mi = |dom(ai)|.                           weighting [Baily and Jain, 1978]. The purpose of the eval-
  To summarize, the number of neighborhoods of t covering uation is to compare the new measure with some of the com-
x is cov(t,x) = ∏i Ni, where                          monly used distance measures in a setup involving heteroge-
                                                      neous data. The evaluation uses public benchmark data sets
(2)
                                                     from UC Irvine Machine Learning Repository, which were
    (max(ai) − max({xi,ti}) + 1) × (min({xi,ti}) − min(ai) + 1),
                                                     selected with respect to their balance of ordinal and nominal
                if a is ordinal
N =                i                                  attributes.
 i    m −1
    2 i  ,      if ai is nominal and xi = ti
                                                       We implemented a k-NN algorithm with the novel NCM as
     m −2
     2 i  ,      if ai is nominal and xi 6= ti        well as the measures HEOM, HVDM, IVDM, and DVDM.
                                                      The computational runtimes of the MRM turned out too ex-
                                                      cessive, so it was excluded from the study. In the experiments
4.2  Use of cover as similarity measure               k was set to 1,6,11,16,21,16,31 and to ‘MaxK’ (i.e., k =
We use cov(t,x) as a measure of similarity between t and x, the number of data tuples in the training data). We adopted
which we call the Neighborhood Counting Metric or simply a 10-fold cross-validation procedure, and for each measure
NCM. That is, for any two tuples x and y, the NCM between and each k value we ran the cross-validation 10 times and
them is                                               recorded the results for subsequent analysis.
                                  n                     Due to space limitations, we report only some of the results
(3)         NCM(x,y) =  cov(x,y) = ∏Ni                in detail. Table 1 shows the results when weighting was used
                                 i=1                  and k = MaxK.

where n is the number of attributes and Ni is given by Eq.2. A statistical signiﬁcance analysis was carried out using
It is clear that NCM(x,y) ≥ 0, NCM(x,x) ≥ NCM(x,y) and the Student t-test (two samples, assuming unequal variances)
NCM(x,y) = NCM(y,x). Therefore the NCM is reﬂexive and with α = 0.05 (at a 95% conﬁdence level). For each data
symmetric, the properties generally required for a similarity set, each measure and each k value, we ran a 10-fold cross-
measure [Osborne and Bridge, 1997].                   validation using k-NN 10 times with random partitioning of
  This measure can be interpreted intuitively as follows. If data, resulting in a sample of 10 values. For a pair of samples,
we consider all neighborhoods around a data tuple, those tu- by two different measures, we have a total of 20 values. So
ples closer to this tuple should be included in more neigh- the critical value is 2.1. We then calculate the ‘t’ values. If
borhoods and those farther away should be included in fewer t ≥ 2.1 or t ≤ −2.1 the two samples are signiﬁcantly different
neighborhoods (see Figure 1). As a result, closer tuples (the classiﬁcation rate of one sample is signiﬁcantly higher or
should be assigned higher cover values.               lower than that of the other). Notice that every value in Table
  In contrast to its usual interpretation in terms of distance, 1 is the average of a sample of 10 values.
we interpret the notion of a neighborhood without distance From Table 1 we can see that based on our experimental
through the concept of hypertuples for both nominal and ordi- design the NCM achieved 17 out of 20 ‘signiﬁcantly’ higher
nal attributes. As a consequence, our novel distance measure classiﬁcation success rates compared to the other methods
applies to both ordinal and nominal attributes in a conceptu- (last row in table). Looking at the subtotals for ‘nominal’,
ally uniform way.                                     ‘ordinal’ and ‘mixture’ categories (reﬂected by subtotals from
  Incidentally, the NCM intrinsically handles missing values top to bottom), the NCM achieves the largest margin over its
in a fashion consistent with other measures. Recall, that the competitors for the ‘nominal’ data sets, followed by ‘mixture’
                                                      and then ‘ordinal’. This suggests that, when all data tuples are
NCM is a product of all Ni’s, where i is attribute index. For
two data tuples, t and x, if there is a missing value in t or x for taken into consideration, the NCM is clearly superior under
                                                      all circumstances.
attribute i, then Ni is set to 1. As a result, this attribute does
not contribute towards the NCM value.                   The details of the results involving other values for k are
                                                      not shown. In terms of k-changes, we observe that with-
5  Evaluation                                         out weighting there was no signiﬁcant difference between the
                                                      used measures. When weighting was used, there was still lit-
The new Neighborhood Counting Metric is task-independent tle difference for small k values. The general trend for all
and can therefore be used for classiﬁcation, clustering and measures was: as k got larger the classiﬁcation success rate
other analytical tasks involving distances or similarities. We increased slightly but soon started to decline. The difference
empirically evaluated the NCM in the context of a classiﬁ- then showed up as the NCM displayed a much slower rate
cation task, using the k-nearest neighbor (k-NN) classiﬁca- of decline and, after k > 11, the NCM consistently outper-
tion algorithm with and without distance-based (neighbor) formed the other four measures (see k = MaxK as discussed