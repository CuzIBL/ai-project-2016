              Towards a Theoretical Framework for Ensemble Classification 

                                               Alexander K. Seewald 
                              Austrian Research Institute for Artificial Intelligence 
                                      Freyung 6/VI/7, A-1010 Wien, Austria 
                                        alexsee@oefai.at; alex@seewald.at 


                        Abstract 
     Ensemble learning schemes such as AdaBoost and 
     Bagging enhance the performance of a single clas•
     sifier by combining predictions from multiple clas•
     sifiers of the same type. The predictions from an 
     ensemble of diverse classifiers can be combined 
     in related ways, e.g. by voting or simply by se•
     lecting the best classifier via cross-validation - a 
     technique widely used in machine learning. How•
     ever, since no ensemble scheme is always the best 
     choice, a deeper insight into the structure of mean•
     ingful approaches to combine predictions is needed 
     to achieve further progress. In this paper we offer 
     an operational reformulation of common ensemble 
     learning schemes - Voting, Selection by Crossvali-
                                                              Figure 1: Illustration of Stacking on a dataset with three classes 
     dation (X-Val), Grading and Bagging - as a Stacking 
                                                              (a, b and c), n examples and TV base classifiers. refers to the 
     scheme with appropriate parameter settings. Thus, 
     from a theoretical point of view all these schemes       probability given by classifier i for class j on example number k. 
     can be reduced to Stacking with an appropriate 
     combination method. This result is an important          the class prob.dist. for classifier i on the unknown test in•
     step towards a general theoretical framework for         stance. A fixed arbitrary order on the class values and N base 
     the field of ensemble learning.                          classifiers is assumed. Classk is the true class for instance k. 
                                                                        is the attribute vector of instance k. n is the num•
1 Introduction                                                ber of instances. All indices are zero-based, e.g. the instance 
                                                              id k satisfies the equation 0 k n — 1. 
We apologize that space restrictions arc forcing us to be terse. 
For a more detailed version, see [Seewald, 2003]. We will 
first explain how Stacking works in order to lay the founda•
tions for our functional definitions of meta classifiers later on. 
Fig. 1 shows Stacking on a hypothetical dataset. 
  During training, all base classifiers arc evaluated via cross- As we mentioned, we assume that all ensemble learning 
validation on the original dataset. Each classifier's output is a schemes return class probability distributions. If predictions 
class probability distribution for every example, see Fig. 1(b) are needed, the position of the maximum class probability in 
The concatenated class probability distributions of all base  the vector - i.e. the predicted class - is easily obtained via 
classifiers, followed by the class value, forms the meta-level formula (2) Trivially, Stacking with predictions can also be 
training set for Stacking's meta classifier, see Fig. 1(c) After simulated by this variant; simply by transforming the class 
training the meta classifier, the base classifiers are retrained distributions meta-dataset to predictions via (2) prior to ap•
on the complete training data.                                plying the meta classifier. 
  For testing, the base classifiers are queried for their class We can now characterize every ensemble learning scheme 
probability distributions. These form a meta-example for the  by what features it extracts from the meta-dataset during 
meta classifier which outputs the final class prediction.     training and how these features define the mapping from 
                                                              meta-dataset to final class probability distribution. 
2 Definitions                                                 2.1 Voting 
An arbitrary training dataset with n examples and k classes,  Voting, a straight-forward extension of voting for distribution 
and a single test instance, is given. N arbitrary base classifiers classifiers, is the simplest case. During training, no features 
have been cross-validated on this dataset, and afterwards re• are extracted from the meta-dataset. In fact Voting does not 
trained on the whole dataset. All base classifiers output class even need the internal crossvalidation. During testing, Voting 
probability distributions, i.e. estimated probabilities for each determines the final class probability distribution as follows. 
class instead of deciding on a single class. 
  Then, refers to the probability given by classifier i for 
class j on example number k during is the class prob•                                                                (3) 
ability distribution of classifier i on instance k. refers to 


POSTER PAPERS                                                                                                      1443 Thus, it can be easily seen that the meta-classifier defined by 3 Discussion & Conclusion 
just computing the mean probability distribution of the base   By definition StackingC [Seewald, 2002], can also be mapped 
classifiers - as above - simulates Voting with probability dis• onto Stacking via a special meta classifier. In fact, the avail•
tributions. Voting with predictions can be mapped similarity.  able implementation is a specialized subclass of a common 
In this case, we use instead of in formula (3) is the          meta classifier, MLR. Another recent variant, sMM5 [Dzeroski 
vector of p., for all j, where                                 and Zenko, 2002], is also implemented via a special meta 
                                                               classifier and thus amenable to the same kind of mapping. 
                                                       (4)     However, AdaBoost [Freund and Schapire, 1996] cannot be 
                                                               simulated by Stacking because of its sequential nature. 
2.2 X-Val                                                        While the given formal definitions of meta classifiers are 
For X-Val, which chooses the best base classifier on each fold mainly intended to enable further theoretical work, a direct 
by an internal ten-fold CV, we first determine the accuracy    implementation is also feasible. The cost penalty for the sim•
per classifier, which can be computed directly from the meta-  ulation is small, since training the meta classifiers usually 
level dataset, see (6) Then, we derive as feature the id of the contributes little to the total runtime. 
classifier with the maximum accuracy. Thus, the value of         Concluding, we have shown that Stacking is equivalent to 
bestC corresponds to our learned model.                       common ensemble learning schemes, namely Selection by 
                                                               Crossvalidation (X-Val), Voting of either class probability dis•
                                                       (5)    tributions or predictions, a competitive variant of Grading, 
                                                              and Bagging. Recent variants such as StackingC [Seewald, 
                                                       (6)    2002] and sMM5 [Dzeroski and Zenko, 2002] arc also equiv•
                                                              alent. Thus we conclude that our approach offers a unique 
                                                              viewpoint on Stacking which is an important step towards a 
                                                              theoretical framework for ensemble learning. 
                                                       (7) 
2.3 Grading                                                   Acknowledgements This research is supported by the Austrian 
For Grading [Seewald and Furnkranz, 2001], the case is dif•   Fonds zur Forderung der Wissenschafdichen Forschung (FWF) un•
ficult. While we cannot simulate the original Grading, we     der grant no. P12645-INF. The Austrian Research Institute for Arti•
will simulate a competitive variant equivalent to accuracy-   ficial Intelligence is supported by the Austrian Federal Ministry of 
weighted voting. More details see [Seewald, 2003].            Education, Science and Culture. 
  During training, the accuracies of base classifiers are ex•
tracted using formula (6) The accuracies of all our base clas• References 
sifiers correspond to our learned model. During testing,      [Breiman, 1996] Breiman, L. (1996): Bagging Predictors. 
we build the combined class probability distribution similar     Machine Learning (24), pp. 123-140. 
to Voting using predictions but with an additional weight -
namely the accuracy we extracted earlier.                     [Dzeroski and Zenko, 2002] Dzeroski S., Zenko B. (2002): 
                                                                 Is Combining Classifiers Better than Selecting the Best 
                                                       (8)       One?, in Proceedings of the 19th International Confer•
                                                                 ence on Machine Learning, 1CML-2002, Morgan Kauf-
                                                                 mann Publishers, San Francisco, 2002. 
                                                              [Freund and Schapire, 1996] Freund, Y., Schapire R.E. 
2.4 Bagging                                                      (1996): Experiments with a new boosting algorithm, 
Bagging [Breiman, 1996] is another common ensemble tech•         Proceedings of the International Conference on Machine 
nique. Here, the same type of classifier is repeatedly trained   Learning (ICML-96), pages 148-156, Morgan Kaufmann. 
on new datasets, which have been generated from the orig•     [Seewald and Furnkranz, 2001] Seewald A.K., Furnkranz J. 
inal dataset via random sampling with replacement. After•        (2001): An Evaluation of Grading Classifiers, in Hoff•
wards, the component classifiers are combined via simple un•     mann F. et al. (eds.), Advances in Intelligent Data Analy•
weighted voting.                                                 sis, 4th International Conference, IDA 2001, Proceedings, 
  Thus, we use same meta-classifier as for Voting with pre•      Springer, Berlin, pp. 115-124,2001. 
dictions. The number of base classifiers is equal to the iter•
ation parameter of bagging - each base classifier for Stack•  [Seewald, 2002] Seewald A.K. (2002): How to make Stack•
ing corresponds to one instantiation of the base learner for     ing Better and Faster While Also Taking Care of an Un•
bagging. In order to simulate the random sampling from the       known Weakness, in Proceedings of the 19th International 
training set, the base learner's training sets have to be modi•  Conference on Machine Learning, ICML-2002, Morgan 
fied before training, via formula (9)                            Kaufmann Publishers, San Francisco, 2002. 
                                                       (9)    [Seewald, 2003] Seewald A.K. (2003): Towards a Theoreti•
                                                                 cal Framework for Ensemble Classification (extended ver•
During training, Formula (9) is used to create - for each base 
                                                                 sion), Technical Report, Austrian Research Institute for 
classifier separately - a training set of the same size as the 
                                                                 Artificial Intelligence, Vienna, TR-2003-08. 
original training set via random sampling from the original 
training set, exactly as in Bagging. These training sets are  [Ting and Witten, 1999] Ting, K. M., Wittcn, 1. H. (1999): 
then used to train the base classifiers. This approach can also  Issues in stacked generalization. Journal of Artificial Intel•
be seen as a probabilistic wrapper around each base classifier.  ligence Research 10 (1999) 271-289. 
No features are extracted from the meta-level dataset during  [Wolpert, 1992] Wolpert, D. H. (1992): Stacked generaliza•
training, as for Voting.                                         tion. Neural Networks 5(2) (1992) 241-260. 
  During testing, each base classifier gives a prediction. 
These predictions are then voted to yield the final prediction, 
exactly as for Voting with predictions, i.e. (3) modified via 
(4) - for more details refer to subsection 2.1. Concluding, we 
have shown the equivalence of Bagging and Stacking. 

1444                                                                                                   POSTER PAPERS 