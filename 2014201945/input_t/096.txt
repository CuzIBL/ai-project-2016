          Improving (Revolutionary Search for Optimal Multiagent Behaviors 

              Liviu Panait                        R. Paul Wiegand                            Sean Luke 
   Department of Computer Science                  Krasnow Institute            Department of Computer Science 
       George Mason University                George Mason University                George Mason University 
           Fairfax, VA 22030                      Fairfax, VA 22030                      Fairfax, VA 22030 
          lpanait@cs.gmu.edu                      paul@tesseract.org                     sean@cs.gmu.edu 


                        Abstract                                 EC fits nicely with multiagent systems because it is already 
                                                               population-oriented: it searches over a set of multiple agents 
     Evolutionary computation is a useful technique for        (the individuals). Further, an EC population may be broken 
     learning behaviors in multiagent systems. Among           down into distinct subpopulations, each yielding agents to be 
     the several types of evolutionary computation, one        tested together in a multiagent environment, with each sub-
     natural and popular method is to coevolve multi-          population "evolving" in parallel. This notion of separately 
     agent behaviors in multiple, cooperating popula•          evolving, interacting populations of agents is known as co-
     tions. Recenl research has suggested that revo•           evolution. Coevolution has proven a useful technique for 
     lutionary systems may favor stability rather than         multiagent problems where the quality of agent is typically 
     performance in some domains. In order to im•              assessed in the context of competing or cooperating peers. 
     prove upon existing methods, this paper examines            But coevolution is no panacea. Recent research has shown 
     the idea of modifying traditional coevolution, bias•      that a coevolutionary system does not necessarily search 
     ing it to search for maximal rewards. We introduce        for better teams of agents, but can instead search for agent 
     a theoretical justification of the improved method        populations that represent stable equilibria in the coopera•
     and present experiments in three problem domains.         tive search space [Ficici and Pollack, 2000; Wiegand et a/., 
     We conclude that biasing can help coevolution find        2002b]. This paper will explore this problem, then introduce 
     better results in some multiagent problem domains.        a method for biasing coevolution so that the search for stabil•
                                                               ity coincides with optimization for improvement. 
                                                                 We continue this paper with a brief description of coevo•
1 Introduction                                                 lution and present an experimental and a theoretical frame•
                                                               work. We then suggest a method for biasing the coevolu•
Multi-agent learning is an area of intense research, and is 
                                                               tionary process, describe a theoretical investigation on how 
challenging because the problem dynamics are often com•
                                                               biasing modifies the search space, and discuss experimental 
plex and fraught with local optima. These difficulties have 
                                                               results on three problem domains. The paper ends with a set 
made evolutionary computation (EC) an attractive approach 
                                                               of conclusions and directions for future work. 
to learning multiagent behaviors (for example, flba, 1996; 
Luke et a/., 1998; Wu et a/., 1999; Bull et a/., 1995; 
Bassett and De Jong, 2000; Bull, 1997]). This work has led to  2 Evolutionary Computation and Coevolution 
interesting research questions in applying EC in a multiagent  Evolutionary computation is a family of techniques, known 
setting, including communication, representation, generaliza•  as evolutionary algorithms, widely used for learning agent 
tion, teamwork, and collaboration strategies.                  behaviors. In EC, abstract Darwinian models of evolution 
  As it is very general (and relatively knowledge-poor), evo•  are applied to refine populations of agents (known as indi•
lutionary computation is particularly useful in problems that  viduals) representing candidate solutions to a given problem. 
are of high dimensionality, are non-Markovian, or yield few    An evolutionary algorithm begins with an initial population 
heuristic clues about the search space that otherwise would    of randomly-generated agents. Each member of this popu•
make reinforcement learning or various supervised learning     lation is then evaluated and assigned a fitness (a quality as•
methods good choices. We believe that multiagent learning      sessment). The EA then uses a fitness-oriented procedure 
problem domains often exhibit such features. These problem     to select agents, breeds and mutates them to produce child 
domains are often complex and "correct" actions cannot be      agents, which are then added to the population, replacing 
known beforehand in a given situation. Further, even rela•     older agents. One evaluation, selection, and breeding cycle 
tively simple problems can require large numbers of external   is known as a generation. Successive generations continue to 
and, even more challenging, internal state variables. Last,    refine the population until time is exhausted or a sufficiently 
many such problems exhibit changing environments, even         fit agent is discovered. 
ones that adapt to make the problem harder for the learner       Coevolutionary algorithms (CEAs) represent a natural ap•
(due to the presence of co-learning opponents).                proach to applying evolutionary computation to refine mul-


MULTIAGENT SYSTEMS                                                                                                    653  tiagent behaviors. In a CEA, the fitness of an individual is                                                          (1) 
 based on its interaction with other individuals in the popula•                                                        (2) 
 tion: thus the fitness assessment is context-sensitive and sub•
jective. In competitive systems, agents benefit at the expense 
                                                                                                                       (3) 
 of other agents; but in cooperative systems, agents succeed 
 or fail together in collaboration. The focus of this paper is in 
 cooperative coevolutionary algorithms. Interesting CEA is•                                                            (4) 
 sues include communication [Bull et al., 1995], teamwork, 
 and collaboration [Bull, 1997].                               ...where x' and y' represent the new population distributions 
   A standard approach iPotter, 1997] to applying coopera•     for the next generation. Here it is assumed that an individ•
 tive coevolutionary algorithms (or CCEAs) to an optimiza•     ual's fitness is assessed through pair-wise collaborations with 
 tion problem starts by identifying a static decomposition of  every member of the cooperating population. We call this 
 the problem representation into subcomponents, each repre•    idea complete mixing. The equations above describe a two-
 sented by a separate population of individuals. For example,  step process. First, the vectors u and w are derived; these 
 if a task requires two agents whose collaboration must be op• represent the fitness assessments of strategies in the genera•
timized, one might choose to use two populations, one per      tions x and y respectively. Note that an infinite population 
 agent in the task. The fitness of an individual in a popula•  model considers the fitness assessment for a strategy, and not 
 tion is then determined by testing the individual in collabo• for a particular instance of that strategy (an individual). Then 
ration with one or more individuals from the other popula•     selection is performed by computing the proportion of the fit•
tion. Aside from this collaborative assessment, each popula•   ness of a specific strategy over the sum fitness of the entire 
tion follows its own independent evolution process in parallel population. 
with other populations. 
                                                               2.2 Optimization versus Balance 
2.1 Formalizing the CCEA                                       CCEA researchers apply these algorithms hoping to optimize 
An appealing abstract mathematical model for this system       the collaborations between the populations, but it isn't clear 
comes from the Biology literature: Evolutionary Game The•      that this system is meant to do this. In fact, the system seeks 
ory (EGT) [Maynard-Smith, 1982; Hofbauer and Sigmund,          a form of balance between strategies, which may not corre•
 1998]. EGT provides a formalism based on traditional game     spond with what we, as external viewers of the system, would 
theory and dynamical systems techniques to analyze the lim•    consider optimal. In the context of a payoff matrix, an op•
iting behaviors of interacting populations under long-term     timal position is the pair of strategies that yield the highest 
evolution. For specifics about applying EGT to the analysis of payoff for the cooperating agents. This position is a stable 
multi-population cooperative coevolutionary algorithms, see    attracting fixed point of such a system; but it is also the case 
[Wiegand et a/., 2002a].                                       that there are other suboptimal points, which can also attract 
                                                               trajectories [Wiegand et al., 2002b]. Indeed, it is possible that 
   In this paper, we consider only two-population models. In 
                                                               most, if not all, trajectories can be pulled toward suboptimal 
such a model, a common way of expressing the rewards from 
                                                               spots. These points correspond to Nash equilibria: subopti•
individual interactions is through a pair of payoff matrices 
                                                               mal combinations of strategies where if any one strategy is 
We assume a symmetric model such that when individuals 
                                                               changed, the net reward for both agents will decrease. 
from the first population interact with individuals from the 
second, one payoff matrix A is used, while individuals from      As a result, individuals in a CCEA are not necessarily re•
the second population receive rewards defined by the trans•    fined to be the optimal subcomponent of the optimal com•
                                                               ponent; instead they are refined to be jacks-of-all-trades that 
pose of this matrix (AT). In our theoretical exploration of 
EGT in this paper, we will use an infinite population: thus    dovetail nicely with the current individuals from the other 
a population can be thought of not as a set of individuals,    population. What does this mean for practitioners wanting 
but rather as a finite-length vector x of proportions, where   to coevolve "optimal" (or perhaps, even "good") cooperative 
each element in the vector is the proportion of a given indi•  strategies using a coevolutionary algorithm? It means that 
vidual configuration (popularly known as a genotype or, as     CEAs are not necessarily optimizers in the sense that one 
we will term it, a strategy) in the population. As the pro•    might intuitively expect them to be. Something must be done 
portions in a valid vector must sum to one, all legal vectors  to modify the existing algorithms or our expectations of what 
make up what is commonly known as the unit simplex, de•        these algorithms really do. 
noted A", where n here is the number of distinct strategies 
possible,                                                      3 Biasing for Optimal Cooperation 
   Formally we can model the effects of evaluation and pro•    One reason CCEAs tend toward "balance" is that an individ•
portional selection over time using a pair of difference equa• ual's fitness is commonly assessed based on how well it per•
tions, one for each population. The proportion vectors for the forms with immediate individuals from the other population. 
two populations are x and y respectively. Neglecting the is•   To find optimal cooperation, the search process may need to 
sue of mutation and breeding and concentrating only on the     be more optimistic than this: assessing fitness based more 
effects of selection, we can define the dynamical system of a  on the highest-reward interactions between an individual and 
two-population cooperative coevolutionary algorithm as:        various members of the other population. A previous investi•
                                                               gation in this direction is reported in [Wiegand et al, 2001]: 


654                                                                                             MULTIAGENT SYSTEMS Table 1: Joint reward matrixes for the Climb (left) and 
Penalty (right) domains. 

assessing an individual's fitness based on its maximum perfor•
mance with other agents in a collaborative domain was shown    Figure 1: Probability of converging to the optimum as the 
to yield better results than when using the mean or minimum    bias parameter 5 is varied between 0 and 1. 
performance. The idea presented in this paper is relatively 
simple: base an individual's fitness on a combination of its 
                                                               rative fitness for an individual. However, if the approximation 
immediate reward while interacting with individuals in the 
                                                               is too large (or has too strong an effect on the overall fitness), 
population, and on an estimate for the reward it would have 
                                                               and if it appears too early in the evolutionary run, then it can 
received had it interacted with its ideal collaborators. The 
                                                               deform the search space to drive search trajectories into sub-
fraction of reward due to the immediate (as opposed to the 
                                                               optimal parts of the space from which they cannot escape. On 
ideal) interaction changes during the course of the run. 
                                                               the other hand, if the approximation affects the fitness mea•
   We note that this notion of bias towards maximum possible   surement very weakly, and too late in the run, then it may not 
reward has also been used in the reinforcement learning litera• be of much help, and the system will still gravitate towards 
ture in subtly different ways than we use it here. For example, "balance". 
maximum reward was used by lOaus and Boutilier, 19981 to         To better see this tradeoff, we again alter equations 1 
modify the exploration strategy of the agent, and by lLauer    and 2, this time adding a bias weight parameter 5. Now, 
and Riedmiller, 2000] to modify the update rule for the Q ta•
                                                               u = (1 -8)-i4y+8-max,i7' andvT'= (1 -5)ATx+5m'd*ArT. 
ble. To some extent, the "Hall of Fame" method introduced      Varying 8 between 0 and 1 will control the degree to which 
by [Rosin and Belew, 1997] for competitive coevolution is      the model makes use of the bias. Consider the Climb payoff 
also related to biased cooperative coevolution.                matrix on the left side of Table 1. We select 500 initial points 
   We justify the use of such a bias in a CCEA as follows. 
                                                               of the dynamical system uniformly at random from A" x Am, 
Recall that if an individual's fitness is based on its immediate and iterate the system until it converges. While convergence 
interaction with individuals from the other population, then   is virtually guaranteed in traditional two-matrix EGT games 
u = Ay and w — ATx as described in equations 1 and 2. Now,     LHofbauer and Sigmund, 19981, it is not necessarily guar•
let us consider a function maxA that returns a column vector   anteed in our modified system. In our experimental results, 
corresponding to the maximum value of each row in matrix       however, we obtained convergence in all cases to within some 
A. Now, if an individual's fitness is based on its maximum     degree of machine precision. Figure 1 shows the probability, 
possible performance in conjunction with any individual from   for various levels of 8, of the dynamical system converging 
the other population, then we may modify equations 1 and 2     to the optimum when the penalty is set to -30, -300, -3000 
to be w = maxAT and w = max^r'.                                or -30000. Notice that, as the penalty worsens, the transition 
   In this modified system, the tendency to optimize perfor•   between optimal and suboptimal convergence becomes more 
mance is clear. At each iteration of the model, the fitness of severe. This suggests that for some problems, any benefits 
each strategy will be its best possible fitness. If there is a provided by this type of bias may be quite sensitive to the 
unique maximum, that result will have the highest fitness and  degree of bias. 
so the proportion of the corresponding strategy will increase 
in the next step. When the global maxima are not unique, the   4 Experiments 
resulting fixed point is a mixed strategy, with weights split 
between those maxima.                                          While this theoretical discussion helps justify our intuition 
   The reason for this is straightforward: the problem has lost for including a performance bias in fitness evaluation, it is 
the dimensionality added due to the nature of the interactions not immediately applicable to real problems. In a more real•
between the agents. Without this, the problem reduces to a     istic setting, simplifying model assumptions such as infinite 
simple evolutionary algorithm: regardless of the content of    populations, lack of variational operators, complete mixing, 
the opposing population, the fitness measure for a given strat• and a priori knowledge of the maximum payoff are not pos•
egy is the same. As shown in [Vose, 1999], an infinite popu•   sible. To convert theory into practice, we have adopted an 
lation model of this reduced evolutionary algorithm will con•  approximation to the performance bias that is based on his•
verge to a unique global maximum.                              torical information gathered during the evolutionary run. We 
   But it is difficult to imagine how a real CCEA algorithm    also decreased the bias through the course of a run to take ad•
would know the maximum possible reward for a given indi•       vantage of the fact that initial partners are likely to be weak, 
vidual a priori. One approach is to use historical information while later partners are stronger. 
during the run to approximate the maximum possible collabo-      We performed several experiments to compare simple co-


MULTIAGENT SYSTEMS                                                                                                   655                                                                          Table 2: Proportion of runs that converged to global optimum 
                                                                         and average best individual fitness, Climbing Domain 

                                                                                                              Penalty 

 Figure 2: Joint reward in the continuous Two Peaks domain 

evolution (SC) with biased coevolution (BC) in three prob•
 lem domains detailed later. Both SC and BC base fitness on 
the immediate performance of an individual in the context of             Table 3: Proportion of runs that converged to global optimum 
individuals from one other cooperating population. BC ad•                and average best individual fitness, Penalty Domain 
ditionally includes a bias factor: part of the fitness is based 
on an approximation of what an individual's fitness would be 
                                                                         ular strategy, over all possible partner strategies. In the exper•
were it to cooperate with its ideal partners. 
                                                                         iments in this paper, we chose to approximate MaxReward by 
   We compared these two techniques in combination with 
                                                                         setting it to the maximum reward seen so far in the run for the 
two approaches to representing an individual. In the Pure 
                                                                         given strategy. 
Strategy Representation (PSR), an individual represented a 
                                                                            In all experiments, the most fit individual survived auto•
single strategy. PSR individuals stored a single integer repre•
                                                                         matically from one generation to the next. To select an in•
senting the strategy in question. A PSR individual bred chil•
                                                                         dividual for breeding, we chose two individuals at random 
dren through mutation: a coin was repeatedly tossed, and the 
                                                                         with replacement from the population, then selected the fitter 
individual's integer was increased or decreased (the direction 
                                                                         of the two. Each experiment was repeated 100 times. The 
chosen at random beforehand) until the coin came up heads. 
                                                                         experiments used the ECJ9 software package [Luke, 2002]. 
In the Mixed Strategy Representation (MSR), an individual 
represented not a single strategy but a probability distribu•            4.1 Problem Domains 
tion over all possible strategies. When evaluating an MSR 
                                                                         We experimented with three different single-stage game do•
individual with a partner agent, 50 independent trials were 
                                                                         mains: two simpler ones (Climb and Penalty) introduced in 
performed, and each time each agent's strategy was chosen 
                                                                         [Claus and Boutilier, 1998], and a more complex artificial 
at random from the the agent's probability distribution. MSR 
                                                                         problem (Two Peaks). Evolutionary runs in the Climb and 
individuals used one-point crossover, followed by adding ran•
                                                                         Penalty problem domain lasted 200 generations and used 20 
dom Gaussian noise (ju — 0, a = 0.05) to each of the distribu•
                                                                         individuals per population. Runs in the Two Peaks domain 
tion values, followed by renormalization of the distribution. 
                                                                         lasted 500 generations and used populations of 100 individu•
Observe that using MSR creates a potentially more difficult 
                                                                         als each. 
problem domain than using PSR, for reasons of search space 
size and stochasticity of the fitness result.                               The joint reward matrices for the Climb and the Penalty 
                                                                         domains are presented in Table 1. The domains are difficult 
   We chose a common approach to cooperative coevolution 
                                                                         because of the penalties associated with miscoordinated ac•
fitness assessment. An individual is assessed twice to deter•
                                                                         tions and the presence of suboptimal collaborations that avoid 
mine fitness: once with a partner chosen at random, then once 
                                                                         penalties. Figure 2 presents a continuous version of a the Two 
partnered with the individual in the other population that had 
                                                                         Peaks coordination game, where the x and y axes represent the 
received the highest fitness in the previous generation. An in•
                                                                         continuous range of actions for the two agents, and the z axis 
dividual's fitness is set to the maximum of these two assess•
                                                                         shows the joint reward. The reward surface has two peaks, 
ments. During a fitness assessment, an individual receives 
                                                                         one lower but spread over a large surface, and the other one 
some number of rewards for trying certain strategies in the 
                                                                         higher but covering a small area. Because an agent's strategy 
context of partners. For a PSR individual, the assessment was 
                                                                         space is continuous over [0,1], we discretized it into increas•
simply the single reward it received for trying its strategy with 
                                                                         ingly difficult sets of 8, 16, 32, 64 or 128 strategies. The 
its partners. As an MSR individual tried fifty strategies, its as•
                                                                         discretizations result in slightly different optimal values. 
sessment was the mean of the fifty rewards it received. 
   SC and BC differ in that BC adds into the reward a bias               5 Results 
term, that is, Reward <— (1 - 5) • Reward 4- 5 • MaxReward, 
where 5 is a decreasing bias rate that starts at 1.0 and lin•            Tables 2-4 present the proportion (out of 100 runs) that con•
early decreases until it reaches 0 when 3/4 of the maximal run           verged to the global optimum, plus the mean fitness of the 
length has passed. Ideally, the MaxReward bias factor would              best individuals in the runs. MSR individuals were consid•
be the highest possible reward received for trying that partic-          ered optimal if and only if the optimal strategy held over 50 


656                                                                                                             MULTIAGENT SYSTEMS Table 4: Proportion of runs that converged to global optimum 
and average best individual fitness, Two Peaks Domain 


                                                                                         200 300 
                                                                                          Generations 

                                                               Figure 4: Distance from best-of-generation individuals to op•
                                                               timal strategy for the 32 actions Two Peaks domain using SC 
                                                               (top) and BC (bottom). 

                                                               successful applications of this biasing method are tied to suc•
                                                               cessfully determining the appropriate degree of bias to apply. 
                                                               Due to MSR's increased difficulty, it may be more challeng•
                                                               ing to find an appropriate balance for the bias. Figure 3 sug•
                                                               gests exactly this. Notice that, in the early part of the run 
                                                               (when 8 is strong), the algorithm tends towards the optimal 
Figure 3: Distance from best-of-generation individuals to op•  solution; however, as the bias is reduced, it becomes over•
timal strategy for the 8 actions Two Peaks domain using SC     whelmed and the trajectories are eventually drawn toward the 
(top) and BC (bottom).                                         suboptimal local attractor. Moreover, as the problem becomes 
                                                               larger (i.e., Figure 4, as well as others not shown), this failure 
                                                               occurs earlier in the run. This suggests more careful attention 
percent of the distribution (in fact, most optimal MSR indi•
                                                               is needed to set the parameters and to adjust the bias rate when 
viduals had over 90 percent). 
                                                               using MSR versus PSR. Indeed, by running longer and allow•
  Biased coevolution consistently found the global optima 
                                                               ing for more interactions during evaluation, we were able to 
as often as, or more often than, standard coevolution. The 
                                                               obtain convergence to the global optimum when using MSR 
only times where standard coevolution held its own was in 
                                                               (not shown). 
the Climbing and Penalty domains, where PSR individuals 
found the optimum 100% of the time, as well as in the harder 
Two Peaks domain, where no MSR individuals found the op•       6 Conclusions and Future Work 
timum. For those problems when individuals found the op•       Although cooperative coevolution has been successfully ap•
timum less than 100% of the time, we also compared dif•        plied to the task of learning multiagent behaviors, as re•
ferences in mean best fitness of a run, using a two-factor     search about these algorithms advances, it becomes increas•
ANOVA with repetitions, factored over the method used and      ingly clear that these algorithms may favor stability over opti-
the problem domain.                                            mality for some problem domains. In this paper, we develop 
  The ANOVA results allow us to state with 95% confi•          a very simple idea: improve coevolution through the use of 
dence that biased coevolution is better than simple coevolu•   a maximum reward bias. We introduce a theoretical justifi•
tion when MSR is used in the Climbing domain, and also        cation for the idea, then present experimental evidence that 
in the Two Peaks domain when PSR is used; the tests give      confirms that biasing coevolution can yield significantly bet•
only a 90% confidence for stating that BC+MSR is better than  ter results than standard coevolution when searching for op•
SC+MSR in the Penalty domain.                                 timal collaborations. Our work further reveals that domain 
  In order to better understand what happens when using       features greatly influence the levels of biasing necessary for 
MSR in the Two Peaks domains, we plotted the average eu-      convergence to optima: for some problems the performance 
clidian distance from the best individual per generation to the changes slowly when the level of bias is modified, while for 
known global optima (Figures 3 and 4). The graphs present     other domains there is a rapid degradation in results. This 
the 95% confidence interval for the mean of the fitnesses. In• suggests that, while adding some kind of maximum reward 
vestigations showed that SC converged to suboptimal interac•  bias can be helpful, there is still work to be done in under•
tions (the lower, wider peak in Figure 2) in all cases. On the standing how best to apply this bias in different problem do•
other hand, the trajectories of the search process are radically mains. 
different when using BC. Let's take a closer look as to why      Our initial experimental results in this paper suggest that 
this might be so.                                             it is effective to use a history as an approximation to the true 
  As we learned from our discussion surrounding Figure 1,     maximal collaborative reward for a given strategy. For future 


MULTIAGENT SYSTEMS                                                                                                   657 