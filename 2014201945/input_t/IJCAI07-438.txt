                            Pseudo-Aligned Multilingual Corpora

                                 Fernando Diaz and Donald Metzler
                                    Department of Computer Science
                                      University of Massachusetts
                                          Amherst, MA 01003
                                     {fdiaz,metzler}@cs.umass.edu

                    Abstract                          document in hand. When this is not the case, cross-lingual
                                                      information retrieval systems allow users to query a corpus
    In machine translation, document alignment refers in their native language and retrieve documents in a foreign
    to ﬁnding correspondences between documents       language. The results can then either be manually or machine
    which are exact translations of each other. We    translated. We offer a hybrid approach which embeds all doc-
    deﬁne pseudo-alignment as the task of ﬁnding      uments in multiple languages into a single semantic space.
    topical—as opposed to exact—correspondences       By providing a language-neutral embedding space, we can
    between documents in different languages. We ap-  collectively analyze a foreign collection of documents with-
    ply semisupervised methods to pseudo-align multi- out being constrained to document-based and query-based
    lingual corpora. Speciﬁcally, we construct a topic- analysis. For example, a user may be interested in cluster-
    based graph for each language. Then, given exact  ing or visualizing all documents in every language simultane-
    correspondences between a subset of documents,    ously.
    we project the unaligned documents into a shared    We will now deﬁne our collection alignment problem. As-
    lower-dimensional space. We demonstrate that      sume that we are given two document collections. For ex-
    close documents in this lower-dimensional space   ample, consider one in English and one in Mandarin. In ad-
    tend to share the same topic. This has applica-   dition, we are given some training correspondences between
    tions in machine translation and cross-lingual in- documents we know are exact translations of each other. For
    formation analysis. Experimental results show that example, assume we have a handful of English documents
    pseudo-alignment of multilingual corpora is fea-  manually translated into Mandarin. Our task is, for each En-
    sible and that the document alignments produced   glish or Mandarin document in the untranslated set, to ﬁnd
    are qualitatively sound. Our technique requires   the topically most similar documents in the foreign corpus.
    no linguistic knowledge of the corpus. On aver-   This process results in a pseudo-aligned corpus.
    age when 10% of the corpus consists of exact cor-   Our approach aligns the underlying topical structures of
    respondences, an on-topic correspondence occurs   two parallel collections.1 Given a parallel corpus, the lexi-
    within the top 5 foreign neighbors in the lower-  con and distribution of terms within each side of the corpus
    dimensional space while the exact correspondence  will be quite different. However, since the corpus is parallel,
    occurs within the top 10 foreign neighbors in this the underlying topical structure is likely to be very similar
    this space. We also show how to substantially im- regardless of the underlying language.
    prove these results with a novel method for incor-  We conceptualize this topical structure in the form of a
    porating language-independent information.        manifold over documents, where documents that are topically
                                                      related are ‘close’ to each other on the manifold. Thus, we
1  Introduction                                       can view a corpus as a sample from some underlying mani-
Electronic information is available in many different lan- fold. We are interested in the case where the topical distribu-
guages. If a user can only read Greek, then the amount of tions between languages are very similar. Here, our working
information available online is somewhat limited compared hypothesis is that the true underlying topical manifolds of any
to a user who understands English. Therefore, in order to two languages are isomorphic.
allow as many people to access as much information as pos- We use techniques from spectral graph theory to automat-
sible, it is increasingly important to develop technologies that ically pseudo-align documents in different languages. Un-
allow users to access information in a language-neutral fash- like machine translation systems, which focus on exact 1-to-1
ion.                                                     1A multilingual corpus is a set of documents C such that each
  Two language technologies have been developed to tackle document is written in one of a set of languages L = {l1,l2,...}.
this task. First, machine translation systems attempt to bridge A parallel corpus has the additional property that for every docu-
the language barrier by translating content on demand. This ment di ∈ C, there are |L|−1 other documents in C that are exact
approach is appropriate when someone has a known-relevant translations of di into the other languages in L.

                                                IJCAI-07
                                                  2727alignments of documents or sentences, we instead focus on a space and align documents using distances in this joint em-
looser sense of alignment, based on topical relevance. Our re- bedding space.
sults show that it is possible to recover topic and exact align-
ments of documents using a reasonably small set of training 3.1 Representing Document Collections
examples and very na¨ıve linguistic processing. We also show Graph-based representations of document collections view
how to improve these results with a novel method for incor- documents as nodes in a graph. Edges in this graph exist be-
porating language-independent information.            tween documents which share a property such as topic, genre,
                                                      author, etc. Because we are interested in topical alignment of
2  Related Work                                       collections, we will be focusing on topical edges. In this sec-
                                                      tion, we will be discussing one method of detecting topical
Parallel corpora are a fundamental concept in machine trans- relationships. Although others certainly exist, graph-based
lation. Traditionally, the alignment problem focuses on align- representations have consistent behavior across afﬁnity mea-
ing sentences between two documents known to be exact sures [Diaz, 2005].
translations [Gale and Church, 1993]. Statistical machine Given a corpus containing n documents and |V | terms, one
translation systems require this level of granularity to learn of the most popular document representations is the length-
relationships between words in different languages. Our ap- |V | term vector. Constructing the vector often requires a
proach relaxes both the granularity and exact-translation con- term-weighting scheme such as tf.idf. In our work, we will
straints.                                             assume that document vectors are language models (multi-
  Oftentimes, we know parallel corpora exist but do not have nomial term distributions) estimated using the document text
the correspondences. This happens frequently on the world [Croft and Lafferty, 2003]. By treating documents as prob-
wide web where entire hierarchies may be represented in sev- ability distributions, we can use distributional afﬁnity to de-
eral languages. The solution to this problem usually requires tect topical relatedness between documents. Speciﬁcally, we
inspecting and aligning URLs and structural tags in the docu- use the multinomial diffusion kernel [Lafferty and Lebanon,
ments [Resnik and Smith, 2003]. While this approach works 2005]. Given two documents i and j, the afﬁnity is measured
well for structured and explicitly-linked data, when this in- between the two distributions, θi and θj,as
formation is missing or inexact, the solution may not work.                                   
                                                                               −1      2
Our approach only requires relationships between the content K(θi,θj)=exp−t      arccos    θi ·  θj   (1)
within a language and is robust to noise.
  Another alternative to re-alignment uses external dictio- where t is a parameter controlling the decay of the afﬁnity.
naries to create probabilistic relationships between unaligned The diffusion kernel has been shown to be a good afﬁnity
documents [Resnik and Smith, 2003]. While this technique metric for tasks such as text classiﬁcation and retrieval.
is applicable to our task, we are interested in methods which A document graph for a particular language, then, is con-
do not require external resources such as dictionaries. structed by treating the n documents as nodes and, for each
  Our work is also related to the task of aligning multidi- document, adding undirected, weighted edges to the k nearest
mensional data sets. When viewing documents as, say, En- neighbors as measured by the diffusion kernel. We represent
glish term vectors and Mandarin term vectors, we can use these a document graph as the n × n adjacency matrix W .In
techniques such as canonical correlation analysis or Gaussian our experiments, we ﬁx t =0.50 and k =25. We use a sim-
processes to compute a transformation between the spaces ple maximum likelihood estimate for the document language
[Hardoon et al., 2004; Shon et al., 2006]. Correspondences models.
and translations can also be addressed in terms of graphi-
cal models [Barnard et al., 2003]. Solutions using spectral 3.2 Functions on Graphs
graph theory are the most related to our work [Carcassoni Because our alignment algorithm uses results from spectral
and Hancock, 2003; Ham et al., 2005; Shon et al., 2006; clustering, we will brieﬂy review some fundamentals before
Verbeek and Vlassis, 2006]. We apply these spectral tech- presenting our solution. A more thorough treatment of the
niques and extend them to include manifold-independent in- material can be found in other sources [Chung, 1997].
formation.                                              We deﬁne a function f over the nodes of a graph as a
                                                      length-n vector. We can measure the smoothness of this func-
                                                                   (  −   )2
3  Collection Alignment                               tion as ij Wij fi fj . This is known as the Dirichlet sum
                                                      and computes the difference in the function value between
Our procedure for aligning corpora consists of two phases: connected nodes.
                                                                                        T
representing monolingual document collections and aligning The Dirichlet sum can be written as f (D − W )f where
the monolingual representations. In the ﬁrst phase, we con- D is a diagonal matrix such that Dii = j Wij. The ma-
sider a graph-based representation of the document collec- trix Δ=D − W is known as the combinatorial Laplacian.
tion. Graphs provide intuitive and ﬂexible collection models We can introduce alternative Laplacians to provide different
suitable for a variety of tasks such as classiﬁcation and re- measures of smoothness. In this paper, we will always use the
trieval [Diaz, 2005; Zhu et al., 2003]. The second phase is to approximate Laplace-Beltrami operator [Lafon, 2004]. This
ﬁnd topically similar nodes in the foreign graph using labeled is deﬁned as,
document alignments. We employ spectral graph theory to
                                                                                −1/2    −1/2
project documents in all languages into a single embedding         Δ=I      − Dˆ    Wˆ Dˆ             (2)

                                                IJCAI-07
                                                  2728                                             ˆ   =
where we use the normalizing afﬁnity matrix W          We can rewrite Equation 4 using these augmented matrices,
  −1    −1     ˆ =    n   ˆ
D   WD     with D     j=1 Wij. This approximation pro-                             TΔz
                                                                         ( )=h        h
vides a density normalization effect that we have found im-            C h          T                 (7)
portant when dealing with document collections.                                    h h
  The k eigenvectors associated with the lowest non-zero where h =[f TgT]T and we deﬁne the composite Laplacian
eigenvalues of the Laplacian represent the functions f min- matrix, Δz = Δˆ x + Δˆ y. This can be seen as using the com-
imizing the Dirichlet sum. In turn these eigenvectors can bined Laplacian, Δz, of a new graph with 2n − m nodes.
be used to embed documents in a lower dimensional space When viewing alignment as analyzing a larger graph, we
                                               ×
[Belkin and Niyogi, 2002]. If we let E represent the n k notice that Δz contains zero submatrices between unaligned
matrix of these eigenvectors, we can represent each docu- nodes across languages. This is problematic since graph
ment i using the corresponding row vector of E. We then Laplacian techniques exploit link structure to detect topics.
compute the Euclidean distance between documents in the k- In order to address this issue, we “seed” these submatrices
dimensional space,                                    with predicted alignments from a simple baseline. We repre-
                                                            2( −   )
            2(    )=        (   −     )2              sent the n  m  unaligned documents in an m-dimensional
           d xi,xj           Eik  Ejk           (3)   space. The elements of each document vector represent the
                          k                           afﬁnity with the training documents. That is, we use the
                                                      (  −   ) ×                           x       y
3.3  Aligning Collections                              n   m     m lower left submatrices of W and W .We
                                                      calculate the seed afﬁnities by L2 normalizing the rows and
We now deﬁne our collection alignment problem. Assume             x    y T
                                                      computing Wul (Wul) . This (n − m) × (n − m) matrix
that we are given two document graphs represented by the deﬁnes our initial predictions of the alignments between un-
  ×                     x       y
n   n adjacency matrices W and W . In addition, we are aligned documents. We will refer to this as our baseline in
given m<ntraining correspondences between documents   experiments. In the case of these experiments, we place the
we know are exact translations of each other. We can reorga- prediction matrix in the middle-right/lower-middle blocks of
nize the adjacency matrices so that the indexes of correspond- x y
                                                      Wˆ  and Wˆ .
ing documents match and are located in the m × m upper left
         x      y                                       We can align documents by ﬁrst projecting all 2n−m doc-
blocks,   and    . Our task is to ﬁnd the most topically
      Wll     Wll                                     uments into a lower-dimensional space and then computing
similar documents for the unlabeled 2( − ) documents.
                                n   m                 distances in that lower-dimensional space. With enough la-
  We use the manifold alignment method proposed by Ham beled instances, the projection should improve the baseline
    [               ]
et al Ham et al., 2005 . Speciﬁcally, we are interested in predictions. We use the Laplacian-based projection method
ﬁnding the functions f and g minimizing the following ob-
                                                      described in Section 3.2. Given a document xi in language
jective,                                              x, its predicted aligned pair in language y is the closest doc-
                        f TΔxf + gTΔyg                ument in the embedding space. Highly ranked documents,
           C(f,g)=                              (4)   then, are likely to be topically related.
                          f Tf + gTg
                                                      3.4  Incorporating Language-Independent
such that fi = gi for i<m. The pairs of functions mini-
mizing this objective can be used to project documents into a Information
single lower-dimensional space.                       In many cases, documents contain language-independent in-
  Although both Laplacians, Δx and Δy, are the same size, formation which can be exploited for alignment. Examples
the indexes m ≤ i<nrefer to potentially different doc- include named entities, hyperlinks, and time-stamps. In this
uments. Therefore, we build adjacency matrices with three section, we extend Ham’s alignment algorithm to consider
sets of vertices: the ﬁrst set of vertices is common between such manifold-independent information. Speciﬁcally, we ex-
languages; these are the training instances with known align- ploit the temporal information present in the document.
                                                        Recall that we viewed our alignment as using the combined
ment (0 ≤ i<m). The second set is documents from lan-            z
                               ≤                      Laplacian, Δ , of a new graph with 2n−m nodes. We would
guage x with unknown alignment (m i<n), and the third                                     2  −
set is language y with unknown alignment (n ≤ i<2n−m). like to consider a second graph over these n m nodes in-
This results in the (2n − m) × (2n − m) matrices,     corporating the language-independent knowledge. This graph
                                                      will be deﬁned so that edges occur when two documents share
                        x    x                                                                        t
                       Δll   Δlu  0                   the same date; call this unweighted adjacency matrix W .
             ˆ x         x    x                                                Δz    Δt
            Δ    =     Δul  Δuu   0             (5)   This gives us two Laplacians, and over the large graph.
                        000                           We then measure the smoothness of the function h on both
                        y       y                   graphs,
                       Δll  0Δlu
            Δˆ y =      000                                                 T  z           T  t
                                                (6)             t         λh Δ  h +(1−  λ)h Δ  h
                       Δy   0Δy                                C (h)=                                 (8)
                         ul      uu                                                hTh
Notice that we are augmenting these graphs so that there are where the parameter λ allows us to weight the temporal in-
no edges to new nodes. We will see in Section 3.4 how to formation. Here, our solution falls from embedding docu-
incorporate language-independent knowledge we might have ments in a lower dimensional space deﬁned by the lowest
about the relationship to these foreign documents.    non-constant eigenvectors of λΔz +(1− λ)Δt.

                                                IJCAI-07
                                                  2729    1.  compute n × n afﬁnity matrices for languages x ming was performed. The Chinese corpora was tokenized us-
           y
        and                                           ing character unigrams. No additional segmentation or anal-
    2.  add the 25 nearest neighbors for each document to
          x      y                                    ysis was performed. After tokenization, documents were in-
        W   and W
                             x     y                                                 [                  ]
    3.  compute the Laplacians, Δ and Δ               dexed using the Indri retrieval system Strohman et al., 2004 .
    4.  compute the predicted alignments              We use only date stamps (not time stamps) as our language-
    5.  construct the combined Laplacian, Δz          independent information.
    6.  if language-independent information exists inter- One concern we had when using parallel corpora was that
        polate (1 − λ)Δz + λΔt                        the graph structures would be identical. We found that, even
    7.  compute the k eigenvectors associated with the for our machine translated corpus, the graphs were quite
        smallest non-zero eigenvalues; stack in matrix E different. Nevertheless, we conducted a set of experiments
    n   number of documents in one side of the parallel where random subsets of the nodes were removed from each
        corpus
    k                                                 side of the corpus. This is equivalent to having non-parallel
        dimensionality of the joint embedding space   collections with identical topical distributions.
    λ   interpolation parameter for language-independent
        information                                   4.2  Evaluation
    En×      k  projection of all documents into k-   We train our algorithms by providing m example correspon-
        dimensional space                             dences randomly selected from the collection; in our experi-
                                                      ments, we present the number of training correspondence as a
Figure 1: Pseudo-alignment algorithm. Input are k and λ. fraction of the corpus. We evaluate the re-alignment of paral-
The output is a set of distance between all unlabeled docu- lel corpora using two measures. First, we consider the mean
ments. The closest pairs represent predicted alignments. reciprocal rank (MRR) of the true match. That is, we compute
                                                      distance from a document in language x to all documents in
                                                      language y; the reciprocal rank of the true translation of this
  When we evaluate our algorithms using parallel corpora, document gives us the score for this document. We use the
this temporal information is powerful but unrealistic. Doc- mean reciprocal rank over all 2(n − m) testing documents.
uments with shared topics will rarely have exactly the same We noticed that even at few training correspondences,
date. Therefore, we consider a corruption of the date infor- though the MRR was quite low (on average the true trans-
mation in our corpus. We accomplish this by corrupting the lation in the top 100 documents), the qualitative matches ap-
date information through the following process: for each doc- peared quite good. For example, the closest neighbors to a
ument i, select a date d from a Gaussian distribution whose document about Sri Lanka—while not including the exact
mean is the date of document and a variance, σ. Select a doc- translation—contained documents about Sri Lanka. Because
ument j uniformly from amongst all of the documents on that our qualitative analysis suggested that MRR was underrep-
date. Construct an edge between i and j. Repeat this pro- resenting our performance, we wanted to evaluate the topi-
cess 50 times for each document. The parameter σ allows us cal alignment. Fortunately, a subset of the English-Mandarin
to control the error in establishing links between documents. corpus contains assessments for topical equivalence between
Alowσ  will result in constructing edges to 50 nodes which documents. We therefore adapted the MRR measure to look
share the same date as i; a high σ will result in construct- for the top ranking on-topic document; we refer to this as
ing edges to 50 nodes less temporally local to i. This has TMRR.
the effect of modeling documents on the same topic as being
published on different but close dates.               5   Results
  We present a summary of our alignment algorithm in Fig- Our ﬁrst set of experiments investigates the performance
ure 1.                                                of our algorithms with respect to the training alignments.
                                                      The number of eigenvectors was ﬁxed at 300. Figure 2
4  Methods and Materials                              depicts learning evaluated by MRR for the Arabic-English
4.1  Corpora                                          and English-Mandarin corpora and TMRR for the English-
Parallel corpora allow us to evaluate the document-level Mandarin corpus. The baseline algorithm uses only the dis-
alignment for collections where the topical distributions tances to the training documents to predict alignments. Our
are identical. We used two parallel corpora: an Arabic- alignment algorithm uses both these predictions as well as
English corpus of United Nations documents and an English- information about the relationship between unaligned docu-
Mandarin corpus of newswire documents.  The Arabic-   ments.
English corpus consists of 30K United Nations documents The results in Figure 2 demonstrates that our alignment
manually translated into both languages [Ma et al., 2004]. algorithm improves the baseline at few training documents.
Because some dates were under-represented or missing, we However, as the training size increases, this improvement dis-
only used documents between 1994 and 1999. The English- appears. We speculate that this task is such that, after a cer-
Mandarin corpus consists of 50K Chinese newswire docu- tain point, the number of training alignments provide enough
ments published between August and September 2003 and information to adequately distinguish unaligned documents;
                                                                                                   x
their machine translated representations in English [Fiscus the additional information encoded in the matrices Wuu and
                                                        y
and Wheatley, 2004].                                  Wuu  do not add any discriminating information. In fact, be-
  The English and Arabic sides of the corpora were tok- cause the Laplacian-based alignment technique discards in-
enized on whitespace and punctuation. No stopping or stem- formation in the projection, performance may suffer. This can

                                                IJCAI-07
                                                  2730                 Arabic−English/MRR             English−Mandarin/MRR           English−Mandarin/TMRR


                                      0.20                       ●
       0.4                                                     ●      0.35
                                                              ●
                                                            ●
                                                                                            ●
                                                          ●
                                                         ●                                      ●
                                 ●                                                             ●
                                ●                                     0.30                   ●
                              ●                        ●                                ●
                             ●                                                       ●
                           ●
       0.3               ●            0.15            ●
                        ●                                             0.25                ●
                      ●                             ●
                     ●                                                                 ●
                   ●
                  ●
                                                   ●
                ●                                                                 ●
                                                                      0.20          ●
       0.2                            0.10
               ●                                 ●
      MRR                            MRR
                                                                     TMRR 0.15
                                                                                 ●

                                                ●                              ●
             ●                                                        0.10
       0.1                            0.05
                                              ●
                                                                      0.05    ●
            ●                                                               ●
                            ● laplacian      ●              ● laplacian    ●               ● laplacian
       0.0                    baseline     ●                  baseline 0.00                  baseline
                                      0.00
          0      500     1000   1500     0       500    1000    1500     0      500     1000   1500

                   num aligned                     num aligned                    num aligned

Figure 2: Mean reciprocal rank of the true translation for Arabic-English alignment (left) and English-Mandarin alignment
(center). Topic mean reciprocal rank for English-Mandarin alignment (right). All algorithms used 300 eigenvectors.


be seen in performance curves for the Arabic-English corpus.               English−Mandarin/MRR
Nevertheless, when training data is sparse, the structure ex-
                                                                                       ●
                                                                 0.25        ●
tracted by the Laplacian-based technique can be leveraged to            ●
                                                                       ●            ● ●     ●
                                                                         ●              ●
                                                                     ● ●    ●
improve on the baseline.                                                           ●      ●
                                                                              ● ● ●
                                                                          ●           ●
                                                                               ●           ●

  In all cases, both document-level and topical alignment are    0.20
feasible even when only using content information. For ex-


ample, at 500 training examples, we get the true alignment       MRR

in the top 4 for the Arabic-English corpus and the top 10 for    0.15
the English-Mandarin corpus. When looking at topical align-
ment, we can get an on-topic document in the 7 for the same
                                                                                       ● laplacian
number of training instances.                                    0.10                    baseline

  Our ﬁrst experiments evaluated the realignment of parallel        0.00  0.25  0.50 0.75  1.00
corpora. We were interested in testing the robustness of our                 fraction unaligned
techniques to non-parallel corpora. In order to accomplish
this, we ﬁrst ﬁxed the number of training correspondences,
                                                      Figure 3: Performance as a function of unaligned documents
m, at 1000. We also ﬁxed the number of testing correspon-
                                                      added to the collection. We ﬁxed the training and testing set
dences at 1000. We then added 20000 documents from each
                                                      sizes to 1000 and added 20000 documents from each side of
collection. These documents were selected such that some
                                                      the corpus. Of these 20000, some fraction were not required
fraction of them were included as pairs of aligned documents.
                                                      to be aligned pairs.
The remaining fraction were randomly sampled, potentially
unaligned documents from the collections. Varying the frac-
tion of unaligned documents in the 20000, we plotted the                  English−Mandarin/TMRR

MRR for the test correspondences in Figure 3. We notice           1.0 ● text
that both our baseline and new algorithm are not effected by           text+date
the addition of unaligned documents.                              0.8
                                                                               1●
                                                                               2●
  We evaluated our temporal alignment using several values        0.6

for σ (the date corruption parameter); we present results for                  5●
         {1 2 5}                                                 MRR
the values , ,  . We were interested in the performance           0.4
over various values of λ. Fixing the training correspondences        ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
at 10000 and number of eigenvalues at 150. we varied the          0.2
value of λ. We present these results in Figure 4. Here, the im-
provements gained by introducing temporal information are         0.0
very dependent on the value of λ. Obviously, at low val-            0.10 0.25   0.50   0.75 0.90
ues, the algorithms will reduce to the text-based alignment.                    λ
However, the reduction in performance observed at the higher
values for λ are likely due to documents being ranked exclu- Figure 4: Incorporation of language-independent informa-
sively by their temporal proximity.                   tion. We ﬁxed the number of training correspondences to be
  We caution that the temporal corruption process introduces .20 of the collection and evaluated performance as a function
temporal dimensions to topics which are potentially atempo- of the weight, λ, placed on the language-independent infor-
ral. For example, there is no reason to believe that two doc- mation.

                                                IJCAI-07
                                                  2731