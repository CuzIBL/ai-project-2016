                     Unsupervised Learning by Backward Inhibition 


                                                 Tomas    Hrycej 
                                       PCS Computer Systeme GmbH 
                    Pfaelzer-Wald-Str. 36, D-8OO1 ) Muenchen 90, West Germany 


                       Abstract                            al. [1985]. (Analogical mathematical concepts can be 
                                                           found in discriminant and regression analysis 
     Backward inhibition in a two-layer                    [Kohonen, 1984]. For symbolic supervised learning 
     connectionist network can be used as an               see, e.g., Winston [1986], Quinlan [1982] or 
     alternative to, or an enhancement of, the             Kodratoff and Ganascia [1986].) However, in some 
     competitive model for unsupervised learning.          situations, the data available are not sufficient for 
     Two feature discovery algorithms based on             supervised learning: 
     backward inhibition are presented. It is                 1) "Correct" outputs, or responses, are not always 
     shown that they are superior to the                        known. In some cases, merely an overall 
     competitive feature discovery algorithm in                 criterion of response quality is given (e.g., the 
     feature independence and controllable grain.               survival criterion for living organisms, or reaching 
     Moreover, the representation in the feature                a positive biological or emotional state for 
     layer is distributed, and a certain                        humans). This has lead to a modification of 
     "classification hierarchy" is defined by the               supervised learning - instead of a set of correct 
     features discovered.                                       responses, only a reinforcement criterion is 
                                                                given. 
                                                             2) Even with such a "weaker" reinforcement, direct 
 1 Introduction                                                 learning of correct responses may fail because of 
                                                                a high complexity of input. This can be illustrated 
 Connectionist, or neural network, models, i.e., 
                                                                on the backpropagation model [Rumehart et al., 
 models consisting of networks of simple processing             1986]. It has been shown that only a limited class 
 nodes, have been shown to possess cognitive                    of input-output encodings can be materialized in 
 capabilities, in particular, the capabilities of shape         a two-layer connectionist system. So more 
 recognition [Hinton and Lang, 1985, Fukushima,                 complex, multiple-layer systems and 
 1980, or von der Malsburg and Bienenstock, 1986],              corresponding learning schemes have been 
 development of concepts [Dalenoort, 1987], learning            developed. However, multiple-layer systems 
 [Grossberg, 1982, 1987, Ackley et al.. 1985,                   scale poorly - systems with five or more layers 
 Rumelhart et al., 1986, Le Cun, 1986], storing of              seem to be computationally intractable. So there 
 patterns [Hopfield, 1982], generalization [Anderson,           are obvious limitations of the supervised learning: 
 1986], etc..                                                   more complex inputs require more layers, but too 
   Connectionist models provide some important                  many layers are, in turn, intractable. 
 advantages over symbolic models: 
                                                             A proposal for solution of the latter problem 
   • automatic development of representation, 
                                                           [Ballard, 1987] is to partition the network in some 
   • correction of noisy input,                            modular way. A part of the network would find (e.g., 
   • graceful degradation if some cells are erroneous,     by "autoassociation", a form of unsupervised learning, 
   • generalization and                                    in Ballard's model) a distributed and highly invariant 
   • inherent parallelism.                                 representation of input and supervised learning would 
                                                           be then applied to this discovered representation, 
   Connectionist models are typically less transparent 
                                                           instead of the original input. 
 than symbolic ones. So the preferable (or even the 
                                                             More general learning models capable of 
 only possible) form of knowledge input is learning by 
                                                           performing unsupervised classification by discovering 
 examples. 
                                                           characteristic features or regularities of input data 
   The most straightforward form of learning is 
                                                           have been presented by Rumelhart and Zipser [1985] 
 supervised learning, i.e., associating inputs with 
                                                           and Grossberg [1987]. They perform essentially some 
 some known outputs. The best-known algorithms are 
                                                           clustering algorithm in a network form (for a 
 backpropagation algorithm of Rumelhart et al. [1986] 
                                                           symbolic analogy, see, e.g., conceptual clustering of 
 and Boltzman machine learning algorithm of Ackley et 
                                                           Stepp and Michalski [1986] or the "hybrid" model of 


170 Parallel and Distributed Processing Lebowitz [1985]). However, those models suffer from                  This rule is a special case of adaptation law studied 
several deficiencies. As argued in Section 2.2, the                by Kohonen [1984, page 103, Case 5]. As shown by 
model of Rumelhart and Zipser suffers from 1)                      Kohonen, the weight vector converges to the dominant 
random and uncontrollable grain of feature discovery,              eigenvector, i.e., the eigenvector corresponding to the 
2) constructing redundant and local feature                        largest (dominant) eigenvalue, of the input correlation 
representation, and 3) difficulties in building feature            matrix E(X*X). 
hierarchies (for more details, see Section 2.2).                      Since this is equally true for all weight vectors, all 
  This paper presents a novel type of feature                      feature units discover the same feature, the dominant 
discovery model, a backward inhibition model, that                 eigenvector. So this model is no serious candidate for 
does not suffer from the above deficiencies.                       feature discovery. However, it can be modified in 
Mathematically, it is related rather to variance                   several ways to perform more satisfactorily. 
analysis than to cluster analysis. 
                                                                   2.2 Competitive correlation model 
2 Existing models 
                                                                   One of the possible modifications is the competitive 
In recent past, several models for feature discovery               model proposed by Rumelhart and Zipser [1985] (for 
have been presented. Most of them can be classified                a more general model, see Grossberg [1987]). The 
as "competitive models". A well-known representant                 feature layer of this model consists of multiple feature 
of this class is critically examined in Section 2.2.               nodes. Instead of activating each feature 
   To introduce some mathematical concepts                         proportionally to the weighted input into the feature 
necessary for explanation of backward inhibition                   cell, only the feature unit with maximal activation, 
concept, a rudimentary one-feature model is                        i.e., with maximal similarity (= inner vector product) 
presented in Section 2.1.                                          of its weight vector with the input vector "fires" and 
                                                                   only its own weights are modified. 
2.1 Simple correlation model                                          An obvious interpretation of the learning process in 
The basis for all models treated in this paper is a                this model is that, for a given input vector, the feature 
simple noncompetitive two-layer correlation model,                 with the maximal value of similarity measure (inner 
further referred to as "basic adaptive model". All                 vector product) between the input and its currents 
nodes of the input layer are connected to a single node            weights learns by moving its weights toward the input 
of the feature layer (see Figure 2). A feature y is                vector. So competition causes each input vector to 
represented by the vector w of weights assigned to                 "attract" the "nearest" feature, which results in 
connections to a feature node. Activation level of a y             partitioning input into exclusive "clusters". Since, 
is given by w'x, x being input vector (activation                  because of competition between features, very 
rule). This model learns by a widely used correlation              different inputs will probably fire different features 
learning rule, whose continuous form is:                           while similar ones will fire a single common one, the 
   dw = a*y*x*dt,                                                  system will converge to a stable state in which each 
                                                                   feature corresponds to a cluster of input stimuli. The 
   with a<<la constant. 
                                                                   similarity between members within a single cluster will 
                                                                   be significantly higher than the similarity between 
                                                                   those of different clusters. A more formal description 
                                                                   of this learning process is given in [Rumelhart and 
                                                                   Zipser, 1985]. 
                                                                      A shortcoming of this model is that some feature 
                                                                   cells can remain inactive (i.e., they never fire because 
                                                                   of being always outperformed by others. 
                                                                      A direct consequence of this behaviour is that 
                                                                   typically very coarse features are found. The only way 
                                                                   to refine the grain of discovered features is by 
                                                                   increasing number of feature cells. But the more 
                                                                   feature cells there are, the higher the probability that 
                                                                   many of them remain inactive. 
                                                                     There are two proposals for remedy in [Rumelhart 
                                                                   and Zipser, 1985], but our analysis of them has shown 
                                                                   that, in general case, they do not exhibit better 
                                                                   performance than the basic competitive model 
                                                                   [Hrycej, 1987]. 
                                                                      There are some additional deficiencies of the 
                                                                   competitive model: 
                                                                      1) Features found by the competitive model are 
                                                                        disjoint. If their number were substantially higher 
                                                                        than two, features would be (almost) 
                                                                        independent. But satisfying this condition cannot 


                                                                                                                  Hrycej 171      always be guaranteed - on the contrary, finding 
     only two features is very frequent (see above and 
     Section 3.4). But then, there is, in fact, only one     with x - original input, xx - corrected input, y\ -
     independent feature - the features are simply        feature unit. The suppressed feature unit will not be 
     logical complements of each other.                   further activated, since 
   2) The representation of input by the feature layer is 
     local since each input activates a single feature       The suppression can be simulated by propagating 
     node. So the representation found does not           inhibitory signal (in the size of feature activation) 
     possess advantageous properties (error backwards to input units. E.g., if feature node A has 
     correction, efficient coding, automatic been activated to activation level 2 via connections of 
     generalization [Hinton et al., 1986]) of strengths 0.5 and 0.3 to input nodes B and C, 
     distributed representations.                         respectively, it inhibits nodes B and C by amounts 
   3) None of the features discovered by the -2*0.5=-l and -2*0.3=0.6, respectively. 
     competitive model represents a finer partitioning       These properties of backward inhibition will be 
     of others. This results directly from their          used in the next two sections for constructing feature 
     disjointness. So all features are of the same        discovery algorithms. 
     hierarchical level. A hierarchical modification of 
     such a system would be possible if additional        3.2 Eigenvectors as features 
     layers were added so that clusters discovered by 
                                                          The adaptive rule of Section 2.1 discovers only the 
     lower levels would be further partitioned by on 
                                                          dominant eigenvector of input correlation matrix. We 
     higher levels. This arrangement requires an a 
                                                          can use backward inhibition principle of the previous 
     priori structuring of the network (a tree 
                                                          section for furhter features to converge to further 
     structure). For above reasons, such a structure 
                                                          eigenvectors. So all eigenvectors of input correlation 
     would be very inefficient: successor nodes of an 
                                                          matrix can be found by successively applying 
     inactive feature node would always remain 
                                                          backward inhibition. 
     inactive, too. 
                                                             Eigenvectors of a correlation matrix have some 
   4) An additional drawback of this algorithm is that    properties which make them intuitively good 
     maximum finding is no parallel operation.            candidates for meaningful features: 
     (However, there are some parallel alternatives to       a)They extract highly correlated input lines. 
     maximization [Reggia, 1985, Chun et al., 
                                                             b) If all input lines are mutually independent, 
     1987].) 
                                                               eigenvectors are unit vectors, while the dominant 
   Note: The above criticism concerns only                     eigenvector corresponds to the input line with 
feature-discovery properties of the competitive model.         largest variance, or "the most important input 
There are further valuable properties of competition,          feature". 
e.g., noise reduction or contour enhancement, not 
                                                             c) If all input lines are completely correlated, the 
covered by the alternative backward-inhibition model 
below.                                                         dominant eigenvector corresponds to the vector 
                                                               of input variances (or to the correlation matrix 
                                                               diagonal). 
3 Backward inhibition 
                                                             d) If the correlation matrix can be partitioned to 
The drawbacks of the above competitive model result            several diagonal submatrices corresponding to 
from its lack of capability to discover finer features         mutually independent groups of highly correlated 
simultaneously with coarse ones. This seems to be the          input lines, there are eigenvectors corresponding 
very nature of competition. This section presents a            to each of these groups. 
concept of "backward inhibition" which copes with            e) Eigenvectors of a correlation matrix (which is 
this problem.                                                  always a symmetric matrix) are orthogonal and 
                                                               thus independent. 
3.1 Backward inhibition concept                              f) They define a linear transformation which 
                                                               recodes the input as completely as possible, for a 
The basis for my further reflections is the idea that 
                                                               given dimensionality. In other words, they 
more subtle features can be discovered only if strong 
                                                               explain the variability of the input in the most 
features overshadowing them are suppressed in some 
                                                               compact way (in the sense of main components). 
way. 
   The implementation of this idea is very simple.           So the set of all eigenvectors seems to be an 
After a strong feature y\ has been successfully learned,  appropriate feature system. 
it is suppressed in the input for some time. The 
optimal way to suppress a feature in the next input       3.3 Latency time algorithm 
vector is to isolate its component orthogonal to the      As stated in the previous section, all eigenvectors of 
suppressed feature, i.e., to subtract the orthogonal      input correlation matrix can be found by successively 
projection of input vector x on the feature weight        applying backward inhibition. The simplest method to 
vector wj. Since the feature to be suppressed is          do this would be to use a sequential algorithm. First, 
represented by wj with |wj| = 1, we get                   it learns the dominant eigenvector, i.e., it lets the 


172 Parallel and Distributed Processing weight vector of the first feature node converge to the                  to be discovered. If the latency time is long, week 
dominant eigenvector. Then, it inhibits this dominant                    features are discovered, too. 
feature in the input (via orthogonal projection, see                  3) The representation of input by the feature layer is 
above) so that the eigenvector with the second largest                   distributed (see the following example). 
eigenvalue becomes the dominant eigenvector of such 
                                                                      4) The eigenvectors being ordered by their 
a modified input, and can be in turn discovered by 
                                                                         eigenvalues, they represent a certain implicit 
some other feature node, and so on until all 
                                                                         hierarchy, as illustrated by the following 
eigenvectors have been discovered. It is, in fact, a 
                                                                         example. 
 Gram-Schmidt process for evaluation of orthogonal 
projections [Kohonen, 1984, page 165]. The ordering                   Example 1: Let us have 8 input patterns (see Figure 
                                                                   2) represented by 16 binary input lines. (For 
of features corresponds to descending ordering of 
                                                                   symmetry, -1 are substituted for zeros.) Although 
 eigenvectors by the absolute value of corresponding 
                                                                   theoretically 16 features could be found, we will be 
 eigenvalues. 
                                                                   satisfied with 6 features. Features discovered by 
 For all F from 1 to number of features: 
                                                                   backward inhibition algorithm are presented in Table 
   learn F-th feature by the basic adaptive rule while 
                                                                    1 as they are contained in the patterns (the activation 
   backward-inhibiting the input by all features FF<F. 
                                                                   value of each feature multiplied by 10 is given in each 
   An obvious drawback (and cognitive inadequacy) of               feature column). For easier orientation, an abridged 
 this algorithm is its sequentiality. This is, to a certain        code is given in the middle column. Symbol meanings 
 degree, an intrinsic characteristics of the problem               are: + for feature activation > 0.7, - for feature 
 since our adaptation rule always learns only the                  activation < 0.7, o otherwise. 
 dominant feature. In other words, small features have 
 to be enabled to come out of the shadow of large 
 features by filtering large features out. This can occur 
 only sequentially. However, control can be made 
 parallel (or better to say local in each feature node) by 
 the following extended activation rule applied 
 simultaneously to all features: 
 Step 1: Activate feature nodes by the basic adaptive 
   model activation rule. 
 Step 2: If the weights of a certain feature node have 
   reached a (temporarily) stable state, this feature 
   node performs backward inhibition of input nodes 
   for a certain number of cycles, the latency time. 
   The learning rule remains identical to that of the 
 basic adaptive model (learning of a latent feature node 
 is suspended during its latency time). 
   For this algorithm, a criterion of weight stability is 
 needed. A simple test of weight increment 8w for 
 being zero, or very small, is not appropriate since 
 input is inherently varying and thus weights never 
 become completely stable. So an average over recent 
 periods has to be used. I have used an exponentially 
 weighted average of weight increment 8vv over past 
 cycles EdwI Edwabs (a ratio of average increment and 
 average absolute value of increment) with Edw = 
 p*Edw + (7-p)*8w and Edwabs = p* Edwabs + 
 (V-p)*|8w|. This ratio converges to zero if positive 
 increments are periodically traded-off by negative                   A "classification hierarchy", formed by descending 
 increments while their absolute values may be                     order of eigenvalues is given in Figure 3. As we can 
                                                                   see, dominant Feature 2 partitions the paterns to 
 relatively large (depending on the a-coefficient of the 
                                                                    "diagonally oriented" (+) and "margin-oriented" (-). 
 adaptive rule). 
                                                                   Those diagonally oriented are partitioned by Feature 6 
   This feature discovery model is superior to the 
                                                                   to "bottom right - top left" (+) and "bottom left - top 
 competitive one in several aspects: 
                                                                   right" (-). Features 1 and 3 measure "balancedness" 
    1) Eigenvectors of a correlation matrix are 
                                                                   of diagonal patterns. 
      orthogonal (since a correlation matrix is always 
                                                                      This hierarchy is not explicit in the feature vector. 
      symmetric) and thus independent. 
                                                                   However, an implicit encoding is all that is needed for 
    2) The grain of feature discovery can be                       input into a further associative network. Some 
      controlled by latency time parameter. If latency             associations can depend only on strong features like 
      time is short, strong features are rapidly learned           "diagonal" vers, "margin-oriented", encoded by a 
      while there is not enough time for weak features             single feature cell F2 while weaker ones like diagonal 


                                                                                                                  Hrycej 173 orientation are encoded by F2 and F6 together, yet 
more special features by adding a further feature cell,   3.4 Competitive algorithm 
etc., so that the actual encoding corresponds to the 
                                                          Backward inhibition can also be used to improve grain 
"path" in the hierarchy tree.                             control of the competitive model of Section 2.2. After 
                                                          a feature unit y\ has won the competition and its 
                                                          weights have incrementally learned, the feature 
                                                          represented by its weight vector wj is (partially) 
                                                          suppressed for some time: 
                                                             xx = x - b*yi*v/i 
                                                             with 0 < b < 1. Parameter b controls the extent of 
                                                          suppression and thus the grain of feature discovery. 
                                                          Because of competition features do not correspond to 
                                                          eigenvectors. 
                                                             Although this model is difficult to treat 
                                                          mathematically, it can be supposed (and has been 
                                                          verified experimentally, see Example 3) that even 
                                                          weak features get their chance to attract a weight 
                                                          vector so that the grain of feature discovery will be 
                                                          refined. 
  The feature system can be used to classify novel           Example 3: To illustrate grain control, a set of five 
patterns, as illustrated by the following example.        input vectors has been taken [3,1,0,0,0], [1,3,0,0,0], 
  Example 2: Four novel patterns have been                [0,0,3,1,1], [0,0,1,3,1], and [0,0,1,1,3]. Their 
experimentally classified by the system (see Figure 4     correlation matrix is 
and the Table 2). Note that Pattern 9 has been 
classified as "bottom right - top left" and "balanced" 
but undecided if "margin-oriented" or "diagonal". 
Pattern 10 is "margin-oriented", Pattern 11 
"diagonal" and "balanced" with undecided direction. 


                                                            Five learning trials have been made with five and 
                                                          ten feature units and b = 0.0 to 0.9 (b = 0.0 is 
                                                          equivalent to the original model without backward 
                                                          inhibition). Average numbers of active features are 
                                                          given in the table below. 


                                                            The 10-unit case showed poor convergence for 
                                                          b>0.6, so corresponding results are not presented. It 
                                                          can be seen that basic competitive algorithm 
  Note: With short latency times, only partitioned the patterns typically into three groups, no 
approximations of eigenvectors are found. Since the       matter if there were five or ten feature units. The 
"residue" after suppressing "strong" eigenvectors may     backward-inhibition-based algorithm has found, by 
then substantially differ from the exact one, features    contrast, three to five groups (five being the maximum 
corresponding to "weak" eigenvectors may since there are only five different input vectors), 
substantially differ from exact eigenvectors, too.        depending on the backward inhibition strength b. 
However, in all cases tested they showed very high 
orthogonality, so that good properties of features        4 Conclusion 
discovered were preserved. 
                                                          Backward inhibition in a two-layer connectionist 
                                                          network can be used as an alternative to, or an 
                                                          enhancement of, the competitive feature discovery 
                                                          model. 


174 Parallel and Distributed Processing 