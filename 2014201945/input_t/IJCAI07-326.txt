                     Planning under Risk and Knightian Uncertainty
         Felipe W. Trevizan                Fabio´ G. Cozman                Leliane N. de Barros
Instituto de Matem´atica e Estat´ıstica    Escola Polit´ecnica      Instituto de Matem´atica e Estat´ıstica
     Universidade de S˜ao Paulo        Universidade de S˜ao Paulo        Universidade de S˜ao Paulo
        Rua do Mat˜ao, 1010          Av. Prof. Mello Moraes, 2231           Rua do Mat˜ao, 1010
        S˜ao Paulo, SP, Brazil            S˜ao Paulo, SP, Brazil           S˜ao Paulo, SP, Brazil
        trevisan@ime.usp.br                fgcozman@usp.br                  leliane@ime.usp.br

                    Abstract                          gebraic MDPs (AMDPs); we forgo some generality because
                                                      we want to employ models with solid behavioral justiﬁcation,
    Two noteworthy models of planning in AI are prob- and to exploit the speciﬁc structure of combined probabilistic-
    abilistic planning (based on MDPs and its gener-  nondeterministic planning. Even though general algorithms
    alizations) and nondeterministic planning (mainly such as AMDP-based value iteration are useful theoretically,
    based on model checking). In this paper we: (1)   we ﬁnd that a more speciﬁc approach based on Real-Time
    show that probabilistic and nondeterministic plan- Dynamic Programming leads to encouraging results on com-
    ning are extremes of a rich continuum of prob-    putational complexity.
    lems that deal simultaneously with risk and (Knigh-
                                                        This necessarily brief review of closest literature should in-
    tian) uncertainty; (2) obtain a unifying model for dicate the central motivation of this work: we strive to work
    these problems using imprecise MDPs; (3) derive   with decision processes that have solid behavioral founda-
    a simpliﬁed Bellman’s principle of optimality for
                                                      tion and that can smoothly mix problems of practical signif-
    our model; and (4) show how to adapt and analyze
                                                      icance. A similar approach has been proposed by Eiter and
    state-of-art algorithms such as (L)RTDP and LDFS  Lukasiewicz [2003], using nonmonotonic logic and causal se-
    in this unifying setup. We discuss examples and
                                                      mantics to deﬁne set-valued transitions in Partial Observable
    connections to various proposals for planning un-
                                                      MDPs (and leaving algorithms for future work). We offer a
    der (general) uncertainty.                        model that assumes full observability, and we obtain algo-
                                                      rithms and complexity analysis for our model.
1  Introduction                                         The remainder of this paper is organized as follows. In
Planning problems can be classiﬁed, based on the effects of Section 3 we discuss how MDPSTs capture the continuum of
actions, in deterministic, probabilistic or nondeterministic. In planning problems from “pure” probabilistic to “pure” non-
this paper we are concerned with action dynamics under gen- deterministic planning. In Section 4 we show that MDPSTs
eral forms of uncertainty; indeed, we are interested in plan- are Markov decision processes with imprecise probabilities
ning under both risk and Knightian uncertainty.Weshow (MDPIPs), a model that has received attention in operations
how to use these concepts to express probabilistic and non- research and that displays a solid foundation. We also com-
deterministic planning (and combinations thereof) as Markov ment on the various relationships between MDPSTs and other
decision processes with set-valued transitions (MDPSTs). models in the literature. In Section 5 we show that MDPSTs
  Similar generalizations of Markov decision processes lead to important simpliﬁcations of their “minimax” Bellman-
(MDPs) have appeared before in research on artiﬁcial intel- style equation (we note that such simpliﬁcations are men-
ligence. For example, Givan et al. [2000] use intervals to tioned without a proof by Buffet and Aberdeen [2005] for
encode a set of exact MDPs, which is used to conduct space BMDPs). We obtain interesting insights concerning compu-
state reduction of MDPs. Their bounded-parameter MDPs tational complexity of MDPSTs and related models. Section
(BMDPs) form  neither a superset nor a subset of MDP- 6 investigates algorithms that produce minimax policies for
STs. Buffet and Aberdeen [2005] use BMDPs to produce  MDPSTs. Although our results yield easy variants of value
robust policies in probabilistic planning. They also show and policy iteration for MDPSTs, we are interested in more
that Real-Time Dynamic Programming (RTDP) can be used efﬁcient algorithms based on RTDP. In Section 6 we derive
in BMDPs. Our perspective is different: we wish to unify the conditions that must be true for RTDP to be applied. Sec-
various strands of planning that have proven practical value, tion 7 brings a few concluding remarks.
using a theory that has a behavioral basis on preferences and
beliefs — otherwise, we do follow a similar path to Buf- 2 Background
fet and Aberdeen’s in that we exploit RTDP in our models.
Another recent work that should be mentioned is Perny et 2.1 Varieties of planning
al.’s [2005], where transitions must only satisfy a few alge- We start reviewing a few basic models of planning problems,
braic properties. Our models are a strict subset of their Al- attempting to unify them as much as possible as suggested by

                                                IJCAI-07
                                                  2023recent literature [Bonet and Geffner, 2006]:          that compute the optimal policies for each one of these mod-
                                                      els, and algorithms that can be specialized to all of them
M1 a discrete and ﬁnite state space S,
                                                      [Bonet and Geffner, 2006]. However, we should emphasize
M2 a nonempty set of initial states S0 ⊆S,            that previous unifying frameworks do not intend to handle
M3 a goal given by a set SG ⊆S,                       smooth “mixtures” of these planning problems. In fact, one
                                                      of our goals in this paper is to provide a framework where
M4 a nonempty set of actions A(s) ⊆Arepresenting the
                              s                       NONDET and MDPs are the extreme points of a continuum
   actions applicable in each state ,                 of planning problems.
M5 a state transition function F (s, a) ⊆Smapping state s
   and action a ∈A(s)  into nonempty sets of states, i.e.
   |F (s, a)|≥1,and                                   2.2  Varieties of uncertainty
M6 a positive cost C(s, a) for taking a ∈A(s) in s.   Probability theory is often based on decision theory [Berger,
Adapting M2, M5andM6, one can produce:                1985], a most appropriate scheme in the realm of planning.
                                                      Thus a decision maker contemplates a set of actions, each
  •
    Deterministic models (DET), where the state transition one of which yields different rewards in different states of
                          |F s, a |
    function is deterministic: ( ) =1. In “classical” nature. Complete preferences over actions imply that a pre-
                                             |S |
    planning, the following constraints are added: (i) 0 = cise probability value is associated with each state — a situa-
         S    ∅        ∀s ∈S,a∈As      C s, a
    1; (ii) G = ; and (iii)         ( ): (   )=1.     tion of risk [Knight, 1921; Luce and Raiffa, 1957].Anobvi-
  • Nondeterministic models (NONDET), where the ac-   ous example of sequential decision making under “pure” risk
    tions may result in more than one successor state without is probabilistic planning. However, often preferences over
    preferences among them.                           actions are only partially ordered (due to incompleteness in
  • Probabilistic models (MDPs), where actions have   beliefs, or lack of time/resources, or because several experts
    probabilistic consequences. Not only the function disagree), and then it is not possible to guarantee that precise
    |F (s, a)|≥1 is given, but also the model includes: probabilities represent beliefs. In those cases, a set of proba-
                                                      bility measures is the adequate representation for uncertainty;
    (MDP1) a probability distribution P0(·) over S0;and
                                                      such sets are often referred to as credal sets [Levi, 1980;
    (MDP2) a probability distribution P (·|s, a) over F (s, a)
                                                                                  ]
    for all s ∈S,a∈A(s).                              Kadane et al., 1999; Walley, 1991 . Even though terminology
                                                      is not stable, this situation is said to contain Knightian uncer-
  For any of these models, we expect that a solution (e.g. a tainty (other terms are ambiguity or simply uncertainty). An
policy) is evaluated on its long-term costs. The cost of a so- extreme case is nondeterministic planning,whereno proba-
lution can be evaluated in a ﬁnite-horizon, in which the max- bilities are speciﬁed.2
                                           k ∈ R
imum number of actions to be executed is limited to +.  Note that actual decision making is rarely restricted to ei-
An alternative is to consider discounted inﬁnite-horizon,in ther “pure” risk nor “pure” Knightian uncertainty; in fact
which the number of actions is not bounded and the cost of the most realistic scenario mixes elements of both. Not sur-
actions is discounted geometrically using a discount factor
  <γ<                                            k    prisingly, such combinations are well studied in economics,
0         1. Since it is difﬁcult to ﬁnd an appropriate psychology, statistics, and philosophy. We note that credal
for each problem, in this paper we assume the discounted
                       1                              sets have raised steady interested in connection with arti-
inﬁnite-horizon framework.                            ﬁcial intelligence, for example in the theory of probabilis-
  Due to the assumption of full observability and discounted [            ]                       [
inﬁnite-horizon cost, a valid solution is a stationary policy, tic logic Nilsson, 1986 , in Dempster-Shafer theory Shafer,
that is, a function π mapping states s ∈Sinto actions 1976], in theories of argumentation [Anrig et al., 1999],
a ∈A(s). Bellman’s principle of optimality deﬁnes the op- and in generalizations of Bayesian networks [Cozman, 2005;
                 ∗
timal cost function V (s)=mina∈A(s) QV ∗ (s, a) [Bellman, Fagiuoli and Zaffalon, 1998].
1957],where:8                                           The usual prescription for decision making under risk is to
         >C(s, a)+γV  (s),s ∈ F (s, a) for DET,     select an action that maximizes expected utility. In the pres-
         >                    
         <C(s, a)+γ   max  V (s ) for NONDET, and     ence of Knightian uncertainty, matters become more com-
                    s∈F (s,a)
 QV (s, a)=          X                          (1)   plex, as now a decision maker carries a set of probability
         >C(s, a)+γ      P (s|s, a)V (s)
         :>                           for MDPs.       measures and consequently every action is associated with
                   s∈F (s,a)                         an interval of expected costs [Walley, 1991]. Thus a decision
 This principle characterizes V ∗ (also called optimal value maker may choose one of several criteria, such as minimax-
                                                                                 [            ]
function) and induces the optimal policy for each model: ity, maximality, E-admissibility Troffaes, 2004 .Inthispa-
 ∗                                                    per we follow a minimax approach, as we are interested in
π (s)  =  argmina∈A(s) QV ∗ (s, a). The deﬁnition of
Q   s, a                                              actions that minimize the maximum possible expected cost;
 V (   ) clariﬁes the guarantees of each model. In DET, we leave other criteria for future work.
guarantees given by π∗ do not depend on its execution; in
NONDET guarantees are on the worst-case cost; and in
MDPs guarantees are on expected cost. There are algorithms 2The term “nondeterministic” is somewhat unfortunate as non-
                                                      determinism is often equated to probabilism; perhaps the term plan-
  1Results presented here are also applicable to ﬁnite-horizon, and ning under pure Knightian uncertainty, although longer, would offer
can be easily adapted to address partial observability. a better description.

                                                IJCAI-07
                                                  2024            Drug d1                      Drug d2                 Heart transplant
                                                                                         Cost of actions
      0.8  cardiopathy                  cardiopathy               cardiopathy
                                                 0.8                                 State d1 d2  HT Noop
                                                                                  cardiopathy 30 20 75 −−−
    severe           controlled                                        0.8
                 0.4              severe         controlled severe        controlled
   cardiopathy       cardiopathy                                                  severe
                                cardiopathy     cardiopathy cardiopathy 0.6 cardiopathy cardiopathy 30 25 70 −−−
                                                                                  unrecoverable
            0.6                       0.7                               0.7       cardiopathy 40 30 70 −−−
                                        0.2                                controlled
                                                controlled                        controled
  unrecorverable      controlled                                          cardiopathy cardiopathy −−− −−− −−− 0
   cardiopathy       cardiopathy unrecorverable cardiopathy unrecorverable
                                cardiopathy               cardiopathy     with sequels
              0.7    with sequels               with sequels                      controled
                  0.3                                                             cardiopathy −−− −−− −−− 2
                                                                          0.2     with sequels
                0.2                        0.3   0.4                    0.3
         stroke                       stroke                                      stroke   −−− −−− −−− 85
                  death                      0.6 death         stroke  0.4 death
                                                                                  death    −−− −−− −−− 100

Figure 1: An MDPST representing the Example 1. Dotted lines indicate each one of the reachable sets. Cost of taking actions
d1, d2 and HT in the Example 1. States with “–” indicates the action is not applicable. Action Noop represents the persistence
action for the absorbing states.

3  Markov decision processes with set-valued          Example 1  A hospital offers three experimental treatments
   transitions                                        to cardiac patients: drug d1,drugd2 and heart transplant
                                                      (HT). State s0 indicates patient with cardiopathy. The effects
In this section we develop our promised synthesis of proba- of those procedures lead to other states: severe cardiopathy
bilistic and nondeterministic planning. We focus on the tran- (s1), unrecoverable cardiopathy (s2), cardiopathy with se-
                                       F  s, a ⊆S
sition function; that is, on M5. Instead of taking ( ) , quels (s3), controlled cardiopathy (s4), stroke (s5), and death
                      F  s, a ⊆  S \∅       F  s, a
we now have a set-valued (  )   2   ;thatis,  (   )   (s6). There is little understanding about drugs d1 and d2, and
             s          a ∈As
maps each state and action   ( ) into a set of nonempty considerable data on heart transplants. Consequently, there
subsets of S. We refer to each set k ∈ F(s, a) as a reachable is “partial” nondeterminism (that is, there is Knightian un-
                         s            a
set. A transition from state given action is now asso- certainty) in the way some of the actions operate. Figure 1
ciated with a probability P (k|s, a); note that there is Knight-
                                                     depicts transitions for all actions, indicating also the mass
ian uncertainty concerning P (s |s, a) for each successor state assignments and the costs. For heart transplant, we suppose
s ∈ k
      . We refer to the resulting model as a Markov deci- that all transitions are purely probabilistic.
sion process with set-valued transitions (MDPSTs):transi-
tions move probabilistically to reachable sets, and the prob- 4 MDPSTs, MDPIPs and BMDPs
ability for a particular state is not resolved by the model. In
fact, there is a close connection between probabilities over In this section we comment on the relationship between
F(s, a) and the mass assignments that are associated with the MDPSTs and two existing models in the literature: Markov
theory of capacities of inﬁnite order [Shafer, 1976];toavoid decision processes with imprecise probabilities (MDPIPs)
confusion between P (k|s, a) and P (s|s, a), we refer to the [White III and Eldeib, 1994; Satia and Lave Jr, 1973] and
former as mass assignments and denote them by m(k|s, a). bounded-parameter Markov decision processes (BMDPs)
                                                      [               ]
  Thus an MDPST is given by M1, M2, M3, M4, M6, MDP1,  Givan et al., 2000 .
                                       S                An MDPIP is a Markov decision process where transitions
MDPST1 a state transition function F(s, a) ⊆ 2 \∅ mapping are speciﬁed through sets of probability measures; that is, the
        states s and actions a ∈A(s) into reachable sets of effects of an action are modelled by a credal set K over the
        S
         ,and                                         state space. An MDPIP is given by M1, M2,M3, M4, M6,
MDPST2 mass assignments m(k|s, a) for all s, a ∈A(s), MDP1and
            k ∈ F s, a
        and      (   ).                               MDPIP1 a nonempty credal set Ks(a) for all s ∈Sand
                                                              a ∈As
  There are clearly two varieties of uncertainty in a MDPST:         ( ), representing probability distributions
                                                              P s|s, a                    S
a probabilistic selection of a reachable set and a nondetermin- (    ) over successor states in .
istic choice of a successor state from the reachable set. An- In this paper we assume that a decision maker seeks a min-
other important feature of MDPSTs is that they encompass imax policy (that is, she selects a policy that minimizes the
models discussed in Section 3:                        maximum cost across all possible probability distributions).
                                                      This adopts an implicit assumption that probabilities are se-
  • DET: There is always a single successor state: ∀s ∈ lected in an adversarial manner; other interpretations for MD-
    S,a ∈A(s):|F(s, a)| =1and  ∀s ∈S,a  ∈A(s),k  ∈    PIPs are possible [Troffaes, 2004]. Under the minimax inter-
    F(s, a):|k| =1.                                   pretation, the Bellman principle of optimality is [Satia and
                                                      Lave Jr, 1973]:
  • NONDET: There is always a single reachable set, but                                 X
                                                        ∗                                          ∗  
    selection within this set is left unspeciﬁed (nondeter- V (s)= min max   {C(s, a)+γ    P (·|s, a)V (s )};
                                                             a∈A(s) P (·|s,a)∈Ks(a)
    ministic): ∀s ∈S,a  ∈A(s):|F(s, a)|   =1,and                                       s∈S
    ∃s ∈S,a∈A(s),k   ∈ F(s, a):|k| > 1.                                                                (2)
                                                      moreover, this equation always has a unique solution that
  • MDPs: Selection of k ∈ F(s, a) is probabilistic and it yields the optimal stationary policy for the MDPIP. To inves-
    resolves all uncertainty: ∀s ∈S,a∈A(s):|F(s, a)| > tigate the relationship between MDPSTs and MDPIPs, the
    1,and∀s ∈S,a∈A(s),k    ∈ F(s, a):|k| =1.          following notation is useful: when k ∈ F(s, a), we denote by

                                                IJCAI-07
                                                  2025             MDPST                BMDP                                       MDPIP
                     .1
             .5  .4                      [.1,.5]                            DET MDP
                                   [.5,.9]                          BMDP              MDPST
     (a)   s                (b) s          s                                NON−DET
            0         s1         0          1
                                   [.7,.7]
                                         [.3,.3]
             .7      .3
                                      [0,p]           Figure 3: Relationships between models (BMDPs with pre-
                                          s
            p     s  s                     1
                   1  2               [0,p]           cise rewards).
                                          s
                    s                      2
     (c)  s          3      (d)  s    [0,1]           as an BMDP, and an BMDP that cannot be expressed as an
                                          s3
             1−p                                                                               F  s, a
                    s4               [0,1−p]          MDPST. As a technical aside, we note that the ( ) de-
                                          s
                                           4          ﬁne Choquet capacities of inﬁnite order, while transitions
                                                      in BMDPs deﬁne Choquet capacities of second order [Wal-
Figure 2: This ﬁgure illustrates two examples of planning un- ley, 1991]; clearly they do not have the same representational
der uncertainty modeled through MDPSTs and BMDPs. Ex- power.
ample 1 is the Heart example from Perny et al. (2005). Ex- The results of this section are captured by Figure 3. In
ample 2 is a simple example in which none of the models can the next two sections we present our main results, where we
express the problem modelled by the other one.        explore properties of MDPSTs that make these models rather
                                                      amenable to practical use.
                                               
D k, s, a                      k \             k
 (     ) the set of states such that k∈F(s,a)=k .   5   A simpliﬁed Bellman equation for MDPSTs
Thus D(k, s, a) represents all nondeterministic effects of k
                k                                     We now present a substantial simpliﬁcation of the Bellman
that belong only to .Wenowhave:                       principle for MDPSTs. The intuition behind the following
Proposition 1 Any MDPST p =  S,S0,SG, A, F,C,P0,m
    result is this. In Equation (2), both minima and maxima are
is expressible by an MDPIP q =  S,S0,SG, A,F,C,P0, K
. taken with respect to all combinations of actions and possible
                                                      probability distributions. However, it is possible to “pull” the
Proof (detailed in [Trevizan et al., 2006]) It is enough to
                                                      maximum inside the summation, so that less combinations
prove that ∀s ∈S,a∈As   , F s, a (MDPST1) and m s, a
                     ( )  (   )               (   )   need be considered.
(MDPST2) imply Ks(a) (MDPIP1). First, note that MDPST2
bounds for all s ∈Sthe probability of being in state s after Theorem 2 For any MDPST and its associated MDPIP, (2)
                                                      is equivalent to:
applying action a in state s as follows                                        X
                                                        ∗                                       ∗ 
                                                     V  (s)= min  {C(s, a)+γ     m(k|s, a)maxV  (s )} (5)
  m  {s }|s, a ≤ P s |s, a ≤     m  k|s, a ≤ .                 a∈A(s)                      s∈k
    (       )    (     )           (     )  1   (3)                           k∈F(s,a)
                        k∈F(s,a)∧s∈k
                                                                    ∗         ∗
(To see that, use the deﬁnition of reachable sets: let k ∈ Proof Deﬁne VIP (s) and VST (s) as a shorthand for the val-
F(s, a);ifs ∈ k, then it is not possible to select s as a ues obtained through, respectively, (2) and (5). We want to
nondeterministic effect of a.)                        prove that for all MDPST p =  S,S0,SG, A, F,C,P0,m
,
  From MDPST1andMDPST2 it is possible to bound the sum its associated MDPIP q =  S,S0,SG, A,F,C,P0, K
,and
                                                               ∗       ∗
of the probabilities of each state in a reachable set k ∈ F(s, a) ∀s ∈S, VIP (s) = VST (s). Due to the Proposition 1, we have
                                                                                                  ∗ 
and in the associated set D(k, s, a):                 that the probability measure induced by maxs∈k V (s ) for
                                                    k ∈ F(s, a) in (5) is a valid choice according to Ks(a),there-
   ≤     P  s|s, a ≤ m k|s, a ≤  P  s|s, a ≤                     ∗         ∗
 0         (     )    (     )       (     )   1 (4)   fore ∀s ∈S,VST (s) ≥ VIP (s). Now, it is enough to show
                              s∈k                               ∗        ∗
   s ∈D(k,s,a)                                        that ∀s ∈S,VST (s) ≤ VIP (s) to conclude this proof.
                                                        For all sˆ ∈S, we denote by Fsˆ(s, a) the set of reach-
The set of inequalities (3) and (4) for s ∈Sand a ∈A(s)                                      ∗   
                                                      able sets {k ∈ F(s, a)| sˆ =argmaxs∈k VST (s )}.The
describe a possible credal set Ks(a) for MDPIP1.                      ∗         ∗
                                                      proof of ∀s ∈S,VST  (s) ≤  VIP (s) proceeds by contra-
Deﬁnition 1 The MDPIP q obtained through Proposition 1 diction as follows. For all s ∈Sand all a ∈A(s),let
is called the associated MDPIP of p.                  P (·|s, a) be the probability measure chosen by the operator
                                                             V ∗ s                V ∗  s >V∗    s
  As noted in Section 1, BMDPs are related to MDPSTs. In- max in IP ( ) and suppose that ST( ) IP ( ).There-
                                                                    s ∈S                      m k|s, a >
tuitively, BMDPs are Markov decision processes where tran- fore, there is a such that k∈Fs(s,a) (    )
sition probabilities and rewards are speciﬁed by intervals [Gi- P (s|s, a);asP (·|s, a) is a probability measure, there is also a
             ]                                        s ∈S               m  k|s, a <P  s|s, a   V ∗  s >
van et al., 2000 . Thus BMDPs are not comparable to MD-     s.t. k∈Fs(s,a) (     )    (    ) and  IP ( )
PIPs due to possible imprecision in rewards; here we only V ∗ (s).Now,letP(·|s, a) be a probability measure deﬁned
consider those BMDPs that have real-valued rewards. Clearly IP
                                                      by: P(s|s, a)=P (s|s, a) ∀s ∈S\{s, s}, P(s|s, a)=
these BMDPs form a strict subset of MDPIPs. The relation- P s|s, a  P  s|s, a  P s|s, a −    >
ship between such BMDPs and MDPSTs is more complex.     (   )+   and  (    )=(       )   ,for    0.Note
                                                                P  s|s, a V ∗ >      P  s|s, a V ∗
Figure 2.a and 2.b presents an MDPST and an BMDP that that  s∈S  (     ) IP      s∈S  (     ) IP , a con-
are equivalent (that is, they represent the same MDPIP). Fig- tradiction by the deﬁnition of P (·|s, a). Thus, the rest of this
ure 2.c and 2.d presents an MDPST that cannot be expressed proof shows that P (s|s, a) satisﬁes Proposition 1.

                                                IJCAI-07
                                                  2026  Due to the deﬁnition of P(·|s, a),wehavethattheleftside caseofanMDPmodelledasanMDPST,i.e.   ∀s ∈S,a  ∈
and the right side of (3) are trivially satisﬁed, respectively, by A(s), |F(s, a)|≤|S|and ∀k ∈ F(s, a), |k| =1, this worst
P (s|s, a) and P (s|s, a). To treat the other case for both s and case complexity is O(|S|2|A|), the same for one round using
s, it is sufﬁcient to deﬁne  as follows:             the Bellman principle for MDPs [Papadimitriou, 1994].
        X                             X
 =min{    m(k|s, a)−P (s|s, a),P(s|s, a)− m(k|s, a)}.
                                                      6   Algorithms for MDPSTs
     k∈Fs(s,a)                      k∈Fs(s,a)
                                                      Due to Proposition 1, every algorithm that ﬁnds the optimal
 Using this deﬁnition, we have that >0 by hypothesis; policy for MDPIPs can be directly applied to MDPSTs. In-
since (4) gives a lower and an upper bound to the sum of stances of algorithms for MDPIP are: value iteration, pol-
P (·|s, a) over, respectively, k ∈ F(s, a) and D(k, s, a) ⊆ k. icy iteration [Satia and Lave Jr, 1973], modiﬁed policy iter-
If {s, s}∈D(k, s, a) or {s, s} ∈D(k, s, a), then noth- ation [White III and Eldeib, 1994], and the algorithm to ﬁnd
ing changes and these bounds remain valid. There is one all optimal policies presented in [Harmanec, 1999].How-
more case for each bound that its satisfaction is trivial too: ever, a better approach is to use Theorem 2. This proposi-
(i) for the upper bound when s ∈D(k, s, a); and (ii) for tion gives a clear path on how to adapt algorithms from the
the lower bound when s ∈ k.  A nontrivial case is for realm of MDPs — algorithms such as (L)RTDP [Bonet and
the lower bound in (4) when s ∈ k and s ∈ k.This     Geffner, 2003] and LDFS [Bonet and Geffner, 2006].These
bound still holds because m({s}|s, a) ≤ P (s|s, a) (by hy- algorithms are deﬁned for Stochastic Shortest Path problems
                  P  s|s, a         m  {s}|s, a
pothesis) and s∈k (     )=     s∈k  (       )+     (SSPs) [Bertsekas, 1995] (SSPs are a special case of MDPs,
                      
δ   ≤            m {s }|s, a    δ    P s|s, a   ≤    in which there is only one initial state (M2) and the set of
         s ∈k\{s} (       )+      +    (    )
                                                     goal states is nonempty (M3)). To ﬁnd an optimal policy, an
  s∈k\{s} P (s |s, a)+P (s|s, a),δ≥ 0 (using Proposition
    P  ·|s, a                                         additional assumption is required: the goal must be reachable
1for  (    )).                                        from every state with nonzero probability (the reachability as-
  The last remaining case (to prove that Equation (4) is sumption). For MDPSTs, this assumption can be generalized
       P  ·|s, a             s ∈Dk, s, a       s ∈
true for (    )) happens when      (     ) and        by requirement that the goal be reachable from every state
D k, s, a
 (     ) for the upper bound in (4). This case is valid with nonzero probability for all probability measures in the
because there is no k ∈ F(s, a) such that k = k and
                                                     model. The following proposition gives a sufﬁcient, however
s∈ k by the deﬁnition of reachable set, thus P(s|s, a) ≤ not necessary, condition to prove the reachability assumption
          m  k|s, a   m k|s, a
  k∈Fs(s,a) (    )=    (     ) by hypothesis. If there for MDPSTs.
is not a s ∈  k \{s}  s.t. P (s|s, a) > 0 this up-
                                                     Proposition 3 If, for all s ∈S, there exists a ∈A(s) such
per bound still holds, else, choosing s as s will validate      k ∈ F s, a      s ∈ k P s|s, a >
                      P ·|s, a                        that, for all  (   ) and all   ,  (     )   0,thenit
all the bounds. Since  (    ) respects Proposition 1, is sufﬁcient to prove that the reachability assumption is valid
                                  P  s|s, a V s >
we get a contradiction because s∈S (    ) (  )      using at least one probability measure for each s ∈Sand
      P s|s, a V s                   P ·|s, a
  s∈S  (     ) ( ) but by  hypothesis  (   )=       a ∈A.
                        P  s|s, a V ∗ s
argmaxP (·|s,a)∈Ks(a) s∈S (    )  ( ).   Therefore,
∀s ∈S,V∗   s  ≤ V ∗ s                               Proof If the reachability assumption is true for a speciﬁc se-
        ST ( )   IP ( ), what completes the proof.    quence of probability measures P =  P 1,P2,....Pn
,then
  An immediate consequence of Theorem 2 is a decrease there exists a policy π and a history h, i.e. sequence of vis-
of the worst case complexity order of MDPIPs algorithms ited states and executed actions, induced by π and P such
                                                          h    s0 ∈ S ,π s0 ,s1,...,π sn−1 ,sn ∈ S 

used for solving MDPSTs. Consider ﬁrst one iteration of the that =  0  (  )        (    )      G  is min-
                                  s ∈S                                     0   i≤n  i i  i−1   i−1
Bellman principle of optimality for each (one round)  imum and ∀s ∈S,P0(s   )  i=1 P (s |s ,π(s   )) > 0.
using Equations (2). Deﬁne an upper bound of |F(s, a)| Since si+1 can always be reached, because there exists an ac-
for all s ∈Sand  a ∈A(s)   of an MDPST instance by    tion a ∈A(si) such that P (si+1|si,a) > 0, then, for any
                                   |S|
F =maxs∈S  {maxa∈A(s) |F(s, a)|} ≤ 2 .IntheMDPIP      sequence of probability measures in the model, every history
                                          ∗                                             n
obtained through Proposition 1, computation of V (s) con- h induced by π contains h, i.e., reaches s ∈ SG. 
sists of solving a linear program induced by the max operator
                                    |S|               Example 2  Consider the planning problem in the Example
on (2). Because this linear program has variables and 1 and the cost of actions in Figure 1. We have obtained the
its description is proportional to F, the worst case complex-
                             q                        following optimal policy for this MDPST:
ity of one round is O(|A||S|p+1F ),forp ≥ 2 and q ≥ 1.
           p    q
The value of and is related to the algorithm used to solve     s0  s1   s2    s3    s4     s5    s6
                                                         π∗ =
this linear program (for instance, using the interior point al- d1 d2  HT   Noop   Noop  Noop   Noop
gorithm [Kojima et al., 1988] leads to p =6and q =1,and
the Karmarkar’s algorithm [Karmarkar, 1984] leads to p to
3.5 and q to 3).
  However, the worst case complexity for one round using 7Conclusion
Equation (5) is O(|S|2|A|F). This is true because the prob- In this paper we have examined approaches to planning across
ability measure that maximizes the right side of Equation many dimensions: determinism, nondeterminism, risk, un-
                                      
(2) is represented by the choice maxs∈k V (s ) in Equation certainty. We would like to suggest that Markov decision
(5), avoiding the cost of a linear program. In the special processes with set-valued transitions represent a remarkable

                                                IJCAI-07
                                                  2027