                               Challenges in Web Search Engines 

        Monika R. Henzinger Rajeev Motwani* Craig Silverstein 
             Google Inc. Department of Computer Science Google Inc. 
       2400 Bayshore Parkway Stanford University 2400 Bayshore Parkway 
      Mountain View, CA 94043 Stanford, CA 94305 Mountain View, CA 94043 
        monika@google.com rajeev@cs.stanford.edu csilvers@google.com 

                     Abstract                          or a combination thereof. There are web ranking optimiza•
                                                       tion services which, for a fee, claim to place a given web site 
    This article presents a high-level discussion of   highly on a given search engine. 
    some problems that are unique to web search en•      Unfortunately, spamming has become so prevalent that ev•
    gines. The goal is to raise awareness and stimulate ery commercial search engine has had to take measures to 
    research in these areas.                           identify and remove spam. Without such measures, the qual•
                                                       ity of the rankings suffers severely. 
                                                         Traditional research in information retrieval has not had to 
1 Introduction                                         deal with this problem of "malicious" content in the corpora. 
                                                       Quite certainly, this problem is not present in the benchmark 
Web search engines are faced with a number of difficult prob• document collections used by researchers in the past; indeed, 
lems in maintaining or enhancing the quality of their perfor• those collections consist exclusively of high-quality content 
mance. These problems are either unique to this domain, or such as newspaper or scientific articles. Similarly, the spam 
novel variants of problems that have been studied in the liter• problem is not present in the context of intranets, the web that 
ature. Our goal in writing this article is to raise awareness of exists within a corporation. 
several problems that we believe could benefit from increased One approach to deal with the spam problem is to construct 
study by the research community. We deliberately ignore in• a spam classifier that tries to label pages as spam or not-spam. 
teresting and difficult problems that are already the subject This is a challenging problem, which to the best of our knowl•
of active research. An earlier version of this paper appeared edge has not been addressed to date. 
in [Henzinger et al, 2002]. 
  We begin with a high-level description of the problems that 
                                                       Content Quality. Even if the spam problem did not exist, 
we describe in further detail in the subsequent sections. 
                                                       there are many troubling issues concerned with the quality of 
                                                       the content on the web. The web is full of noisy, low-quality, 
Spam. Users of web search engines tend to examine only unreliable, and indeed contradictory content. A reasonable 
the first page of search results. Silverstein et al. [Silverstein approach for relatively high-quality content would be to as•
et al., 1999] showed that for 85% of the queries only the first sume that every document in a collection is authoritative and 
result screen is requested. Thus, inclusion in the first result accurate, design techniques for this context, and then tweak 
screen, which usually shows the top 10 results, can lead to the techniques to incorporate the possibility of low-quality 
an increase in traffic to a web site, while exclusion means content. However, the democratic nature of content creation 
that only a small fraction of the users will actually see a link on the web leads to a corpus that is fundamentally noisy and 
to the web site. For commercially-oriented web sites, whose of poor quality, and useful information emerges only in a sta•
income depends on their traffic, it is in their interest to be tistical sense. In designing a high-quality search engine, one 
ranked within the top 10 results for a query relevant to the has to start with the assumption that a typical document can•
content of the web site.                               not be "trusted" in isolation; rather, it is the synthesis of a 
                                                       large number of low-quality documents that provides the best 
  To achieve this goal, some web authors try to deliberately 
                                                       set of results. 
manipulate their placement in the ranking order of various 
search engines. The result of this process is commonly called As a first step in the direction outlined above, it would be 
search engine spam. In this paper we will simply refer to it extremely helpful for web search engines to be able to iden•
as spam. To achieve high rankings, authors either use a text- tify the quality of web pages independent of a given user re•
based approach, a link-based approach, a cloaking approach, quest. There have been link-based approaches, for instance 
                                                       PageRank [Brin and Page, 1998], for estimating the quality 
   *Part of this work was done while the author was visiting Google of web pages. However, PageRank only uses the link struc•
Inc. Work also supported in part by NSF Grant IIS-0118173, and ture of the web to estimate page quality. It seems to us that 
research grants from the Okawa Foundation and Veritas. a better estimate of the quality of a page requires additional 


INVITED SPEAKERS                                                                                      1573  sources of information, both within a page (e.g., the reading-   Web pages in HTML fall into the middle of this contin•
 level of a page) and across different pages (e.g., correlation uum of structure in documents, being neither close to free 
 of content).                                                  text nor to well-structured data. Instead HTML markup pro•
                                                               vides limited structural information, typically used to con•
                                                               trol layout but providing clues about semantic information. 
 Quality Evaluation. Evaluating the quality of different 
                                                               Layout information in HTML may seem of limited utility, 
 ranking algorithms is a notoriously difficult problem. Com•
                                                               especially compared to information contained in languages 
 mercial search engines have the benefit of large amounts 
                                                               like XML that can be used to tag content, but in fact it is a 
 of user-behavior data they can use to help evaluate rank•
                                                               particularly valuable source of meta-data in unreliable cor•
 ing. Users usually will not make the effort to give explicit 
                                                               pora such as the web. The value in layout information stems 
 feedback but nonetheless leave implicit feedback information 
                                                               from the fact that it is visible to the user: Most meta-data 
 such as the results on which they clicked. The research issue 
                                                               which is not user-visible and therefore is particularly sus•
 is to exploit the implicit feedback to evaluate different rank•
                                                               ceptible to spam techniques, but layout information is more 
 ing strategies. 
                                                               difficult to use for spam without affecting the user experi•
                                                               ence. There has only been some initial, partly related work 
 Web Conventions. Most creators of web pages seem to fol•      in this vein [Nestorov et al, 1998; Chakrabarti et al, 2001; 
 low simple "rules" without anybody imposing these rules on    Chakrabarti, 2001]. We believe that the exploitation of layout 
 them. For example, they use the anchor text of a link to pro• information can lead to direct and dramatic improvement in 
 vide a succinct description of the target page. Since most    web search results. 
 authors behave this way, we will refer to these rules as web 
 conventions, even though there has been no formalization or   2 Spam 
 standardization of such rules. 
   Search engines rely on these web conventions to improve     Some web authors try to deliberately manipulate their place•
 the quality of their results. Consequently, when webmasters   ment in the rankings of various search engine. The result•
 violate these conventions they can confuse search engines.    ing pages are called spam. Traditional information retrieval 
 The main issue here is to identify the various conventions that collections did not contain spam. As a result, there has not 
 have evolved organically and to develop techniques for accu•  been much research into making search algorithms resistant 
 rately determining when the conventions are being violated.   to spam techniques. Web search engines, on the other hand, 
                                                               have been consistently developing and improving techniques 
                                                               for detecting and fighting spam. As search engine techniques 
 Duplicate Hosts. Web search engines try to avoid crawling     have developed, new spam techniques have developed in re•
 and indexing duplicate and near-duplicate pages, as they do   sponse. Search engines do not publish their anti-spam tech•
 not add new information to the search results and clutter up  niques to avoid helping spammers to circumvent them. 
 the results. The problem of identifying duplicates within a set Historical trends indicate that the use and variety of spam 
 of crawled pages is well studied. However, if a search engine will continue to increase. There are challenging research is•
 can avoid crawling the duplicate content in the first place, the sues involved in both detecting spam and in developing rank•
 gain is even larger. In general, predicting whether a page will ing algorithms that are resistant to spam. Current spam falls 
 end up being a duplicate of an already-crawled page is chancy into following three broad categories: text spam, link spam, 
 work, but the problem becomes more tractable if we limit it to and cloaking. A spammer might use one or some combina•
 finding duplicate hosts, that is, two hostnames that serve the tion of them. 
 same content. One of the ways that duplicate hosts can arise 
 is via an artifact of the domain name system (DNS) where two  2,1 Text Spam 
 hostnames can resolve to the same physical machine. There 
                                                               All search engines evaluate the content of a document to de•
 has only been some preliminary work on the duplicate hosts 
                                                               termine its ranking for a search query. Text spam techniques 
problem [Bharat et al., 2000]. 
                                                               are used to modify the text in such a way that the search en•
                                                               gine rates the page as being particularly relevant, even though 
Vaguely-Structured Data. The degree of structure present       the modifications do not increase perceived relevance to a hu•
 in data has had a strong influence on techniques used for     man reader of a document. 
search and retrieval. At one extreme, the database commu•        There are two ways to try to improve ranking. One is to 
 nity has focused on highly-structured, relational data, while at concentrate on a small set of keywords and try to improve 
the other the information retrieval community has been more    perceived relevance for that set of keywords. For instance, 
concerned with essentially unstructured text documents. Of     the document author might repeat those keywords often at the 
late, there has been some movement toward the middle with      bottom of the document, which it is hoped will not disturb 
the database literature considering the imposition of structure the user. Sometimes the text is presented in small type, or 
over almost-structured data. In a similar vein, document man•  even rendered invisible (e.g., by being written in the page's 
agement systems use accumulated meta-information to intro•     background color) to accomplish this. 
duce more structure. The emergence of XML has led to a           Another technique is to try to increase the number of key•
flurry of research involving extraction, imposition, or mainte• words for which the document is perceived relevant by a 
nance of partially-structured data.                            search engine. A naive approach is to include (some subset 


1574                                                                                                 INVITED SPEAKERS of) a dictionary at the bottom of the web page, to increase the the benefits of link and text spam without inconveniencing 
chances that the page is returned for obscure queries. A less  human readers of the web page. 
naive approach is to add text on a different topic to the page 
to make it appear that this is the main topic of the page. For 2.4 Defending against Spam 
example, porn sites sometimes add the names of famous per•     In general, text spam is defended against in a heuristic fash•
sonalities to their pages in order to make these pages appear  ion. For instance, it was once common for sites to ''hide" text 
when a user searches for such personalities.                   by writing it in white text on a white background, ensuring 
                                                               that human readers were not affected while search engines 
2.2 Link Spam                                                  were misled about the content. As a result, search engine 
The advent of link analysis by search engines has been ac•     companies detected such text and ignored it. Such reactive 
companied by an effort by spammers to manipulate link anal•    approaches are, obviously, not optimal. Can pro-active ap•
ysis systems. A common approach is for an author to put a      proaches succeed? Perhaps these approaches could be com•
link farm at the bottom of every page in a site, where a link  bined; it might be possible for the search engine to notice 
farm is a collection of links that points to every other page in what pages change in response to the launch of a new anti-
that site, or indeed to any site the author controls. The goal is spam heuristic, and to consider those pages as potential spam 
to manipulate systems that use raw counts of incoming links    pages. 
to determine a web page's importance. Since a completely-        Typically, link-spam sites have certain patterns of links that 
linked link farm is easy to spot, more sophisticated techniques are easy to detect, but these patterns can mutate in much the 
like pseudo web-rings and random linkage within a member       same way as link spam detection techniques. A less heuristic 
group are now being used.                                      approach to discovering link spam is required. One possi•
   A problem with link farms is that they distract the reader  bility is, as in the case of text spam, to use a more global 
because they are on pages that also have legitimate content.   analysis of the web instead of merely local page-level or site-
A more sophisticated form of link farms has been developed,    level analysis. For example, a cluster of sites that suddenly 
called doorway pages. Doorway pages are web pages that         sprout thousands of new and interlinked webpages is a can•
consist entirely of links. They are not intended to be viewed  didate link-spam site. The work by [Kumar et al, 1999] on 
by humans; rather, they are constructed in a way that makes it finding small bipartite clusters in the web is a first step in this 
very likely that search engines will discover them. Doorway    direction. 
pages often have thousands of links, often including multiple    Cloaking can only be discovered by crawling a website 
links to the same page. (There is no text-spam equivalent      twice, once using an HTTP client the cloaker believes is a 
of doorway pages because text, unlike links, is analyzed by    search engine, and once from a client the cloaker believes is 
search engines on a per-page basis.)                           not a search engine. Even this is not good enough, since web 
   Both link farms and doorway pages are most effective        pages typically differ between downloads for legitimate rea•
when the link analysis is sensitive to the absolute num•       sons, such as changing news headlines. 
ber of links. Techniques that concentrate instead on the         An interesting challenge is to build a spam classifier that 
quality of links, such as PageRank [Brin and Page, 1998;       reliably detects a large fraction of the currently existing spam 
Brin et al, 1998], are not particularly vulnerable to these    categories. 
techniques. 
                                                               3 Content Quality 
2.3 Cloaking                                                   While spams are attempts to deliberately mislead search en•
Cloaking involves serving entirely different content to a      gines, the web is replete with text that — intentionally or 

search engine crawler than to other users.1 As a result, the   not — misleads its human readers as well. As an example, 
search engine is deceived as to the content of the page and    there is a webpage which claims (falsely!) that Thomas Jef•
scores the page in ways that, to a human observer, seem rather ferson was the first president of the United States. Many web•
arbitrary.                                                     sites, purposefully or not, contain misleading medical infor•
   Sometimes cloaking is used with the intent to "help" search mation.2 Other sites contain information that was once cor•
engines, for instance by giving them an easily digestible, text- rect but is now out of date; for example, sites giving names of 
only version of a page that is otherwise heavy with multi•     elected officials. 
media content, or to provide link-based access to a database     While there has been a great deal of research on determin•
which is normally only accessible via forms (which search      ing the relevance of documents, the issue of document quality 
engines cannot yet navigate). Typically, however, cloaking is  or accuracy has not been received much attention, whether in 
used to deceive search engines, allowing the author to achieve web search or other forms of information retrieval. For in•
                                                               stance, the TREC conference explicitly states rules for when 

   1A search engine crawler is a program that downloads web pages it considers a document to be relevant, but does not men•
for the purpose of including them in the search engine results. Typ• tion the accuracy or reliability of the document at all. This is 
ically a search engine will download a number of pages using the understandable, since typical research corpora, including the 
crawler, then process the pages to create the data structures used to 
service search requests. These two steps are repeated continuously 2 One study showed many reputable medical sites contain contra•
to ensure the search engine is searching over the most up-to-date dictory information on different pages of their site [Berland et ai, 
content possible.                                              2001] — a particularly difficult content-quality problem! 


INVITED SPEAKERS                                                                                                    1575  ones used by TREC and found in corporate intranets, consist      One approach is to simply collect the click-through data 
 of document sources that are deemed both reliable and au•     from a subset of the users — or all users — for two ranking 
 thoritative. The web, of course, is not such a corpus, so tech• algorithm. The experimenter can then compute metrics such 
 niques forjudging document quality is essential for generat•  as the percentage of clicks on the top 5 results and the number 
 ing good search results. Perhaps the one successful approach  of clicks per search. 
 to (heuristically) approximating quality on the web is based    Recently, Joachims [2002] suggested another experimen•
 on link analysis, for instance PageRank [Brin and Page, 1998; tal technique which involves merging the results of the two 
 Brin et al., 1998] and HITS [Kleinberg, 1998]. These tech•    ranking algorithms into a single result set. In this way each 
 niques are a good start and work well in practice, but there is user performs a comparison of the two algorithms. Joachims 
 still ample room for improvement.                             proposes to use the number of clicks as quality metric and 
   One interesting aspect of the problem of document quality   shows that, under some weak assumptions, the clickthrough 
is specific to hypertext corpora such as the web: evaluating   for ranking A is higher than the clickthrough for B if and only 
the quality of anchor text. Anchor text is the text, typically if A retrieves more relevant links than B. 
displayed underlined and in blue by the web browser, that 
 is used to annotate a hypertext link. Typically, web-based    5 Web Conventions 
search engines benefit from including anchor-text analysis in 
their scoring function [Craswell et al, 2001]. However, there  As the web has grown and developed, there has been an 
has been little research into the perils of anchor-text analy• evolution of conventions for authoring web pages. Search 
sis e.g. due to spam and on methodologies for avoiding the     engines assume adherence to these conventions to improve 
pitfalls.                                                      search results. In particular, there are three conventions that 
                                                               are assumed relating to anchor text, hyperlinks, and META 
   For instance, for what kinds of low-quality pages might the 
                                                               tags. 
anchor text still be of high quality? Is it possible to judge the 
quality of anchor text independently of the quality of the rest  • As discussed in Section 3, the fact that anchor text is 
of the page? Is it possible to detect anchor text that is in•       meant to be descriptive is a web convention, and this can 
tended to be editorial rather than purely descriptive? In addi•     be exploited in the scoring function of a search engine. 
tion, many fundamental issues remain open in the application 
                                                                 • Search engines typically assume that if a web page au•
of anchor text to determination of document quality and con•
                                                                    thor includes a link to another page, it is because the 
tent. In case of documents with multiple topics, can anchor 
                                                                    author believes that readers of the source page will find 
text analysis be used to identify the themes? 
                                                                    the destination page interesting and relevant. Because 
   Another promising area of research is to combine estab•
                                                                    of the way people usually construct web pages, this as•
lished link-analysis quality judgments with text-based judg•
                                                                    sumption is usually valid. However, there are promi•
ments. A text-based analysis, for instance, could judge the 
                                                                    nent exceptions: for instance, link exchange programs, 
quality of the Thomas Jefferson page by noting that most ref•
                                                                    in which web page authors agree to reciprocally link in 
erences to the first president of the United States in the web 
                                                                    order to improve their connectivity and rankings, and ad•
corpus attribute the role to George Washington. 
                                                                    vertisement links. Humans are adept at distinguishing 
                                                                    links included primarily for commercial purposes from 
4 Quality Evaluation                                                those included primarily for editorial purposes. Search 
                                                                    engines are less so. 
Search engines cannot easily improve their ranking algo•
rithms without running tests to compare the quality of the          To further complicate matters, the utility of a link is not 
new ranking technique with the old. Performing such com•            a binary function. For instance, many pages have links 
parisons with human evaluators is quite work-intensive and          allowing you to download the latest version of Adobe's 
runs the danger of not correctly reflecting user needs. Thus, it    Acrobat Reader. For visitors that do not have Acrobat 
would be best to have end users perform the evaluation task,        Reader, this link is indeed useful, certainly more useful 
as they know their own needs the best.                              than for those those who have already downloaded the 
                                                                    program. Similarly, most sites have a terms of service 
   Users, typically, are very reluctant to give direct feedback. 
                                                                    link at the bottom of every page. When the user first 
However, web search engines can collect implicit user feed-
                                                                    enters the site, this link might well be very useful, but as 
back using log data such as the position of clicks for a search 
                                                                    the user browses other webpages on the site, the link's 
and the time spent on each click. This data is still incom•
                                                                    usefulness immediately decreases. 
plete. For instance, once the user clicks on a search result, 
the search engine does not know which pages the user visits      • A third web convention concerns the use of META tags. 
until the user returns to the search engine. Also, it is hard to   These tags are currently the primary way to include 
tell whether a user clicking on a page actually ends up finding    metadata within HTML. In theory META tags can in•
that page relevant or useful.                                       clude arbitrary content, but conventions have arisen for 
  Given the incomplete nature of the information, the exper•        meaningful content. A META tag of particular impor•
imental setup used to college implicit user data becomes par•       tance to search engines is the so-called Content META 
ticularly important. That is: How should click-through and          tag, which web page authors use to describe the content 
other data be collected? What metrics should be computed           of the document. Convention dictates that the content 
from the data?                                                     META tag contains either a short textual summary of the 


1576                                                                                                 INVITED SPEAKERS      page or a brief list of keywords pertaining to the content  A host is merely a name in the domain name system 
     of the page.                                              (DNS), and duphosts arise from the fact that two DNS 
     Abuse of this META tags is common, but even when          names can resolve to the same IP address.3 Companies typi•
     there is no attempt to deceive, there are those who break cally reserve more than one name in DNS, both to increase 
     the convention, either out of ignorance or overzealous-   visibility and to protect against domain name "squatters." 
     ness. For instance, a webpage author might include a      For instance, currently both bikesport. com and bike-
     summary of their entire site within the META tag, rather  sportworld. com resolve to the same IP address, and as 
     than just the individual page. Or, the author might in•   a result the sites http://www.bikesport.com/ and 
     clude keywords that are more general than the page war•   http://www.bikesportworld.com/ display identi•
     rants, using a META description of "cars for sale" on a   cal content. 
     web page that only sells a particular model of car.         Unfortunately, duplicate IP addresses are neither necessary 
     In general, the correctness of META tags is difficult for nor sufficient to identify duplicate hosts. Virtual hosting can 
     search engines to analyze because they are not visible    result in different sites sharing an IP address, while round-
     to users and thus are not constrained to being useful     robin DNS can result in a single site having multiple IP ad•
     to visitors. However, there are many web page authors     dresses. 
     that use META tags correctly. Thus, if web search en•
                                                                 Merely looking at the content of a small part of the site, 
     gines could correctly judge the usefulness of the text in a 
                                                               such as the homepage, is equally ineffective. Even if two 
     given META tag, the search results could potentially be 
                                                               domain names resolve to the same website, their homepages 
     improved significantly. The same applies to other con•
                                                               could be different on the two viewings, if for instance the 
     tent not normally displayed, such as ALT text associated 
                                                               page includes an advertisement or other dynamic content. On 
     with the IMAGE tag. 
                                                               the other hand, there are many unrelated sites on the web that 
   While link analysis has become increasingly important as    have an identical "under construction" home page. 
a technique for web-based information retrieval, there has not 
                                                                 While there has been some work on the duphosts prob•
been as much research into the different types of links on the 
                                                               lem [Bharat et al.  2000], it is by no means a solved prob•
web. Such research might try to distinguish commercial from                     t
                                                               lem. One difficulty is that the solution needs to be much less 
editorial links, or links that relate to meta-information about 
                                                               expensive than the brute-force approach that compares every 
the site ("This site best viewed with [start link]browser X[cnd 
                                                               pair of hosts. For instance, one approach might be to down•
link]") from links that relate to the actual content of the site. 
                                                               load every page on two hosts, and then look for a graph iso•
   To some extent, existing research on link analysis is help•
                                                               morphism. However, this defeats the purpose of the project, 
ful, since authors of highly visible web pages are less likely 
                                                               which is to not have to download pages from both of two sites 
to contravene established web conventions. But clearly this 
                                                               that are duphosts. 
is not sufficient. For instance, highly visible pages are more, 
rather than less, likely to include advertisements than the av•  Furthermore, web crawls are never complete, so any link-
erage page.                                                    structure approach would have to be robust against missing 
   Understanding the nature of links is valuable not only for  pages. Specifically, a transient network problem problem, or 
itself, but also because it enables a more sophisticated treat• server downtime, may keep the crawler from crawling a page 
ment of the associated anchor text. A potential approach       in one host of a duphost pair, but not the other. Likewise, 
would be to use text analysis of anchor text, perhaps com•     due to the increasing amount of dynamic content on the web, 
bined with meta-information such as the URL of the link, in    text-based approaches cannot check for exact duplicates. 
conjunction with information obtained from the web graph.        On the other hand, the duphosts problem is simpler than 
                                                               the more general problem of detecting mirrors. Duphosts al•
6 Duplicate Hosts                                              gorithms can take advantage of the fact that the urls between 
Web search engines try to avoid crawling and indexing du•      duphosts are very similar, differing only in the hostname com•
plicate and near-duplicate pages, since such pages increase    ponent. Furthermore, they need not worry about content re•
the time to crawl and do not contribute new information to     formatting, which is a common problem with mirror sites. 
the search results. The problem of finding duplicate or near-    Finally — and this is not a trivial matter — duphost 
duplicate pages in a set of crawled pages is well studied [Brin analysis can benefit from semantic knowledge of DNS. 
et al., 1995; Broder, 1997], There has also been some re•      For instance, candidate duphosts http://foo.com and 
search on identifying duplicate or near-duplicate directory    http://foo.co.uk are, all other things being equal, 
trees [Cho et al., 2000], called mirrors.                      likely to be duphosts, while candidates http: //foo. com 
  While mirror detection and individual-page detection try     and ht tp: / /bar. com are not as likely to be duphosts. 
to provide a complete solution to the problem of duplicate 
pages, a simpler variant can reap most of the benefits while re•
quiring less computational resources. This simpler problem is 
                                                                  3In fact, it's not necessary that they resolve to the same IP ad•
called duplicate host detection. Duplicate hosts ("duphosts")  dress to be duphosts, just that they resolve to the same webserver. 
are the single largest source of duplicate pages on the web,   Technically even that is not necessary; the minimum requirement is 
so solving the duplicate hosts problem can result in a signifi• that they resolve to computers that serve the same content for the 
cantly improved web crawler.                                   two hostnames in question. 


INVITED SPEAKERS                                                                                                     1577 