        Building Portable Options: Skill Transfer in Reinforcement Learning

                                George Konidaris    and  Andrew Barto
                  Autonomous Learning Laboratory, Department of Computer Science
                                University of Massachusetts at Amherst
                                      {gdk, barto}@cs.umass.edu

                    Abstract                          2   Background
    The options framework provides methods for rein-  2.1  The Options Framework
    forcement learning agents to build new high-level
    skills. However, since options are usually learned An option o consists of three components:
    in the same state space as the problem the agent
    is solving, they cannot be used in other tasks that              πo :(s, a)   → [0, 1]
    are similar but have different state spaces. We in-              Io :  s      →{0, 1}
    troduce the notion of learning options in agent-                 βo :  s      → [0, 1],
    space, the space generated by a feature set that is
                                                      where πo is the option policy (a probability distribution over
    present and retains the same semantics across suc-
                                                      actions at each state in which the option is deﬁned), Io is the
    cessive problem instances, rather than in problem- initiation set indicator function, which is 1 for states where
    space. Agent-space options can be reused in later
                                                      the option can be executed and 0 elsewhere, and βo is the
    tasks that share the same agent-space but have dif- termination condition, giving the probability of the option
    ferent problem-spaces. We present experimental    terminating in each state [Sutton et al., 1999]. The options
    results demonstrating the use of agent-space op-  framework provides methods for learning and planning using
    tions in building transferrable skills, and show that options as temporally extended actions in the standard rein-
    they perform best when used in conjunction with   forcement learning framework [Sutton & Barto, 1998].
    problem-space options.
                                                        Algorithms for learning new options must include a
                                                      method for determining when to create an option or expand
1  Introduction                                       its initiation set, how to deﬁne its termination condition, and
Much recent research in reinforcement learning has focused how to learn its policy. Policy learning is usually performed
on hierarchical methods [Barto & Mahadevan, 2003] and in by an off-policy reinforcement learning algorithm so that the
particular the options framework [Sutton et al., 1999],which agent can update many option simultaneously after taking an
integrates macro-action learning into the reinforcement learn- action [Sutton et al., 1998].
ing framework. Most option learning methods work within Creation and termination are usually performed by the
the same state space as the problem the agent is solving at the identiﬁcation of goal states, with an option created to reach a
time. Although this can lead to faster learning on later tasks goal state and terminate when it does so. The initiation set is
in the same state space, learned options would be more useful then the set of states from which the goal is reachable. Previ-
if they could be reused in later tasks that are related but have ous research has selected goal states by a varierty of methods
distinct state spaces.                                (e.g., variable change frequency [Hengst, 2002], local graph
  We propose learning portable options by using two sepa- partitioning [S¸ims¸ek et al., 2005] and salience [Singh et al.,
rate representations: a representation in problem-space that is 2004]). Other research has focused on extracting options by
Markov for the particular task at hand, and one in agent-space exploiting commonalities in collections of policies over a sin-
that may not be Markov but is retained across successive task gle state space [Thrun & Schwartz, 1995; Bernstein, 1999;
instances (each of which may require a new problem-space, Perkins & Precup, 1999; Pickett & Barto, 2002].
possibly of a different size or dimensionality) [Konidaris & All of these methods learn options in the same state space
Barto, 2006]. Options learned in agent-space can be reused in in which the agent is performing reinforcement learning, and
future problem-spaces because the semantics of agent-space thus can only be reused for the same problem or for a new
remain consistent across tasks.                       problem in the same space. The available state abstraction
  We present the results of an experiment showing that methods [Jonsson & Barto, 2001; Hengst, 2002] only allow
learned agent-space options can signiﬁcantly improve perfor- for the automatic selection of a subset of this space for option
mance across a sequence of related but distinct tasks, but are learning, or require an explicit transformation from one space
best used in conjunction with problem-speciﬁc options. to another [Ravindran & Barto, 2003].

                                                IJCAI-07
                                                   895                                                             j
2.2  Sequences of Tasks and Agent-Space               where di is the problem-space state descriptor (sufﬁcient to
                                                                                        S   cj
We are concerned with an agent that is required to solve a se- distinguish this state from the others in j), i is an agent-
                                                                          j
quence of related but distinct tasks, deﬁned as follows. The space descriptor, and ri is the reward obtained at the state.
agent experiences a sequence of environments generated by The goal of reinforcement learning in each task Sj is to ﬁnd
the same underlying world model (e.g., they have the same a policy πj that maximizes reward.
physics, the same types of objects may be present in the en- The agent is also either given, or learns, a set of higher-
vironments, etc.). From the sensations it receives in each en- level options to reduce the time required to solve the task.
vironment, the agent creates two representations. The ﬁrst Options deﬁned using d are not portable between tasks be-
is a state descriptor sufﬁcient to distinguish Markov states in cause the form and meaning of d (as a problem-space de-
the current environment. This induces a Markov Decision scriptor) may change from one task to another. However, the
Process (MDP) with a ﬁxed set of actions (because the agent form and meaning of c (as an agent-space descriptor) does
does not change) but a set of states, transition probabilities not. Therefore we deﬁne agent-space option components as:
and reward function that depend only on the environment the          π :(cj,a)    → [0, 1]
agent is currently in. The agent thus works in a different state      o     i
                                                                     I :  cj      →{0, 1}
space with its own transition probabilities and reward func-          o    i
                                                                     β :   cj     → [0, 1]
tion for each task in the sequence. We call each of these state       o    i             .
spaces a problem-space.                               Although the agent will be learning task and option policies
  The agent also uses a second representation based on the in different spaces, both types of policies can be updated
features that are consistently present and retain the same se- simultaneously as the agent receives both agent-space and
mantics across tasks. This space is shared by all of the tasks problem-space descriptors at each state.
in the sequence, and we call it an agent-space.Thetwo
spaces stem from two different representational requirements: 4 Experiments
problem-space models the Markov description of a particular
task, and agent-space models the (potentially non-Markov) 4.1 The Lightworld Domain
commonalities across a sequence of tasks. We thus consider An agent is placed in an environment consisting of a sequence
a sequence of tasks to be related if they share an agent-space. of rooms, with each room containing a locked door, a lock,
  This approach is distinct from that taken in prior reinforce- and possibly a key. In order to leave a room, the agent must
ment learning research on ﬁnding useful macro-actions across unlock the door and step through it. In order to unlock the
sequences of tasks [Bernstein, 1999; Perkins & Precup, 1999; door, it must move up to the lock and press it, but if a key is
Pickett & Barto, 2002; Thrun & Schwartz, 1995],wherethe present in the room the agent must be holding it to success-
tasks must be in the same state space but may have different fully unlock the door. The agent can obtain a key by moving
reward functions. An appropriate sequence of tasks under our on top of it and picking it up. The agent receives a reward of
deﬁnition requires only that the agent-space semantics remain 1000 for leaving the door of the ﬁnal room, and a step penalty
consistent, so each task may have its own completely distinct of −1 for each action. Six actions are available: movement in
state space. Konidaris and Barto (2006) have used the same each of the four grid directions, a pickup action and a press
distinction to learn shaping functions in agent-space to speed action. The environments are deterministic and unsuccessful
up learning across a sequence of tasks.               actions (e.g., moving into a wall) result in no change in state.
  One simple example of an appropriate sequence of tasks We equip the agent with twelve light sensors, grouped into
is a sequence of buildings in each of which a robot equipped threes on each of its sides. The ﬁrst sensor in each triplet
with a laser range ﬁnder is required to reach a target room. detects red light, the second green and the third blue. Each
Since the laser-range ﬁnder readings are noisy and non- sensor responds to light sources on its side of the agent, rang-
Markov, the robot would likely build a metric map of the ing from a reading of 1 when it is on top of the light source,
building as it explores, thus forming a building-speciﬁc prob- to 0 when it is 20 squares away. Open doors emit a red light,
lem space. The range ﬁnder readings themselves form the keys on the ﬂoor (but not those held by the agent) emit a green
agent space, because their meaning is consistent across all light, and locks emit a blue light. Figure 1 shows an example.
of the buildings. The robot could eventually learn options in Five pieces of data form a problem-space descriptor for any
agent-space corresponding to macro-actions like moving to lightworld instance: the current room number, the x and y co-
the nearest door. Because these options are based solely on ordinates of the agent in that room, whether or not the agent
sensations in agent space without referencing problem-space has the key, and whether or not the door is open. We use the
(any individual metric map), they can be used to speed up light sensor readings as an agent-space because their seman-
learning in any building that the robot encounters later. tics are consistent across lightworld instances. In this case the
                                                      agent-space (with 12 continuous variables) has higher dimen-
3  Options in Agent-Space                             sion than any individual problem-space.
We consider an agent solving n problems, with state spaces Types of Agent
S1, ..., Sn, and action space A.Weviewtheith state in task We used ﬁve types of reinforcement learning agents: agents
Sj as consisting of the following attributes:         without options, agents with problem-space options, agents
                                                      with perfect problem-space options, agents with agent-space
                   j    j  j  j
                  si =(di ,ci ,ri ),                  options, and agents with both option types.

                                                IJCAI-07
                                                   896                                                      functions to solve the underlying task instance, because their
                                                      agent-space descriptors are only Markov in one room light-
                                                      worlds, which were not present in our experiments.
                                                      Experimental Structure
                                                      We generated 100 random lightworlds, each consisting of 2-
                                                      5 rooms with width and height of 5-15 cells. A door and
                                                      lock were were randomly placed on each room boundary,
                                                          1
                                                      and 3 of rooms included a randomly placed key. This re-
                                                      sulted in state space with between 600 and approximately
                                                      20, 000 state-action pairs (4900 on average). We evaluated
                                                      each problem-space option agent type on 1000 lightworlds
                                                      (10 samples of each generated lightworld).
                                                        To evaluate the performance of agent-space options as the
                                                      agents gained more experience we similarly obtained 1000
         Figure 1: A small example lightworld.        samples, but for each sample we ran the agents once with-
                                                      out training and then with between 1-10 training experiences.
                                                      Each training experience for a sample lightworld consisted
                                    λ      
  The agents without options used Sarsa( ) with -greedy of 100 episodes in a training lightworld randomly selected
              α  =0.1  γ =0.99   λ =0.9    =0.01
action selection (    ,         ,       ,         )   from the remaining 99. Although the agents updated their
to learn a solution policy in problem-space, with each state- options during evaluation in the sample lightworld, these up-
                                500
action pair assigned an initial value of .            dates were discarded before the next training experience so
  Agents with problem-space options had an (initially un- the agent-space options never received prior training in the
learned) option for each pre-speciﬁed salient event (picking evaluation lightworld.
up each key, unlocking each lock, and walking through each
door). Options were learned in problem-space and used the Results
same parameters as the agent without options, but used off- Figure 2 shows average learning curves for agents employ-
policy trace-based tree-backup updates [Precup et al., 2000] ing problem-space options, and Figure 3 shows the same
for intra-option learning. Options got a reward of 1 when for agents employing agent-space options. The ﬁrst time an
completed successfully, used a discount factor of 0.99 per agent-space option agent encounters a lightworld it performs
action, and could be taken only in the room in which they similarly to an agent without options (as evidenced by two
were deﬁned, and in states where their value function ex- topmost learning curves in each ﬁgure), but its performance
ceeds a minimum threshold (0.0001). Because these options rapidly improves with experience in other lightworlds. Af-
are learned in problem-space, they are useful but must be re- ter experiencing a single training lightworld the agent has a
learned for each individual lightworld.               much shallower learning curve than an agent using problem-
  Agents with perfect problem-space options were given pre- space options alone, until by 5 experiences its learning curve
learned options for each salient event. They still performed is similar to that of an agent with perfect problem-space op-
option updates and were otherwise identical to the standard tions (compare with the bottom-most learning curve of Figure
agent with options, but they represented agents with perfectly 2), even though its options are never trained in the same light-
transferrable fully learned options.                  world in which it is tested. The comparison between Figures
  Agents with agent-space options still learned their solution 2 and 3 shows that agent-space options can be successfully
policies in problem-space but learned their option policies in transferred between lightworld instances.
agent-space. Each agent employed three options: one for Figure 4 shows average learning curves for agents em-
picking up a key, one for going through an open door and ploying both types of options.1 The ﬁrst time such agents
one for unlocking a door, with each one’s policy a function encounter a lightworld they perform as well as agents us-
of the twelve light sensors. Since the sensor outputs are con- ing problem-space options (compare with the second high-
tinuous we used linear function approximation for each op- est curve in Figure 2), and thereafter rapidly improve their
tion’s value function, performing updates using gradient de- performance (performing better than agents using only agent-
scent (α =0.01) and off-policy trace-based tree-backup up- space options) and again by 5 experiences performing nearly
dates. The agent gave each option a reward of 1 upon com- as well as agents with perfect options. We conjecture that
pletion, and used a step penalty of 0.05 and a discount factor this improvement results from two factors. First, the agent-
of 0.99. An option could be taken at a particular state when space is much larger than any individual problem-space,
its value function there exceeded a minimum threshold (0.1). so problem-space options are easier to learn from scratch
Because these options are learned in agent-space, they can be
                                                         1
transferred between lightworld instances.                In 8 of the more than 200, 000 episodes used when testing
                                                      agents with both types of options an agent-space value function ap-
  Finally, agents with both types of options were included to proximator diverged and we restarted the episode. Although this is
represent agents that learn both general portable and speciﬁc a known problem with the backup method we used [Precup et al.,
non-portable skills simultaneously.                   2000], it did not occur at all during the same number of samples
  Note that all agents used discrete problem-space value obtained for agents with agent-space options only.

                                                IJCAI-07
                                                   897                                                                   x 105
          7000                                                    14
                                Perfect Options
                                Learned Options
                                                                  12
          6000                  No Options

                                                                  10
          5000

                                                                   8
          4000

                                                                   6

         Actions 3000

                                                                   4
          2000
                                                                   2
          1000
                                                                   0
                                                                    NO LO PO 0 1 2 3 4 5 6 7 8 9 10
           0
               10  20  30  40  50  60  70
                        Episodes

                                                                              70
Figure 2: Learning curves for agents with problem-space op- Figure 5: Total steps over episodes for agents with no
tions.                                                options (NO), learned problem-space options (LO), perfect
                                                      options (PO), agent-space options with 0-10 training experi-
          7000                          
                                 0 experiences        ences (dark bars), and both option types with 0-10 training
                                 1 experience
          6000                   2 experiences        experiences (light bars).
                                 5 experiences
                                 10 experiences
          5000

          4000                                        over 70 episodes for agents using no options, problem-space


         Actions 3000                                 options, perfect options, agent-space options, and both option
                                                      types. Experience in training environments rapidly drops the
          2000                                        number of total steps required to nearly as low as the num-
          1000                                        ber required for an agent with perfect options. It also clearly

           0                                          shows that agents using both types of options do consistently
               10  20  30  40  50  60  70
                        Episodes                      better than those using agent-space options alone. We note
                                                      that the error bars in Figure 5 are small and decrease with
                                                      experience, indicating consistent transfer.
Figure 3: Learning curves for agents with agent-space op-
tions, with varying numbers of training experiences.  4.2  The Conveyor Belt Domain
          7000
                                 0 experiences
                                 1 experience         A conveyor belt system must move a set of objects from a row
          6000                   2 experiences
                                 5 experiences        of feeders to a row of bins. There are two types of objects
                                 10 experiences
          5000                                        (triangles and squares) and each bin starts has a capacity for
                                                      each type. The objects are issued one at a time from a feeder
          4000                                        and must be directed to a bin. Dropping an object into a bin
         Actions 3000                                 with a positive capacity for its type decrements that capacity.

          2000                                          Each feeder is directly connected to its opposing bin
                                                      through a conveyor belt, which is connected to the belts above
          1000                                        and below it at a pair of ﬁxed points along its length. The
           0
               10  20  30  40  50  60  70             system may either run the conveyor belt (which moves the
                        Episodes                      current object one step along the belt) or try to move it up
                                                      or down (which only moves the object if it is at a connection
Figure 4: Learning curves for agents with agent-space and point). Each action results in a penalty of −1, except where it
problem-space options, with varying numbers of training ex- causes an object to be dropped into a bin with spare capacity,
periences.                                            in which case it results in a reward of 100.Asmallexample
                                                      conveyor belt system is shown in Figure 6.


                                                                     1
than agent-space options. This explains why agents us-
ing only agent-space options and no training experiences             2

perform more like agents without options than like agents            3
with problem-space options. Second, options learned in our
problem-space can represent exact solutions to speciﬁc sub-
goals, whereas options learned in our agent-space are gen- Figure 6: A small example conveyor belt problem.
eral and must be approximated, and are therefore likely to be
slightly less efﬁcient for any speciﬁc subgoal. This explains Each system has a camera that tracks the current object and
why agents using both types of options perform better in the returns values indicating the distance (up to 15 units) to the
long run than agents using only agent-space options.  bin and each connector along the current belt. Because the
  Figure 5 shows the mean total number of steps required space generated by the camera is present in every conveyor-

                                                IJCAI-07
                                                   898belt problem and retains the same semantics it is an agent-     1000                           
space, and because it is discrete and relatively small (13, 500
states) we can learn policies in it without function approxima-   0

tion. However, because it is non-Markov (due to its limited     −1000
range and inability to distinguish between belts) it cannot be
used as a problem-space.                                        −2000
  A problem-space descriptor for a conveyor belt instance      Reward
                                                                −3000
consists of three numbers: the current object number, the                            Learned Options
                                                                                     Perfect Options
                                                                −4000
belt it is on and how far along that belt it lies (technically                       No Options
we should include the current capacity of each bin, but we
                                                                −5000  
can omit this and still obtain good policies). We generated       0   10  20  30  40 50  60  70
                                                                              Episodes
100 random instances with 30 objects and 20-30 belts (each
of length 30-50) with randomly selected interconnections, re-
sulting in problem-spaces of 18, 000-45, 000 states.  Figure 7: Learning curves for agents with problem-space op-
  We ran experiments where the agents learned three options: tions.
one to move the current object to the bin at the end of the belt 1000                          

it is currently on, one for moving it to the belt above it, and   0
one for moving it to the belt below it. We used the same
agent types and experimental structure as before, except that   −1000
the agent-space options did not use function approximation.
                                                                −2000
                                                                Reward

Results                                                                                0 experiences
                                                                −3000                  1 experience
Figures 7, 8 and 9 show learning curves for agents employing                           3 experiences
                                                                                       5 experiences
                                                                −4000                  8 experiences
no options, problem-space options and perfect options; agents                          10 experiences
employing agent-space options; agents employing both types
                                                                −5000  
                                                                  0   10  20  30  40  50  60 70
of options, respectively.                                                      Episodes
  Figure 8 shows that the agents with agent-space options
and no prior experience initially improve quickly but eventu-
ally obtain lower quality solutions than agents with problem- Figure 8: Learning curves for agents with agent-space op-
space options (Figure 7). One or two training experiences re- tions, with varying numbers of training experiences.
sult in roughly the same curve as agents using problem-space    1000                           
options but by 5 training experiences the agent-space options
are a signiﬁcant improvement, although due to their limited       0

range they are never as good as perfect options. This initial   −1000
dip is probably due to the limited range of the agent-space op-
                                                                −2000


tions (due to the limited range of the camera) and the fact that Reward
                                                                                        0 experiences
they are only locally Markov, even for their own subgoals.      −3000                   1 experience
                                                                                        3 experiences
  Figure 9 shows that agents with both option types do not                              5 experiences
                                                                                        8 experiences
experience this initial dip, and outperform problem-space op-   −4000                   10 experiences
tions immediately, most likely because the agent-space op-
                                                                −5000  
                                                                  0   10  20  30  40  50  60 70
tions are able to generalise across belts. Figure 10 shows                     Episodes
the mean total reward for each type of agent. Agents us-
ing agent-space options eventually outperform agents using
                                                      Figure 9: Learning curves for agents with both types of op-
problem-space options only, even though the agent-space op-
                                                      tions, with varying numbers of training experiences.
tions have a much more limited range; agents using both types
of options consistently outperform agents with either option
type and eventually approach the performance of agents using
pre-learned problem-space options.                    be closely related to the difference between allocentric and
                                                      egocentric representations of space [Guazzelli et al., 1998].
                                                        We also expect that learning an option in agent-space will
5  Discussion                                         often actually be harder than solving an individual problem-
The concept of an agent-centric representation is closely re- space instance, as was the case in our experiments. In such
lated to the notion of deictic or ego-centric representations situations, learning both types of options simultaneously is
[Agre & Chapman, 1987], where objects are represented from likely to improve performance. Since intra-option learning
the point of view of the agent rather than in some global frame methods allow for the update of several options from the
of reference. We expect that for most problems (especially in same experiences, it may be better in general to simultane-
robotics) agent-space representations will be egocentric, ex- ously learn both general portable skills and speciﬁc, exact but
cept in manipulation tasks where they will likely be object- non-portable skills, and allow them to bootstrap each other.
centric. In problems involving spatial maps, we expect that The addition of agent-space descriptors to the reinforce-
the difference between problem-space and agent-space will ment learning framework introduces a design problem similar

                                                IJCAI-07
                                                   899