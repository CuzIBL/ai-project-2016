              Change of Representation for Statistical Relational Learning
            Jesse Davis, Irene Ong, Jan Struyf,                  V´ıtor Santos Costa
             Elizabeth Burnside David Page
            University of Wisconsin - Madison          Universidade Federal do Rio de Janeiro
                     1210 West Dayton                    Centro de Tecnologia, Bloco H-319
                 Madison, WI 53706, USA                         Rio de Janiero, Brasil
                email: jdavis@cs.wisc.edu

                    Abstract                          consider the well-known task of predicting whether two ci-
                                                      tations refer to the same underlying paper. The CoAuthor
    Statistical relational learning (SRL) algorithms  relation is potentially useful for disambiguating citations; for
    learn statistical models from relational data, such as example, if S. Russell and S.J. Russell both have similar lists
    that stored in a relational database. We previously of coauthors, then perhaps they are interchangeable in cita-
    introduced view learning for SRL, in which the    tions. But the CoAuthor relation may not have been provided
    view of a relational database can be automatically to the learning system. Furthermore, CoAuthor can be used
    modiﬁed, yielding more accurate statistical mod-  as a building block to construct further explicit features for
    els. The present paper presents SAYU-VISTA, an    the system, such as a new predicate SamePerson.
    algorithm which advances beyond the initial view    A start already has been made in change of representa-
    learning approach in three ways. First, it learns tion for SRL systems, under the name of View Learning.
    views that introduce new relational tables,rather Initial approaches to View Learning [Davis et al., 2005b;
    than merely new ﬁelds for an existing table of the 2005a] showed that even an application with a single table
    database. Second, new tables or new ﬁelds are not can beneﬁt from new views: views can represent connections
    limited to being approximations to some target con- between related examples. Moreover, a greedy approach to
    cept; instead, the new approach performs a type   view learning can be useful. The present paper introduces
    of predicate invention. The new approach avoids   important extensions for change of representations in SRL.
    the classical problem with predicate invention, of First, it provides a mechanism for learning a new view as
    learning many useless predicates, by keeping only afullnew relational table,suchasCoAuthor. Second, it
    new ﬁelds or tables (i.e., new predicates) that im- permits a newly-invented relation, or predicate, to be used
    mediately improve the performance of the statisti- in the invention of other new relations,suchasSamePerson.
    cal model. Third, retained ﬁelds or tables can then Such re-use goes beyond simply introducing “short-cuts” in
    be used in the deﬁnitions of further new ﬁelds or ta- the search space for new relations; because the new approach
    bles. We evaluate the new view learning approach  also permits a relation to be from aggregates over existing re-
    on three relational classiﬁcation tasks.          lations, re-use actually extends the space of possible relations
                                                      that can be learned by the approach. Because this new work
                                                      extends SAYU by providing a mechanism for View Inven-
1  Introduction                                       tion by Scoring TAbles, we call the resulting system SAYU-
Most statistical relational learning (SRL) algorithms are con- VISTA.
strained to operate with the speciﬁc representation they are
given—a PRM  [Friedman et al., 1999] must use the schema 2ILPandSAYU
of the input database, a logic-based system must use the pred- Inductive logic programming (ILP) is a popular approach for
icates provided. Yet, in many cases the original relational rep- learning in a relational environment. Given a set of posi-
resentation was not designed to empower the learning algo- tive and negative examples and background knowledge, an
rithm. For example, the schema for a medical database may ILP system ﬁnds a logical description of the underlying data
have been chosen to simplify billing; biological databases of- model that differentiates between the positive and negative
ten have a hierarchical schema, forcing an SRL system to use examples. This description is a set of ﬁrst-order logical rules
long slot-chains or long clause bodies to ﬁnd related data. or clauses, which form a logic program.
  In some cases an SRL user may have the freedom to mod- SAYU [Davis et al., 2005a] is an SRL system that com-
ify the representation for the sake of learning. Even so, con- bines ILP with Bayesian network learning, by default with
sidering all relevant features or relations to include for a task tree-augmented naive Bayes learning [Friedman et al., 1997].
is a difﬁcult job by itself. Ideally, we would like the learn- SAYU is an acronym for “Score As You Use.” Unlike other
ing algorithm to be able to discover and incorporate relevant, approaches to using ILP for deﬁning new features, SAYU
intermediate concepts into the representation. For instance, scores each clause not by a standard ILP measure, but by

                                                IJCAI-07
                                                  2719how much it helps the model in which the clause is used. thermore these new predicates are no longer approximations
Another system, known as nFOIL, was developed in paral- to the target concept, but may be any concept that improves
lel with SAYU and has this same property [Landwehr et al., the statistical model.
2005]. In the SAYU approach, we start from an empty model The original motivation for view learning centered on
(or a prior model). Next, an ILP system generates rules. Each learning a statistical expert system to provide decision sup-
generated rule represents a new feature which is added to the port to radiologists [Davis et al., 2005b]. There we used SRL
current model. We then evaluate the generalization ability because the learned statistical model sits on top of the Na-
of the model extended with the new feature, where gener- tional Mammography Database (NMD) schema, a standard
alization ability is measured as the area under the precision established by the American College of Radiology [ACR,
recall curve on a held-aside subset of the data. We retain the 2004]. The goal of the data set is to predict which abnormali-
new model if the inclusion of the new feature signiﬁcantly ties on a mammogram are malignant. We will use mammog-
improves the model’s generalization ability; otherwise we re- raphy as a running example to help illustrate the key compo-
main with the original model. This results in a tight coupling nents of the algorithm.
between feature construction and model building.        SAYU-VISTA, nFOIL and SAYU   all learn deﬁnite clauses
  SAYU needs an ILP system to propose rules. In our work, and evaluate clauses by how much they improve the statisti-
we use Aleph [Srinivasan, 2001], which implements the Pro- cal classiﬁer. The key difference in the algorithms rests in the
gol algorithm [Muggleton, 1995] to learn rules. This algo- form that the head of the learned clauses takes. In nFOIL and
rithm induces rules in two steps. Initially, it selects a positive SAYU, the head of a clause has the same arity and type as
instance to serve as the “seed” example. It searches the back- the example, allowing us to precisely deﬁne whether a clause
ground knowledge for the facts known to be true about the succeeds for a given example and hence whether the corre-
seed example. The combination of these facts forms the ex- sponding variable is true. In the Mammography domain, a
ample’s most speciﬁc or saturated clause. The key insight positive example has the form malignant(ab1),where
of the Progol algorithm is that some of these facts explain ab1 is a primary key for some abnormality. Every learned
this example’s classiﬁcation. Thus, generalizations of those rule has the head malignant(A) such as in the following
facts could apply to other examples. Aleph deﬁnes the search rule:
space to be clauses that generalize a seed example’s saturated malignant(Ab1) if:
clause, and performs a general to speciﬁc search over this ArchDistortion(Ab1,present),
space.                                                    same_study(Ab1,Ab2),
  SAYU greedily searches for new ﬁelds or variables, de-  Calc_FineLinear(Ab2,present).
ﬁned by ﬁrst-order logic clauses, that improve the prediction
                                                      The Bayesian network variable corresponding to this rule will
of the class ﬁeld. SAYU modiﬁes the standard Aleph search
                                                      take value true for the example malignant(ab1) if the
as follows. Instead of using coverage, Aleph passes each
                                                      clause body succeeds when the logical variable A is bound to
clause it constructs to SAYU, which converts it to a binary
                                                      ab1.
feature. The feature is added to the current training set and
                                                        SAYU-VISTA removes the restriction that all the learned
SAYU learns a new Bayes net, a TAN network in our case,
                                                      clauses have the same head. First, SAYU-VISTA learns pred-
incorporating this new feature. We measure performance by
                                                      icates that have a higher-arity than the target predicate. For
looking at the area under the precision recall curve on a held-
                                                      example, in the Mammography domain, predicates such as
aside tune set. If the feature degrades the performance of the
                                                      p11(Abnormality1, Abnormality2), which relate
network, SAYU discards the feature and reverts back to the
                                                      pairs of abnormalities, are learned. Subsection 3.1 discusses
old classiﬁer. Then SAYU returns control to Aleph to con-
                                                      scoring predicates that have higher arities than the target rela-
struct the next clause. If the new feature improves the score
                                                      tion. Second, SAYU-VISTA learns predicates that have types
of the network, then SAYU retains the feature in the network.
                                                      other than the example key in the predicate head. For ex-
In contrast to Aleph, after accepting a rule, SAYU randomly
                                                      ample, a predicate p12(Visit), which refers to attributes
selects a new seed example and reinitializes the search. Thus,
                                                      recorded once per a patient visit, could be learned. In order
for a given seed, SAYU does not search for the best rule,
                                                      to score predicates of this form, we introduce the concept of
but only the ﬁrst rule that helps. However, nothing prevents
                                                      Linkages, which are discussed in subsection 3.2. After dis-
the same seed from being selected multiple times during the
                                                      cussing how to evaluate these types of predicates, we will
search.
                                                      present the full SAYU-VISTA algorithm.
3  Learning New Predicates                            3.1  Scoring Higher Arity Predicates
                                                      SAYU-VISTA can learn a clause such as:
The initial approach to View Learning [Davis et al., 2005b],
suffers from two important drawbacks. First, it only creates p11(Ab1,Ab2) if:
new ﬁelds, not new tables. The new ﬁeld deﬁnition has the density(Ab1,D1),
same arity as the target predicate. Second, the new ﬁelds are prior-abnormality-same-loc(Ab1,Ab2),
just learned approximations to the target concept. SAYU-  density(Ab2,D2),
VISTA addresses both shortcomings. It creates new predi-  D1  > D2.
cates with arity greater than one, some capturing many-to- This rule says that p11 is true of a pair of abnormalities
many relations, which require a new table to represent. Fur- Ab1 and Ab2 if they are at the same location, Ab1 was ob-

                                                IJCAI-07
                                                  2720served ﬁrst, and Ab2 has higher density than Ab1. Thus be kept. We used p =0.02 in all experiments. Optionally, the
p11 may be thought of as “density increase.” Unfortunately, user may input an initial feature set to the algorithm. Algo-
it is not entirely clear how to match an example, such as rithm 1 shows pseudocode for the SAYU-VISTA algorithm.
malignant(ab1), to the head of this clause for p11.     The clause search proceeds as follows. We randomly se-
SAYU-VISTA maps, or links, one argument to the example lect an arity for the predicate. To limit the search space, we
key and aggregates away any remaining arguments using ex- restrict the arity to be either the arity of the target relation, or
istence or count aggregation. The next section describes the the arity of the target relation plus one. Next, we randomly
approach used for linkage; the remainder of this paragraph select the types for the variables that appear in the head of
discusses aggregation. In existence aggregation, the clause the clause. The clause search uses a top-down, breadth-ﬁrst
succeeds for the given example (key) if there exists any bind- reﬁnement search. We deﬁne the space of candidate liter-
ings of the remaining variables for which the clause succeeds. als to add using modes, as in Progol [Muggleton, 1995] or
Count aggregation computes the number of bindings for these Aleph [Srinivasan, 2001]. We score each proposed clause by
remaining variables for which the clause succeeds. Currently, adding it as variable in the statistical model. To construct the
SAYU-VISTA discretizes aggregated features using a binning feature, we ﬁrst link the predicate back to the example key
strategy that creates three equal-cardinality bins, where three as described in subsection 3.2. Then we perform the nec-
was chosen arbitrarily before the running of any experiments. essary aggregation, discussed in subsection 3.1, to convert
                                                      the clause into a feature. By default, the algorithm ﬁrst tries
3.2  Linkages                                         existence aggregation and then tries count aggregation. The
So far we have simpliﬁed matters by assuming that the ﬁrst clause search terminates in three cases: (i) it ﬁnds a clause
argument to the learned predicate has the same type as the that meets the improvement threshold; (ii) it fully explores the
example key. In our examples so far, this type has been ab- search space; (iii) it exceeds the clause limit. After satisfying
normality id. There is no need to enforce this limitation. For one of these conditions, the algorithm re-initializes the search
example, in predicting whether an abnormality is malignant, process. Every clause that meets the improvement threshold
it might be useful to use the following clause, where Visit is added into the background knolwedge. Therefore, future
is a key that refers to all abnormalities found on a given mam- predicate deﬁnitions can re-use previously learned predicates.
mogram:                                               As in prior work [Davis et al., 2005a], the algorithm termi-
                                                      nates when it exceeds the global time limit.
p(Visit) :-
   visit(Visit,Ab),
   MassesShape(Ab,oval).                              4   Data and Methodology
Predicate p is true of a visit, or mammogram, that contains at Cora. The objective of this dataset is to predict whether two
least one abnormality with an oval shape.             citations refer to the same paper. The dataset was originally
  Linkage declarations are background knowledge that can constructed by McCallum et al. [2000]. We used the same
establish the connection between objects in the examples and version of the data as Kok and Domingos [2005]. Cora in-
objects in the newly invented predicates. When these ob- cludes 1295 citations to 112 Computer Science papers, re-
jects are of the same type, the linkage is trivial; otherwise, sulting in 25072 positive examples and 597310 negative ex-
it must be deﬁned. For mammography, we use linkage def- amples. The background knowledge includes data on title,
initions that link an abnormality to its patient or to its visit venue, author(s), and year for each citation. We deﬁned pa-
(mammogram). The linkages for the other datasets we use are per, title, venue, author and year as keys that can appear in
equally straightforward and are presented when we describe heads of clauses. We link a paper to its title, venue, author(s)
those datasets.                                       and year ﬁelds. We aggregate over papers and authors.
                                                        UW-CSE.   This common SRL dataset was constructed by
3.3  Predicate Learning Algorithm                     Richardson and Domingos [2006] and is publicly available.
At a high level SAYU-VISTA learns new predicates by per- The goal is to predict the advisor of a graduate student. The
forming a search over the bodies of deﬁnite clauses and se- information comes from the University of Washington CS
lecting those bodies that improve the performance of the sta- Department and contains 113 positive examples versus 2,711
tistical model on a classiﬁcation task. We use tree-augmented negative examples. We deﬁned students, professors, courses
naive Bayes (TAN) [Friedman et al., 1997] as our statistical and publications as keys that could appear in the head of a
model.                                                clause. We link a course to a graduate student by the TA re-
  The predicate invention algorithm takes several inputs from lationship, and we link papers to a graduate student by the
a user. First, it needs a training set, which is used to learn author relationship. We link a course to a professor by the
the statistical model, and a tuning set, which is used to eval- teaches relationship and we link papers to a professor by the
uate the statistical model. The user provides a pre-deﬁned author relationship. We aggregate over students, professors,
set of distinguished types, which can appear in the head of papers and courses.
a clause. The user provides background knowledge, which Mammography.    The objective of this dataset is to pre-
must include linkage deﬁnitions for each distinguished type. dict whether an abnormality on a mammogram is benign or
The algorithm using an improvement threshold, p, to decide malignant [Davis et al., 2005b]. This dataset consists of a
which predicates to retain in the model. A new predicate must radiologist’s interpretation of a mammogram and not the raw
improve the model’s performance by at least p%inorderto image data. The dataset contains 435 positive examples and

                                                IJCAI-07
                                                  2721        Input: Train Set Labels T , Tune Set Labels S, Distinguished Types D, Background Knowledge B,
              Improvement Threshold p, Initial Feature Set Finit
        Output: Feature Set F , Statistical Model M
        F  = Finit;
        BestScore  =0;
        while time remains do
            Randomly select the arity of predicate to invent;
            Randomly select types from D for each variable in the head of the predicate;
            SelectedFeature =false;
            while not(SelectedFeature) do
               Predicate = Generate next clause according to breadth ﬁrst search;
               /*Link the predicate back to the target relation */ ;
               LinkedClause = Link(Predicate, B);
               /* Convert the LinkedClause into a feature that the statistical model can use */;
               NewFeature  = aggregate(LinkedClause, T , S);
               Fnew = F ∪ NewFeature;
               Mnew = BuildTANNetwork(T , Fnew);
               NewScore  =AreaUnderPRCurve(M,  S, Fnew);
               /*Retain this feature*/ ;
               if NewScore > p∗ BestScore then
                  F = Fnew;
                  BestScore = NewScore;
                  M  = Mnew;
                  Add predicate into background knowledge;
                  SelectedFeature = true;
               end
            end
        end
                                         Algorithm 1: SAYU-VISTA


65365 negative examples. We used the same version of the ferences tested by a paired two-tailed t-test on areas under the
data as Davis et al. [2005b].Wedeﬁneabnormality, visit and precision-recall curves (AUCPR) across the different folds.
patient as keys that can appear in the head of the clause. We We are careful to repeat any tuning of parameters on each
aggregate over abnormalities.                         fold of cross-validation, without looking at the test set for
                                                      that fold, by dividing the data into a training set and tuning
                                                      set. In this we follow the methodology of the developers of
5  Experiments and Results                            both MLNs and SAYU. For SAYU-VISTA, as for SAYU, we
We compare SAYU-VISTA to two SRL systems in our ex-   use the training set to learn the network parameters, while we
periment. First, we compare SAYU-VISTA to SAYU [Davis use the tuning set to score potential clauses. For all datasets
et al., 2005a] as it is the state-of-the-art view learning im- we use AUCPR as our score metric. However, we only look
plementation, a follow-up to the original view learning pa- at AUCPR for recalls ≥ 0.5. We do this for two reasons.
per [Davis et al., 2005b]. However, SAYU only learns ad- First, precision can have high variance at low levels of re-
ditional ﬁelds for existing tables; these ﬁelds are deﬁned by call. Second, in domains such as Mammography, we are only
learned rules that are approximations to the target concept. interested in high levels of recall. A practicing radiologist
We also compare SAYU-VISTA against another leading SRL would need to achieve at least this level of recall. A clause
system—one that already has been applied with success (as must improve the AUCPR (for recall ≥ 0.5) by at least 2%
measured by cross-validated precision-recall curves) to two in order to be retained in the network. This is an arbitrary
of our application tasks and that has been receiving consid- parameter setting; in fact we did not try any other thresholds.
erable attention: Markov Logic Networks [Richardson and We had a time-based stop criteria for both SAYU and SAYU-
Domingos, 2006], publicly available as the Alchemy system. VISTA. For UW-CSE each fold was given two hours to run,
Finally, we compared these three SRL systems against Aleph whereas for Mammography and Cora each fold received three
on all three data sets, and the SRL systems signiﬁcantly out- hours runtime. We gave UW-CSE less time because it was a
performed Aleph on all three data sets. Therefore, to simplify smaller data set. In practice, the time is not a limiting factor
the presentation we limit the remaining discussion to the three because few changes occur after the ﬁrst 30 minutes for any
SRL systems.                                          of the tasks. MLN runs were not time-bounded. To offset po-
  All three SRL systems are evaluated by precision-recall tential differences in computer speeds, all experiments were
curves estimated by cross-validation with signiﬁcance of dif- run on identically conﬁgured machines.

                                                IJCAI-07
                                                  2722        1                                                      1
                                                                               SA-VSA
                                                                                       ML
                                                                                      SA
      .                                                     .

      .                                                     .

      .                                                     .
    recision                                               recision

      .2                                                    .2
             SA-VSA
                     ML
                    SA
                .2     .     .      .       1                         .2     .      .     .       1
                         ecall                                                 ecall
Figure 1: Precision-Recall Curves comparing SAYU-VISTA Figure 2: Precision-Recall Curves comparing SAYU-VISTA,
and SAYU on Cora                                      MLNs, and SAYU on UW-CSE

  We employed the default structure learning algorithm for predicate head (SAYU-VISTA).
MLNs and performed limited manual tuning of the parame-
ters of the system to maximize AUCPR, while maintaining Table 1 reports the average AUCPR for UW-CSE and the p-
acceptable execution times. We report the best AUCPR val- value for a two-tailed paired t-test comparing SAYU-VISTA
                                                      to the other approaches. SAYU-VISTA comes close to per-
ues we obtained, over all attempted parameter settings. Note             0.05 <p<0.06
that we did not do any parameter tuning for SAYU-VISTA. forming signiﬁcantly (         ) better than SAYU on
The average, per-fold run-times for MLNs were all signiﬁ- this domain. Although performance varies widely between
cantly longer than for either SAYU or SAYU-VISTA. The av- the 5 folds, SAYU-VISTA had a higher AUCPR than SAYU
erage, per fold run time for learning structure were ﬁve hours on each fold. SAYU-VISTA also comes close to outperform-
for Cora, seven hours for UW-CSE and three hours for Mam- ing MLNs on this data set, winning on four out of ﬁve folds.
mography.                                               Figure 2 shows precision-recall curves for SAYU, SAYU-
                                                      VISTA and MLNs on this dataset. We pooled results across
5.1  Discussion of Results                            all ﬁve folds to generate the curves. Even though we mea-
                                                                            ≥  0.5
Cora. Following Kok and Domingos [2005] we performed  sured AUCPR for recall      , SAYU-VISTA dominates
two-fold cross validation on this dataset for ﬁve different ran- SAYU for most levels of recall. However, MLNs domi-
dom train-test splits. We divided the training set in half, to nate SAYU-VISTA for low levels of recall, whereas SAYU-
form a new training set and a tuning set. Each fold received VISTA tends to dominate for the high levels of recall. We
three hours of CPU time to run. SAYU and SAYU-VISTA   also compared the performance of SAYU-VISTA (average
could evaluate up to 300 clauses, before selecting a new seed AUCPR of 0.468) and MLNs (average AUCPR of 0.355) for
or clause head.                                       AUCPR for all levels of recall. Again, there is no signiﬁ-
  Table 1 reports the average AUCPR (recall ≥ 0.5)forCora cant difference. SAYU-VISTA has a higher variation in per
and the p-value for a two-tailed paired t-test between SAYU- fold AUCPR score than MLNs do. One reason for SAYU-
VISTA and the other two algorithms. SAYU-VISTA per-   VISTA’s increased performance for high recall is that we are
forms signiﬁcantly better than SAYU on this domain. Fig- expressly optimizing for this metric. MLNs also receive one
ure 1 shows precision-recall curves for all algorithms on advantage over SAYU and SAYU-VISTA in this domain, in
this dataset. We pooled results across all folds to gener- that they start with an expert deﬁned knowledge base.
ate the curves. SAYU-VISTA dominates SAYU through-      Mammography.    Following Davis et al. [2005b] we per-
out precision-recall space. However, MLNs have a slightly formed ten-fold cross validation on this dataset. We used four
higher average AUCPR than SAYU-VISTA does, although   folds for a training set and ﬁve folds as a tuning set. Each al-
the difference is not signiﬁcant. MLNs received an advantage gorithm could evaluate at most 300 clauses for a given seed
over SAYU and SAYU-VISTA in this task, as MLNs started (SAYU) or clause head (SAYU-VISTA).
with an expert knowledge base.                          For the previous two datasets, we initially started with a
  UW-CSE.  Following Richardson and Domingos [2006],  Bayesian network that only contained a feature for the target
we performed ﬁve-fold cross validation on the UW-CSE  predicate. However, in the Mammography domain we have
dataset. We used two folds for the training set and two folds access to a set of expert deﬁned features (from the NMD).
for a tuning set. Each approach could evaluate up to 10000 Furthermore, we could deﬁne a set of aggregate features as
clauses, before either selecting a new seed (SAYU) or a new Davis et al. [Davis et al., 2005b] did. Opposed to starting

                                                IJCAI-07
                                                  2723