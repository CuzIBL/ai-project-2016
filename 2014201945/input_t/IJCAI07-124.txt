                         Sequence Labelling in Structured Domains
                       with Hierarchical Recurrent Neural Networks

                Santiago Fernandez´  1 and  Alex Graves1   and  Jurgen¨ Schmidhuber1,2
                         1 IDSIA, Galleria 2, 6928 Manno-Lugano, Switzerland
                   2 TU Munich, Boltzmannstr. 3, 85748 Garching, Munich, Germany
                                  {santiago,alex,juergen}@idsia.ch

                    Abstract                          the performance at the top of the hierarchy. Nonetheless,
                                                      HMMs do not efﬁciently represent information at different
    Modelling data in structured domains requires es- scales, indeed they do not attempt to abstract information in
    tablishing the relations among patterns at multiple hierarchical form [Nevill-Manning and Witten, 1997].
    scales. When these patterns arise from sequential   The only paper known to us that describes a hierar-
    data, the multiscale structure also contains a dy- chy of CRFs has been presented recently by Kumar and
    namic component that must be modelled, partic-    Hebert [2005] for classifying objects in images. The hier-
    ularly, as is often the case, if the data is unseg- archy is not trained globally, but sequentially: i.e. estimates
    mented. Probabilistic graphical models are the pre- of the parameters in the ﬁrst layer are found and, then, with
    dominant framework for labelling unsegmented se-  these parameters ﬁxed, those in the second layer (and transi-
    quential data in structured domains. Their use re- tion matrices) are estimated.
    quires a certain degree of a priori knowledge about This paper uses a hierarchical approach to extend the appli-
    the relations among patterns and about the patterns cability of CTC to sequence labelling in structured domains.
    themselves. This paper presents a hierarchical sys- Hierarchical CTC (HCTC) consists of successive levels of
    tem, based on the connectionist temporal classi-  CTC networks. Every level predicts a sequence of labels and
    ﬁcation algorithm, for labelling unsegmented se-  feeds it forward to the next level. Labels at the lower lev-
    quential data at multiple scales with recurrent neu- els represent the structure of the data at a lower scale. The
    ral networks only. Experiments on the recognition error signal at every level is back-propagated through all the
    of sequences of spoken digits show that the system lower levels, and the network is trained with gradient descent.
    outperforms hidden Markov models, while making    The relative weight of the back-propagated error and the pre-
    fewer assumptions about the domain.               diction error at every level can be adjusted if necessary, e.g.
                                                      depending on the degree of uncertainty about the target label
1  Introduction                                       sequence at that level, which can depend on the variability
Assigning a sequence of labels to an unsegmented stream of in the data. In the extreme case in which only the error at the
data is the goal of a number of practical tasks such as speech top level is used for training, the network can, potentially, dis-
recognition and handwriting recognition. In these domains, cover structure in the data at intermediate levels that results
the structure at multiple scales is often captured with hierar- in accurate ﬁnal predictions.
chical models to assist with the process of sequence labelling. The next section brieﬂy introduces the CTC algorithm.
  Probabilistic graphical models, such as hidden Markov Section 3 describes the architecture of HCTC. Section 4 com-
models [Rabiner, 1989, HMMs] and conditional random   pares the performance of hierarchical HMMs and HCTC on a
ﬁelds [Lafferty et al., 2001, CRFs], are the predominant speech recognition task. Section 5 offers a discussion on sev-
framework for sequence labelling. Recently, a novel algo- eral aspects of the algorithm and guidelines for future work.
rithm called connectionist temporal classiﬁcation [Graves et Final conclusions are given in section 6.
al., 2006, CTC] has been developed to label unsegmented
sequential data with recurrent neural networks (RNNs) only. 2 Connectionist Temporal Classiﬁcation
Like CRFs, and in contrast with HMMs, CTC is a discrimi- CTC is an algorithm for labelling unsegmented sequential
nant algorithm. In contrast with both CRFs and HMMs, CTC data with RNNs only [Graves et al., 2006]. The basic idea
is a general algorithm for sequence labelling, in the sense that behind CTC is to interpret the network outputs as a prob-
CTC does not require explicit assumptions about the statisti- ability distribution over all possible label sequences, condi-
cal properties of the data or explicit models of the patterns tioned on the input data sequence. Given this distribution,
and the relations among them.                         an objective function can be derived that directly maximises
  Hierarchical architectures are often used with HMMs. the probabilities of the correct labellings. Since the objec-
There exist efﬁcient algorithms for estimating the parameters tive function is differentiable, the network can then be trained
in such hierarchies in a global way, i.e. a way that maximises with standard backpropagation through time [Werbos, 1990].

                                                IJCAI-07
                                                   774The algorithm requires that the network outputs at different 2.2 Training
times are conditionally independent given the internal state The objective function for CTC is derived from the principle
of the network. This requirement is met as long as there are of maximum likelihood. That is, it attempts to maximise the
no feedback connections from the output layer to itself or the log probability of correctly labelling the entire training set.
network.                                              Let S be such a training set, consisting of pairs of input and
  A CTC network has a softmax output layer [Bridle, 1990] target sequences (x, z), where the length of sequence z is less
with one more unit than the number of labels required for the than or equal to the length of the input sequence x. We can
task. The activation of the extra unit is interpreted as the prob- express the objective function to be minimised as:
ability of observing a “blank”, or no label at a given time step.                         
The activations of the other units are interpreted as the prob-  OML(S)=−           ln p(z|x) .       (5)
abilities of observing the corresponding labels. The blank                    (x,z)∈S
unit allows the same label to appear more than once consec-
utively in the output label sequence. A trained CTC network The network can be trained with gradient descent by dif-
produces, typically, a series of spikes separated by periods of ferentiating equation (5) with respect to the network outputs.
blanks. The location of the spikes usually corresponds to the This can be achieved by calculating the conditional probabil-
                                                             |
position of the patterns in the input data; however, the algo- ities p(z x) with a dynamic programming algorithm similar
rithm is not guaranteed to ﬁnd a precise alignment.   to the forward-backward recursions used for HMMs [Graves
                                                      et al., 2006].
2.1  Classiﬁcation                                      We deﬁne  αt(s) as the probability of having passed
For an input sequence x of length T we require a label se- through sequence l1...s and being at symbol s at time t:
quence l ∈ L≤T , where L≤T is the set of sequences of length
                                                            αt(s)=P  (π1...t : B(π1...t)=l1...s,πt = s|x)
≤ T  on the alphabet L of labels. We begin by choosing a
                                                                              t
label (or blank) at every timestep according to the probabil-                       
                                                                 =                yt  ,               (6)
ity given by the network outputs. This deﬁnes a probability                        πt
                                                                         :     
                      T                                                π      t =1
distribution over the set L of length T sequences on the           B(π1...t)=l1...s
extended alphabet L = L ∪{blank} of labels with blank
                                                      and βt(s) as the probability of passing through the rest of the
included. To disambiguate the elements of LT from label
                                                      label sequence (ls...|l|) given that symbol s has been reached
sequences, we refer to them as paths.                 at time t:
  The conditional probability of a particular path π ∈ LT is
                                                         β (s)=P  (π +1   : B(π   )=l    |l||π = s, x)
given by:                                                 t         t  ...T    t...T  s...  t
                                                                             T
                      T                                                        
                |         t  ∀  ∈  T                         =                 yt  .                 (7)
             p(π x)=     yπt , π  L             (1)                              πt
                                                                      :    
                      t=1                                            π     t =t+1
                                                                B(πt:T )=ls:|l|
      t
where yk is the activation of output unit k at time t.
  Paths can be mapped into label sequences by ﬁrst remov- To allow for blanks in the output paths, for every label se-
                                                                  ≤T                                    
ing the repeated labels, and then removing the blanks. For ex- quence l ∈ L we consider a modiﬁed label sequence l ,
                                                      with blanks added to the beginning and the end and inserted
ample, the path (a,-,a,b,-), and the path (-,a,a,-,-,a,b,b) would                             
both map onto the labelling (a,a,b). The conditional probabil- between every pair of labels. The length of l is therefore
                       ≤                              | |   | |
ity of a given labelling l ∈ L T is the sum of the probabilities l =2l +1. In calculating αt(s) and βt(s), we allow
of all the paths corresponding to it:                 all transitions between blank and non-blank labels, and also
                                                     those between any pair of distinct non-blank labels. The se-
               p(l|x)=        p(π|x)            (2)   quences can start with either blank or the ﬁrst symbol in l,
                      π∈B−1(l)                        and can end with either blank or the last symbol in l.
          T     ≤                                      From equations (1), (2), (6) and (7), we ﬁnd that for any t:
where B : L  → L  T is the many-to-one map implied by
the above process.                                                           |l|
  Finally, the output of the classiﬁer h(x) is the most proba-       p(l|x)=    αt(s)βt(s).           (8)
ble labelling for the input sequence:                                        s=1
               h(x) = arg max p(l|x).           (3)
                           l                          Noting that the same label (or blank) may be repeated several
                                                      times in a single labelling l, we deﬁne the set of positions
In general, ﬁnding this maximum is intractable, but there are                              
                                                      where label k occurs as lab(l,k)={s : l = k}, which
several effective approximate methods [Graves et al., 2006].                               s
                                                      may be empty, and differentiate equation (8) with respect to
The one used in this paper assumes that the most probable
                                                      the network outputs:
path will correspond to the most probable labelling:
                           ∗                                        |          
                 h(x) ≈B(π  )                   (4)              ∂p(l x)   1
                                                                        =            αt(s)βt(s).      (9)
                    ∗             |                               ∂yt     yt
             where π  =argmaxp(π   x)                                k     k s∈lab(l,k)
                            π
and π∗ is just the concatenation of the most active outputs at Setting l = z and differentiating the objective function,
every time-step.                                      we obtain the error signal received by the network during

                                                IJCAI-07
                                                   775                      seven                                                 seven

      level I                                                level I


                                                                                             se ven
                                       se ven
      level 2                                                level 2

      level 1                          s e v e n             level 1                         s e v e n


               unsegmented speech data                                unsegmented speech data

Figure 1: Schematic representation of an HCTC network la- Figure 2: Schematic representation of the error signal ﬂow on
belling an unsegmented input data stream. Outputs at each an HCTC network. The network is trained globally with gra-
level correspond to the probability of having detected a par- dient descent. External error signals injected at intermediate
ticular label (or emitting the blank label) at a particular time. levels are considered optional. If applied, their contribution
                                                      to the total error signal can be adjusted.
training:
                                                      pair (x,Z), where x =(x1,x2,...,xT ) is the external in-
     ML                          
  ∂O    ({(x, z)}, Nw)       1                        put sequence, and Z =(z1, z2,...,z ) is the set of target
                     =  yt −           α (s)β (s)                                      I
            t            k              t    t        sequences, where |z |≤T ∀i.
          ∂uk               Zt                                         i
                               s∈lab(z,k)               The system is trained globally with gradient descent. Fig-
                                               (10)   ure 2 illustrates the error signal ﬂow for an HCTC network.
       t     t       unnormalised    normalised
where uk and yk are the          and           out-   The error signal due to the target sequence z , received by
puts of the softmax layer, respectively, and                                                 i
                                                      network gi, is given by equation (10). To simplify the nota-
                     |l|                            tion, we deﬁne:
                Z  =     α (s)β (s)            (11)                         ∂OML({(v   , z )})
                  t       t   t                                     target     i      i  i           (12)
                     s=1                                          Δi     :=         t
                                                                                  ∂ui,k
is a normalization factor.
                                                      where the input sequence vi is the external input sequence
                                                      for network g1 and the output of network gi−1 for all levels
3  Hierarchical Connectionist Temporal                i =1  .
                                                                        target
   Classiﬁcation                                        The error signal Δi   is back-propagated into the net-
                                                      work gi, and then into all lower levels j : j<i. The con-
A hierarchy of CTC networks is a series g =(g1,g2,...,gI ) tribution of this term to the error signal at the unnormalised
of CTC networks, with network g1 receiving as input the ex-     t                                   
                                                      activation ui,k of the kth output unit of network gi : i = I is:
ternal signal and all other networks gi receiving as input the                                  
output of network gi−1. This architecture is illustrated in Fig-                     
                                                          backprop
                                                                     t              −          t
ure 1.                                                  Δi       =  yk     δm  wmk        wmk  yk   (13)
                                                                                       
  Note that every network gi in the hierarchy has a softmax           m∈M             k ∈K
output layer. This forces the hierarchical system to make
                                                      where M is the set of units in level i +1which are connected
decisions at every level and use them in the upper levels to
                                                      to the set of softmax output units, K,inleveli; δ is the
achieve a higher degree of abstraction. The analysis of the                                       m
                                                      error signal back-propagated from unit m ∈ M; w is the
structure of the data is facilitated by having output probabili-                                 mk
                                                      weight associated with the connection between units m ∈ M
ties at every level.                                        ∈        t
                                                      and k   K; and yk are the output activations of the softmax
  Because of the modular design, the ﬂexibility of the system layer at level i.
allows using other architectures (such as feeding the external
                                                        Finally, the total error signal Δi received by network gi is
inputs to several levels in the hierarchy) as long as the mathe- the sum of the contributions in equations (12) and (13). In
matical requirements of the CTC algorithm are met (see sec- general, the two contributions can be weighted depending on
tion 2).                                              the problem at hand. This is important if, for example, the
3.1  Training                                         target sequences at some intermediate levels are uncertain or
                                                      not known at all.
                                                                
In general, HCTC requires target sequences for every level in       target
the hierarchy, given a particular input sequence in the train-    Δi                   if i = I,
                                                           Δi =       target   backprop              (14)
ing set. Each example in the training set S consists of a         λiΔi     +Δi         otherwise,

                                                IJCAI-07
                                                   776              Digit   Phonemes                        Delta and Acceleration coefﬁcients were added giving a vec-
              ZERO    Z-II-R-OW                       tor of 39 coefﬁcients in total. For the network, the coefﬁcients
              ONE     W-AX-N                          were normalised to have mean zero and standard deviation
              TWO     T-OO                            one over the training set.
              THREE   TH-R-II
              FOUR    F - OW - R                      4.2  Setup
              FIVE    F - AY - V                      The  HMM    system was  implemented  with the HTK
              SIX     S-I-K-S                         Toolkit [Young et al., 2005]. Three-states left-to-right models
              SEVEN   S - EH-V-E-N
              EIGHT   EY - T                          were used for each one of the nineteen phonetic categories.
              NINE    N - AY - N                      A silence model and a “short pause” model (allowed at the
              OH      OW                              end of a digit) were also estimated. Observation probabili-
                                                      ties were modelled by a mixture of Gaussians. The grammar
Table 1: Phonetic labels used to model the digits in the exper- model allowed any sequence, preceded and followed by si-
iments.                                               lence, of one or more digits. Neither linguistic information
                                                      nor probabilities of partial phone sequences were included in
      ≤     ≤                                         the system.
with 0   λi   1. In the extreme case where λi =0,no     The number of Gaussians and the insertion penalty that op-
target sequence is provided for the network gi, which is free
                                         backprop     timised performance on the validation set was selected. Us-
to make any decision that minimises the error Δi re-  ing the training set, the number of Gaussians was increased in
ceived from levels higher up in the hierarchy. Because train- steps of two until performance on the validation set stabilised
ing is done globally, the network can, potentially, discover or, as in our case, decreased slightly (96 Gaussians). Every
structure in the data at intermediate levels that results in ac- time the number of Gaussians was increased, the parameter
curate predictions at higher levels of the hierarchy. estimation algorithm (HERest) was applied twice and results
                                                      were collected on the validation set with insertion penalties
4  Experiments                                        varying from 0 to -100 in steps of -5. The best performance
                                                      on the validation set was obtained with 80 Gaussians and an
In order to validate the HCTC algorithm we chose a speech insertion penalty of -85. The total number of parameters for
recognition task because this is a problem known to contain the HMM is 384,126.
structure at multiple scales. HMMs remain state-of-the-art A 2-level HCTC was used. The top, second level, had
in speech recognition, and therefore HCTC performance is as target the sequence of digits corresponding to the input
compared with that of HMMs.                           stream. The bottom, ﬁrst level, used as target the sequence
  The task was to ﬁnd the sequence of digits spoken in an of phonemes corresponding to the target sequence of digits
standard set of utterances using, at an intermediate level, the for the top level. Each CTC network uses the bi-directional
sequence of phonemes associated to every word.        LSTM  recurrent neural network [Graves and Schmidhuber,
                                                      2005; Graves et al., 2006], primarily because on a phoneme
4.1  Materials                                        recognition task better results than for other RNNs have been
The speaker-independent connected-digit database, TIDIG- reported [Graves et al.,2005]. Also, the CTC formalism
ITS [Leonard and Doddington, 1993], consists of more than is best realised with a bi-directional recurrent neural net-
25 thousand digit sequences spoken by over 300 men, women work [Schuster and Paliwal, 1997] because the network out-
and children. The database was recorded in the U.S. and it is put at a particular time depends on both past and future events
dialectically balanced. The utterances were distributed into in the input sequence.
a test and a training set. We randomly selected ﬁve percent Within each level, the input layer was fully connected to
of the training set to use as a validation set. This left 11922 the hidden layer and the hidden layer was fully connected to
utterances in the training set, 627 in the validation, and 12547 itself and the output layer. In the ﬁrst level, the input layer
in the test set.                                      was size 39, the forward and backward layers had 128 blocks
  The following eleven digits are present in the database: each, and the output layer was size 20 (19 phonetic categories
“zero”, “one”, “two”, “three”, ..., “nine” and “oh”. Utter- plus blank). In the second level, the input layer was size 20
ances consist of a sequence of one to seven digits. The repre- also, the forward and backward layers had 50 blocks each and
sentation of the digits at the phonetic level used in the exper- the output layer was size 12 (eleven digits plus the blank la-
iments can be seen in Table 1. Nineteen phonetic categories bel). The input and output cell activation functions were a
were used, nine of which are common to two or more digits. hyperbolic tangent. The gates used a logistic sigmoid func-
  Samples were digitized at 20 kHz with a quantization tion in the range [0, 1]. The total number of weights in the
range of 16 bits. The acoustic signal was transformed into HCTC network is 207,852.
mel frequency cepstral coefﬁcients (MFCC) with the HTK  Training of the HCTC network was done by gradient de-
Toolkit [Young et al., 2005]. Spectral analysis was carried scent with weight updates after every training example. In
out with a 40 channel Mel ﬁlter bank from 130 Hz to 6.8 kHz. all cases, the learning rate was 10−4, momentum was 0.9,
A pre-emphasis coefﬁcient of 0.97 was used to correct spec- weights were initialized randomly in the range [−0.1, 0.1]
tral tilt. Twelve MFCC plus the 0th order coefﬁcient were and, during training, Gaussian noise with a standard devia-
computed on Hamming windows 25.6 ms long, every 10 ms. tion of 1.0 was added to the inputs to improve generalisation.

                                                IJCAI-07
                                                   777            System         LER                        a partially trained network to explore alternatives that max-
            HMM            0.89 %                     imise performance at the top level of the hierarchy.
            HCTC (λ1 =1)   0.61 ± 0.04 %                In the future, we would also like to explore ways of im-
            HCTC (λ1 =0)   0.51 %                     proving system scalability. For example, large vocabulary
                                                      speech recognition requires systems that work with many
Table 2: Label Error Rate (LER) on TIDIGITS. Results for thousands of words. Using output layers of that size for
HCTC (λ1 =1) are means over 5 runs with different random HCTC is not currently practical. Instead of assigning a unique
weights, ± standard error. This gives a 95 % conﬁdence in- output unit to every possible label, other methods can be ex-
terval of (0.50; 0.72), with a t-value of 2.8 for 4 degrees of plored such as assigning labels to speciﬁc activation patterns
freedom and a two-tailed test. For HCTC (λ1 =0) the best in a group of output units. This will require modifying CTC’s
result obtained is shown; this coincides with the best result training algorithm.
obtained with λ1 =1.                                    Another aspect we would like to investigate is the poten-
                                                      tial of HCTC for word spotting tasks. As a discriminant
                                                      method, HCTC may improve detection rates due to its capa-
  Performance was measured as the normalised edit distance
                                                      bility of discriminating between keywords and non-speech or
(label error rate; LER) between the target label sequence and
                                                      other speech events. Besides this, HCTC provides estimates
the output label sequence given by the system.
                                                      of a posteriori probabilities that can help to directly assess
4.3  Results                                          the level of conﬁdence in the predictions. This is in contrast
                                                      to generative approaches, such as HMMs, which use unnor-
Performance rates for the systems tested can be seen in Ta- malized likelihoods.
ble 2. The best HMM-based system achieved an error rate of
0.89% in continuous digit recognition. The label error rate
for HCTC was on average 0.61 % (95 % conﬁdence interval 6 Conclusion
of (0.50; 0.72)). This is an improvement of more than 30 % This paper has presented the HCTC algorithm, which uses a
with respect to the HMM and with approximately half the hierarchy of recurrent neural networks to label sequences of
number of parameters.                                 unsegmented data in structured domains. HCTC is capable
  The best results without injecting the error signal associ- of ﬁnding structure at multiple levels and using it to achieve
ated to the phoneme labels (λ1 =0) was 0.51 %, which is accurate predictions further up in the hierarchy. The use of
as good as the best performance achieved with λ1 =1.In neural networks offers a ﬂexible way of modelling the domain
this case, however, the pattern of activations of seven output while at the same time allowing the system to be trained glob-
units (instead of twenty) was enough to encode the dynamic ally. Experimental results demonstrated that this approach
structure of the data at the lower level in order to make ac- outperformed hidden Markov models on a speech recognition
curate predictions at the top level of the sequence of digits task.
in the utterance (see Figure 3). Some of these seven output
units become active/inactive at speciﬁc points in time for each Acknowledgments
digit pattern. And some of them are active for different digits.
Nonetheless, they cannot be associated directly to phonetic This research was funded by SNF grant 200021-111968/1.
categories and might encode another type of information in
the signal.                                           References
                                                      [Bridle, 1990] John S. Bridle. Neurocomputing: Algorithms,
5  Discussion and Future Work                            architectures and applications, chapter Probabilistic in-
Inserting levels in the hierarchy without a corresponding tar- terpretation of feedforward classiﬁcation network outputs,
get label sequence is interesting if the target labels are not pages 227–236. Springer-Verlag, 1990.
known, or, for instance, if the phonetic labels used are sus- [Graves and Schmidhuber, 2005] Alex Graves and Jurgen¨
pected to be invalid due to, e.g., the presence of dialectal vari- Schmidhuber. Framewise phoneme classiﬁcation with
ations in the dataset. If the sources of variability in the data bidirectional LSTM and other neural network architec-
cannot be speciﬁed accurately, a system with less constraints tures. Neural Networks, 18(5–6):602–610, June/July 2005.
might be capable of making more effective decisions.
                                                      [               ]
  Experimentally, this means that the system will be more Graves et al., 2005 Alex Graves, Santiago Fernandez,´ and
difﬁcult to train for a particular goal. HCTC with and without Jurgen¨ Schmidhuber. Bidirectional LSTM networks for
target label sequences at the phonetic level achieved similar improved phoneme classiﬁcation and recognition. In Pro-
performance, albeit by different means. Nevertheless, HCTC ceedings of the 2005 International Conference on Artiﬁ-
                                                         cial Neural Networks, Warsaw, Poland, 2005.
with λ1 =0suffered to a larger extent from the problem of
local minima.                                         [Graves et al., 2006] Alex Graves, Santiago Fernandez,´
  Assuming that reasonable target label sequences can be Faustino Gomez, and Jurgen¨ Schmidhuber. Connectionist
speciﬁed a priori for intermediate levels, a possible solution temporal classiﬁcation: Labelling unsegmented sequence
is to train HCTC until the error has decreased signiﬁcantly, data with recurrent neural networks. In Proceedings of
and then remove the contribution to the error signal of the the 23rd International Conference on Machine Learning,
target label sequences for intermediate levels. This will free Pittsburgh, PA, USA, 2006.

                                                IJCAI-07
                                                   778