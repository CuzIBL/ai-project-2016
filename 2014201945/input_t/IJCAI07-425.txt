                                    Distributed Data Mining:
                          Why Do More Than Aggregating Models

                               Mohamed Aoun-Allah       and  Guy Mineau
                        Computer Science and Software Engineering Department
                                 Laval University, Quebec City, Canada
                          {Mohamed.Aoun-Allah, Guy.Mineau}@ift.ulaval.ca


                    Abstract                          base DB  =  ∪iDBi; it is used only as a reference to as-
                                                      sess the potential loss of accuracy of our method since, by
    In this paper we deal with the problem of min-    assumption, we stated that we could not process DB because
    ing large distributed databases. We show that the of download/processing time constraints.
    aggregation of models, i.e., sets of disjoint clas- This paper proceeds as follows. We present in sections 2
    siﬁcation rules, each built over a subdatabase is a survey of some well known model aggregation techniques.
    quite enough to get an aggregated model that is   Then, in section 3, we present our solution for distributed data
    both predictive and descriptive, that presents ex- mining (DDM) by model aggregation (DDM-MA) based on a
    cellent prediction capability and that is conceptu- majority vote that is pondered by some conﬁdence coefﬁcient.
    ally much simpler than the comparable techniques. Section 4 introduces a conceptual comparison between our
    These results are made possible by lifting the dis- method and those found in the literature. In section 5, we
    joint cover constraint on the aggregated model and present experimental results which prove the viability of our
    by the use of a conﬁdence coefﬁcient associated   method. We will show that it bares comparable accuracy rates
    with each rule in a weighted majority vote.       while being simpler that other DDM methods. We ﬁnally
                                                      present a conclusion and our future work.
1  Introduction
This paper deals with the problem of mining several large and 2 Model aggregation existing techniques
geographically distributed databases (DBi) with the goal of As we present in this paper a technique developed in a dis-
producing a set of classiﬁcation rules that explains the vari- tributed data mining perspective, we will ignore some non
ous groupings found in the observed data. The result of this relevant techniques as the Ruler System [Fayyad et al., 1993]
mining is both a predictive and descriptive meta-classiﬁer. In [Fayyad et al., 1996] that was developed for the aggregation
other words, we aim at producing a model which is not only of several decision trees build on the same data set in a cen-
capable of predicting the class of new objects, but which is tralized system, the Distributed Learning System [Sikora and
also able to explain the choices of its predictions. We believe Shaw, 1996] developed in a context of information manage-
that this kind of models, based on classiﬁcation rules, should ment system that builds a distributed learning system, and
also be easy to understand by humans, which is also one of the Fragmentation Approach [W¨uthrich, 1995] which uses
our objectives. Also, our application context is one where it probalistic rules. Also, we will ignore purely predictive tech-
is impossible to gather all these databases on the same site, niques such as bagging [Breiman, 1996], boosting [Schapire,
and this, either because of downloading time, or because of 1990], stacking [Tsoumakas and Vlahavas, 2002],andthear-
the difﬁculty to mine the aggregated database.        biter and combiner methods [Chan, 1996], [Prodromidis et
  In the literature, we ﬁnd very few distributed data mining al., 2000].
techniques which are both predictive and descriptive. The
majority of them try to produce a meta-classiﬁer in the form 2.1 The MIL algorithm
of a set of rules with disjoint cover, i.e., where an object is The MIL algorithm (Multiple Induction Learning) was ini-
covered by one and only one rule. We will show in this pa- tially proposed by Williams [Williams, 1990] in order to re-
per that this constraint of disjoint cover is not necessary to solve conﬂicts between conﬂictual rules in expert systems.
produce a reliable meta-classiﬁer and furthermore, introduces Authors of [Hall et al., 1998a; 1998b] took again the tech-
unnecessary complexity to the mining technique. Therefore, nique of Williams [Williams, 1990] to aggregate decision
we propose a simpler technique where an object can be cov- trees built in parallel and transformed beforehand into rules.
ered by several rules, and where the lifting of this constraint The process of aggregation proposed by these authors is a re-
enables us to produce a conceptually very simple classiﬁer grouping of rules accompanied by a process of resolution of
with good prediction capability as will be shown below. the possible conﬂicts between them. It should be noted that
  The performance of our meta-classiﬁer (from a prediction this resolution of the conﬂicts treats only one pair of conﬂict
point of view) is compared to C4.5 applied on the whole data- rules at a time. Two rules are considered in conﬂict when

                                                IJCAI-07
                                                  2645their premises are consistent while they produce two differ- accompanied by the values necessary to calculate the measure
ent classes [Williams, 1990] (called conﬂict of type I) , or associated with each rule.
when the conditions of the premises overlap partially [Hall et However, in [Hall et al., 1999], the authors show that in
al., 1998a] (called conﬂict of type II) or when the rules have the extreme case the property of invariant-partitioning could
the same number of predicates with different values for con- not be satisﬁed. Thus, they prove that the precision of the
ditions and they classify objects into the same class [Hall et aggregate rule set can be very different from the precision of
al., 1998b] (called conﬂict of type III).The conﬂict resolution the rules built on the training set. Moreover, the authors show
consists in either specializing one or the two rules in conﬂict that conﬂicts between rules can be solved, as described by
(conﬂicts type I and II), or in adjusting the value of the condi- [Hall et al., 1998b] and [Williams, 1990].
tion, i.e., the test boundary, for the conﬂicts of type II and III In addition, [Hall et al., 1999] proposes a new type of con-
and eventually in combining the two rules in conﬂict (conﬂict ﬂict between rules: a rule whose premise contains some inter-
of type III). In certain cases (conﬂicts of type I and II), new val that overlaps an interval that is contained in the premise
rules are added based on the training sets to recover the cover of a second rule. In this case, a more general rule is created
lost by the specialization process.                   by combining the two conﬂicting rules and by adjusting the
                                                      border values of these intervals.
2.2  The DRL system (Distributed Rule Learner)
The DRL technique (Distributed Rule Learner) [Provost and 3 The proposed model aggregation technique
Hennessy, 1996] was conceived based on the advantage of the The proposed technique is very simple. We build in parallel
invariant-partitioning property [Provost and Hennessy, 1994]. over each distributed DBi a model, i.e., a set of classiﬁcation
The DRL technique begins by partitioning the training data E rules Ri, called base classiﬁer. Figure 1 shows an example
into nd disjoined subsets, assigns each one (Ei) to a different of such rules.
machine, and provides the infrastructure for the communica-
                                                      IF adoption_of_the_budget_resolution =           n
tion between different learners (named RL). When a rule r
                                                      IF physician_fee_freeze = y
satisﬁes the evaluation criterion for a subset of the data (i.e.,
                                                    THEN CLASS: republican
f (r, Ei,nd) ≥ c ; f being an evaluation function1 of a rule
and c a constant), it becomes a candidate to satisfy the global
                                                      IF adoption_of_the_budget_resolution =           u
evaluation criterion; the extended invariant-partitioning prop-
                                                      IF physician_fee_freeze = y
erty guarantees that each rule which is satisfactory on the
                                                      THEN CLASS: democrat
whole data set will be acceptable at least on one subset. When
a local learner discovers an acceptable rule, it sends the rule
to the other machines so that they update its statistics on the Figure 1: An example of rules contained in a base classiﬁer.
remainder of the examples. If the rule meets the global evalu-
ation criterion (f(r, E) ≥ c; f being the principal evaluation Then, we compute for each rule a conﬁdence coefﬁcient
function and c a constant), it is asserted as a satisfactory rule. (see below for details). Finally, in a centralized site, base
In the opposite case, its local statistics are replaced by the classiﬁers are aggregated in the same set of rules (R = ∪iRi)
global statistics and the rule is made available to be special- which represents our ﬁnal model, called meta-classiﬁer.The
ized some more. The property of invariant-partitioning guar- global algorithm of our distributed data mining technique is
antees that each satisfactory rule on the whole data set will be described by Figure 2.
found by at least one of the RLs.
                                                        1. Do in parallel over each database    DBi
2.3  Combining rule sets generated in parallel             (a) Apply on  DBi a  classification
                    [             ]                           algorithm producing a set     of disjoint
The work presented by Hall et al., 1999 is a mixture of the   cover rules.    The produced set is
last two techniques presented above, i.e., that of [Williams,
1990], [Hall et al., 1998b] and [Provost and Hennessy, 1996].             Ri = {rik | k ∈ [1..ni]}
In details, they associate to each rule a measurement of its
                                                              where  ni is the number of   rules;
”quality” which is based on its prediction precision as well as                  r
on the number and the type of the examples that it covers. (b) Compute for each   ik a confidence
                                                              coefficient   cr  (see hereafter);
  The technique suggested in [Hall et al., 1999] is based                    ik
                                                        2. In a central site create:
on the use of what [Provost and Hennessy, 1996] proposes                         [
    §
(see 2.2), with a small difference where the deletion of the                R =      Ri
rule from the space of rules under consideration is made only                   i=1...nd
when the rule classiﬁes all the data of the various distributed  nd
bases, which is the case when its measure f(r, E) is lower where    is the  number of distributed
                                                          databases.
than a certain threshold. It should be noted that each rule
does not ”travel” alone from one site to another, but is indeed
                                                         Figure 2: Algorithm of the proposed DDM technique.
  1This rule evaluation function could be for example the Laplace
precision estimator [Segal and Etzioni, 1994][Webb, 1995]. Since different rule sets are going to be merged together

                                                IJCAI-07
                                                  2646whereas they are issued from different data subsets, and since the adaptation of the technique of Williams [Williams, 1990]
each rule r has its proper error rate Er and coverage n,we in order to treat distributed bases implies an increase in the
compute for each rule a conﬁdence coefﬁcient cr. This conﬁ- volume of data exchanged between the various sites. Indeed,
dence coefﬁcient is computed in straightforward manner from on the one hand, each rule travels accompanied by the index
the lower bound of an error rate conﬁdence interval proposed of the covered objects and, on the other hand, in the event
by [Langford, 2005].Wedeﬁneditasone minus the worst   of conﬂict, all the objects covered by one of the two rules in
error rate in (1 − δ) of the time :                   conﬂict must be downloaded from the training site to the site
                                                      resolving the conﬂict.
              cr =1−  Bin−(n, nEr,δ)

                  def                                 4.2  The DRL system
where Bin−(n, k, δ) = min{r  :1−  Bin(n, k, r) ≥ δ}
                       
    Bin(n, k, r) def= k n ri(1 − r)n−i                The most signiﬁcant disadvantage of the DRL  system
and                 i=0 i                             [Provost and Hennessy, 1996] is its execution time. Indeed,
       R
  Since  is not a set of disjoint cover rules where an object when a rule is considered to be acceptable by a given site, it
is covered by a unique rule, we explain hereafter how we can must go across to all the other sites. In other words, any ac-
use this meta-classiﬁer as a predictive and descriptive model. ceptable rule on a site must classify all the data of all the other
               R                                      sites. Thus, the rule must, on the one hand,“travel” through
3.1  The use of   as a predictive model               all the sites, and on the other hand, classify the data of each
The set R represents the aggregation of all base classiﬁers site. If a rule is not considered to be satisfactory on the whole
(R = ∪iRi). This rule set is used as a predictive model as data set, this rule is specialized and the process starts again if
well as a descriptive one. From a predictive point of view, it is considered to be locally acceptable. It is clear that this
the predicted class of a new object is the class predicted by a process could be very time consuming.
majority vote of all the rules that cover it, where the rules are
weighted by their conﬁdence coefﬁcients2. It should be noted 4.3 Combining rule sets generated in parallel
that, contrarily to what is identiﬁed in the literature (see §2),
we have restricted the notion of rules in conﬂict to be those As for the system of combining rule sets generated in parallel,
that cover the same object but with different classiﬁcation re- it is identical to the previous one with a little difference: any
sults. If several rules cover the same object and predict the rule generated in a given site must cross over to all the other
same class, we do not consider them as being in conﬂict. sites. Thus, the number of rules traveling between the various
  It is to be noted that any object can be covered by at most sites is more signiﬁcant than the number of rules of the DRL
nd rules, knowing that nd is the number of sites.     system. Consequently, it is clear that this technique is slower
                                                      than the preceding one.
3.2  The use of R as a descriptive model
As a classiﬁcation system is often developed as support to 4.4 The proposed technique
decision-making, the different rules covering an object may To overcome the problems of MIL technique, the proposed
be proposed to the user who could then judge, from his ex- one is based on a majority vote that is known to be rather a ro-
pertise, of their relevance, helped by a conﬁdence coefﬁcient. bust model against noisy data. Indeed, the prediction process
Presenting to a decision maker more than one rule may have gives good results especially in noisy bases (see §5below).
its advantages since it may provide a larger and more com- In the proposed technique (see §3) rules “travel” only in
plete view of the ”limits” of each class. We bring to mind, one way, from the distributed database site to the central site
that in machine learning, the limit which deﬁnes separation and the amount of data is almost minimal where a rule is aug-
between various classes is generally not unique nor clear cut, mented by no more than its conﬁdence coefﬁcient. Thus the
and consequently, several rules producing the same class can problem of excess communication found in the DRL system
represent the ”hyper-planes” separating the various classes, and its successor is avoided.
providing various views on these data.                  From  an execution point of view, an asymptotic analy-
                                                      sis was conducted in [Aounallah, 2006] and [Aounallah and
4  A conceptual comparison                            Mineau, 2006] of our technique and those presented in §2of
4.1  The MIL technique                                this paper. This asymptotic analysis shows clearly that in the
                                                      worst case our technique is faster than existing ones.
                 [               ][               ]
The MIL technique Hall et al., 1998a Hall et al., 1998b In the best case, we expect our technique to be at least com-
suffers from several problems. First of all, the process of con- parable to existing ones. Since having no conﬂicts between
ﬂict resolution only specializes the rules based on the classi- different base classiﬁers is very rare, we believe that our tech-
ﬁcation rules data sets. The generated rules could show poor nique is faster than existing ones because our technique does
classiﬁcation ability when they are applied to new objects, es- not conduct any conﬂict resolution step.
pecially in the case of very noisy training data. In addition,
                                                        Moreover, the proposed technique is no more than a sim-
  2However, in an unlikely tie situation, we propose to carry out a ple aggregation of base classiﬁers. Consequently, there is no
simple majority vote. In very rare cases, when the simple majority doubt that it is conceptually by far simpler that existing com-
vote also leads to a tie, we choose the majority class in the different parable ones, i.e., it should be faster and simpler to implement
training sets.                                        than those found in the literature (see §2).

                                                IJCAI-07
                                                  26475  An empirical comparison
                                                      Table 1: Comparison between R and R (Original data sets).
To evaluate the performance of our DDM technique, we con-            
                                                                   R     Lower B.  Upper B.   R    Cmp.
ducted some experiments in order to assess its prediction (ac-
                                                       BCW        7.0%     3.2%     10.8%    6.5%
curacy) rate. We compared it to a C4.5 algorithm built on the Chess 0.9%   0.2%     1.6%     2.5%    -
whole data set, i.e., on the aggregation of the distributed data-
                                R                     Crx        18.4%   12.5%     24.3%   19.6%
bases. This C4.5, produces a rule set , which is used as a Iono   20.5%   12.1%     28.9%   19.3%
reference for its accuracy rate since we assumed in the intro- Mush 0.4%   0.1%     0.7%     0.3%
duction that it is impossible to gather all these bases onto a Pima 23.4% 17.4%     29.4%   26.6%
single site, and this, either because of downloading time, or Tic-tac-toe 18.3% 13.4% 23.2% 21.3%
because of the difﬁculty to learn from the aggregated base be- Vot e 3.0%  0.1%     5.9%     3.0%
                          
cause of its size. The rule set R is considered to be the ideal Wdbc 6.3%  2.3%     10.3%    4.9%
case, where theoretically it is not possible to perform better
than a model built on the whole data set.
  The conducted experiments have been tested on nine     Table 2: Comparison between R and R (10% noise).
data sets: chess end-game (King+Rook versus King+Pawn),            R    Lower B.  Upper B.   R    Cmp.
Crx, house-votes-84, ionosphere, mushroom, pima-indians- BCW      8.2%     4.1%     12.3%    5.3%
diabetes, tic-tac-toe, Wisconsin Breast Cancer (BCW)[Man- Chess   0.9%     0.2%     1.6%     3.4%    -
gasarian and Wolberg, 1990] and Wisconsin Diagnostic   Crx        14.7%    9.3%     20.1%   20.2%    -
Breast Cancer (WDBC), taken from the UCI repository    Iono       25.0%   16.0%     34.0%   18.2%
[Blake and Merz, 1998]. The size of these data sets varies Mush   0.3%     0.0%     0.6%     0.3%
from 351 objects to 5936 objects (Objects with missing val- Pima  33.3%   26.6%     40.0%   26.0%    +
ues have been deleted). Furthermore, in order to get more Tic-tac-toe 20.8% 15.7%   25.9%   22.1%
realistic data sets, we introduced noise in the nine aforemen- Vot e 3.7%  0.5%     6.9%     3.0%
tioned databases, and this by reversing the class attribute3 of Wdbc 7.7%  3.3%     12.1%    8.5%
successively 10%, 20%, 25% and 30% of objects. Hence,
since for each data set we have, in addition to the original set,
4 other noisy sets, giving a total number of databases of 45. which produces a decision tree that is then directly trans-
  In order to simulate a distributed environment, the data sets formed into a set of rules. The conﬁdence coefﬁcient of each
have been divided as follows. We divided each database into rule was computed using the program offered by Langford
a test set with proportion of 1/4. This data subset was used [Langford, downloaded in 2006] with a basis of 95% conﬁ-
                                                                      δ =0.05
as a test set for our meta-classiﬁer and for R , our reference dence interval (i.e., ).
                                          3/4           In order to assess the prediction capability of our technique
classiﬁer. The remaining data subset (of proportion ), was                                    
divided randomly into 2, 3, 4 or 5 data subsets in order to we compared its prediction rate to the one of R over the 45
simulate distributed databases. The size of these bases was aforementioned data sets. Table 1 to 5 detail the results ob-
                                                      tained. The third and the forth columns of these tables con-
chosen to be disparate and in such a way so there was a sig-                                       
niﬁcant difference between the smallest and the biggest data tain respectively the lower and the upper bound of R error
subset. As an example of such subdivision see Figure 3. rate conﬁdence interval computed at 95% conﬁdence. The
                                                      last column contains:
            mushroom.data                               • “+” if our technique outperforms R,
             (5936objets)
                                                        • “-” if R outperforms our meta-classier and
                                                        • a blank if the two techniques are statistically compara-
  mushTest.test        mushTest.data                      ble.
(1/4 1484objets)      (3/44452objets)
                                                        From these tables, we can see that our meta-classier perfor-
                                                      mance is very comparable to the one of R since in 35 cases
     mush2.data( 1635obj .) mush1.data( 1035obj .) mush3.data( 1782obj .)

            mush1.test(34 17obj .) mush2.data + mush3.data                         R    R
            mush2.test(2 817obj .) mush1.data + mush3.data Table 3: Comparison between and (20% noise).
                                                                     
            mush3.test(2 670obj .) mush1.data + mush2.data         R     Lower B.  Upper B.   R    Cmp.
                                                       BCW        8.8%     4.6%     13.0%    6.5%
                                                       Chess      2.3%     1.3%     3.3%     2.8%
                                                       Crx        21.5%   15.2%     27.8%   15.3%
Figure 3: Example of subdivision for a database from the Iono     38.6%   28.4%     48.8%   34.1%
UCI.                                                   Mush       0.3%     0.0%     0.6%     0.5%
                                                       Pima       28.1%   21.7%     34.5%   28.1%
  For the construction of the base classiﬁers we used C4.5 Tic-tac-toe 18.8% 13.9%  23.7%   20.4%
release 8 [Quinlan, 1996][Quinlan, downloaded in 2004] Vot e      8.1%     3.5%     12.7%    2.2%    +
                                                       Wdbc       12.7%    7.2%     18.2%   14.8%
  3Please note that all data sets have a binary class attribute.

                                                IJCAI-07
                                                  2648                                                      ﬁdence coefﬁcient used in the process uses these rules with a
                            R     R
   Table 4: Comparison between and   (25% noise).     weight that reﬂects their individual prediction power.
               
             R     Lower B. Upper B.   R     Cmp.       Moreover, since the granularity of the majority vote is at
 BCW        7.1%    3.3%     10.9%    7.6%            rule level (instead of classiﬁer level), the meta-classiﬁer R
 Chess      4.3%    2.9%      5.7%    4.8%            can be used as a descriptive model, where the predictive class
 Crx        23.3%   16.8%    29.8%    25.8%           of an object is described by the rules covering it.
 Iono       40.9%   30.6%    51.2%    36.4%             Due to these good results, we can imagine that our tech-
 Mush       0.3%    0.0%      0.6%    0.5%            nique could be applied on very large centralized databases
 Pima       38.5%   31.6%    45.4%    37.0%
 Tic-tac-toe 20.8%  15.7%    25.9%    24.6%           that could be divided into smaller ones before applying our
 Vot e      8.1%    3.5%     12.7%    3.0%     +      technique, rather than applying a data mining tool over the
 Wdbc       9.9%    5.0%     14.8%    14.1%           centralized database; this is what we propose to explore in a
                                                      near future.
                                                        Furthermore, we propose, on the one hand, to test the pro-
                            R     R                  posed technique on n-ary databases and, on the other hand, to
   Table 5: Comparison between and   (30% noise).     compare it experimently to exiting techniques.
             R    Lower B. Upper B.   R     Cmp.
 BCW        10.0%   5.5%     14.5%    7.1%
 Chess      8.1%    6.2%     10.0%    4.9%     +      References
 Crx        33.7%   26.4%    41.0%    24.5%    +      [Aounallah and Mineau, 2006] Mohamed Aounallah  and
 Iono       40.9%   30.6%    51.2%    46.6%              Guy Mineau.  Le forage distribu´e des donn´ees : une
 Mush       1.8%    1.1%      2.5%    0.5%     +         m´ethode simple, rapide et efﬁcace. Revue des Nouvelles
 Pima       36.5%   29.7%    43.3%    36.5%              Technologies de l’Information, extraction et gestion des
 Tic-tac-toe 24.6%  19.2%    30.0%    25.0%              connaissances, 1(RNTI-E-6):95–106, 2006.
 Vot e      10.4%   5.3%     15.5%    4.4%     +
 Wdbc       12.7%   7.2%     18.2%    16.9%           [Aounallah, 2006] Mohamed Aounallah. Le forage distribue´
                                                         des donnees´ : une approche basee´ sur l’agregation´ et le
                                                         rafﬁnement de modeles` . PhD thesis, Laval University, Feb-
over 45 its error rate is statistically comparable and only in 3 ruary 2006.
                    
cases it is worst than R . Moreover, surprisingly, our meta- [           ]
                                                      Blake and Merz, 1998 C.L. Blake   and  C.J.  Merz.
classiﬁer could outperform R in 7 cases; this is especially the UCI repository of machine learning databases.
case when noise in distributed data sets is important. In these http://www.ics.uci.edu/∼mlearn/MLRepository.html,
cases, we could easily see the advantage of using a noise ro- 1998.
bust model as the weighted majority vote in a non disjoint
                                                      [            ]
cover rule set, instead of using a single model with disjoint Breiman, 1996 Leo Breiman. Bagging predictors. Machine
cover rule set.                                          Learning, 1996.
  These results, prove also the viability of the conﬁdence co- [Chan, 1996] Philip Kin-Wah Chan. An extensible meta-
efﬁcient proposed in this paper.                         learning approach for scalable and accurate inductive
                                                         learning. PhD thesis, Columbia University, 1996.
6Conclusion                                           [Fayyad et al., 1993] U.M. Fayyad, N. Weir, and S. Djorgov-
                                                         ski. Skicat: A machine learning system for automated cat-
The objective of this paper is to present a very simple dis-
                                                         aloging of large scale sky surveys. In Machine Learning:
tributed data mining technique (DDM) by model aggregation
                                                         Proceedings of the Tenth International Conference, pages
(MA). With this intention, we presented, on the one hand, a
                                                         112–119, San Mateo, CA, 1993. Morgan Kaufmann.
rapid survey of existing model aggregation techniques which
are most comparable to ours. And on the other hand, we pre- [Fayyad et al., 1996] Usama M. Fayyad, S. George Djorgov-
sented a description of our DDM-MA technique.            ski, and Nicholas Weir. Advances in Knowledge Discov-
  Throughout this paper, we have shown that the proposed ery and Data Mining, chapter Automating the analysis
DDM  technique is conceptually by far simpler that existing and cataloging of sky surveys, pages 471–493. AAAI
comparable techniques. Indeed, it consists of a simple aggre- Press/The MIT Press, Menlo Park, California, 1996.
gation of distributed models (base classiﬁers).       [Hall et al., 1998a] O. Lawrence Hall, Nitesh Chawla, and
  Experiments demonstrate that our technique, from a pre- W. Kevin Bowyer. Combining Decision Trees Learned in
diction point of view, performs as well as or even better, than Parallel. In Working notes of KDD, 1998.
a classiﬁer built over the whole data set, R, the theoretically
                                                      [              ]
ideal case since it is built over the whole data. Our meta- Hall et al., 1998b O. Lawrence Hall, Nitesh Chawla, and
classiﬁer R could outperform R due to the weighted major- W. Kevin Bowyer. Decision tree learning on very large
ity vote pondered by a conﬁdence coefﬁcient associated to data sets. In IEEE International Conference on Systems,
each rule. This conﬁdence coefﬁcient is based on the lower Man, and Cybernetics, 1998, volume 3, pages 2579–2584,
bound of an error rate conﬁdence interval proposed by [Lang- oct 1998.
ford, 2005]. In other words, such a mojority vote over imper- [Hall et al., 1999] O. Lawrence Hall, Nitesh Chawla, and
fect rules gives very good predictive results because the con- W. Kevin Bowyer. Learning rules from distributed

                                                IJCAI-07
                                                  2649