             Learning Semantic Descriptions of Web Information Sources∗
                            Mark James Carman       and  Craig A. Knoblock
                                    University of Southern California
                                     Information Sciences Institute
                                          4676 Admiralty Way,
                                       Marina del Rey, CA 90292
                                      {carman, knoblock}@isi.edu

                    Abstract                          input and what type of data it produces as output. In previ-
                                                      ous work [Heß and Kushmerick, 2003; Lerman et al., 2006],
    The Internet is full of information sources provid- researchers have addressed the problem of classifying the at-
    ing various types of data from weather forecasts to tributes of a service into semantic types (such as zipcode).
    travel deals. These sources can be accessed via   Once the semantic types for the inputs are known, we can
    web-forms, Web Services or RSS feeds. In order    invoke the service, but are still not able to make use of the
    to make automated use of these sources, one needs data it returns. To do that, we need also to know how the out-
    to ﬁrst model them semantically. Writing seman-   put attributes relate to the input. For example, a weather ser-
    tic descriptions for web sources is both tedious and vice may return a temperature value when queried with a zip-
                                                      code. The service is not very useful, until we know whether
    error prone. In this paper we investigate the prob- the temperature being returned is the current temperature, the
    lem of automatically generating such models. We   predicted high temperature for tomorrow, or the average tem-
    introduce a framework for learning Datalog deﬁ-   perature for this time of year. These three possibilities can
    nitions for web sources, in which we actively in- be described by Datalog rules as follows: (Note that the $-
    voke sources and compare the data they produce    symbol is used to distinguish the input attributes of a source.)
    with that of known sources of information. We per- 1 source($zip, temp) :- currentTemp(zip, temp).
    form an inductive search through the space of plau- 2 source($zip, temp) :- forecast(zip, temp).
    sible source deﬁnitions in order to learn the best
                                                      3 source($zip, temp) :- averageTemp(zip, temp).
    possible semantic model for each new source. The
    paper includes an empirical evaluation demonstrat- The expressions state that the input zipcode is related to the
    ing the effectiveness of our approach on real-world output temperature according to domain relation called cur-
    web sources.                                      rentTemp, forecast,andaverageTemp respectively, each of
                                                      which is deﬁned in some domain ontology. In this paper we
                                                      describe a system capable of inducing such deﬁnitions au-
1  Introduction                                       tomatically. The system leverages what it knows about the
We are interested in making use of the vast amounts of in- domain, namely the ontology and a set of known sources, to
formation available as services on the Internet. In order to learn a deﬁnition for a newly discovered source.
make this information available for structured querying, we
must ﬁrst model the sources providing it. Writing source de- 1.1 An Example
scriptions by hand is a laborious process. Given that different We introduce the problem of inducing deﬁnitions for online
services often provide similar or overlapping data, it should sources by way of an example. In the example we have four
be possible to use knowledge of previously modeled services semantic types, namely: zipcode, distance, latitude and lon-
to learn descriptions for newly discovered ones.      gitude. We also have three known sources of information,
  When presented with a new source of information (such each of which has a deﬁnition in Datalog. The ﬁrst source,
as a Web Service), the ﬁrst step in the process of modeling aptly named source1, takes in a zipcode and returns the lat-
the source is to determine what type of data it requires as itude and longitude coordinates of its centroid. The second
  ∗                                                   calculates the great circle distance between two pairs of co-
   This research is based upon work supported in part by the De- ordinates, while the third converts a distance from kilometers
fense Advanced Research Projects Agency (DARPA), through the
Department of the Interior, NBC, Acquisition Services Division, un- into miles. Deﬁnitions for the sources are as follows:
der Contract No. NBCHD030010, in part by the National Science source1($zip, lat, long):- centroid(zip, lat, long).
Foundation under Award No. IIS-0324955, and in part by the Air source2($lat1, $long1, $lat2, $long2, dist):-
Force Ofﬁce of Scientiﬁc Research under grant number FA9550-04- greatCircleDist(lat1, long1, lat2, long2, dist).
1-0105. The views and conclusions contained herein are those of the
                                                      source3($dist1, dist2):- convertKm2Mi(dist1, dist2).
authors and should not be interpreted as necessarily representing the
ofﬁcial policies or endorsements, either expressed or implied, of any The goal in this example is to learn a deﬁnition for a new
of the above organizations or any person connected with them. service, called source4, that has just been discovered on the

                                                IJCAI-07
                                                  2695Internet. This new service takes in two zipcodes as input and between values of the type. The set of relations R may in-
returns a distance value as output:1                  clude interpreted predicates, such as ≤. Each source s ∈ S
            source4($zip, $zip, distance)             is associated with a type signature, a binding constraint (that
                                                      distinguishes input from output) and a view deﬁnition, that is
The system described in this paper takes this type signature as a conjunctive query over the relations in R. The new source
well as the deﬁnitions for the known sources and searches for to be modeled s∗, is described in the same way, except that
an appropriate deﬁnition for the new source. The deﬁnition its view deﬁnition is unknown. The solution to the Source
discovered in this case would be the following conjunction of Deﬁnition Induction Problem is a deﬁnition for this source.
calls to the known sources:                             By describing sources using the powerful language of con-
source4($zip1, $zip2, dist):-                         junctive queries, we are able to model most information
  source1(zip1, lat1, long1),                         sources on the Internet (as sequential compositions of sim-
  source1(zip2, lat2, long2),                         ple functionality). We do not deal with languages involving
  source2(lat1, long1, lat2, long2, dist2),           more complicated constructs such as aggregation, union or
        (     ,    ).
  source3 dist2 dist                                  negation because the resulting search space would be pro-
This deﬁnition states that the output distance can be calcu- hibitively large. Finally, we assume an open-world seman-
lated from the input zipcodes, by ﬁrst giving those zipcodes tics, meaning that sources may be incomplete with respect to
to source1, calculating the distance between the resulting co- their deﬁnitions (they may not return all the tuples implied by
ordinates using source2, and then converting the distance into their deﬁnition). This fact complicates the induction problem
miles using source3. To test whether this source deﬁnition is andisaddressedinsection3.4.
correct the system must invoke the new source and the deﬁn-
ition to see if the values generated agree with each other. The 3 Algorithm
following table shows such a test:
                                                      The algorithm used to search for and test candidate deﬁnitions
   $zip1   $zip2   dist (actual) dist (predicted)     takes as input a type signature for the new source (also called
   80210   90266      842.37         843.65           the target predicate). The space of candidate deﬁnitions is
   60601   15201      410.31         410.83           then enumerated in a best-ﬁrst manner, in a similar way to
   10005   35555      899.50         899.21           top-down Inductive Logic Programming (ILP) systems like
In the table, the input zipcodes have been selected randomly FOIL [Cameron-Jones and Quinlan, 1994]. Each candidate
from a set of examples, and the output from the source and produced is tested to see if the data it returns is in some way
the deﬁnition are shown side by side. Since the output values similar to the target:
are quite similar, once the system has seen a sufﬁcient number 1 Invoke target with set of random inputs;
of examples, it can be conﬁdent that it has found the correct 2 Add empty clause to queue;
semantic deﬁnition for the new source.                  3 while queue = ∅ do
  The deﬁnition above was written in terms of the source 4   v ←  best deﬁnition from queue;
                                                                    
predicates, but could just as easily have been written in terms 5 forall v ∈ expand(v) do
                                                                        
of the domain relations. To do so, one needs to replace each 6   if eval(v ) ≥ eval(v) then
                                                                          
source predicate by its deﬁnition as follows:           7           insert v into queue;
source4($zip1, $zip2, dist):-                           8        end
  centroid(zip1, lat1, long1),                          9    end
         (    ,    ,     ),
  centroid zip2 lat2 long2                              10 end
  greatCircleDist(lat1, long1, lat2, long2, dist2),
  convertKm2Mi(dist1, dist2).                              Algorithm 1: Best-First Search Algorithm
Written in this way, the new deﬁnition for source4 makes
sense at an intuitive level: The source is simply calculating 3.1 Invoking the Source
the distance in miles between the centroids of the zipcodes. The ﬁrst step in our algorithm is to generate a set of tuples
                                                      that will represent the target predicate during the induction
2  Problem Formulation                                process. In other words, we try to invoke the new source
                                                      to sample some data. Doing this without biasing the induc-
We are interested in learning deﬁnitions for sources by invok- tion process is not trivial. The system ﬁrst tries to invoke the
ing them and comparing the output they produce with that of source with random combinations of input values taken from
known sources of information. We formulate the problem as a
            ∗                                       the examples of each type. Many sources have implicit re-
tuple T,R,S,s  ,whereT is a set of semantic data-types, R strictions on the combination of input values. For example, a
is a set of domain relations, S is a set of known sources,and
 ∗                                                    geocoding service which takes a number, street,andzipcode
s is the new source. Each of the semantic types comes with as input, may only return an output if the address actually ex-
a set of example values and a function for checking equality ists. In such cases, randomly combining values to form input
  1The assignment of semantic types to the inputs and outputs of tuples is unlikely to result in any successful invocations. Af-
a service can be performed automatically as described in [Lerman ter failing to invoke the source a number of times, the system
et al., 2006]. In general, sources will output relations rather than will try to generate examples from other sources whose out-
singleton values.                                     put contains the required combination of attribute types. Fre-

                                                IJCAI-07
                                                  2696quency distributions can also be associated with the example In practice, the fact that deﬁnitions for the known sources
values of each semantic type, such that common constants may contain multiple literals means that many different con-
(like Ford ) can be chosen more frequently than less common junctions of domain predicates will reformulate to the same
ones (like Ferrari).                                  conjunction of source predicates, resulting in a much larger
                                                      search space. For this reason, we perform the search over the
3.2  Generating Candidates                            source predicates and rely on post-processing to remove re-
Once the system has assembled a representative set of tuples dundant literals from the unfolding of the deﬁnition produced.
for the new source, it starts generating candidate deﬁnitions
by performing a top-down best-ﬁrst search through the space 3.3 Limiting the Search Space
of conjunctions of source predicates. In other words, it be-
gins with a very simple source deﬁnition and builds ever more The search space generated by this top-down search algo-
complicated deﬁnitions by adding one literal (source predi- rithm may be very large even for a small number of sources.
cate) at a time to the end of the best deﬁnition found so far. As the number of sources available increases, the search
It keeps doing this until the data produced by the deﬁnition space becomes so large that techniques for limiting it must
matches that produced by the source being modeled. For ex- be used. We employ some standard (and other not so stan-
ample, consider a newly discovered source that takes in a dard) ILP techniques for limiting this space:
zipcode and a distance, and returns all the zipcodes that lie
within that radius (along with their respective distances). The 1. Maximum clause length
target predicate representing the source is:            2. Maximum predicate repetition
          source5($zip1, $dist1, zip2, dist2)           3. Maximum existential quantiﬁcation level
                                                        4. Deﬁnitions must be executable
Now assume we have one known source, namely source4
from the previous example:                              5. No repetition of variables allowed within a literal
             source4($zip1, $zip2, dist)              Such limitations are often referred to as inductive search bias
                                                      or language bias [N´edellec et al., 1996]. The ﬁrst restriction
and we also have the interpreted predicate:           limits the length of the deﬁnitions produced, while the second
                 ≤($dist1, $dist2)                    limits the number of times the same source predicate can ap-
The search for a deﬁnition for source5 might then proceed as pear in a given candidate. The third restricts the complexity
follows. The ﬁrst deﬁnition generated is the empty clause: of the deﬁnitions by reducing the number of literals that do
                                                                                               3
                 source5($ , $ , , ).                 not contain variables from the head of the clause. The fourth
                                                      requires that source deﬁnitions can be executed from left to
The null character ( )representsadon’t care variable, which right, i.e., that the inputs of each source appear in the head
means that none of the inputs or outputs have any restrictions
placed on their values. Literals (source predicates) are then of the clause or in one of the literals to the left of that literal.
added one at a time to reﬁne this deﬁnition.2 Doing so pro- Finally, we disallow deﬁnitions in which the same variable
duces the following candidate deﬁnitions, among others: appears multiple times in the same literal (in the body of the
                                                      clause). For example, the following deﬁnition which returns
       ($   , $    , , )         (   , ,     ).
source5 zip1  dist1     :- source4 zip1 dist1         the distance between a zipcode and itself, would not be gen-
source5($zip1, $ , zip2, ) :- source4(zip1, zip2, ).  erated, because zip1 appears twice in the last literal:
source5($ , $dist1, , dist2) :- ≤(dist1, dist2).
                                                      source5($zip1, $ , , dist2) :- source4(zip1, zip1, dist2).
Note that the semantic types in the signature of the target
predicate limit greatly the number of candidate deﬁnitions Such deﬁnitions occur rarely in practice, thus it makes sense
produced. The system checks each of these candidates in to exclude them, thereby greatly reducing the search space.
turn, selecting the best one for further expansion. Assum-
ing that the ﬁrst of the three scores the highest, it would be 3.4 Comparing Candidates
expanded to form more complicated candidates, such as:
                                                      We proceed to the problem of evaluating candidate deﬁni-
source5($zip1, $dist1, , dist2) :-                    tions. The basic idea is to compare the output produced by
        (    , ,    ),  ≤(     ,    ).
  source4 zip1  dist1     dist1 dist2                 the source with that produced by the deﬁnition on the same
The size of the search space is highly dependent on the arity input. The more similar the tuples produced, the higher the
of the sources. Sources with multiple attributes of the same score for the candidate. We then average the score over dif-
type make for an exponential number of possible deﬁnitions ferent input tuples to see how well the candidate describes
at each expansion step. To limit the search in such cases, the source overall. In the motivating example, a single out-
we ﬁrst generate candidates with a minimal number of join put tuple (distance value) was produced for every input tuple
variables in the ﬁnal literal and progressively constrain the (pair of zipcodes). In general, multiple output tuples may be
best performing deﬁnitions (by further equating variables). produced by a source (as was the case for source5). Thus the
  The rationale for performing search over the source pred- system needs to compare the set of output tuples produced by
icates rather than the domain predicates is that if the search the target with those produced by the deﬁnition to see if any
were performed over the latter an additional query reformu- of the tuples are the same. Since both the new source and the
lation step would be required each time a deﬁnition is tested. known sources can be incomplete, the two sets may simply

  2Prior to adding the ﬁrst literal, the system checks if any output 3The existential quantiﬁcation level of a literal is the shortest path
echoes an input value, e.g. source5($zip1, $ , zip1, ). from that literal (via join variables) to the head of the clause.

                                                IJCAI-07
                                                  2697overlap, even if the candidate deﬁnition correctly describes representing the same entity from those representing differ-
the new source. Assuming that we can count the number of ent ones. (See [Bilenko et al., 2003] for a discussion of
tuples that are the same, we can use the Jaccard similarity to string matching techniques.) In other cases a simple proce-
measure how well the candidate hypothesis describes the data dure might be available to check equality for a given type, so
returned by the new source:                           that values like “Monday” and “Mon” are equated. The ac-
                                                     tual equality procedure used will depend on the semantic type
                     1    |Os(i) ∩ Ov(i)|
          eval(v)=                                    and we assume in this work that such a procedure is given in
                    | |   | s( ) ∪  v( )|
                    I  i∈I O  i   O  i                the problem deﬁnition. We note that the procedure need not
                                                      be 100% accurate, but only provide a sufﬁcient level of accu-
Here I denotes the set of input tuples used to test the new racy to guide the system toward the correct deﬁnition. Indeed,
source s. Os(i) denotes the set of tuples returned by the equality rules could even be generated ofﬂine by training a
source when invoked with input tuple i. Ov(i) is the cor- machine learning classiﬁer.
responding set returned by the candidate deﬁnition v.Ifwe
view this hypothesis testing as an information retrieval task, 3.6 Scoring Partial Deﬁnitions
we can consider recall to be the number of common tuples As the search proceeds toward the correct deﬁnition, many
divided by the number of tuples produced by the source, and semi-complete (unsafe) deﬁnitions will be generated. These
precision to be the common tuples divided by the tuples pro- deﬁnitions do not produce values for all attributes of the target
duced by the deﬁnition. The Jaccard similarity takes both predicate but only a subset of them. For example, the follow-
precision and recall into account in a single score.  ing deﬁnition produces only one of the two output attributes
  The table below provides examples of the score for dif- returned by the source:
ferent output tuples. The ﬁrst three rows of the table show  ($    , $   ,    , )
inputs for which the predicted and actual output tuples over- source5 zip1 dist1 zip2 :-
                                                         source4(zip1, zip2, dist1).
lap. In the fourth row, the deﬁnition produced a tuple, while
the source didn’t, so the deﬁnition was penalised. In the last This presents a problem, because our score is only deﬁned
row, the deﬁnition correctly predicted that no tuples would be over sets of tuples containing all of the output attributes of
output from the source. Our score function is undeﬁned at the new source. One solution might be to wait until the de-
this point. From a certain perspective the deﬁnition should ﬁnitions become sufﬁciently long as to produce all outputs
score well here because it has correctly predicted that no tu- before comparing them to see which one best describes the
ples would be returned for that input, but giving a high score new source. There are two reasons why we wouldn’t want to
to a deﬁnition when it produces no tuples can be dangerous. do that: Firstly, the space of complete (safe) deﬁnitions is too
Doing so may cause overly constrained deﬁnitions that can large to enumerate, and thus we need to compare partial deﬁ-
generate very few output tuples to score well, while less con- nitions so as to guide the search toward the correct deﬁnition.
strained deﬁnitions that are better at predicting the output tu- Secondly, the best deﬁnition that the system can generate may
ples on average can score poorly. To prevent this from hap- well be a partial one, as the set of known sources may not be
pening, we simply ignore inputs for which the deﬁnition cor- sufﬁcient to completely model the source.
rectly predicts zero tuples. (This is the same as setting the We can compute the score over the projection of the source
score for this case to be the average for the other cases.) Af- tuples on the attributes produced by the deﬁnition, but then
ter ignoring the last row, the overall score for this deﬁnition we are giving an unfair advantage to deﬁnitions that do not
is calculated to be 0.46.                             produce all of the source’s outputs. That is because it is far
  input      actual         predicted    Jaccard      easier to correctly produce a subset of the output attributes
  tuple i  output Os(i)   output Ov(i)   similarity   than to produce all of them. So we need to penalise such de-
  a, b  {x, y, x, z}  {x, y}       1/2        ﬁnitions accordingly. We do this by ﬁrst calculating the size
  c, d {x, w, x, z} {x, w, x, y} 1/3        of the domain of each of the missing attributes. In the ex-
  e, f {x, w, x, y} {x, w, x, y}  1         ample above, the missing attribute is a distance value. Since
             ∅            {   }                   distance is a continuous variable, we approximate the size of
   g,h                        x, y          0                        (    −     )
  i, j       ∅               ∅         #undef!      its domain using max  min  /accuracy,whereaccuracy
                                                      is the error-bound on distance values. (This cardinality cal-
3.5  Approximate Matches Between Constants            culation may be speciﬁc to each semantic type.) Armed with
                                                      the domain size, we penalise the score by scaling the num-
When deciding whether the two tuples produced by the tar-
                                                      ber of tuples returned by the deﬁnition according to the size
get and the deﬁnition are the same, we must allow for some
                                                      of the domains of all output attributes not generated by it. In
ﬂexibility in the values they contain. In the motivating exam-
                                                      essence, we are saying that all possible values for these ex-
ple for instance, the distance values returned did not match
                                                      tra attributes have been “allowed” by this deﬁnition. (This
exactly, but were “sufﬁciently similar” to be accepted as the
                                                      technique is similar to that used for learning without explicit
same. For certain nominal types, like zipcode, it makes sense
                                                      negative examples in [Zelle et al., 1995].)
to check equality using exact string matches. For numeric
types like temperature, an error bound (like ±0.5◦C)ora
percentage error (such as ±1%) may be more reasonable. 4  Experiments
For strings like company name, edit distances such as the We tested the system on 25 different problems (target predi-
JaroWinkler score do a better job at distinguishing strings cates) corresponding to real services from ﬁve domains. The

                                                IJCAI-07
                                                  2698methodology for choosing services was simply to use any ser- information, and a deﬁnition was learnt involving a similar
vice that was publicly available, free of charge, worked, and service from Yahoo. For this source, the system discovered
didn’t require website wrapping software. The domain model that the current price was the sum of the previous day’s close
used in the experiments was the same for each problem and and today’s change. The fourth deﬁnition is for a weather
included 70 semantic types, ranging from common ones like forecast service, and a deﬁnition was learnt in terms of an-
zipcode to more speciﬁc types such as stock ticker symbols. other forecast service. (The system distinguished high from
It also contained 36 relations that were used to model 35 dif- low and forecast from current temperatures.) The ﬁfth source
ferent publicly available services. These known sources pro- provided information about nearby hotels. Certain attributes
vided some of the same functionality as the targets.  of this source (like the hotel’s url and phone number) could
  In order to induce deﬁnitions for each problem, the new not be learnt, because none of the known sources provided
source (and each candidate) was invoked at least 20 times us- them. Nonetheless, the deﬁnition learnt is useful as is. The
ing random inputs. To ensure that the search terminated, the last source was a classiﬁed used-car listing from Yahoo that
number of iterations of the algorithm was limited to 30, and a took a zipcode and car manufacturer as input. The system dis-
search time limit of 20 minutes was imposed. The inductive covered that there was some overlap between the cars (make,
search bias used during the experiments was: {max. clause model and price) listed on that source and those listed on an-
length: 7, predicate repetition limit: 2, max. existential quan- other site provided by Google.
tiﬁcation level: 5, candidate must be executable, max. vari-       Problems   Candidates
                        }                     ±1%
able occurrence per literal: 1 . An accuracy bound of    Domain    # (#Attr.)  #  (#Lit.) Precis. Recall
was used to determine equality between distance, speed, tem-
                                            ±0 002      geospatial 9  (5.7)    136 (1.9)  100%    84%
perature and price values, while an error bound of .     ﬁnancial  2  (11.5)  1606 (4.5)  56%     63%
degrees was used for latitude and longitude. The JaroWin-
                         0 85                            weather   8  (11.8)   368 (2.9)  91%     62%
kler score with a threshold of . was used for strings such hotels  4  (8.5)     43 (1.3)  90%     60%
as company, hotel and airport names. A hand-written proce- cars    2  (8.5)     68 (2.5)  50%     50%
dure was used for matching dates.
                                                        The table above shows for each domain, the number of
4.1  Results                                          problems tested, the average number of attributes per prob-
Overall the system performed very well and was able to learn lem (in parentheses), the average number of candidates gen-
the intended deﬁnition (albeit missing certain attributes) in 19 erated prior to the winning deﬁnition, and the average number
of the 25 problems. Some of the more interesting deﬁnitions of literals per deﬁnition found (in parentheses). The last two
learnt by the system are shown below:                 columns give the average precision and recall values, where
                                                      precision is the ratio of correctly generated attributes (of the
1 GetDistanceBetweenZipCodes($zip0, $zip1, dis2):-    new source) to all of the attributes generated, and recall is the
   GetCentroid(zip0, lat1, lon2),
             (    ,    ,    ),                        ratio of correctly generated attributes, to all of the attributes
   GetCentroid zip1 lat4 lon5                         that should have been generated. These values indicate the
   GetDistance(lat1, lon2, lat4, lon5, dis10),
   ConvertKm2Mi(dis10, dis2).                         quality of the deﬁnitions produced. Ideally, we would like to
              ($    , $   ,    )                      have 100% precision (no errors in the deﬁnitions) and high
2 USGSElevation lat0  lon1 dis2 :-                    recall (most of the attributes being generated). That was the
   ConvertFt2M(dis2, dis1), Altitude(lat0, lon1, dis1).
                                                      case for the 9 geospatial problems. One reason for the partic-
3 GetQuote($tic0, pri1, dat2, tim3, pri4, pri5, pri6, pri7,
              , ,    , , ,    , ,    )                ularly good performance on this domain was the low number
          cou8  pri10    pri13  com15  :-             of attributes per problem, resulting in smaller search spaces.
   YahooFinance(tic0, pri1, dat2, tim3, pri4, pri5, pri6,
               pri7, cou8),                           As would be expected, the number of candidates generated
   GetCompanyName(tic0, com15, , ),                   was higher for problems with many attributes (ﬁnancial and
   Add(pri5, pri13, pri10), Add(pri4, pri10, pri1).   weather domains). In general, precision was very high, ex-
4 YahooWeather($zip0, cit1, sta2, , lat4, lon5, day6, dat7, cept for a small number of problems (in the ﬁnancial and cars
              tem8, tem9, sky10) :-                   domains). Overall the system performed extremely well, gen-
   WeatherForecast(cit1, sta2, , lat4, lon5, , day6, dat7, erating deﬁnitions with a precision of 88% and recall of 69%.
                  tem9, tem8, , , sky10, , , ),
   GetCityState(zip0, cit1, sta2).                    5   Related Work
5 YahooHotel($zip0, $ , hot2, str3, cit4, sta5, , , , , ) :- Early work on the problem of learning semantic deﬁnitions
             (    ,    ,    ,   ,    , ).
   HotelsByZip zip0 hot2 str3 cit4 sta5               for Internet sources was performed by [Perkowitz and Et-
6 YahooAutos($zip0, $mak1, dat2, yea3, mod4, , , pri7, ) :- zioni, 1995], who deﬁned the category translation problem.
   GoogleBaseCars(zip0, mak1, , mod4, pri7, , , yea3), That problem can be seen as a simpliﬁcation of the source
   ConvertTime(dat2, , dat10, , ),
                ( , ,    , ).                         induction problem, where the known sources have no bind-
   GetCurrentTime   dat10                             ing constraints or deﬁnitions and provide data that does not
  The ﬁrst deﬁnition calculates the distance in miles between change over time. Furthermore, they assume that the new
two zipcodes and is the same as in our original example source takes a single value as input and returns a single tuple
(source4). The second source provided USGS elevation data as output. To ﬁnd solutions to this problem, the authors too
in feet, which was found to be sufﬁciently similar to known used a form of inductive search based on an extension of the
altitude data in meters. The third source provided stock quote FOIL algorithm [Cameron-Jones and Quinlan, 1994].

                                                IJCAI-07
                                                  2699