                                            Loopy SAM

                      Ananth Ranganathan, Michael Kaess, and Frank Dellaert
                  Center for Robotics and Intelligent Machines, College of Computing
                                    Georgia Institute of Technology
                                 {ananth,kaess,dellaert}@cc.gatech.edu

                    Abstract
    Smoothing approaches to the Simultaneous Local-
    ization and Mapping (SLAM) problem in robotics
    are superior to the more common ﬁltering ap-
    proaches in being exact, better equipped to deal
    with non-linearities, and computing the entire robot
    trajectory. However, while ﬁltering algorithms that
    perform map updates in constant time exist, no
    analogous smoothing method is available. We aim
    to rectify this situation by presenting a smoothing-
    based solution to SLAM using Loopy Belief Prop-   Figure 1: The Gaussian Markov Random Field (GMRF) correspond-
    agation (LBP) that can perform the trajectory and ing to an illustrative Smoothing and Mapping (SAM) problem with
    map updates in constant time except when a loop   30 poses in the robot trajectory. Poses in the graph are shown as
    is closed in the environment. The SLAM prob-      circles and landmarks as squares.
    lem is represented as a Gaussian Markov Ran-
    dom Field (GMRF) over which LBP is performed.
                                                      repeated marginalization of poses, recently introduced tech-
    We prove that LBP, in this case, is equivalent to
                                                      niques such as the Sparse Extended Information Filter (SEIF)
    Gauss-Seidel relaxation of a linear system. The in-
                                                      [Thrun et al., 2004] and the Thin Junction Tree Filter (TJTF)
    ability to compute marginal covariances efﬁciently
                                                      [Paskin, 2003] present approximations to keep the informa-
    in a smoothing algorithm has previously been a
                                                      tion matrix sparse. The sparsity of the representation allows
    stumbling block to their widespread use. LBP
                                                      for a constant time update of the ﬁlter.
    enables the efﬁcient recovery of the marginal co-
    variances, albeit approximately, of landmarks and   In contrast to the above ﬁltering approaches which only
    poses. While the ﬁnal covariances are overconﬁ-   compute the current robot pose with the map, a smoothing
    dent, the ones obtained from a spanning tree of the approach recovers the complete robot trajectory and the map.
    GMRF are conservative, making them useful for     The smoothing information matrix naturally stays sparse over
    data association. Experiments in simulation and us- time without the need for any approximations. Further,
    ing real data are presented.                      smoothing can incorporate new information about past robot
                                                      poses as opposed to ﬁltering approaches that cannot update
                                                      a pose after it has been marginalized out. Even though the
1  Introduction                                       number of variables increases continuously for smoothing, in
The problem of Simultaneous Localization and Mapping  many real-world scenarios, especially those involving explo-
(SLAM) is a core competency of robotics which has at- ration, this larger matrix still contains far less entries than in
tracted a large amount of attention. The classical solution to ﬁlter-based methods [Dellaert, 2005][Eustice et al., 2005].
the SLAM problem uses an Extended Kalman Filter (EKF) Filters are more efﬁcient then smoothing only when a limited
[Smith and Cheeseman, 1987] that maintains the joint distri- number of structure points is observed repeatedly. However,
bution on the current robot pose and the landmarks in the map even in this case the inability to correct previous poses may
in the form of a Gaussian distribution. Since updating the co- lead to poor results.
variance matrix of this distribution requires O(n2) time, the The crucial limitation of all the above approaches based on
EKF algorithm does not scale to large maps. Recent work the information form of the Gaussian is that recovering an es-
on the SLAM problem has focussed on improving the time timate of the map and robot pose involves a matrix inversion,
complexity of the solutions. One commonly used technique and hence, is computationally infeasible for large systems.
is to update the information form of the Gaussian distribu- Indeed, in the case of the ﬁltering approaches such as the
tion. Though the information matrix becomes dense through SEIF, obtaining even the mean involves a matrix inversion.

                                                IJCAI-07
                                                  2191The solution is obtained in these cases through an iterative In addition, our algorithm is incremental and largely runs in
process that solves a linear system wherein only a constant O(1) time, which are advantages over Graphical SLAM.
number of iterations is performed at each step so as to main-
tain the runtime bounds of the algorithm. This approximation 2 Smoothing and Mapping
is based on the assumption that the SLAM solution evolves
                                                      We begin by reviewing the SAM framework as given in [Del-
only slowly over time so that a relinearization of the problem
                                                      laert, 2005]. In the smoothing framework, our aim is to re-
at each time step is not required. The iterative method, how-
                                                      cover the maximum a posteriori (MAP) estimate for the entire
ever, still does not yield the covariance, which is needed for    Δ
maximum-likelihood data association and also as an estimate trajectory X = {xi : i ∈ 0 ...M} and landmark locations
                                                                 Δ
of the uncertainty in the landmark locations and pose. (the map) L = {lj : j ∈ 1 ...N}, given the landmark mea-
  This paper deals with the use of Loopy Belief Propagation    Z =Δ {z : k ∈ 1 ...K}            U =Δ {u }
(LBP) [Weiss and Freeman, 1999] to obtain an approximate surements    k             and odometry       i .
                                                      This can be computed by maximizing the joint posterior on
solution to the SLAM problem, including the marginal co-                              P (X, L|Z)
variances on the map and pose. We consider the case wherein the trajectory and landmark locations
the map consists of a set of features, and build on the Smooth-    ∗  Δ
                                                                  Θ   =   argmax P (X, L|Z, U)        (1)
ing and Mapping (SAM) approach as introduced in [Dellaert,                   Θ
2005], which updates the joint distribution on the robot tra-
                                                               Δ
jectory and the map using the square root information form where Θ = {X, L} is the set of unknown variables, i.e the
of the Gaussian distribution. Our approach is based on the robot trajectory and landmark locations. The posterior can be
fact that the SAM problem can be treated from the graph- factorized using Bayes law as
ical model viewpoint as a Gaussian Markov Random Field
                                                         P (X, L|Z, U) ∝   P (Z, U|X, L)P (X, L)
(GMRF)  [Winkler, 1995] on which inference can be per-
                                                                                  M
formed using LBP.                                                            P (x0) i=1 P (xi|xi−1,ui)
                                                                       ∝           K                 (2)
  We provide a constant time update algorithm, which we                                  P (zk|xi ,lj )
call the Wildﬁre algorithm, for estimating the map and robot                         k=1       k   k
trajectory, including the marginal covariances. While a naive Here P (xi|xi−1,ui) is the motion model, parameterized by
                             O(n)                                 u     P (z |x ,l )
implementation of LBP requires    time to update the  the odometry i,and   k  ik jk is the landmark measure-
marginal distributions, where n is the number of nodes in the ment model, assuming that the correspondences (ik,jk) are
graph, the Wildﬁre algorithm discards negligibly small mes- known. The prior on the initial pose P (x0) is taken to be a
sages between nodes. Subsequently, only those nodes which constant, usually the origin, while the priors on the remaining
have either received or sent signiﬁcant messages get updated. poses and landmark locations are assumed to be uniform.
We show that the number of nodes involved in a signiﬁcant We assume Gaussian motion and measurement models, as
message exchange is O(1) unless a signiﬁcant event (such as is standard in the SLAM literature. The motion model is
loop closing) occurs when the complete graph may get up- given as xi = fi(xi−1,ui)+wi,wherewi is zero-mean Gaus-
dated, increasing the time bound to O(n).             sian noise with covariance matrix Qi. Similarly, the measure-
                                                                            z = h  (x ,l )+v         v
  Relinearization of the GMRF graph, which is required ment equation is given as k k ik jk    k where k is
since the SAM problem is non-linear in general, is performed normally distributed zero-mean measurement noise with co-
in constant time using a technique similar to the Wildﬁre al- variance Rk.
gorithm in that only nodes that have changed are relinearized. In practice, only linearized versions of the motion and mea-
Moreover, it need only be done periodically when the differ- surement models are considered, for example for use in the
ence between the current estimate and the linearization point Extended Kalman Filter (EKF) approach to SLAM [Smith et
exceeds a speciﬁed threshold. As before, relinearization may al., 1990], with multiple iterations being used to obtain con-
take linear time after a signiﬁcant update of the GMRF graph. vergence to the minimum. In the following, we will assume
  We prove that LBP on a GMRF is equivalent to Gauss- that either a good linearization point is available or that we are
Seidel relaxation and provides the exact MAP estimates of working on one iteration of a non-linear optimization method.
the nodes. The covariances are, however, overconﬁdent due It can be shown [Dellaert, 2005] that this results in a sparse
to the approximate nature of the inference. As a conservative linear least squares problem in δΘ
                                                                       
approximation, the covariances obtained by restricting infer-            M
ence to a spanning tree of the GMRF graph can be used to ∗                     i−1         i        2
                                                       δΘ   =   argmin      F    δxi−1 + G δxi − ai +
perform data association.                                                      i           i        Qi
                                                                  δΘ     i=1
  In terms of related work, closest are the relaxation based                                     
                                                                    K
approaches given in [Duckett et al., 2000; Frese and Duckett,              ik       jk        2
                                                                        H  δxi + J  δlj − ck        (3)
2003]. However, these are unable to compute the marginal                  k    k    k   k     Rk
covariances on the poses and landmarks. Graphical SLAM              k=1
[Folkesson and Christensen, 2004] is another method that      i−1
                                                      where Fi   is the sparse Jacobian of fi(.) at the lineariza-
builds a graphical model of the smoothing problem. Our ap-
                                                               x0       a =Δ x0 − f (x0  ,u )
proach shares many common features with Graphical SLAM, tion point i−1 and i  i    i  i−1  i is the odome-
                                                                                      ik     jk
including the ability to modify data association on the ﬂy, try prediction error. Analogously, Hk and Jk are respec-
and increase efﬁciency by eliminating variables judiciously. tively the sparse Jacobians of hk(.) with respect to a change

                                                IJCAI-07
                                                  2192  x      l                                (x0 ,l0 )         K
in ik and jk , evaluated at the linearization point ik jk , where is a constant of proportionality.
       Δ          0  0                                  Using this formulation, we can write the edge potentials
and ck = zk − hk(xi ,lj ) is the measurement prediction                     −1
                   k  k                               from (5) as ψ(xi,lj) ∝N  (yij ; ηij , Λij ) ,whereyij is the
error. ||.||Σ is the Mahalanobis distance with respect to the       T
                                      i               vector [ xi lj ] , and we have dropped the index k from the
covariance matrix Σ andweusethematrixGi = −Id×d , d
                                                      corresponding pair {ik,jk} for convenience. The distribution
being the dimension of xi, to avoid treating δxi specially.
                                                      parameters are then given as
  The solution to the least squares problem (3) is given by                
           

                                          T                            ηi           Hi
the linear system AδΘ − b =0where  A  =  J J  is the               Δ    ij           k    −1
                                                                ηij =   j     =      j   Rk  ck       (8)
Hessian matrix, J being the system Jacobian matrix obtained            ηij          Jk
                                                                                     
           

by assembling the individual measurement Jacobians, and            ii   ij                          T
     T                                                           Λ     Λ            Hi         Hi
b = J e where e is the vector of all the measurement errors. Δ     ij   ij           k    −1     k
                                                          Λij =    ji   jj    =      j   Rk      j    (9)
                                                                 Λij  Λij           Jk         Jk
SAM as a Markov Random Field
                                                      and the potentials on the edges linking two poses are deﬁned
We now show that inference on the posterior (2) can be per- analogously.
formed by placing the SAM problem in a Markov Random    The GMRF potentials on the singleton cliques are similarly
Field (MRF) framework. The graph of an MRF is undi-                      −1
                                                      deﬁned as φ(yi)=N    (yi; ηi, Λi) where yi ∈{X, L},and
rected and its adjacency structure indicates which variables           −1
                                                      ηi =Λiμi, Λi = P    follows from (5) and (7). The poten-
are linked by a common factor (a measurement or constraint).           i
                                                      tials φ(yi) represent the beliefs on the unknowns and if only
  A pairwise MRF is described by a factored probability den-
                                                      a uniform prior exists on an MRF node yi, both ηi and Λi can
sity given as
                                                    be set to zero. Note that setting Λi to zero is equivalent to
          P (X, L) ∝   φ(yi)    ψ(yi,yj)        (4)   having a Gaussian prior with inﬁnite covariance.
                     i
                            {i,j}                     Belief Propagation
where the second product is over the pairwise cliques {i, j}, The goal of belief propagation in our case is to ﬁnd the
counted once. The posterior (2) can be modeled as a pair- marginal probability, or the belief, P (yi) of a node yi ∈ Θ=
wise MRF where the singleton cliques correspond to marginal {X, L} in the SAM graph. A complete characterization of
prior distributions or singleton measurements (such as GPS) the belief propagation algorithm for GMRFs is beyond the
on the unknown variables and the edge cliques correspond to scope of this paper, but we provide the high-level equations
the motion and measurement likelihoods respectively.  and data ﬂow below. Details can be found in [Weiss and Free-
  For the linear case considered in the previous section, these man, 1999].
equations translate to                                  The beliefs are computed using a message passing algo-
                      1        2                      rithm wherein each node passes messages to its neighbors
       − ln φ(yi) ∝    xi − μi
                      2        Pi                     and in turn uses its incoming messages to compute its belief.
                      1  i−1         i        2       While belief propagation is guaranteed to compute the correct
  − ln ψ(xi−1,xi) ∝    F   δxi−1 + G δxi − ai (5)
                      2  i           i        Qi      beliefs only if the graphical model does not contain loops, it
                      1                               has been proven that it ﬁnds the correct MAP solution for
                          ik       jk         2
  − ln ψ(xi ,lj ) ∝    H  δxi  + J  δlj − ck        Gaussian graphical models even in the presence of loops, i.e
          k   k       2   k   k    k   k      Rk
                                                      the means of the beliefs are correct while the covariances are
where yi ∈{X, L}. This gives us a Gaussian Markov Ran- incorrect in general [Weiss and Freeman, 1999].
dom Field (GMRF) corresponding to the linearized SAM    LBP works by repeatedly computing the messages and be-
problem. Note that in the context of the SAM problem, the liefs for every node in the graph starting with constant mes-
singleton clique factors are most often set to unity except for sages [Weiss and Freeman, 1999], either in synchronous or
the ﬁrst pose, which is clamped to the origin.        asynchronous fashion. This sweep over the graph is iterated
                                                      until the beliefs converge. Denoting the current iteration by
3  Loopy Belief Propagation for SAM                   a discrete time superscript t, the belief parameters (the infor-
                                                                                    y
We use Loopy Belief propagation (LBP) on the SAM GMRF mation vector and matrix) for node i are given as
                                                                      t                t−1
to solve the linear system (3) and obtain not only the MAP          mi   =   ηi +    mji             (10)
values of the unknown variables but also the covariances.                        j∈N
                                                                                  i
For ease of computation, we use the information form of the           t                 t−1
Gaussian to specify the single and pairwise clique potentials       Mi   =Λi   +      Mji            (11)
in the GMRF given by (5), wherein these are expressed using                      j∈Ni
the information vector η and the information matrix Λ,and and the parameters of the message Mij (yj) from node yi to
we deﬁne the information form as                    node yj as
                                                                                            −1
        −1        Δ            T    1 T                      t       j    ji   ii    t    t−1
      N   (x; η, Λ) =exp C  + η x −  x  Λx      (6)        mij  =   ηij − Λij Λij + Mi − Mji
                                    2                                                           
                                                                                   i     t    t−1
                                                                                  ηij + mi − mji     (12)
where C is an additive constant. If Λ is full-rank, equation                                 
                                                             t       jj    ji  ii    t     t−1 −1  ij
(6) corresponds to a Gaussian density as                   Mij  =Λij   −  Λij Λij + Mi − Mji     Λij (13)
           N (x;Λ−1η, Λ−1)=N   (x; μ, P )       (7)   where use has been made of the deﬁnitions in (8) and (9).

                                                IJCAI-07
                                                  2193Algorithm 1 The Wildﬁre algorithm for LBP               The Wildﬁre algorithm has O(1) running time since in
 1. Push the most recently added nodes and their neighbors in the most situations the number of edges with large message devi-
    SAM graph G(V,E) onto the queue Q                 ations is O(1). However, this is not true when special events
 2. Do until Q is empty                               such as loop closures occur as shown in Figure 2. In such
                                                                                                    O(n)
                                           t          instances, the complete graph is updated resulting in
     (a) Pop node y from Q and compute messages Myk for all
        k ∈N                                          complexity. Note that no special clause is required to handle
             y using (12-13)                          these instances - the algorithm automatically updates all the
          Mt  −Mt−1   >      k    Q
     (b) If yk    yk     , push onto                  nodes in the graph as the message deviations do not die down.
     (c) Update the belief B(y) using (10-11)           Relinearization of the SAM graph, which involves comput-
                                                      ing the clique potentials at new linearization points, can also
                                                      be performed using the Wildﬁre algorithm, the only differ-
                                                      ence being that the deviations of the node beliefs from their
                                                      respective linearization points are now used instead of the
                                                      message deviations. In this case, however, two variants are
                                                      possible. One technique is to delay relinearization until the
                                                      belief deviations are greater than a certain threshold for some
                                                      given proportion of nodes in the graph. As this proportion
                                                      approaches unity, relinearization becomes O(n) but is per-
                                                      formed only very rarely. A second strategy is to perform re-
                                                      linearization at regular time intervals so that it remains O(1).
                                                      The choice between these strategies depends on the type of
                                                      application, the environment, and size of the SAM graph.
                                                        We now have all the components to describe the Loopy
                                                      SAM  algorithm. At each step, a new pose node, new mea-
                                                      surement links and possibly new landmark nodes, are added
Figure 2: The Wildﬁre algorithm only updates nodes with signif- to the SAM GMRF. A ﬁxed number of LBP iterations are
icant message deviations, starting from the most recently added then performed using the Wildﬁre algorithm to update the be-
node. In normal operation (left) only the frontier of the graph is
updated. However, during loop closings (right) most of the graph liefs on the nodes. Periodically, relinearization is performed,
is updated. Updated poses are shown in brown (dark) while the un- again using a variant of the Wildﬁre algorithm. The means
changed nodes are in yellow (light). The robot is moving counter- of the pose and landmark nodes in the SAM GMRF, obtained
clockwise starting at the bottom.                     from the belief information vector and matrix at each node,
                                                      correspond to the MAP solutions for the trajectory and the
                                                      map respectively.
4  Constant Time Loopy SAM
Each iteration of LBP as deﬁned above is O(n) where n is the 4.1 Marginal Covariances
number of nodes in the graph. This is too slow to be useful, We now show how to recover the approximate marginal co-
especially since the trajectory is included in the node count. variances of the nodes from the LBP beliefs. An important
We can, however, improve this bound to O(1) for incremen- use of the covariances is to bound the search area for possible
tal operation, i.e. when new nodes or edges are added to the correspondences in data association. This is usually done us-
GMRF.                                                 ing the projection of the combined pose and landmark uncer-
  The key observation to getting a constant time algorithm is tainty into the measurement space, the computation of which
that at each step only a few of the nodes get updated, i.e. ex- requires knowledge of the marginal covariances of the poses
perience an actual change in their values, and these nodes and landmarks.
are the ones that have been recently added to the graph. If The inverse of the belief information matrix at each node
we commence the belief updation sweep in every iteration in the SAM GMRF gives the marginal covariance. While in
of LBP from the most recently added node, the new mes- general the belief obtained from LBP can be overconﬁdent
sages are initially seen to deviate from their previous values or underconﬁdent, it has been proven to be always overcon-
by large amounts. However, these deviations become smaller ﬁdent for Gaussian MRFs with pairwise cliques [Weiss and
as we move further back in the graph. Once the message de- Freeman, 1999]. Overconﬁdent covariances cannot be used
viations become negligible (smaller than some pre-speciﬁed for data association since this results in many valid corre-
signiﬁcance threshold), the beliefs no longer change and the spondences being rejected. A ﬁrst conservative approxima-
remaining nodes need not be updated.                  tion would be to use the inverse of the diagonal blocks of the
  The Wildﬁre algorithm, so called because the message de- system Hessian matrix as the marginal covariances. However,
viations from the previous iteration spread like wildﬁre from this results in overly conservative estimates.
their point of origin and gradually become negligible, is illus- A better approximation can be obtained by restricting mes-
trated in Figure 2. The algorithm implementation features a sage passing to a spanning tree of the SAM GMRF. Since the
queue onto which nodes with signiﬁcant message deviations spanning tree is obtained by deleting edges from the GMRF
on incoming edges are pushed. A summary of the algorithm and inference on it is exact, we get a conservative estimate of
is given in Algorithm 1.                              the true marginal covariances that is also better than our ﬁrst

                                                IJCAI-07
                                                  2194                                                                            Update time per step

                                                                    Update
                                                             0.16   Messages
                                                                    Beliefs
                                                             0.14

                                                             0.12

                                                              0.1

                                                             0.08
                                                             Time  (sec)

                                                             0.06

                                                             0.04

                                                             0.02

                                                                  500 1000 1500 2000 2500 3000 3500 4000 4500 5000
                                                                               Steps
                                                      Figure 4: Runtime for a simulation consisting of 5000 steps. The
                                                      times for the message and belief computation, and the total update
                                                      step are shown. All times were obtained on a 1.8GHz Linux machine
                                                      using an implementation of the algorithm in Ocaml. Note the two
                                                      spikes (one at the very end) corresponding to loop closures.

Figure 3: (top) A spanning tree of the GMRF graph, with edges col- system Jacobian J,ande is the corresponding measurement
ored blue (dark), for a linear simulation. (bottom) Covariances for error. This result is true since we have
                                                                                  
the same simulation - ground truth is shown in blue (thin dark), over- t t−1             t−1    −i
conﬁdent LBP covariances are shown in green (light), and conserva- mj − mij = ηj +     mkj  = mj
tive covariance estimates obtained by restricting BP to the spanning             k∈Nj \i
tree are shown in red (thick dark).
                                                              −i
                                                      where mj  is the estimate of mj without considering node
                                                                       t     t−1      −i         −i −i
order approximation. Maintenance of the spanning tree and yj, and similarly Mj − Mij = Mj .SinceMj yj  =
                                                        −i
message passing on it require extra work. However, the span- mj holds by the deﬁnition of the information form of the
ning tree can be extended at each step in O(1) so that, with Gaussian, substituting these identities and the deﬁnitions of η
the use of the Wildﬁre algorithm, the overall scheme still re- and Λ from (8-9) into the message equation (12) gives us the
mains constant time. Figure 3 illustrates the nature of this result (15).
approximation.                                          Using (15) in the belief equation (10), we get
                                                                                       
                                                             mt   =   η +     J T e  −     A  y−i
5  Loopy SAM and Gauss-Seidel Relaxation                       i       i       kj i kj       ij j
                                                                          j∈N          j∈N
We prove in this section that computing the MAP solution                     i           i
using LBP for a GMRF is equivalent to a modiﬁed form of                                −i
                                                                  =   ηi + bi −   Aij yj             (16)
Gauss-Seidel relaxation, which is commonly used to solve                      j∈N
linear systems. Relaxation has previously been used in the                       i
context of SLAM [Duckett et al., 2000] but is not capable of which closely corresponds to the relaxation equation (14).
recovering the marginal covariances, a short-coming that LBP Hence, the MAP computation in LBP is simply a modiﬁed
overcomes. The equivalence of the MAP portion of LBP and version of Gauss-Seidel relaxation.
relaxation is an interesting result that connects our technique
with a well-understood method from linear algebra.    6   Results
  Relaxation solves the linear system AδΘ−b =0, encoun-
tered in Section 2, by iterating over a set of equations, each Linear Gaussian Simulations
of which updates the value of exactly one unknown using the We ﬁrst present the results of applying our approach in a con-
current estimate of all the others                   trolled simulation devoid of non-linearities. The simulation
              Aiiyi =   bi −   Aij yj          (14)   consisted of an environment with randomly placed, uniformly
                            j=i                      distributed landmarks. The robot followed a large 8-shaped
                                                      ﬁgure in this environment and the run included two loop clo-
                                        y     y  ∈
where the sum is over all the variables except i,and i sures at the “waist” of the 8-ﬁgure.
Θ={X, L}
           as before.                                   Figure 4 gives the results for a linear simulation consist-
                                              y
  We begin our proof by noting that the message from j to ing of 5000 steps, i.e. a trajectory of length 5000. The total
y
 i given by (12) can be re-written as                 number of nodes in the SAM GMRF was 6742. It can be
                t       T         −i                                          O(1)
              mji  =   Jkiek − Aij yj          (15)   veriﬁed that the algorithm is , and the two spikes in the
      −i                                              graph corresponding to the loop closures are also clearly vis-
where yj is the estimate of yj without considering the node ible. The spikes occur since the LBP update makes a sweep
yi, k is the index of the measurement linking yi and yj in the through the whole graph in these instances.

                                                IJCAI-07
                                                  2195