                                Robust Planning with (L)RTDP

                                Olivier Buffet  and  Douglas Aberdeen
                                       National ICT Australia &
                                   The Australian National University
                  {olivier.buffet,douglas.aberdeen}@nicta.com.au


                    Abstract                          ponent chooses from the possible models to reduce the long-
                                                      term utility.
    Stochastic Shortest Path problems (SSPs), a sub-    Our principal aim is to develop an efﬁcient planner for
    class of Markov Decision Problems (MDPs), can     a common sub-class of MDPs for which some policies are
    be efﬁciently dealt with using Real-Time Dynamic  guaranteed to eventually terminate in a goal state: Stochastic
    Programming (RTDP). Yet, MDP models are often     Shortest Path (SSP) problems. The greedy Real-Time Dy-
    uncertain (obtained through statistics or guessing). namic Programming algorithm (RTDP) [Barto et al., 1995] is
    The usual approach is robust planning: searching  particularly suitable for SSPs, as it ﬁnds good policies quickly
    for the best policy under the worst model. This   and does not require complete exploration of the state space.
    paper shows how RTDP can be made robust in          This paper shows that RTDP can be made robust. In Sec-
    the common case where transition probabilities are tion 2, we present SSPs, RTDP and robustness. Then Sec.
    known to lie in a given interval.                 3 explains how RTDP can be turned into a robust algorithm.
                                                      Finally, experiments are presented to analyse the behavior of
1  Introduction                                       the algorithm, before a discussion and conclusion.1
In decision-theoretic planning, Markov Decision Problems
[Bertsekas and Tsitsiklis, 1996] are of major interest when 2 Background
a probabilistic model of the domain is available. A number of 2.1 Stochastic Shortest Path Problems
algorithms make it possible to ﬁnd plans (policies) optimiz- A Stochastic Shortest Path problem [Bertsekas and Tsitsik-
ing the expected long-term utility. Yet, optimal policy con-
                                                      lis, 1996] is deﬁned here as a tuple hS, s0, G, A, T, ci. It de-
vergence results all depend on the assumption that the proba- scribes a control problem where S is the ﬁnite set of states of
bilistic model of the domain is accurate.
                                                      the system, s0 ∈ S is a starting state, and G ⊆ S is a set of
  Unfortunately, a large number of MDP models are based on goal states. A is the ﬁnite set of possible actions a. Actions
uncertain probabilities (and rewards). Many rely on statistical control transitions from one state s to another state s0 accord-
models of physical or natural systems, such as plant control or ing to the system’s probabilistic dynamics, described by the
animal behavior analysis. These statistical models are some-                              0
                                                      transition function T deﬁned as T (s, a, s ) = P r(st+1 =
times based on simulations (themselves being mathematical 0
                                                      s |st = s, at = a). The aim is to optimize a performance
models), observations of a real system or human expertise. measure based on the cost function c : S × A × S → R+.2
  Working with uncertain models ﬁrst requires answering SSPs assume a goal state is reachable from any state in S,
two closely related questions: 1– how to model this uncer- at least for the optimal policy, so that one cannot get stuck in
tainty, and 2– how to use the resulting model. Existing work a looping subset of states. An algorithm solving an SSP has
shows that uncertainty is sometimes represented as a set of to ﬁnd a policy that maps states to probability distributions
                                           [
possible models, each assigned a model probability Munos, over actions π : S → Π(A) which optimizes the long-term
    ]
2001 . The simplest example is a set of possible mod- cost J deﬁned as the expected sum of costs to a goal state.
                                [
els that are assumed equally probable Bagnell et al., 2001; In this paper, we only consider SSPs for planning pur-
                     ]
Nilim and Ghaoui, 2004 . Rather than construct a possibly poses, with full knowledge of tuple deﬁning the problem:
inﬁnite set of models we represent model uncertainty by al-
                                                      hS, s0, G, A, T, ci. In this framework, well-known stochas-
lowing each probability in a single model to lie in an interval tic dynamic programming algorithms such as Value Iteration
[                               ]
Givan et al., 2000; Hosaka et al., 2001 .             (VI) make it possible to ﬁnd a deterministic policy that cor-
  Uncertain probabilities have been investigated in resource responds to the minimal expected long-term cost J ∗. Value
                 [           ]
allocation problems Munos, 2001 — investigating efﬁcient Iteration works by computing the function J ∗(s) that gives
exploration [Strehl and Littman, 2004] and state aggregation
[Givan et al., 2000] — and policy robustness [Bagnell et al., 1Work presented in more details in [Buffet and Aberdeen, 2004].
2001; Hosaka et al., 2001; Nilim and Ghaoui, 2004]. We fo- 2As the model is not certain, we do not make the usual assump-
                                                                          0
cus on the later, considering a two-player game where the op- tion c(s, a) = Es0 [c(s, a, s )].the expected sum of costs of the optimal policies. It is the A simplistic approach computes the average model over
unique solution of the ﬁxed point Bellman equation:   M, or the most probable model in M, then uses standard SSP
                 X                                    optimization methods. Such approaches guarantee nothing
   J(s)  =   min     T (s, a, s0) [c(s, a, s0) + J(s0)] . (1) about the long-term cost of the policy if the true model differs
             a∈A
                 s0∈S                                 from the one chosen for optimization.
                                                        We follow the approach described in [Bagnell et al., 2001],
Updating J with this formula leads to asymptotic conver-
                                                      that consists of ﬁnding a policy that behaves well under the
gence to J ∗. For convenience, we also introduce the Q-value:
         P            0        0      0               worst possible model. This amounts to considering a two-
Q(s, a) =   0  T (s, a, s )[c(s, a, s ) + V (s )].
           s ∈S                                       player zero-sum game where a player’s gain is its opponent’s
  SSPs can easily be viewed as shortest path problems where
                                                      loss. The player chooses a policy over actions while its “dis-
choosing a path only probabilistically leads you to the ex-
                                                      turber” opponent simultaneously chooses a policy over mod-
pected destination. They can represent a useful sub set of
                                                      els (as this is a simultaneous game, optimal policies can be
MDPs, as they are essentially ﬁnite-horizon MDPs with no
                                                      stochastic). This results in a max-min-like algorithm:
discount factor.

                                                                     max    min  JπM,πA (s0).
2.2  RTDP                                                           πM∈ΠM  πA∈ΠA
Trial based3 Real-Time Dynamic Programming (RTDP), in- In this SSP game, Value iteration does converge to a ﬁxed
troduced in [Barto et al., 1995], uses the fact that the SSP solution [Patek and Bertsekas, 1999].
costs are positive and the additional assumption that every It is also possible to be optimistic, considering that both
trial will reach a goal state with probability 1. Thus, with players collaborate (as they endure the same costs), which
a zero initialization of the long-term cost function J, both turns the max into a min in previous formula. This second
J and Q-values monotonically increase during their iterative case is equivalent to a classical SSP where a decision consists
computation.                                          of choosing an action and a local model.
  The idea behind RTDP (Algorithm 1) is to follow paths
from the start state s0, always greedily choosing actions with Locality — Such a max-min algorithm would be particu-
the lowest long-term cost and updating Q(s, a) as states s are larly expensive to implement. Even restricting the search to
encountered. In other words, the action chosen is the one a deterministic policy over models, it requires computing the
expected to lead to the lowest future costs, until the iterative optimal long-term cost function for each model before pick-
computations show that another action may do better.  ing the worst model found and the optimal policy associated
                                                      with it. However, a simpler process may be used to com-
Algorithm 1 RTDP algorithm for SSPs                   pute J while looking simultaneously for the worst model. It
                                                      requires the hypothesis that next-state distributions T (s, a, ·)
  RTDP(s:state) // s = s0
  repeat                                              are independent from one state-action pair (s, a) to another.
    RTDPTRIAL(s)                                      This assumption does not always hold. However, this makes
  until // no termination condition                   things easier for the opponent because it now has larger set of
                                                      models from which to choose. This consequence is conserva-
  RTDPTRIAL(s:state)                                  tive for producing robust policies.
  while ¬GOAL(s) do                                     Because we assume independence at the state-action level
    a =GREEDYACTION(s)                                (not only at the state level), it is equivalent to a situation
    J(s)=QVALUE(s, a)                                 where the second player makes a decision depending on the
    s =PICKNEXTSTATE(s, a)                            current state and the ﬁrst player’s action. This situation
  end while                                           amounts to a sequential game where the previous players
                                                      move is known to the next player: both players can act in a
                                                      deterministic way without loss of efﬁciency.
  RTDP has the advantage of quickly avoiding plans that lead The result of this assumption is that the worst model can
to high costs. Thus, the exploration looks mainly at a promis- be chosen locally when Q is updated for a given state-action
ing subset of the state space. Yet, because it follows paths by pair. As can be seen from Algorithm 2, the worst local model
simulating the system’s dynamics, rare transitions are only a
                                                      ms  may change while Q-values evolve. Previous updates of
rarely taken into account. The use of a simulation makes it reachable states’ long-term costs may change their relative or-
possible to get good policies early, but at the expense of a dering, changing which outcomes are considered to be worst.
slow convergence, because of the bad update frequency of The key contribution of this paper is to show that RTDP
rare transitions.                                     can be made robust, allowing for planning in very large and
                                                      uncertain domains, retaining worst (or best) case behaviour
2.3  Robust Value Iteration                           guarantees.
Pessimism and Optimism —   We now turn to the problem
of taking the model’s uncertainty into account when looking 3 Robust RTDP
for a “best” policy. The (possibly inﬁnite) set of alternative
models is denoted M.                                  From  now  on, we   consider interval-based uncertain
                                                      SSPs, where  T (s, a, s0) is known to be in an interval
  3We always assume a trial based RTDP implementation. [P rmin(s0|s, a), P rmax(s0|s, a)]. Figure 1 is an example. WeAlgorithm 2 Robust Value Iteration (for an SSP)       to the worst outcome. This requires ﬁrstly sorting reachable
                                                                                              0      0
 1: Initialize J to 0.                                states in decreasing order of the values: c(s, a, s1) + J(s1) ≥
                                                             0      0             0      0
 2: repeat                                            c(s, a, s2) + J(s2) ≥ · · · c(s, a, sk) + J(sk). Then, the worst
 3:  for all s: state do                              distribution is the one giving the highest probability to the
                                                               0        0               0
 4:    for all a: action do                           ﬁrst state s1, then to s2, and so on up to sk. As pointed out in
 5:       Q   (s, a) ← max a   a                      [Givan et al., 2000], this is equivalent to ﬁnding the highest
           max            ms ∈Ms
              P              0    0            0    index r ∈ [1..k] such that
                    T  a (s, a, s ) J(s ) + c a (s, a, s )
                s0∈S ms                 ms
 6:    end for                                                       r−1        k
 7:    J(i) ← mina∈A  Qmax(s, a)                                     X   max   X   min
                                                                        pi   +    pi  ≤  1.
 8:  end for                                                         i=1       i=r
 9: until J converges
                                                      The resulting transition probabilities are
                                                                                max
discuss the pessimistic approach, the optimistic one leading          0         pi   if i < r
                                                                  P r(si) =      min                  (2)
to similar results.                                                             pi  if i > r
                                                                                   k
 [.7]                [.7] [.7,.7]             [.5,.9]                 0           X        0
           s0                        s0                          P r(sr)  =  1 −       P r(si).       (3)
                          (c=1)               (c=.9)
      (c=1)   (c=.87)                                                            i=1,i6=r

                                                                                         Pk    min
                                                      Using the pre-computed bound Bmin =  i=1 pi , Alg. 3
     a0           a1          a0            a1        gives a complete implementation. The insertion sort algo-
                                                      rithm4 is chosen as the list will usually be ordered from pre-
      [.3]      [.3]           [.3,.3]   [.1,.5]      vious J updates.
                               (c=1)     (c=.8)
           s1                        s1
                                                      Algorithm 3 Worst Model for State-Action Pair (s, a)
      a) certain SSP           b) uncertain SSP
                                                        WORSTMODEL(s: state, a: action)
                                                              0      0
Figure 1: Two views of one SSP, depending on whether    R = (s1, · · · , sk) = REACHABLESTATES(s,a)
model uncertainty is taken into account (costs in parenthesis). SORT(R)
In the uncertain SSP, action a will be prefered as it quickly i = 1, bound = Bmin
                         0                                             min    max
reaches the goal s .                                    while (bound − pi  + pi  <  1) do
               1                                                            min   max
                                                          bound ←  bound − pi  + pi
                                                          P r(s0 ) ← pmax
  For a given state-action pair (s, a), there is a list R =    i    i
  0     0                                                 i ← i + 1
(s1, · · · , sk) of reachable states. For each reachable state
       0          min  max                              end while
T (s, a, si) ∈ Ii = [pi , pi ]. Thus, possible models are r = i
the ones that comply with these interval constraints while en- P r(s0 ) ← 1 − (bound − pmin)
      P  T (s, a, s0 ) = 1                                   r                 r
suring  i       i     . Fig. 2 illustrates this with three for all i ∈ {r + 1, . . . , k} do
reachable states.                                              0    min
                                                          P r(si) ← pi
            0                          0
            s1                        s1                end for
                                                        return (R, P r(·))

     max
    p 0
     s1                                                 To summarise, Robust VI on an interval-based SSP con-
                                                      sists of applying normal Value Iteration with the transition
  min                                                 probabilities updated through Alg. 3.
 ps0
   1                                                    We need only a single worst model to compute the cor-
                                                                                                        0
  0                    0    0                    0    responding Q-value. Yet, because several reachable states si
 s3                   s2   s3                   s2                                0     0    0
                                                      may have the same value c(s, a, si)+J(si) as sr (we call this
                                                                  0
                                                      set of states Sr), there may be an inﬁnite number of equiva-
Figure 2: A triangle is a probability simplex representing lent worst local models. Any model differing only in how the
all possible probability distributions with three different out-
           0             0                            probability mass is distributed among the equally bad states
comes (P r(si) = 1 at the si vertex). On the left triangle 0
                                             0        of Sr is also a worst local model.
is the trapezium showing the interval constraint for s1. The
right triangle shows possible models at the intersection of the
three interval constraints.                           Worst Global Models —   Contrary to VI, RTDP does not
                                                      necessarily visit the complete state-space. This is why [Barto
                                                      et al., 1995] introduces the notion of relevant states, which
                                                      we extend to the uncertain case: a state s is relevant for M if
Worst Local Models —  The maximisation step to compute
Q(s, a) in Alg. 2 is done by giving the highest probability 4http://en.wikipedia.org/wiki/Insertion  sortthere exists a start state s0, a model m ∈ M and an optimal transformation ensures: 1- the additional maxm∈M in
policy π under that model such that s can be reached from the update formula (Alg. 2, Line 5) does not break the re-
state s0 when the controller uses π under m.              quirement that Jt is increasing and non-overestimating;
  This notion is important because two equally worst lo-  and 2- relevant states are visited inﬁnitely often, guaran-
cal models on a given state-action pair may forbid different teeing a complete and optimal policy.
states, so that for two models m1 and m2, a state may be rel- 3. RTDP-SSPG and robust RTDP differ because RTDP-
evant in m1 but not in m2. Yet, RTDP should not ﬁnd an    SSPG assumes an optimal opponent. In the robust case
optimal policy just for the relevant states of only one worst we wish to plan for opponents that may choose the
global model. Neither does the policy have to apply to all wrong model. Thus robust RTDP is RTDP-SSPG fol-
possible states. It should apply to all reachable states under
                                                          lowing model md — for simulation only — that allows
any model (for optimal policies) i.e., for relevant states. But states to be visited that RTDP-SSPG optimal opponents
covering all relevant states in the worst model used for up-
                                                          would forbid. Under md (described in Sec. 3), no state
dating Q-values does not necessarily cover all relevant states reachable under any model in M is excluded from the
in M: it depends on the model used to choose the next state, search. Thus we have redeﬁned an expanded set of rel-
i.e., to simulate the system’s dynamics.                  evant states for added robustness. All relevant states are
  To avoid missing relevant states, each local model used for still visited inﬁnitely often.
the simulation should ensure that all reachable states can be
visited. As can be seen in Fig. 2, the set of possible local mod-
els for a state-action pair is an n-dimensional convex poly-
                                                        Our use of the model m for simulation ensures that the
tope. Any model inside this polytope, excluding the bound-                   d
                                     0                policy will cover all states any opponent could lead us to, but
ary, is therefore adequate because, for all si, it ensures that
   0                                                  assumes the worst model will apply afterwards.
P (si|s, a) > 0.
  So there exists a global model md that can be used to ef-
fectively simulate the system’s dynamics without missing any 4 Experiments
potentially reachable state.                          Labelled RTDP [Bonet and Geffner, 2003] is a modiﬁed ver-
                                                      sion of RTDP which can be made robust in a similar way.
3.1  Robust (Trial-Based) RTDP                        The experiments conducted illustrate the behavior of robust
Robust RTDP differs from the original RTDP in that:   LRTDP. To that end, it is compared to Bagnell et al.’s Robust
                                                      Value Iteration, as well as LRTDP. In all cases, the conver-
  • Each time the algorithm is updating a state’s evaluation,              −3
    an opponent is looking for a worst local model which gence criteria is  = 10 for LRTDP, and for VI we stop
    serves to compute the Q-values.                   when the maximum change in a state long-term cost over one
                                                      iteration is less than 10−3.
  • For exploration purposes, the algorithm follows dynam-
    ics of the system that consider all possible transitions 4.1 Heart
    (using model m ).
                 d                                    In this ﬁrst experiment, we compare a non-robust optimal pol-
  • “Relevant” states are now the states reachable by follow- icy with a robust one on the small example of Fig. 1-b. Ta-
    ing any optimal policy under any possible model.  ble 1 shows the theoretical expected long-term costs of each
  From this, we adapt to our context convergence Theorem policy on the normal (most probable) model, as well as the
3 from [Barto et al., 1995] and the corresponding proof, dis- pessimistic and optimistic ones. The robust policy is largely
cussing mainly its changes.                           better in the pessimistic case.
Proposition 1. In uncertain undiscounted stochastic short-
est path problems, robust Trial-Based RTDP with the initial Table 1: Theoretical evaluation of robust and non-robust poli-
state of each trial restricted to a set of start states, converges cies on various models, which match empirical evaluation.
                      ∗
(with probability one) to J on the set of relevant states, and         Normal   Pessimistic Optimistic
the controller’s policy converges to an optimal policy (pos- Non-robust 2.90      8.90        1.70
sibly nonstationary) on the set of relevant states, under the Robust    3.33      3.33        3.33
same conditions as Theorem 3 in [Barto et al., 1995].
Proof. The proof outline is:
                                                      4.2  Mountain-Car
 1. As shown in Sec. 2.3, robustness is achieved by consid-
    ering a Stochastic Shortest Path Game (SSPG). In par- We use here the mountain-car problem as deﬁned in [Sutton
    ticular, this one is a sequential SSPG, for which Bert- and Barto, 1998]: starting from the bottom of a valley, a car
    sekas and Tsitsiklis [1996] establish value iteration and has to get enough momentum to reach the top of a mountain
                                                      (see Fig. 3). The same dynamics as described in the moun-
    Q-learning algorithms. Provided all states are visited in-      5
    ﬁnitely often, asynchronous value iteration can also be tain car software have been employed. The objective is to
    used to solve sequential SSPGs (AVI-SSPG).        minimize the number of time steps to reach goal.
 2. We adapt the proof of Barto et al. [1995] to transform 5http://www.cs.ualberta.ca/˜sutton/· · ·
    AVI-SSPG to an RTDP algorithm (RTDP-SSPG). The    MountainCar/MountainCar.html                                                                                       Long-Term Cost Function
                                     goal                                                  Example Path
               road reaction                                  V(x,v)
                        acceleration
                                                              600

                                                              500

                                                              400

                                                              300

                                                              200
                      gravity                                 100

                                                            -0.07 0

        −1.2          position          0.6                    -0.035

                                                                     0                         0.425
                                                                v                          0
          Figure 3: The mountain-car problem.                         0.035         -0.425
                                                                               -0.85    x
                                                                         0.07
                                                      a) Value Iteration
  The continuous state-space is discretized (32 × 32 grid)
                                                                                       Long-Term Cost Function
and the corresponding uncertain model of transitions is ob-                                Example Path
tained by sampling 1000 transitions from each state-action    V(x,v)
pair (s, a). For each transition, we computed intervals in   1600
                                                             1400
which the true model lies with 95% conﬁdence.                1200
                                                             1000
                                                              800
                                                              600
Results —  Preliminary remark: simulating a path generally    400
shows a car oscillating several times before leaving the valley.  200
                                                               0
This has two main explanations: 1- the speed gathering is   -0.07
just sufﬁcient to reach the summit, but not over it; and 2- the -0.035
                                                                     0                         0.425
discretized model is not accurate enough and applying the       v                          0
policy obtained on the true mathematical model (instead of            0.035         -0.425
                                                                               -0.85    x
                                                                         0.07
the discretized one) should be much better.           b) Robust Value Iteration
  Fig. 4 shows the long-term cost function obtained by using

value iteration, LRTDP, and their robust counterparts on the                           Long-Term Cost Function
                                                                                           Example Path
mountain-car problem. The x and y axes are the car’s position V(x,v)

and speed. The z axis is the expected cost to the goal. On the  600
surface is an example path from the start state to a goal state:  500
it follows the greedy policy under the average model.         400
  The general shape of the surface obtained is always the     300
                                                              200

same, with some unexplored parts of the state-space in        100

LRTDP and Robust LRTDP (as expected). The vertical scales   -0.07 0

are much larger in robust cases. This reﬂects the fact that    -0.035

reaching the goal is much more time-consuming under a pes-           0                         0.425
simistic model. Because J can here be interpreted as the av-    v                          0
                                                                      0.035         -0.425
erage time to the goal, these graphs show how a small uncer-                   -0.85    x
                                                                         0.07
tainty can lead to longer policies. Here the times are multi- c) LRTDP
plied by more than 2.5.
                                                                                       Long-Term Cost Function
  While executing the four different algorithms, an eval-                                  Example Path
uation of the current greedy policy was made every 10 ∗       V(x,v)
nStates = 10 240 Q                                           1600
                  -value updates. The result appears in      1400
Fig. 5, the y axis being the expected cost to the goal from  1200
                                                             1000
the start state. On both sub-ﬁgures, LRTDP-based algo-        800
rithms obtain good policies quickly, but have slow conver-    600
                        6                        6            400
gence times of VI=2.83 × 10 updates, LRTDP=6.76 × 10 ,        200
rVI=8.31 × 106, rLRTDP=11.06 × 106.                         -0.07 0
                                                               -0.035
                  [                      ]
  Other experiments Buffet and Aberdeen, 2004 also con-              0                         0.425
ﬁrm these results, with one example showing that LRTDP can      v                          0
                                                                      0.035         -0.425
have a faster convergence than VI, and another one illustrat-                  -0.85    x
                                                                         0.07
ing this approach on a temporal planning problem.     d) Robust LRTDP
5  Discussion and Conclusion                          Figure 4: Long-term cost functions for the mountain-car
                                                      problem. In each case, an example path is generated based
A simple extension to this work, suggested by Hosaka et al.
                                                      on the most likely model.
[2001], is to ﬁnd the set of worst models as we do in this