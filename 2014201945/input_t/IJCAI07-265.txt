               Learning from the Report-writing Behavior of Individuals

           Mohit Kumar                      Nikesh Garera∗                Alexander I. Rudnicky
     Carnegie Mellon University        Johns Hopkins University         Carnegie Mellon University
          Pittsburgh, USA                   Baltimore, USA                    Pittsburgh, USA
       mohitkum@cs.cmu.edu                ngarera@cs.jhu.edu                  air@cs.cmu.edu


                    Abstract                          maries. We would however argue that end-user involvement
                                                      is more likely to generate quality products that reﬂect both
    We describe a brieﬁng system that learns to pre-  functional needs and user preferences and is indeed worth
    dict the contents of reports generated by users who the effort. The current paper describes the application of
    create periodic (weekly) reports as part of their nor- this approach in the context of an engineering project class in
    mal activity. The system observes content-selection which students were expected to produce weekly summaries
    choices that users make and builds a predictive   of their work. For the course we worked with, a reporting re-
    model that could, for example, be used to generate quirement was already in place. We minimally modiﬁed the
    an initial draft report. Using a feature of the inter- process (by augmenting and instrumenting the web-based re-
    face the system also collects information about po- porting software already in use) to collect relevant data from
    tential user-speciﬁc features. The system was eval- users; apart from this their normal activities were not per-
    uated under realistic conditions, by collecting data turbed. The data collected was used to perform off-line learn-
    in a project-based university course where student ing experiments.
    group leaders were tasked with preparing weekly     While each student produces their own logs, their team
    reports for the beneﬁt of the instructors, using the leader was additionally tasked with reading the logs and se-
    material from individual student reports.         lecting log entries (referred as ‘items’ hereon) suitable for a
    This paper addresses the question of whether data summary of group activity. Learning is based on information
    derived from the implicit supervision provided by about which items are selected by the team leader. We fur-
    end-users is robust enough to support not only    ther collect data identifying the “features” deemed important
    model parameter tuning but also a form of feature by the leader creating the summary. This is done by asking
    discovery. Results indicate that this is the case: sys- the team leader to highlight key words/phrases in the selected
    tem performance improves based on the feedback    summary items (the phrases that they believe led them to se-
    from user activity. We ﬁnd that individual learned lect that item). This might be thought of as having the users
    models (and features) are user-speciﬁc, although  directly select Summary Content Units [Nenkova and Pas-
    not completely idiosyncratic. This may suggest that sonneau, 2004] or identify Rouge-like units (n-grams, word
    approaches which seek to optimize models globally sequences) [Lin, 2004]. Although in this paper we focus on
    (say over a large corpus of data) may not in fact extractive summarization, we believe that this approach can
    produce results acceptable to all individuals.    be extended to prime information for abstractive summariza-
                                                      tion, one of our long-term goals for this work.
                                                        The plan of this paper is as follows. We describe relevant
1  Introduction                                       ideas from the literature. We then describe the domain and
In this paper we describe a personalized learning-based the report generation tasks performed by the class and give
approach to summarization that minimizes the need for details of the data collected. The Learning System section
learning-expert time and eliminates the need for expert- outlines key details (feature representation, weighting, rank-
generated evaluation materials such as a “gold standard” sum- ing, model selection etc.). After describing the basic Learning
mary, since each user provides their own standard. While the framework, we describe the Experiments and Results from
work we describe is speciﬁc to summarization, we believe the different settings of our system followed by the Conclu-
that the basic techniques are extensible to other learning situ- sions.
ations that share the characteristic of repeated execution and
the availability of unambiguous user feedback.        2   Related Work
  Of course this comes at a cost, which is the end-user time
                                                      Although automatic text summarization has been explored
needed to teach the system how to produce satisfactory sum-
                                                      for almost a half-century [Luhn, 1958], most current work
  ∗This work was done in part when the author was a student at focuses on generic newswire summaries [DUC, 2002 2003
CMU.                                                  2004 2005 2006].  [Radev and McKeown, 1998] provide

                                                IJCAI-07
                                                  1641a useful distinction between brieﬁngs and the general con-
cept of summaries where they focus on generating multi-
document newswire brieﬁngs. [Mani et al., 2000] also fo-
cus on generating brieﬁngs, however their approach contrasts
with ours and is in a different domain. They assume that users
are given an outline of the brieﬁng and then try to populate the
outline, whereas our system does not provide an initial struc-
ture (except for a generic template). On the other hand [Mani
et al., 2000] extend their work to multimedia inputs. From
our current perspective we believe that the system should fo-
cus on identifying important information, allowing the user to
concentrate on its organization into a coherent presentation.
  A few personalized interactive learning systems have also
been proposed for summarization. [Amini, 2000] describes
a query-relevant text summary system based on interactive
learning. Learning is in the form of query expansion and sen-
tence scoring by classiﬁcation. [Leuski et al., 2003] have ex-
plored interactive multi-document summarization, where the
interaction with the user was in terms of giving the user con-
trol over summary parameters, support rapid browsing of doc- Figure 1: System Demonstration:Acquiring user based fea-
ument set and alternative forms of organizing and displaying tures via phrase highlighting
summaries. Their approach of ‘content selection’ to identify
key concepts in unigrams, bigrams and trigrams based on the
likelihood ratio [Dunning, 1993] is different from our statis- writers were not experienced researchers who were asked to
tical analysis and is of some interest. [Zhang et al., 2003] generate weekly reports but were students taking a course that
have proposed a personalized summarization system based already had a requirement for weekly report generation. The
on the user’s annotation. They have presented a good case course was project-based and taught in a university engineer-
of the usefulness of user’s annotations in getting personal- ing school. The students, who were divided into groups work-
ized summaries. However their system differs from the cur- ing on different projects were required to produce a weekly
rent one in several respects. Their scenario is a single docu- report of their activities, to be submitted to the instructors.
ment newswire summary and is very different from a brieﬁng. Each group had well-deﬁned roles, including that of a leader.
Also, their system is purely statistical and does not include the Students in the class logged the time spent on the different
concept of a human-in-the-loop that improves performance. activities related to the course. Each time-log entry included
  [Elhadad et al., 2005] have applied personalized summa- the following ﬁelds: date, category of activity, time spent and
rization in the medical domain where, given a patient proﬁle details of the activity. The category was selected from a pre-
and a set of documents from a search query, the system gen- deﬁned set that included Coding, Group Meeting, Research
erates a personalized summary relevant to the patient. The and others (all previously set by the instructors).
active learning community [Raghavan et al., 2006] has also An example time-log entry is:“Visited the Voyager site.
been moving towards using user feedback for identifying im- Spoke with the marine architects. Got a feel for the possibili-
portant features. Some other interesting domains in which ties and scope of the project.1” The data in this domain is not
summarization systems have been developed are technical as structured and grammatical as newswire documents. It is
chat [Zhou and Hovy, 2005], newsgroup conversations [New- also different from chat-like/conversational corpora in its us-
man and Blitzer, 2002], email threads [Rambow et al., 2004], age of words although it does contain a few spelling mistakes.
spoken dialogues [Zechner, 2001] as well as others.   One important characteristic of the data is that the content of
  [Garera and Rudnicky, 2005] describe a summarization the time-logs changes over time as the project progresses. So
system for a recurring weekly report-writing taking place in a the word-based features that may indicate importance in the
research project. They found that knowledge-engineered fea- earlier weeks may not be useful in the later weeks.
tures lead to the best performance, although this performance The task of the team leader is to prepare a weekly report
is close to that based on n-gram features. Given this, it would for the beneﬁt of the instructor, using the time-log entries of
be desirable to have a procedure that leverages human knowl- the individual team members as raw material. As the students
edge to identify high-performance features but does not re- were already using an on-line system to create their logs, it
quire the participation of experts in the process. We describe was relatively straightforward to augment this application to
an approach to this problem below.                    allow the creation of leader summaries. The augmented ap-
                                                      plication provided an interface that allowed the leader to more
3  Target Domain                                      easily prepare a report and was also instrumented to collect
                                                      data about their behavior. Instrumentation included mouse
We identiﬁed a domain that while similar in its reporting
structure to the one studied by [Garera and Rudnicky, 2005] 1Italicized text shows the report-writer’s highlighting for this
differed in some signiﬁcant respects. Speciﬁcally, the report- time-log.

                                                IJCAI-07
                                                  1642and keyboard level events (we do not report any analysis of it would not be meaningful to train models on non-observed
these data in this paper).                            features.
                                                        The resulting model is used to classify each candidate item
3.1  Data Collection Process                          as belonging in the summary item or not. The conﬁdence
Following [Garera and Rudnicky, 2005], the leader selected assigned to this classiﬁcation was used to rank order the raw
items from a display of all items from the student reports. items and the top 5 items were designated as the (predicted)
Figure 1 shows the interface used for the data collection. The summary for that week. The following sections explain the
leader was instructed to go through the items and select a learning system in more detail.
subset for inclusion in the report. Selection was done by
highlighting the “important” words/phrases in the items (de- 4.1 Classiﬁer Settings
scribed to the participant as being those words or phrases that Features
led them to select that particular item for the report). The The features used in the classiﬁer are words (or unigrams),
items with highlighted text automatically become the candi-
                2                                     stemmed and with stop words removed. We experimented
date brieﬁng items. The highlighted words and phrases sub- with three different classes of features: a) NEfalse: only
sequently were designated as custom user features and were unigrams b) NEclass: unigrams + abstracted named enti-
used to train a model of the user’s selection behavior. Exami- ties using only the NEclass label (i.e., person, organization
nation of the data indicated that users followed these instruc- etc.) eg. ’White House’ is treated as a unique token (‘Lo-
tions and did not, for example, simply select entire items or cation’) representing its class.3 c) NEuniq: raw unigrams +
just the ﬁrst word.                                   each named entity substituted by a unique token eg. ’White
3.2  Data Collected                                   House’ is treated as a unique token (‘Location-1’). We used
                                                      BBN-Identiﬁnder [BBN-Technologies, 2000] for extracting
We were able to collect a total of complete 61 group-weeks the Named Entities.
of data. One group-week includes the time logs written by
the members of a particular group and the associated extrac- Feature Extraction
tive summaries. The class consisted of two stages, design We extract the features in two different settings. Firstly, we
and implementation, lasting about 6 and 9 weeks respectively. use the entire sentences for extracting the unigram features:
The groups and roles were reconstituted after the ﬁrst stage. FRaw. Secondly, we combine the entire sentence features
This meant that after the ﬁrst stage we had new “authors” and with the user-speciﬁc features:FUser (FUser ⊆ FRaw)which
could not combine the ﬁrst stage data with the second stage is similar to the idea of combining the ‘context’ and ‘anno-
into a continuous sequence. There were 5 groups in the ﬁrst tated keywords’ described in [Zhang et al., 2003].4 The de-
stage with average 4.8 students per group and 5 groups in tails of how the ﬁnal scores were computed are given below.
the second stage with average 3.6 students per group (several
students dropped the course after the ﬁrst few weeks). There System Description
were on average 4.5 weekly summaries per group for Stage 1 Standard Information Retrieval (IR) metrics are used to score
and 7.5 weekly summaries per group in Stage 2. As is evident the features. We did not ﬁx the parameters in the schemes as
from the averages, not all the groups submitted a summary ev- the characteristics of the data did not particularly recommend
ery week. To provide consistent data for development, testing any particular setting. The tunable parameters in the schemes
and analysis of our system, we selected those 3 groups from and their possible values are: a) Term weighing method - TF
the later stage of the class that produced reports most consis- (term frequency), TF.IDF, Salton-Buckley(SB) [Yates, 1999].
tently (these are described further in the Evaluation section b) Corpus for measuring IDF: For any word, the inverse doc-
below).                                               ument frequency can be obtained by considering either the
                                                      documents in the training set or the test set or both. Therefore
4  Learning System                                    we have three different ways of calculating IDF. c) Normal-
                                                      ization scheme for the various scoring functions: no normal-
We modeled our Learning process on the one described by ization, L1 and L2.
[Garera and Rudnicky, 2005]; that is, models were rebuilt on Feature scoring in the ﬁrst setting of extracting unigram
a weekly basis, using all training data available to that point features FRaw is straightforward using the above mentioned
(i.e., from the previous weeks). This model was then used to IR parameters (TF, TF.IDF or SB). For combining the scores
predict the user’s selections in the current week. For example, under the second setting with the ‘user-speciﬁc’ features we
a model built on weeks 1 and 2 was tested on week 3. Then used the following equation:
a model built on weeks, 1, 2 and 3 was tested on week 4, and
                                                                      S  =(1+α)   ∗ S
so on.                                                                 f             fbase            (1)
  Because the vocabulary varied signiﬁcantly from week to
week, we trained models using only those words (features) 3The idea being to capture user’s preference wrt particular
that were to be found in the raw data for the target week, since classes of NEs i.e. the user prefers to select an item where a per-
                                                      son and organization are mentioned together.
  2The interface enforces a minimum (10%) and maximum (40%) 4We also experimented with using just the user-speciﬁc features
number (as percentages of the total number of items for the particu- in isolation but found these less useful than a combination of all
lar week) of item selections to make sure some summary is reported. features.

                                                IJCAI-07
                                                  1643                            Group1                    Group2                    Group3
         Week No   TNI   ANW     NIS   ANS    TNI   ANW    NIS   ANS    TNI   ANW    NIS   ANS
             1       8     8      2     2     15     8.6    6     2      21    6.9    5      3
             2      10     11     4     1     15     8.6    5     2.8    13    9.5    3      3
             3      26     7      7    2.6    24     7.4    7     1.4    22    7.5    4     1.3
             4      18    7.9     5    1.6    17     5.5    3     2.7    11    3.2    2      2
             5      25    8.6     6    4.3    18    10.7    3     8.3    18    7.7    4     7.3
             6      15    8.5     2    2.5    16    14.7    7     6.1    12    10.3   2      5
             7      20    8.4     3    1.3    24    13.4    7     7.4    17    11.7   3     3.3
             8      25    9.3     6    3.8    26     7.2    6     1.7    16    9.5    4      8
             9      28    7.3     4    2.5     -      -     -      -     -      -      -     -

Table 1: Table showing the week-wise item details for the three selected groups. TNI - Total Number of Items in that week,
ANW - Average number of words per Item in that week, NIS - Number of Items selected for that week, ANS - Average number
of words highlighted per selected Item for that week

where α is the weight contribution for the user-speciﬁc fea- 5 Experiment and Results
        S
tures and fbase is the base score (TF or TF.IDF or SB). We
              α                                       We selected for experimentation the three groups that most
empirically ﬁxed to ‘1’ for the current study.        consistently generated complete weekly datasets . These
  We tested the above mentioned variations of feature de- groups had 9, 8 and 8 complete weeks of data. Detailed statis-
scription, feature extraction and feature scoring using four tics of the data are shown in Table 1. Since the groups had
learning schemes: Naive Bayes, Voted Perceptron, Support different number of weeks, we grouped the observations into
Vector and Logistic Regression. In the event, preliminary three phases by combining the results from successive weeks
testing indicated that Support Vector and Logistic Regression to create a phase. Merging the data into three phases is also
were not suited for the problem at hand and so these were consistent with our intuition about the task, that is, an activity
eliminated from further consideration. We used the Weka has an initial starting period, middle activities and then the
[                   ]
Witten and Frank, 2000 package for developing the system. closing activity. We combined the observations with a sliding
                                                      window to form the three phases. The window size and the
4.2  Evaluation                                       overlap were different for the groups, though the same within
The base performance metric is Recall, deﬁned in terms of a group.
the items recommended by the system compared to the items Since we treated modeling technique as a free variable,
ultimately selected by the user.5 We justify this by noting we ran experiments for all the possible combinations of fea-
that Recall can be directly linked to the expected time sav- ture and system dimensions described in the previous section.
ings for the eventual users of a prospective summarization The overall best performing system based on the jointly op-
system based on the ideas developed in this study. The ob- timized metrics of WMR and Slope was selected as the ﬁnal
jective functions that we used for selecting the system model model from which to generate the system predictions. The
(built on the basis Recall) are:                      ﬁnal model has the following system parameters:- Learning
  1. Weighted mean recall (WMR): of the system across all Scheme: Voted Perceptron, Term Weighing Scheme: Salton-
weeks. The weeks are given linearly increasing weights (nor- Buckley, Document Frequency Set: Training set, Normaliza-
malized) which captures the intuition that the performance in tion scheme: None. The feature set that gave best perfor-
the later weeks is increasingly more important as they have mance was NEuniq.
consecutively more training data.                       Figure 2 shows the key results of our experiments with the
2. Slope of the phase-wise performance curve (Slope): We above-mentioned model. The performance is averaged across
ﬁrst calculate the three phase-wise recall performance values the three groups.
(normal average of the recall values) and then ﬁnd out the Figure 2(a) shows that the performance obtained with
slope of the curve for these three points.            generic n-gram features is comparable to the performance
  Note that these metrics are used as a selection criterion incorporating user-speciﬁc features. While one would have
only. Results in Figure 2 are stated in terms of the original hoped that user-speciﬁc features would do better than generic
Recall values averaged over the phase and across the three ones, it’s clear that users select consistent personal features.
users. We compare these with the results for the random base- Figure 2(b) shows that user-selected features are at least as
line. The random baseline is calculated by randomly selecting good as those selected based on information gain, arguably
items over a large number (10000) of runs of the system and an “optimal” way to select useful features. Table 2 shows
determining the mean performance value. 6             the mean number of active features in each phase (that is,
                                                      how many previously identiﬁed features occur in the test set).
  5We are not doing a ROUGE-like [Lin, 2004] evaluation as we Here we see something interesting, which is that information
have modeled our problem as creating a draft summary in terms of
the log items to be selected and not a ﬁnal polished summary. tionally used such as the ‘ﬁrst’ sentence of a document etc. are not
  6For our domain, other baseline methods which have been tradi- suitable because of a characteristically different corpora.

                                                IJCAI-07
                                                  1644    0.7                                0.7                                0.7

    0.6                                0.6                                0.6

    0.5                                0.5                                0.5

    0.4                                0.4                                0.4
                                                                         Recall
    Recall 0.3                         Recall 0.3                         0.3

    0.2                                0.2                                0.2
                                                                                           Cross-Group
                      User-Combined                       IG-Combined
                                                                                           IG-Combined
                                                          User-Combined
    0.1               Raw N-grams      0.1                                0.1              User-Combined
                                                          Raw N-grams
                                                                                           Raw N-grams
                      Baseline                            Baseline                         Baseline
     0                                  0                                  0
           123                                123                                123
                 Phase                               Phase                             Phase
  (a) Recall Values for the Final model for (b) Recall comparison between Informa- (c) Recall comparison between Individual
  Individual Users comparing the Raw N- tion Gain selected features and User se- user training and cross-group training
  gram features with User-Combined fea- lected features
  tures

Figure 2: Figure showing the various experiments. ‘Raw N-grams’ represents the entire n-grams in the items. ‘User-combined’
represents the group-wise (single user) selected features combined with Raw n-gram features. ‘IG-Combined’ is the Information
Gain selected features combined with Raw n-grams. ‘Cross-group’ is the training data pooled across groups and the user’s
selections combined with Raw n-grams.

gain (IG) selects very few features, while humans select a Phase No  IG-selected SU-selected  Overlap
broader number, not only those that might directly predict    1          1.1         5.5         0
item importance, but also features that could in the future be 2         1.3        11.1        0.6
of use. It is also curious that there is little overlap between the 3    1.6        17.6        0.4
IG selections and the human ones. One interpretation is that
humans are selecting a broader and potentially more robust Table 2: Table showing the Phase-wise number of features
set of features. The latter is supported by the observation that under the Information Gain (IG) and Single User (SU) se-
the IG-based model fails to select any active feature 27% of lected conditions. Last column shows the features that are
the time, while the human model fails only 9% of the time for common between the two techniques.
this reason.
  Figure 2(c) shows what happens if we pool user-selected Phase No   CG-selected  SU-selected Overlap
features across any two groups and use these to train a sin- 1           5.5         5.5        2.6
gle model for testing performance on the third group. Here   2          12.1         11.1       5.1
we see something different: cross-group features do not per-
                                                             3          20.1         17.6       8.9
form as well as group-speciﬁc features. This suggests that
the speciﬁc features do matter, as they reﬂect a particular Table 3: Table showing the Phase-wise number of features
group’s activities (as well as perhaps the leader’s emphases under the Cross-group pooled (CG) and Single User (SU)
in his or her reports). Table 3 is comparable to Table 2 and training data conditions. Last column shows the features that
indicates that, while there is better overlap, pooling data in- are common between the two techniques.
troduces signiﬁcant error into the model. Together these re-
sults suggest that important features are both task-speciﬁc and
user-speciﬁc. The latter interpretation is consistent with the els are sufﬁciently consistent to predict, with increasing ac-
                [                      ]
results reported by Garera and Rudnicky, 2005 .       curacy, the summarization behavior of these users. We also
  Table 4 shows the most frequent active features selected show that naive end-users are able to consistently select fea-
under the conditions of Single User selection, Information tures at least as well as an automatic process. There is more-
Gain and the Cross-group pooled data.                 over evidence that the feature sets selected by humans may
                                                      turn out to be more robust in the long run than automatic
6  Conclusions                                        features (since predictive power is spread over a larger num-
We show that consistent summarization models can be built ber of features). Although this in itself is a useful result, it
from relatively sparse user-generated data and that these mod- also opens the possibility of understanding and eventually au-

                                                IJCAI-07
                                                  1645