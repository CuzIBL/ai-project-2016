                        Temporal-Difference Networks with History

                                 Brian Tanner and Richard S. Sutton
                                          University of Alberta
                     Reinforcement Learning and Artiﬁcial Intelligence Laboratory
                                 Edmonton, Alberta, Canada T6G 2E8
                                    {btanner,sutton}@cs.ualberta.ca

                    Abstract                          represent any nth-order Markov model or partially observ-
                                                      able Markov decision process (POMDP) [Singh et al., 2004;
    Temporal-difference (TD) networks are a formal-   Littman et al., 2002]. Further, they showed that the size of the
    ism for expressing and learning grounded world    PSR scales at least as well as the existing approaches; a linear
    knowledge in a predictive form [Sutton and Tan-   PSR model is at least as compact as the equivalent POMDP
    ner, 2005]. However, not all partially observ-    or nth-order Markov model. TD networks are a generaliza-
    able Markov decision processes can be efﬁciently  tion of linear PSRs and therefore inherit their representational
    learned with TD networks. In this paper, we ex-   power [Singh, 2004].
    tend TD networks by allowing the network-update     TD networks have been applied successfully to simple en-
    process (answer network) to depend on the recent  vironments, both fully and partially observable. Although TD
    history of previous actions and observations rather networks have the expressive power to accurately model com-
    than only on the most recent action and observa-  plex partially-observable environments, we show that the ex-
    tion. We show that this extension enables the so- isting learning algorithm is insufﬁcient for learning some of
    lution of a larger class of problems than can be  these models.
    solved by the original TD networks or by history-   In this paper, we explore the classes of environment whose
    based methods alone. In addition, we apply TD net- model could not previously be learned and we improve TD
    works to a problem that, while still simple, is sig- networks so that they can learn models of these environments.
    niﬁcantly larger than has previously been consid-   In Section 1 we review the TD networks speciﬁcation. In
    ered. We show that history-extended TD networks   Sections 2 and 3 we examine the class of problems that we
    can learn much of the common-sense knowledge of   have been unable to learn with the existing speciﬁcation of
    an egocentric gridworld domain with a single bit of TD networks, augment the speciﬁcation, and present new re-
    perception.                                       sults. We present the results of applying these augmented TD
                                                      networks to a more complex grid-world domain in Section 5.
                                                      Finally we conclude and discuss the direction of our future
Temporal-difference (TD) networks are a formalism for ex- research in Section 6.
pressing and learning grounded knowledge about dynamical
systems [Sutton and Tanner, 2005]. TD networks represent 1 TD Networks without History
the state of the dynamical system as a vector of predictions In the following text we describe a speciﬁc instantiation of
about future action–observation sequences. Each prediction the original TD networks speciﬁcation that is instructive for
is an estimate of the probability or expected value of some understanding the details of our work. For information on the
future event. For example, a prediction might estimate the general speciﬁcation of TD networks we direct the reader to
probability of seeing a particular observation at the next time the original work [Sutton and Tanner, 2005].
step. The predictions generated at each time step are thought The problem addressed by TD networks is a general one
of as “answers” to a set of “questions” asked by the TD net- of learning to predict aspects of the interaction between a de-
work.                                                 cision making agent and its environment (a dynamical sys-
  Representations that encode the state of a dynamical sys- tem). At each of a series of discrete time steps t, and
tem as a vector of predictions are known as predictive repre- agent takes an action at ∈ A and the environment responds
sentations [Littman et al., 2002; Jaeger, 1998; Rosencrantz by generating an observation ot ∈ O. In this work, we
et al., 2004]. Predictive representations are a relatively will consider TD networks with two observations, 0 and
new area of research; as a community we are answering 1. The action and observation events occur in sequence,
fundamental questions about their possibilities and limita- at−1, ot, at, ot+1, at+1, ot+2. This sequence will be called
tions. So far, the results have been encouraging. Singh et experience. We are interested in predicting not just each next
al. have shown that one particular representation known as observation, but more general, action-conditional functions
linear predictive state representations (or linear PSRs) can of future experience.  The focus of this work current is on partially observable extraneous questions.
                                                                  i
environments—environments where the observation ot is not Formally, yt ∈ [0, 1], i = 0, . . . , n − 1 denotes the predic-
a sufﬁcient statistic to make optimal predictions about future tion for node i at time step t. The column vector of predic-
                                                                 0      n−1 T
experience (ot does not uniquely identify the state of the envi- tions yt = (yt , . . . , yt ) is updated according to a vector-
ronment). We will be using TD networks to learn a model of valued prediction function with modiﬁable parameter W:
the environment that is accurate and can be maintained over
time.                                                                    yt = σ(Wtxt)                 (1)
  A TD network is a network of nodes, each representing This prediction function corresponds to the answer net-
a single scalar prediction. The nodes are interconnected by             m
                                                      work, where xt ∈ <  is a feature vector, Wt is a n × m
links representing target relationships between predictions, matrix of weights, and σ is the S-shaped logistic function
observations, and actions. These nodes and links determine σ(s) = 1 . The feature vector is a function of the pre-
a set of questions being asked about the data and predictions, 1+e−s
                                                      ceding action, observation, and node values.
and accordingly are called the question network.
                                                        In previous work on TD networks, x had a binary compo-
  Each node on the TD network is a function approximator                              t
                                                      nent for each unique combination of the current observation
that outputs a prediction using inputs such as the current ob-
                                                      and previous action (one of which is 1, the rest 0), and n
servation, the previous action, and the predictions made at the
                                                      more for the previous node values y . Additionally, x has
previous time step. This computation part of the TD network                         t−1              t
                                                      a bias term (constant value of 1). In our experiments we also
is thought of as providing the answers to the questions, and
                                                      included real valued features with the complement of the pre-
accordingly is called the answer network.
                                                      dictions from the last time step (1 − y ), although we later
  Figure 1 shows a typical question network. The question                             t−1
        0                                             found this was not necessary. The learning algorithm for each
of node y at time t is ‘If the next action is a1, what is the    ij
                                                      component wt of Wt is of the form
probability that the next observation ot+1 will be 1?’. Sim-
           2
ilarly, node y asks ‘If the next action is a1, what will node   ∆wij = α(zi − yi)(yi)(1 − yi)xjci,    (2)
y0 predict at time t + 1?’. This is a desired relationship be-     t      t    t  t      t  t t
                                                                                     i               i
tween predictions, but also a question about the data. We can where α is a step-size parameter, zt is a target for y de-
                                                                                    i
unroll this interpredictive (or TD) relationship to look at the ﬁned by a question network, and ct ∈ {0, 1} corresponds
extensive relationship between y2 and the data, which yields to whether the action condition for yi was met at time t.
the question ‘If the next two actions are a1, what is the prob-
ability that ot+2 will be 1?’.                        2   Cycle-World Counterexamples
  In a fully observable (Markov) environment, it is natural
for the question network to be a set of questions that are In our experimentation we found that TD networks are able
in some way interesting to the experimenter. In partially- to solve certain, but not all of our small testing problems.
observable environments the structure of the question net- The success of learning does not seem to be directly corre-
work has additional constraints; the answers should be a suf- lated with the complexity of the environment. In previous
ﬁcient statistic that accurately represents the state and can be work, TD networks were able to learn an accurate model of
updated as new data becomes available. Although the ques- a partially-observable seven-state deterministic random walk
tion of how to discover a question network that expresses a as well as the probabilities in a fully observable stochastic
minimal sufﬁcient statistic is important, it is tangental to the random walk [Sutton and Tanner, 2005]. Since that time, we
focus of this work. The question networks we use are not have discovered that TD networks are unable to learn an accu-
minimal, and a network like that in Figure 1 likely contains rate model of certain small partially-observable deterministic
                                                      environments. Careful analysis has determined that the ques-
                                                      tion networks that were used were sufﬁcient to represent the
                                                      appropriate model, so the issue must lie somewhere in the TD
                        Ot+1
                 a1             a2                    network speciﬁcation.
                                                        Figure 2 presents a simple example of a task and question
              0                    1
         a1  y   a2                y                  network for which the solution is representable but not learn-
                                                      able by TD networks without history. The cycle world con-
        y2         y3        y4         y5
     a1    a2   a1    a2  a1     a2  a1    a2         sists of the four states shown on the left. The current state
                                                      of the system cycles clockwise through the states. There is a
      6    7     8    9
      y   y      y   y                                single observation bit that is 1 at the top of the cycle, and 0
                                                      at all other times. On the right is a question network which
                                                      asks what the observation bit will be one, two, and three time
                                                      steps in the future. Recall that at time t, yt is calculated as a
Figure 1: Symmetric action-conditional question network. function of (yt−1, at−1, ot, Wt). If we assume that the yt−1
The network forms a symmetric tree, with a branching fac- is correct, there is a solution for the weights that will keep yt
tor of |A|. This example has depth d = 4. Some of the labels correct at each successive time step. Unfortunately, yt−1 will
have been left out of this diagram for clarity, each of these never be correct; the solution exists but will not be found.
nodes should have a label yi and each is conditioned on some To illustrate this, we must look at the question network,
action.                                               and remember that it speciﬁes target values for the answernetwork. At the start of training, yt−1 will be likely be incor-       Features Initial Final
rect. There are no actions in this environment, so the current           1        1    1
observation ot is the only useful input feature in xt. For the       history = 000 0   0
network to become correct, it is necessary that some sequence        history = 001 0   0
of questions can eventually be answered, starting or anchored        history = 010 0   0
                                                                     history = 011 0   0
only with knowledge of ot. Also note that when training be-
gins, the only node with a valid target is y0, because its tar-      history = 100 1   1
get is not a prediction, but rather the grounded observable          history = 101 0   0
                                                                     history = 110 0   0
value of ot+1. As training progresses, the agent interacts with      history = 111 0   0
the environment and some answers will be learned, anchored               0
                                                                         y t-1    .5   0
                                                                         1
only on the grounded observations. Eventually, the environ-              y t-1    .5   0
                                   0                                     2
ment will reach a point where ot = 0 and yt should be 1. The             y t-1    .5   1
information that distinguishes this case from the case where
           0                                   1
ot = 0 and yt should be 0 lies in a correct answer for yt−1.
                        1     0
Unfortunately, the target for yt−1 is yt . In this case, the cyclic Figure 3: Input vector for cycle world with 3-step history
dependency between the question network and the temporal and 3 levels of predictive nodes. On the left is the deﬁni-
ﬂow of information eliminates the possibility of the TD net- tion of each feature. The ﬁrst feature is the bias term. The
work learning a correct solution.                     next 8 features correspond to the 8 distinct 3-step histories
                                                      {ot−2ot−1ot} (not all are possible in this world). The ﬁnal 3
  After considering this problem, it became apparent that it features are the predictions from the previous time step. The
is necessary to ﬁnd an augmentation to the TD network spec- middle vector is a sample input vector for the third state from
iﬁcation to eliminate cyclic information dependencies. Such
                               i                  i   the top of the cycle world at the start of learning. At this point,
a dependency occurs when the target z for node i relies on y all of the predictions are at their initial value, .5. Finally, the
making accurate predictions. This dependency can be elim- rightmost vector is the input vector for the third state when
inated by providing additional input features to the TD net- learning is complete, all of the predictions are accurate.
work. This additional information acts as an anchor for the
TD network predictions.
                                                      3   TD Networks with History
  Incidentally, good approximate solutions to the cycle world
can be learned by TD networks consisting of a single node. The cycle world is a problem in which there is a simple re-
Clearly, there is no way that a single predictive node can solve lationship between the observations and recent experience.
this problem perfectly, but it can achieve very low error in an Methods that try to directly learn such relationships are called
unusual way. When ot = 1, the network predicts approxi- history-based methods. We will consider history-based meth-
mately .001. On the next time step, the network will multiply ods which predict ot+1 using a different variable for each
the previous prediction so as to predict approximately .01. unique k-length window of history where a k-length window
On the next step it will predict .1, and then ﬁnally .99 for the of history is deﬁned as at−kot−k+1...at−1ot. In this case, a
step where ot will again be 1. Unfortunately this solution is window of length 3 would be sufﬁcient to uniquely identify
not stable, and continued training leads to oscillation between each state of the system and thus would be able to make accu-
this sort of approximate solution and strictly predicting 0 at rate predictions. Incorporating short history into the feature
each step.                                            vector xt of a TD network should allow the TD network to
                                                      learn a correct solution to this problem. Figure 3 shows an
                                                      example of a hybrid input vector that uses 3 time steps of
                                                      history and 3 predictive nodes.
            Cycle World Question Network                In order to test the hypothesis that incorporating history
                                                      into the input vector xt of a TD network would allow it to
                 1            Ot+1                    solve a greater class of problem, we used a cycle world like
                                                      that in Figure 2, except with six states instead of four. This
                 0             y0                     size was chosen to clearly illustrate the effectiveness of dif-
                                                      ferent conﬁgurations of history and predictive nodes. On this
                                                      problem, we tested three different methods: (1) TD networks
                 0             y1                     as previously speciﬁed without history, (2) a simple history-
                                                      based approach, (3) a combination of TD networks and his-
                 0             y2                     tory together. For each method we used several values for the
                                                      step size parameter; the best of these was used as the perfor-
                                                      mance measure for that method. For each method and step
Figure 2: A counterexample for TD-network learning with- size, the network was trained for at least one million time
out history. On the left is a representation of the cycle world. steps. The 1-step prediction errors were averaged over the ﬁ-
This environment has four states that are cycled through de- nal 20000 steps to produce an overall performance measure
terministically. On the right is the associated question net- for each method.
work. There are no actions in this world.               The results, shown in Figure 4, show that the simple                                                                               State A

                                                                                1

                                                                   State D              State B
                                 History only
                                                                        0               0

 RMS         5

Error                                                                           0
                            3
                    4
            1
                            2                                                State C

                                       1
                                                       Figure 5: An indeﬁnite-memory problem, the four-state de-
                                                       terministic ring world. There are two actions in this world,
                 Length of History                     next and previous. Next advances in clockwise rotation while
                                                       previous advances in counter-clockwise rotation. Prediction
                                                       methods using a ﬁnite length history will lose localization af-
                                                       ter some number of transitions back and forth between states
 Figure 4: Performance on the 6-state cycle world of TD net- B and C or C and D.
 works extend to incorporate various lengths of history. The
 different lines correspond to different depths of the question
 network, as indicated by the numeric label.           can uniquely identify the current state of the environment.
                                                       We will refer to problems in this class as indeﬁnite-memory
                                                       problems.
 history-based method only performed well when it had
                                                         One simple example of an indeﬁnite-memory problem is
 enough history to solve the problem exactly. TD networks
                                                       the ring world shown in Figure 5. Because states B, C, and D
 without history correspond to the data points with history
                                                       are indistinguishable, there are sequences of actions that keep
 length one. These performances are better than history of
                                                       the environment in that subset of states and will eventually ﬁll
 length one, but not as good as the TD networks with his-
                                                       a ﬁxed-length memory with useless information. In contrast,
 tory. It is also interesting to notice that the TD network is
                                                       a TD network can easily represent this environment, and can
 able to solve the problem with a much shorter window than
                                                       never be made to forget its location in the environment.
 the history-based method alone. This illustrates that our com-
 bined algorithm is not simply using history instead of the pre- We applied TD networks with various depths of question
 dictive representation, but rather is leveraging the history to network and lengths of history to the 5-state ring world prob-
 learn a predictive representation. It is interesting that the per- lem. The performance measure used was the same as in the
 formance of the various combinations of history and predic- previous experiments, except in this case averaged over 25
 tive nodes do not follow a clear pattern. For example, when independent runs of 10 million time steps. The results are
 there are 2 predictive nodes, it appears that 2 or 4 steps of shown in Figure 6. As the history window increases, the
 history is better than having 3 steps. We have veriﬁed that the history-only method more closely approximates the correct
 minimum length of history required to solve the 6-state cycle solution. This improvement seems to diminish as the his-
 world exactly with 2 predictive nodes is a 4-step history. This tory window gets larger, and is further hampered by the fact
 means that the low error seen with 2 steps of history is a case that the number of unique histories grows exponentially with
 of the TD network stumbling on a good approximate solution the length of the window. With the predictive approach, the
 when it could not represent an exact solution (as discussed at problem is solved correctly with only 1 level of history and a
 the end of Section 2).                                predictive question network of depth 3.
                                                         It is interesting that, provided enough time, the TD network
                                                       can learn a correct model of this environment without history,
 4  Indeﬁnite-memory Problems                          something which it could not do for the cycle world. Espe-
 In the previous section, we introduced history to the TD net- cially puzzling was that these two problems seemed highly
 work speciﬁcation in order to eliminate cyclic dependencies related, the cycle world seemingly even less complex than
 and increase the class of problems where solutions can be the ring world. The fact that actions have inverses in the ring
 learned with TD networks. There appeared to be a tradeoff world eliminates the information ﬂow dependencies that ex-
 between predictive levels in the question network and lengths isted in the cycle world. In the ring world, the agent can in-
 of history. From this example alone, it may not be clear that crementally learn more and more about the environment. In
 the combined approach is superior to a history-only approach. early training, the agent can anchor itself when ot = 1 be-
 There is a potentially large class of problems that cannot be cause this observation uniquely identiﬁes this state. As time
 represented with a history-only approach, but can be repre- passes, the agent can learn accurate 1-step predictions from
 sented and solved by TD networks. Environments in this that anchored location. It can also learn 2-step predictions
 class are such that there is no ﬁnite length of history that that involve leaving this position and then returning imme-RMS                History Alone
Error
                  1 Predictive Level

               2 Predictive Levels                    Figure 7: The gridworld used in the ﬁnal experiment. The
                                                      agent’s location is represented by the triangle. The agent has
                            3 Predictive Levels       a very limited perceptual space, it observes 1 if the arrow
                                                      is pointing at a block, and 0 if the arrow is pointing at an
          1           2           3            4            5
                Levels of History                     open space. This agent has only two actions, forward and
                                                      turn. Forward will move the agent 1 space in the direction
                                                      that the arrow points (if it is not blocked), while turn will
                                                      rotate the arrow clock-wise 90 degrees. There are 104 unique
Figure 6: Performance on the 5-state ring world as a function environmental states in this world.
of length of history and depth of question network. The his-
tory method suffers from diminishing returns as size of the
history window increases. Learning also slows considerably There are other types of common-sense knowledge that are
because the number of unique histories that can be observed harder to learn with history. If you are facing wall, then
grows exponentially.                                  you turn around (180 degrees), observe no wall and walk
                                                      forward; what common-sense predictions could we make?
                                                      First, we would know that turning 180 degrees again will
diately. This process can continue until this chaining effect be clear, and walking forward from there will take us back
has allowed the agent to make accurate predictions from all to the wall. We also know that if we rotate 360 degrees
positions in the network.                             any number of times at any step of this process, the en-
                                                      tire process remains intact, nothing changes. This concept
5   Gridworld Experiments                             is impossible to learn with a ﬁxed length history. This sce-
                                                      nario exempliﬁes the conceptual and practical difference be-
In previous work, and so far in this paper, TD networks have
                                                      tween predictive representations and history-based represen-
been applied only to abstract problems with fewer than 10
                                                      tations. After facing a wall, turning 180 degrees and going
states. In this ﬁnal section of the paper we present suggestive
                                                      forward, a 3-level history-based approach knows ‘I am in
results for a gridworld environment that is an order of mag-
                                                      the state described as {wall, turn, clear, turn, clear, forward,
nitude more complex than those previously considered. The
                                                      clear}’. The predictive agent has a different representation,
gridworld in Figure 7 shows the environment that we chose.
                                                      more like ‘I am in the state where{P r(clear|turn, turn) =
In this environment, the agent has a single perceptual input,
                                                      1, P r(wall|turn, turn, forward) = 1, etc.}’.
a single bit indicating whether there is a wall directly in front
of it. The agent also has a very limited action space, it can We ran a TD network of depth 5 with a history of length
either attempt to move forward or it can rotate 90 degrees 6 in this environment for more than 50 million time steps to
clock-wise. We encourage the reader to consider two analo- give it the opportunity to learn about the causal structure of
gous tasks suitable for a human. First, consider sitting at a its environment. At the end of this time we took control of its
table with two buttons and a light bulb. You are told that the decisions to explore particular places in the world as shown
light bulb will turn on and off based on the buttons pressed in Figure 8. On the left, this ﬁgure shows the state of the en-
according to some unobservable process. If that unobserv- vironment at various points in time as the agent is controlled
able process were the map in Figure 7, could a human learn from state to state. On the right a representation of the agents
this problem?                                         predictions at each step is shown. These predictions show
  A second interpretation of the problem which should be that the agent has learned a great deal about its environment.
more familiar is navigating around a room with the lights off. At t = 0 the agent is in a corner, and we can see that it
At all times you can feel if you are touching a wall, otherwise knows there is a wall on its left side and a wall behind it.
nothing. What sorts of common-sense knowledge would you Notice just to the right of the ’A’ is a dark bar. This bar rep-
apply to this type of scenario? The simplest knowledge is that resents the prediction for turning right, going forward twice,
if you are touching a wall and you attempt to walk forward, and then turning right three more times. The fact that this bar
you will still be touching a wall. You would also know that 4 is dark indicates that the agent correctly knows something
consecutive 90 degree turns ends with the same observation about the nearest interior blocked cell. Above the ’A’, the
that it started with. These are the types of knowledge that can agent also predicts a wall and below the ’A’ it predicts gray.
be learned easily with a history-based approach.      This shows that the agent does not completely understand that