                          Intimate Learning: A Novel Approach for
                          Combining Labelled and Unlabelled Data
                                  Zhongmin Shi    and  Anoop Sarkar
                    School of Computing Science, Simon Fraser University, Canada
                                   {zshi1,anoop}@cs.sfu.ca

                    Abstract                          scope-limited features which are more speciﬁc to local sub-
                                                      sets of the data.
    This paper introduces a new bootstrapping method
    closely related to co-training and scoped-learning.
    The method is tested on a Web information ex-     2   The Information Extraction task
    traction task of learning course names from web   We apply intimate learning to the problem of identifying
    pages in which we use very few labelled items     course names from those pages identiﬁed as course web pages
    as seed data (10 web pages) and combine with      in WebKB which consists of web pages collected from four
    an unlabelled set (174 web pages). The over-      universities. 2 For our experiment we used: 10 web pages as
    all performance improved the precision/recall from seeds, 174 web pages as unlabeled training data and 40 web
    3.11%/0.31% for a baseline EM-based method to     pages as test data. All web pages are tokenized using space
    44.7%/44.1% for intimate learning.                and punctuation symbols. We used an EM-based decision list
                                                      learning algorithm [Collins and Singer, 1999] as our baseline
1  Intimate learning                                  method for combining labeled and unlabeled data (more de-
                                                      tails in §2.1). The course name feature model is initialized
The expensive nature of labelling data for machine learning
                                                      using the seed data and then trained using EM on the train-
methods and the lack of success in using purely unsuper-
                                                      ing data. Our observation on the training data indicated that
vised methods have motivated the study of learning meth-
                                                      the course number, which does not form part of our target la-
ods that combine labelled and unlabelled data. Successful
                                                      bel is likely to co-occur with a course name. Course names
methods in this area include bootstrapping methods like co-
                                                      have similar characteristics to other named entities like names
training and scoped-learning. In this paper we introduce a
                                                      of people or organizations. Compared with course names,
novel method called intimate learning for bootstrapping that
                                                      course numbers have far more regular forms, which usually
is related to these methods. The task is to learn the target
                                                      consist of the department abbreviation and the number of the
function h (X) ∈ Y where X is the set of all feature sets
         t                                            course, e.g., CMPT 825. As a result identifying a course num-
and Y is the set of class labels. The input to our learning al-
                                                      ber is easier and we take this class to be our intimate class
gorithm is a set of n labeled examples of the form (x , y ).
                                              i i     Y 0. Our target class Y determines whether a string is a course
x ∈  X has p features x , x , ..., x associated with the
 i          i         i1 i2    ipi                    name. In this paper, we have Y and Y 0 with one element each
ith example. y ∈ Y is the label of the ith example. In inti-
            i                                         (+course-name and +course-number), rather than a 2-class
mate learning, we ﬁnd another class label y0 ∈ Y 0 which is
                                     i                classiﬁcation task (+course-name, −course-name). See §2.1
relevant to y and we assume that a new function h0(X) ∈ Y 0
          i                                           for details on how test examples are handled. We explain the
requires fewer labelled items to learn. We say that Y 0 is rel-
                                                      details of our algorithm using Figure 1 (for now, ignore the
evant to Y if accurately ﬁnding y0 ∈ Y 0 can help in identi-
                            i                         dashed arrow). The algorithm is a two-stage process. The left
fying y ∈ Y in example i. We then create the new target
      i                                               half of Figure 1 illustrates the ﬁrst stage, in which the intimate
function h(X, h0(X)) ∈ Y that performs the same classiﬁca-
                                                      class is learned by a classiﬁcation algorithm that is identical
tion as h (X) does. We call Y 0 the intimate class, h0(X)
       t                                              to the EM-based baseline system (see §2.2). The second stage
the intimate function. Intimate learning is related to the
                                                      in the right half implements almost the same EM-based al-
co-training algorithm [Blum and Mitchell, 1998], in which
                                                      gorithm, but with course numbers added from the ﬁrst stage
for training examples (x , y ), x is decomposed into a pair
                     i i   i                          as an extra feature towards learning of the course name (the
(x , x ) corresponding to two different “views”, and the tar-
  i1 i2                                               darker line with an arrow).
get function h(X) = h1(X1) = h2(X2) predicting a sin-
gle label class. While in our model, X has only one “view” 2.1 EM Learning for course number identiﬁcation
but labeled into two classes (target and intimate classes), i.e.,
          0    1                                      The features used for identifying a course number are:
ht(X) 6= h (X) . Other related work is scoped-learning
[Blei et al., 2002], which uses a classiﬁer trained on global • The HTML format of the course number, which is the
features from the entire training data and classiﬁers trained on pair of HTML tags before and after the course number

  1Intimate learning is not the same as feature selection: Y 0 is not 2Other applications of intimate learning include ﬁnding product
directly observed. Furthermore Y 0 doesn’t determine Y ; X com- names in web pages, which appear together with prices, quantities,
bined with Y 0 can improve prediction of Y .          locations which are generally much easier to identify.                                                      Figure 2: Experimental results of course name identiﬁcation.
                                                      The precision and recall curves for partial and full matchings.

    Figure 1: Course name identiﬁcation system chart    • Each word in the course name and the number of words.
                                                        • Each separator symbol between the course number and
    respectively.                                         course name and total number of such symbols.
                                                      The course name feature model F  is initialized from the
  • Department name S and length |S|.                                              nam
                                                      seed data and trained by the EM algorithm deﬁned in §2.1.
  • Course id: integer I and its digit length |I|.    Applying the trained feature model to the test data generates
  • The separator symbol sequence between S and I.    all course name candidates with their probabilities. We again
                                                      apply the 2-means clustering algorithm deﬁned in §2.1. Note
Features are collected for each string sj from the seed data.
                                            0         that the chosen candidates are individual words instead of the
We estimate the conditional probability Fnum = P (y | xi,j)
                 0                                    full string as a course name. We simply group contiguous
of seeing the label y given the feature xi,j. Fnum deﬁnes
                          0                           words as a single course name. The baseline system is iden-
a decision list of rules xi,j → y ranked by conﬁdence score
   0                                                  tical to the course name feature model Fnam except that it
P (y | xi,j). In training, we use EM to (re)estimate the course
                             0                        does not use the identiﬁed course number as an input feature.
number probability model Pnum(y | s), where s is the input
string. In the E-step, Pnum is re-estimated based on Fnum.
We assume all features do not equally contribute to course 3 Experiments and discussion
number identiﬁcation and assign different weight ci to xi, and Two metrics are applied to the performance evaluation of
each parameter of Pnum is computed by:                course name identiﬁcation: partial matching, in which the
                       P        0                     course name is correctly recognized if any of its words is pre-
               0         i ci · P (y | xi,j)          dicted, and full matching, in which the course name is cor-
           P (y | sj) =     P                   (1)
                              i ci                    rectly predicted only if all words of the course name are pre-
In the M-step, F is adjusted with respect to P :      dicted and in the correct order. The precision/recall of the
             num                         num          baseline system is 3.11%/0.31% for full matching. Figure 2
                             0            0
    0         Count(xi,j) · P (y | xi,j) + P (y | sj) illustrates the performance of our implementation on course
 P (y | xi,j) =                                 (2)
                       Count(xi,j) + 1                name identiﬁcation, for full and partial matchings. Our ex-
      0                                               periments show that the intimate learning algorithm exhibits
Since y is a single label +course-number, for test examples signiﬁcant gains in performance over the baseline system ob-
we pick those to be labelled as a course number by using 2- taining 44.7%/44.1% for full matching and 45.8%/46.5% for
means clustering to separate those examples that have high
                        0                             partial matching. Since the course number and course name
conﬁdence scores (high P (y | xi,j) values) from those ex- are two related classes, in addition to intimate learning, a co-
amples that have low conﬁdence scores. This method avoids training-based extension can also be applied to training one
hand-picking or using a held-out set to pick a conﬁdence class by the other, and vice versa as shown by the dashed ar-
threshold.                                            row in Figure 1. For details and full set of references, please
2.2  Course name identiﬁcation                        refer to http://natlang.cs.sfu.ca/researchProject.php?s=299.
The learning procedure of the course name identiﬁcation is References
the same as that for the course number, except that the course
                                                      [            ]
number identiﬁed using the model deﬁned in §2.1 becomes Blei et al., 2002 D. Blei, D. Bagnell, and A. McCallum. Learning
                                                        with scope, with application to information extraction and classi-
an important feature in predicting whether an example is la- ﬁcation. In Proc. UAI, 2002.
belled as a course name. For each candidate course name, the
features used are:                                    [Blum and Mitchell, 1998] A. Blum and T. Mitchell. Combining la-
                                                        beled and unlabeled data with co-training. In Proc. COLT, 1998.
  • Intimate class: course number preceding candidate
                                                      [Collins and Singer, 1999] M. Collins and Y. Singer. Unsupervised
  • The pair of HTML tags before and after course name. models for named entity classiﬁcation. In Proc. EMNLP, 1999.