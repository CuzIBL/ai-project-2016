     Convergence of reinforcement learning with general function approximators 

                                   Vassilis A-Papavassiliou and Stuart Russell 
                    Computer Science Division, U. of California, Berkeley, CA 94720-1776 
                                          {vassilis^russell} @cs.berkeley.edu 


                         Abstract                               TO-learning with probability 1 when the value function is rep•
                                                                resented as a table where each state has its own entry. For 
      A key open problem in reinforcement learning is           large state spaces, however, compact parametric representa•
      to assure convergence when using a compact hy•            tions are required; for such representations, we are interested 
     pothesis class to approximate the value function.          in whether an algorithm will converge to the function that is 
      Although the standard temporal-difference learning        closest, by some metric, to the true value function (a form of 
      algorithm has been shown to converge when the hy•         agnostic learning). Gordon [1995] proved that TO conveiges 
      pothesis class is a linear combination of fixed ba•       in this sense for representations called "averagers" on which 
      sis functions, it may diverge with a general (non•        the TO update is a max-norm contraction (see Section 2). Tsit-
      linear) hypothesis class. This paper describes the        siklis and van Roy [ 1996] proved convergence and established 
      Bridge algorithm, a new method for reinforcement          error bounds for TD( A) with linear combinations of fixed ba•
      learning, and shows that it converges to an approxi•      sis functions. 
      mate global optimum for any agnostically learnable          With nonlinear representations, such as neural networks, 
      hypothesis class. Convergence is demonstrated on         TO has been observed to give suboptimal solutions [Bert-
      a simple example for which temporal-difference            sekas and Tsitsiklis, 1996] or even to diverge. This is a se•
     learning fails. Weak conditions are identified un•        rious problem since most real problems require nonlinearity. 
     der which the Bridge algorithm converges for any          Baird [1995] introduced residual algorithms, for which con•
     hypothesis class. Finally, connections are made be•       vergence can be proved when combined with a gradient de•
     tween the complexity of reinforcement learning and        scent learning method (such as used with neural networks). 
     the PAC-learnability of the hypothesis class.             Unfortunately the error in the resulting approximation can be 
                                                               arbitrarily large and furthermore the method requires two in•
 1 Introduction                                                dependent visits to each sampled state. 
                                                                  This paper describes the Bridge algorithm, a new RL 
 Reinforcement learning (RL) is a widely used method for       method for which we establish convergence and error bounds 
 learning to make decisions in complex, uncertain environ•     with any agnostically learnable representation. 
 ments. Typically, an RL agent perceives mid acts in an en•
                                                                  Section 2 provides the necessary definitions and notation. 
 vironment, reeei ving rewards that provide some indication of 
                                                               Section 3 explains the problem of nonconvergence and pro•
the quality of its actions. The agent's goal is to maximize 
                                                               vides examples of this with TO. Section 4 outlines the Bridge 
the sum of rewards received. RL algorithms work by learn•
                                                               algorithm, sketches the proof of convergence, and shows how 
 ing a value/unction that describes the long-term expected sum 
                                                               it solves the examples for which TO fails. Section 5 briefly 
of rewards from each state; alternatively, they can learn a 
                                                               covers additional results on convergence to local optima for 
 Q-function describing the value of each action in each state. 
                                                               any representation and on the use of PAC-learning theory. 
These functions can then be used to make decisions. 
                                                               Section 6 mentions some alternative techniques one might 
   Temporal-difference (TO) learning [Sutton, 1988] is a com•  consider. The paper is necessarily technically (tense given the 
 monly used family of reinforcement learning methods. TO al•   space restrictions. The results, however, should be of broad 
gorithms operate by adjusting the value function to be locally interest to the AI and machine learning communities. 
consistent. When used With function approximators, such as 
neural networks, that provide a compact parameterized repre•
sentation of the value function, TO methods can solve real-    2 Definitions 
world problems with very large state spaces. Because of this, 
one would like to know if such algorithms can be guaranteed    2.1 MDP 
to work—i.e., to converge and to return optimal solutions.     A Markov decision process M = is a set of 
   The theoretical study of RL algorithms usually divides the  states S, a set of actions A, transition probability distributions 
problem into two aspects: exploration policies that can guar•           that define the next state distribution given a current 
antee complete coverage of the environment, and value deter-   state x and action a, reward distributions r that define 
mination to find the value function that corresponds to a given the distribution of real-valued reward received upon execut•
policy. This paper concentrates on the second aspect. Prior    ing a in x, and a discount factor . Since we are in•
work [Jaakkola et al. , 1995] has established convergence of   terested in the problem of value determination, we assume we 


748 MACHINE LEARNING are given a fixed policy (choice of action at each state). When erative process In practice, the 
executing only this fixed policy, the MDP actually becomes a  mapping usually cannot be performed exactly because, even 
Markov chain, and we may therefore also write the transition  if we have access to the necessary expectation to compute 
probabilities as p(-|x) and the reward distributions as r(-\x). TV(x) exactly, it is infeasible to do so for all states x. Thus 
We assume that we are able to define the stationary distribu­ we perform an approximate mapping using samples. We will 
tion π of the resulting Markov chain and also that the rewards take the state sample distribution to be the stationary distri­
lie in the range                                              bution Π. In general when we cannot compute TV(ar) ex­
                                                              actly, we approximate by generating samples (x, y) 
                                                              with sample distribution 

observed random rewards, i.e. Rk has distribution r(-\Xk). 
Define the true value function V* at a state x to be the ex­  and passing them to a learning algorithm for H. The joint 
pected, discounted reward to go from state x:                 probability density (from which we generate the samples) 
                                                              simply combines (From which we sample the state x) with 
                                                              the conditional probability density ) (from which we 
  The problem of value determination is to determine the true 
                                                              generate an estimate y for TV(x)). 
value function or a good approximation to it. Classical TD so­
                                                                In this paper we focus on agnostic learning. In this case, the 
lutions have made use of the backup operator T, which takes 
                                                              learning operator seeks the hypothesis h that best matches 
an approximation V and produces a better approximation 
                                                              the target function V, even though typically the target func­
                                                              tion is not in the hypothesis class. If we measure distance us­
                                                              ing the -norm, then we can define the learning operator for 
                                                              agnostic learning to be: 
  An operator A is said to be a contraction with factor 
under some norm || • || if                                      As already mentioned, in the typical case we do not have 
                                                              access to the exact function V to be learned, but rather we can 
                                                              draw samples (x, y) from a sample distribution P such that the 
If = 1, A is said to be a nonexpansion. If we define the      expected value of the conditional distribution is V(x). 
                                   V(x)and the -normto        If, in addition, V samples the states according to (or what­
                                    then T is a contraction   ever distribution was used to measure distance in the previous 
with factor and fixed point V* under both max-norm and        definition) then an equivalent definition for agnostic learning 
 -norm {Tsitsiklis and Van Roy, 1996]. Therefore repeated     is based on minimizing risk; 
application of T (i.e. the iterative process 
converges to . We will use Tj to represent the operator T     where we define the risk of a hypothesis h with respect to a 
applied j times.                                              distribution V to be: 
  If the transition probabilities p and reward distributions r 
are known, then it is possible to compute TV directly from 
its definition. However, if p and r are not known, then it is 
not possible to compute the expectation in the definition of T. In practice, is approximately performed by generating 
In this case, by observing a sequence of states and rewards in enough samples (x, y) from the sample distribution P so as 
the Markov chain, 'we can form an unbiased estimate of TV.    to be able to estimate risk well, and thus to be able to output 
Specifically, if we observe state x and reward r followed by  the hypothesis in % that has minimal risk with respect to this 
                                                              distribution. In the algorithm we present, we assume the abil­
state x , then the observed backed-up value, , is 
      2                                                       ity to compute exactly for the given hypothesis class H. 
an unbiased estimate of TV(x). Formally we define 
                                                              This is certainly not a trivial assumption. In a later section, 
to be the conditional probability density of observed backed-
                                                              we briefly discuss an extension to our algorithm for the case 
up values from state x: 
                                                              where is a PAC-agnostic learning step rather than an ex­
                                                              act agnostic learning step. 
where and are, as defined above, random variables               Finally, let us define our goal. is defined to be the best 
with distributions respectively. Thus if                      approximation to V* possible using %\ 
a random variable Y has associated density then 
E[Y] = . Similarly, we defineto be the                        We seek techniques that return a value function V that mini­
conditional probability density of j-step backed-up values ob­ mizes a relative or absolute error bound: 
served from state x. 
2.2 Function Approximation 
As state spaces becomes large or even infinite, it becomes in- 3 Nonconvergence of TD 
feasible to tabulate the value function for each state x, and 
we must resort to function approximation. Our approximation   In this section, we examine the non-convergence problem of 
scheme consists of a hypothesis class H of representable func­ TD when used with non-linear function approximators. We 
tions and a learning operator which maps arbitrary value      present simple examples which we will reconsider with the 
functions to functions in %.                                  Bridge algorithm in the next section. 
  The standard T based approaches that use function approx­     As mentioned above, standard TD with function approxi­
imation essentially compute or approximately compute the it-  mation is based on the iterative process 


                                                                             PAPAVASSIUOU AND RUSS€LL 749      is a non-expansion under the same norm that makes T a      gence results and another look at the examples from the pre•
 contraction, then the composite operator T is a contraction    vious section. 
 and this process will converge to some error bound relative to   The main algorithm Bridge ValueDet, determines the 
     . For example, Tsitsiklis and Van Roy [1996] consider a    value function within some error bound by making repeated 
 linear hypothesis class, for which is simply a projection.     calls to BridgeStep. We will now describe the first invoca•
 If one uses a nonlinear hypothesis class H for which IIH is not tion of BridgeStep. 
 a nonexpansion then this iterative process can either diverge    Metaphorically it consists of throwing a bridge across the 
 or get stuck in a local minimum arbitrarily far from           treacherous terrain that is the hypothesis class H, towards a 
   We now give simple examples demonstrating ways in            point on the far side of the optimal solution. If the bridge 
 which TO can fail when it is used with a nonlinear hypothesis  lands somewhere close to where we aimed it, we will be able 
 class. Consider an MDP with two states where the probabil-     to walk along it in a productive direction (achieve a contrac•
 ity of going from one state to the other is 1, and the rewards tion). If the bridge lands far from our target, then we know that 
 are also deterministic. The stationary distribution is 0.5 for there isn't any H-expressible value function near our taiget on 
 each state, the discount factor is .8 and the hypothesis class H which the bridge could have landed (hence an error bound). 
                                                                This is made precise by Lemma 2 in the next section. 
                                                                  We are given an old approximation V from which we try 
                                                                tocreate a better approximation Vnew. Webasically have two 
                                                                tools to work with: T and IIH. As can be seen in Figure 2 (and 
                                                                in the example in the previous section), if we combine these 
                                                                two operators in the standard way, Vnew = H«TV, we can 
                                                                get stuck in a local minimum. We will instead use them more 
                                                                creatively to guarantee progress or establish an error bound. 


                                                                            Figure 2: Stuck in a local minimum 

                                                                  We begin by using not T but rather where j is deter•
                                                                mined by the main algorithm BridgeValueDet before it 
                                                                calls BridgeStep. We can then ask the question, given we 
                                                                know where V and are, what does that tell us about the 
                                                                location of V* ? It turns out that V* is restricted to lie in some 
                                                                hypersphere whose position can be defined in terms of the po•
                                                                sitions of V and . This is made precise by Lemma 1 in 
                                                                the next section. The hypersphere is depicted in Figure 3 and 
                                                                as required, V* lies inside it. 

   Figure 1: (a) Suboptimal Fixed Point and (b) Oscillation 

   If we modify the rewards slightly to be (r1,r2) = 
 (10, —7.8) then the true value function V* =is 
 no longer in H. The best representation of V* = 

 IIH V* = . If we start from (0,0) as above, we will 
 again reach a suboptimal fixed point around (5, —5). How•
ever, starting from V0 = (30,0) (or even V0 = (15,0)) 
the result of repeated applications of T as shown in Fig•
ure 1(b) displays a different type of failure — oscillation be•
                                                                              Figure 3: The bridge is aimed 
tween points approaching (7.5,7.5) and (16,0). As in the pre•
vious example,is small, so the relative error                     We now define a new operator B based on and the iden•
bound is large.                                                tity operator I. 

4 The Bridge Algorithm 

We begin with a high level description of the algorithm (de•   B simply amplifies the Bellman residual by a factor of 
tails are in the Appendix). This is followed by the conver•    As can be seen in Figure 3, B V is the point on the far side 


750 MACHINE LEARNING of the hypersphere from V. This operates- is what we use to   three lengths (and n) determine the relative position of 
throw a bridge. We aim the bridge for BV, which is beyond     with respect to V and W (See Figure 5). to practice we es•
anywhere where our goal might be, i^.the true value function  timate the true risk with the empirical risk [Haussler, 1992], 
lies somewhere between V and BV. The motivation for us•       which we calculate using samples drawn from the distribution 
ing B is in a sense to jump over all local minima between V 
and V* 
                                                                    have just described a single invocation of BridgeStep 
  Ideally we would be able to represent B V (just as in the   that represents the first iteration of the main algorithm. Each 
standard approach we would want to represent TV) but this     iteration builds a new bridge based on the previous one, so a 
function is most likely not in our class of representable func• generic iteration would begin with a V that was the of 
tions. Therefore we must apply the operator H to map it into  the previous iteration (see Figure 6). In particular, the input 
H. The result, W = IlHBV e H, is shown in Figure 4. The       V of a generic iteration is not in H, but is rather a linear com•
bridge is supported by V and W and is shown as a line be•     bination of the initial approximation and all previous W 
tween them. In summary we throw the bridge aiming for B V,    functions. Thus the final result is a tall weighted tree whose 
but H determines the point W on which it actually lands.      leaves are in U. If we insist on a final result that is in H, then 
                                                              we can apply a final mapping at the very end. 
                                                                Just as the standard TD algorithm was summarized as 
                                                                                 , the Bridge algorithm can be essentially 
                                                              summarized as 


            Figure 4: The bridge is established 

  In practice we perform the mapping IIH by generating sam•
ples from an appropriate distribution and passing them to a 
learning algorithm for H. In particular to compute HBV, 
we generate samples (x, y) according to the distribution: 

                                                                       Figure 6: Generic iteration of BridgeStep 
                                                        I 
The key feature of this distribution is that if a random variable 
Y has associated density i then E[Y] = BV(x).                 4.1 Convergence of the Bridge algorithm 
  The final step is to walk along the bridge. The bridge is a We will state the main convergence theorem for the Bridge al•
line between V and W and our new approximation Vnew will      gorithm, but space limitations allow us to state only the two 
be some point on this line (see Figure 5). This point is deter• most important Lemmas used in the proof. We begin with a 
mined by projecting a point n < 1 of the way from V to        very useful observation about the geometric relationship be•
onto the line, where n is a function of the input parameters. tween V, TV and V*. 
(We could just project , but using n is a refinement that     Lemma 1 Let A be a contraction with contraction factor 
yields a better guaranteed effective contraction factor.)     under some norm. Let V* be the fixed point of A. For any 
                                                              point 


                                                                In words, given the positions of V and AV, let c = 
                                                                         . Then we know that the position of V* has to be 
                                                              on or inside the hypersphere of radius centered at O (see 
                                                              Figure 7). This hypersphere is simply the set of points that are 
                                                              at least a factor of C closer to AV than to V. Note that the dis•
                                                              tance from V to the furthest point on the hypersphere is 
             Figure 5: The new approximation                    We apply Lemma 1 using for A and for This de•
                                                              fines a hypersphere inside of which the true value function 
  Thus the new approximation VneW, which is not necessar•     must lie. Lemma 1 is used mainly to prove Lemma 2, which 
ily inH, is a weighted average of the old approximation V and characterizes the behavior of BridgeStep and provides most 
W %. Calculating the weights (p and 1 - p) in this average    of the meat of the convergence proof. 
requires the ability to measure distance and risk. In particular 
we need to measure the distance between V and W and the       Lemma 2 Given an approximation V and parameters 
risk of V and W with respect to the distribution . These      andj > 1, BridgeStep returns a new approxima-


                                                                              PAPAVASSILIOU AND RUSSELL 751                                                                 in Figure 9 with center at B V, for otherwise W would not be 
                                                                the closest hypothesis to BV. Furthermore we know that V* 
                                                                must lie on or inside the small hypersphere in Figure 9. Thus 
                                                                there is a separation between V* and H and this separation 
                                                                allows us to prove, for any possible position of V*, an upper 
                                                                bound on the relative error 


 tion Vnew that satisfies at least one of the following two con-
 ditions, where the error bound is 
 defined in the Appendix 

                                           (Error Bound) 
    Intuitively, if the bridge lands close to where we aimed it, 
 we will achieve a contraction towards the goal. If the bridge 
 lands far away, we will prove a relative error bound for the 
 result. The key quantity that determines which of these two 
 events happens, is the angle formed between the bridge and 
 the line from V to BV. If W = is close to BV, 
 then will be small, the bridge will lie close to the hyper-
 sphere, and we will be able to walk along the bridge and make 
 progress. If instead W is far from BV, then will be large 
 and walking along the bridge will not take us closer to the    Figure 9: Relative error bound is established when 9 is large 
 goal, but we will be able to prove that we are already close 
 enough.                                                          It should be noted that in general we do not know and we 
   Figure 8 shows the case where the angle is small. As de•     cannot measure 0 to determine which of the two conditions 
 scribed previously, the small hypersphere represents the set   of Lemma 2 Vnew satisfies. We only know that it satisfies at 
 of points that are at least a factor of closer to than least one of them. 
 they are to V. This follows from applying Lemma 1 to the         By Lemma 2, if V already satisfies the relative error bound 
 operator . Now think of BridgeStep as an operator that         then so will Vnew, because if Vnew achieves a contraction 
 takes V and returns , and ask the question, what set of        over V, its error decreases. Thus each successive approxima•
 points are at least a factor of „ (which is an input parame•   tion is better than the one before, until we achieve the relative 
 ter to BridgeStep) closer to than to V? Applying               error bound from which point every subsequent approxima•
 Lemma 1 to this question defines another, much larger hy•      tion will also achieve that bound. 
 persphere which is depicted in Figure 8 with center at O for     We now give the main result, which is guaranteed conver•
 the case = arcsin — arcsin Note that this larger hy•           gence to a relative or absolute error bound. Moreover, the 
 persphere completely contains the smaller hypersphere which    maximum number of invocations of BridgeStep, and thus 
 contains V*. Thus V* also lies inside the larger hypersphere   the maximum number of hypotheses in the linear combina•
 and so is at least a factor of closer to V* than V is.         tion, can be specified. 
 This holds for = arcsin — arcsin .If is smaller than 
                                                                Theorem 1 Let v > 1 and e0 > 0 be the desired relative 
 this, the achieved contraction is even better.                 and absolute error bounds respectively. Let N be an upper 
                                                                bound on the desired number of iterations. Then the algorithm 
                                                                BridgeValueDet(z/, €o,N) produces an approximation V, 
                                                                consisting of a linear combination of at most N+l hypotheses 
                                                               from H, that satisfies at least one of either the relative error 
                                                                bound v or the absolute error bound eo-' 


                                                                  The proof of the theorem follows directly from Lemma 2; 
                                                                rewards are bounded, so the true value function is bounded, so 
      Figure 8: Contraction is achieved when is small           the absolute error of the initial approximation can be bounded. 
                                                                If all N iterations achieve a contraction, then the absolute er•
   Figure 9 shows the case where the angle is large, is large   ror will be smaller than requested. If at least one of the itera•
 when it is not possible to find a hypothesis in % close to BV. tions failed to achieve a contraction, then it achieved a relative 
 In fact we choose W = to be the closest such hy•               error bound and all subsequent iterations, including the last 
pothesis, so the rest of % must lie further away. In particular one, will achieve the requested relative error bound. Again, 
% must lie completely outside the big hypersphere depicted      since we do not know which of the two conditions of Lemma 2 


752 MACHINE LEARNING 