               Multiple-Goal Reinforcement Learning with Modular Sarsa(O) 

                         Nathan Sprague                                        Dana Ballard 
                 Computer Science Department                         Computer Science Department 
                     University of Rochester                             University of Rochester 
                       Rochester, NY 14627                                 Rochester, NY 14627 
                    sprague@cs. rochester. edu                            dana@cs. rochester edu 


                        Abstract                               then be used to fairly distribute control among the mod•
                                                               ules. This approach has been independently explored in 
     We present a new algorithm, GM-Sarsa(O), for find•        [Humphrys, 1996] and [Karlsson, 1997]. It is attractive in 
     ing approximate solutions to multiple-goal rein•          its simplicity, and it has shown good empirical performance 
     forcement learning problems that are modeled as           in a number of domains. 
     composite Markov decision processes. According 
                                                                  In this paper we will highlight a previously unrecognized 
     to our formulation different sub-goals are modeled 
                                                               problem with existing modular Q-learning algorithms. Exist•
     as MDPs that are coupled by the requirement that 
                                                               ing algorithms learn component policies that may be highly 
     they share actions. Existing reinforcement learning 
                                                               sub-optimal in the context of the composite task, because they 
     algorithms address similar problem formulations 
                                                               do not take into account the fact that the component mod•
     by first finding optimal policies for the component 
                                                               ules are forced to share control. We will show how to fix 
     MDPs, and then merging these into a policy for the 
                                                               this problem by replacing the Q-learning with the closely re•
     composite task. The problem with such methods is 
                                                               lated Sarsa(O) learning rule. The resulting algorithm shows 
     that policies that are optimized separately may or 
                                                               improved performance on a large sample problem. 
     may not perform well when they are merged into a 
     composite solution. Instead of searching for opti•
     mal policies for the component MDPs in isolation,         2 The problem formalized 
     our approach finds good policies in the context of 
                                                               The underlying formalism for many reinforcement learning 
     the composite task. 
                                                               algorithms is the Markov decision process. An MDP, denoted 
                                                               M is described by a 4-tuple (5, A, T, R), where S is the state 
   keywords: reinforcement learning 
                                                               space, A is the action space, and T(s, a, s') is the transition 
                                                               function that indicates the probability of arriving in state s' 
 1 Introduction                                                when action a is taken in state s. The reward function R(s, a) 
Traditional reinforcement learning algorithms can success•     denotes the expected one-step payoff for taking action a in 
fully solve small, single-goal tasks. The main challenge in    state s. The goal of reinforcement learning algorithms is to 
the area of reinforcement learning is scaling up to larger and discover an optimal policy 7r*(s) that maps states to actions 
more complex problems. The scaling problem takes a num•        so as to maximize discounted long term reward. 
ber of forms. We may have a problem that has a very large        Here we consider the problem of discovering a joint pol•
state space, a problem that is best described as a set of hi•  icy for a set of N MDP's Throughout we will use 
erarchically organized goals and subgoals, or a problem that   subscripts to distinguish the MDPs. These MDP's each have 
requires the learning agent to address several tasks at once. It a distinct state space, but they share a common action space, 
is the last form of scaling that this paper is concerned with. and are required to execute the same action on each time step. 
   The naive approach to learning to solve composite tasks is  This model is intended to map to the case of a single agent 
to create a state space that includes all of the information that that is simultaneously faced with a set of different goals. 
is relevant to each sub-task. The agent would then learn in      The N component MDPs implicitly define a larger com•
this joint space, receiving reward when any of the sub-goals   posite MDP. Formally, the goal is to find the optimal policy 
are accomplished. The problem with this approach is that it    for this composite MDP. The optimal composite policy is de•
suffers from the curse of dimensionality; as additional state  fined as the policy that maximizes summed discounted reward 
dimensions are added for each new sub-task, the size of the    across the component MDPs. 
joint state space grows exponentially.                           The state space of the composite MDP is the cross prod•
   A more promising approach to this sort of multiple-goal     uct of the state spaces of the component MDPs: S — S\ x 
problem is to use the well known Q-learning algorithm to       £2 x ... x SN- The composite reward function is defined as: 
train one learning module to handle each of the sub-goals.     R(s, a) = In the case where the component 
The internal Q-values of the different learning modules can    MDPs are independent, the composite transition function can 


POSTER PAPERS                                                                                                       1445 be written as: In the case 
where the component MDPs are not independent, the exact 
composite transition function will depend on on the particu•
lar dependencies between the models. 
   In theory, there is no reason that the composite MDP could 
not be solved directly using the traditional Q-learning algo•
rithm. However, this is generally not practical because the 
size of the composite state space may grow exponentially 
with the number of component MDPs. 
3 Modular Q-Learning 
Humphrys and Karlsson [Humphrys, 1996; Karlsson, 1997] 
independently developed similar approaches to the problem 
of multiple-goal reinforcement learning. The idea is that a 
separate learning module is created for each component MDP. 
The agent takes actions in the environment, and each module 
i is trained with the standard Q-learning update rule:        Figure 1: Results of one training run for GM-Sarsa(O) and 
                                                              three Q-learning based algorithms on a food gathering task. 
                                                       (1) 
                                                              Training is divided into trials lasting 100 time steps. Data 
Where i\ is the immediate reward, a is the learning rate      points are generated by suspending training every 10000 tri•
parameter, and is a discount factor applied to future re•     als and computing the mean performance for 1000 trials with•
wards. In single goal reinforcement learning problems, these  out exploration. Each algorithm uses an e-grecdy exploration 
Q-values are used only to rank order the actions in a given   policy with e linearly reduced from .4 to 0 during the first half 
state. The key observation here is that the Q-values can also of trials. All algorithms use a fixed learning rate of .05, and a 
be used in multiple-goal problems to indicate the degree of   discount factor of .9. 
preference that modules have for different actions. There are 
several possible ways these values can be used to select a    3.1 The problem with modular Q-learning 
compromise action to execute. The different approaches will 
                                                              Q-learning has some attractive qualities as a basis for 
be referred to as action selection mechanisms. 
                                                              multiple-goal reinforcement learning. Chief of these is the 
  Karlsson's suggestion, which he calls "greatest mass", is to 
                                                              fact that it is an off-policy learning method. This means that 
generate an overall Q-value as a simple sum of the Q-values 
                                                              Q-learning for a single MDP is guaranteed to converge to the 
of the individual modules: Q(s,a) — The 
                                                              optimal solution regardless of what policy is followed during 
action with the maximum summed value is then chosen to 
                                                              training, as long as each state-action pair is visited infinitely 
execute. We will refer to this approach as GM-Q for greatest 
                                                              often in the limit. This fact makes it easy to prove conver•
mass Q-learning. 
                                                              gence results for the composite reinforcement learning algo•
  Humphrys considers the greatest mass approach, but raises 
                                                              rithms introduced above. In particular, it is easy to see that 
the objection that the action with the highest sum may not 
                                                              each module is guaranteed to converge to the optimal policy 
be particularly good for any of the modules, with the result 
                                                              and value function for its own MDP. Since the action selec•
that no module is able to reach its goal. He explores several 
                                                              tion mechanisms generate a policy deterministically from the 
winner-take-all alternatives that constrain the chosen action 
                                                              component value functions, the composite policy is also guar•
to be optimal for at least one module. For a given state .s 
                                                              anteed to converge, although there is no guarantee concerning 
each of the N modules promotes its own action with a value 
                                                              the quality of the composite solution. 
Wl(sl). The module with the largest W value is then allowed 
to execute its preferred action.                                 Unfortunately, the off-policy character of Q-learning is 
                                                              also a serious limitation. The difficulty is that the one-step 
  The simplest method for generating the W-values, which 
                                                              value updates for each module are computed under the as•
we will refer to as Top-Q, is to set W (s ) = 
                                    t t                       sumption that all future actions will be chosen optimally for 
thus giving control to the module with the highest Q-value 
                                                              that MDP. This assumption is not valid under the action selec•
in the current state. This method suffers from the draw•
                                                              tion mechanisms described above; future actions will repre•
back that the module with the highest Q-value may have no 
                                                              sent some compromise policy in which the different modules 
preference over what action is chosen, while another module 
                                                              share control. This means that the computed Q-values do not 
stands to lose a great deal if its action is not selected. The 
                                                              converge to the actual expected return under the composite 
method sometimes exhibits reasonable performance, but this 
                                                              policy. Instead, the max in equation (1) results in Q-values 
is strongly dependent on the structure of the reward functions. 
                                                              with a positive bias. 
  A better alternative, referred to as negotiated W-learning, 
is to grant control to the module that stands to lose the most 
long term reward if it is not selected. This module can be    4 Modular Sarsa(O) 
discovered by examining the Q-values for the current state.   A possible solution is to the problem of positive bias is to 
Refer to [Humphrys, 1996] for a detailed description of the   replace Q-learning with an on-policy learning algorithm. In 
algorithm.                                                    particular we will explore the use of Sarsa(O) [Rummery and 


1446                                                                                                  POSTER PAPERS  Niranjan, 1994; Singh and Sutton, 1996; Sutton, 1996]. The    based algorithms from Section 3 on this task. Of the four 
 update rule for Sarsa(O) is:                                  algorithms, GM-Sarsa(O) exhibits the best performance. 

                                                        (2)    6 Conclusion 
 This update rule is virtually identical to that for Q-learning We have presented a method for learning approximately opti•
 except that the max over Q-values on the right has been re•   mal policies for a certain class of composite Markov decision 
placed with the Q-value of the state action pair that is actu• processes. Empirical results demonstrate that our approach 
 ally observed on the next step. For the case of single MDPs   performs better than a number of existing algorithms. Future 
 Sarsa(O) has been proved to converge to the optimal policy    work will focus on proving convergence results for our algo•
 as long as the exploration rate is asymptotically decayed to• rithm. A longer version of this paper, including a discussion 
 ward zero according to an appropriate schedule [Singh et al,  of related work is available as [Sprague and Ballard, 2003]. 
 2000]. 
   The key observation for our purposes is that, since Sarsa(O) Acknowledgments 
 is an on-policy method, it does not suffer from the problem 
of positive bias. Since updates are based on the actions that  This material is based upon work supported by a grant 
are actually taken, rather than on the best possible action, we from the Department of Education under grant number 
expect Sarsa(O) based modules to discover Q-values that are    P200A000306, a grant from the National Institutes of Health 
closer to the true expected return under the composite policy. under grant number 5P41RR09283 and a grant from the Na•
   Any of the action selection mechanisms from Section 3       tional Science Foundation under grant number El A-0080124. 
could be recast to use Sarsa(O) rather than Q-learning to train Any opinions, findings, and conclusions or recommendations 
the modules. However, we focus on the method of greatest       expressed in this material are those of the authors and do not 
mass. We refer to the resulting algorithm as GM-Sarsa(O).      necessarily reflect the views of the above mentioned institu•
Recall that the goal is to maximize the summed reward across   tions. 
all of the component MDPs. Assuming that we have trust•
worthy utility estimates from each of the modules, it makes    References 
sense to choose the action that has the highest summed util•   [Humphrys, 1996] M. Humphrys. Action selection methods 
ity across all of the modules. By definition, this is the action  using reinforcement learning. In From Animals to Animats 
that will lead to the greatest summed long term reward. This      4: Proceedings of the Fourth International Conference on 
reasoning did not hold under Q-learning, because the utility      Simulation of Adaptive Behavior, pages 135-144, Cam•
estimates were inaccurate under the composite policy.             bridge, MA, 1996. 
   Thus far we have no convergence proof for the GM-
                                                               [Karlsson, 1997] J. Karlsson. Learning to Solve Multiple 
Sarsa(O) algorithm. Refer to the associated technical report 
                                                                  Goals. PhD thesis, University of Rochester, 1997. 
[Sprague and Ballard, 2003] for a discussion of the possible 
convergence characteristics.                                   [Rummery and Niranjan, 1994] G. A. Rummery and M. Ni•
                                                                  ranjan. On-line Q-learning using connectionist systems. 
5 Examples                                                        Technical Report CUED/F-1NFENG/TR 166, Cambridge 
                                                                  University Engineering Department, 1994. 
Figure 1 demonstrates the performance of GM-Sarsa(O) on a 
                                                               [Singh and Cohn, 1998] S. Singh and D. Cohn. How to dy•
sample composite task (the task is adapted from [Singh and 
                                                                  namically merge markov decision processes. In Advances 
Cohn, 1998]). The goal of the agent in this task is to gather 
                                                                  in Neural Information Processing Systems, volume 10, 
stationary food items while avoiding a predator in a 5 x 5 grid. 
                                                                  1998. 
There are three food items present at all times. 
   The agent moves in any of the eight possible directions at  [Singh and Sutton, 1996] S. Singh and R. Sutton. Reinforce•
each time step. A random move is made with a probability          ment learning with replacing eligibility traces. Machine 
of .1. If the agent contacts any of the food items it receives a  Learning, 22(1-3), 1996. 
reward of 1.0, and the item is randomly moved to a new posi•   [Singh et al, 2000] S. Singh, T. Jaakkola, M. L. Littman, 
tion. The agent receives a reward of .5 for every time step that  and C. Szepesvari. Convergence results for single-step 
it avoids the predator. The predator moves deterministically      on-policy reinforcement-learning algorithms. Machine 
one position toward the agent on every other time step.           Learning, 2000. 
   The positions of the food items as well as the positions 
                                                               [Sprague and Ballard, 2003] N. Sprague and D. Ballard. 
of the agent and predator result in 25  ` 10 million distinct 
                                    5                             Multiple-goal reinforcement learning with modular 
states. This is too large for a monolithic tabular learning al•
                                                                  sarsa(O). Technical Report 798, University of Rochester 
gorithm to be practical. The task is a good candidate for a 
                                                                  Computer Science Department, 2003. 
modular reinforcement learning algorithm because it can be 
decomposed into several small MDPs. One MDP describes          [Sutton, 1996] R. Sutton. Generalization in reinforcement 
the agent's interaction with the predator, and three MDPs         learning: Successful examples using sparse coarse cod•
describe the interaction with the food items. Each of these       ing. In Advances in Neural Information Processing Sys•

component MDPs has 252 = 625 states. Figure 1 shows the           tems, volume 8, 1996. 
performance of GM-Sarsa(O) as well as the three Q-learning 


POSTER PAPERS                                                                                                       1447 