                              Searching for Interacting Features

                                      ZhengZhaoandHuanLiu
                           Department of Computer Science and Engineering
                                        Arizona State University
                                    {zheng.zhao, huan.liu}@asu.edu


                    Abstract                          An intrinsic character of feature interaction is its irreducibil-
                                                      ity [Jakulin and Bratko, 2004], i.e., a feature could lose its
    Feature interaction presents a challenge to feature relevance due to the absence of its interacting feature(s).
    selection for classiﬁcation. A feature by itself may
    have little correlation with the target concept, but Existing efﬁcient feature selection algorithms usually as-
                                                                             [                          ]
    when it is combined with some other features, they sume feature independence Dash and Liu, 1997; Hall, 2000 .
    can be strongly correlated with the target concept. Because of the irreducible nature of feature interactions, these
    Unintentional removal of these features can result algorithms cannot select interacting features such as A1 and
    in poor classiﬁcation performance. Handling fea-  A2 in MONK1. Others attempt to explicitly address feature
    ture interaction can be computationally intractable. interactions by ﬁnding some low-order interactions (2- or 3-
                                                              [                     ]
    Recognizing the presence of feature interaction, we way). In Jakulin and Bratko, 2003 , the authors suggest to
    propose to efﬁciently handle feature interaction to use interaction gain as a practical heuristic for detecting at-
    achieve efﬁcient feature selection and present ex- tribute interaction. Using interaction gain, their algorithms
    tensive experimental results of evaluation.       can detect if datasets have 2-way (one feature and the class)
                                                      and 3-way (two features and the class) interactions. They fur-
                                                      ther provide in [Jakulin and Bratko, 2004] a justiﬁcation of
1  Introduction                                       the interaction information, and replace the notion of ‘high’
The high dimensionality of data poses a challenge to learning and ‘low’ in [Jakulin and Bratko, 2003] with statistical sig-
tasks such as classiﬁcation. In the presence of many irrele- niﬁcance and illustrate the signiﬁcant interactions in the form
vant features, classiﬁcation algorithms tend to overﬁt train- of interaction graph. Below we apply four feature selection
ing data [Guyon and Elisseeff, 2003; Dash and Liu, 1997]. algorithms to synthetic data with known interaction and ob-
Many features can be removed without performance dete- serve how they fare: FCBF [Yu and Liu, 2003],CFS[Hall,
rioration [Gilad-Bachrach et al., 2004]. Feature selection 2000], ReliefF [Kononenko, 1994],andFOCUS[Almuallim
is one effective means to remove irrelevant features [Blum and Dietterich, 1994], all available in WEKA [Witten and
and Langley, 1997]. Optimal feature selection requires an Frank, 2005].
exponentially large search space (O(2N ),whereN is the  Motivating examples: Synthetic data with known fea-
number of features) [Almuallim and Dietterich, 1994].Re- ture interaction. Four synthetic data sets are used to exam-
searchers often resort to various approximations to determine ine how various algorithms deal with known feature interac-
relevant features (e.g., relevance is determined by correla- tions in feature selection. The ﬁrst data set is Corral [John et
tion between individual features and the class) [Hall, 2000; al., 1994], having six boolean features A0,A1,B0,B1,I,R.
Yu and Liu, 2003]. However, a single feature can be con- The class Y is deﬁned by Y =(A0 ∧ A1) ∨ (B0 ∧ B1) and
sidered irrelevant based on its correlation with the class; but features A0,A1,B0,B1 are independent of each other. Fea-
when combined with other features, it becomes very rele- ture I is irrelevant to Y and its values have a uniform ran-
vant. Unintentional removal of these features can result in dom distribution; and feature R is correlated with Y 75% of
the loss of useful information and thus may cause poor clas- the time and is redundant. The other three training data sets
siﬁcation performance. This is studied in [Jakulin and Bratko, are MONKs data. Their target Concepts are: (1) MONK1:
2003] as attribute interaction. For example, MONK1 is a data (A1 = A2) or (A5 =1); (2) MONK2: Exactly two of
set involving feature interaction. There are six features in A1 =1,A2 =1,A3 =1,A4 =1,A5  =1,A6   =1;and
                                                                     =3         =1        =4        =3
MONK1 and the target concept of MONK1 is: (A1 = A2)   (3) MONK3: (A5     and A4    )or(A5     and A2    )
or (A5 =1).HereA1   and A2 are two interacting features. (5% class noise added to the training data).
Considered individually, the correlation between A1 and the Results are presented in Table 1. For Corral, all four algo-
class C (similarly for A2 and C) is zero, measured by mu- rithms remove the irrelevant feature I, but only FOCUS re-
tual information. Hence, A1 or A2 is irrelevant when each moves the redundant feature R. Features A0,A1 and B0,B1
is individually evaluated. However, if we combine A1 with interact with each other to determine the class label of an in-
A2, they are strongly relevant in deﬁning the target concept. stance. CFS, FCBF and ReliefF cannot remove R because R

                                                IJCAI-07
                                                  1156       Table 1: Features selected by each algorithm on artiﬁcial data, and the   indicates a missing relevant feature.
                       Relevant Features FCBF         CFS         ReliefF       FOCUS
                Corral A0,A1,B0,B1  A0,A1,B0,B1, R A0, , , , R A0,A1,B0,B1, R A0,A1,B0,B1
                Monk1   A1,A2,A5    A1, , A3, A4,A5   , , A5     A1,A2,A5      A1,A2,A5
                Monk2  A1,A2,A3,A4,     , , ,A4,     , , ,A4,  A1,A2,A3,A4,  A1,A2,A3,A4,
                          A5,A6         A5,A6        A5,A6        A5,A6         A5,A6
                Monk3   A2,A4,A5      A2, ,A5, A6   A2, , A5     A2,A4,A5    A1,A2,A4,A5
is strongly correlated (75%) with Y . For all the three MONKs cannot be directly applied to identify relevant or interact-
data sets, ReliefF can ﬁnd true relevant features1 as seen in ing features when the dimensionality of a data set is high.
Table 1. Both FCBF and CFS perform similarly, and FCBF Many efﬁcient feature selection algorithms identify relevant
ﬁnds more features. FOCUS can handle feature interaction features based on the evaluation of the correlation between
when selecting features. However, as an exhaustive search the class and a feature (or a selected feature subset). Rep-
algorithm, FOCUS ﬁnds irrelevant feature A1 for MONK3 resentative measures used for evaluating relevance include:
due to 5% noise in the data. In a sense, it overﬁts the training distance measures [Kononenko, 1994; Robnik-Sikonja and
data. FOCUS is also impractical because ﬁnding moderately  Kononenko, 2003], information measures [Fleuret, 2004],
                                          m    N
high-order interactions can be too expensive, as i=1 i and consistency measures [Almuallim and Dietterich, 1994],
can be too large, when dimensionality N is large.     to name a few. Using these measures, feature selection algo-
  In this work, we design and implement an efﬁcient ap- rithms usually start with an empty set and successively add
proach to deal with feature interactions. Feature interactions “good” features to the selected feature subset, the so-called
can be implicitly handled by a carefully designed feature eval- sequential forward selection (SFS) framework. Under this
uation metric and a search strategy with a specially designed framework, features are deemed relevant mainly based on
data structure, which together take into account interactions their individually high correlations with the class, and rele-
among features when performing feature selection.     vant interacting features of high order may be removed [Hall,
                                                      2000; Bell and Wang, 2000], because the irreducible nature
2  Interaction and Data Consistency                   of feature interaction cannot be attained by SFS.
                                                        Recall that ﬁnding high-order feature interaction using rel-
One goal of feature selection is to remove all irrelevant fea- evance (Deﬁnitions 1 and 2) entails exhaustive search of all
tures. First, we deﬁne feature relevance as in [John et al., feature subsets. In order to avoid exponential time complex-
1994].LetF   be the full set of features, Fi be a feature, ity, we derive a feature scoring metric based on the con-
Si = F −{Fi}  and P denote the conditional probability of sistency hypothesis proposed in [Almuallim and Dietterich,
class C given a feature set.                          1994] to approximate the relevance measure as in Deﬁni-
Deﬁnition 1 (Feature Relevance) AfeatureFi is relevant iff tion 1. With this metric, we will design a fast ﬁlter algorithm
                                                      that can deal with feature interaction in subset selection.
     ∃   ⊆              (  |    ) = ( | )
      Si   Si,suchthatPC    Fi,Si    P C Si .           Let D be a data set of m instances, D = {d1,d2,...,dm},
                                                      and F be the feature space of D with n features, F = {F1, F2,
Otherwise, feature Fi is said to be irrelevant.
                                                      ...,Fn}, we have the following:
  Deﬁnition 1 suggests that a feature can be relevant only
                                                      Deﬁnition 3 (Inconsistent Instances) If two instances di
when its removal from a feature set will reduce the prediction
                                                      and dj in D have the same values except for their class la-
power. From Deﬁnition 1, it can be shown that a feature is
                                                      bels, di and dj are inconsistent instances or the two matching
relevant due to two reasons: (1) it is strongly correlated with
                                                      instances are inconsistent.
the target concept; or (2) it forms a feature subset with other
features and the subset is strongly correlated with the target Deﬁnition 4 (Inconsistent-Instances Set) D ⊆ D, D is an
concept. If a feature is relevant because of the second reason, inconsistent-instances set iff ∀ di,dj ∈ D, i = j, either di
there exists feature interaction. Feature interaction is charac- and dj are inconsistent or they are duplicate. D is a maximal
terized by its irreducibility [Jakulin and Bratko, 2004].Akth inconsistent-instances set, iff ∀d ∈ D and d/∈ D, D ∪{d}
feature interaction can be formalized as:             is not an inconsistent-instances set.
                                                                                            D
Deﬁnition 2 (kth order Feature Interaction) F is a feature Deﬁnition 5 (Inconsistency Count) Let be    an
                                                                                  k
subset with k features F1, F2, ...,Fk.LetC denote a metric inconsistent-instances set with elements d1, d2,...,dk,
                                                                                           D
that measures the relevance of the class label with a feature or and c1,c2,...,ct are the class labels of , we partition
                                                      D
a feature subset. Features F1, F2, ..., Fk are said to interact into t subsets S1,S2,...,St by the class labels, where
                                                         = {  |            }                        D
with each other iff: for an arbitrary partition F = {F1, F2, Si dj dj has label ci . The inconsistency count of is:
F      F  }  F        ≥  2    F  =
  3, ..., l of ,wherel    and   i  φ, we have                inconsistencyCount(D)=k   −  max Si
                                                                                         1≤i≤t
             ∀i ∈ [1,l], C (F) > C (Fi)
                                                      Deﬁnition 6 (Inconsistency Rate) Let D1, D2, ..., Dp de-
  Identifying either relevant features or a kth order feature note all maximal inconsistent-instances sets of D, the incon-
                                                      sistency rate (ICR) of D is:
interaction requires exponential time. Deﬁnitions 1 and 2             
  1                                                                     1≤i≤p inconsistencyCount(Di)
   Interestingly, using the full data sets ReliefF missed A1,A5 for ICR(D)=
MONK2, A1 for MONK3.                                                                m

                                                IJCAI-07
                                                  1157Deﬁnition 7 (Consistency Contribution) or c-contribution ranked features that are not yet eliminated (Slist initialized
Let π denote the projection operator which retrieves a sub- with the full set of features). Instances with the same hash
set of columns from D according to the feature subset, the key will be insert into the same entry in the hash table. And
c-contribution (CC) of feature Fi for F is deﬁned as: the information about the labels is recorded. Thus each en-
                                                      try in the hash table corresponds to a maximal inconsistency
       (   F)=      (       (  )) −    (  (  ))
    CC  Fi,     ICR  πF −{Fi} D    ICR  πF D                    (  )                                 (  )
                                                      set of πSlist D . Hence, the inconsistency rate of πSlist D
  It is easy to verify that the inconsistency rate is monotonic can be obtained by scanning the hash table. Property (I) says
in terms of the number of features, i.e., ∀Si, Sj, Si ⊆ Sj that in order to generate an entry in the hash table for a new
                                                      Slist (after eliminating a feature), it is not necessary to scan
⇒  ICR(πSi (D)) ≥ ICR(πSj (D)). Hence, c-contribution
of a feature is always a non-negative number with the zero the data again, but only the current hash table. Property (II)
meaning no contribution. C-contribution of a feature Fi is a suggests that after each iteration of elimination, the number of
                                                                                  list
function of F −{Fi},whereF is the set of features for D. entries of the hash table for new S should decrease. There-
C-contribution of a feature is an indicator about how signiﬁ- fore, the hashing data structure allows for efﬁcient update of
cantly the elimination of that feature will affect consistency. c-contribution iterative feature elimination.
C-contribution of an irrelevant feature is zero. C-contribution 3.2 Dealing with the feature order problem
can be considered as an approximation of the metric in Def-
inition 1 by using inconsistency rate as an approximation of We now consider the feature order problem in applying c-
P , the conditional probability of class C given a feature set. contribution. If we can keep removing the current, most irrel-
  The monotonic property of inconsistency rate suggests that evant feature, we will likely retain the most relevant ones in
                                                      the remaining subset of selected features. Assuming that a set
the backward elimination search strategy ﬁts c-contribution                             1
best in feature selection. That is, one can start with the full of features can be divided into subset S including relevant
                                                      features, and subset S2 containing irrelevant ones. By consid-
feature set and successively eliminating features one at a time                2                1
based on their c-contributions. Backward elimination allows ering to remove features in S ﬁrst, features in S are more
every feature to be evaluated with the features it may inter- likely to remain in the ﬁnal set of selected features. Therefore,
act with. Hence, backward elimination with c-contribution we apply a heuristic to rank individual features using symmet-
should ﬁnd interacting features. However, backward elimina- rical uncertainty (SU) in an descending order such that the
tion using inconsistency rate or c-contribution has two prob- (heuristically) most relevant feature is positioned at the begin-
lems. The ﬁrst problem is that it is very costly as it needs ning of the list. SU is often used as a fast correlation measure
to calculate inconsistency rate for each potentially remov- to evaluate the relevance of individual features [Hall, 2000;
able feature. As in the work of FOCUS [Almuallim and  Yu and Liu, 2003]. This heuristic attempts to increase the
             ]                                        chance for a strongly relevant feature to remain in the se-
Dietterich, 1994 , FOCUS relies on exhaustive search. It is             (  )      (    )
impractical to do so when the dimensionality is reasonably lected subset. Let H X and H X, C denote entropy and
                                                      joint entropy respectively, and M(X, C)=H(C)+H(X)−
large, which separates this work from FOCUS. We will de- (   )
sign a speciﬁc data structure next to achieve efﬁcient calcu- H X, C the mutual information measuring the common in-
lation of c-contribution for our algorithm INTERACT. The formation shared between the two variables. SU between the
                                                      class label C and a feature Fi is:
second problem is that c-contribution measure is sensitive to                               
which feature is selected to compute ﬁrst, the so-called the                        (     )
                                                                    (    )=2      M  Fi,C
feature order problem. This is because features evaluated ﬁrst   SU  Fi,C
                                                                                H(Fi)+H(C)
for their consistency are more likely to be eliminated ﬁrst.
  Solutions to the two problems will enable c-contribution to This ranking heuristic cannot guarantee that the interact-
be used in building an efﬁcient algorithm of backward elimi- ing features are ranked high. For MONK1, for example,
nation. We present our solutions and the algorithm next. SU(A1,C)=SU(A6,C)=0. Either one can be evaluated
                                                      ﬁrst for its c-contribution. Since CC(A1, F) >CC(A6, F),
3  Eliminating Irrelevant Features                    A6 is eliminated. We will experimentally examine the rank-
                                                      ing effect in Section 4.2.
We ﬁrst present our solutions that form two pillar components
for the algorithm INTERACT and then discuss its details. 3.3 INTERACT - An algorithm
                                                      The above solutions pave the way for c-contribution to be
3.1  Efﬁcient update of c-contribution                used in feature selection. We present an algorithm, INTER-
C-contribution relies on the calculation of inconsistency rate. ACT, that searches for interacting features. It is a ﬁlter al-
With the monotonicity of inconsistency rate, the following gorithm that employs backward elimination to remove those
two properties are true after a feature fi is eliminated from features with no or low c-contribution. The details are shown
aset{f1, .., fi, ..., fn} where i =1, 2, ..., n: (I) all inconsis- in Figure 1: Given a full set with N features and a class at-
tent instances sets are still inconsistent; and (II) each maximal tribute C, it ﬁnds a feature subset Sbest for the class concept.
inconsistent instances set will be either of equal size or big- The algorithm consists of two major parts. In the ﬁrst part
ger. Based on these two properties, we implement a hashing (lines 1-6), the features are ranked in descending order based
mechanism to efﬁciently calculate c-contribution: Each in- on their SU values. In the second part (lines 7-16), features
stance is inserted into the hash table using its values of those are evaluated one by one starting from the end of the ranked
features in Slist as the hash key, where Slist contains the feature list. Function getLastElement() returns the feature

                                                IJCAI-07
                                                  1158     input:
       F: the full feature set with features F1,      Table 2: Summary of the benchmark data sets. F: number of
          F2,... ,FN                                  features; I: number of instances and C: number of classes.
       C: the class label
       δ: a predeﬁned threshold                          Data Set F    I    C   Data Set   F     I   C
                                                         lung-cancer 56 32  3   vehicle    18   846  4
     output:                                             zoo      16  101   7   kr-vs-kp   36   3196 2
       Sbest: the best subset                            wine     13  178   3
                                                         soy-large 35 306  19   internet-ads 1558 3278 2
                                                                                 ×
     1  Slist =NULL                                      cmc      9   1473  3   45 4026+2C 4026 45   2
     2  for i=1 to N do
                                                                                =005
     3     calculate SUFi,c for Fi                    only the relevant features (δ .  ). Now we evaluate
     4     append Fi to Slist                         INTERACT using benchmark data sets in comparison with
     5  end                                           some representative feature selection algorithms with the fol-
     6orderSlist in descending values of SUi,c        lowing aspects: (1) number of selected features, (2) predictive
     7  F = getLastElement(Slist)                     accuracy with feature selection, and (3) run time. We also ex-
     8  repeat                                        amine how effective the two solutions (given in Sections 3.1
     9     if F <>NULL then                           and 3.2) are through a lesion study by removing one of them
     10       p = CC(F, Slist) // c-contribution
     11       if p ≤ δ then                           at a time from INTERACT.
     12          remove F from Slist
     13       end                                     4.1  Experiment setup
     14    end                                        In our experiments, we choose four representative feature se-
     15    F = getNextElement(Slist,F)                lection algorithms for comparison. They are FCBF [Yu and
     16 until F == NULL                               Liu, 2003],CFS[Hall, 2000], ReliefF [Kononenko, 1994],
     17 Sbest = Slist                                 and FOCUS  [Almuallim and Dietterich, 1994]. All are avail-
     18 return Sbest                                  able in the WEKA environment [Witten and Frank, 2005].
                                                      INTERACT is implemented in the WEKA’s framework. It
           Figure 1: Algorithm INTERACT               will be made available upon request. All the experiments
in the end of the list, Slist. If c-contribution of a feature is less were conducted in the WEKA environment.
than δ, the feature is removed, otherwise it is selected. Func- From 28 data sets, [Jakulin, 2005] identiﬁed 10 data sets
                   (      )
tion getNextElement Slist,F returns the next unchecked having feature interactions without selecting interacting fea-
feature just preceding F in the ranked feature list (line 15). tures. Here we focus on our discussion on the 10 data sets2.
The algorithm repeats until all features in the list are checked. The datasets are from the UCI ML Repository [Blake and
                       0       1
δ is a predeﬁned threshold ( <δ< ). Features with their Merz, 1998]. We also include another two datasets in the
c-contribution <δare considered immaterial and removed. experiment: the ‘internet-ads’ data from the UCI ML Repos-
Alargeδ is associated with a high probability of removing itory, and the ‘45×4026+2C’ data from [Alizadeh and et al.,
relevant features. Relevance is deﬁned by c-contribution: the 2000]. The information about the 9 data sets (without 3
higher value of CC(F, Slist) indicates that F is more rele- MONKS data sets) is summarized in Table 2. For each data
      =00001
vant. δ  .     if not otherwise mentioned. The parameter set, we run all 5 feature selection algorithms and obtain se-
can also be tuned using the standard cross validation. lected feature subsets of each algorithm. For data sets con-
Time complexity of INTERACT: The ﬁrst part of the algo- taining features with continuous values, if needed, we apply
                                  (    )
rithm has a linear time complexity of O NM ,whereN is the MDL discretization method (available in WEKA). We re-
the number of features and M is the number of instances of move all index features if any. In order to evaluate whether
a given data set. For the second part of the algorithm, the the selected features are indeed good, we apply two effec-
calculation of a feature’s c-contribution using a hash table tive classiﬁcation algorithms C4.5 and a linear Support Vector
takes also O(NM).ForN    features, the time complexity              3
                    2                                 Machine (SVM)  (both available from Weka) before and af-
of INTERACT is O(N   M). This is the worst case analy- ter feature selection and obtain prediction accuracy by 10-fold
sis. Its average time complexity is less because (1) we only cross-validation (CV). Although C4.5 itself evaluates features
use the hash table of current Slist in the calculation of c- (one at a time), its performance is sensitive to the given sets of
contribution, and (2) the number of the entries in the hash features (e.g., its accuracy rates on MONK1 are 88.88% and
table decreases after each iteration. If we assume it decreases 75.69% for (A1,A2,andA5) and for all 6 features, respec-
                                   0        1
to a percentage of the initial size, where <a< ,then  tively). Because of FOCUS’s exponential time complexity,
in N iterations, the overall time complexity of INTERACT we only provide those results of FOCUS obtained in 3 hours
    (   (1 −  N+1)  (1 −  ))
is O NM      a     /     a  (the proof is omitted). In of dedicated use of the PC. INTERACT, FCBF, CFS and Re-
other words, INTERACT is expected to be comparable with
heuristic algorithms such as FCBF [Yu and Liu, 2003].    2In which 3 data sets are MONKs data. We consider them syn-
                                                      thetic data and discussed them earlier separately.
4  Empirical Study                                       3Since Naive Bayes Classiﬁer (NBC) assumes conditional inde-
                                                      pendence between features [Irina Rish, 2001], selecting interacting
We empirically evaluate the performance of INTERACT in features or not has limited impact on it. Our experimental results of
search of interacting features. For the four synthetic data sets NBC conform to the analysis and will be presented elsewhere due to
with known feature interaction (Table 1), INTERACT ﬁnds the space limit.

                                                IJCAI-07
                                                  1159Table 3: Number of selected features for each algorithm (IN:    16
INTERACT, FC: FCBF, RE: ReliefF, FO: FOCUS, FS: Full Set).
NA denotes not available.                                       14
                                                                12
    Data Set   IN    FC   CFS   RE    FO    FS
    lung-cancer 6    6     11    22   4     56                  10
    zoo         5    8     9     14   5     16
    wine        510811513                                       8
    soy-large  13132132NA35
    cmc         9    2     3     5    9     9                   6
    vehicle    18    4     11    7    18    18
    kr-vs-kp   29    7     3     8    NA    36                  4
    internet-ads 49  38    11    2    NA   1558
    45×4026+2C  3    64    42    15   NA   4026                 2
    average   15.22 16.89 13.22 12.89 8.20 640.78
                                                                0
                                                                   zoo lung-cancer wine soy-large cmc vehicle kr-vs-kp 45x4026+2C internet-ad
liefF all complete their runs in seconds. This is consistent
with our understanding and expectation of these algorithms.
4.2  Results and discussion

Number of selected features. Table 3 presents the numbers      Figure 2: TINTERACT\D /TINTERACT
of features selected by the ﬁve algorithms. All algorithms sig-
niﬁcantly reduced the number of features in many cases (e.g.,
from 56 to as few as 6). The average numbers of selected   Table 4: Run time (in second) for each algorithm.
features for the 9 data sets are 15.22 (INTERACT), 16.89   Data Set   IN    FC     CFS   Re     FO
(FCBF), 13.22 (CFS), 12.89 (ReliefF), and 640.78 (Full Set). lung-cancer 0.09 0.02 0.05  0.02   2.31
For four data sets (indicated by NA in the table), FOCUS did zoo      0.08  0.01   0.01  0.02   0.81
                                                           wine       0.09  0.02   0.02  0.02   0.33
not ﬁnish after 3 hours. For the 4 synthetic data sets with soy-large 0.14  0.06   0.05  0.09   NA
known relevant features (Table 1), INTERACT selected only  cmc        0.14  0.02   0.03  0.11   2.81
the relevant ones. Next we examine the effect of this reduc- vehicle  0.20  0.05   0.06  0.11   976.58
tion on accuracy.                                          kr-vs-kp   1.31  0.27   0.31  0.61   NA
                                                           internet-ads 150.86 60.484 54.344 31.359 NA
  Predictive accuracy before and after feature selection.  45×4026+2C 20.30 1.02   323.25 1.50  NA
For the 9 data sets with known interactions, we obtained pre- average 19.25 6.88   42.01 3.76   196.57
dictive accuracy rates by 10-fold cross validation using C4.5
and SVM. The results are shown in the two sub-tables of Ta-
ble 5. For both classiﬁers, the reduction of features by IN- FCBF employs SU, we observe in Table 5 that FCBF does
TERACT obtains results comparable with using all features: not perform as well as INTERACT in selecting interacting
average accuracy 83.58% (INTERACT) vs. 79.74% (Full   features. The results suggest that the combination of SU and
Set) for C4.5, and 82.88% (INTERACT) vs. 81.04% (Full c-contribution helps INTERACT to achieve its design goal.
Set) for SVM. Comparing INTERACT with other feature se- Clearly, using SU is a heuristic. There could be other alter-
lection algorithms, INTERACT performs consistently better natives to rank the features. Studying the effects of different
for C4.5 with better average accuracy. For SVM, INTER- ranking algorithms is one line of our on-going research.
ACT is comparable with other feature selection algorithms. The effect of the data structure. We devised a hashing
One exception is the ‘soy-large’ data for the result of SVM. data structure to speed up the time consuming calculation
We notice that the data set has 35 features, 306 instances, and of c-contribution during feature selection. Here we examine
19 classes (Table 2); INTERACT identiﬁes 13 features (Ta- how effective the data structure is by comparing the run time
ble 3) - the smallest number of selected features (FCBF also
                                                      of INTERACT with that of INTERACT\D   which does not
selected 13 features). We surmise that it may be too easy for employ the hashing data structure. Since data size is a deter-
the inconsistency measure to be satisﬁed with a small feature mining factor for run time, we ﬁrst reorganize the 9 data sets
subset when each class has a small number of instances. In according to their sizes (approximated by N ∗ M - number
sum, for both classiﬁers, INTERACT can help achieve bet- of features multiplied by number of instances without con-
ter or similar accuracy, and hence, INTERACT is effective in sidering feature types such as nominal or continuous). Fig-
search of interacting features.                       ure 2 shows the ratios using time of INTERACT as the base:
  The effect of feature ranking. INTERACT ranks features
                                                      TINTERACT     divided by TINTERACT. It shows that the
before backward elimination of features begins. As a part        \D
                                                      run time difference between INTERACT and INTERACT\D
of the lesion study, we remove the ranking component from is more pronounced for larger data sets.
INTERACT to form a version INTERACT\R. We summa-
rize the results here due to the space limit: INTERACT is Run time comparison. Table 4 records the run time for
always better than or equivalent to INTERACT\R;theaver- each feature selection algorithm. Except for FOCUS, all al-
age 10-fold CV accuracy for C4.5 and SVM for (INTERACT, gorithms ﬁnished their runs within the given time. The algo-
INTERACT\R)   are (83.58,78.69) and (82.88 78.54), respec- rithms are ordered as ReliefF, FCBF, INTERACT, CFS and
tively. Noticing that INTERACT ranks features using SU and FOCUS: ReliefF is the fastest, and FOCUS is the slowest if it
                                                      ﬁnishes the run within 3 hours.

                                                IJCAI-07
                                                  1160