       Structure Inference for Bayesian Multisensory Perception and Tracking

                 Timothy M. Hospedales       Joel J. Cartwright    Sethu Vijayakumar
                School of Informatics, University of Edinburgh, EH9 3JZ, Scotland, UK
           t.hospedales@ed.ac.uk, j.j.cartwright@sms.ed.ac.uk, sethu.vijayakumar@ed.ac.uk


                    Abstract                          about the association of observations with latent states. More-
                                                      over, we illustrate that using the EM algorithm, inference can
    We investigate a solution to the problem of multi- be performed simultaneously with parameter estimation for
    sensor perception and tracking by formulating it in unsupervised learning of perceptual models.
    the framework of Bayesian model selection. Hu-
    mans robustly associate multi-sensory data as ap-
    propriate, but previous theoretical work has fo-  2Theory
    cused largely on purely integrative cases, leaving Humans and machines equipped with multiple sensor modal-
    segregation unaccounted for and unexploited by    ities need to combine information from various senses to ob-
    machine perception systems. We illustrate a uni-  tain an accurate uniﬁed perception of the world. Perception
    fying, Bayesian solution to multi-sensor perception requires computing tangible quantities of interest in the world
    and tracking which accounts for both integration  (e.g. people’s locations) as well as the association between
    and segregation by explicit probabilistic reasoning sensor observations (e.g. who said what). To formalize the
    about data association in a temporal context. Unsu- perceptual problems faced by a human or machine equipped
    pervised learning of such a model with EM is illus- with multiple sensor modalities, we use a probabilistic gener-
    trated for a real world audio-visual application. ative modelling framework. The task of perception then be-
                                                      comes that of performing inference in the generative model,
1  Introduction                                       where object states and data association are both inferred.
                                                        We can frame the inference of data association equivalently
There has been much recent interest in optimal multi-sensor as a model selection or a structure inference problem. A
fusion both for understanding human multi-modal perception graphical model for the process of generating observations in
[Ernst and Banks, 2002; Alais and Burr, 2004] and for ma-
                                                      two different modalities D = {x1,x2} from a single source
                         [
chine perception applications Beal et al., 2003; Perez et al., with latent state l is illustrated in Fig. 1(a). The source state
    ]
2004 . Most of these have considered the simple cases in is drawn independently along with binary visibility/occlusion
which the observations are known to be generated from the
                                                      variables (M1, M2) specifying its visibility in each modality.
same latent source, and the task is to make the best estimate
                                                      The observations are then generated with xi being dependent
of the latent source state by fusing the observations. (We call
                                                      on l if Mi =1or on a background distribution if Mi =0.
models assuming such a fused structure pure fusion models.) Alternately, all the structure options could be explicitly enu-
However, in most real world situations any given pair of ob- merated into four separate models, and the generation process
servations are unlikely to have originated from the same latent then ﬁrst selects one of the four generative models before se-
source. A more general problem in multi-sensor perception lecting l and generating the observations according to the de-
is therefore to infer the association between observations and pendencies encoded in that model. Inference then consists of
any latent states of interest as well as any fusion (integration) computing the posterior over the latent state and the gener-
or ﬁssion (segregation) that may be necessary. This data as- ating model (either as speciﬁed by the two binary structure
sociation problem has been of more long standing interest in
                                                      variables Mi or a single model index variable) given the ob-
                  [                   ]
the radar community Bar-Shalom et al., 2005 . The data as- servations. An observation in modality i is perceived as being
sociation may not merely be a nuisance variable required for associated with (having originated from) the latent source of
correct sensor combination. It can be of intrinsic interest for
                                                      interest with probability p(Mi =1|D), which will be large if
understanding the data. For example, a key task in interpret- the observation is likely under the foreground distribution and
ing a meeting for a human or machine is not just to infer who small if it is better explained by the background distribution.
was there and what was said, but to correctly associate visual
and acoustic observations to understand who said what. An Illustrative Example
  In this paper, we illustrate the commonality of these multi- To illustrate with a toy but concrete example, consider the
sensor perception problems and provide a unifying, princi- problem of inferring a single dimensional latent state l rep-
pled Bayesian account of their solution, reasoning explicitly resenting a location on the basis of two point observations

                                                IJCAI-07
                                                  2122 (a)           (b)                   (c)              marginal foreground distributions. The posterior of the fully
                                                      segregative model depends on the background distributions
                                                      and hence tends toward being independent of the data except
                                                      via the normalization constant. In contrast, the posterior of
                                                      the fully integrative model depends on the three way agree-

    X , X  Correlated                   Pure Fusion Posts
     1 2       Infer X1, X2 Visible Integrative Post  ment between the observations and the prior. The assumption
(d)                                                   of a one dimensional Gaussian prior and likelihoods is to fa-
                                       p(L|D)
                          p(L|D)
              p(M|D)                                  cilitate illustrative analytical solutions; this is not in general a
   Likelihood
     X  Discrepant Infer X  Visible Integrate X  Only Location Error
     2             1            1                     restriction of our framework as can be seen in Sec. 3.

(e)                                                     Fig. 1 illustrates a schematic of some informative types of
                                       p(L|D)
                          p(L|D)
              p(M|D)


   Likelihood                                         behavior produced by this model. If the data and the prior are

   X1,X2 Both Discrepant Infer Neither Visible Segregation: Prior State Precision Error all strongly correlated (Fig. 1(d)) such that both observations

(f)                                                   are inferred with near certainty to be associated with the latent
                                       p(L|D)
                          p(L|D)
              p(M|D)

   Likelihood                                         source of interest, the fused posterior over the location is ap-
                                                                                              ˆ
  X1,X2 Medium Discrepancy Visibility Uncertain State Uncertain Precision Error proximately Gaussian with p(l|x1,x2) ≈N(l|l, pl|x) where
(g)                                                                      ˆ   p1x1+p2x2
                                                       l|x =  1 + 2 +  l,  =          .If 2 is strongly dis-


                          p(L|D)                      p      p   p    p  l               x
                                       p(L|D)
              p(M|D)                                                            pl|x
   Likelihood
        L        M1  M2        L
                                            L         crepant with x1 and the prior (Fig. 1(e)), it would be inferred
                                                      that sensor 2 was occluded and its observation irrelevant. In
Figure 1: (a) Graphical model to describe un-reliable gen- this case, the posterior over the location is again near Gaus-
                                                                                                 ˆ   p1x1
eration of multi-modal observations xi, occlusion semantic              1             l|x = 1 + l =
                                                      sian but fusing only x and the prior; p p p , l pl|x .
(b,c) generation with one or two source objects, multi-object If both x1 and x2 are strongly discrepant with each other and
semantic (c-g) Inference in occlusion semantic model. Ob- the prior (Fig. 1(f)), both observations are likely to be back-
servations (d) x1,x2 strongly correlated, (e) x2 strongly dis- ground originated, in which case the posterior over the latent
crepant, (f) x1,x2 both strongly discrepant, (g) x1,x2 both                        ˆ
                                                      state reverts to the prior pl|x = pl, l =0. Finally, if the corre-
moderately discrepant. Likelihoods of the observations in
                                                      lation between the observations and the prior is only moderate
each of two modalities in black, prior in grey.
                                                      (Fig. 1(g)) such that the posterior over the structural visibility
                                                      variables is not near certain, then the posterior over the la-
in separate modalities. For the purpose this illustration, let tent state is a (potentially quad-modal) mixture of Gaussians
l be governed by an informative Gaussian prior centered at corresponding to the 4 possible models. For real world data,
zero, i.e., p(l)=N (l|0,pl) and the binomial visibility vari-
                         (  )=                        occlusion, or other cause for meaningless observation is al-
ables have prior probability p Mi πi. (Note that we use most always possible, in which case assuming a typical pure
precisions rather than covariances throughout). If the state
                                                      fusion model (equivalent to constraining M1 = M2 =1) can
is observed by sensor i (Mi =1) then the observation in
                                                      result in dramatically inappropriate inference (Fig. 1(box)).
that modality is generated with precision pi, such that xi ∼
N ( |   )
   xi l, pi . Alternately, if the state is not observed by the Incorporating Temporal Dependencies
sensor, its observation is generated by the background distri-
bution N (xi|0,pb), which tends toward un-informativeness Now we consider the case where the latent state of inter-
with precision pb → 0. The joint probability can then be writ- est and data association are correlated in time. A graphical
ten as in Eq. 1. If we are purely interested in computing the model to describe the generation of such data is illustrated
posterior over latent state, we integrate over models or struc- in Fig. 2(a). The state l and model variables Mi are each
ture variables. For the higher level task of inferring the cause connected through time, producing a factorial hidden Markov
or association of observations, we integrate over the state to model [Ghahramani and Jordan, 1997]. To generate from this
compute the posterior model probability, beneﬁting from the model, at each time t the location and model variables are
automatic complexity control induced by Bayesian Occam’s selected on the basis of their states at the previous time and
razor [MacKay, 2003]. Deﬁning for brevity mi ≡ (Mi =1)                         t+1 t        t+1   t
                                                      the transition probabilities p(l |l ) and p(M |M ). Con-
and mi ≡ (Mi =0), we can write down the posteriors as in                                    i     i
Eq. 2.                                                ditional on these variables, each observation is then generated
                                                      in the same way as for the previous independently and iden-
                    M1          (1−M1)         M2
 p(D, l, M)=N (x1|l, p1) N (x1|0,pb)  N (x2|l, p2)    tically distributed (IID) case. Inference may then consist of

                     (1−M2)                           computing the posterior over the latent variables at each time
           ·N (x2|0,pb)   N (l|0,pl)p(M1)p(M2)  (1)                                   t   t 1:T  1:T
                                                      given all T available observations, p(l , M |x ,x ) (i.e.,
 p(m , m |D) ∝N(x |0,p )N (x |0,p )                                                         1    2
    1  2         „1   b    2   b   «                  smoothing) if processing is off-line. If the processing must
                    1  2                              be on-line, the posterior over the latent variables given all
 p(m1, m2|D) ∝ exp − x1p1pl/(p1 + pl) N (x2|0,pb) (2)                             t   t 1:t 1:t
                    2                                 the data up to the current time p(l , M |x1 ,x2 ) (i.e., ﬁlter-
 p(m1,m2|D) ∝                                         ing) may be employed. Multi-modal source tracking is per-
        »                                      –
          1 x2p (p + p ) − 2x x p p + x2p (p + p )    formed by computing the posterior of l, marginalizing over
    exp  −   1 1 2    l    1 2 1 2   2 2  1   l
          2              p + p + p                    possible associations. We have seen previously that the pos-
                         1    2   l                   terior distribution over location at a given time is potentially
  Intuitively, the structure posterior (Eq. 2) is dependent non-Gaussian (Fig. 1(e)). To represent such general distribu-
on the relative data likelihood under the background and tions, we can discretize the state space of l. In this simple

                                                IJCAI-07
                                                  2123case, exact numerical inference on the discretized distribu-
tion is tractable. Given state transition matrices p(lt+1|lt) and
p(Mt+1|Mt), we can write down recursions for inference in
this factorial hidden Markov model in terms of the posteriors
 t     t   t   1:t     t     t   t   1:T              (a)
α   p(l , M1,2|D ) and γ  p(l , M1,2|D ) as

   t
  α  ∝                                          (3)
      X                        Y2
               t t   t    t t−1       t  t−1  t−1                 Actual Input            Pure Fusion Posterior
            p(D |l ,M1,2)p(l |l ) p(Mi |Mi  )α
    t−1  t−1                   i=1
    l  ,M1,2
  γt ∝                                                (b)                        (e)

                                                (4)       Location                  Location
                        Q
      X         p(lt+1|lt) 2 p(M t|M t−1)αt
            P            i=1Q   i  i         γt+1
                    p(lt+1|lt) 2 p(M t|M t−1)αt
         t+1  t  t                  i  i
    t+1       l ,M1,2        i=1                                 Smoothed Posterior         IID Posterior
    l  ,M1,2

  Filtering makes use of the forward α recursion in Eq. 3 (c)                    (f)

and smoothing the backward γ recursion in Eq. 4, which are Location                 Location
analogues of the α and γ recursions in standard HMM in-
ference. The beneﬁts of temporal context for inference of
source state and data association are illustrated in Fig. 2(b-    Model Posterior          Filtered Posterior
g). Fig. 2(b) illustrates data from a series of T observations,
 t      t          t  t T
xi ∼N(l  ,p), D = {x1,x2}t=1, in two independent modal- (d)                      (g)
                                                                                    Location
ities, of a continuously varying latent source l. These data p(Obs|D)
include some occlusions/sensor failures, where the observa-
                                                                    Time                     Time
tion(s) are generated from a background distribution, and an
unexpected discontinuous jump of the source. The temporal
state evolution models for l and M are simple diffusion mod- Figure 2: (a) Graphical model to describe generation of ob-
els. The robustness of source location inference by a pure servations xi with temporal dependency. (b) Synthetic input
fusion model without temporal context (Fig. 2(e)) is very lim- dataset in modality x1 and x2. (c) Posterior probability of l
ited, as it must always averages over observations, which is and (d) posterior probability of model structure for the tem-
inappropriate when they are actually disassociated. A data poral data association model. Posterior probability of l in (e)
association model (Fig. 2(f)) is slightly more robust, inferring pure fusion model (f) IID data association model (g) ﬁltered
that the generation structure was likely to have been differ- data association model.
ent when the observations are discrepant. However, without
temporal context, it cannot identify which observation was
discrepant, and hence produces a non-Gaussian, multi-modal (Fig. 1(b)) can also be expressed compactly as structure in-
posterior for l. Including some temporal history, an on-line ference as before by also using two latent state variables as in
                                                      the single source case, but requiring equality between them
ﬁltering data association model can infer which observations =1                    =0
are discrepant, and discount them, producing much smoother if M and independence if M  (Fig. 1(c)). It is pos-
inference (Fig. 2(g)). In this case, after the discontinuity in sible to enumerate all ﬁve possible model structures and per-
state, the fully disassociated observation structure is inferred form the Bayesian model selection given the data. However,
and based on the temporal diffusion model, approximately frequently the semantics of a given perceptual problem cor-
constant location is inferred until enough evidence is accumu- respond to a prior over models which either allows the four
lated to support the new location. Finally, an off-line smooth- discussed earlier (“occlusion semantic”) or a choice between
ing data association model (Fig. 2(c)) infers a robust, accurate one or two sources (“multi-object semantic”). The occlu-
trajectory. For this case, the marginal posterior of the associa- sion semantic arises for example, in audio-visual processing
tion variables is shown in Fig. 2(d). The illustrative scenarios where a source may independently be either visible or audi-
discussed here generalize in the obvious way to more obser- ble. The multi-object semantic arises, for example in some
                                                                             [                ]
vations. With many sensors, the dis-association of a smaller psychophysics experiments Shams et al., 2000 where both
number of discrepant sensors can be inferred even without sensors have deﬁnitely observed an interesting event, and the
prior information. In a pure fusion scheme, a single highly task is to decide what they observed, which is conditionally
discrepant sensor can throw off the others during averaging. dependent on whether they observed the same source or not.
                                                        We will now illustrate the latter case with a toy but concrete
An Illustrative Example with Multiple Objects         example of generating observations in two different modali-
There is another simple way in which two multi-modal point ties x1,x2 which may both be due to a single latent source
observations can be generated, i.e., each could be gener- (M =1), or two separate sources (M =0). Using vec-
                                                                                                       T
ated by a separate source instead of a single source. The tor notation, the likelihood of the observation x =[x1,x2]
                                                                                    T
choice of the multi-source versus the fused generating model given the latent state l =[l1,l2] is N (x|l, Px) where

                                                IJCAI-07
                                                  2124Px  =  diag([p1,p2]). Let us assume the prior distribu-
tions over the latent locations are Gaussian but tend to un-
informativeness. In the multi-object model the prior over lis
p(l|M =0)=N     (l|0, P0) is uncorrelated, so P0 = p0I
and p0 → 0. In the single object model, the prior over lis
p(l|M =1)=N    (l|0, P1) requires the lis to be equal so P1
is chosen to be strongly correlated. The joint probability of
the whole model and the structure posterior are given in Eq. 5.

 p(x, l,M)=N (x|l, P )N (l|0, P )(1−M)N (l|0, P )M p(M)
          Z        x        0            1
                              (1−M)        M
 p(M|x) ∝   N (x|l, Px)N (l|0, P0) N (l|0, P1) p(M)
           l
                      −1    −1 −1
 p(M =0|x) ∝N(x|0,  (Px + P0  )  )p(M =0)
                      −1    −1 −1
 p(M =1|x) ∝N(x|0,  (Px + P1  )  )p(M =1)       (5)
  A schematic of interesting behavior observed is illustrated
in Fig. 3. If x1 and x2 are only slightly discrepant (Fig. 3(a)),
then the single object model is inferred with high probabil-
ity. The posterior over l is also strongly correlated and Gaus-
sian about the point of the fused interpretation; p(l|x) ≈
    ˆ           ˆ    −1
N (l|l, Pl|x) where l = Pl|x Pxx, Pl|x = Px+P1. The loca-
tion marginals for each li are therefore the same and aligned
  ˆ
at l.Ifx1 and x2 are highly discrepant (Fig. 3(b)), then the
two object model is inferred with high probability. In this
case the posterior p(l|x) is spherical and aligned with the ob-
servations themselves rather than a single fused estimate; i.e.
ˆ    −1
l = Pl|x Pxx ≈ x, Pl|x = Px + P0.
  The inferences discussed so far have been exact. There
are various potential approximations such as computing the
location posterior given the MAP model, which may be ac- Figure 3: Inference in multi-object semantic toy model. (a)
ceptable, but which crucially misrepresents the state poste- For correlated inputs, x1 ≈ x2, the presence of one objects is
rior for regions of input space with intermediate discrepancy inferred and its location posterior is the probabilistic fusion of
(c.f. Fig. 1(d)). Alternately, the model probability could be the observations. (b) For very discrepant inputs, x1 = x2,the
approximated using a MAP or ML estimate of the state. The presence of two objects is inferred and the location posterior
agreement between the Bayesian and MAP solution depends for each is at the associated observation.
on how sharp the state posterior is, which depends on both
the agreement between the observations and the precision of in complex ways on the latent state, a topic we will address
their likelihoods. Using the ML estimate of the state will not in the real world application discussed next.
work at all as the most complex model is always selected.
  Previous probabilistic accounts of human multi-sensory 3 Bayesian Multi-sensory Perception for
combination (e.g. [Ernst and Banks, 2002; Alais and Burr,
2004]) are special cases of our theory, having explicitly or im- Audio-Visual Scene Understanding
plicitly assumed a pure fusion structure. [Trieschandvonder To illustrate the application of these ideas to a real, large scale
Malsburg, 2001] describe a heuristic democratic adaptive cue machine perception problem, we consider a task inspired by
integration perceptual model, but again assume a pure fusion [Beal et al., 2003]; that of unsupervised learning and infer-
structure. Hence these do not, for example, exhibit the ro- ence with audio-visual (AV) input. [Beal et al., 2003] demon-
bust discounting (sensory ﬁssion or segregation) of strongly strated inference of an AV source location and learning of
discrepant cues observed in humans [Ernst and Banks, 2002]. its auditory and visual templates based on correlations be-
As we have seen, such ﬁssion is necessary for perception in tween the input from a camera and two microphones - use-
the real world as outliers can break pure fusion schemes. We ful for example, in teleconferencing applications . The AV
provide a principled probabilistic, adaptive theory of tempo- localization part of this task is similar to the task required
ral sensor combination which can account for fusion, ﬁssion in psychophysics experiments such as [Alais and Burr, 2004]
and the spectrum in-between. The combination strategy is where humans are also reported to exhibit near Bayes optimal
handled by a Bayesian model selection without recourse to sensor fusion. We now tackle the bigger scene understand-
heuristics, and the remaining parameters can be learned di- ing problem of inferring how the AV data should be associ-
rectly from the data with EM. A more challenging question is ated through time (pure fusion was previously assumed), i.e,
that of realistic multi-dimensional observations which depend whether the source should be associated with both modalities,

                                                IJCAI-07
                                                  2125                                                                         J
                                                      H  = {a, v,t,l,W,Z}j=1 factorizes as

                                                               JY−1
                                                      p(D, H)=    p(lj+1|lj )p(W j+1|W j )p(Zj+1|Zj )
                                                               j=1
                                                              YJ
                                                             ·   p(x1|W, a)p(x2|W, a,t)p(a)p(t|l)p(v)p(y|Z, v,l)
                                                              j=1
                                                            
                                                             YJ
                                                                         W           (1−W )
                                                         =      N (x1|a,v1) N (x1|0,σ1)   N (a|0,η)
                                                             j=1
                                                             ·N(x  |T a,v )W N (x |0,σ )(1−W )N (τ|αl + β,ω)
                                                                  2 t   2      2    2            ”
                                                                         Z          (1−Z)
                                                             ·N(y|Tlv, ΨI) N (y|γ1, I)   N (v|μ, φ)
                                                              JY−1
                                                             ·   Γlj ,lj+1 Θwj ,wj+1 Ωzj ,zj+1         (6)
    Figure 4: Graphical model for AV data generation.         j=1

or only one, or if there is no source present at all. 3.2  Inference
                                                      Consider ﬁrst the inference for a single frame of data. The
3.1  Introduction                                     posterior marginal of interest for this task is that of the dis-
                                                                                               (     | )
A graphical model to describe the generation of a single crete location and visibility structure variables p l, W, Z D .
frame of AV data D = {x1, x2, y} is illustrated in Fig. 4. Because of the linear-Gaussian structure of the model, the
                                                                              a     v
A discrete translation l representing the source state is se- latent appearance variables and can be analytically in-
lected from its prior distribution πl and its observability in tegrated out, leaving only the inter-microphone delay t to
                                                                                               (     | )
each modality (W, Z) are selected from their binomial pri- be summed out numerically when computing p l, W, Z D .
ors. For simplicity, we only consider source translation along Conditioned on the fused model, and other discrete variables
                                                      (  =1     =1    )
the azimuth. Consider ﬁrst the all visible case (W, Z =1). Z , W   ,t,l the posteriors over the latent signals are
                                                               N (a|       )     N (v|       )
The video appearance v is sampled from a diagonal Gaus- Gaussian,   μa|x,t,νa and    μv|y,l,νv , with preci-
                                                                                   −1               T
sian distribution N (v|μ, φ) with parameters deﬁning its soft sion and mean given by μa|x,t = νa (λ1ν1x1 +λ2ν2Tt x2),
                                                                  2       2              −1        T
template. The observed video pixels are generated by sam- νa = η + λ1ν1 + λ2ν2, μv|y,l = νv (φμ + Tl Ψy),
pling from another Gaussian N (y|Tlv, ΨI) the mean of νv = φ +Ψ. The marginal video likelihood is also Gaussian
                                                                             −1      −1  T −1
which is the sampled appearance, translated by l using the with μy|l = Tlμ, νy|l =(Ψ + Tlφ Tl ) . Expressions
transformation matrix Tl. The latent audio signal a is sam- for the likelihood of the fully fused model and the source lo-
pled from a zero mean, uniform covariance Gaussian i.e., cation (Eq. 7) and the likelihood of the fully ﬁssioned model
N (a|0,ηI). The time delay between the signals at each mi- (Eq. 8) can be derived in terms of these statistics. Deﬁning
crophone is drawn as a linear function of the translation of again zi ≡ (Zi =1)and zi ≡ (Zi =0)etc, we can write
the source N (t|αl + β,ω). Given the latent signal and the           Z           X  Z
                  x
delay, the observation i at each microphone is generated by p(D|w, z, l) ∝ p(y, v|l, z) p(x1, x2, a,t|l, w)
sampling from a uniform diagonal Gaussian with the mean               v           t  a
a      x                           x   N (x |a  I)                           X
 , with 2 shifted t samples relative to 1; 1 ,v1  ,           ∝N(y|μ    ,ν  )   p(t|l, D)exp(μT ν μ  )
N (x |T a   I)                             (  =0)                     y|l y|l              a|t,x a a|t,x (7)
    2  t ,v2 . If the video modality is occluded Z ,                          t
the observed video pixels are drawn from a Gaussian back- p(D|w,z) ∝ p(x|w)p(y|z)
ground distribution N (y|γ1, I) independently of l and au-
                                                              = N (x |0,σ I)N (x |0,σ I)N (y|γ1, I)
dio data. If the audio modality is silent (W =0),thesam-            1   1      2   2                   (8)
ples at each speaker are drawn from background distributions For a single observed modality, the likelihood is a mixture
N (x |0  I)
    i ,σi  independently of each other, l and the video. of terms from Eqs. 7 and 8, along the lines of Eq. 2. To infer
  To describe the generation of a series of correlated frames, the posterior over location and observability for IID frames,
the IID observation model in Fig. 4 is replicated and a fac- these likelihoods can be combined with the discrete prior dis-
tored Markov model is deﬁned over the location and associ- tributions over the location and observabilities. To infer the
             (     )
ation variables l, W, Z exactly as the toy model was devel- probability of location and observability over time given the
oped previously (refer Fig. 2(a)). Using j to index time, the data, the likelihoods are used in recursions exactly like those
state evolution distribution over the location shift is deﬁned in
                 j+1 j                                in Eqs. 3 and 4.
the standard way p(l |l )=Γlj ,lj+1 , where the subscripts
pick out the appropriate element of the matrix Γ. The observ-
                                        j+1   j       3.3  Learning
ability transitions are deﬁned similarly as p(W |W )=                                                  =
               j+1  j                                 All   the   parameters  in  this  model    θ
Θ  j j+1     (    |  )=Ωj     j+1
 w ,w   and p Z    Z       z ,z . Suppressing index-  {λ1,2,ν1,2,η,α,β,ω,πl,μ,φ,Ψ, Γ, Θ, Ω,πw,πz,γ, ,σ1,2}
ing by j for clarity, the joint probability of the model in- are jointly optimized by a standard EM procedure of al-
                             J
cluding visible D = {x1, x2, y}j=1 and hidden variables ternately inferring the posterior distribution q(H|D) over

                                                IJCAI-07
                                                  2126