      Using a Hierarchical Bayesian Model to Handle High Cardinality Attributes
                   with Relevant Interactions in a Classiﬁcation Problem                ∗
                   Jorge Jambeiro Filho                           Jacques Wainer
               Secretaria da Receita Federal                   Instituto de Computac¸˜ao
           Alfndega do Aeroporto de Viracopos            Universidade Estadual de Campinas
             Rodovia Santos Dummont, Km 66                        Caixa Postal 6176
           Campinas-SP, Brazil, CEP 13055-900          Campinas - SP, Brazil, CEP 13083-970
               jorge.ﬁlho@jambeiro.com.br                      wainer@ic.unicamp.br

                    Abstract                            With only 1.2% of positive examples, dataset is imbal-
                                                      anced what is usually handled with different resampling
    We employed a multilevel hierarchical Bayesian    strategies [Chawla et al., 2002]. However, resampling re-
    model in the task of exploiting relevant interactions quires retraining the classiﬁers for each different assignment
    among high cardinality attributes in a classiﬁcation of costs for false positives and false negatives. In our con-
    problem without overﬁtting. With this model, we   text, such costs are not known in advance (priorities changes
    calculate posterior class probabilities for a pattern acording to other anti-fraud demands) and they may vary
    W  combining the observations of W in the train-  from example to example (not all false negatives cost the
    ing set with prior class probabilities that are ob- same). Thus we cannot train the classiﬁers for all possible
    tained recursively from the observations of patterns cost assignments in advance.
    that are strictly more generic than W . The model
    achieved performance improvements over standard     On the other hand, if we can produce reliable probability
    Bayesian network methods like Naive Bayes and     estimates directly from the original dataset the work of the
    Tree Augmented Naive Bayes, over Bayesian Net-    human resource allocation system becomes much easier. It
    works where traditional conditional probability ta- can, for example, at any time, deﬁne a selection rate SR that
    bles were substituted by Noisy-or gates, Default Ta- matches the available human resources for the speciﬁc task
    bles, Decision Trees and Decision Graphs, and over of detecting wrong customs codes considering all other anti-
    Bayesian Networks constructed after a cardinality fraud demands at the moment. The examples to be veriﬁed
    reduction preprocessing phase using the Agglom-   will naturally be the SR examples that are most likely to
    erative Information Bottleneck method.            involve a misclassiﬁcation. The allocation system may also
                                                      combine the probability estimates with costs that may vary
                                                      from to example to example without any retraining. It be-
1  Introduction                                       comes also unnecessary that customs administration discuss
                                                      their cost criteria with us. Thus we decided to concentrate on
In most countries, imported goods must be declared by the Bayesian techniques and not to use resampling or any other
importer to belong to one of large set of classes (customs technique that requires retraining when costs change.
codes). It is important that each good is correctly classiﬁed, Domain specialists claim that there are combinations of at-
because each of the customs codes mean not only different tributes values (some involving all of them) that make the
customs duties but also different administrative, sanitary, and probability of an instance being positive signiﬁcantly higher
safety requirements. The goal of this project is to develop then it could be expected looking at each value separately.
a tool that, considering four attributes: declared custom code They call such combinations critical patterns. To beneﬁt
(DCC), importer (IMP), country of production (CP) and entry from critical patterns we would like to use a Bayesian Net-
point in the receiving country (EPR), will estimate, for each work (BN)[Pearl, 1988] where all attribute nodes are parents
new example, the probability that it involves a misclassiﬁca- of the class node. We call such structure the Direct BN Struc-
tion. Such estimates will be used latter by a larger system that ture.
allocates human resources for different types of anti-fraud op-
erations.                                               In a BN, considering that xji is a possible value for node
  Our data set has 47826 examples of correct classiﬁcation Xj and πjk is a complete combination of values for Πj,the
(which we will call negative examples) and 590 examples of set of parents of node Xj, the vector, θjk, such that θjki =
misclassiﬁcation (positive examples). In this dataset, the ﬁrst P (xji|πjk), contained in the CPT of a node Xj, is assessed
attribute has 3826 distinct values, the second, 1991 values, from the frequencies of the values of Xj among the training
the third, 101 values, and the fourth 52 values.      instances where Πj = πjk. The distributions of Xj given any
                                                      two different combinations of values for the parents of Xj are
  ∗This work is part of the HARPIA project and is supported by assumed to be independent and a Dirichlet prior probability
Brazil’s Federal Revenue                              distribution for θjk is usually adopted. Applying Bayes rule

                                                IJCAI-07
                                                  2504and integrating over all possible values for θjk it is found that: of being proportional to the product of the cardinality of all
                                                      parents attributes, becomes proportional to the sum of their
                               Njki + αjki
           ( jki)=  ( ji| jk)=                        cardinality. However, causal independence assumptions are
         E θ      P  x  π          +            (1)
                               Njk   αjk              incompatible with our goal of capturing critical patterns.
where Njki is the number of simultaneous observations of It is possible to use more ﬂexible representations for the
xji and πjk in the training set, Njk = ∀i Njki, αjki is the conditional probability distributions of a node given its par-
value of one of the parameters of the Dirichlet prior probabil- ents, like Default Tables (DFs) [Friedman and Goldszmidt,
ity distribution and αjk = ∀i αjki, the equivalent sample 1996], Decision Trees (DTs) [Friedman and Goldszmidt,
size of the prior probability distribution.           1996] and Decision Graphs (DGs) [Chickering et al., 1997].
  The Dirichlet prior probability distribution is usually as- According to [Friedman and Goldszmidt, 1996], using such
sumed to be noninformative, what yields to:           representations together with adequate learning procedures
                                                      induces models that better emulate the real complexity of
                           Njki + λ
              P (xji|πjk)=                      (2)   the interactions present in the data and the resulting network
                          Njk + λMj                   structures tend to be more complex (in terms of arcs) but
where all parameters of Dirichlet distribution are equal to a require fewer parameters. Fewer parameter may result in
small smoothing constant λ,andMj is the number of possible smaller overﬁtting problems. On the other hand, using tra-
values for node Xj. We call this Direct Estimation (DE). ditional CPTs, we assume that the probability distributions
  In the Direct BN Structure the node whose CPT is to be for a node given any two combinations of values for the par-
estimated is the class node and all other attribute nodes are ents are independent. If some of these distribution are actu-
its parents. The Conditional Probability Table (CPT) of the ally identical, DTs, DFs and DGs, can reﬂect it and represent
class node in such a structure contains more than 40 × 109 the CPD using a variable number of parameters that is only
parameters. It is clear that for rarely seen combinations of proportional to the number of actually different distributions.
attributes the choice of the structure in the Direct BN Struc- Using DTs, DFs or DGs to represent the conditional distri-
ture and equation 2 tends to produce unreliable probabilities bution of a node given its parents, we assume that the prob-
whose calculation is dominated by the noninformative prior ability distribution of the node given two different combina-
probability distribution. This suggests that using the Direct tions of values for the parents may be either identical or com-
BN Structure and traditional CPTs we will have overﬁtting pletely independent. It is possible that neither of the two as-
problems.                                             sumptions hold.
  Instead of the Direct BN Structure, we can choose a net- In [Gelman et al., 2003] it is asserted that modeling hierar-
work structure that does not lead to too large tables. This chical data nonhierarchically leads to poor results. With few
can be achieved limiting the number of parents for a network parameters nonhierarchical models cannot ﬁt the data accu-
node. Naive Bayes [Duda and Hart, 1973] is an extreme ex- rately. With many parameters they ﬁt the existing data well
ample where the maximum number of parents is limited to but lead to inferior predictions for new data. In other words
one (the class node is the only parent of any other node). Tree they overﬁt. In contrast hierarchical models can ﬁt the data
augmented Naive Bayes (TAN) [Friedman et al., 1997] adds well without overﬁtting. They can reﬂect similarities among
a tree to the structure of Naives Bayes connecting the non- distributions without assuming equality.
class attributes, and thus limits the maximum number of par- Observing an slight modiﬁcation in equation 2 used
ent nodes to two. However, limiting the maximum number of in [Friedman et al., 1997] in the deﬁnition of a smoothing
parents also limits our ability to capture interactions among schema for TAN we can see that the data that is used to es-
attributes and beneﬁt from critical patterns. Thus, we would timate the CPT of any node that has at least one parent is
prefer not to do it.                                  hierarchical:
  Since the high cardinality of our attributes is creating trou-              Njki + S · P (xji)
ble, it is a reasonable idea to preprocess the data, reducing the P (xji|πjk)=                        (3)
                                                                                  Njk + S
cardinality of the attributes. We can use, for example, the Ag-
glomerative Information Bottleneck (AIBN) method [Slonim where S is a constant that deﬁnes the equivalent sample size
and Tishby, 1999] in this task. However, the process of re- of the prior probability distribution. We call this Almost Di-
ducing the cardinality of one attribute is blind in respect to rect Estimation (ADE). ADE uses the probability distribution
the others (except to the class attribute), and thus it is un- assessed in a wider population to build an informative prior
likely that cardinality reduction will result in any signiﬁcant probability distribution for a narrower population and so it
improvement in the ability to capture critical patterns, which has a hierarchical nature. Such approach was also used, for
always depend on more than one attribute.             example, in [Cestnik, 1990]. ADE is the consequence of in-
  When the number of probabilities to be estimated is too stead of a noninformative Dirichlet prior probability distribu-
large when compared to the size of the training set and tion, adopting a Dirichlet prior probability Distribution where
we cannot ﬁll the traditional conditional probability tables αjki ∝ P (xji).
(CPTs) satisfactorily and [Pearl, 1988] recommends the  ADE gets closer to the true probability distribution, but its
adoption of a model that resorts to causal independence as- discrimination power is not signiﬁcantly better than DE. It is
sumptions like the Noisy-Or gate. Using a Noisy-Or the num- a linear combination of two factors Njki/Njk and P (xji).
ber of parameters required to represent the conditional prob- The second factor is closer to the true probability distribution
ability distribution (CPD) of a node given its parents, instead than its constant counterpart in Direct Estimation but it is still

                                                IJCAI-07
                                                  2505equal for any combination of values of Πj and thus has no For example, if W is {A = a, B = b, C = c}, g(W ) is:
discrimination power.                                   {{   =     =   } {  =     =  } {  =      =  }}
  ADE jumps from a very speciﬁc population (the set of     B   b, C   c , A   a, C  c , A    a, B  b
training examples where Πj = πjk) to a very general pop- We consider that only g(W ) inﬂuences P (Cr|G(W )) di-
ulation (the whole training set). In contrast, we present a rectly, so that P (Cr|G(W )) = P (Cr|g(W )). The inﬂuence
model, that we call Hierarchical Pattern Bayes (HPB), which of the other patterns in G(W ) are captured by the recursive
moves slowly from smaller populations to larger ones bene- process. The ﬁrst step for the decomposition of P (Cr|g(W ))
ﬁting from the discrimination power available at each level. in an expression that can be evaluated recursively is to apply
                                                      Bayes theorem:
2  The Hierarchical Pattern Bayes Classiﬁer
                                                                           P (g(W )|Cr)P (Cr)
                                                          P (Cr|g(W )) =
HPB is a generalization of ADE that employs a hierarchy of                     P (g(W ))
patterns. It combines the inﬂuence of different level patterns         ∝    (               |  ) (  )
in a way that the most speciﬁc patterns always dominate if                 P W1,W2,...,WL   Cr P  Cr
they are well represented and combines patterns in the same where W1,W2,...,WL are the elements of g(W ).
level making strong independence assumptions and a calibra- Then we  approximate   the   joint  probability
tion mechanism.                                       P (W1,W2,...,WL|Cr)   by the product of the marginal
  HPB works for classiﬁcation problems where all attributes probabilities:
are nominal. Given a pattern W and a training set of pairs
                                                                                    L
(X, C),whereC  is a class label and X is a pattern, HPB                             
                                                                 ( | (  )) ∝  (  )     (   |  )
calculates P (Cr|W ) for any class Cr where a pattern is as    P  Cr g W      P Cr     P Wj  Cr       (5)
deﬁned below:                                                                       j=1
Deﬁnition 1 A pattern is a set of pairs of the form   but apply a calibration mechanism:
(Attribute = Value), where any attribute can appear at                       
                                                             P (Cr|g(W )) ∝ P (Cr|g(W )) + B.P(Cr)    (6)
most once. An attribute that is not in the set is said to be
undeﬁned or missing.                                  where B is a calibration coefﬁcient.
                                                        Given equations 5 and 6 we need to calculate P (Wj |Cr).
Deﬁnition 2 A pattern Y is more generic than a pattern W
                                                      Applying Bayes theorem:
if and only if Y ⊆ W .IfY is more generic than W ,wesay
that W satisﬁes Y .                                                           P (Cr|Wj)P (Wj )
                                                                  P (Wj|Cr)=                          (7)
Deﬁnition 3 A pattern Y is strictly more generic than W if                         P (Cr)
and only if Y ⊂ W .
                                                        We estimate P (Cr) using the maximum likelihood ap-
Deﬁnition 4 The level of a pattern W , level(W ), is the num- proach: P (Cr)=Nr/N ,whereNr is the number of ex-
ber of attributes deﬁned in W .                       amples in the training set belonging to class Cr,andN is the
Deﬁnition 5 G(W ) is the set of all patterns strictly more total number of examples in the training set. If it happens
generic than a pattern W                              that Nr is zero we cannot use equation 7. In this case we just
                                                      deﬁne that P (Cr|W ) is zero for any pattern W .
2.1  The Hierarchical Model                             We know that when we substitute P (Wj|Cr) by the right
                                       (  |  )        side of equation 7 into equation 5 we are able to clear out the
HPB  calculates the posterior probability P Cr W ,us-        (   )
ing a strategy that is similar to Almost Direct Estimation, factor P Wj because it is identical for all classes, so we do
but the prior probabilities are considered to be given by not need to worry about it.
                                                        Since Wj is a pattern, the estimation of P (Cr|Wj ) can
P (Cr|G(W )).
  The parameters of the Dirichlet prior probability distrib- be done recursively using equation 4. The recursion ends
                                                      when g(W  ) contains only the empty pattern. In this case
utionusedbyHPBaregivenby:    αr =  S · P (Cr|G(W )),
                                                        ( r| (  )) becomes ( r|{{}})=   (  r).
where S is a smoothing coefﬁcient. Consequently:      P  C  g W           P C          P C

                    Nwr +  S · P (Cr|G(W ))           2.2  Calibration Mechanism
         P (Cr|W )=                             (4)
                           Nw  + S                    In spite of its strong independence assumptions, Naive Bayes
                                                      is know to perform well in many domains when only misclas-
where Nw is the number of patterns in the training set satisfy- siﬁcation rate is considered [Domingos and Pazzani, 1997].
ing the pattern W and Nwr is the number of instances in the However, Naive Bayes is also know to produce unbalanced
training set satisfying the pattern W whose class label is Cr. probability estimates that are typically too “extreme” in the
  Given equation 4, the problem becomes to calculate
 (   | (  ))                        (   | (  ))       sense that they are too close to zero or too close to one.
P Cr G  W  . Our basic idea is to write P Cr G W as a In the aim of obtaining better posterior probability distribu-
function of the various P (Cr|Wj) where the Wj are patterns
            (  )                 (   |  )             tions, calibration mechanisms which try to compensate the
belonging to G W and calculate each P Cr Wj recursively overly conﬁdent predictions of Naive Bayes have been pro-
using equation 4.                                     posed [Bennett, 2000; Zadrozny, 2001].
Deﬁnition 6 g(W ) is the subset of G(W ) whose elements Using equation 5 we are making stronger independence
have level equal to level(W ) − 1.                    assumptions than Naive Bayes. Naive Bayes assumes that

                                                IJCAI-07
                                                  2506attributes are independent given the class, what is at least • PRIOR: Trivial classiﬁer that assigns the prior probability to every instance.
possible. Equation 5 assumes that some aggregations of at- All models involving DGs were constructed follow-
tributes are independent given the class. Since many of these ing [Chickering et al., 1997]. The models involving DFs and
aggregations have attributes in common we know that such DTs were constructed following [Friedman and Goldszmidt,
assumption is false. The main consequences of our stronger 1996] using the MDL scoring metric.
and unrealistic assumption are even more extreme probabil-
                                                        We tried different parameterizations for each method and
ity estimates than Naive Bayes’ ones. This is compensated
                                                      sticked with the parameter set that provided the best results,
by the calibration mechanism in equation 6. This calibration
                                                      where best results mean best area under the hit curve 1 up to
mechanism is analogous to the one used in [Zadrozny, 2001]
                                                      20%  of selection rate (AUC20) 2.Inthey axis, we chose to
in the calibration of decision tree probability estimates.
                                                      represent the Recall = NTruePositives/NPositives, instead
2.3  Selecting HPB Coefﬁcients                        of the absolute number of hits, because this does not change
                                                      the form of the curve and makes interpretation easier. We rep-
Equations 4 and 6 require respectively the speciﬁcations of resented the selection rate in log scale to emphasize the begin-
coefﬁcients S and B. In the classiﬁcation of a single in- ning of the curves. Besides using the hit curve, we compared
stance, these equations are applied by HPB in the calculation
    (  |  )                                           the probability distributions estimated by the models with the
of P Cr W  for several different patterns, W . The optimal distribution actually found in the test set using two measures:
values of S and B can be different for each pattern.  Root Mean Squared Error (RMSE) and Mean Cross Entropy
  In the case of the B coefﬁcients, we use an heuristic mo- (MCE). Figure 1, table 1 and table 2 show our results.
tivated by the fact that the level of any pattern in g(W ) is
                                                        Selection of method parameters is explained below:
level(W ) − 1. The higher such level is, the more attributes
                                                        The smoothing coefﬁcients employed by HPB are all au-
in common the aggregations have, the more extreme proba-
                                                      tomatically optimized. Such optimization involves a leave
bility estimates are and the stronger must be the effect of the
                                                      one out cross validation that takes place absolutely within
calibration mechanism. Thus, we made the coefﬁcient B in
                                                      the current training set (the 5 fold cross validation varies the
equation 6 equal to b(level(W ) − 1) where b is an experi-
                                                      current training set) eliminating the possibility of ﬁtting the
mental constant.
                                                      test set. The B coefﬁcients are deﬁned by the heuristic de-
  In the case of the S coefﬁcients, we employ a greed opti-
                                                      scribed in section 2.3 and by the constant b.Wevariedb
mization approach that starts from the most general pattern
                                                      over the enumeration {0.5, 1.0, 1.5, 2.0, 2.5, 3.0} and sticked
family and move toward the more speciﬁc ones, where a pat-
                                                      with 2.0, which was the constant that produced the best re-
tern family is the set containing all patterns that deﬁne exactly
                                                      sults in a 5 fold cross validation process. To avoid the effects
the same attributes (possibly with different values).
                                                      of ﬁne tuning, we reshufﬂed the data set before starting an-
  Assuming that the S coefﬁcients have already been ﬁxed
                                                      other 5 fold cross validation process. The HPB results that
for all pattern families that are more generic than a family
                                                      we present here came from the second process.
F , there is a single S coefﬁcient that needs to be speciﬁed
to allow the use of equation 4 to calculate P (Cr|W ) where
                                                                     HPB
W  is any pattern belonging to F . We select this coefﬁcient,        DG BSM CB
using leave one out cross validation, in order to maximize the       AIBN/TAN
                                                                     NB
area under the hit curve that is induced when we calculate           TAN
P (Cr|W ) for all training patterns, W ,inF .                        PRIOR

3  Experimental results                                    Recall
All classiﬁcation methods were tested by the Weka Experi-
menter tool [Witten and Frank, 1999] using 5 fold cross val-
idation. We compared classiﬁers built using the following
methods:                                                     0.0 0.2 0.4 0.6 0.8 1
  •                                                             010204070
    HPB: HPB as described in this paper;                                     Selection rate
  • NB:NaiveBayes;
  • Noisy-Or: BN with the Direct BN Structure using Noisy-Or instead of a CPT;
  • TAN: TAN with traditional CPTs;                    Figure 1: Selection Rate X Recall (to avoid pollution we
  • ADE: Almost Direct Estimation. BN with the Direct BN Structure and the smoothing schema only present curves related to a subset of the tested meth-
    described in [Friedman et al., 1997];              ods)
  • DE: Direct Estimation. BN with the Direct BN Structure and traditional CPTs;
  • AIBN/TAN: TAN with traditional CPTs trained over a dataset where cardinality reduction The optimization of AIBN/TAN involves 3 parameters:
    using AIBN was previously applied;                The TAN smoothing constant (STAN), the AIBN smooth-
  • DG CBM: BN with DGs. Complete splits, binary splits and merges enabled;
                                                         1
  • DG CB: BN with DGs. Complete splits and binary splits enabled; We employed hit curves, instead of the more popular ROC
  • DG C: BN with DGs. Only complete splits enabled;  curves, because they match the interests of the customs adminis-
  • DG CM: BN with DGs. Complete splits and merges enabled; trations directly, i.e, the human resource allocation system deﬁnes
  • HC DT: BN with Decision Trees learned using Hill Climbing (HC) and MDL as the scoring a selection rate and needs an estimate for the number of positives
    metric;                                           instances that will be detected.
                                                         2
  • HC DF: BN with Default Tables learned using HC and MDL. All selection rates of interest are below 20%

                                                IJCAI-07
                                                  2507ing constant SAIBN , and the minimum mutual informa-  learned Default Tables included very few rows. The learned
tion constant MMI.  Varying STAN  over the enumera-   Decision Trees and Decision Graphs involved very few splits.
tion {0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0}, SAIBN over the enu- As a consequence, in all cases, the resulting models had little
meration [0.1, 0.2, 0.5, 1.0] and MMI over the enumeration discrimination power.
{0.99, 0.999, 0.9999} we found that the best results are ob- Using Decision Graphs with binary splits enabled, the most
tained with the triple BestTriple =(Stan =0.5,SAIBN = critical patterns were separated from the others, what re-
0.5,MMI   =0.999). We did not cover the whole grid    sulted in a signiﬁcant improvement in the beginning of the
but since we did not observe any abrupt variations in hit hit curves. The other patterns were, almost all, left together
curves and since we covered all immediate neighbors of the what resulted in loss of discrimination power for selection
BestTriplewe believe that such exhaustive covering was not rates above 5%.
necessary.                                              Hypothesis tests, show that HPB is signiﬁcantly better than
                                                      all other classiﬁers in what regards to AUC, AUC20, RMSE
    Method     1%     2%      5%    10%    20%        and MCE. We also performed Hypothesis tests for every se-
    HPB     17.8±3.225.4±1.343.2±2.356.4±2.172.3±5.0                 1%   100%           1%
    TAN      8.8±2.617.4±1.334.0±2.048.6±3.367.1±3.4  lection rate from to      in steps of . HPB is signif-
    AIBN/TAN 10.8±2.317.2±2.732.2±2.747.6±3.268.2±2.7 icantly better than all other classiﬁers for every selection rate
    NB       8.8±0.914.2±3.128.9±3.947.4±4.665.4±5.8
    Noisy-Or 8.6±2.115.2±2.130.3±1.545.9±3.361.5±1.5  below 30% with the exceptions that it is not signiﬁcantly bet-
    BNDG CB 17.6±3.421.8±4.528.6±3.340.8±4.853.7±6.5  ter than: TAN in [17%, 19%] and [27%, 28%];AIBNTANin
    BNDG CBJ 14.7±2.620.6±3.032.2±3.540.6±2.951.5±2.6
    BNDG C   9.1±2.212.2±2.521.1±2.930.1±2.246.4±1.6  [18%, 21%] and [24%, 28%]; BNDG CBJ at 1%; BNDG CB
    ADE     14.0±2.815.4±2.820.5±2.428.1±3.343.4±4.0    1%     2%
    DE      10.2±2.012.0±2.518.1±2.224.7±3.141.1±3.6  at   and    .
    BNDG CJ  3.7±0.77.8±0.715.5±3.724.0±2.439.3±3.4     HPB beneﬁts from critical patterns involving many or even
    HC DF    4.0±1.85.0±3.010.8±3.922.2±3.135.4±1.8
    HC DT    1.0±1.01.3±1.16.7±2.411.5±2.524.7±3.8    all attributes but also considers the inﬂuence of less speciﬁc
    PRIOR    0.8±0.01.7±0.04.2±0.010.2±0.020.3±0.0    patterns. As a consequence, it performs well for any selection
                                                      rate.
 Table 1: Recall for different selection rates with std. dev.
                                                      4   Conclusions
  Method     AUC   AUC20   RMSE    MCE Parameterization
  HPB     84.5±1.653.5±2.310.6±0.04.0±0.1  b =2.0     In the domain of preselection of imported goods for veriﬁ-
  TAN     82.2±0.946.2±2.514.1±0.87.1±0.4 s =0.025    cation some combinations of attribute values can constitute
  AIBN/TAN 82.1±1.346.0±1.212.5±0.55.3±0.3 BestTriple
  NB      81.0±2.143.6±3.414.5±0.16.6±0.2 s =0.025    critical patterns whose inﬂuence over the probability of ﬁnd-
  Noisy-Or 78.2±0.943.0±1.611.9±0.0 infinity          ing a positive instance is very signiﬁcant. Due to the high
  BNDG CB 68.4±3.740.1±3.711.4±0.16.2±0.2  s =0.5
  BNDG CBJ 70.9±2.739.2±2.511.7±0.17.1±1.2 s =0.1     cardinality of the attributes in this domain, exploiting such
  BNDG C  70.4±2.030.1±1.711.0±0.14.6±0.1 s =0.01
  ADE     73.7±1.128.7±3.011.1±0.24.9±0.1 s =0.025    patterns without overﬁtting is challenging. We addressed the
  DE      72.7±1.325.9±2.737.5±0.031.6±0.0 s =2.0     problem using HPB a novel classiﬁcation method based on a
  BNDG CJ 68.5±1.424.2±2.211.1±0.14.7±0.1 s =0.01
  HC DF   62.9±3.421.3±1.810.9±0.04.6±0.0 s =0.05     multilevel hierarchical Bayesian model.
  HC DT   51.8±0.512.9±1.810.9±0.04.7±0.0 s =0.25       HPB was shown capable of capturing the inﬂuence of crit-
          50.0±0.010.2±0.010.9±0.04.7±0.0
  PRIOR                                               ical patterns without overﬁtting and without loosing discrim-
                                                      ination power after the exhaustion of critical patterns in the
Table 2: Area Under Curve, Accuracy of Probability Esti- test set. HPB resulted in a hit curve that is almost unambigu-
mates and Optimal parametrization                     ously better than any of the other methods. HPB also pro-
  Noisy-or and PRIOR have no parameters. The optimiza- duced better probability estimates according to two accuracy
tion of all other methods involves only the smoothing con- measures (table 2).
stant, which, in all cases, was exhaustively varied over the HPB was only validated in a specialized domain, however,
enumeration {0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.0}.This its equations are too simple to reﬂect any particularities of
enumeration covers different magnitudes for the smoothing the domain, except that it is characterized by few high cardi-
constant and at the same time avoids ﬁne tuning.      nality attributes and relevant interactions among them. Thus
  Naive Bayes and Noisy-Or are both unable to capture inter- the use of HPB may be a good option for domains with the
actions among the attributes and cannot explore critical pat- same characteristics or, at least, provide some light on how to
terns. This explains their performance in the very beginning develop good models for such domains.
of the hit curve. Tree Augmented Naive Bayes explores in- HPB training time is exponential in the number of at-
teractions among some attributes and performed better than tributes, linear in the number of training instances and in-
Naive Bayes or Noisy-Or. However, it cannot beneﬁt from dependent of the attributes cardinality. Thus HPB is only
some critical patterns involving many attributes that are deci- applicable to domains where there are few attributes, but in
sive at the selection rate of 1% and 2%.              such domains it is much faster than methods whose training
  Applying cardinality reduction to the attributes before con- time depends on the cardinality of the attributes.
structing the TAN model did not lead to any signiﬁcant im- HPB is not a full Bayesian model in the sense of [Gel-
provements in the hit curve.                          man  et al., 2003], where the parameters associated with a
  Substituting the traditional CPTs of Bayesian Network by sub-population are assumed to be drawn from a general dis-
Decision Trees, Default Tables and Decision Graphs with bi- tribution and the calculation of all involved probability dis-
nary splits disabled only made the hit curves worse. The tributions is done at once considering all available evidence.

                                                IJCAI-07
                                                  2508