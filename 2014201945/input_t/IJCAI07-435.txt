                                Directed Graph Embedding 

                      Mo Chen                                Qiong Yang, Xiaoou Tang 
                  Tsinghua University                         Microsoft Research Asia 
              Beijing, 100084, P.R.China                     Beijing, 100080, P.R.China 
                mochen80@gmail.com                         {qyang, xitang}@microsoft.com 

                   Abstract                      learning techniques, such as Support Vector Machine (SVM), 
                                                 operating data on a vector space or an inner product space. 
    In this paper, we propose the Directed Graph Em-
    bedding (DGE) method that embeds vertices on a Embedding the data of directed graphs to vector spaces be-
    directed graph into a vector space by considering comes quite appealing for tasks of data analysis of directed 
    the link structure of graphs. The basic idea is to graphs. The motivations are: 
    preserve the locality property of vertices on a di- 1) Instead of designing new algorithms for each task in data 
    rected graph in the embedded space. We use the mining on directed graphs that are directly applied to link 
    transition probability together with the stationary structure data, we can first provides a unified framework 
    distribution of Markov random walks to measure to embed the link structure data into the vector space, and 
    such locality property. It turns out that by exploring then utilize the mature algorithms that already exist for 
    the directed links of the graph using random walks, mining on the vector space. 
    we can get an optimal embedding on the vector 2) Directly analyzing data on directed graphs is quite hard, 
    space that preserves the local affinity which is in- since some concepts such as distance, inner product, and 
    herent in the directed graph. Experiments on both margin, which are important for data analysis, are hard to 
    synthetic data and real-world Web page data are define in directed graph. But for vector data, these con-
    considered. The application of our method to Web cepts are already well defined. Tools for analyzing data 
    page classification problems gets a significant im- can be easily obtained. 
    provement comparing with state-of-art methods.  3) Given a huge directed graph with complex link structure, it 
                                                   is highly difficult to perceive the latent relations of the 
 1 Introduction                                    data.  Such information may be inherent in the topological 
 We consider the problem that embeds nodes on directed structure and link weights. Embedding these data into 
 graph into a Euclidean vector space while preserving locality vector spaces will help people to analyze these latent re-
 which is inherent in the graph structure. There is a large lations visually.  
 amount of problems which can be naturally represented as a Some works have been done for embedding on the undirected 
 directed graph. Typical examples are web information re- graph. Manifold learning techniques [M. Belkin and P. Ni-
 trieval based on hyperlink structure, document classification yogi , 2002] [S.T.Roweis and L.K.Saul, 2000] first connect 
 based on citation graphs [C. Lee Giles et al., 1998] and pro- data into an undirected graph in order to approximate the 
 tein clustering based on the pairwise alignment scores [W. manifold structure where the data is assumed to be lying on. 
                                                 Then they embed the vertices of the graph into a low di-
 Pentney and M. Meila, 2005]. Some works have been done to 
                                                 mensional space. Edges of the graph reflect the local affinity 
 deal with the ranking problem on link structure of the Web 
                                                 of node pairs in the input space. In the next, an optimal em-
 including the PageRank [S. Brin and L. Page, 1998] and 
                                                 bedding is achieved by preserving such a local affinity. 
 HITS [J. Dean and M. Henzinger, 1999] algorithms, yet it is 
                                                 However, in the directed case, the edge weight between two 
 still a hard task to do general data analysis on directed graphs 
                                                 graph nodes is not necessarily symmetric. It can not be di-
 such as classification and clustering. In [D. Zhou et al., 2005] 
                                                 rectly used as a measure of affinity. Motivated by [D. Zhou et
 the authors proposed a semi-supervised learning algorithm 
                                                 al., 2005], we formulate the directed graph in a probabilistic 
 for classification on directed graph, and also an algorithm to 
                                                 framework. We use random walks to measure the local af-
 partition the directed graph. In [W. Pentney and M. Meila, finity of vertices on the directed graph. Based on that, we 
2005] the authors proposed algorithms to do clustering on propose an algorithm embedding the nodes on the directed 
protein data which was formulated into a directed graph graph into a vector space by using random walk metric.  
based on asymmetric pairwise alignment scores. However, The rest of the paper is organized as follows: In section 2, we 
up to now, works are quite limited due to the difficulty in give denotations used in our paper. In section 3, we introduce 
exploring the complex structure of directed graphs. On the the details of our method. Implementation issues are ad-
other hand, there are a lot of data mining and machine dressed in section 4. The relation between our method and 


                                            IJCAI-07
                                             2707 previous works is considered in section 5. Experimental 
                                                 the embedded line. The term TV is used to measure the im-
 results are shown in section 6 and the last part is the conclu-
                                                 portance of a vertex on the graph. If Tu() is large, then the 
 sion.                                                                       V
                                                 relation between vertex u  and its neighbors should be em-
 2 Preliminary                                   phasized.  By minimizing such a target, we are able to get an 
                                                 optimized embedding for the graph on one dimensional space. 
                                                 The embedding considers both the local relation of node pairs 
                                                 and global relative importance of nodes. 
                                                 In the following, we address the embedding problem of di-
                                                 rected graphs under two assumptions: 
                                                 1) Two vertices are relevant if there is edge between them. 
                                                   The relevance strength is related to the edge weight. 
 Figure 1. The World Wide Web can be modeled as a directed 2) The out-link of the vertex which has many out-links car-
 graph. Web pages and hyperlink can be represented as vertices ries relatively low information about the relevance be-
            and directed edge of the graph.        tween vertices. 
 An example of directed graph is the World Wide Web as The assumptions are reasonable in many tasks. Let’s again 
 shown in Fig.1. A directed graph GVE(, ) consists of a take the Web by example. Web page authors usually insert 
 finite vertex set V  which contains n vertices, together with links to pages which are relevant to their own pages. There-
an edge set EVV   . An edge of a directed graph is an fore, if a Web page A has a hyper-link pointing to Web page 
                                                 B, we assume A and B might be relevant in some sense. We 
 ordered pair vertex (,uv)  from uv to  . Each edge may 
                                                 should preserve such a relation in the embedded space. 
 associate a positive weightw . An un-weighted directed Let’s consider a web page which has many out links, such as 
 graph can be simply viewed as all the weight of the edge is the home page of www.yahoo.com. The page linked by the 

 one. The out-degree dvO ()of a vertex v  is defined as home page of yahoo may share less similarity with it, and 
 dv()        wvu (, ), where the in-degree dv() of a ver- then in the embedded feature space the two web pages should 
  O      uv, u                      I            have a relatively large distance. We can use the transition 
 tex v  is defined as dv() wuv (,) , u v means u  
                 I      uu, v                    probability of random walks to measure the locality property. 
 has a directed link pointing to v . On the directed graph, we When a web page has many out-links, each out-link will have 
                                                 a relatively low transition probability. Such a measure meets 
 can define a transition probability matrix Ppuv[(,)] of a 
                                         uv,     the assumption 1 and 2. 
 Markov random walk through the graph. It satis- Different web pages have different importance in the Web 
 fies  puv(,) 1, u. We also assume the stationary dis- environment. Ranking web pages according to their impor-
      v
                                                 tance is a well studied area. The stationary distribution of 
 tribution for each vertexv is ( 1), which can be 
                        v   v v                  random walks on the link-structure environment is well 
 guaranteed if the chain is irreducible. For a connected di- known as a good measure of such importance which is used 
 rected graph, a natural definition of the transition probability in many ranking algorithms including PageRank. In order to 
                                                 emphasize those important pages in the embedding feature 
 matrix can be pu(,)vwuvdu (,)/O () in which a random 
 walker on a node jumps to its neighbors with a probability space, we use the stationary distribution u  of random walks 
 proportion to the edge weight. For a general directed graph, to weigh the page u  in the optimization target.  
 we can define a slightly different transition matrix. We will Taking all above into account, we rewrite the optimization 
 be back to this issue in the implementation section. target as follows:  
                                                                                2
                                                                 uupuv(,)( y  yv )  
 3 Algorithm                                                  uv,u  v
                                                 We can further rewrite the formula as follows: 
 We aim to embed vertices on directed graph into a vector           22
 space preserving the locality property of vertex u to all its uuvuvup(,)(uvyy ) ( yy ) puv (,)
 neighbors. First, let’s consider the problem mapping the uvuv,,       uv
                                                   1
 connected directed graph to a line. We define a general op- ()(,)()(,y ypuv22 yypvu)
 timization target as:                             2     uvu               vuv
                                                     uv,,              vu                
                                  2
                                                   1         2
              TuVEu() Tuvy (,)( yv ) 
                                                      ()(,)(,)yyuv u puv v pvu
            uvuv,
                                                   2 uv,
 yu  is the coordinate of vertex u in embedded one dimension Thus the problem is equivalent to embedding the vertices 

 space. The term TE  is used to measure the importance of a into a line while preserving the local symmetric measure 
                                                    puv(,)  pvu (,)/2  of each vertices pair, here 
 directed edge between two vertices. If  TuvE (,) is large, then uv

 the two vertices u  and v should be close to each other on u puv(,) is the probability of a random walker jumps to 


                                            IJCAI-07
                                             2708 vertex u  then to v , i.e. the probability of the random walker the ith row provides the embedding of the ith vertex. 
 passes the edge (,uv) . This also can be deemed as a per- Therefore we minimize  
                                                                           2      T
centage of flux in the total flux at stationary state when we uuvpuv(,) Y Y   2 trY ( LY ) 
continuously import water into the graph. This directed force uvuv,
manifests the impact of u on v , or the volume of message It can be rewrite as  
 that uvconveys to .                                              mintr ( YT LY )
                                                                              
 By optimizing the target we consider not only the local          s..tYT Y  I
 property reflected by edges between node pairs, but also the 
                                                                      **      *         *
 global reinforcement to the relation by taking the stationary The solution is given by Y [2 ,...,k 1 ] where i  is  the 
 distribution of random walks into account.      eigenvector of ith smallest eigenvalue of the generalized 
 We denote                                       eigenvalue problem Ly y . 
                        PPT
                L              
                          2                      4 Implementation Issue  

where P  is the transition matrix, i.e. Pij pi(, j ),  is the 
 diagonal matrix of the stationary distribution, i.e. Input: adjacency matrix W, dimension of target space k and 
                                                 a perturbation factor  
    diag(1 ,...,n ) . Clearly, from the definition L  is sym-
 metric. Then we have the following proposition. 
                                                                       1           1
 Proposition 1                                   1.  Compute PDW()(1)1    eTT       ee, where 
                             2   T                                 O
             uuvpuv(,)( y y )  2 y Ly                                  nn
          uvuv,
                                                        is a vector that i 1  if row i of W  is 0, and DO is 
                T
where yy(1 ,..., yn ) .                              the diagonal matrix of the out degrees. 
The proof of this proposition is given in the appendix. The 
                                                                              T    T
above L  is known as combinatorial Laplacian on a directed 2. Solve the eigenvalue problem P subject to a 
graph [F. R. K. Chung, 2005]. From the proposition we can normalized equationT e 1 . 
see that L  is a semi-positive definite matrix. 
Therefore, the minimization problem reduces to find  3. Construct the combinatorial Laplacian of the directed 
                        T                                        PPT
                 argmin yLy                          graph L            , where   diag( ,..., )
                   y                                               2                   1   n
                 st..  yT y 1
                                                 4.  Solve the generalized eigenvector problem Ly y , 
The constraint yyT 1 removes an arbitrary scaling factor 
                                                     let * ,..., * be the eigenvectors ordered according to 
of the embedding. Matrix  provides a natural measure of 1    n
                                                                      *
 the vertex on the graph. The problem is solved by the general their eigenvalues with 1 having the smallest eigenvalue 
 eigendecomposition problem: 
                                                      1  (in fact zero). The image of Xi  embedded  into k 
                                                                            **       *
                   Ly     y                          dimensional space is given byY [21 ,...,k ] . 
                       T
Alternatively, we could use yy 1 as the constraint. Then     Table 1. The DGE algorithm 
the solution is achieved by solving Ly y .       The irreducibility of the Markov chain guarantees that the 
                                                 stationary distribution vector  exists. We here will build a 
Let e be a vector with all entry 1. It can be easily shown 
                                                 Markov chain with a primitive transition probability ma-
that e is an eigenvector with eigenvalue 0 for L . If the 
                                                 trix P . In general, for a directed graph, the matrix of transi-
transition matrix is primitive, e is the only eigenvector for 
                                                 tion probability P defined by puv(,) wuv (,)/ d () u is not 
    0 . The meaning of the first eigenvector is to map all                           O
data to a single point, which minimizes the optimization irreducible. We will use the so called teleport random walk 
target. To eliminate this trivial solution we put an addition [A. Langville and C. Meyer, 2004] on a general directed graph. 
constraint of orthogonality:                     The transition probability matrix is given by 
                       T                                             11
                argmin yLy                                PDW()(1)1     eTT       ee 
                   y                                            O    nn
                st..  yT  y 1                    where W  is the adjacent matrix of the directed graph,  is a 
                        T
                       ye0                       vector that i 1  if row i of W  is 0, and DO is the diagonal 
Thus the solution is given by the eigenvector of the smallest matrix of the out degree. Then P will be stochastic, irre-
non-zero eigenvalue. Generally, embedding the graph into ducible and primitive. This can be interpreted as a probabil-
  k                                              ity  of transiting to an adjacent vertex and a probability 
 Rk (k>1) is given by the n  matrix Yyy[...]1 k where 
                                                 1    of jumping to any point on the graph uniform ran-


                                            IJCAI-07
                                             2709 domly. For those vertices that don’t have any out link, just be seen that the eigenvector corresponding to the second 
 uniform randomly jump to any point on the graph. Such a largest eigenvalue of  is in fact the eigenvector v corre-
 setting can be viewed as adding a perturbation to the original sponding to the second smallest eigenvalue of I . 
 graph. The smaller the perturbation is the more accurate Note that we have such an equation  
 result we can get. So in practice we only need to set  to a TT1/2 1/2 T
                                                  yLy        L                   1/2
very small value. In this paper, we simply set  to be 0.01.               , y        
                                                 yyTTT
The stationary distribution vector then can be obtained by 
                          T    T                 Therefore, the cutting result is equal to embedding data into a 
solving an eigenvalue problem P  subject to a nor-
                                                 line by DGE, then using threshold 0 to cut the data. 
malized equationT e 1 . 
 The algorithm for embedding vertices on a directed graph 6 Experiments  
 into a vector space is summarized in Table1. 
                                                 In this section, experiments are designed to show the em-
 5  Relation with Previous Works                 bedding effect in both toy problems and real world data. 
                                                 Using DGE as a preprocess procedure, we also consider an 
 In [M. Belkin and P. Niyogi , 2002] the authors proposed the application of the proposed directed graph embedding algo-
 Laplacian Eigenmap algorithm for nonlinear dimensional rithm to a web page classification problem with comparisons 
 reduction. We can see if our algorithm is applied to an un- to a state-of-art algorithm. 
 directed graph we will get a similar solution to Laplacian 
 Eigenmap. In the case of undirected graph, we can define the Toy Problems 
 transition probability as puv(,) wuv (,)/ du , where wuv(,) We first test our algorithm on the toy data shown in Fig.1.  

 is the weight of the undirected edge (,)uv , du is the degree of The edge weights are set as binary values. Fig.2(a) shows the 
 vertex u . If the graph is connected then the stationary dis- result of embedding the directed graph into a plane. The red 
 tribution on vertex u  can be proved equal to dVolG/() , nodes and blue nodes in Fig.2(a) are corresponding to the 
                                     u           three nodes on the left and four nodes on the right in Fig.1 
 whereVol()G is the volume of the graph, thus    respectively. From the figure, we can see the locality prop-
                    22
    uuvuvupuv(,)( y y )  ( y y )  puv (,)        erty of the graph is well preserved, and the embedding result 
  uvuv,,               uv                        reflects subgraph structure of the original graph. 
           2
     ()(,)/()yywuvVolGuv
   uv,
 yyyDyVolGTT/() 

whereDdiagd(1 ,..., dn ) . Then the problem reduces to the 
Laplacian Eigenmap. 
In [D. Zhou et al., 2005] Zhou proposed a semi-supervised 
classification algorithm on a Directed Graph, by solving an                                    
optimization problem. The basic assumption is the smooth 
                                                          (a)                                                      (b) 
assumption that the class labels of the vertices on the directed 
graph should be similar if the vertices are closely related. The Figure 2. Embedding result of two toy problems on 2D space 
algorithm is to minimize a regularization risk between the In another experiment, a directed graph consisting of 60 
least square error and a smooth term. Consider the problem vertices is generated. There are three subgraphs, and each 
that the data in the same class is scattered and the decision consists of 20 vertices. Weights of the inner directed edges in 
boundary is complicated, and the smooth assumption does the subgraph are drawn uniformly from interval [0.25, 1]. 
not hold. Then the classification result may be hindered. Weights of directed edges between the subgraphs are drawn 
Another problem is that by using least square error the data uniformly from interval [0, 0.75]. By generating the edge 
far away from the decision boundary also contribute a large weights in such a manner, each subgraph is relatively impact. 
penalty in the optimization target. Thus considering the im- The graph is a full-connected directed graph. If only given 
balanced data, the side with more training data may have the graph without a prior knowledge of the data, we can 
more total energy, and the decision boundary is biased. In the hardly see the latent relation of the data. The embedding 
experiment section we will show a comparison between result by DGE in two dimensional space is shown in Fig.2(b). 
Zhou’s algorithm and our method used together with an We can see that tightly related nodes on the directed graph 
SVM classifier.                                  are clustered in the 2D Euclidean space. After embedding the 
In [D. Zhou et al., 2005] the author also proposed the directed data into a vector space, we can easily perceive the clustered 
 version of normalized cut algorithm. The solution is given by structure of the original graph, which gives us insight on the 
 the eigenvector corresponding to the second largest eigen- principal issues such as latent complexity of the directed 
 value of matrix ()1/ 2PP 1/ 2 1/ 2T 1/ 2 /2 .  It can graph. 


                                            IJCAI-07
                                             2710                                                                                              
          (a)                                               (b)                                              (c)                                               (d) 
 Figure 4. Classification Result: a) two-class problem, dimension=20; b) multi-class problem, dimension=20; c) multi-class in dif-
 ferent dimension spaces by nonlinear SVM; d) accuracy against dimension on a fixed (500 labeled samples) training set by linear 
                                            SVM 
                                                rithm (referred as Zhou) proposed in [D. Zhou et al., 2005]. 
Web Page Data                                   Here we use nu-SVM [B. Schölkopf and A. J. Smola, 2002], 
To address directed graph embedding task on real world data, a modified version of SVM, which is easy for model selec-
we test our method on the WebKB dataset. We consider a tion. Both linear and nonlinear SVM are tested. In the 
subset containing the pages from the three universities nonlinear setting, RBF kernel is used.  In all experiments, the 
Cornell, Texas and Wisconsin. We remove the isolated training data are randomly sampled from the data set. To 
pages, resulting in 847, 809 and 1227 pages respectively. We ensure that there is at least one training sample for each class, 
may assign a weight to each hyperlink according to the tex- we conduct the sampling again when there is no labeled 
tual content or the anchor text. However here we are only point for some class. The testing accuracies are averaged 
interested in how much we can obtain from link structure over 20 times’ experimental results. Different dimensional 
only and hence adopt the binary weight function. Fig.3 embedding spaces are also considered to study the dimen-
shows the embedding result of the WebKB data in three sionality of the embedded space. 
dimensional space. The red, blue and black nodes are cor- The comparing results of binary classification problem are 
responding to the web pages of three universities: Cornell shown in Fig.4(a). We consider the web pages of two Uni-
Texas and Wisconsin respectively. From the figure we can versities randomly selected from the WebKB data set. We 
see that the embedding results of web pages in each univer- first use DGE to embed the whole dataset into a 
sity are relatively impact, while those of web pages in dif- 20-dimensional space, then use SVM to do the classification 
ferent universities are well separated. This shows our method task. The parameter nu is set to 0.1 for both linear SVM and 
is effective in analyzing the link structure between different nonlinear SVM. The parameter of RBF kernel is set to be 
universities, where the inner links in one university are 38 for nonlinear SVM. The parameter for Zhou’s algo-
denser than that between universities.          rithm is set to be 0.9 as proposed in his paper. From the 
                                                figure, we can see that in all cases where the number of 
                                                training samples varies from 2 to 1000, DGE used together 
                                                with either liner or nonlinear SVM consistently achieves 
                                                better performance than Zhou’s algorithm. The reason might 
                                                be that Zhou’s method directly applies the least square risk to 
                                                the direct graph, which is convenient and suitable for re-
                                                gression problems, but not so efficient in some case of clas-
                                                sification problems, such as imbalanced data. The reason is 
                                                that the nodes far away from the decision boundary also 
                                                contribute large penalty for the shape of the decision 
      Figure 3. Embedding result of WebKB data  boundary.  After embedding the data into vector space, we 
                                                are able to analyze the decision boundary carefully. 
Application in Web Page Classification          Nonlinear SVM can show its advantage in such a situation. 
                                                Fig.4(b) show the result of the multi-class problem, in which 
Our method can be used in many applications, such as clas- each university is considered as a single class, and then 
sification, clustering, information retrieval. As an example, training data are randomly sampled. For SVM, we use 
we apply our algorithm to a web page classification task. one-against-one extension for multi-class problem. For 
Web pages of four universities Cornell, Texas, Washington Zhou’s algorithm, the multi-class setting in paper [D. Zhou 
and Wisconsin in WebKB dataset are used. Still the binary et al., 2004] is used. The parameter setting is the same as the 
edge weight setting is adopted. We first use DGE to embed binary class experiments. From the figure we can see that 
the vertices into certain Euclidean space, and then train an significant improvements are achieved by our method. 
SVM classifier to do the classification task. After that, we Zhou’s method is not very efficient in multi-class problem. 
compare the results with the state-of-art classification algo- Besides the reason we discussed above, another problem of 


                                           IJCAI-07
                                            2711