   Color Learning on a Mobile Robot:             Towards Full Autonomy under Changing
                                            Illumination

                                  Mohan Sridharan     and  Peter Stone
                                   University of Texas at Austin, USA
                            smohan@ece.utexas.edu, pstone@cs.utexas.edu


                    Abstract                          colored objects in the environment, but only when manually
                                                      given a sequence of actions to execute while learning (a cur-
    A central goal of robotics and AI is to be able to de- riculum) [19]. Separately, it has been shown that a robot can
    ploy an agent to act autonomously in the real world recognize illumination changes and switch among color maps
    over an extended period of time. It is commonly   at appropriate times, given a ﬁxed set of pre-trained color
    asserted that in order to do so, the agent must be maps [18]. The prior work was also limited to controlled en-
    able to learn to deal with unexpected environmental vironments with only solid-colored objects.
    conditions. However an ability to learn is not suf- This paper signiﬁcantly extends these results by enabling a
    ﬁcient. For true extended autonomy, an agent must robot i) to recognize when the illumination has changed sufﬁ-
    also be able to recognize when to abandon its cur- ciently to require a completely new color map rather than us-
    rent model in favor of learning a new one; and how ing one of the existing ones; and ii) to plan its own action se-
    to learn in its current situation. This paper presents quence for learning the new color map on-line. Furthermore,
    a fully implemented example of such autonomy in   we introduce a hybrid color-map representation that enables
    the context of color map learning on a vision-based the robot to learn in less controlled environments, including
    mobile robot for the purpose of image segmenta-   those with textured surfaces. All algorithms run in real-time
    tion. Past research established the ability of a robot on the physical robot enabling it to operate autonomously in
    to learn a color map in a single ﬁxed lighting con- an uncontrolled environment with changing illumination over
    dition when manually given a “curriculum,” an ac- an extended period of time.
    tion sequence designed to facilitate learning. This
    paper introduces algorithms that enable a robot to i)
    devise its own curriculum; and ii) recognize when 2   Problem Speciﬁcation
    the lighting conditions have changed sufﬁciently to Here, we formulate the problem and describe our solution.
    warrant learning a new color map.                 Section 2.1 presents the hybrid color-map representation used
                                                      for autonomous color learning. Section 2.2 describes our ap-
1  Motivation                                         proach to detecting signiﬁcant illumination changes.
Mobile robotic systems have recently been used in ﬁelds as 2.1 What to learn: Color Model
diverse as medicine, rescue, and surveillance [1; 10].One
key enabler to such applications has been the development of To be able to recognize objects and operate in a color-coded
powerful sensors such as color cameras and lasers. However, world, a robot typically needs to recognize a discrete number
with these rich sensors has come the need for extensive sensor of colors (ω ∈ [0,N − 1]). A complete mapping identiﬁes a
calibration, often performed manually, and usually repeated color label for each point in the color space:
whenever environmental conditions change signiﬁcantly.  ∀p, q, r ∈ [0, 255], {C1,p,C2,q,C3,r} → ω|ω∈[0,N−1] (1)
  Here, we focus on the visual sensor (camera), arguably the
richest source of sensory information. One important subtask where C1,C2,C3 are the color channels (e.g. RGB, YCbCr),
of visual processing is color segmentation: mapping each im- with the corresponding values ranging from 0 − 255.
age pixel to a color label. Though signiﬁcant advances have We start out modeling each color as a three-dimensional
been made in this ﬁeld [3; 6], most of the algorithms are com- (3D) Gaussian with mutually independent color channels.
putationally expensive to implement on a mobile robot and/or Using empirical data and the statistical technique of boot-
involve a time consuming off-line preprocessing phase. Fur- strapping [5], we determined that this representation closely
thermore, the resulting segmentation is typically quite sensi- approximates reality. The Gaussian model simpliﬁes calcula-
tive to illumination variations. A change in illumination could tions and stores just the mean and variance as the statistics
require a repetition of the entire training phase.    for each color, thereby reducing the memory requirements
  Past research established that a robot can efﬁciently train and making the learning process feasible to execute on mobile
its own color map based on knowledge of the locations of robots with constrained resources.

                                                IJCAI-07
                                                  2212  The aprioriprobability density functions (color ω ∈ 2.2  When to learn: Detecting illumination changes
[0,N − 1]) are then given by:
                                                    To detect signiﬁcant changes in illumination, we need a
                    1             1 3     −      2   mechanism for representing illumination conditions and for
 (       | ) ∼ √            · exp −      ci  μCi
p c1,c2,c3 ω       3                                 differentiating between them.
                2                 2        σCi
                 π   i=1 σCi        i=1                 We hypothesized that images from the same lighting condi-
                                                (2)   tions would have measurably similar distributions of pixels in
        ∈ [     =0        = 255]                      color space. The original image is in the YCbCr format, with
where, ci Cimin    ,Cimax       represents the value at values ranging from [0-255] along each dimension. To reduce
a pixel along a color channel Ci while μCi and σCi represent storage, but still retain the useful information, we transformed
the corresponding means and standard deviations.      the image to the normalized RGB space, (r, g, b):
  Assuming equal priors (P (ω)=1/N, ∀ω ∈ [0,N − 1]),           R+1            G+1             B+1
                                                        r =          ,g=             ,b=              (5)
each color’s a posteriori probability is then given by:     R+G+B+3         R+G+B+3         R+G+B+3
             ( |       ) ∝  (       | )
            p ω c1,c2,c3  p c1,c2,c3 ω          (3)   Since r + g + b =1, any two of the three features are a
                                                      sufﬁcient statistic for the pixel values. We represent a partic-
The Gaussian model for color distributions, as described in ular illumination condition with a set of distributions in (r, g)
our previous work [19], performs well inside the lab. In addi- space, quantized into N bins in each dimension, correspond-
tion, it generalizes well with limited samples when the color ing to several images captured by the robot.
distributions are actually unimodal; it is able to handle minor Next, we need a well-deﬁned measure capable of detect-
illumination changes. However, in settings outside the lab, ing the correlation between discrete distributions. Based on
factors such as shadows and illumination variations cause the experimental validation, we use KL-divergence,anentropy-
color distributions to be multi-modal; the robot is now unable based measure. For two distributions A and B in the 2D (r, g)
to model colors properly using Gaussians.             space, N being the number of bins along each dimension:
  In order to extend the previous work to less controlled                   N−1 N−1
                                                                                            Bi,j
settings, we propose a hybrid color representation that uses  KL(A, B)=−            (Ai,j · ln  )     (6)
Gaussians and color histograms. Histograms provide an ex-                    i=0 j=0        Ai,j
cellent alternative when colors have multi-modal distribu- The more similar two distributions are, the smaller is the KL-
    [  ]
tions 20 . Here, the possible color values (0–255 along each divergence between them. Since KL-divergence is a function
channel) are discretized into bins that store the count of pixels of the log of the observed color distributions, it is reasonably
that map into that bin. A 3D histogram can be normalized to robust to large peaks in the observed color distributions and is
provide the probability density function:             hence less affected by images with large amounts of a single
                         Histω(b1,b2,b3)              color. The lack of symmetry in KL-divergence is eliminated
          p(c1,c2,c3|ω) ≡                       (4)   using the Resistor-Average KL-divergence (RA-KLD) [8].
                         SumHistV alsω                  Given a set of distributions corresponding to M different il-
                                                                                                [  ]
where b1, b2, b3 represent the histogram bin indices cor- lumination conditions, we have previously shown 18 that it
responding to the color channel values c1, c2, c3,and is possible to effectively classify the distribution correspond-
SumHistV alsω  is the sum of the values in all the bins of ing to a test image into one of the illumination classes. A ma-
the histogram for color ω.Thea posteriori probabilities are jor limitation was that we had to know the illumination con-
thengivenbyEquation3.                                 ditions in advance and also had to provide manually trained
  Unfortunately, histograms do not generalize well with lim- color maps for each illumination. Here, we make a signif-
ited training data, especially for samples not observed in the icant extension in that we do not need to know the different
training set, such as with minor illumination changes. Re- illumination conditions ahead of time.
source constraints prevent the implementation of operations For every illumination condition i, in addition to a set of
                                                      (r, g) distributions (rgsamp[i]), we calculate the RA-KL dis-
more sophisticated than smoothing. Also, histograms require                    (  )
more storage, wasteful for colors that can be modeled as tances between every pair of r, g distributions to get a dis-
Gaussians. We combine the two representations such that tribution of distances, (Di), which we model as a Gaussian.
they complement each other: colors for which a 3D Gaus- When the illumination changes signiﬁcantly, the average RA-
                                                      KL distance between a test (r, g) distribution and rgsamp[i]
sian is not a good ﬁt are modeled using 3D histograms.The                           95%
goodness-of-ﬁt decision is made online, for each color. maps to a point well outside the range of the intra-
                                                      illumination distances (Di). This feature is used as a measure
  Samples for which a 3D Gaussian is a bad ﬁt can still be
                                                      of detecting a change in illumination.
modeled analytically using other distributions (e.g. mixture
of Gaussians, Weibull) through methods such as Expectation-
Maximization [4]. But most of these methods involve param- 3 Algorithms: When, What, How to Learn
eter estimation schemes that are computationally expensive Our algorithms for color learning and adaptation to illumina-
to perform on mobile robots. Hence, we use a hybrid repre- tion change are summarized in Algorithm 1 and Algorithm 2.
sentation with Gaussians and histograms that works well and Algorithm 1 enables the robot to decide when to learn.The
requires inexpensive computation. In addition, the robot au- robot ﬁrst learns the color map for the current illumination by
tomatically generates the curriculum (action sequence) based generating a curriculum using the world model, as described
on the object conﬁguration, as described in Section 3. in Algorithm 2. Next, it represents this illumination condition

                                                IJCAI-07
                                                  2213Algorithm 1 Adapting to Illumination Change – When to Algorithm 2 Autonomous Color Learning – How to learn?
learn?                                                Require: Known initial pose and color-coded model of the
Require: For each illumination i ∈ [0,M − 1], color map   robot’s world - objects at known positions. These can
   and distribution of RA-KLD distances Di.               change between trials.
 1: Begin: M =0, current = M.                         Require: Empty Color Map; List of colors to be learned.
 2: Generate curriculum and learn all colors - Algorithm 2. Require: Arrays of colored regions, rectangular shapes in
 3: Generate rgsamp[current][], N (r, g) space distribu-  3D; A list for each color, consisting of the properties
   tions, and distribution of RA-KLD distances, Dcurrent. (size, shape) of the regions of that color.
 4: Save color map and image statistics, M = M +1.    Require: Ability to approximately navigate to a target pose
 5: if currentT ime − testT ime ≥ timeth then             (x, y, θ).
 6:  rgtest = sample (r, g) test distribution.         1: i =0,N  = MaxColors
 7:  for i =0to M  − 1do                              2: Timest =  CurrT ime, Time[] —  the maximum time
               [ ]= 1            (            [ ][ ])
 8:    dAvgtest i   N   j KLDist  rgtest,rgsamp i j       allowed to learn each color.
 9:  end for                                           3: while i<Ndo
10:  if dAvgtest[current] lies within the threshold range 4: Color = BestColorToLearn( i );
     of Dcurrent then                                  5:   TargetPose=  BestTargetPose( Color );
11:    Continue with current color map.                6:   Motion = RequiredMotion( TargetPose)
12:  else if dAvgtest[i] lies within the range of Di,i = 7: Perform Motion {Monitored using visual input and
     current then                                           localization}
13:    Use corresponding color map, current = i.       8:   if TargetRegionFound( Color ) then
14:  else if ∀i ∈ [0,M − 1],dAvgtest[i] lies outside the 9:   Collect samples from  the  candidate region,
     range of Di then                                         Observed[][3].
15:    Re-learn color map autonomously: Algorithm 2.  10:     if PossibleGaussianFit(Observed) then
16:    Save (r, g) distributions for new illumination. 11:      LearnGaussParams( Colors[i] )
17:    Transition to the new color map for subsequent op- 12:   Learn Mean and Variance from samples
       erations.                                      13:     else { 3D Gaussian not a good ﬁt to samples }
18:    current = M, M  =  M +1.                       14:       LearnHistVals( Colors[i] )
19:  end if                                           15:       Update the color’s 3D histogram using the sam-
20:  testT ime = currentT ime.                                  ples
21: end if                                            16:     end if
                                                      17:     UpdateColorMap()
                                     (   )            18:     if !Valid( Color ) then
by collecting sample image distributions in r, g and com- 19:   RemoveFromMap(  Color )
puting the distribution of RA-KL distances, DcurrIll.
                    =05                               20:     end if
  Periodically (timeth . ), the robot generates a test dis- 21: else
tribution, rgtest, and computes its average distance to each set
                                   [ ]          [ ]   22:     Rotate at target position.
of previously stored distributions, rgsamp i .IfdAvgtest i 23: end if
lies within the threshold range (95%) of the corresponding
                                                      24:   if  CurrT ime  −  Timest  ≥   Time[Color]  or
Di, the robot transitions to the corresponding illumination
                                                            RotationAngle ≥ Angth  then
condition. But, if it lies outside the threshold range of all 25: i = i +1
known distribution of distances, the robot learns a new color
                                                      26:     Timest = CurrT ime
map and collects image statistics, which are used in subse- 27: end if
quent comparisons. Changing the threshold changes the res- 28: end while
olution at which the illumination changes are detected but the 29: Write out the color statistics and the Color Map.
robot is able to handle minor illumination changes using the
color map corresponding to the closest illumination condi-
tion (see Section 4.2). With transition thresholds to ensure
that a change in illumination is accepted iff it occurs over a color to facilitate color learning outside the lab: it decides
few frames, it also smoothly transitions between the learned what to learn. It also automatically determines how to learn,
maps. The algorithm requires no manual supervision.   i.e. it generates the curriculum for learning colors, for any
  Next, we brieﬂy describe the planned color learning algo- robot starting pose and object conﬁguration.
rithm, Algorithm 2, used in lines 2 and 15 of Algorithm 1. The robot starts off at a known location without any color
Our previous algorithm [19] (lines 11, 12, 17 − 20)hadthe knowledge. It has a list of colors to be learned and a list of
robot move along a prespeciﬁed motion sequence, and model object descriptions corresponding to each color (size, shape,
each color as a 3D Gaussian. But, outside the controlled lab location of regions). Though this approach does require some
setting, some color distributions are multi-modal and cannot human input, in many applications, particularly when object
be modeled effectively as Gaussians. The current algorithm locations change less frequently than illumination, it is more
signiﬁcantly extends the previous approach in two ways. It efﬁcient than hand-labeling several images. To generate the
automatically chooses between two representations for each curriculum, the robot has to decide the order in which the col-

                                                IJCAI-07
                                                  2214ors are to be learned and the best candidate object for learning Algorithm 3 PossibleGaussianFit(), line 10 Algorithm 2 –
a particular color. The algorithm currently makes these deci- What to learn?
sions greedily and heuristically, i.e. it makes these choices 1: Determine Maximum-likelihood estimate of Gaussian
one step at a time without actually planning for the subse- parameters from samples, Observed.
quent steps. The aim is to get to a large enough target object 2: Draw N samples from Gaussian – Estimated,N=size
while moving as little as possible, especially when not many of Observed.
colors are known. The robot computes three weights for each 3: Dist = KLDist(Observed, Estimated).
object-color combination (c, i):                       4: Mix Observed and Estimated – Data, 2N items.
                                                       5: for i =1to NumTrials  do
 w1 = fd( d(c, i)),w2 = fs( s(c, i)),w3 = fu( o(c, i)) 6:   Sample N items with replacement from Data – Set1,
                                                (7)         remaining items – Set2.
where the functions d(c, i), s(c, i) and o(c, i) represent the 7: Disti = KLDist(Set1,Set2)
distance, size and object description for each color-object 8: end for
combination. Function fd( d(c, i))assigns larger weights to 9: Goodness-of-ﬁt by p-value:whereDist lies in the distri-
smaller distances, fs( s(c, i))assigns larger weights to larger bution of Disti.
candidate objects, and fu( o(c, i))assigns larger weights iff
the object i can be used to learn color c without having to wait
for any other color to be learned or object i consists of color a color map and/or the action sequence each time the envi-
c and other colors that have already been learned.    ronment or the illumination changes, we now just provide the
                                                      positions of objects in the robot’s world and have it plan its
  The BestColorToLearn() (line 4) is then given by:
                                                      curriculum and learn colors autonomously. The adaptation to
  arg max     max   ( fd( d(c, i))
     c∈[0,9] i∈[0,Nc−1]                               illumination changes makes the entire process autonomous.
                                             
                                                      A video of the robot learning colors can be seen online:
                   + fs( d(c, i))+fu( o(c, i))) (8)
                                                      www.cs.utexas.edu/∼AustinVilla/?p=research/auto vis.
where the robot parses through the different objects available
for each color (Nc) and calculates the weights. Once a color 4 Experiments
is chosen, the robot determines the best target for the color, We ﬁrst provide a brief overview of the robotic platform used,
using the minimum motion and maximum size constraints: followed by the experimental results.
  arg  max     fd( d(c, i))
     i∈[0,N −1]
          c                                          4.1  Experimental Platform
                    + fs( d(c, i))+fu( o(c, i)) (9)   The SONY  ERS-7 Aibo is a four legged robot whose primary
                                                      sensor is a CMOS camera located at the tip of its nose, with
For a chosen color, the best candidate object is the one with a limited ﬁeld-of-view (56.9o horz., 45.2o vert.). The im-
the maximum weight for the given heuristic functions. The ages, captured in the YCbCr format at 30Hz with a resolution
robot chooses the BestTargetPose() (line 5) to learn color of 208 × 160 pixels, possess common defects such as noise
from this object and moves there (lines 6,7). It searches for and distortion. The robot has 20 degrees-of-freedom, three in
candidate image regions satisfying a set of constraints based each leg, three in its head, and a total of ﬁve in its tail, mouth,
on current robot location and target object description. If a and ears. It has noisy IR sensors and wireless LAN for inter-
suitable image region is found (TargetRegionFound() – line robot communication. The legged as opposed to wheeled lo-
8), the pixels in the region are used as samples, Observed,to comotion results in jerky camera motion. All processing for
verify goodness-of-ﬁt with a 3D Gaussian (line 10). The test vision, localization, motion and action selection is performed
is done using bootstrapping [5] using KL-divergence as the on-board using a 576MHz processor.
distance measure, as described in Algorithm 3.          One major application domain for the Aibos is the
  If the samples generate a good Gaussian ﬁt, they are used RoboCup Legged League [16], a research initiative in which
to determine the mean and variance of the color distribution teams of four robots play a competitive game of soccer on an
(LearnGaussParams() – line 11). If not, they are used to pop- indoor ﬁeld ≈ 4m × 6m. But applications on Aibos and mo-
ulate a 3D histogram (LearnHistVals() – line 14). The learned bile robots with cameras typically involve an initial calibra-
distributions are used to generate the Color Map, the mapping tion phase, where the color map is produced by hand-labeling
from the pixel values to color labels (line 17). The robot uses images over a period of an hour or more (Section 5). Our
the map to segment subsequent images and ﬁnd objects. The approach has the robot autonomously learning colors in less
objects help the robot localize to positions suitable for learn- than ﬁve minutes and adapting to illumination changes.
ing other colors, and to validate the learned colors and remove
spurious samples (lines 18,19).                       4.2  Experimental Results
  To account for slippage and motion model errors, if a suit- We tested our algorithm’s ability to answer three main ques-
able image region is not found, the robot turns in place to tions: When to learn - the ability to detect illumination
ﬁnd it. If it has rotated in place for more than a threshold changes, How to learn - the ability to plan the action se-
                 o
angle (Angth = 360 ) and/or has spent more than a thresh- quence to learn the colors, and How good is the learning -
old amount of time on a color (Time[Color] ≈ 20sec), it the segmentation and localization accuracy in comparison to
transitions to the next color in the list. Instead of providing the standard human-supervised scheme.

                                                IJCAI-07
                                                  2215When to Learn?                                        get location and it is unable to learn the colors. The motion
First, we tested the ability to accurately detect changes in il- planning works well and we are working on making the algo-
lumination. The robot learned colors and (r, g) distributions rithm more robust to such failure conditions. The localization
corresponding to an illumination condition and then moved accuracy with the learned map is comparable to that with a
around in its environment chasing a ball. We changed the hand-labeled color map (≈ 8cm, 10cm, 6deg in comparison
lighting by controlling the intensity of speciﬁc lamps and the to 6cm, 8cm, 4deg in X, Y ,andθ).
robot identiﬁed signiﬁcant illumination changes.      How good is the learning?
    (%)     Change   Changec     Table 1 presents re- To test the accuracy of learning under different illuminations,
  Change     97.1       2.9      sults averaged over  we had the robot learn colors under controlled lab condi-
         c                                            tions and in indoor corridors outside the lab, where the over-
  Change      3.6      96.4      1000 trials with the
                                 rows and  columns    head ﬂuorescent lamps provided non-uniform illumination
Table 1: Illumination change detection: representing the (between 700-1000lux) and some of the colors (ﬂoor, wall
few errors in 1000 trials.       ground truth and ob- etc) could not be modeled well with 3D Gaussians. We ob-
served values respectively. There are very few false positives served that the robot automatically selected the Gaussian or
or false negatives. The errors due to highlights and shadows Histogram model for each color and successfully learned all
are removed by not accepting a change in illumination unless the colors.
it is observed over a few consecutive frames.
                                                      Table 4 shows the     Conﬁg     Localization Error
  To test the ability to transition between known illumina- localization accu-
tions, the robot learned color maps and statistics for three con-                   Dist (cm)   θ (deg)
                                                      racies under  two             9 8 ± 4 8  6 1 ± 4 7
ditions: Bright(1600lux), Dark(450lux), Interim(1000lux).                    Lab     .     .    .    .
                                                      different illumina-   Indoor  11.7 ± 4.4 7.2 ± 4.5
The intensity of the   Illum.  Transition Accuracy    tion conditions (lab,
overhead lamps was             Correct (%) Errors     indoor    corridor) Table 4: Localization accuracy: compa-
changed to one of the  Bright     97.3       4        based on the learned rable to that with a hand-labeled map.
three conditions once                                 color maps. We had the robot walk to 14 different points
     ≈  10             Dark       100        0
every      sec. Ta-   Interim     96.1       6        and averaged the results over 15 trials. The differences
ble 2 shows results av-                               were not statistically signiﬁcant. The corresponding seg-
eraged over ≈  150   Table 2: Illumination transition accu- mentation accuracies were 95.4% and 94.3% respectively,
trials each. The few racy: few errors in ≈ 150 trials. calculated over 15-20 images, as against the 97.3% and
false transitions, due to shadows or highlights, are quickly 97.6% obtained with a hand-labeled color map (differences
corrected in the subsequent tests. When tested in conditions not statistically signiﬁcant). The learned maps are as good as
in between the known ones, the robot ﬁnds the closest illumi- the hand-labeled maps for object recognition and high-level
nation condition and is able to work in the entire range. task competence. But, our technique takes 5 minutes of
                                                      robot time instead of an hour or more of human effort.
How to Learn?                                         Sample images under different testing conditions and a video
In previous work [19], ﬁxed object locations resulted in a sin- of the robot localizing in a corridor can be seen online:
gle curriculum to learn colors. To test the robot’s ability to www.cs.utexas.edu/∼AustinVilla/?p=research/gen color.
generate curricula for different object and robot starting po- To summarize, our algorithm enables the robot to plan its
sitions, we invited a group of seven graduate students with motion sequence to learn colors autonomously for any given
experience working with the Aibos to suggest challenging object conﬁguration. It is able to detect and adapt to illumi-
conﬁgurations. It is difﬁcult to deﬁne challenging situations nation changes without manual training.
ahead of time but some examples that came up include hav-
ing the robot move a large distance in the initial stages of 5 Related Work
the color learning process, and to put the target objects close
                                                      Color segmentation is a well-researched ﬁeld in computer vi-
to each other, making it difﬁcult to distinguish between them.
                                                      sion with several effective algorithms [3; 6]. Attempts to
The success ratio and the corresponding localization accuracy
                                                      learn colors or make them robust to illumination changes have
over15trialsareshowninTable3.
                                                      produced reasonable success [13; 14]. But they are compu-
A trial is a success if all colors Conﬁg Success (%)  tationally expensive to perform on mobile robots which typi-
are learned successfully. The  Worst      70          cally have constrained resources.
localization error is the differ- Best    100           On Aibos, the standard approaches for creating map-
                                                                                                   [
ence between the robot’s esti- Avg     90 ± 10.7      pings from the YCbCr values to the color labels 7; 11;
mate and the actual target posi-                      12] require hand-labeling of images (≈ 30) over an hour or
tions, measured by a human us- Table 3: Planning Accu- more. There have been a few attempts to automatically learn
ing a tape measure. We observe racy in challenging conﬁgu- the color map on mobile robots. In one approach, closed ﬁg-
that the robot is mostly able to rations.             ures are constructed corresponding to known environmental
plan a suitable motion sequence and learn colors. In the cases features and the color information from these regions is used
where it fails, the main problem is that the robot has to move to build color classiﬁers [2]. The algorithm is time consum-
long distances with very little color knowledge. This, cou- ing even with the use of ofﬂine processing and requires hu-
pled with slippage, puts it in places far away from the tar- man supervision. In another approach, three layers of color

                                                IJCAI-07
                                                  2216