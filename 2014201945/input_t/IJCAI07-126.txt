             Feature Selection and Kernel Design via Linear Programming

                             Glenn Fung, Romer Rosales, R. Bharat Rao
               Siemens Medical Solutions, 51 Valley Stream Parkway, Malvern, PA, USA.
           glenn.fung@siemens.com, romer.rosales@siemens.com,       bharat.rao@siemens.com


                    Abstract                          may not be appropriate for another. This task dependency has
                                                      been noted in a number of approaches that seek to build these
    The deﬁnition of object (e.g., data point) similar- functions for a particular classiﬁcation or regression model
    ity is critical to the performance of many machine given some training data (e.g., [Cohen et al., 1998; Xing et
    learning algorithms, both in terms of accuracy and al., 2002; Schultz and Joachims, 2003; Lanckriet et al., 2004;
    computational efﬁciency. However, it is often the Athitsos et al., 2004]).
    case that a similarity function is unknown or chosen
                                                        In this paper, we are interested in automatically ﬁnd-
    by hand. This paper introduces a formulation that
                                                      ing good similarity functions after some basic information
    given relative similarity comparisons among triples
                                                      about the task of interest has been provided (e.g., by an
    of points of the form object i is more like object j
                                                      expert user). Unlike approaches that rely on class labels
    than object k, it constructs a kernel function that
                                                      (e.g., [Lanckriet et al., 2004]) or on building sets of points
    preserves the given relationships. Our approach is
                                                      that are globally similar (or dissimilar) [Xing et al., 2002;
    based on learning a kernel that is a combination of
                                                      Wagstaff et al., 2001], here we explore a simpler-to-specify
    functions taken from a set of base functions (these
                                                      form of user supervision: that provided by statements like ob-
    could be kernels as well). The formulation is based
                                                      ject i is more similar to object j than to object k1. We remark
    on deﬁning an optimization problem that can be
                                                      that we are not only interested on building functions that are
    solved using linear programming instead of a semi-
                                                      consistent with this information provided by the user, but we
    deﬁnite program usually required for kernel learn-
                                                      are also interested in obtaining functions that are efﬁcient to
    ing. We show how to construct a convex problem
                                                      compute. Thus, in addition we focus on functions that can
    from the given set of similarity comparisons and
                                                      be evaluated by looking only at part of the features or input
    then arrive to a linear programming formulation by
                                                      dimensions; therefore, implying some form of feature selec-
    employing a subset of the positive deﬁnite matrices.
                                                      tion.
    We extend this formulation to consider represen-
                                                        More formally, let us represent the objects of interest
    tation/evaluation efﬁciency based on formulating a                                      D
                                                      as points xk in a D-dimensional space  , with k =
    novel form of feature selection using kernels (that
                                                      {1, 2, ..., N}. We would like to obtain a function K :
    is not much more expensive to solve). Using pub-    D     D
                                                         ×     →that satisﬁes the above similarity com-
    licly available data, we experimentally demonstrate
                                                      parisons and that in addition can be well approximated by
    how the formulation introduced in this paper shows K˜ : d ×d →                 d<D
    excellent performance in practice by comparing it                   which only uses     components of
                                                      xk. For this we will rely on (Mercer) Kernel representations
    with a baseline method and a related state-of-the art                                     K
    approach, in addition of being much more efﬁcient [Cristianini and Shawe-Taylor, 2000] and deﬁne (similarly
                                                         K˜           K(x, y)=     α K  (x, y)     α ∈
    computationally.                                  for  )asakernel             j j  j     where      ;
                                                      thus, K is a mixture of kernel functions2. Throughout this pa-
1  Introduction and Related Work                      per, we will work with this representation of similarity func-
                                                      tions, deﬁne convex problems for obtaining K, and later ex-
The deﬁnition of a distance or a similarity function over the tend this idea to focus on K˜ . Some of the motivation for the
input or data space is fundamental in the performance of ma- framework in this paper is related to earlier work in metric
chine learning algorithms, both in terms of accuracy and ef- learning [Rosales and Fung, 2006]; however, here the focus
ﬁciency. This can be easily veriﬁed by looking at the role is on a different problem, kernel design.
that the distance or similarity plays in common algorithms
(e.g., k-means, nearest neighbors, support vector machines or 1Example objects of interest include database records, user opin-
any other kernel method, etc.). However, since the concept ions, product characteristics, etc.
of similarity depends on the task of interest (objects can be 2Note that K is any afﬁne combination of kernels not just a con-
similar or dissimilar in many ways depending on the appli- vex combination. Also, we are not restricting Kj to be itself a ker-
cation), a similarity function that is appropriate for one task nel.

                                                IJCAI-07
                                                   786  In this paper, we ﬁrst show how relative constraints, involv- et al., 2004; Cohen et al., 1998; Schultz and Joachims, 2003;
ing triples of points of the form, K(xi, xj) >K(xi, xk) can Rosales and Fung, 2006], where these relationships are de-
be used to learn K. One can think of this ﬁrst formulation as ﬁned with respect to distances or rankings. The use of rela-
parallel to other approaches for learning K that rely on super- tive similarity constraints of this form for kernel design offers
vised learning with class labels (and, unlike the one presented different challenges. Note that we are not interested in pre-
here, are classiﬁcation-algorithm dependent). These formu- serving absolute similarities, which are in general much more
lations involve solving a semideﬁnite programming problem difﬁcult to obtain (absolute similarities imply relative similar-
(SDP) [Graepel, 2002; Lanckriet et al., 2004],butweshow ities, but the converse is not true).
how a different Linear Programming formulation, introduced One important observation for kernel design is that the ker-
in this paper, is also sufﬁcient in practice and much more nel conditions implied by these relationships are linear rela-
efﬁcient computationally (e.g., it can be use to solve much tionships among individual kernel entries in the kernel matrix
larger problems, faster). In addition, we extend this formu- (deﬁned over the points of interest) and can be expressed as
lation to consider the issue of representation/evaluation efﬁ- linear constraints. In addition to this, unlike any of the meth-
ciency based on a form of feature selection. This formulation ods referred to above, by restricting the kernel family to have
leads to a linear programming approach for solving the ker- only kernels that depend in one original feature at the time,
nel design problem that also optimizes for succinct feature we are able to learn kernels that depend on a minimal set of
representations.                                      the original features. This can be seen as implicitly perform-
                                                      ing feature selection with respect to the original data space.
1.1  Kernel Design                                    In combination with the beneﬁts of a new linear program-
In recent years the idea of kernel design for learning algo- ming approach, the formulation proposed in this paper has a
rithms has received considerable attention in the machine unique set of attributes with signiﬁcant advantages over re-
learning community. Traditionally, an appropriate kernel is cent approaches for kernel design.
found by choosing a parametric family of well-known ker-
nels, e.g., Gaussian or polynomial, and then learning a gen- 1.3 Notation and background
erally sub-optimal set of parameters by a tuning procedure. In the following, vectors will be assumed to be column vec-
This procedure, although effective in most cases, suffers from tors unless transposed to a row vector by a superscript .
important drawbacks: (1) calculating kernel matrices for a The scalar (inner) product of two vectors x and y in the d-
large range of parameters may become computationally pro- dimensional real space d will be denoted by xy.The2-
hibitive, especially when the training data is large and (2) very
                                                      norm and 1-norm of x will be denoted by x2 and x1
little (to none) prior knowledge about the desired similarity respectively. A column vector of ones of arbitrary dimension
can be easily incorporated into the kernel learning procedure.
                                                      will be denoted by e, and one of zeros will be denoted by0.A
                              [
  More recently, several authors Bennett et al., 2002; kernel matrix for a given set of points will be denoted by K,
Lanckriet et al., 2004; Fung et al., 2004] have considered the
                                                      individual components by Kij , and kernel functions by K()
use of a linear combination of kernels that belong to a fam- or simply K.
ily or superset of different kernel functions and parameters;
this transforms the problem of choosing a kernel model into
one of ﬁnding an optimal linear combination of the members 2 A Linear programming formulation for
of the kernel family. Using this approach there is no need to kernel design
predeﬁne a kernel; instead, a ﬁnal kernel is constructed ac-
cording to the speciﬁc classiﬁcation problem to be solved. In 2.1 Using a linear combination of Kernels
                                            3                                                      D
this paper we also use a mixture model representation ;how- Let us say we are given a set of points xk ∈ with
ever, our approach does not depend on and is not attached k = {1, ..., N} for which an appropriate similarity function is
to any speciﬁc machine learning algorithm (for classiﬁca- unknown or expensive to compute. In addition we are given
tion, regression, inference, etc). Instead, inspired by the fact information about a few relative similarity comparisons. For-
that most kernels can be seen as similarity functions, we rely mally, we are given a set T = {(i, j, k)|K(xi, xj ) >
on explicit conditions on similarity values among subset of K(xi, xk)} for some kernel function K.ThekernelK is not
points in the original training dataset; this is, our formulation known explicitly, instead a user may only be able to provide
allows the user to explicitly ask for conditions that have to these similarity relationships sparsely or by example. We are
be satisﬁed or reﬂected in the desired kernel function to be interested in ﬁnding a kernel function K() that satisﬁes these
learned.                                              relationships.
1.2  Relative Similarity Constraints                    For the rest of the paper, let us suppose that instead of
                                                      the kernel K being deﬁned by a single kernel mapping (e.g.,
As explained above, we concentrate on examples of proxim- Gaussian, polynomial, etc.), the kernel K is instead com-
                                                i
ity comparisons among triples of objects of the type object is posed of a linear combination of kernel functions Kj,j =
more like object j than object k. In choosing this type of rel- {1,...,k},asbelow:
ative relationships, we were inspired primarily by [Athitsos
                                                                              k
  3                                                                  α
   But note that the elements of the mixture could be functions that K (x, y)=   αjKj(x, y).          (1)
are not kernels.                                                              j=1

                                                IJCAI-07
                                                   787                                                                   
                                                              α
                                              Ω=            K   =         αiKi(xi, xj)   S
  As pointed out in [Lanckriet et al., 2004],theset   where   ij     Ki∈Ω            and   is a set of train-
{K1(x, y),...,Kk(x, y)} can be seen as a predeﬁned set of ing points (e.g., those points used to deﬁne the triples in T ).
                                                                                            +
initial guesses of the kernel matrix. Note that the set Ω could Finally, deﬁning auxiliary variables rij ∈ , γ1 ≥ 0,and
contain very different kernel matrix models, all with different γ2 ≥ 0, formulation (3) can be rewritten as a linear program-
parameter values. The goal is then to ﬁnd a kernel function ming problem in the following way:
Kα  that satisﬁes relative kernel value constraints (speciﬁed                                  
                                                                           T                          α
                T                                                 min         t + γ1  si  +   γ2   K
by the user as a set ). A general formulation for achieving      α,,s,r   t=1        i            i  ii
this is given by:                                                   s.t
                                                                             α     α
        min         + γh(Kα)                          ∀t =(i, j, k) ∈T     Kij − Kik + t ≥   1
           α,    t t                                     ∀i = j|i, j ∈S      −r   ≤ Kα   ≤   r
           s.t.                                                                  ij    ij      ij
                  α                   α                        ∀i, j ∈S     Kα −       r   ≥   0
  ∀(i, j, k) ∈T K  (xi, xj )+t >K(xi,     xk)  (2)                           ii    j=i ij
            ∀tt                ≥   0                     ∀j ∈{1, ..., k}−sj         ≤ αj  ≤   sj
                           Kα       0,                              ∀tt                   ≥   0,
                                                                                                      (4)
                                                                 k
where t are slacks variables, t indexes T , the function where s ∈ (same dimensionality as α)andsj ≥ 0.Note
    α                                    α
h(K  ) is a regularizer on the kernel matrix K (or alter- that to simplify notation we have overloaded the variables
                          α
natively the kernel function K ) for capacity control, and i, j, k (but their meaning should be clear from the context).
the parameter γ ≥ 0 controls the trade-off between con- In order to better understand the motivation for formulation
straint satisfaction and regularization strength (usually ob- (4) it is important to note that:
tained by tuning). Note that this formulation seems sufﬁ-             
                                                       (i) Minimizing    Kα   is equivalent to minimizing
cient, we can optimize the set of values αi in order to obtain a       i  ii 
                                          α                              α     k
positive semideﬁnite (PSD) linear combination K (x, y)=      j  rij since Kii ≥ j=1 rij , ∀i.
k                                                          j=i               j=i
  j=1 αjKj(x, y) suitable for the speciﬁc task at hand. This                            
                                                       (ii) Since we are implicitly minimizing j rij , at the op-
formulation, however requires solving a relatively expensive                              j=i
semideﬁnite program (SDP). Enforcing αi ≥ 0, ∀i, [Fung et timal solution {α∗,r∗,∗,s∗} to problem (4), we have
al., 2004; Lanckriet et al., 2004] and moreover restricting all                
                                       α                  that:
the kernels in the set Ω to be PSD results in K being PSD.             ∗     α∗ 
                                                                  0 ≥ rij = Kij  , ∀(i, j ∈S,i= j)
However this strongly limits the space of attainable kernel
        Kα
functions  .                                          (iii) Combining (i)and(ii) we obtain:
  Instead of using these restrictions, we explore an alterna-                         
                                                                         α∗       ∗          α∗
tive, general deﬁnition that allows us to consider any kernel       ∀(i)Kii ≥    rij =    |Kij |
function in Ω and any αk ∈without compromising compu-                         j        j
tational efﬁciency (e.g., without requiring us to solve a SDP).
                                                                             ∗
For this we will explicitly focus on a well characterized sub- which implies that Kα is diagonal dominant and hence
family of the PSD matrices: the set of diagonal dominant  positive semideﬁnite.
matrices. In order to provide a better understanding of the
motivation for our next formulation we present the following 2.2 Learning kernels that depend on fewer input
theorem as stated in [Golub and Van Loan, 1996]:           features (K˜ )
Theorem 1 Diagonal Dominance Theorem   Suppose that   Next, we will modify formulation (4) in order to learn kernels
       D×D
M  ∈       is symmetric and that for each i =1,...,n , that depend in a small subset of the original input features.
we have:                                              As far as we know, all of the existing direct objective opti-
                                                     mization methods for feature selection with kernels perform
                  Mii ≥    |Mij | ,                   feature selection in the implicit feature space induced by the
                        j=i                          kernel and not with respect to the original feature space. Do-
                                                      ing this using traditional kernels, leads to nonlinear, noncon-
then M is positive semi-deﬁnite (PSD). Furthermore, if the
                                                      vex optimization problems that are difﬁcult to solve due to the
inequalities above are all strict, then M is positive deﬁnite.
                                                      nonlinearity relations among the original features introduced
Based on the diagonal dominance theorem (for matrices with by the kernel mappings.
positive diagonal elements) above, by simplifying the nota- In order to overcome this difﬁculty we propose a simple but
            α      α
tion letting Kij ≡ K (xi, xj), we arrive to the following effective idea. Instead of using kernels that depend on all the
alternative formulation:                              features we will only consider a set Ω¯ comprised of weak ker-
                  T                                  nels that depend on only one feature at a time. For example, if
          minα,    t=1 t + γ α1                          1      f      D             1      f     D
                                                      xi =(x  ,...,x ,...,x ) and xj =(x  ,...,x ,...,x )
              s.t                                            i      i      i             j      j     j
                      α     α                         are two vectors on D, then a weak Gaussian kernel only
 ∀t =(i, j, k) ∈T   Kij − Kik + t ≥   1
                                α                 α   depending on feature f is deﬁned by:
          ∀i ∈T               Kii  ≥     j∈S,j=i |Kij |
              ∀t≥                     0,                                                   2
                                 t                                                    f    f 
                                                               K(f)(xi, yj )=exp(−μ  xi − xj  )     (5)
                                                (3)                                           2

                                                IJCAI-07
                                                   788                                                 ¯
Let us denote by If the set of indices i of kernels Ki ∈ Ω
that only depend on feature f for f ∈{1,...,D}. Then, any          Table 1: Benchmark Datasets
linear combination of weak kernels in Ω¯ can be written as:    Name          Pts (N)Dims(D)      Classes
                         D  
        α     α
      Kij = K  (xi, xj)=        αpKp(xi, xj )   (6)

                         f=1 p∈If                       1 Housing-Boston     506      13         2
                                                        2 Ionosphere         351      34         2
Note that if αp =0, ∀p ∈ If for a given f,thisimpliesthat
  α                                                     3 Iris               150      4          3
Kij does not depend on the original feature f. This motivates
our next formulation for feature selection that uses weak one- 4 Wine        178      13         3
                                                        5 Balance Scale      625      4          3
dimensional kernels:                      
                    T                            α      6 Breast-Cancer Wisc. 569     30         2
            min        t + γ1   sf  +   γ2    K
            α,,s   t=1         f             i  ii     7 Soybean Small      47       35         4
              s.t                                       8 Protein            116      20         6
                        α     α
 ∀t =(i, j, k) ∈T     Kij − Kik + t ≥   1              9 Pima Diabetes      768      8          2
                                  α
          ∀i = j        −rij ≤ Kij ≤   rij
                          α
              ∀iKii         −   j rij ≥  0
      ∀f,∀p ∈ If           −sf ≤ αp  ≤   sf           3.1  Evaluation Settings
                                  t ≥   0,           The datasets employed in these experiments are generally
                                                (7)   used for classiﬁcation since class labels are available. How-
where now s is indexed by the feature number f rather ever, the various methods to be compared here do not re-
than the kernel number alone. It is interesting to note that quire explicit class labels. The method introduced in this pa-
for each feature f, formulation (7) is minimizing Mf = per requires relative similarity information among a subset of
max {|αp| /p ∈ If }. This is appropriate since:       points (clearly class labels provide more information). We
  Mf  =0   ⇒|αp|≤0,     ∀p ∈ If                       use the available class labels to generate a set of triples with
           ⇒   αp =0, ∀p ∈ If                         similarity comparisons that respect the classes. More explic-
                      α                               itly, given a randomly chosen set of three points (from the
           ⇒∀i, j, Kij  does not depend on featuref.
                                                (8)   training set), if two of these belong to the same class and a
                                                      third belongs to a different class, then we place this triple in
                                                             T     i    j                             k
3  Numerical Evaluation                               our set  (i.e., and are the points in the same class, is
                                                      the remaining point). For the case of [Xing et al., 2002],the
For our experimental evaluation we used a collection of nine
                                              4       supervision is in the form of two sets, one called a similar set
publicly available datasets, part of the UCI repository .A and the other a dissimilar set. In order to identify these sets,
summary of these datasets is shown in Table 1. These datasets we can again use the class labels, now to build a similar set of
are commonly used in machine learning as a benchmark for pairs (likewise for a dissimilar set of pairs). Given this level
performance evaluation. Our choice of datasets is motivated of supervision, this method attempts to ﬁnd an optimal Ma-
primarily by their use in evaluating a competing approach halanobis distance matrix to have same-class points closer to
[Xing et al., 2002] aimed to learn distance functions. each other than different-class points (see [Xing et al., 2002]
  We evaluate our approach against single kernels by com- for details).
paring against the standard Gaussian (at various widths), lin- For every triple (i, j, k) ∈Tused in our approach for
ear, and polynomial kernels. These kernels are also the ones learning, we use (i, j) ∈Sand (i, k) ∈Dfor learning in
used as the basis for our mixture kernel design; thus it is a rea- [Xing et al., 2002];whereS and D are the similar and dis-
sonable baseline comparison. We compare both of our formu- similar sets. We believe this provides a fair level of supervi-
lations. One formulation attempts to perform implicit feature sion for both algorithms since roughly the same information
selection by deﬁning weak kernels, while the other uses full is provided. It is possible to obtain a superset of T from S
kernel matrices (that depend on all input dimensions). and D, and by construction S and D can be obtained from T .
  We have also chosen to compare our formulation against In order to evaluate performance for the various methods,
that proposed in [Xing et al., 2002]. This obeys various rea- we use a 0.85/0.15 split of the data into training and testing
sons. In addition to being a state-of-the-art method, the pri- (for the methods where training is required). From the train-
mary reason for our choice is that it uses a similar (but not ing portion, we generate 1500 triples, as explained above, for
identical) type of supervision, as explained below. Unlike actual training. This information is provided, in the appropri-
other related approaches, computer code and data has been ate representation, to both algorithms. For testing, we repeat-
                        5
made public for this method . In addition, this method out- edly choose three points at random, and if their class labels
performed a constrained version of K-means [Wagstaff et al., imply that any two points are more similar to each other than
2001] in the task of ﬁnding good clusterings.         to a third (i.e., again if two points have the same class and a
  4http://www.ics.uci.edu/∼mlearn/MLRepository.html.  third has a different class label), then we check that the correct
  5Data for all experiments and code for [Xing et al., 2002] was relationships were learned. That is, whether the two points in
downloaded from http://www.cs.cmu.edu/∼epxing/papers/. The the same class are more similar (or closer) to each other than
class for dataset 1 was obtained by thresholding the median value any of these points (chosen at random) to the third point. This
attribute to 25K.                                     same measure is used for all algorithms. Thus, we deﬁne the

                                                IJCAI-07
                                                   789percentage correct simply as the proportion of points from   1
the test set (sampled at random) that respect the class-implied 0.9


similarity or distance relationship.                         100) 0.8
  Our method requires setting two balancing parameters. We  ×
set them by using cross validation by splitting the training 0.7
set in two halves. The values tested (for both parameters)   0.6
        −4   −2   −1        2
were {10  , 10 , 10 , 1, 10, 10 }. These parameters have     0.5
an effect on the number of dimensions employed since a
                                                             0.4
higher value for γ1 favors using fewer kernels (or dimen-
sions). Likewise, larger values for γ2 favors kernel matrices 0.3   Gauss μ=1
                                                                    Gauss μ=0.1
                                                             0.2
with a smaller trace.                                               Poly degree 2


                                                            %  correct distance relationship ( Mix Full Kernels
                                                             0.1
3.2  Discussion and Results                                         Mix Weak Kernels
                                                             0
                                                              0   1  2   3   4   5  6   7   8   9  10
Fig. 1 shows the performance of our approach compared                        Dataset index
against various Gaussian and polynomial kernels. A linear
kernel performed almost identically to the polynomial kernel Figure 1: Performance of single kernels and our approachs in the
of degree two and it is omitted in the graph. The mean and nine UCI datasets. We show both instances of our approach (us-
standard deviation (of the performance measure) for each in- ing full kernel matrices and weak kernels). Bars show performance
dividual kernel was computed from 10 random samplings of results on 10 random splits of training/test points (training only ap-
the dataset. In order to be consistent across approaches, the plicable to kernel mixtures). Performance is measured in terms of
number of samples used for testing was set to 1000 triples. the percentage of randomly chosen points (1000) from test set whose
  One could expect an optimal mixture of kernels to provide distance relationship respect the class labels. The number of triples
a higher performance in both cases (full and weak kernel mix- used for training for all runs was 1500. Error bars show one standard
tures) when compared with single kernels. This is generally deviation.
the case. It can be seen in the ﬁgure that in most cases single
predetermined kernels are suboptimal. In the case of weak method effectively reduces the number of features for all but
kernels, the performance is always better than single kernels. one dataset.
However, for the case of mixtures of full kernels there are
cases where a single kernel provides a higher performance in Fig. 3 shows the comparison of the present formulation
                                                                               [               ]
average. This can be at least due to two reasons: (1) simply (using weak kernels) against Xing et al., 2002 .Weshow
unlucky selection (sampling) of test points. In datasets 1, 3, percentage correct averaged over 10 random splits of the data
and 9, the standard deviation is large enough to justify this along with one-standard-deviation bars. For each of the 10
possibility. A much more interesting reason is (2) the im- splits, 1000 triples from the test set are randomly chosen.
                                                      When comparing the performance of both methods, we note
posed restrictions on the solution space of the mixture kernel           5
(e.g., dataset 6). Recall that we concentrated our solution on that, except for dataset , our method clearly outperforms the
a subspace of the PSD matrices, that of diagonal dominant competing approach. Interestingly, this dataset was the same
matrices. If the full cone of PSD matrices were to be incor- for which the optimal number of dimensions was determined
porated as solution space, the kernel mixture could perform to always be equal to the original dimensionality.
as good as any base kernel (since a mixture of a single kernel In addition to the implicit non-linear representations im-
is a valid solution). Note however that, although this is possi- plied by the kernels employed, we believe that a key reason
ble, one would be required to pay much higher computational for the superior performance of the mixture of weak kernels is
costs (e.g., for large datasets or number of constraints). the automatic identiﬁcation of relevant dimensions. This re-
  Interestingly, the performance for the mixture of weak ker- duction in dimensionality appears to provide an important ad-
nels is superior. This can be due to the larger number of vantage at the time of generalization. It is generally accepted
degrees of freedom (the number of α mixing parameters is that a simpler representation is preferable (e.g., [Blumer et
larger) or also a feature selection effect on overﬁtting. How- al., 1987]) and it can reduce overﬁtting in practice.
ever, even though this is the case for the datasets considered From a computational efﬁciency perspective at test time,
in this paper (and results suggest that this may often be the being able to represent the original data more succinctly is
case), we remark that this result is not theoretically guaran- especially advantageous. In particular, when similarities can
teed since non-linear interactions among multiple dimensions be calculated directly using a low-dimensional representa-
or features may not be representable using a linear combina- tion, computational time savings can be signiﬁcant for on-line
tion of single features.                              applications. The projection step in this approach can be pre-
  Fig. 2 shows the average optimal number of dimensions computed off-line. In retrieval applications (e.g., query-by-
found by this process in a 10-fold cross validation experiment example), the objects can be stored in their low-dimensional
and the corresponding one-standard-deviation error bars. The representation. From a conceptual point of view, this formu-
number of dimensions was identiﬁed by counting the number lation also has the advantage of providing a more effective
of α’s larger than 0.01. This automatic choice of dimension- tool for understanding the data since it can identify whether
ality was done using cross-validation as explained above and variables (dimensions) are of high or low relevance for a task
is a valuable property of the method presented. Note that the of interest.

                                                IJCAI-07
                                                   790