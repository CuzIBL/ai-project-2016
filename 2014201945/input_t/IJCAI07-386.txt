      Using Graph Algebra to Optimize Neighborhood for Isometric Mapping

                           Guihua Wen , Lijun Jiang and Nigel R Shadbolt
                   South China University of Technology, Guangzhou 510641, China
                 University of Southampton, Southampton SO17 1BJ, United Kingdom
                     crghwen@scut.edu.cn cssturat@sohu.com nrs@ecs.soton.ac.uk


                    Abstract                          with fuzzy LDA [Weng et al., 2005] etc. The other related
                                                      studies are also performed such as the selection of the op-
    Most nonlinear dimensionality reduction ap-       timal parameter value for LLE and Isomap [Kouropteva et
    proaches such as Isomap heavily depend on the     al., 2002; Shao and Huang, 2005], integration of LLE and
    neighborhood structure of manifold. They deter-   Isomap [Saxena et al., 2004] etc. LLE and Isomap have their
    mine the neighborhood graph using Euclidean dis-  own superiorities so that they have been being developed si-
    tance so that they often fail to nicely deal with multaneously for various context applications.
    sparsely sampled or noise contaminated data. This   Despite Isomap performs well in many cases, it often fails
    paper applies the graph algebra to optimize the   to nicely deal with noisy data or sparsely sampled data [Geng
    neighborhood structure for Isomap. The improved   et al., 2005]. This is because in these cases the local neighbor-
    Isomap outperforms the classic Isomap in visual-  hood structure on which Isomap largely depends is critically
    ization and time complexity, as it provides good  distorted. This makes Isomap vulnerable to short-circuit er-
    neighborhood structure that can speed up the subse- rors that some neighors of the current point come from other
    quent dimensionality reducing process. It also has different folds so that these neighbors are not nearest ones
    stronger topological stability and less sensitive to on manifold [Silva and Tenenbaum, 2003]. This can in turn
    parameters. This indicates that the more compli-  lead to drastically incorrect low-dimensional embedding. Ac-
    cated or even time-consuming approaches can be    cordingly some supervised Isomap are proposed [Vlachos et
    applied to construct the better neighborhood struc- al., 2002; Geng et al., 2005] which employ the class labels
    ture whilst the whole time complexity will not raise of the input data to guide the manifold learning. They im-
    .The conducted experiments on benchmark data      prove the Isomap in classiﬁcation and visualization, but they
    sets have validated the proposed approach.        can not work when the class labels of data are not avail-
                                                      able. Due to that Isomap can not works on the discon-
1  Introduction                                       nected neighborhood graph, some approaches to construct-
Classic dimensionality reduction approaches can not be reli- ing connected neighborhood graph are proposed such as the
ably applied to ﬁnd the meaningful low-dimensional struc- k-connected or k-edge-connected spanning subgraph of the
                                                                                             [
tures hidden in the high-dimensional observations for ex- complete Euclidean graph of all data points Yang, 2004;
                                                          ]
ploratory data analysis such as classiﬁcation and visualiza- 2005 . This paper deals with the neighborhood graph from
tion, so that two new approaches have recently been devel- another perspective. It considers the implicit correlation
oped. One is locally linear embedding (LLE) that is based on among data points using the path algebra on the neighbor-
local approximation of the geometry of the manifold[Roweis hood graph, which is further applied to improve Isomap.
and Saul, 2000]. LLE has many variants, such as Laplacian
eigenmaps [Belkin and Niyogi, 2003]and Hessian eigenmaps 2 Approach to optimize the neighborhood
[Donoho and Grimes, 2003], incremental LLE [Kouropteva In many data analysis and pattern recognition tasks, similar-
and Pietikainen, 2005], supervised LLE [de Ridder et al., ity between two objects can be established by direct com-
2003; Kouropteva and Pietikainen, 2005], integrated LLE parison and induced by mediating objects. Namely, two ob-
with classic PCA or LDA [Abusham et al., 2005; Chang et jects might be considered similar when they are connected
al., 2004], integrated LLE with SOM [Xiao et al., 2005] etc. by a chain of intermediate objects where all dissimilarities
The other is Isomap that preserves the manifold geometry at or distances between neighboring objects in the chain are
all scales and has better ability to deal with nonlinear sub- small [Fischer and Buhmann, 2003b]. This concept can be
spaces [Tenenbaum et al., 2000]. It also has many variants, generalized by assuming that object similarity behaves tran-
such as Landmark Isomap [Silva and Tenenbaum, 2003],su- sitive in many applications. Based on this intuition, the path
pervised Isomap [Geng et al., 2005], spatio-temporal exten- algebra-based clustering approach is proposed that can ex-
sion of Isomap [Jenkins and Mataric, 2004], incremental ex- tract elongated structures from the data in a robust way which
tension of Isomap [Law and Jain, 2006],integratedIsomap is particularly useful in perceptual organization [Fischer and

                                                IJCAI-07
                                                  2398Buhmann, 2003a; 2003b]. This paper applies it to build the           
neighborhood graph for Isomap. The intuitive picture that
two objects should be considered as neighbors if they can be
connected by a mediating path of intermediate objects.
  Let G =(V,V  × V ) be the complete Euclidean graph of
all data points. It weights the edges using Euclidean distance                  x
de .Apathl  from v0 to vn in G is deﬁned as a sequence
of vertex (v0, ···,vi, ···,vn) such that (vi,vi+1) ∈ V × V ,
0 ≤ i<n.                                                                                        
  Deﬁnition Let P (v0,v )={l|l be a path from v0 to v in
                     n                         n                                              d     d
G}, we deﬁne path algebra as a set P with two binary oper- Figure 2: The different neighborhoods using m and e
ations ∨ and · which have the following properties [B.Carre,
1979]:
                                                      in terms of categories. By contrast, if dm distance is utilized
 1. For all x, y, z ∈ P ,the∨ is idempotent, commutative, to determine the neighbors for x, the result is
                  x ∨ x = x, x ∨ y = y ∨ x (x ∨ y) ∨ z =
    and associative:                   ,                   N m(x)={circle, circle, circle, circle, circle}
    x ∨ (y ∨ z)
          x, y, z ∈ P   ·                             This is a correct result that indicates all neighbors of x lie on
 2. For all         ,the is associative, and distributive                    d
    over ∨: (x · y) · z = x · (y · z),x· (y ∨ z)=(x · y) ∨ the manifold. It seems that m can pulls points belonging to
    (x · z), (y ∨ z) · x =(y · x) ∨ (z · x)           the same class closer and propels those belonging to different
                                                      classes further away. It ensures to preserve the intrinsic struc-
 3. The set P contains a zero element φ and a unit element e ture of the data set and therefore can be applied to improve
    such that: φ· x = φ, φ ∨x = x = x∨φ, e · x = x = x· e the Isomap.
  Obviously, there are many ways to deﬁne the path algebra
based on different intuitions. This paper focuses on a simple 3 Proposed Isomap with Optimized
but important way to deﬁne the path algebra as follows, where Neighborhood
for all x, y ∈ R, φ =0,ande = ∞
                                                      Isomap can perform dimensionality reduction well when the
         def
 1. x ∨ y = min(x, y)                                 input data are well sampled and have little noise. In the
                                                      presence of the noise or when the data are sparsely sam-
        def
 2. x · y = max(x, y)                                 pled, short-circuit edges pose a threat to Isomap by fur-
                                                      ther deteriorating the local neighborhood structure of the
In this way a new distance dm can be deﬁned as follows [Fis-
cher and Buhmann, 2003b]:                             data. Subsequently Isomap generates drastically incorrect
                                                      low-dimensional embedding. To relieve this negative effect
d (v ,v )=min           {max         {d (v ,v   )}}              d
 m  0  n        l∈P (v0,vn)  (vi,vi+1)∈l e i i+1      of the noise, m can be applied to determine the true neigh-
                                                (1)   borhoods as opposed to using Euclidean distance de.This
The difference between dm and de can be illustrated in Fig- can get better estimation of geodesic distances and in turn
ure 1. This difference can signiﬁcantly impact the construc- give lower residual variance and robustness. It also reduces
                                                      the time complexity because the better neighborhood struc-
       de (ba, )     d                                ture can speed up the subsequent optimization process.
                c                                                                                  d
                           b                            To avoid computational expense to calculate the m be-
                                                      tween any two points, we ﬁnd an approximate way to build
                a                                     the neighborhood graph using dm that can be computed ef-
                                                      ﬁciently. It ﬁrst applies de to build the neighborhood graph,
                                                      and then utilize the idea of dm to optimize this neighborhood
                                                      graph. The algorithm is given as follows.
  d (ba,b) = min{max{d (a,c),d (c,d),d (d,b)},d (a, )}  Algorithm 1: OptimizeNeighborhood(X, k, m,d)
   m                e      e      e        e        
                                                        Input: X =  {xi} be the high dimensional data set, k be
                                                      the neighborhood size, m be the scope for optimization of
       Figure 1: The difference between d and d
                                    m      e          neighborhood, and d be the dimension of the projected space.
                                                        Output: The optimized neighborhood set N = {N(xi)}
tion of neigborhood graph on data manifold. For exam- for all points.
                                      x      d
ple, the categories of 5-nearest neighbors of using e dis- 1. Calculate the neighborhood N(x ) for any point x us-
tance,circled as in Figure 2, is                                                      i              i
                                                      ing de,whereN(xi) is sorted ascendingly.
    N e(x)={square, square, square, circle, circle}     2. Compute n, which is the number of points in X.
                                                        3. For i=1 to n
Generally points belonging to the same class are often closer 4. For j=1tok
to each other than those belonging to different classes. Ob- 5. Select j-th point from N(xi), denoted as xij
viously the short circuiting problem happens here because 6.   For p=1tom
points marked by square are not the most similar points to x 7.  Select p-th point from N(xij ), denoted as xijp

                                                IJCAI-07
                                                  2399  8.      If de(xij ,xijp) <de(xi,xik) and xijp ∈/ N(xi) the neighborhood of point x has changed, the optimization
          and parent(xijp) ∈ N(xi)                    restarts from the smallest neighbor x1 of the point x and en-
                                                                             x
  9.          Delete xik from N(xi)                   ter the next cycle. Due to 11 has been in neighborhood of
                                                      x  x                                (3)
  10.        Insert xijp to N(xi) ascendingly          ,  12 will be explored, shown as Figure 3 . Because the
  11.        Let j=1                                  condition in step 8 is satisﬁed, all steps between step 9 and
  12.        Break                                    step 12 will be executed. This results in that x12 will re-
  13.      EndIf                                      place x2.Consequently the neighorhood of x is optimized as
  14.    EndFor                                       N(x)={x1,x11,x12},shown as Figure 3(4). Due to m=2,
  15.  EndFor                                         x13 will never be exploited. So far all neighbors of x1 have
                                                                                                        x
  16. EndFor                                          been tested. Next cycle will explore the second neighbor of
                                                                     x
  17. Return N = {N(x )} that is the optimized neighbor- in step 5, namely 11. It goes through all steps as exploring
                     i                                               x    x
hoods for X                                           the ﬁrst neighbor 1 of , which is not explained more here.
  In algorithm 1, step 1 calculates the neighborhood graph Now OptimizeNeighborhood can be applied to improve the
using Euclidean distance, as the same as most dimensionality basic Isomap. For hereafter comparison, we denote this im-
reduction approaches such as Isomap do. Its time complexity proved Isomap as m2-Isomap, which is described as algo-
<O(n3). From step 3 to step 16, the neighborhood of the rithm 2.
          x                                  d          Algorithm 2: m2-Isomap(X, k, m, d)
given point i determined in step 1 is optimized using m.It     X =  {x }
includes two cycles. Although step 11 reset j, k and m are Input:     i  be the high dimensional data set, k be
neighborhood sizes so that they can be regarded as small con- the neighborhood size, m be the scope for optimization of
                                                      neighborhood, and d be the dimension of the projected space.
stants. It means that optimization of neighborhood of a point                                Y  = {y }
can be ﬁnished in O(1). Therefore the neighborhood graph Output: The dimensionally reduced dataset  i .
can be efﬁciently optimized with additional time complexity 1. Utilize OptimizeNeighborhood(X, k, m, d) to calculate
O(n). In Step 8, condition parent(xijp) ∈ N(xi) means that the optimized neighborhoods for each point in input data
xijp must be reachable from xi in the neighborhood graph  set X, which will be applied to construct the neighbor
determined in step 1.                                     graph in next step.
                                                        2. Construct the weighted neighbor graph G =(V,E)
             x                         x                                                      e
    (1)                       (2)                         for the dataset X, by connecting each point to all its k-
      2   6     9               2    4     6
                                                          nearest neighbors, where (vi,vi+1) ∈ E,if xj is a mem-
                                                          ber of k neighbors of x determined in step 1. This edge
    x1       x2      x3       x1                                             i
                                       x11     x2
                                                          has weight de(xi,xj ).
  4      12                 4       12
    5                          5                        3. Employ the Ge to approximately estimate the geodesic
                                                          distance dg(xi,xj ) between any pair of points as the
   x    x12  x13             x     x12 x13
    11                        11                          shortest path through graph that connects them.
                                                        4. Construct d-dimensional embedding from Ge using
    (3)      x                (4)      x                  MDS
      2   4     6               2    4    5             Compared with the basic Isomap, m2-Isomap adds step 1
                                                      and then modiﬁes the step 2. It should be noted that the edges
    x1                        x1
            x11      x2               x11      x12
                                                      of neighborhood graph are weighted by de instead of dm.The
  4      12                 4      12                 d
    5                          5                       m is only applied to optimize the neighborhood rather than
                                                      applied to estimate the geodesic distance. The step 3 and step

   x11  x12  x13             x11  x12  x13            4 remains the same as the basic Isomap. Therefore the time
                                                      complexity increases from step 1 where the additional com-
                                                               O( )
Figure 3: Illustration of optimizing neighborhood of point x  plexity is n . However the optimized neighborhood will
                                                      speed up the step 4 so as to decrease the running time of the
                                                      whole algorithm. This will be proved empiricially in later
  In order to illustrate the idea behind the algorithm 1, we
give an example to optimize the neighborhood of point x, experiment.
shown as Figure 3, where k=3 and m=2. Figure 3(1) shows
the neighborhood graph of x determined using Euclidean dis- 4 Experimental Results
tance, where only parts are described. It can be observed Several experiments are conducted to compare Isomap with
that x has neighbors N(x)={x1,x2,x3}. In algorithm    m2-Isomap in visualization and time complexity. Isomap
1, step 5 chooses its ﬁrst neighbor x1 andthenstep7se- uses the published matlab code [Tenenbaum et al., 2000].The
lects the neighbor x11 for x1.Instep8,x11 will be com- m2-Isomap is also implemented in MATLAB 7.0. In exper-
pared with the largest neighbor of x1, namely x3. Because iments, there are two parameters involved. Isomap has a pa-
the condition is satisﬁed, All steps between step 9 and step rameter k to determine the size of local neighborhood whereas
12 will be executed. This results in that x11 will replace m2-Isomap introduces an additional parameter m that con-
x3.  Consequently the neighorhood of x is optimized as trols the scope of the neighborhood to be modiﬁed, where
N(x)={x1,x11,x2}. It is shown as Figure 3(2). Because k=5-15 and m =1-10 will be explored.

                                                IJCAI-07
                                                  2400  A. Swiss Roll Data set                              ”12/6/0.00059146” indicates that the parameter k takes 12,m
  Swiss Roll dataset is widely applied to compare many takes 6, and the residual variance is 0.0.00059146. It can be
non-linear dimensionality reduction approaches [Roweis and observed that on well sampled data sets without noise, m2-
Saul, 2000; Tenenbaum et al., 2000; Balasubramanian and Isomap outperforms Isomap. It gets better mapping results
Schwartz, 2002; Geng et al., 2005]. We take many ran- and lower residual variances on any data set.
dom samples from the Swiss Roll surface shown as Figure 4, Experiment 2 On well sampled but noise contaminated
and do visualization experiments on them to compare Isomap data sets
with m2-Isomap in the following cases: (1)well sampled data To compare the topologically stability of Isomap and m2-
without noise (2) well sampled data with noise (3)sparsely Isomap on data sets with noise, we do experiments on the
sampled data.                                         ﬁrst data set with 1000 points used in experiment 1, but here
                                                      it is contaminated by adding random Gaussian noise. The
                                                      mean of the noise is 0 and the variance is 0.64. It can be seen
                                                      from Figure 6 that Isomap gets the best result with residual
                                                      variance 0.001286 when k=6, while m2-Isomap gets the best
                                                      result with the lower residual variance 0.0010773 with pa-
                                                      rameters k=12 and m=6. Obviously m2-Isomap outperforms
                                                      Isomap in terms of visualization and residual variance. It can
                                                      be also observed from Figure 6 that as k increases, the results
                                                      of Isomap change drastically, but those of m2-Isomap do not
          Figure 4: Swiss roll surface sample         change much. This indicate that m2-Isomap is also less sen-
                                                      sitive to k than Isomap.
  The quality of visualization can be evaluated subjectively,
while it is often quantitatively evaluated by the residual vari-
ance [Roweis and Saul, 2000; Tenenbaum et al., 2000; Bal-
asubramanian and Schwartz, 2002; Geng et al., 2005].The
lower the residual variance is, the better high-dimensional
data are represented in the embedded space.
  Experiment 1 On well sampled noiseless data sets


                                                      Figure 6: The mapped results of two approaches on a well
                                                      sampled but noise contaminated data set

                                                        Experiment3 On sparsely sampled data sets
                                                        Generally, in sparsely sampled data sets, the Euclidean
                                                      distance between points in neighborhood becomes larger as
                                                      compared to the distance between different folds of the man-
Figure 5: The mapped results of two approaches on ﬁve well ifold. This easily makes two approaches faces the problem of
sampled noiseless data sets                           short-circuiting. This experiment makes comparison between
                                                      two approaches about robust to samples with different sam-
  To make comparison on well sampled noiseless data sets, pling density. Five data sets with 300,400,500,600 and 700
ﬁve sample data sets with 1000 points are sampled randomly points respectively are randomly sampled from Swiss Roll
from Swiss Roll surface. Firstly on the ﬁrst sampled data surface. Two approaches are tuned to select the best visu-
set, two approaches are tested to choose the optimal param- alization results on each data set. It can be observed from
eters in the given scope, and then these parameters are ap- Figure 7 that m2-Isomap outperforms Isomap on each datset
plied to run the remaining four data sets. The mapped results in terms of visualization performance and residual variance.
from left to right corresponding to ﬁve data sets are shown B. Face image data set
as Figure 5, where the parameters and residual variance for This data set consists of 698 images (each contains 64×64
each data set are added as caption. For example on the ﬁrst pixels) of a human face rendered with different poses and
data set,in Figure 5(a)Isomap, the caption ”6/0.0015475” in- lighting directions [Tenenbaum et al., 2000]. To compare two
dicates that the parameter k takes 6 whereas the residual vari- approaches on sparsity data set, we choose 350 (half of the
ance is 0.0015475. In Figure 5(b)m2-Isomap, the caption data set)images from this data set to form a new data set. The

                                                IJCAI-07
                                                  2401                                                           0.5
                                                                                              Isomap
                                                                                              m2−Isomap
                                                           0.4

                                                           0.3

                                                           0.2
                                                          Residual  variance
                                                           0.1

                                                            0
                                                             1   2    3   4    5   6    7   8    9   10
                                                                              Dimension

                                                      Figure 10: The residual variance of two approaches along the
                                                      dimension on face images varying in pose and illumination
                                                      (N=350).
Figure 7: The mapped results of two approaches on ﬁve
sparsely sampled data sets                            residual variances and the correct intrinsic dimensionality of
                                                      data set, while Isomap gives the higher estimate to intrinsic
mapped results are shown in Figure 8 and Figure 9. In ﬁg- dimensionality than that of the data set. The intrinsic dimen-
                                                      sionality of the data can be estimated by looking for the ”el-

      100                                             bow” at which this curve ceases to decrease signiﬁcantly with

      80                                              added dimensions.
      60                                                C. Time complexity comparison
      40                                                We generate 10 data sets with the same size randomly sam-
      20                                              pled from Swiss Roll surface. Two approaches run on these
       0                                              ten data sets respectively and then average running time for
      −20
      −20   0   20   40  60   80  100  120 140        two approaches are calculated. And then the experiments are
                                                      performed on different sizes: 500 900 1300 1700 2100 2500
               Figure 8: Isomap (k=5)                 2900 3300. It can be observed from Table 1 that m2-Isomap
                                                      also exceeds the Isomap in time complexity. The larger the
                                                      data set size is, the superiorities of m2-Isomap is more obvi-
                                                      ous. This is because good neighborhood structure of data set
      100
                                                      is beneﬁcial to speed up the subsequent optimization process.
      80

      60
      40                                              5Conclusion
      20
       0                                              This paper applies the graph algebra to build the neighbor-
      −20
      −20  0   20  40  60  80  100 120 140 160        hood graph for Isomap. The improved Isomap outperforms
                                                      the classic Isomap in visualization and time complexity. It
            Figure 9: m2-Isomap (k=5 m=1)             also has stronger topological stability and less sensitive to pa-
                                                      rameters. This indicates that the more complicated or even
ures, the x-axis represents the left-right poses of the faces, time-consuming approaches can be applied to construct the
and the y-axis represents the up-down poses of the faces. The better neighborhood structure whilst the whole time complex-
corresponding points of the successive images from left to ity will not raise. Because the transitivity of similarity is
right in the middle are marked by circles and linked by lines. not absolutely correct for any data set, in the future, we will
The nine critical face samples are marked by plus at the left- apply the fuzzy theory and probability theory to deﬁne the
bottom corner of each image indicates the point representing graph algebra and then to build the neighborhood graph. And
the image. It can be observed from these ﬁgures that Isomap besides, the proposed approach should be combined with k-
can hardly reveal the different face poses. The middle left- edge-connected spanning subgraph so as to guarantee to build
right line is heavily curved, and the arrangement of the nine the connected neighborhood graph for any data set.
face samples is tangle some. By contrast, m2-Isomap puts the
middle left-right line better. The nine face samples are also Acknowledgments
mapped to the approximately right positions corresponding
to the face poses. These indicate that m2-Isomap performs This work is supported by Natural Science Foundation
better than Isomap.                                   of China(60003019),Hubei Science and Technology Project
  This also can be supported from their residual variances. It (2005AA101C17),and UK Engineering and Physical Sci-
can be observed from Figure 10 that m2-Isomap gets lower ences Research Council under grant number GR/N15764/01.

                                                IJCAI-07
                                                  2402