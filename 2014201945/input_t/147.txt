            Point-based value iteration: An anytime algorithm for POMDPs 
                          Joelle Pineau, Geoff Gordon and Sebastian Thrun 
                                       Carnegie Mellon University 
                                            Robotics Institute 
                                           5000 Forbes Avenue 
                                          Pittsburgh, PA 15213 
                                  {jpineau,ggordon,thrun}@cs.cmu.edu 

                     Abstract                          for distinct histories. But, they can act independently: plan•
                                                       ning complexity can grow exponentially with horizon even 
  This paper introduces the Point-Based Value Iteration in problems with only a few states, and problems with a 
  (PBVI) algorithm for POMDP planning. PBVI approx•    large number of physical states may still only have a small 
  imates an exact value iteration solution by selecting a number of relevant histories. In most domains, the curse 
  small set of representative belief points and then tracking of history affects POMDP value iteration far more strongly 
  the value and its derivative for those points only. By us•
                                                       than the curse of dimensionality [Kaelbling et al.9 1998; 
  ing stochastic trajectories to choose belief points, and by Zhou and Hansen, 2001]. That is, the number of distinct his•
  maintaining only one value hyper-plane per point, PBVI tories which the algorithm maintains is a far better predictor 
  successfully solves large problems: we present results on of running time than is the number of states. The main claim 
  a robotic laser tag problem as well as three test domains of this paper is that, if we can avoid the curse of history, there 
  from the literature.                                 are many real-world POMDPs where the curse of dimension•
                                                       ality is not a problem. 
1 Introduction                                           Building on this insight, we present Point-Based Value It•
The value iteration algorithm for planning in partially ob• eration (PBVI), a new approximate POMDP planning al•
                                                       gorithm. PBVI selects a small set of representative belief 
servable Markov decision processes (POMDPs) was intro•
                                                       points and iteratively applies value updates to those points. 
duced in the 1970s [Sondik, 1971]. Since its introduction 
                                                       The point-based update is significantly more efficient than 
numerous authors have refined it [Cassandra et al> 1997; 
                                                       an exact update (quadratic vs. exponential), and because it 
Kaelbling et a/., 1998; Zhang and Zhang, 2001] so that it can 
                                                       updates both value and value gradient, it generalizes better 
solve harder problems. But, as the situation currently stands, 
                                                       to unexplored beliefs than interpolation-type grid-based ap•
POMDP value iteration algorithms are widely believed not to 
                                                       proximations which only update the value [Lovejoy, 1991; 
be able to scale to real-world-sized problems. 
                                                       Brafman, 1997; Hauskrecht, 2000; Zhou and Hansen, 2001; 
  There are two distinct but interdependent reasons for the Bonet, 2002]). In addition, exploiting an insight from policy 
limited scalability of POMDP value iteration algorithms. The search methods and MDP exploration [Ng and Jordan, 2000; 
more widely-known reason is the so-called curse of dimen• Thrun, 1992], PBVI uses explorative stochastic trajectories to 
sionality [Kaelbling et al.% 1998]: in a problem with n phys• select belief points, thus reducing the number of belief points 
ical states, POMDP planners must reason about belief states necessary to find a good solution compared to earlier ap•
in an (n - l)-dimensional continuous space. So, naive ap• proaches. Finally, the theoretical analysis of PBVI included 
proaches like discretizing the belief space scale exponentially in this paper shows that it is guaranteed to have bounded error. 
with the number of states. 
  The less-well-known reason for poor scaling behavior is This paper presents empirical results demonstrating the 
                                                       successful performance of the algorithm on a large (870 
what we will call the curse of history: POMDP value iter•
                                                       states) robot domain called Tag, inspired by the game of 
ation is in many ways like breadth-first search in the space 
                                                       lasertag. This is an order of magnitude larger than other prob•
of belief states. Starting from the empty history, it grows 
                                                       lems commonly used to test scalable POMDP algorithms. In 
a set of histories (each corresponding to a reachable belief) 
                                                       addition, we include results for three well-known POMDPs, 
by simulating the POMDP. So, the number of distinct action-
                                                       where PBVI is able to match (in control quality, but with 
observation histories considered grows exponentially with the 
                                                       fewer belief points) the performance of earlier algorithms. 
planning horizon. Various clever pruning strategies [Littman 
et al„ 1995; Cassandra et al% 1997] have been proposed to 
whittle down the set of histories considered, but the pruning 2 An overview of POMDPs 
steps are usually expensive and seem to make a difference The POMDP framework is a generalized model for plan•
only in the constant factors rather than the order of growth. ning under uncertainty [Kaelbling etal, 1998; Sondik, 1971]. 
  The two curses, history and dimensionality, are related: the A POMDP can be represented using the following n-tuple: 
higher the dimension of a belief space, the more room it has                where 5 is a (finite) set of discrete 


PROBABILISTIC PLANNING                                                                               1025                                                        In practice, many of the vectors in the final set V may be 
                                                      completely dominated by another vector 
                                                      or by a combination of other vectors. Those vectors can be 
                                                      pruned away without affecting the solution. Finding dom•
                                                      inated vectors can be expensive (checking whether a single 
                                                      vector is dominated requires solving a linear program), but is 
                                                      usually worthwhile to avoid an explosion of the solution size. 
                                                        To better understand the complexity of the exact update, let 


                                                      tance of pruning away unnecessary vectors is clear. It also 
                                                      highlights the impetus for approximate solutions. 
                                                      3 Point-based value iteration 
                                                      It is a well understood fact that most POMDP problems, even 
                                                      given arbitrary action and observation sequences of infinite 
                                                      length, are unlikely to reach most of the points in the belief 
                                                      simplex. Thus it seems unnecessary to plan equally for all 
                                                      beliefs, as exact algorithms do, and preferable to concentrate 
                                                      planning on most probable beliefs. 
                                                        The point-based value iteration (PBVI) algorithm solves a 
                                                      POMDP for a finite set of belief points 
                                                      It initializes a separate a-vector for each selected point, and 
                                                      repeatedly updates (via value backups) the value of that a-
                                                      vector. As shown in Figure 1, by maintaining a full a-vector 
                                                      for each belief point, PBVI preserves the piece-wise linear•
                                                      ity and convexity of the value function, and defines a value 
                                                      function over the entire belief simplex. This is in contrast 
                                                      to grid-based approaches [Lovejoy, 1991; Brafman, 1997; 
                                                      Hauskrecht, 2000; Zhou and Hansen, 2001; Bonet, 2002], 
                                                      which update only the value at each belief grid point. 


A number of algorithms have been proposed to implement 
this backup by directly manipulating a-vectors, using a com•
bination of set projection and pruning operations fSondik, Figure 1: POMDP value function representation using PBVI (on the 
1971; Cassandra et ai, 1997; Zhang and Zhang, 2001].  left) and a grid (on the right). 
We now describe the most straight-forward version of exact 
POMDP value iteration.                                  The complete PBVI algorithm is designed as an anytime 
                                                      algorithm, interleaving steps of value iteration and steps of 
                                                      belief set expansion. It starts with an initial set of belief points 
                                                      for which it applies a first series of backup operations. It then 
                                                      grows the set of belief points, and finds a new solution for the 
                                                      expanded set. By interleaving value backup iterations with 


1026                                                                             PROBABILISTIC PLANNING expansions of the belief set, PB VI offers a range of solutions, 
gradually trading off computation time and solution quality. 
We now describe how we can efficiently perform point-based 
value backups and how we select belief points. 

3.1 Point-based value backup 
To plan for a finite set of belief points we modify the 
backup operator such that only one vector per be•
lief point is maintained. For a point-based update 
we start by creating projections (exactly as in Eqn 


 Next, the cross-sum operation (Eqn 6) is much simplified by 
the fact that we are now operating over a finite set of points. 
We construct 

                                                 (9) 

 Finally, we find the best action for each belief point (Step 3): 
                                                (10) 

  When performing point-based updates, the backup creates 
          projections as in exact VI. However the final so•
lution is limited to containing only components (in 
time Thus a full point-based value up•
date takes only polynomial time, and even more crucial, the 
size of the solution set remains constant. As a result, the 
pruning of a vectors (and solving of linear programs), so cru•
cial in exact POMDP algorithms, is now unnecessary. The 
only pruning step is to refrain from adding to V any vector 
already included, which arises when two nearby belief points 
support the same vector 
  In problems with a finite horizon /i, we run h value backups 
before expanding the set of belief points. In infinite-horizon 
problems, we select the horizon so that                The last inequality holds because each a-vector represents 
                                                       the reward achievable starting from some state and following 
                                                       some sequence of actions and observations. 
3.2 Belief point set expansion 
As explained above, PBVI focuses its planning on relevant 
beliefs. More specifically, our error bound below suggests 
that PBVI performs best when its belief set is uniformly dense 
in the set of reachable beliefs. So, we initialize the set B to 
contain the initial belief and expand B by greedily choos•
                                                         2Thc actual choice of norm doesn't appear to matter in practice; 
ing new reachable beliefs that improve the worst-case density some of our experiments below used Euclidean distance (instead of 
as rapidly as possible.                                L\) and the results appear identical. 
                                                         3We experimented with other strategies such as adding a fixed 
                                                       number of new beliefs, but since value iteration is much more ex•
                                                       pensive than belief computation the above algorithm worked best. If 
                                                       desired, we can impose a maximum size on B based on time con•
                                                       straints or performance requirements. 
                                                         4If not all beliefs are reachable, we don't need to sample all of 
                                                       densely, but replace by the set of reachable beliefs below. The 
                                                       error bounds and convergence results hold on 


PROBABILISTIC PLANNING                                                                               1027                                                                  Figure 3 shows the performance of PBV1 on the Tag do•
                                                               main. Results are averaged over 10 runs of the algorithm, 
                                                               times 100 different (randomly chosen) start positions for each 
                                                               run. It shows the gradual improvement in performance as 
                                                               samples are added (each shown data point represents a new 
                                                               expansion of the belief set with value backups). In addition to 
                                                               PBVI, we also apply the QMDP approximation as a baseline 


 4 Experimental results 
 The domain of Tag is based on the popular game of lasertag.   domains. In the Tag domain, however, it lacks the represen•
 The goal is to search for and tag a moving opponent [Rosen-   tational power to compute a good policy. 
 crantz et al, 2003]. Figure 2a shows the live robot as it 
 moves in to capture an opponent. In our POMDP formula•        5 Additional experiments 
 tion, the opponent moves stochastically according to a fixed 
 policy. The spatial configuration of the domain used for plan• 5.1 Comparison on well-known problems 
 ning is illustrated in Figure 2b. This domain is an order of  To further analyze the performance of PBVI, we applied it 
 magnitude larger (870 states) than mosr other POMDP prob•     to three well-known problems from the POMDP literature. 
 lems considered thus far in the literature [Cassandra, 1999], We selected Maze33, Hallway and Hallway 2 because they arc 
and is proposed as a new challenge for fast, scalable, POMDP   commonly used to test scalable POMDP algorithms [Littman 
algorithms. A single iteration of optimal value iteration on a etal, 1995; Brafman, 1997; Poon, 2001 J. Figure 3 presents 
 problem of this size could produce over 1020 a-vectors before results for each domain. Replicating earlier experiments, re•
pruning.                                                       sults for Maze33 arc averaged over 151 runs (reset after goal, 
                                                               terminate after 500 steps); results for Hallway and Hallway2 
                                                               are averaged over 251 runs (terminate at goal, max 251 steps). 
                                                               In all cases, PBVI is able to find a good policy. Table 1 
                                                               compares PBVl's performance with previously published re•
                                                               sults, comparing goal completion rates, sum of rewards, pol•
                                                               icy computation time, and number of required belief points. 
                                                               In all domains, PBVI achieves competitive performance, but 
                                                               with fewer samples. 

                                                               5.2 Validation of the belief set expansion 

   Figure 2: Tag domain (870 states, 5 actions, 30 observations) To further investigate the validity of our approach for gen•
                                                               erating new belief states (Section 3.2), we compared our 
   The state space is described by the cross-product of        approach with three other techniques which might appear 
two features, Robot =and Opponent —                            promising. In all cases, we assume that the initial belief b0 
                        Bom agenis siart in independently-     (given as part of the model) is the sole point in the initial set, 
selected random positions, and the game finishes when          and consider four expansion methods: 
Opponent The robot can select from five actions: 
                                                              1. Random (RA) 
{North, South, East, West, lag}. A reward of -1 is imposed 
                                                              2. Stochastic Simulation with Random Action (SSRA) 
for each motion action; the Tag action results in a +10 re•
ward if Robot = Opponent, or -10 otherwise. Throughout        3. Stochastic Simulation with Greedy Action (SSGA) 
the game, the Robot's position is fully observable, and the ef• 4. Stochastic Simulation with Explorative Action (SSEA) 
fect of a Move action has the predictable deterministic effect, The RA method consists of sampling a belief point from a 
e.g.:                                                          uniform distribution over the entire belief simplex. SSEA is 
                                                               the standard PBVI expansion heuristic (Section 3.2). SSRA 
                                                               similarly uses single-step forward simulation, but rather than 
 The position of the opponent is completely unobservable un•   try all actions, it randomly selects one and automatically ac•
less both agents are in the same cell. At each step, the op•   cepts the posterior belief unless it was already in B. Finally, 
ponent (with omniscient knowledge) moves away from the 
                                                               SSGA uses the most recent value function solution to pick the 
robot with Pr = 0.8 and stays in place with Pr = 0.2, e.g.: 
                                                               greedy action at the given belief 6, and performs a single-step 
                                                               simulation to get a new belief 
                                                                 We revisited the Hallway, Hallway2, and Tag problems 
                                                               from sections 4 and 5.1 to compare the performance of these 


1028                                                                                         PROBABILISTIC PLANNING        Figure 3: PBVI performance for four problems: Tag(left), Maze33(center-left), Hallway(center-right) and Hallway2(right) 

                                                      restricting value updates to a finite set of (reachable) beliefs. 
                                                        There are several approximate value iteration algorithms 
                                                      which are related to PBVI. For example, there are many grid-
                                                      based methods which iteratively update the values of discrete 
                                                      belief points. These methods differ in how they partition the 
                                                      belief space into a grid [Brafman, 1997; Zhou and Hansen, 
                                                      2001]. 
                                                        More similar to PBVI are those approaches which update 
                                                      both the value and gradient at each grid point [Lovejoy, 1991; 
                                                      Hauskrecht, 2000; Poon, 2001]. While the actual point-based 
                                                      update is essentially the same between all of these, the over•
                                                      all algorithms differ in a few important aspects. Whereas 
                                                      Poon only accepts updates that increase the value at a grid 
                                                      point (requiring special initialization of the value function), 
                                                      and Hauskrecht always keeps earlier a-vectors (causing the 
                                                      set to grow too quickly), PBVI requires no such assumptions. 
                                                      A more important benefit of PBVI is the theoretical guaran•
                                                      tees it provides: our guarantees are more widely applicable 
                                                      and provide stronger error bounds than those for other point-
                                                      based updates. 
Table 1: Results for POMDP domains. Those marked were com•
puted by us; other results were likely computed on different plat• In addition, PBVI is significantly smarter than previous 
forms, and therefore time comparisons may be approximate at best. algorithms about how it selects belief points. PBVI selects 
All results assume a standard (not lookahead) controller. only reachable beliefs; other algorithms use random beliefs, 
                                                      or (like Poon's and Lovejoy's) require the inclusion of a large 
four heuristics. For each problem we apply PBVI using each number of fixed beliefs such as the corners of the probabil•
of the belief-point selection heuristics, and include the QMDP ity simplex. Moreover, PBVI selects belief points which im•
approximation as a baseline comparison. Figure 4 shows the prove its error bounds as quickly as possible. In practice, our 
computation time versus the reward performance for each do• experiments on the large domain of lasertag demonstrate that 
main.                                                 PBVFs belief-selection rule handily outperforms several al•
  The key result from Figure 4 is the rightmost panel, which ternate methods. (Both Hauskrecht and Poon did consider 
shows performance on the largest, most complicated domain. using stochastic simulation to generate new points, but nei•
In this domain our SSEA rule clearly performs best. In ther found simulation to be superior to random point place•
smaller domains (left two panels) the choice of heuristic mat• ments. We attribute this result to the smaller size of their test 
ters less: all heuristics except random exploration (RA) per• domains. We believe that as more POMDP research moves to 
form equivalently well.                               larger planning domains, newer and smarter belief selection 
                                                      rules will become more and more important.) 
6 Related work                                          Gradient-based policy search methods have also been used 
Significant work has been done in recent years to improve to optimize POMDP solutions [Baxter and Bartlett, 2000; 
the tractability of POMDP solutions. A number of increas• Kearns et al, 1999; Ng and Jordan, 2000], successfully solv•
ingly efficient exact value iteration algorithms have been ing multi-dimensional, continuous-state problems. In our 
proposed [Cassandra et al., 1997; Kaelbling et al., 1998; view, one of the strengths of these methods lies in the fact 
Zhang and Zhang, 2001]. They are successful in finding op• that they restrict optimization to reachable beliefs (as does 
timal solutions, however are generally limited to very small PBVI). Unfortunately, policy search techniques can be ham•
problems (a dozen states) since they plan optimally for all pered by low-gradient plateaus and poor local minima, and 
beliefs. PBVI avoids the exponential growth in plan size by typically require the selection of a restricted policy class. 


PROBABILISTIC PLANNING                                                                               1029 