                 DiPRA: Distributed Practical Reasoning Architecture∗

                  Giovanni Pezzulo, Gianguglielmo Calvi, Cristiano Castelfranchi
                        Istituto di Scienze e Tecnologie della Cognizione - CNR
                        Via San Martino della Battaglia, 44 - 00185 Roma, Italy
      giovanni.pezzulo@istc.cnr.it; gianguglielmo.calvi@noze.it; cristiano.castelfranchi@istc.cnr.it


                    Abstract                          to their values in the FCM (and thus, as we will see, to their
                                                      contextual relevance), so that more relevant modules inﬂu-
    DiPRA (Distributed Practical Reasoning Architec-  ence more the computation. At the same time, the modules
    ture) implements the main principles of practi-   act in the environment and provide feedback for the values
    cal reasoning via the distributed action selection of the FCM used by the Reasoner. For example, a Condi-
    paradigm. We introduce and motivate the under-    tion can be veriﬁed or falsiﬁed by an action of the agent (in
    lying theoretical and computational peculiarities of the example we will provide, detecting if a door is open or
    DiPRA and we describe its components, also pro-   close), or a Plan can succeed or fail; the results are notiﬁed to
    viding as a case study a guards-and-thieves task. the Reasoner which updates the values of the corresponding
                                                      nodes in the FCM. As a result, practical reasoning is realized
1  Introduction                                       with central deliberation and a decentralized control struc-
                                                      ture: differently from BDI Interpreters, the Reasoner simply
Practical reasoning [Bratman et al., 1988] is a kind of rea- activates the (modules encapsulating) adopted plans, but af-
soning which is focused on the role of Intentions. BDI (“Be- ter this phase the control ﬂows between the modules in a dy-
lief, Desire, Intention”) [Rao and Georgeff, 1995] is the most namic way. Plans activate actions and subgoals without a new
famous agent architecture implementing it, which underes- intervention of the Reasoner; any further deliberation (choos-
timates however some architectural and cognitive features ing subgoals) is performed inside the plan. The activity of the
such as resource-boundedness, knowledge-boundedness and modules (success of action, testing of beliefs and conditions)
context-sensitiveness [Bratman et al., 1988].         provides feedback to the Reasoner, too.
  There are four main functions of practical reasoning: In this work we only focus on present-directed Intentions
means-ends reasoning, opportunity analysis, ﬁltering and de- [Bratman et al., 1988]: Intentions which are selected to be
liberation. The peculiarity of practical reasoning is that these activated here and now. We illustrate DiPRA, a modular, par-
operations are managed in a plan-centered way: the adopted allel and resources-bounded architecture, arguing that it per-
plan, ﬁlled in with the Intention, drives means-ends reasoning mits to model the four functions of practical reasoning as an
(plans are means for the end, the Intention), provides con- interplay of knowledge, goals, contextual factors and oppor-
straints for analyzing and ﬁltering opportune options (only tunities; we also provide a case study.
options which are relevant with the current intention are eval-
uated) and sets a priority level for its beliefs (only relevant be- 2 DiPRA Speciﬁcation and Components
liefs will inﬂuence further practical reasoning). The rationale
behind our work is that a rational agent architecture perform- The components of DiPRA are: the Reasoner, Goals, Plans,
ing practical reasoning can be implemented as a modular and Actions and Beliefs. Each of these components is imple-
parallel system, in which each Belief, Goal, Action and Plan mented as a concurrent module in the multi-thread framework
is a module operating asynchronously (with different activity AKIRA [akira, 2003]; DiPRA is also interfaced with an envi-
levels) and having relations with other modules (such as: Be- ronment (e.g. the physical simulator [irrlicht, 2003]), called
   β              γ                                   the World Engine, which evaluates its actions.
lief supports Goal ). A special module, the Reasoner,       S                       P +
maintains a consistent representation of the modules’ acti- Let be a set of worldstates, a set of atoms, and
                                                      π  : P + × S →  [0..1] a function assigning a truth value
vation level and their relations by using a Fuzzy Cognitive                         P
           [          ]                               to each atom in each worldstate. is a set of atoms and
Map (FCM)  Kosko, 1986 . It weighs the alternative goals                π(p, s)==1−   π(¬p, s) L
(exploiting a mixture of means-ends reasoning, opportunity negated atoms where               .  is a propo-
                                                      sitional language over P and the logical connectives ∧ and ∨,
analysis and ﬁltering) and deliberates. There is a continuous π(p ∧ q, s):=π(p, s) ⊗ π(q, s) ⊗
interplay between the Reasoner and the other modules: af- where:                         and   is any con-
                                                      tinuous triangular norm (e.g. min(p, q),pq); π(p ∨ q, s):=
ter selecting a (new) Intention the Reasoner assigns to mod- π(p, s) ⊕ π(q, s) ⊕
ules an activity level (i.e. the thread’s priority) proportional     and   is any continuous triangular conorm
                                                      (e.g. max(p, q),x+ y − xy) (see [Safﬁotti et al., 1995]).
  ∗Work supported by the EU project MindRACES, FP6-511931. DiPRA is described by a tuple (Ψ, Γ, Π, Φ,Bel,Ω), where:

                                                IJCAI-07
                                                  1458- Ψ is the reasoner, a tuple (FCM, Body). FCM is a Fuzzy
Cognitive Map [Kosko, 1986], a representation of the state
and the relations between all the modules; Body is the proce-
dural body, whose main task is to assign the status intended
to a goal and adopted to a plan.
- Γ is the set of goals, tuples (Type, Status, GCond, AbsRel,
ConRel). Type is the type of goal: Achieve or Maintain; Sta-
tus is the current status of the goal: Intended, Instrumental,
Waiting or Not Intended; GCond ∈ L is the (graded) satisfac-
tion condition of the goal; AbsRel is the absolute relevance;
ConRel is the contextual relevance.
 Π
-  is the set of plans, tuples (Status, SCond, ECond, PCond, Figure 1: The FCM used for the Thief in the House Scenario
ActionSet, Body, Goals, Results, AbsRel, PCondRel, Con-
Rel). Status is the current status of the plan: Adopted or
Not Adopted; SCond is the set of start conditions sc ∈ L
which are checked at the beginning of plan execution and
must be true to start it; ECond is the set of enduring con-
ditions ec ∈ L which are checked continuously during plan
execution; if an enduring condition becomes false, the plan
is stopped; PCond is the set of beliefs β ∈ L which are ex-
pected to be true after the plan (but not all of them have to
be intended); ActionSet is the set of actions φ or (sub)goal
γ activated by the plan. Actions and goals are chained in-
side ActionSet by logical connectives λ; Body is the behavior
which is executed; it normally consists in activating actions Figure 2: The House Scenario (description in the text)
and (sub)goals in the ActionSet; Goals is the set of goals γ
that make the plan satisﬁed; they are the subset of PCond adoption; θ is the commitment level of intended goals; κ is
which are intended (the reasons for activating the plan); Re-                           ϑ
                               pr ∈ L                 the commitment level of adopted plans; is the total amount
sults is the set of the ﬁnal plan results , corresponding of resources available to the whole system.
to the GCond of the Goals at the end of the plan; AbsRel is
the absolute reliability value of the Plan, i.e. how reliably it 2.1 The Reasoner
succeeds; PCondRel is the set of the reliability values ar ∈ L
of the plan with respect to its PConds, i.e. how reliably it pro- The Reasoner maintains a consistent representation of the
duces its PConds; ConRel is the contextual relevance. activity of all the modules and performs deliberation using
                                                                                       [          ]
 Φ                                                    an additive fuzzy system called FCM Kosko, 1986 whose
-  is the set of actions, tuples (SCond, PCond, Body, Goals, nodes and edges represent the modules and their links, and
Results, AbsRel, PCondRel, ConRel). SCond is the set of
             sc ∈ L                                   in which activation spreads between nodes. Fig. 1 shows a
start conditions   which are checked at the beginning of sample FCM in the House Scenario (Fig. 2, introduced later).
action execution and must be true to start it; PCond is the set
        β ∈ L                                           Deliberation consists in intending a goal and adopting (the
of beliefs    which are expected to be true after the action best) plan for it; it is performed by the FCM by weighting
(but not all of them have to be intended); Body is the behav- the alternative plans and goals and, at the same time, by eval-
ior which is executed once the action is executed; Goals is
             γ                                        uating chains of goals and plans, including of course condi-
the set of goals that make the action satisﬁed; they are the tions. In traditional practical reasoning there are three differ-
subset of PCond which are intended (actually the reasons for ent mechanisms for generating alternatives: means-ends anal-
activating the action); Results is the set of the ﬁnal action re-
    ar ∈ L                                            ysis, opportunity ﬁltering, ﬁlter overriding. The FCM formal-
sults     , corresponding to the GCond of the Goals at the ism permits to represent the constraints of all these mecha-
end of the action; AbsRel is the absolute reliability value of nisms in a compact way, and to provide at the same time suit-
the Action, i.e. how reliably it succeeds; PCondRel is the set
                    ar ∈ L                            able values for deliberation. The FCM can represent many
of the reliability values  of the action with respect to typical situations in practical reasoning: Goals concurrence
its PConds, i.e. how reliably it produces its PConds; ConRel (via inhibitory links); Beliefs sustaining a Plan or a Goal; a
is the contextual relevance.                          Plan preferred to another one because one of its precondi-
- Bel is the set of epistemic states, i.e. beliefs β ∈ L. All tions is already matched; a Goal activating one or more Plans
the conditions (GCond, SCond, PCond, ECond) are kinds of which are able to satisfy it, etc.
                   β                β ∈ L
beliefs. Bel are tuples ( , AbsRel, ConRel). is the value In the FCM there are six kinds of (weighted) links: (1) Sat-
of the belief or condition; AbsRel is the absolute relevance; isfaction A goal links plans and actions whose PCond satisfy
ConRel is the contextual relevance.                   its GCond; in this way, activation is spread from intended
- Ω is a set of parameters used to control the energetic dy- goals to plans which realize them. (2) Predecessor A plan
namics of the modules: σ is the activation of the reasoner; ζ links (sub)goals whose GCond realize its SCond or ECond;
is the threshold for goal intention; η is the threshold for plan in this way, if a plan has a missing PCond or ECond it can

                                                IJCAI-07
                                                  1459“subgoal”. (3) Support A belief links goals, plans or actions Contextual Relevance and Impact. The value of the nodes
which correspond to their GCond, SCond or ECond; this is in the FCM represent the contextual relevance of the corre-
a way to represent contextual conditions: goals, plans and sponding modules. The value of the edges in the FCM repre-
actions which are “well attuned” with the context are pre- sent the impact of the corresponding modules; by default they
ferred. (4) Feedback a plan or action feedbacks on goals; this are set according to the “epistemic component” of the mod-
special case of support permits to select goals having good ule: the value of a Belief, the GCond of the goal, the SCond
paths to action (5) Inhibition Goals and plans with conﬂict- or ECond of the plan. For example, if a belief has β =0.4
ing GConds or PConds, and plans realizing the same GCond and its sustains a goal, the impact of its edge in the FCM is
have inhibitory links. In this way it is possible for a goal or +0.4. The impact of the modules varies during the compu-
plan (especially if intended or adopted) to inhibit competi- tation; for example, an achievement goal which is close to
tors. (6) Contrast Beliefs have inhibitory links with plans satisfaction inhibits more and more its competitors.
and goals having conﬂicting GConds, PConds, SConds and  Not all the modules have to be represented at once in the
EConds: this is a kind of “reality check”.            FCM (and not all the threads have to run). FCM nodes hav-
                                                      ing contextual relevance equal (or close) to zero have no im-
The Cycle of the Reasoner                             pact and can be deleted (and the threads of the correspond-
The Reasoner (and the FCM) runs concurrently with all other
                         σ                            ing modules stopped): in this way only relevant knowledge is
modules, having an activation : in a real-time system, even considered, and the FCM never exceeds a certain size. This
reasoning takes resources. The Reasoner has two main tasks: feature is very useful in means-ends analysis: at the begin-
(1) to deliberate (select a goal and a plan) and (2) to set the ning, only top-level plans are considered in the FCM; plans
activation of the modules. Both are realized in this cycle: (and the FCM) are ﬁlled in with subplans only as long as the
1. Set the values of the FCM nodes according to the activity activity proceeds. Knowledge augments in a bounded way,
level of the corresponding modules1, and their links; too, as long as conditions and Beliefs related to active Plans,
2. Run the FCM and obtain the values of the nodes; as ex- Actions and Goals are checked.
plained later, this value represents the contextual relevance 2.2 Beliefs, Conditions and Goals
(ConRel) of the corresponding modules;
                                                                                                  [
                                              ζ       All the declarative components use fuzzy logic Kosko,
3. The most active Goal is selected (if over a threshold ); if 1986]. All the conditions (goal conditions, pre and post con-
not already achieved, its status becomes Intended (intended ditions of plans and actions, etc.) are special kinds of beliefs.
Goals replace old intended ones). Otherwise, another Goal For example, a Belief (“Ofﬁce is far”) can be matched using
has to be selected. A recurrent connection (with weight θ)is
                                            2         fuzzy rules with the PreCondition of a Plan (“Ofﬁce is close”)
set for the Intended Goal, which thus gains activation . and generate a graded truth value. Also goal conditions share
4. The most active Plan for the intended Goal is selected (if this formalism; in this way they can be matched e.g. against
over a threshold η); its status becomes: Adopted. The plan post conditions in order to verify their satisfaction (e.g. the
is ﬁlled in with the intended goal: the Goal of the plan be- Goal “go to Ofﬁce” becomes more and more satisﬁed when
comes the GCond. If there is an already adopted Plan, it is the truth value of “Ofﬁce is close” increases).
stopped only if its PCond conﬂict with the conditions of the There are policies for both achieve and maintain goals. In
new adopted one. A recurrent connection (with weight κ)is Achievement goals (such as “reach Ofﬁce”), the contextual
set for the Adopted Plan.                             relevance increases on nearing the goal (when the truth value
5. If no Plans are possible for the intended Goal, its status of the GCond increases). In Maintain goals (such as “stay
becomes Waiting (and maintains the recurrent connection); a close to Ofﬁce”), the contextual relevance lowers on nearing
new Goal has to be intended (this is unlikely, since the evalu- (when the truth value of the GCond increases).
ation of a Goal also depends on how suitable are its plans);
6. If no goals or plans are over the thresholds ζ and η, the Intended vs. Instrumental Goals. In practical reasoning
thresholds lower and the cycle restarts. Otherwise sets the it is assumed that only one goal is Intended, but many goals
activity level of the modules to the value of the nodes in the can be active at once (and activate plans or actions); they are
FCM. Thus, even if the Reasoner runs concurrently with the named instrumental goals as opposed to intended ones; their
other modules, it resets their activity level only if a new Plan purpose is to favor the intention, e.g. by creating appropriate
is adopted (either or not a new Goal is intended). Resources contextual conditions. If the intended goal (or another instru-
boundedness is guaranteed by the parameter ϑ: the total mental goal) they depend on is achieved, they are stopped.
amount of activation to be assigned to all the modules can
be ﬁxed so that the computation never exceeds that threshold. 2.3 Plans
                                                      Plans are the main control structures in DiPRA; they do
  1Goals, plans, actions and beliefs can also be more or less rele- not depend on the Reasoner except for starting. Plans are
vant in absolute; this is represented by the AbsRel value, which is activated for satisfying an intended goal; once the plan is
also the value of a recurrent connection in the corresponding node adopted, a subset of their PCond is set as Goal. A Plan is
in the FCM (not shown in Fig. 1). As a result, the activation and basically an execution scheme, activating Actions and Goals
inﬂuence of more relevant/reliable modules grow faster than others.
                                                      from the ActionSet and subgoaling; this is their behavior:
  2[Castelfranchi and Paglieri, 2007] argues that some characteris-
tic supporting beliefs are also necessary for Intending a goal. Here - If the intended Goal is already achieved, the Plan returns
we do not check them and simply assume that it is always the case. immediately and no action is executed.

                                                IJCAI-07
                                                  1460- If any SCond or ECond is false, the Plan “delegates” their can lead to adopt an explicit plan leading to Ofﬁce. (2) By
satisfaction to other modules by passing them activation via indirectly introducing a pressure. Even not intended goals
the Predecessor links; subgoals activated in this way gain the have an inﬂuence which is proportional to their activation.
status of Instrumental.                               For example, in choosing between two plans, an active but
- If all the SCond and ECond are met, the Plan starts exe- not intended goal can do the difference (e.g. by reinforcing a
cuting the actions in the ActionSet, chaining them according plan whose PCond are close to its GCond, or by weakening a
to the connectives in the ActionSet3. Plans can load from plan whose PCond are far from its GCond). (3) By updating
the ActionSet not only actions, but even goals. This mecha- knowledge related to them (e.g. GCond); in this way more
nism produces the typical subgoaling of practical reasoning: beliefs which are pertinent to the goal (and in principle can
(sub)goals activate (sub)plans or actions, and so on. Even reinforce it or weaken the other ones) are produced.
goals activated in this way gain the status of Instrumental.
- Plans continue subgoaling and executing their body until all Epistemic Dynamics. Knowledge is distributed and avail-
the possible actions and subgoals fail. A failed plan returns able to different extent to deliberation, depending on the ac-
the control to the calling goal, which remains not satisﬁed and tivation level of the modules corresponding to beliefs. For
activates another plan. However, it is likely that unsuccessful example, not all the consequences of adopting a plan (e.g.
plans are stopped before exhausting all the possibilities; in subplans, PConds) can be considered in means-ends analy-
fact, if many conditions of a plan fail, despite commitment it sis, but only those currently available; this is why sometimes
weakens in the FCM and other plans replace it.        long term conﬂicts are discovered only after a plan is adopted.
                                                      This is represented by putting in the FCM only beliefs, plans
Plans and Subgoaling. There are two subgoaling mecha- and goals having a non-zero contextual relevance.
nisms realized by the plans: the ﬁrst one consists in activating It is important to note that more active Beliefs intervene
goals which realize their SCond and ECond (if they are not more into the computation: they activate more the Goals and
already realized); the second one consists in activating goals Plans they support. This aspect models their availability: for
instead of actions from the ActionSet. Both kinds of goals example, an highly active belief is ready to be exploited for
are Instrumental. At the same time, plans spread activation to reasoning and, if it sustains a goal, gives it more activation.
instrumental goals, which gain priority.                Beliefs are retrieved in an activity-based and bounded way:
                                                      not all the knowledge is ready to be used, but modules ac-
2.4  Actions                                          tively search for and produce knowledge (and that activity
                                                      takes time) with a bias toward knowledge useful in the con-
An Action is the minimal executable operation; typically it
                                                      text of the most active goals. Goals, plans and actions assign
consists in an interaction with the World Engine; but actions
                                                      an updated truth value to their conditions (and to related be-
can also check, add, remove or modify a Belief (epistemic
                                                      liefs) during their execution, for example by reading a sen-
actions). Actions are activated by goals whose GCond corre-
                                                      sor or asking memory; more active beliefs (receiving activa-
spond to their PCond or by Plans via the ActionSet.
                                                      tion from more active goals and plans) will auto-update their
                                                      truth value more frequently. Produced (or updated) beliefs
Post Conditions vs. Goals. In practical reasoning, not all are added to the current state (and to the FCM) and linked
the expected results of actions and plans are intended. When to the relevant goals, plans or actions: new knowledge can
the plan is adopted, one of its PCond (corresponding to the lead to intention reconsideration or to replanning. More ac-
intended goal) is selected and becomes the Goal; the same tive goals and plans can build longer means-ends chains and
happens to actions. Depending on the situation, actions and have more “up-to-date” knowledge, since they can perform
plans can be activated for different reasons: their post condi- more epistemic actions. The epistemic component of DiPRA
tions are the same, but the Goal is different.        (what the agent knows) is inﬂuenced by its current activity
                                                      (what the agent is doing): in practical reasoning a crucial role
2.5  Dynamics in DiPRA                                of Intentions is selecting relevant information.
Even if there is only one intention, many modules can be ac-
tive at once in DiPRA. Not intended and not adopted goals 3 Practical Reasoning in DiPRA
and plans have a certain amount of activation, too, which can
                                                      In DiPRA means-ends analysis, opportunity ﬁltering and ﬁl-
be used for fulﬁlling operations such as building up parts of
                                                      ter overriding are “weak constraints” of the same mechanism
the FCM, although these operations tend to be slower.
                                                      and, at the same time, provide suitable values to deliberation.

Goal-Driven Pressures. Goals represent desired states of
                                                      Means-Ends Analysis and    Deliberation. Means-ends
the system. In DiPRA an active goal “drives” the computation
                                                      analysis builds causal chains: what is necessary for achiev-
toward a certain result (such as “Ofﬁce”) in three ways: (1)
                                                      ing a goal. Deliberation evaluates utility: what is better for
By actively competing for being intended: in this way they
                                                      achieving a goal. These two activities are related. Means-
  3Actions are set independently on any control structure such as ends analysis consists in building causal chains of means (of
Plans, that only order them. Depending on the connectives the same plans, (sub)goals, conditions and actions) to achieve ends.
set of actions can be executed in different ways. For example, the Normally this process is incremental: even if declarative
OR connective can be used for running two actions in parallel. knowledge about plans and their effects is already available,

                                                IJCAI-07
                                                  1461in order to fulﬁll new goals a “chain of means” has to be built with existing states (or desired ones such as goals) are simply
anew. We have seen that the FCM is more and more ﬁlled in much less likely to be selected and, at the same time, become
with elements of this chain, as long as the analysis proceeds less and less relevant. This is mainly due to the inhibition and
(new nodes are added as the result of epistemic actions of contrast links, but also to the fact that selected goals and plans
the modules, e.g. a plan verifying its preconditions); and as create areas of high “relevance” around them: conditions and
long as the agent acts (its actions have consequences which beliefs which potentially activate them are very likely to be
can be added as beliefs). The rationale is that knowledge re- added to the FCM.
lated to the goals and plans (e.g. about conditions and ac- In general, some requisites of practical reasoning (such as
tions) becomes more and more “relevant” and is thus added opportunity analysis) are perhaps too strong; we argue that
to the FCM. Normally means-ends analysis is performed only a cognitive agent (with limited rationality and bounded re-
for top-level plans, which are not totally ﬁlled in. However, sources) implements weaker requirements. For example, an
plans whose chains (from top-level plans to terminal actions) intended goal or an adopted plan do not rule out their com-
are stronger (having reliable subplans and actions and true petitors, but simply gain more contextual relevance and weak-
conditions) are privileged, because top-level plans gain more ens the other alternatives, too. New goals can be intended and
activation from them. As long as the analysis proceed, with or new plans adopted if they are able to overwhelm the “weight”
without adopting a plan (e.g. if the threshold η is not reached, of the previous ones. Intention reconsideration (changing
or if there is another adopted plan), new knowledge about the Goal) or replanning (changing Plan) only occur when needed.
plan is added and it can make it more likely to be selected. Once a Goal is intended, it only has to be replaced if a goal
  Means-ends analysis, which is mainly qualitative, pro- which is more important or was previously intended but was
duces at the same time results which are suitable for delib- not executable becomes achievable. Once a Plan is adopted,
eration, because the utility of a course of actions depends it only has to be replaced if one of its ECond become false or
also on the availability of the conditions and the reliability if its Goal is no more intended. All these situations happen
of the actions. Since in the FCM the plan receives activation naturally in the FCM.
from all its conditions, while performing means-ends analy-
sis the “best” plans receive also more and more activation. It Commitment. The most distinctive point of a practical rea-
is also very likely that the most active plan has many PCond soning agent is that it is committed to its intentions (and to do-
and ECond already met (at least partially). In a similar way, ing what it plans). Commitment, however, comes in grades,
plans having highly reliable actions are more likely to be very since agents should also be able to be opportunistic and re-
active. In this way, deliberation exploits the results of means- vise their intentions. Commitment is implemented in BDI as
ends analysis: the values of the nodes in the FCM, built dur- a strict rule; in DiPRA is comes in grades and it is regulated
ing means-ends analysis, can be directly used for selection by two parameters, θ and γ. Commitment to a Goal or a Plan
(we provide as a simple heuristic: choose the highest one, but is also maintained by the structure of the links, since achieve-
more sophisticated ones are possible).                ment goals which are close to satisfaction impact more and
  All the preference factors normally related to Goals and more, and adopted plans are more and more reinforced by
Plans in the BDI (e.g. urgency, utility) are encoded into mod- their conditions which increase their truth value.
ules activation. Preference is mainly based on epistemic fac-
tors: Goals and Plans are activated by knowledge, that can be
explicitly represented (e.g. “Goal x is very important”), im- 4 A Case Study: The House Scenario
plicitly represented into the modules (e.g. a Pre Condition of We implemented the House Scenario (see Fig. 2) using the
a Plan) or encoded in the relations between the components framework AKIRA [akira, 2003] and the 3D engine [irrlicht,
(e.g. a link between a Goal and a Plan means that the Plan is 2003]; the House has ﬁve rooms and seven doors which open
able to satisfy the Goal). The rationale is that the belief struc- and close randomly. The agent we model is the Thief; it ap-
ture of an Agent motivates its choices and preferences; the pears in a random position in the house, having the achieve-
causal structure built by means-ends reasoning is also used ment goal to possess the valuable V (that is hidden in the
for deliberation. There are two main difference with practical house) and the maintenance goal to avoid the Guard (an
reasoning as traditionally implemented (e.g. in BDI): (1) con- agent which moves randomly, but when spots the thief moves
ditions satisfaction and action evaluation are treated as “weak straight toward it). The Guard and the Thief have the same
constraints”; (2) there is an active and bounded view of how size and speed, and a limited range of vision. Four imple-
knowledge to be evaluated is added.                   mentations of the Thief were tested: (1) DiPRA; (2) a base-
                                                      line (random system); (3) the A* algorithm [Hart et al., 1968]
                                                      (which has full knowledge of the environment, including the
Opportunity Analysis and Filtering. In traditional im- location of V, plans the shortest path to it but and replans
plementations of practical reasoning the consistency of new when something changes in the environment, e.g. a door
plans or goals with old ones is routinely checked; inopportune closes); (4) a classic BDI, based on [Rao and Georgeff, 1995]
plans and goals are ruled out. Eventually, an intention which (having the same goals, plans and beliefs of DiPRA).
is discarded because of its incompatibility can be reconsid- Percentage of success (having V without being captured by
ered in another mechanism, the ﬁlter overriding. In DiPRA the Guard) was measured in 100 runs: analysis of variance
these brittle and costly operations are replaced by “weak con- (ANOVA) shows that DiPRA (81%) performs signiﬁcantly
straints” in the FCM: plans or actions which PCond conﬂict better than the other strategies (p<0, 0001 in all cases):

                                                IJCAI-07
                                                  1462