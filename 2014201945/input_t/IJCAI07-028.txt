   A Call Admission Control Scheme using NeuroEvolution Algorithm in Cellular 
                                              Networks

                     Xu Yang                                            John Bigham 
                   MPI-QMUL                                 Department of Electronic Engineering 
      Information Systems Research Centre                    Queen Mary University of London, 
        A313, Macao Polytechnic Institute                           London E1 4NS, U.K. 
                Macao SAR, China                                john.bigham@elec.qmul.ac.uk
                xuy@mpi-qmul.org

                    Abstract                          As a user moves from one cell to another, the call requires 
                                                      reallocation of channels in the destination cell. This proce-
    This paper proposes an approach for learning call 
                                                      dure is called handoff.  If there are no available channels in 
    admission control (CAC) policies in a cellular net-
                                                      the destination cell, the call may be prematurely terminated 
    work that handles several classes of traffic with due to handoff failure, which is highly undesirable. The 
    different resource requirements. The performance 
                                                      potential performance measures in cellular networks are long 
    measures in cellular networks are long term revenue, 
                                                      term revenue, utility, CBR (which is calculated by the 
    utility, call blocking rate (CBR) and handoff failure number of rejected setup calls divided by the number of setup 
    rate (CDR). Reinforcement Learning (RL) can be 
                                                      requests) or handoff failure rate (CDR, which is calculated by 
    used to provide the optimal solution, however such 
                                                      the number of rejected handoff calls divided by the number 
    method fails when the state space and action space of handoff requests). CDR can be reduced by reserving some 
    are huge. We apply a form of NeuroEvolution (NE) 
                                                      bandwidth for future handoffs. However, the CBR may in-
    algorithm to inductively learn the CAC policies, 
                                                      crease due to such bandwidth reservation. Hence, reduction 
    which is called CN (Call Admission Control scheme of CBR and CDR are conflicting requirements, and optimi-
    using NE). Comparing with the Q-Learning based 
                                                      zation of both is extremely complex.
    CAC scheme in the constant traffic load shows that 
                                                        Several researchers have shown that the guard channel 
    CN can not only approximate the optimal solution  threshold policy, which a priori reserves a set of channels in 
    very well but also optimize the CBR and CDR in a 
                                                      each cell to handle handoff requests, is optimal for mini-
    more flexibility way. Additionally the simulation 
                                                      mizing the CDR in a non-multimedia situation. However the 
    results demonstrate that the proposed scheme is   computational complexity of these approaches becomes too 
    capable of keeping the handoff dropping rate below 
                                                      high in multi-class services with diverse characteristics con-
    a pre-specified value while still maintaining an ac-
                                                      text, and exact solutions become intractable [Soh and Kim, 
    ceptable CBR in the presence of smoothly varying  2001]. [Choi, 2002] suggest a bandwidth reservation scheme 
    arrival rates of traffic, in which the state space is too 
                                                      using  an effective mechanisms to predict a mobile terminal 
    large for practical deployment of the other learning 
                                                      (MT)’s moving direction and reserve bandwidth dynamically 
    scheme.                                           based on the accurate handoff prediction. However this 
                                                      scheme incurs high communication overheads for accurate 
                                                      prediction of a MT’s movement.
1 Introduction                                          There are a variety of artificial intelligence techniques that 
Next Generation Wireless Systems are expected to support have been applied to the CAC schemes, such as RL [Senouci, 
multimedia services with diverse quality of services (QoS), 2004]. However RL can be difficult to scale up to larger 
such as voice, video and data. Due to the rapid growth in domains due to the exponential growth of the number of state 
mobile users and scarce radio resources, CAC has become variables. So in complex real world situations learning time 
vital to guarantee the QoS for the multiple services and util- is very long [Sutton, 1998].  
ize the network resources [Ahmed, 2005].                This paper proposes a novel approach to solve the CAC in 
  Generally a cellular network has a limited number of multimedia cellular networks with multiple classes of traffic 
bandwidth units (BWU) or channels, which could be fre- with different resource and QoS requirements, and the traffic 
quencies, time slots or codes depending on the radio access loads can vary according to the time. A near optimal CAC 
technique used, viz, FDMA, TDMA, or CDMA respectively. policy is obtained through a form of NE algorithm called 
Arriving calls are accepted to or rejected from access to the NeuroEvolution of Augmenting Topologies (NEAT) 
network by the CAC scheme based on the predefined policy. [Stanley, 2004]. By prioritizing the handoff calls and per-
                                                      ceiving the real time CBR and CDR, the proposed learning 


                                                IJCAI-07
                                                   186scheme called CN can be trained offline and the learned best Generally the outputs are the possible actions that can be 
policy can dynamically adjust the CAC policy to adapt to the performed in the real application. In CAC, there are only two 
varying of traffic loads: when the perceived CDR become possible actions: Accept and Reject, so one output is enough. 
higher, the system will reject more setup calls to decrease the We define the output is a real number, and its value is be-
CDR, and vice versa.                                  tween 0 and 1, if it is larger than 0.5, then the action selected 
  The learned policies are compared to a greedy scheme, is Accept; otherwise, it is Reject. 
which always accepts a call if there is enough bandwidth 3. How to evaluate each policy and how to formulate the 
capacity to accept this call, and a RL CAC scheme, which is fitness function? 
similar as the scheme proposed in [Senouci et al., 2004].The The fitness function determines what is good or bad pol-
simulation results shows that our proposed scheme can learn icy. A good policy has a high fitness score and so will have a 
very flexible and near optimal CAC policies, which can higher probability to create offspring policies. The fitness 
maintain the QoS constraints in the presence of smoothly function is described in a later section. 
changing arrival rate of traffic, a scenario for which the state 4. Internal and External Supervisor 
space is too large for practical deployment of the other During the learning period, many NNs are randomly gen-
learning scheme evaluated.                            erated and some of them can lead to very bad performance. 
  The paper is organized as follows: Section 2 gives a brief To prevent damage or unacceptable performance, an Internal 
introduction to NEAT, which is a form of NE Method, and Supervisor is needed to filter out clearly bad performance 
describes how to apply the NEAT to the CAC application; policies. In CAC, NNs that always reject all kinds of calls are 
section 3 describes the test system model, and formulates the frequently generated and so would be evaluated. However 
fitness function, while section 4 compares the performance in these Always Reject policies are obviously not good and 
constant and time-varying traffic loads.              evaluation is pointless. So the Internal Supervisor gives their 
                                                      fitness score the value of 0.
2  Applying NEAT to CAC                                 Additionally, the actions generated by NEAT are not al-
                                                      ways feasible. For example, during evaluation the evolved 
NEAT is a kind of NE method that has been shown to work NNs may try to accept a request call when the system is full, 
very efficiently in complex RL problems. NE is a combina- which is physically unrealizable. Therefore an External Su-
tion of neural networks and genetic algorithms where neural pervisor is added and uses prior knowledge in order to fil-
networks (NNs) are the phenotype being evaluated. [Stanley, tering impossible actions due to the system constraints.
2004]
  The NEAT method for evolving artificial NNs is designed 2.2  The Work Process of Setting up a connection   
to take advantage of neural network structure as a way of 
minimizing the dimensionality of the search space. The Figure 1 illustrates the work process of setting up a connec-
                                                      tion using our scheme.
evolution starts with a randomly generated small set of NNs 
with simple topologies. Each of these NNs is assigned a 
                                                              Input 0 and 1 
fitness value depending on how well it suits the solution.                      Update real-time CBR or CDR 
                                                             Real-time CBR and 
Once all the members of the population are assigned fitness  CDR

values, a selection process is carried out where better indi- Input 2 
viduals (high fitness value) stand a greater chance to be se- Ongoing setup calls NEAT
lected for the next operation. Selected individuals undergo  in class I 
recombination and mutation to result in new individuals.      Input 3        Selected NN 
                                                             Ongoing handover 
Structural mutations add new connections and nodes to        calls in class I 
                                                                                                Valid 
                                                                               Internal  External
                                                                                                Output
networks in the population, leading to incremental growth.   Input 4          Supervisor Supervisor 
The whole process is repeated with this new population until Ongoing setup calls 
                                                             in class II 
some termination criterion is satisfied. [Stanley, 2004]                      population
  This section specifies the main requirements of applying    Input 5 
                                                             Ongoing handover 
NEAT to the CAC application, and describes the work proc-    calls in class II 
ess of setting up a connection. 
                                                              Input 6 
                                                             The new user request
2.1  Main Requirements                                                         Cell
                                                                                        User Request 
NEAT can be seen as a black box, which can provide a neural 
network for receiving the inputs and generating the outputs. 
There area several issues need to be considered:              Figure 1 the process of setting up a connection 
  1.  What are the inputs? 
  Normally the inputs are the perceived state of the envi- 1. The network cell receives a user request event (in this 
ronment that is essential to make the action decision.  For the case from a network traffic simulator) , perceives the 
CAC problem the perceived state may include the new re-     network state (such as checking all the ongoing con-
quest call and all the currently carried connections in the cell.  nections in the cell, calculates the real time CBR and 
  2.  What are the outputs?                                 CDR),  and send the inputs to NEAT; 


                                                IJCAI-07
                                                   187  2.  The NN currently being evaluated in NEAT generates calls and handover calls in class i .The relationship below 
      the output according to the received inputs, and sends also holds. 
      the output to the External Supervisor;
                                                          kNis,,pand               kN          ih    p,
  3.  If the output is invalid as determined by the prede- is,,,mmis,               ih                ih
      fined rules in the External Supervisor, a Reject action      ()is,, ih                () is ,, ih
      will be sent to the cell. This occurs only when the        ii00
      capacity of cell is at its upper limit and the output 
                                                        Where pis, and pih, denotes the acceptance rate of new 
      decision of the CAC policy is still Accept. In this way setup request calls and handover calls in class i .
      the policy will not be punished by negative fitness 
                                                             the number of accepted setup calls in class i
      value for creating impossible actions, because during pis,
      exploration it is very difficult to create a policy that     request setup calls in class i
                                                             the number of accepted handover calls in class i
      can always generate correct actions for any possible p
      environment.                                        ih,      request handover calls in class i
  4.  The cell performs the output action, which means to 
      accept or reject the request event.             To simplify the above formula, define the service demand 
  5.  If the decision is to accept, the cell will allocate the parameter  as 
      requested bandwidth; if the decision is to reject, the 
                                                              ris,,11           and     r      ih
      cell will not take any action.                       is,,,, imm       is      ih   i            ih
                                                                    is,, ih                   is ,, ih
3  Fitness Function                                             ii00
                                                                        m
Our objective of CAC is to accept or reject request (setup or Therefore F1,,,,()ispp is ih ih                   (2)   
hand off) calls so as to maximize the expected revenue over            i 0
an infinite planning horizon and meet the QoS constraints, It can be seen that the total revenue is determined by both 
and more specifically to trade off CBR and CDR so that the of the service demand parameters  and the learned CAC 
specified CDR will not exceed a predefined upper bound performance pis, and pih, , which can be calculated after 
while retaining an acceptable CBR level.              evaluation of each individual policy.
  In this paper we only consider non-adaptive services, e.g. 
the bandwidth of a connection is constant. We assume m
                                                      3.2 Handle CBR and CDR trade-off--   F2 .
classes of traffic: {1, 2,...m } . A connection in class i  con-

sumes bi  units of bandwidth or channels, and the network can Two parameters are used to trade off between CBR and 
obtain ri   rewards per logical second by carrying it. We also CDR: the Importance Factor and the Penalty Fitness f .
assume that traffic from each class arrives according to The Importance Factor indicates the preference of the sys-
Poisson distribution and their holding time is exponentially tem as shown in Equation (3). For a network it is commonly 
distributed. All the arrival distributions and call holding more desirable to reject new setup requests and accept more 
distributions are independent of each other. The arrival handoff requests, so normally is,, is ih ,, ih .  By  carefully 
events include new call arrival events and handoff arrival selecting for each kind of traffic, the expected CAC policy 
events. Since the call departures do not affect the CAC de- can be evolved. The fitness function is shown as below: 
cisions, we only consider the arrival events in the CAC state     mm
                                                             Fppff()()       (3) 
space. Additionally, we only consider a cell with fixed ca-   2,,,,,,is is is ih ih ih      is ,, ih
pacity. The fixed total number of bandwidth (channels) is C .    ii00
                                                        The other parameters in Equation (3) are the Penalty Fit-
  We denote  is,  and ih,  as the arrival rates of new setup 
                                1       1             ness f .When the observed CBR or CDR exceeds their pre-
and handover requests in class i , is, and ih,  as the av-
                                                      defined upper limit bound ( T and T ), the evaluated policy 
erage holding time of setup and handover calls in class i .                   cdr   cbr
                                                      needs to be punished by a negative fitness score f , which 
                                                                                                 F
3.1 First Goal: maximize revenue- F1                  should be large enough to affect the total fitness 2 . Addi-
                                                      tionally, although dropping handoff calls is usually more 
If the goal is simply to maximize revenue, the fitness can be 
                                                      undesirable than blocking a new connection, designing a 
assessed by F1  , which can be calculated from the average 
reward obtained per request connection as equation (1). Let system with zero CDR is practically impossible and will lose 
N  be the total number of request connections (includes unacceptable revenue. In practice, an acceptable CDR is 
setup and handoff calls), n is the total number of accepted around 1 or 2%. fis, and fih,  give an opportunity to prevent 
connections.                                          the network from losing too much revenue by rejecting setup 
                 Nm                                   calls. Generally, we consider that CDRs are more important 
                  Rrk11          k
                   n     i is,, is ih ,, ih           than CBRs, therefore f  f .
  For large N, F ni10                          (1)                       ih,, is
              1  NN                                     These two parameters   and f work cooperatively to 
   Here m  denotes the total number of classes of traffic, i trade off between CBRs and CDRs:  gives extra weight for 
denotes the class of traffic for each individual connection, each different kind of traffic to indicate the preference of the 

kis, and kih,   denotes the number of the accepted new setup network, fih, can increase the speed to find policies that can 


                                                IJCAI-07
                                                   188decrease CDR under Tcdr , and fis, defines the upper limit of              C                C
                                                          Parameters        1                 2
Tcbr to trade off between CBRs and CDRs.                             Setup Handover Setup Handover 
4  Training Design and Performance Com-                    i         13.33 4.44 3.33           2.22 
                                                                       3 10 5 80 
    parison                                                i
                                                           ii         40 44.44 16.67 177.88 
This section presents two kinds of training processes: one for 
constant traffic load, another for time-varying traffic load.      Table 2.CN simulation parameters 
We use computer simulations to evaluate the performance of 
the learning approach. A single cell within a cellular network During learning period, NEAT evolves 13 generations, each 
with a limited number of channelsC 20 is considered. We generation had 200 policies, and each policy was evaluated 
                                                      for 20,000 events.
define two classes of traffic labeled C1  and C2 , which require 
bandwidth capacity 1 and 2 respectively. The reward rate is RL based CAC scheme 
r  1/sec/connection for carrying each connection.     We implemented a lookup table based Q-Learning CAC 
  To evaluate the traffic load in each cell, we choose to scheme (RL-CAC), which is similar to the one proposed in 
normalized offered load with respect to the radio bandwidth [Senouci, 2004]. The Q-function is learned through the fol-
capacity in a wireless cell, which is defined as “Normalized lowing update equation: 
offered load” mentioned in [Soh, 2001] . 
                   2                                  Qsa,,(,)max',', Qsa rsa          Qsa      Qsa
                1         11                                                        a'
             Lbbis,,      is i ih ,, ih i
                C i 1                                  The reward r(s,a) assess the immediate payoff due to a 
  The simulation is divided as two periods: a Learning Pe- action decision in time t that it is in state s  and has taken 
riod and an Evaluation Period. During the Learning Period action a . Since the network can obtains the rewards by car-
CN is trained to evolve higher fitness policies. During the rying each connection in a while, and receive nothing when 
evaluation period, the highest fitness policy is selected to just accept or reject it, the immediate reward is calculated by 
perform CAC.                                          the total rewards obtains during the time between two 
                                                      consecutive events: one is happened in time t , and another is 
4.1 Constant traffic load                             the previous event. 
                                                                                  1
In this experiment the CN is compared with a Q Learning RL  Learning rate t              ,
based scheme and a Greedy Policy, all for a constant nor-                   1(,)visittt s a t
malized offered load. The traffic parameters is defined in 
                                                            where visittt( s , a t ) is the total number of times this 
Table 1. 
                                                            state-action pair has been visited. 
                                                            Discount factor 01.
                     C1               C2
    Parameters                                              Exploration probability 1 .
               Setup Handover Setup Handover 
                                                            Important factor  is used to differentiate each kind 
      1
                0.2 0.1 0.1 0.05                            of traffic, and prioritize the handoff calls. 
      1
      i         30 20 20 15 
                                                                 1, s      1, h     2,s      2,h
               Table 1.Traffic parameters 
                                                                 5 10 1 80 

The proposed scheme--CN                                          Table 3.Important Factor in RL-CAC 
Training in constant traffic loads, the service demand pa-
rameter  doesn’t change, so the fitness is only concerned Greedy Policy 
with the learned CAC policy pis, and pih,  . CN requires five The system always accepts a request if there is enough 
inputs. Input 0 indicates the setup utility of different kinds of bandwidth capacity to accept the new call. 
request event (the utility is calculated as equation(4)), and 
input 1 to 4 indicates the number of ongoing calls for each Performance Comparison 
kinds of traffic carrying in the cell. For better performance Three CN schemes are compared with RL-CAC and the 
these inputs are normalized from 0 to 1.              Greedy Policy. The total CBR and CDR used in Fig 2 
             (importance factor) (reward rate) r      evaluate the total rejected rate for setup and handoff calls. 
       Utility                                           (4) 
                  b (bandwidth capacity)                As shown in Fig 2, the CN-C01, CN-C02 has similar per-
   Equation (3) is used as the fitness function, and Table 2 formance with RL-CAC, which is evidence that the CN 
shows the value of the related parameters. Comparing the scheme obtains near optimal policy.  
                                                        Table 4 and 5 shows that by defining different thresholds 
values of ii, the setup calls of class C2 has lest value, 
therefore it has the most probability to be rejected. for each kinds of traffic, CN can learn very flexible CAC 
                                                      policies, and not only trade-off between setup and handoff 
                                                      calls but also trade-off between different classes of traffic, 


                                                IJCAI-07
                                                   189which is difficult implemented by RL based CAC Schemes. the changes of traffic loads. The simulation results 
Because for such constraints semi-Markov decision problem demonstrate that the CBR and CDR can be good candidates. 
(SMDP), the state space should be defined by QoS con- Because using any non-adaptive CAC policy, when the traf-
straints, it becomes too large, and some form of function fic load increases the CBR and CDR will become higher, the 
approximation is needed to solve the SMDP problem.    CBR and CDR can be a factor to reflect the traffic loads.
                                                        With time-varying traffic loads CN requires seven inputs 
                                                      (Figure 1): Input 0 indicates the real-time CBR calculated by 
             Total CBR and CDR Comparison             the number of rejected setup calls divided by the latest 500 

        20.00                                         setup requests.  If the number of setup requests is less than 
        18.00                           Greedy 
        16.00                                         500, then the number of rejected setup calls is divided by the 
        14.00                           RL-CAC        number of setup request; Input 1 indicates the real-time CDR 
        12.00                           CN-C01
        10.00                                         calculated in a similar way as input 0; Input 2 to 6 are same as 
        8.00                            CN-C02
       100%                                           the inputs in the experiment with constant traffic loads. 
        6.00
                                        CN-C03
        4.00                                                    IF  310ppp     5    80 p                     (5) 
        2.00                                                          1,shs 1,   2,    2, hf
        0.00
               TotalCBR     TotalCDR                    0.70
                                                                                   0.70
                                                        0.65                                 (b)
                                                                    (a)            0.65
                                                        0.60
                                                                                   0.60
         Figure 2. Total CBR and CDR Comparison         0.55
                                                                                   0.55
                                                        0.50
             T                   T                                                 0.50
              cbr                 cdr                   0.45
                     Total CBR           Total CDR                                 0.45
 CN-C02 10%           9.91%       1%      0.87%         0.40                       0.40

                                                        0.35                      Normalized  Offered Loads L 0.35
 CN-C03 20%           18.07% 0.50% 0.37%               L   Loads  Offered Normalized
                                                        0.30                       0.30
                                                           12345678910               0  200 400 600 800 1000
             Table 4.CN schemes comparison                     Training Intervals        Evaluation Intervals
  CAC              Total Total
         Parameters           C     C    C    C
 Schemes           CDR   CBR   1,h   2,h  1, s 1, s         Figure 3 Traffic loads during learning and evaluation 
 RL-CAC CBR or CDR 1.68% 8.33% 1.1% 2.88% 3.49% 18.54%
                                                        The experiment is still divided into two periods: the 
         Upper limit No No 1% 2% 10% 10% 
 CN-C01                                               Learning Period and Evaluation Period. In the Learning 
         CBR or CDR 1.15% 8.97% 0.79% 1.89% 8.53% 9.91% Period, each policy is trained with ten different normalized 
                                                      offered loads, which is called one Training Session (Figure 
           Table 5. CN and RL-CAC comparison          3.a). The load was generated by segmenting the Training 
   In addition,  Simulation results show that by average CN Session into 10 time intervals where during each training 
schemes lost around 25% of revenue during learning by interval the load was held at the constant level.  The changes 
                                                           1
filtering clear bad performance policies.             of  i  follow a sine function as shown in equation(6).  Each 
                                                      policy is evaluated interval by interval and obtains an interval 
4.2 Time-varying traffic load                         fitness score calculated by interval fitness function IF as
In real world, the traffic load in a cellular system varies with equation (5). After finishing the evaluation of the 10th in-
time. A dynamic CAC policy which can adapt to the     terval, the ten interval fitness scores are averaged as the final 
time-varying traffic loads is required: when the handoff fitness of the policy. 
failure rates grows higher, the system will reject more setup        111n
                                                                 learning min min  sin(     )         (6)
calls in order to carry more handoff requests, and vice versa.                        10  18
In this dynamic environment, most learning schemes trained 

in constant traffic loads cannot obtain the optimal perform-           111n
                                                                  evaluation min min sin(  )                   (7)
ance and maintain the QoS constraints.                                                  1000
  To simplify the problem, only the arrival rates of each kind 
of traffic are changed, the averages of holding times are kept 
                                                                           C                C
as constant during the whole simulation.                  Parameters        1                 2
  In this dynamic environment, the state space is very large,        Setup Handover Setup Handover 
                                                            1
and until now there is no technical report to provide any    min     0.25 0.125 0.2             0.1 
optimal solution by using RL algorithm. Therefore we only   1
compare our schemes with Greedy Policy.                      i        25 20 10                  5 
  In the time-varying traffic loads, the service demand factor 
   varies according to the time and is difficult to be calculate, Table 6.The traffic definition in time-varying traffic loads 
We use equation (5) to ignore the , however it does not The Evaluation Period evaluates the learned best policy. It 
adapt to the changes of offered loads. In this case the CAC is divided into 1000 equal lengths of time intervals with 
scheme requires  feedback from the environment to indicate 


                                                IJCAI-07
                                                   190