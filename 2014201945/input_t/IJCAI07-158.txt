                   Local Search for Balanced Submodular Clusterings

                    M. Narasimhan∗†                                   J. Bilmes†
                         Live Labs                            Dept. of Electrical Engg.,
                   Microsoft Corporation                      University of Washington
                   Redmond WA 98052                               Seattle WA 98195
              mukundn@microsoft.com                      bilmes@ee.washington.edu

                    Abstract                          distances. Submodularity allows us to model these decom-
                                                      posable criteria, but also allows to model more complex crite-
    In this paper, we consider the problem of produc- ria. However one problem with all of these criteria is that they
    ing balanced clusterings with respect to a submodu- can be quite sensitive to outliers. Therefore, algorithms which
    lar objective function. Submodular objective func- only optimize these criteria often produce imbalanced parti-
    tions occur frequently in many applications, and  tions in which some parts of the clustering are much smaller
    hence this problem is broadly applicable. We show than others. We often wish to impose balance constraints,
    that the results of Patkar and Narayanan [8] can be which attempt to tradeoff optimizing Jk with the balance con-
    applied to cases when the submodular function is  straints. In this paper we show that the results that Patkar
    derived from a bipartite object-feature graph, and and Narayanan [8] derived for Graph Cuts are broadly appli-
    moreover, in this case we have an efﬁcient ﬂow    cable to any submodular function, and can lead to efﬁcient
    based algorithm for ﬁnding local improvements.    implementations for a broad class of functions that are based
    We show the effectiveness of this approach by ap- of bipartite adjacency. We apply this for clustering words in
    plying it to the clustering of words in language  language models.
    models.
                                                      2   Preliminaries and Prior Work
                                                         V                          Γ:2V  →  R
1  Introduction                                       Let   be a ground set. A function        ,deﬁnedon
                                                      all subsets of V is said to be increasing if Γ(A) ≤ Γ(B) for
The clustering of objects/data is a very important problem all A ⊆ B. It is said to be submodular if Γ(A)+Γ(B) ≥
found in many machine learning applications, often in other Γ(A ∪ B)+Γ(A ∩ B), symmetric if Γ(A)=Γ(V \ A),and
guises such as unsupervised learning, vector quantization, di- normalized if Γ(φ)=0. For any normalized increasing sub-
                                                                         V     +                 V     +
mensionality reduction, image segmentation, etc. The clus- modular function Γ:2 → R , the function Γc :2 → R
tering problem can be formalized as follows. Given a ﬁnite deﬁned by Γc(X)=Γ(X)+Γ(V \X)−Γ(V ) is a symmetric
set S, and a criterion function Jk deﬁned on all partitions of S submodular function. This function is called the connectivity
into k parts, ﬁnd a partition of S into k parts {S1,S2,...,Sk} function of Γ, and is normalized, symmetric and submodu-
so that Jk ({S1,S2,...,Sk}) is maximized. The number of                 Γc(X)=Γc(V   \ X)=Γc(X, V    \ X)
                                         n            lar. We can think of
k-clusters for a size n>kdata set is roughly k /k! [1] so as the cost of “separating” X from V \ X. Because Γc is
exhaustive search is not an efﬁcient solution. In [9],itwas submodular, there are polynomial time algorithms for ﬁnding
shown that a broad class of criteria are Submodular (deﬁned the non-trivial partition (X, V \ X) that minimizes Γc.Such
below), which allows the application of recently discovered normalized symmetric submodular functions arise naturally
polynomial time algorithms for submodular function mini- in many applications. One example is the widely used Graph
mization to ﬁnd the optimal clusters. Submodularity, a for- Cut criterion.
malization of the notion of diminishing returns, is a powerful Here, the set V to be partitioned is the set of vertices of a
                                                                                                       +
way of modeling quality of clusterings that is rich enough to graph G =(V,E). The edges have weights we : E → R
model many important criteria, including Graph Cuts, MDL, which is proportional to the degree of similarity between the
Single Linkage, etc. Traditionally, clustering algorithms have ends of the edge. The graph cut criterion seeks to partition
relied on computing a distance function between pairs of the vertices into two parts so as to minimize the sum of the
objects, and hence are not directly capable of incorporating weights of the edges broken by the partition.
complicated measures of global quality of clusterings where For any X ⊆ V ,let
the quality is not just a decomposable function of individual
                                                        γ(X)=   set of edges having at least one endpoint in X
  ∗
   Part of this work was done while this author was at the Univer- δ(X)= set of edges having exactly one endpoint in X
sity of Washington and was supported in part by a Microsoft Re-
search Fellowship.                                    For  example,  if X     =    {1, 2, 5} (the red/dark-
  †This work was supported in part by NSF grant IIS-0093430 and shaded set in Figure 1-left), then γ(X)=
an Intel Corporation Grant.                           {(1, 2), (1, 3), (2, 3), (2, 4), (2, 5), (5, 4)} (the set of edges

                                                IJCAI-07
                                                   981                                                      
                                                               w (f)
                         6           f                  f∈γ(X)  F   , which can be shown to be a normalized in-
                                                      creasing submodular function for any positive weight func-
                                                                     +
                         5           e                tion wF : V → R ,andΓc(X)=Γ(X)+Γ(V      \ X)− Γ(V )
                                                      measures the weight of the common features. For the ex-
                                                                                            X  =  {1, 2, 5}
                         4           d                ample shown in Figure 1-right, if we take
      5     6                                         (the red/dark-shaded set), then γ(X)={a, b, d, e},and
                                                      δ(X)={b, d, e}
                         3           c                              . We will refer to this as the Bipartite Adja-
      2     4     8                                   cency Cut criterion.
                                                                Γ
                         2           b                  Because  c is symmetric and submodular, we can use
                                                      Queyranne’s algorithm [4] to ﬁnd the optimal partition in
      1     3     7                                             3
                                     a                time O(|V | ). There are two problems with this approach.
                         1                                                             3
                                                      First, since the algorithm scales as |V | , it becomes imprac-
                                                               |V |
Figure 1: Left: The Undirected Graph Cut Criterion: Γc(X) tical when becomes very large. A second problem is that
is the sum of weights of edges between X and V \ X. Right: the criterion is quite sensitive to outliers, and therefore tends
The Bipartite Adjacency Criterion: Γc(X) is the number of to produce imbalanced partitions in which one of the parts is
elements of F (features) adjacent to both X and V \ X. substantially smaller than the others. For example, if we have
                                                      a graph in which one vertex is very weakly connected to the
                                                      rest of the graph, then the graph cut criterion might produce
which are either dashed/red or solid/black) and δ(X) is a partitioning with just this vertex in a partition by itself. For
the set of solid/black edges {(1, 3), (2, 3), (2, 4), (5, 4)}. many applications it is quite desirable to produce clusters that
It is easy to verify that for any (positive) weights that are somewhat balanced. There is some inherent tension be-
                                    +
we assign to the edges wE : E →  R   , the function  tween the desire for balanced clusters, and the desire to mini-
                                                                                          Γ (X)
Γ(X)=wE(γ(X)) =         e∈γ(X) wE (e) is a normalized mize the connectivity between the clusters c : we would
                                                                                  Γ  (X)
increasing submodular function, and hence the function like to minimize the connectivity c , while making sure
                                                      that the clustering is balanced. There are two similar criteria
Γc(X)=wE   (δ(X)) = wE(γ(X))+wE   (γ(V \X))−wE(E)     that capture this optimization goal

is a normalized symmetric submodular function1. We will                            Γ (V )
                                                                     (V ,V )=       c  1
refer to this as the Undirected Graph Cut criterion.         ratioCut  1  2   w  (V ) · w (V )
  In this paper, we will be particularly interested in a slightly               V  1    V   2
different function, which also falls into this framework. V                          Γc (V1)
                                     F                       normCut (V1,V2)=
will be the left part of a bipartite graph, and the right part                min(wV  (V1),wV (V2))
of the graph. We think of V as objects, and F a set of fea-
tures that the objects may posses. For example, V might be a
vocabulary of words, and F could be features of those words The two criteria are clearly closely related. Unfortunately,
(including possibly the context in which the words occur). minimizing either criterion is NP-complete [11],andsowe
Other examples include diseases and their symptoms, species need to settle for solutions which cannot necessarily be shown
and subsequences of nucleotides that occur in their genome, to be optimal. The normalized cut is also closely related to
and people and their preferences.                     spectral clustering methods [6], and so spectral clustering has
  We construct a bipartite graph B =(V,F,E) with an edge been used to approximate normalized cut. In this paper, we
between an object v ∈ V and a feature f ∈ F if the object o present a local search approach to approximating normalized
                                                 +
has the feature f. We assign positive weights wV : V → R cut. The advantage of local search techniques is that they al-
               +
and wF : F →  R  . The weight wF (f) measures the “im- low us to utilize partial solutions (such as a preexisting clus-
portance” of the feature f, while the weight wV (v) is used to tering of the objects) which is useful in dynamic situations.
determine how balanced the clusterings are. In some applica- Further, since local search techniques produce a sequence of
tions, such as ours, there is a natural way of assigning weights solutions, each one better than the last, they serve as anytime
to V and F (probability of occurrence for example). In this algorithms. That is, in time-constrained situations, we can
case, for X ⊆ V , we can set                          run them only for as much time as available. The local search
                                                      strategy we employ will allow us to make very strong guar-
 γ(X)={f   ∈ F : X ∩ ne(f)  = ∅}                      antees about the ﬁnal solution produced. Such a local search
 δ(X)={f   ∈ F : X ∩ ne(f)  = ∅, (V \ X) ∩ ne(f)  = ∅} technique was originally proposed by Patkar and Narayanan
                                                      [8] for producing balanced partitions for the Graph Cut crite-
where ne(·) is the graph neighbor function. In other words, rion. In this paper, we show that the results in his paper are
if X bi-partitions V into sets of two types (X and V \ X) equally applicable for any submodular criterion. It should be
of objects, then γ(X) is the set of features with neighbors noted that the applicability of these general techniques to any
of “type X”, and δ(X) is the set of features with neigh- submodular function does not necessarily make it practical.
bors of both types of object. We let Γ(X)=wF (γ(X)) = One of the contributions of [8] was to show that it could be
                                                      done efﬁciently for the Graph Cut criterion via a reduction
  1                                           +
   we use the same notation for the function wE : E → R de- to a ﬂow problem. In this problem, we show that the Bipar-
                             E    +
ﬁned on edges and the function wE :2 → R , the modular exten- tite Adjacency criteria can also be solved in a similar efﬁcient
sion deﬁned on all subsets                            fashion by reducing to a (different) ﬂow problem.

                                                IJCAI-07
                                                   9823  Local Search and the Principal Partition           Proposition 1 (Narayanan 2003, Proposition 7). Let (V1,V2)
                                                                     V    V =  V ∪ V      V ∩ V  = φ
In a local search strategy, we generate a sequence of solutions, is a bipartition of (so 1 2, and 1 2 ), and
                                                         φ  = U ⊂ V                   V
each solution obtained from the previous one by a (sometimes let  1 be a proper subset of 1 satisfying
small) perturbation. For the case of clustering or partitioning,
                                                                           Γc (V1) − Γc (V1 \ U)
this amounts to starting with a (bi)partition V = V1 ∪ V2,       μ(G, V )=
                                                                       1         w  (U)
and changing this partition by picking one of a set of moves.                      V
This set of moves that we will consider is going from the bi-
                                                      Then ratioCut (V1 \ U, V2 ∪ U) < ratioCut (V1,V2)
partition {V1,V2} to the bipartition {U1,U2},wherethenew
partition is obtained by moving some elements from one par-
                                                      Proof. By assumption,
tition to the other. For example, we could go from {V1,V2} to
{V1 \ X, V2 ∪ X}      X ⊆  V1
               ,where        . This amounts to moving                                Γc (V1) − Γc (V1 \ X)
the elements in X from V1 to V2. The key to a local search         μ(G, V1)=   max
                                                                             φ=X⊆V        w  (X)
strategy is have a good way of generating the next move (or                        1        V
        X  ⊆  V               X                                               Γ (V ) − Γ (V \ U)
to pick a      1 so that moving to the other side will                     =   c  1     c  1
improve the objective function). In this section, we show that                      wV (U)
when Γc is a submodular function, then the Principal Par-
tition of the submodular function (to be deﬁned below) can In particular, for X = V1 we have
be used to compute the best local move in polynomial time.
Moreover, for the speciﬁc application we discuss in this pa- Γ (V ) − Γ (V \ V ) Γ (V ) − Γ (V \ U)
                                                         c  1    c  1    1 ≤   c  1     c  1
per, we can actually compute this fast even for very large data w (V )              w  (U)
sets.                                                          V  1                  V
  For any bipartition V = V1 ∪ V2,andX ⊆ V1,let
                                                      Since Γc(V1 \ V1)=0,wehave
           GGain (X)=Γc  (V1) − Γc (V1 \ X)

                       GGain (X)                                    Γc (V1)   Γc (V1) − Γc (V1 \ U)
      averageGain (X)=                                                     ≤
                        wV (X)                                      wV (V1)   wV (V1) − wV (V1 \ U)

             μ(G, V1)=  max   averageGain (X)                       a    a−c
                       φ=X⊆V1                        Observe that if b ≤ b−d ,thenab − ad ≤ ab − bc and so
                                                      a    c
So, for Figure 1-left, if we assign a weight of 1 to all edges b ≥ d . Therefore, we get
and all vertices, (so wE ≡ 1 and wV ≡ 1), and let V1 be the
                      V                                               Γc (V1)   Γc (V1 \ U)
set of blue/light nodes and 2 be the set of red/shaded nodes.                ≥                        (1)
Then                                                                 wV (V1)   wV (V1 \ U)
  GGain ({6})=Γc  (V1) − Γc (V1 \{6})=4−  6=−2
                                                      Dividing both sides by wV (V2),weget
In Figure 1-right (the bipartite graph), we get
                                                                               Γ (V )
   GGain ({6})=Γc  (V1) − Γc (V1 \{6})=3− 3=0                                   c  1
                                                          ratioCut (V1,V2)=
GGain (X) measures the amount of change in the partition                   wV (V1)wV (V2)
cost Γc (V1) (ignoring the balance constraints). Now, we are                   Γ (V  \ U)
                                                                         ≥      c  1
                                   (V1,V2)
really interested in the change in normCut ,whichin-                       wV (V1 \ U)wV (V2)
corporates the balance constraint. μ(G, V1) can be seen to be
                                                                        [            ]
related to the ratio/normalized cut, and so we use solutions to          by Equation 1
μ(G, V1) to ﬁnd the set of moves for the local search algo-                      Γ (V  \ U)
                                                                         >        c  1
rithm. In this section, we present some results which relate               wV (V1 \ U)wV (V2 ∪ U)
changes in       (V1,V2) to the principal partition of the
         normCut                                                        [       w  (V  ∪ U) >w   (V )]
submodular function Γc, and for this, μ(G, V1) will play a               because  V  2         V   2
central role. The principal partition of a submodular function           = ratioCut (V1 \ U, V2 ∪ U)
Γc consists of solutions to minX⊆V1 [Γc (X) − λ · wV (X)]
for all possible values of λ ≥ 0. It can be shown [7;
8] that the solutions for every possible value of λ can be com- We have a similar (but not strict) result for normalized cuts.
puted in polynomial time (in much the same way as the entire
regularization path of a SVM can be computed [12]). We will Corollary 2. Under the same assumptions of the previous
give speciﬁcs of the computation procedure in Section 4. In proposition, we have
this section, we will present results which will relate the so-
                                                            normCut (V1 \ U, V2 ∪ U) ≤ normCut (V1,V2)
lutions of minX⊆V1 [Γc (X) − λ · wV (X)] with solutions to
μ(G, V )=max                    (X)
      1       φ=X⊆V1 averageGain   .                 Proof. Since wV (V1) >wV (V1 \U), and from Equation 1, it
  The following proposition was proven by [Narayanan  follows that Γc (V1) > Γc (V1 \ U). We consider two cases.
2003] for the Graph Cut criterion, but generalizes immedi- First, assume that wV (V1) ≤ wV (V2). In this case, from the
ately for an arbitrary increasing submodular function Γc.In normCut deﬁnition and Equation 1,
particular, this is applicable to the problem we are interested
in which the submodular function is derived from the bipartite                Γc (V1)   Γc (V1 \ U)
                                                            normCut (V1,V2)=          ≥
graph.                                                                        wV (V1)   wV (V1 \ U)

                                                IJCAI-07
                                                   983Hence                                                 Proof. Suppose that λ = μ(G, V1), Then there is a φ  = W ⊆
                                                      V
Γc (V1)   Γc (V1)                                      1 so that
        ≥                        [ because |V1|≤|V2|]
wV (V1)   wV (V2)                                                            Γc (V1) − Γc (V1 \ X)
                                                               λ = μ(G, V1) ≥
          Γc (V1 \ U)                                                              wV (X)
       >                [ because Γc (V1 \ U) < Γc (V1)]
            wV (V2)                                   with equality holding for X = W . It follows that
          Γ  (V \ U)                                  Γ  (V ) − Γ (V \ X) ≤ λ · w (X)
        >   c  1       [       w  (V ∪ U) >w   (V )]   c   1    c  1            V
          w  (V ∪ U)    because  V  2         V  2
           V   2                                                          = λ · wV (V1) − λ · wV (V1 \ X)
Hence
                                                      because wV is always positive. Hence
                   Γc (V1)
 normCut (V1,V2)=
                   w  (V )                              Γc (V1) − λwV (V1) ≤ Γc (V1 \ X) − λ · wV (V1 \ X)
                    V  1                      
                         Γ (V  \ U)  Γ (V  \ U)
                 ≥ max    c  1     ,  c  1            Because the left hand side is a constant, it follows that
                         wV (V1 \ U) wV (V2 ∪ U)
                                                        Γc (V1) − λwV (V1) ≤ min [Γc (V1 \ X) − λwV (V1 \ X)]
                 = normCut (V1 \ U, V2 ∪ U)                                 X⊆V1

For the remaining case, assume that wV (V1) ≥ wV (V2).
Again using Equation 1, we have                       Note that equality holds for X = W . In particular, for Z =
                                                      V1 \ W ,weget
        Γc (V1)   Γc (V1 \ U)   Γc (V1 \ U)
                >            >                           [Γ (V ) − λ · w (V )] = min [Γ (X) − λ · w (X)]
        wV (V2)     wV (V2)    wV (V2 ∪ U)                c   1       V   1          c           V
                                                                               X⊆V1
It follows that                                                           =[Γc  (Z) − λ · wV (Z)]
                         Γ (V )  Γ  (V )
                          c  1    c   1                                 Z = V  \ W
 normCut (V1,V2)=max            ,                     Therefore, by taking   1     , the forward direction fol-
                         wV (V1) wV (V2)                                                           λ>0
                                                    lows. For the reverse direction, suppose that for some ,
                         Γ (V  \ U)  Γ (V  \ U)       we have
                 ≥ max    c  1     ,  c  1
                         wV (V1 \ U) wV (V2 ∪ U)         min [Γc (X) − λ · wV (X)] = [Γc (V1) − λ · wV (V1)]
                                                         X⊆V1
                 = normCut (V1 \ U, V2 ∪ U)
                                                      Then by taking W = V1 \ X,weget

                                                       [Γc (V1 \ W ) − λwV (V1 \ W )] ≥ [Γc (V1) − λ · wV (V1)]
  The two previous results show that if we can ﬁnd a non-
                                                      Therefore,
trivial solution to maxφ=X⊆V1 averageGain (X), then we can
ﬁnd a local move that will improve the normalized cut and the λ · wV (V1) − λ · wV (V1 \ W )=λ · wV (W )
ratio cut. Ideally, we want to show that if it is possible to im-                 ≥ Γ (V ) − Γ (V  \ W )
prove the normalized cut (or the ratio cut), we can in fact ﬁnd                      c  1     c  1
a local move that will improve the current solution. Unfortu- because W  =0and wV > 0,wehave
nately we do not have such a result, but we have one that is
slightly weaker which serves as a partial converse.           Γc (V1) − Γc (V1 \ W )
                                                2         λ ≥
Proposition 3. Suppose that φ  = U ⊂ V1 satisﬁes α ·                wV (W )
normCut (V1,V2) ≥ normCut (V1 \ U, V2 ∪ U) where α =
 wV (V2)       Γc(V1\U)   Γc(V1)
w (V ∪U) .Thenw  (V \U) ≤ w (V ) .
 V  2          V  1       V  1                          Therefore,  if  we   can   compute   solutions to
                                                      min      [Γ (X) − λ · w (X)]
Proof. See Appendix.                                      X⊆V1  c           V    , then we can ﬁnd a lo-
                                                      cal move that will let us improve the normalized cut. In the
  Now, the previous result shows the existence of a set which next section, we show how we can compute these solutions
can be moved to the other side which will let us improve the efﬁciently for our application.
current value of the normalized cut. However, an existence
result is not enough. We need to be able to compute this set. 4 Computing the Principal Partition
The following theorem gives a connection between this set
and the Principal Partition of the Bipartite Adjacency func- Proposition 1 tells us if we have a partition (V1,V2),then
tion. Since we can compute the principal partition, we can if we can ﬁnd a set φ  = U ⊆ V1 satisfying μ(G, V1)=
explicitly compute a local move which will improve the nor- Γc(V1)−Γc(V1\U)
                                                          wV (U)    , then we can improve the current partition
malized cut.                                          by moving U from V1 to the other part of the partition. Propo-
Proposition 4 (Narayanan, 2003, Proposition 6). λ =   sition 4 tells us that we can ﬁnd such a subset by ﬁnding λ so
μ(G, V1) iff there is a proper subset Z ⊂ V1 such that that

   min [Γc (X) − λ · wV (X)] = [Γc (V1) − λ · wV (V1)]   min [Γc (X) − λ · wV (X)] = [Γc (V1) − λ · wV (V1)]
  X⊆V1                                                   X⊆V1

                          =[Γc  (Z) − λ · wV (Z)]                                =[Γc (U) − λ · wV (U)]


                                                IJCAI-07
                                                   984                                                                                    
                                                      n                   (w   )=     k     (w |w    ) ≈
While this can be done in polynomial time for any submodu- -grams of words: Pr 1:k  i=1 Pr  i  1:i−1
lar function [7; 8], in this section, we show that it can be done n−1       k
                                                        i=1 Pr(wi|w1:i−1) · i=n Pr(wi|wi−n+1:i−1) The prob-
                                                                                                 n
especially efﬁciently in our case by reducing it to a paramet- lem is that the number of n-grams grows as |W | ,where
ric ﬂow problem. For parametric ﬂow problems, we can use W
            [ ]                                          is the set of words in the vocabulary. As this grows
the results of 2 , to solve the ﬂow problem for all values exponentially with n, we cannot obtain high-conﬁdence
of the parameter in the same time required to solve a single statistical estimates using naive methods, so alternatives
ﬂow problem. Now, for a ﬁxed parameter λ, we can com-
    min     [Γ (X) − λ · w (X)]                       are needed in order to learn reliable estimates with only
pute   X⊆V1   c           V     by solving a max ﬂow  ﬁnite size training corpora. Brown et al. [3] suggested
problem on the network which is created as follows. Add a clustering words, and then constructing predictive models
                                                      based only on word classes: If c(w) is the class of word w,
           λ · wV (6) 6        F wF (F )              then we approximate the probability of the word sequence
                                                      w         (w   )  ≈    n−1   (w |c(w  ),...,c(w )) ·
                                                      1:k by Pr   1:k       i=1 Pr  i   i−1         1
           λ · wV (5) 5        E wF (E)                 k
                                                        i=n Pr(wi|c(wi−1),...,c(wi−n+1)).   In  this case,
           λ · w (4)             w (D)                the number of probabilities needing to be  estimated
               V                  F                                      n−1
                   4          D                       grows  only  as |C|     ·|W |.    Factored language
 S                                              T     models  [10] generalize this further, where we  use
           λ · wV (3) 3        C wF (C)               Pr(wi|wi−1,c(wi−1),...,wi−n+1,c(wi−n+1))  —    note
                                                      that conditioning on both wi−1 and c(wi−1) is not redundant,
           λ · wV (2) 2        B wF (B)               as backoff-based smoothing methods are such that if, say, an
                                                      instance of wi,wi−1 was not encountered in training data,
           λ · w (1)             w (A)
               V   1          A   F                   an instance of wi,c(wi−1) might have been encountered
                                                                                                   
                                                      via some other word w    = wi−1 such that wi,w  was
Figure 2: A ﬂow network to compute the Principal Partition                c(w)=c(w      )
of the Bipartite Adjacency Cut                        encountered, and with           i−1 . Often, we can
                                                      construct such models in a data-dependent way.
source node S, and connect S to all the nodes in v ∈ V with The quality of these models depends crucially on the qual-
edge capacity λ · wV (v). Add a sink node T , and connect ity of the clustering. In this section, we construct a bipartite
all the nodes in f ∈ F to T , with capacity wF (f) as shown adjacency graph, and use the algorithm described above for
in Figure 2. The remaining edges (from the original graph) generating the clusters. While the algorithm described in this
have inﬁnite capacity. By the max-ﬂow/min-cut theorem, ev- paper only generates a partition with two clusters, we can ap-
ery ﬂow corresponds to a cut, and so we just examine the cuts ply it recursively (in the form of a binary tree) to the gener-
in the network. It is clear that the min cut must be ﬁnite (since ated clusters to generate more clusters (stopping only when
there is at least one ﬁnite cut), and hence the only edges that the number of elements in a cluster goes below a prescribed
are part of the cut are the newly added edges (which are ad- value, or if the height of the tree exceeds a pre-speciﬁed
jacent to either S or T ). Hence if a vertex v ∈ V is on one limit). The bipartite graph we use is constructed as follows:
side of the cut, all its neighbors must be as well. Therefore, V and F are copies of the words in the language model. We
                                                                   v ∈ V         f ∈ F           f
every cut value is of the form wv(V \ X)+wF (γ(X)) =  connect a node     to a node     if the word follows
                                                              v
λ · wV (V ) − λ · wV (X)+wF (X). Minimizing this function the word in some sentence. Ideally, we want to put words
is equivalent to minimizing wF (γ(X)) − λ · wV (X). We can which have the same set of neighbors into one cluster. The
compute this for every value of λ by using the parametric ﬂow model as described ignores the number of occurrences of a
algorithm of [2].In[2], it is also shown that there are distinct bigram pair. However, we can easily account for numbers by
solutions corresponding to at most |V | values of λ,andfur- replicating each word f ∈ F to form f1,f2,...,fk,where
ther, the complexity of ﬁnding the solutions for all values of awordv ∈ V is connected to f1,f2,...,fr if the bigram
λ is the same as the complexity of ﬁnding the solution for a vf occurs r times in the text. It is very simple to modify
single value of λ (namely that of a ﬂow computation in this the network-ﬂow algorithm to solve networks of this type in
network). This algorithm returns the values of λ correspond- the same complexity as the original network. The goal is to
ing to the distinct solutions along with the solutions. Since partition the words into clusters so that words from different
there are at most |V | distinct solutions, each one of them can clusters share as few neighbors as possible (and words from
be examined to ﬁnd the one which results in the maximum the same cluster share as many neighbors as possible). Ob-
improvement of the current partition (i.e., the local move that serve that this criterion does not require us to compute “dis-
improves the normalized cut value by the most). Since the tances” between words as is done in the clustering method
                                      2                                     [ ]
complexity of the ﬂow computation is O(|V | |E|),theﬁnal proposed by Brown et al. 3 . The advantage of our scheme
search through all the distinct solutions does not add to the over a distance based approach is that it more naturally cap-
complexity, and hence the total time required for computing tures transitive relationships.
                        2
a local improvement is O(|V | |E|).                     To test this procedure, we generated a clustering with 497
                                                      clusters on Wall Street Journal (WSJ) data from the Penn
                                                      Treebank 2 tagged (88-89) WSJ collection. Word and (hu-
5  Word Clustering in Language Models                 man generated) part-of-speech (POS) tag information was
Statistical language models are used in many applications, extracted from the treebank. The sentence order was ran-
including speech recognition and machine translation, domized to produce 5-fold cross validation results using
and are often based on estimating the probabilities of (4/5)/(1/5) training/testing sizes. We compared our sub-

                                                IJCAI-07
                                                   985