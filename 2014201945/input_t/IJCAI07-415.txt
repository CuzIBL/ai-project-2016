                                 Global/Local Dynamic Models

             Avi Pfeffer           Subrata Das   and  David Lawless             Brenda Ng
         Harvard University           Charles River Analytics, Inc.   Lawrence Livermore Nat’l Lab
       Cambridge, MA 02138               Cambridge, MA 02138               Livermore, CA 94551
    avi@eecs.harvard.edu                   sdas@cra.com                     bmng@llnl.gov


                    Abstract                            One would hope that in such a system, it will be possi-
                                                      ble to exploit the largely independent nature of the entities
    Many dynamic systems involve a number of enti-    for efﬁcient inference. We are concerned with the inference
    ties that are largely independent of each other but task of monitoring, i.e. the computation of the probability
    interact with each other via a subset of state vari- distribution over the state of the system at each point in time
    ables. We present global/local dynamic models     given the history of observations up to that time point. One
    (GLDMs) to capture these kinds of systems. In a   popular approach to monitoring is particle ﬁltering (PF) [Is-
    GLDM, the state of an entity is decomposed into a ard and Blake, 1998], in which the state is estimated by a set
    globally inﬂuenced state that depends on other en- of particles. However in PF all the inference is performed
    tities, and a locally inﬂuenced state that depends globally, and the largely local structure is not exploited. In
    only on the entity itself. We present an inference particular, when there are many entities the global state is
    algorithm for GLDMs called global/local particle  high-dimensional, and PF will not perform well. Factored
    ﬁltering, that introduces the principle of reasoning particle ﬁltering [Ng et al., 2002] is an approach that attempts
    globally about global dynamics and locally about  to address this issue by decomposing particles into factors.
    local dynamics. We have applied GLDMs to an       However, all the dynamics propagation and conditioning on
    asymmetric urban warfare environment, in which    observations is still performed globally. As a result, factored
    enemy units form teams to attack important targets, PF still has a hard time inferring the correct local state from
    and the task is to detect such teams as they form. observations.
    Experimental results for this application show that
    global/local particle ﬁltering outperforms ordinary In global/local particle ﬁltering, we introduce a simple in-
    particle ﬁltering and factored particle ﬁltering. ference principle: reason globally about global dynamics, and
                                                      locally about local dynamics. Like in factored PF, particles
                                                      are decomposed into factors. Unlike factored PF, however,
1  Introduction                                       only the dynamics propagation for globally inﬂuenced state is
                                                      performed globally. Dynamics propagation for locally inﬂu-
Many systems involve a number of entities that interact with enced state and conditioning on observations are performed
each other. These entities may be largely independent of each locally. This allows global/local PF to more accurately take
other, and only interact with other entities via a subset of state into account the observations when inferring the posterior dis-
variables. An example is a collection of companies, that in- tribution over the local state of each entity.
teract via market conditions. Each company may have inter-
nal state that is conditionally independent of other companies We present an application of our ideas to the task of mon-
given the market conditions. Another example is the spread itoring goal formation of enemy units in an asymmetric ur-
of an infectious disease through a population. Each person ban warfare environment. In this task, there are a number of
may have an individual state, which corresponds to his or her small, mobile enemy units moving around an urban environ-
symptoms, and people will interact via their infectious states. ment. Sometimes the enemy units adopt the goal of attacking
  We present a type of model called global/local dynamic one of a number of possible targets. The units communicate
models (GLDMs) that allows the representation of these kinds with each other to form teams to attack a target. Our task is to
of systems. In a GLDM, the state of an entity is divided into detect when such teams have been formed, given the move-
two subsets: a locally inﬂuenced state that only depends on ments of the units and their communication.
the state of the entity itself, and a globally inﬂuenced state We present experimental results for our application. Our
that depends globally on the states of all entities. In this way, results show that global/local PF considerably outperforms
interactions between the entities are allowed, but much of the ordinary PF and factored PF at the task, and also outperforms
dynamic model is isolated within individual entities. Obser- a method that performs all its inference locally. Our method
vations are restricted to depend only on the state of individual scales reasonably well both to situations with twenty units
entities.                                             and to those with twenty target locations.

                                                IJCAI-07
                                                  25802  Global/local dynamic models                        usual way. The restrictions on the model are extended nat-
We are concerned with a dynamic system that evolves over urally from the non-factored case. The parents of a locally
time. We begin by describing a hidden Markov model    inﬂuenced variable may only be local variables of the same
(HMM)[Rabiner and Juang, 1986]. The state of the system at entity at the previous or current time step. The parents of a
time t is represented by a variable Xt. At each time t there is globally inﬂuenced variable may be any variable at the pre-
an observation represented by the variable Ot. The dynamic vious time step, and any globally inﬂuenced variable at the
system is deﬁned by a transition model P t(Xt|Xt−1); an ob- current time step. The parents of a local observation variable
servation model P t(Ot|Xt); and a distribution over the initial may be any locally or globally inﬂuenced variable of the same
state P 1(X1). We do not assume that the system is homoge- entity at the current time step.
neous, i.e. the transition and observation models may vary
                                                                t1                    t
from one time point to the next. The joint probability dis-
                                                               U 1                  U 1
tribution over a sequence of states and observations is given
                                                                                              t
by
                                                                                             O 1
                                                                t1                    t
                           T            T
P (x1,o1, ..., xT ,oT )=P 1(x1) P t(xt|xt−1) P t(ot|xt).        1                     1
                           t=2           t=1
                                                                t1                    t
  In a  global/local dynamic model (GLDM),welet                 2                     2
E1, ..., En be a set of entities. We assume that the state of                                 t
the system can be decomposed into a product of states of the                                 O 2
individual entities. Each entity Ei has a locally inﬂuenced     t1                    t
      t                             t
state Ui and a globally inﬂuenced state Vi . We denote the     U 2                  U 2
      t  t      t
pair Ui ,Vi  by Xi and call it the local state of entity i at
                    t     t                   t
time t. Also the tuple U1, ..., Un will be denoted by U and
  t     t      t                                          Figure 1: DBN representation of two-entity GLDM.
V1 , ..., Vn by V , and the variable representing the entire
state by Xt. Note that even the globally inﬂuenced state is
                                                        A GLDM can itself be viewed as a DBN, which is factored
called local state, because it pertains to a single entity. To
                                                      into variables representing the local states of individual enti-
summarize, the state space decomposes as
                                                      ties, and the observations. This is the case even if the local
    t     t     t      t  t     t   t      t  t
  X  = X1, ..., Xn = U1,V1 , ..., Un,Vn = U , V . states and observations are not themselves factored, as dis-
                                                      cussed in the previous paragraph. Such a DBN, for two enti-
                                                                                                        t
  In the transition model, the locally inﬂuenced state is con- ties, is shown in Figure 1. Note that there is an edge from V1
                                                          t                              t
strained to depend only on the previous state of the individual to V2 . No assumption is made that the Vi are conditionally
entity, and the current globally inﬂuenced state of the entity. independent.
No restriction is made on the globally inﬂuenced state, and For an example of a GLDM, consider a stock tracking and
no decomposition of the distribution of that part of the state prediction application, where each entity may be a particu-
is assumed. Thus the transition model decomposes into lar company. The locally inﬂuenced state may be the inter-
                             n                       nal state of the company, while the globally inﬂuenced state
    t  t   t−1     t  t  t−1      t  t  t−1   t
  P  (X |X   )=P   (V  |X   )   Pi (Ui |Xi ,Vi ).     may be the market conditions faced by the company. At each
                             i=1                      time point, the internal state of the company depends only
                                                      on its previous internal state and on the market conditions it
         t   t  t−1  t
Note that Pi (Ui |Xi ,Vi ) may be different for different en- faces, whereas the market conditions are all dependent on all
tities Ei.                                            the previous market conditions. The observations may be the
                                                  t
  The observation decomposes into a local observation Oi stock prices of individual companies.
of each entity Ei that depends only on the local state of that For another example, consider an application of tracking
entity. Thus the observation model decomposes into    the spread of an infectious disease through a population. Here
                         n                           the entities may be people, the locally inﬂuenced state may
              t   t  t        t  t  t                 be the symptoms of the person, while the globally inﬂuenced
             P (O |X )=     Pi (Oi |Xi )
                         i=1                          state may be the stage of infection, if any, of the person. The
                                                      transition model for the globally inﬂuenced state may specify
  Just as the state of a HMM can be factored into the prod- that the infection stage of a person at time t depends on the
uct of variables to produce a dynamic Bayesian network infection stages at time t − 1 of the people with whom the
(DBN) [Dean and Kanazawa, 1989], so all the parts of a ﬁrst person comes into contact at time t.
GLDM   can be factored into variables. Thus the locally in- For a third example, which will be expanded on in Sec-
             t
ﬂuenced state Ui is factored into variables Ui,1, ..., Ui,m,and tion 4, consider the task of monitoring enemy units moving
similarly for the globally inﬂuenced state, and the local obser- around an urban environment. From time to time the units
vation. The factoring may be different for different entities. communicate and adopt goals of attacking possible targets.
The probabilistic models are factored into the product of con- Here the locally inﬂuenced state of an entity may consist of
ditional probabilities of variables given their parents in the its current position, while the globally inﬂuenced state is its

                                                IJCAI-07
                                                  2581goal. When a unit communicates with another unit, its goal bility distribution over the local states of all entities. This is
becomes dependent on the goal and position of the other unit exponential in the number of entities.
as well as itself. A unit’s position, however, depends only on Therefore approximate ﬁltering algorithms are used. One
its previous position and its current goal.           standard algorithm for DBNs is the Boyen-Koller algorithm
  Let us consider the expressive power of GLDMs. Trivially (BK) [Boyen and Koller, 1998]. In this algorithm, the vari-
they can capture any HMM or DBN, since we can have a sys- ables of the DBN are partitioned into clusters. In a GLDM,
tem with only one entity. It is more interesting to ask what can each cluster will be the locally and globally inﬂuenced state
naturally be captured using the structure of the model. One of one entity. An approximate belief state Pˆ is maintained as
                                                                            ˆ
question arises regarding the representation of a global state a product of distributions Pi over the clusters, i.e.
that applies to all entities, since each state in our represen-
                                                                               n
tation is a local state of a particular entity. Global state can Pˆ(Xt|o1, ..., ot)= Pˆ (Xt|o1, ..., ot).
be captured in a GLDM by introducing an additional entity                          i  i
representing the global state, and making its state globally in-               i=1
ﬂuenced. In the stocks example, there may be an entity repre- In principle, the method works by beginning with the
                                                                           ˆ   t−1 1     t−1
senting general economic conditions, that inﬂuences the mar- factored distributions Pi(Xi |o , ..., o ), multiplying
ket conditions faced by each company. However, this is only them to obtain Pˆ(Xt−1|o1, ..., ot−1), propagating through
legal if the global state does not inﬂuence locally inﬂuenced the dynamics and conditioning on the observation to ob-
states (e.g. the internal states of the companies). Another Pˆ(Xt|o1, ..., ot)
question involves the representation of observations. There tain        , and then marginalizng onto the fac-
                                                                    Pˆ (Xt|o1, ..., ot)
are no observations that depend on the state of more than one tors to obtain i i  .  The joint distributions
                                                       ˆ  t−1  1     t−1       ˆ  t  1     t
entity. We will see a way to get around this restriction in our P (X |o , ..., o ) and P (X |o , ..., o ) are not rep-
application in Section 4. The reason for this restriction is that resented explicitly. Instead, the factored distributions
                                                       ˆ   t 1     t
it will allow us, when performing inference as described in Pi(Xi |o , ..., o ) are computed more efﬁciently. One way to
Section 3.1, to condition on observations locally.    do that is to create a junction tree representing two time slices
  GLDMs bear a    superﬁcial resemblance to factorial of the DBN, in which each factor is contained in a clique at
HMMs  [Ghahramani and Jordan, 1997]. In a factorial HMM, both the previous and current time points. In practice, even
there are a number of hidden state sequences, each of which though this method often results in more efﬁcient inference
evolves independently. The observation depends jointly on than the exact method, sometimes the cliques of the junc-
all the hidden states. In fact GLDMs and factorial HMMs are tion tree are too large and the method is still too expensive to
quite different. Factorial HMMs cannot easily be modeled as be practical. This may particularly be a problem with some
GLDMs, because the observation depends on the joint state GLDMs, because no restrictions are placed on the way glob-
of all entities and not on the hidden state of a single entity. On ally inﬂuenced variables evolve.
the other hand, in factorial HMMs all sequences evolve inde- An alternative approach to approximate inference is to use
pendently, so there is no globally inﬂuenced state; the state of particle ﬁltering (PF) [Isard and Blake, 1998]. In PF, the joint
each entity is completely locally inﬂuenced.          distribution over the state variables is approximated by a set
                                                      of samples, or particles as they are called. Each particle con-
3  Inference                                          tains an assignment of values to the state variables. The prob-
                                                      ability of any property of the state is the fraction of particles
There are several inference tasks on dynamic systems, includ- that have that property. The basic steps of PF for a GLDM
ing diagnosing the past, predicting the future, and keeping are as follows.
track of the current state. In this paper, we will focus on the            t−1,1    t−1,M
latter task, known variously as monitoring, ﬁltering and state Begin with M particles x , ...,x .
estimation. The ﬁltering task is to compute, at each time point For m =1toM:
t P (Xt|o1, ..., ot)  o1, ..., ot                       Propagate:
,              ,where         is the sequence of obser-           t,m       t  t  t−1,m
vations obtained up to time t. The quantity P (Xt|o1, ..., ot) Sample vˆ from P (V |x ).
is known as the belief state at time t. In principle, this can be For each entity Ei:
                                                                    t,m       t  t t−1,m  t,m
computed simply using Bayesian updating:                    Sample uˆi  from Pi (Ui |xi , vˆi ).
       t 1     t                                        Condition:
  P (X |o , ..., o ) ∝                                             n   t  t t,m
                                                         wm  ←       P (o |xˆ ).
          P (xt−1|o1, ..., ot−1)P t(Xt|xt−1)P t(ot|Xt).            i=1 i  i i
      xt−1                                              Resample:
                                                               =1   M
In practice this is very difﬁcult because the state space may For  to  :
                                                            Choose xt, from xˆt,1, ..., xˆt,M , with probability
be very large. Even if the transition and observation models      xˆt,m                          w
are represented in factored form as in a DBN, the belief state that    is chosen being proportional to m.
cannot be decomposed and must be represented as an explicit The difﬁculty with PF for this problem is that the variance
joint distribution over the state variables, which is exponen- of the method is high and the number of particles required
tial in the number of variables. The same holds for GLDMs. for a good approximation generally grows exponentially with
After a certain amount of time, the local states of all the en- the dimensionality of the problem. Therefore this approach
tities become dependent on each other, and performing the does not scale well with the number of entities. An obser-
ﬁltering exactly requires a belief state which is a joint proba- vation is that the different entities are somewhat independent

                                                IJCAI-07
                                                  2582                                                                    t,m       t  t t−1,m  t,m
of each other. The different entities interact with each other Sample uˆi from Pi (Ui |xi , vˆi ).
only through the globally inﬂuenced state. If these interac- Condition:
                                                                     n   t  t t,m
tions are relatively weak, we might be able to take advantage wm ←   i=1 Pi (oi|xˆi ).
of that fact. We might expect that instead of maintaining par- Resample:
ticles that assign values to all variables for all units, we can For  =1to M:
maintain local particles that only assign values to variables       t,     t,1    t,M
                                                            Choose xi  from xˆi , ..., xˆi , with probability
belonging to a single unit. This is the idea behind factored       t,m
                                                              that xˆ is chosen being proportional to wm.
particle ﬁltering [Ng et al., 2002]. In factored particle ﬁlter-   i
ing, the state variables are divided into factors. The joint dis- Why does this method work? The key point is that in
tribution over all state variables is approximated by the prod- order for local propagation and conditioning to be success-
uct of marginal distributions over the factors, as in BK. Fur- ful, we don’t need to have exactly the right joint distribution
thermore, the marginal factor distributions are approximated over all the globally inﬂuenced states. It is enough that the
by a set of factored particles. Factored PF introduces two marginal distributions over individual entities’ globally inﬂu-
new steps into the PF process described above. The ﬁrst joins enced states is approximately correct. If this happens, when
factored particles together to produce global particles. The we condition the local state of each entity on the local ob-
second projects global particles back down onto the factors. servation, we will produce an approximately correct poste-
In between these two steps, all the usual steps of PF are per- rior distribution over the local state. This is much easier to
formed. In particular, propagating through the dynamics and acheive than producing an approximately correct joint poste-
conditioning on the observations are done with global parti- rior distribution.
cles.                                                   It is important to note that something is lost by propagat-
  For this reason, ordinary factored PF is also not ideal for ing and conditioning locally. We lose the ability to reason
our situation. The problem is that in any global particle, it is from observations of one entity to another entity. For exam-
highly likely that there will be some entities whose local state ple, in the domain of detecting goals of enemy units, we are
is far from the truth. Therefore, it will often be the case that unable to reason about the fact that since unit 1 is moving
for all global particles in the set of particles, the probability towards target 4, and we have some reason to believe that
of the observation will be extremely low. Even if one entity’s unit 1 and unit 3 are on a team, then it is likely that unit 3
local state in the particle is good, other entities’ states may be has a goal of attacking target 4. We do on the other hand
bad and so the observation will not conﬁrm the ﬁrst entity’s successfully reason about the interaction between the units
state. As a result, inference about entities’ true local states when reasoning globally, so we may infer that since unit 1
based on the observations will be poor.               and unit 3 appear to have formed a team, they are both a pri-
3.1  Global/Local Particle Filtering                  ori likely to be attacking target 4. We are just unable to use
                                                      the observation about unit 1 to conﬁrm our beliefs about unit
In order to allow observations about an entity to more effec- 3. The hope is that the inability to perform this type of rea-
tively condition its local state, we introduce global/local par- soning is outweighed by the fact that inferences about indi-
ticle ﬁltering. Global/local PF is based on the principle of vidual units from their own observations are more accurate.
reasoning globally about global dynamics and locally about The success of the global/local reasoning method will depend
local dynamics. The method involves a simple change to on this tradeoff, and how important this type of reasoning is
factored PF, but one that makes a big difference. Instead of in a particular application.
performing all the dynamics propagation globally, and con-
ditioning on observations globally, and only then projecting
down onto the individual factors, we project immediately af- 4 Application: Monitoring dynamic goals of
ter propagating the dynamics for the globally inﬂuenced vari- enemy units
ables. Propagation of dynamics for locally inﬂuenced vari- We have applied GLDMs to monitoring the dynamic goals
ables and conditioning on observations are performed locally. of enemy units in an asymmetric urban warfare environment.
The global/local PF process is as follows:
                            t−1,1    t−1,M            In this scenario, units move about on a streetmap, and adopt
Begin with M factored particles xi , ...,xi           goals of attacking one of a number of target locations. The
  for each entity Ei.                                 motion of units depends on their goals; a unit with a goal of
Join the factored particles for different entities together attacking a target will generally move in the direction of the
  to produce M global particles. (For details on the join target.
  process see [Ng et al., 2002]).                       The goals of units can change dynamically. A unit may
For m =1toM:                                          adopt a new goal in one of three ways: two units communi-
  Propagate globally:                                 cate and jointly agree to adopt a new goal; two units com-
            t,m      t   t t−1,m
    Sample vˆ  from P (V |x     ).                    municate and one invites the other to adopt its goal; or one
  Project:                                            unit spontaneously decides to adopt a goal. In all cases, the
            t−1,m  t,m
    Project x   , vˆ  down to                       goal adoption decision is inﬂuenced by the proximity of the
        t−1,m  t,m
      xi    , vˆi  for each entity Ei.              units to the goal; a unit will prefer to choose to attack a closer
For each entity Ei:                                   target.
  For m =1toM:                                          Our task is to detect threats such as ambushes to targets,
    Propagate locally:                                as soon as possible after they are formed. Two sources of

                                                IJCAI-07
                                                  2583                                                                      TP
evidence can be used. The ﬁrst is a noisy sensor of the cur- cision, which is TP+FP , i.e the fraction of threats reported
rent position of each unit. The second is a noisy indication by the algorithm that were really threats, and recall, which
                                                          TP
of whether or not a unit communicates. Communication pro- is TP+FN, i.e. the fraction of real threats caught by the al-
vides some indication that two units are forming a team. This gorithm. One parameter of the algorithm is the threshold of
is a weak inference, however. Even when a unit communi- probability at which a threat is reported. We varied this prob-
cates, usually the communication will not be related to goal ability in each experiment, thereby trading off precision for
adoption.                                             recall. In all experiments, we adjusted the number of particles
  We model this scenario with a GLDM in which each entity allocated to each algorithm so that they all had approximately
corresponds to a unit. Since a unit’s movement depends only the same running time.
on its own goal, the position of an entity is a locally inﬂu- Figure 2(a) shows the precision-recall curves for each
enced state. Units interact with other units in adopting goals, method for experiments with ten units and six target loca-
therefore the goal of a unit is a globally inﬂuenced state. In tions. The graph shows the recall that could be achieved
the dynamic model, the new goal of a unit depends on its pre- for different levels of precision. Also shown for reference is
vious goal and position, and the previous goal and position of the performance of random guessing. While all methods do
a unit with which it communicates. Since this could be any better than random guessing, our method does best, getting
unit, it depends on all the units’ goals and positions. much higher precision while still achieving high recall. At
  The positional observation of each unit is a local obser- one point it achieves 56% precision with 87% recall. This is
vation. Whether or not two units communicate, however, de- quite good performance considering the difﬁculty of the task.
pends on the goals and positions of both units, so the commu- When there are a number of units moving about the map, it
nication observations cannot be adequately captured in our is quite likely that at some point in time several units will
framework as local observations. To get around this issue, appear to be moving towards a target, even though in actual
we use the technique of evidence reversal [Kanazawa et al., fact they have no intent of attacking it. Thus if we wish to
1995]. Instead of having the observation depend on the state, achieve a high recall, we cannot avoid having a relatively low
we condition the transition model on the observation. That precision. Interestingly, factored PF performs very poorly,
is, for each possible conﬁguration of the communication sen- indicating that it is not simply the factoring that leads to the
sors, we have a different transition model. This is allowed good performance of our method, but reasoning locally about
since we have not assumed that the dynamic model is ho- unit positions. Also, note the relatively poor performance
mogeneous. In the inference process, we can sample from of the method that does all the reasoning locally. This shows
this conditioned dynamic model as follows: We ﬁrst sample that the global part of global/local PF is important.
a conﬁguration of actual communications given the noisy ob- Figure 2(b) shows how the algorithms scale up to a situa-
servations. We then sample which pairs of units communicate tion with 20 units. The task is more difﬁcult, because there is
from the list of communicating units. For each pair of com- more opportunity for units to appear to be heading towards a
municating units, we determine whether they jointly adopt a goal in the course of their business. Again our method does
new goal or one joins the other’s team, based on their previ- best, at one point achieving 57% precision with 76% recall.
ous goals and their proximity to the targets. Then, for each Figure 2(c) shows the performance when the number of tar-
unit whose goal has not been determined by communication, gets is increased to 20. This is a much harder task, because
we determine whether it spontaneously adopts a new goal. some targets are close to each other and it is difﬁcult to iden-
                                                      tify a unit’s goals. Nevertheless, our method is able to achieve
5  Experiments                                        relatively good performance, getting 55% precision with 51%
We tested the global/local particle ﬁltering algorithm on sim- recall.
ulated data generated from the model, and compared its per-
                                                                    1
                                                                                       All evidence
                                                                                  No communication evidence
formance to ordinary PF and factored PF. We also constructed       0.9              No position evidence
an algorithm in which all interactions between units are ig-       0.8
nored and all inference is performed locally, and compared         0.7
our algorithm to that.                                             0.6
                                                                   0.5
                                                                 Recall
  Each experiment consisted of one run of the system, with         0.4

units moving about and choosing targets. Each run of the sys-      0.3
tem lasted 100 time steps. A threat, which was deﬁned to be        0.2
four units sharing a common goal, was considered to be suc-        0.1
                                                                    0
                                                                     0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
cessfully detected if it was discovered within 12 time steps                   Precision
of its development. This was enough time for each unit to
reach two intersections on average, and was considered to be Figure 3: Comparison of performance without different evi-
the minimum amount of time in which our system could rea- dence.
sonably be expected to detect goal-directed behavior. If the
threat was not detected within that time, the result was a false Figure 3 assesses the relative importance of each of the two
negative. If a threat was reported when none was present, sources of evidence, observations about the position of units,
the result was a false positive. For each experiment, we ran and about communications. We see that evidence from posi-
500 runs and counted the number of true positives (TP), false tional observations is more important, but taking communica-
positives (FP), and false negatives (FN). Our metrics are pre- tions into account is also useful. Surprisingly, the method that

                                                IJCAI-07
                                                  2584