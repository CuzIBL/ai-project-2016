        Solving POMDPs Using Quadratically Constrained Linear Programs

               Christopher Amato     and  Daniel S. Bernstein   and  Shlomo Zilberstein
                                    Department of Computer Science
                                      University of Massachusetts
                                          Amherst, MA 01003
                                 {camato,bern,shlomo}@cs.umass.edu

                    Abstract                          obtained by inspecting the manufactured products. Numer-
                                                      ous other POMDP applications are surveyed in [Cassandra,
    Developing scalable algorithms for solving par-   1998b].
    tially observable Markov decision processes         Developing effective algorithms for MDPs and POMDPs
    (POMDPs) is an important challenge. One ap-       has become a thriving AI research area [Cassandra, 1998a;
    proach that effectively addresses the intractable Feng and Hansen, 2002; Feng and Zilberstein, 2005; Hansen,
    memory requirements of POMDP algorithms is        1998; Littman et al., 1995; Meuleau et al., 1999; Poupart and
    based on representing POMDP policies as ﬁnite-    Boutilier, 2003]. Thanks to these new algorithms and im-
    state controllers. In this paper, we illustrate some provements in computing power, it is now possible to solve
    fundamental disadvantages of existing techniques  very large and realistic MDPs. In contrast, current POMDP
    that use controllers. We then propose a new       exact techniques are limited by high memory requirements to
    approach that formulates the problem as a quadrat- toy problems.
    ically constrained linear program (QCLP), which
                                                        POMDP exact and approximate solution techniques cannot
    deﬁnes an optimal controller of a desired size. This
                                                      usually ﬁnd near-optimal solutions with a limited amount of
    representation allows a wide range of powerful
                                                      memory. Even though an optimal solution may be concise,
    nonlinear programming algorithms to be used to
                                                      current exact algorithms that use dynamic programming of-
    solve POMDPs.   Although QCLP optimization
                                                      ten require an intractable amount of space. While work has
    techniques guarantee only local optimality, the
                                                      been done to make this process more efﬁcient [Feng and Zil-
    results we obtain using an existing optimization
                                                      berstein, 2005], the time and space complexities of POMDP
    method show  signiﬁcant solution improvement
                                                      algorithms remain serious challenges. POMDP approxima-
    over the state-of-the-art techniques. The results
                                                      tion algorithms can operate with a limited amount of mem-
    open up promising research directions for solving
                                                      ory, but as a consequence they provide very weak theoretical
    large POMDPs using   nonlinear programming
                                                      guarantees, if any. In contrast, we describe a new approach
    methods.
                                                      that bounds space usage while permitting a principled search
                                                      based on the optimal solution.
1  Introduction                                         Current techniques used to ﬁnd optimal ﬁxed-size con-
                                                      trollers rely solely on local information [Meuleau et al., 1999;
Since the early 1990s, Markov decision processes (MDPs) Poupart and Boutilier, 2003]. We present a formulation of
and their partially observable counterparts (POMDPs) have an optimal ﬁxed-size controller which represents an optimal
been widely used by the AI community for planning under POMDP policy of a given controller size. To illustrate some
uncertainty. POMDPs offer a rich language to describe sit- of its beneﬁts, we employ a standard nonlinearly constrained
uations involving uncertainty about the domain, stochastic optimization technique. Nonlinearly constrained optimiza-
actions, noisy observations, and a variety of possible objec- tion is an active ﬁeld of research that has produced a wide
tive functions. POMDP applications include robot control range of techniques that can quickly solve a variety of large
[Simmons and Koenig, 1995], medical diagnosis [Hauskrecht problems [Bertsekas, 2004]. The quadratic constraints and
and Fraser, 1998] and machine maintenance [Eckles, 1968]. linear objective function of our formulation belong to a spe-
Robots typically have sensors that provide uncertain and in- cial class of optimization problems for which many robust
complete information about the state of the environment, and efﬁcient algorithms have been developed. While these
which must be factored into the planning process. In a medi- techniques only guarantee locally optimal solutions, our new
cal setting, the internal state of the patient is often not known formulation may facilitate a more efﬁcient search of the so-
with certainty. The machine maintenance problem, one of the lution space and produces high-quality results. Moreover,
earliest application areas of POMDPs, seeks to ﬁnd a cost- the optimization algorithms employ more advanced tech-
effective strategy for inspection and replacement of parts in niques than those previously used [Meuleau et al., 1999;
a domain where partial information about the internal state is Poupart and Boutilier, 2003] to avoid convergence at certain

                                                IJCAI-07
                                                  2418suboptimal points.                                     For a given node q and variables xa and xq,a,o
  The rest of the paper is organized as follows. We ﬁrst give Maximize , for
an overview of the POMDP model and explain how a solu- Improvement constraints:
tion can be represented as a stochastic controller. We brieﬂy            
                                                       ∀sV(q, s)+  ≤      xaR(s, a)+
discuss some of the previous work on solving POMDPs using              a                          
                                                                                                 
these controllers and then show how to represent an optimal γ s P (s |s, a) o O(o|s ,a) q xq,a,oV (q ,s)
controller using a quadratically constrained linear program
(QCLP). We conclude by demonstrating for a set of large Probability constraints:
POMDPs that our formulation permits higher valued ﬁxed-   a xa =1and ∀a    q xq,a,o = xa
size controllers to be found than those generated by previ-           
                                                       ∀axa  ≥ 0 and ∀q ,a,oxq,a,o ≥ 0
ous approaches while maintaining similar running times. Our
results suggest that by using our QCLP formulation, small,
                                                                                             x     x 
high-valued controllers can be efﬁciently found for a large Table 1: The linear program for BPI. Variables a and q ,a,o
                                                              P (a|q)   P (q,a|q, o)             q
assortment of POMDPs.                                 represent      and           for a given node, .
2  Background                                                           
                                                      V (q, s)=  a P (a|q) R(s, a)+
                                                                                                     
A POMDP can be deﬁned with the following tuple:                                                   
                                                        γ  s P (s |s, a) o O(o|s ,a) q P (q |q, a, o)V (q ,s)
  M  = S, A, P, R, Ω,O, with
                                                      This equation is referred to as the Bellman equation.
  • S, a ﬁnite set of states with designated initial state dis-
            b
    tribution 0                                       2.1  Previous work
  • A, a ﬁnite set of actions
                                                      While many POMDP approximation algorithms have been
  • P                        P (s|s, a)
      , the state transition model:   is the transition developed, we will only discuss controller-based approaches,
                    s        a              s
    probability of state if action is taken in state  as that is the focus of our work. These techniques seek to
  • R, the reward model: R(s, a) is the expected immediate determine the best action selection and node transition pa-
    reward for taking action a in state s             rameters for a ﬁxed-size controller.
  • Ω, a ﬁnite set of observations                      Poupart and Boutilier [2003] have developed a method
                                                      called bounded policy iteration (BPI) that uses a one step
  • O, the observation model: O(o|s,a) is the probability
               o        a                      s     dynamic programming lookahead to attempt to improve a
    of observing if action is taken, resulting in state POMDP controller without increasing its size. This approach
  We consider the case in which the decision making process alternates between policy improvement and evaluation. It it-
unfolds over an inﬁnite sequence of stages. At each stage the erates through the nodes in the controller and uses a linear
agent selects an action, which yields an immediate reward, program, shown in Table 1, to examine the value of proba-
and receives an observation. The agent must choose an action bilistically taking an action and then transitioning into the old
based on the history of observations seen. Note that because controller. If an improvement can be found for all states, the
the state is not directly observed, it may be beneﬁcial for the action selection and node transition probabilities are updated
agent to remember the observation history. The objective of accordingly. The controller is then evaluated and the cycle
the agent is to maximize the expected discounted sum of re- continues until no further improvement can be found. BPI
wards received. Because we consider the inﬁnite sequence guarantees to at least maintain the value of a provided con-
problem, we use a discount, 0 ≤ γ<1, to maintain ﬁnite troller, but it is not likely to ﬁnd a concise optimal controller.
sums.                                                   A heuristic has also been proposed for incorporating start
  Finite-state controllers can be used as an elegant way of state knowledge and increasing the performance of BPI in
representing POMDP policies using a ﬁnite amount of mem- practice [Poupart and Boutilier, 2004]. In this extension,
ory. The state of the controller is based on the observation se- termed biased BPI, improvement is concentrated in certain
quence, and in turn the agent’s actions are based on the state node and state pairs by weighing each pair by the (unnormal-
of its controller. These controllers address one of the main ized) occupancy distribution, which can be found by solving
reasons for intractability in POMDP exact algorithms by not the set of linear equations:
remembering whole observation histories. We also allow for
                                                      o(q,s)=bp (q,s)+
stochastic transitions and action selection, as this can help to  0
make up for limited memory [Singh et al., 1994]. The ﬁnite-                                    
                                                         γ  q,s,a,o o(q, s)P (s |s, a)P (a|q)O(o|s ,a)P (q |q, a, o)
state controller can formally be deﬁned by the tuple Q, ψ, η,
where Q is the ﬁnite set of controller nodes, ψ : Q → ΔA is for all states and nodes. The variables used are those previ-
the action selection model for each node, mapping nodes to ously deﬁned and the probability, bp0, of beginning in a node
distributions over actions, and η : Q × A × O → ΔQ repre- state pair. A factor, δ, can also be included, which allows
sents the node transition model, mapping nodes, actions and value to decrease by that amount in each node and state pair.
observations to distributions over the resulting nodes. The This makes changes to the parameters more likely, as a small
value of a node q at state s, given action selection and node amount of value can now be lost in note state pairs. As a re-
transition probabilities P (a|q) and P (q|q, a, o), is given by: sult, value may be higher for the start state and node, but it

                                                IJCAI-07
                                                  2419                                                      is taken, rather than calculating the true value of updating
                                                      action and transition probabilities. Since BPI requires that
                                                      there is a distribution over nodes that increases value for all
                                                      states, it will not make any improvements. Biased BPI sets
                                                      equal weights for each state, thus preventing improvement.
                                                      Allowing value to be lost by using a predetermined δ does
                                                      not guarantee controller improvement, and for many chosen
                                                      values quality may instead decrease.
                                                        Likewise, the gradient calculation in GA will have dif-
Figure 1: Simple POMDP for which BPI and GA fail to ﬁnd ﬁculty ﬁnding an optimal controller. Because Meuleau et
an optimal controller                                 al. formulate the problem as unconstrained, some heuristic
                                                      must adjust the gradient to ensure proper probabilities are
                                                      maintained. For the example problem, some heuristics will
could instead decrease value for that pair. While these heuris- improve the controller, while others remain stuck. In general,
tics may increase performance, they are difﬁcult to adjust and no heuristic can guarantee ﬁnding the globally optimal solu-
not applicable in all domains.                        tion. Essentially, this controller represents a local maximum
  Meuleau et al. [1999] have proposed another approach to for both of these methods, causing suboptimal behavior. One
improve a ﬁxed-size controller. The authors use gradient as- premise of our work is that a more general formulation of the
cent (GA) to change the action selection and node transition problem, which deﬁnes an optimal controller, facilitates the
probabilities and increase value. A cross-product MDP is cre- design of solution techniques that can overcome the above
ated from the controller and the POMDP by considering the limitation and produce better stochastic controllers. While no
states of the MDP to be all combinations of the states of the existing technique guarantees global optimality, experimental
POMDP and the nodes of the controller and actions of the results show that our new formulation is advantageous.
MDP to be based on actions of the POMDP and determin-   In general, the linear program used by BPI may allow for
istic transitions in the controller after an observation is seen. controller improvement, but can easily get stuck in local max-
The value of the resulting MDP can be determined and ma- ima. While the authors suggest heuristics for becoming un-
trix operations allow the gradient to be calculated. Unfortu- stuck, our nonlinear approach offers some advantages. Using
nately, this calculation does not preserve the parameters as a single step or even a multiple step backup to improve a con-
probability distributions. This further complicates the search troller will generally not allow an optimal controller to be
space and is less likely to result in a globally optimal solu- found. While one set of parameters may appear better in the
tion. The gradient can be followed in an attempt to improve short term, only the inﬁnite lookahead deﬁned in the QCLP
the controller, but due to the complex and incomplete gradi- representation can predict the true change in value.
ent calculation, this method can be time consuming and error GA also gets stuck often in local maxima. Meuleau et al.
prone.                                                must construct a cross-product MDP from the controller and
                                                      the underlying POMDP in a complex procedure to calculate
2.2  Disadvantages of BPI and GA                      the gradient. Also, their representation does not take into ac-
Both BPI and GA  fail to ﬁnd an optimal controller for count the probability constraints and thus does not calculate
very simple POMDPs. This can be seen with the two state the true gradient of the problem. Techniques more advanced
POMDP with two actions and one observation in Figure 1. than gradient ascent may be used to traverse the gradient, but
The transitions are deterministic, with the state alternating these shortcomings remain.
when action A1 is taken in state 1 or action A2 is taken in
    2
state . When the state changes, a positive reward is given. 3 Optimal ﬁxed-size controllers
Otherwise, a negative reward is given. Since there are no in-
formative observations, given only a single node and an initial Unlike BPI and GA, our formulation deﬁnes an optimal con-
state distribution of being in either state with equal likelihood, troller for a given size. This is done by creating a set of vari-
the best policy is to choose either action with equal probabil- ables that represents the values of each node and state pair.
ity. This can be modeled by a one node stochastic controller Intuitively, this allows changes in the controller probabilities
with value equal to 0. Notice that this example also shows to be reﬂected in the values of the nodes of the controller. To
that with limited memory (one node), a stochastic controller ensure that these values are correct given the action selection
can provide arbitrarily greater total discounted reward than and node transition probabilities, quadratic constraints (the
any deterministic controller of that same size.       Bellman equations for each node and state) must be added.
  If the initial controller is deterministic and chooses either This results in a quadratically constrained linear program. Al-
action, say A1, BPI will not converge to an optimal controller. though it is often difﬁcult to solve a QCLP exactly, many
The value of the initial controller in state 1 is R−γR/(1−γ) robust and efﬁcient algorithms can be applied. Our QCLP
and −R/(1 − γ) in state 2.Forγ>0.5, which is common,  has a simple gradient calculation and an intuitive represen-
value is negative in each state. Based on a one step lookahead, tation that matches well with common optimization models.
assigning any probability to the other action, A2, will raise The more sophisticated optimization techniques used to solve
the value for state 2, but lower it for state 1. This is because QCLPs may require more resources, but commonly produce
the node is assumed to have the same value after a new action much better results than simpler methods.

                                                IJCAI-07
                                                  2420                            
             For variables: x(q ,a,q,o) and y(q, s)      
             Maximize                                       b0(s)y(q0,s)
                                                          s
             Given the Bellman constraints:
                                   ⎡⎛               ⎞                                                        ⎤
                                                                                    
                   ∀q, s y(q, s)=  ⎣⎝    x(q,a,q,o)⎠ R(s, a)+γ     P (s|s, a) O(o|s,a)   x(q,a,q,o)y(q,s)⎦
                                 a     q                        s          o           q
                                                         
             And probability constraints:           ∀q, o    x(q,a,q,o)=1
                                                          q,a
                                                                    
                                                                          
                                             ∀q, o, a   x(q ,a,q,o)=    x(q ,a,q,ok)
                                                     q               q
                                                     ∀q,a,q,ox(q,a,q,o) ≥ 0

            Table 2: The QCLP deﬁning an optimal ﬁxed-size controller. Variable x(q,a,q,o) represents P (q,a|q, o), variable y(q, s)
            represents V (q, s), q0 is the initial controller node and ok is an arbitrary ﬁxed observation.

              The experimental results suggest that many POMDPs have Table 2 describes the QCLP which deﬁnes an optimal
            small optimal controllers or can be approximated concisely. ﬁxed-size controller. The value of a designated initial node is
            Because of this, it may be unnecessary to use a large amount maximized given the initial state distribution and the neces-
            of memory in order to ﬁnd a good approximation. As our sary constraints. The ﬁrst constraint represents the Bellman
            approach optimizes ﬁxed-size controllers, it allows the al- equation for each node and state. The second and last con-
            gorithm to scale up to large problems without using an in- straints ensure that the variables represent proper probabili-
            tractable amount of space. In the rest of this section, we give ties, and the third constraint guarantees that action selection
            a formal description of the QCLP and prove that its optimal does not depend on the resulting observation which has not
            solution deﬁnes an optimal controller of a ﬁxed size. yet been seen.
                                                                  Theorem 1  An optimal solution of the QCLP results in an
            3.1  QCLP formulation                                 optimal stochastic controller for the given size and initial
            Unlike BPI, which alternates between policy improvement state distribution.
            and evaluation, our quadratically constrained linear program Proof The optimality of the controller follows from the Bell-
            improves and evaluates the controller in one phase. The value man equation constraints and maximization of a given node
            of an initial node is maximized at an initial state distribution at the initial state distribution. The Bellman equation con-
            using parameters for the action selection probabilities at each straints restrict the value variables to valid amounts based on
            node P (a|q), the node transition probabilities P (q|q, a, o),
                                               V (q, s)           the chosen probabilities, while the maximum value is found
            and the values of each node in each state . To ensure for the initial node and state. Hence, this represents an opti-
            that the value variables are correct given the action and node mal controller.
            transition probabilities, nonlinear constraints must be added
            to the optimization. These constraints are the Bellman equa-
            tions given the policy determined by the action selection and 4 Methods for solving the QCLP
            node transition probabilities. Linear constraints are used to Constrained optimization seeks to minimize or maximize
            maintain the proper probabilities.                    an objective function based on equality and inequality con-
              To reduce the representation complexity, the action selec- straints. When the objective and all constraints are linear,
            tion and node transition probabilities are merged into one, this is called a linear program (LP). As our formulation has
            with                                                  a linear objective, but contains some quadratic constraints, it
              P (q,a|q, o)=P (a|q)P (q, |q, a, o)               is a quadratically constrained linear program. Unfortunately,
                 
                                                                 our problem is nonconvex. Essentially, this means that there
              and    P (q ,a|q, o)=P (a|q)
                    q                                             may be multiple local maxima as well as global maxima, thus
              This results in a quadratically constrained linear program. ﬁnding globally optimal solutions cannot be guaranteed.
            QCLPs may contain quadratic terms in the constraints, but Nevertheless, a wide range of nonlinear programming al-
            have a linear objective function. They are a subclass of gen- gorithms have been developed that are able to efﬁciently ﬁnd
            eral nonlinear programs that has structure which algorithms solutions for nonconvex problems with many variables and
            can exploit. This produces a problem that is often more dif- constraints. Locally optimal solutions can be guaranteed, but
            ﬁcult than a linear program, but possibly simpler than a gen- at times, globally optimal solutions can also be found. For
            eral nonlinear program. The QCLP formulation also permits example, merit functions, which evaluate a current solution
            a large number of algorithms to be applied.           based on ﬁtness criteria, can be used to improve convergence

                                                            IJCAI-07
                                                              2421Figure 2: Hallway domain with goal state designated by a star

and the problem space can be made convex by approximation
or domain information. These methods are much more ro-
bust than gradient ascent, while retaining modest efﬁciency in
many cases. Also, the quadratic constraints and linear objec-
tive of our problem often permits better approximations and
the representation is more likely to be convex than problems
with a higher degree objective and constraints.
  For this paper, we used a freely available nonlinearly con-
strained optimization solver called snopt [Gill et al., 2005] Figure 3: Hallway domain: the mean values using BPI and
on the NEOS server (www-neos.mcs.anl.gov). The algo-  the QCLP for increasing controller size
rithm ﬁnds solutions by a method of successive approxima-
tions called sequential quadratic programming (SQP). SQP
uses quadratic approximations which are then solved with conducted on different computers, we expect that this affects
quadratic programming (QP) until a solution to the more gen- solution times by only a small constant factor in our experi-
eral problem is found. A QP is typically easier to solve, ments. Mean time for biased BPI with or without a delta is
but must have a quadratic objective function and linear con- not reported as the time for each was only slightly higher than
straints. In snopt, the objective and constraints are combined that for BPI.
and approximated to produce the QP. A merit function is also
                                                      5.1  Hallway benchmark
used to guarantee convergence from any initial point.
                                                      The hallway domain, shown in Figure 2 was introduced by
5  Experiments                                        Littman, Cassandra and Kaelbling [1995] and is a frequently
                                                      used benchmark for POMDP algorithms. It consists of a grid
In this section, we compare the results obtained using our new world with 57 states, 21 observations and 5 actions. There
formulation and the snopt solver with those of BPI and biased are 14 squares in which the robot may face north, south, east
BPI. GA was also implemented, but produced signiﬁcantly or west, and a goal square. The robot begins in a random
worse results and required substantially more time than the location and orientation and must make its way to a goal state
other techniques. In the interest of saving space, we omit by turning, going forward or staying in place. The start state
the details of GA and focus on the more competitive tech- is never the same as the goal state and the observations consist
niques, BPI and biased BPI, which we implemented accord- of the different views of the walls in the domain. Both the
ing to their original descriptions in [Poupart and Boutilier, observations and transitions are extremely noisy. Only the
2003] and [Poupart and Boutilier, 2004]. Choices of the loss goal has a reward of 1 and the discount factor used was 0.95.
         δ
parameter, , in biased BPI often decreased performance in The results for this domain are shown in Table 3 and Figure
the second domain studied. Because of this, the best value 3. We see that for all controller sizes the mean value produced
   δ
for  was found experimentally after much trial and error. by the QCLP is greater than those produced by each version
This search centered around the value suggested by Poupart of BPI. Biased BPI, and in particular biased BPI with a δ of
           (max    R(s, a)−min   R(s, a))/400(1−γ)
and Boutilier,  s,a            s,a                .   0.05, improves upon BPI, but the quality remains limited. The
Note that our goal in these experiments is to demonstrate the value of an optimal POMDP policy is not known. However,
beneﬁts of our formulation when used in conjunction with an the value of an optimal policy for the underlying MDP, which
“off the shelf” solver such as snopt. The formulation is very represents an upper bound on an optimal POMDP policy, was
general and many other solvers may be applied. In fact, we
are currently developing a customized solver that would take
further advantage of the inherent structure of the QCLP and size QCLPhal  BPIhal   QCLPmac     BPImac
increase scalability.                                     1    < 1 min   < 1 min    < 1 min    1.2 mins
  Two domains are used to compare the performance of the  2    < 1min    < 1 min    < 1 min    4.3 mins
QCLP and the two versions of BPI. Each method was ini-    4    < 1 min   < 1 min    7.3 mins  13.9 mins
tialized with ten random deterministic controllers and we re- 6 1.5 mins 1.5 mins  38.2 mins  25.2 mins
port mean values and times after convergence. To slightly 8    6.9 mins  2.5mins   72.2 mins  40.2 mins
increase the quality of the QCLP produced controllers, up- 10  9.1 mins  3.7mins   122.5 mins 59.5 mins
per and lower bounds were added. These represent the value
of taking the highest and lowest valued action respectively Table 3: Mean running times for the QCLP and BPI on the
for an inﬁnite number of steps. While the experiments were hallway and machine problems

                                                IJCAI-07
                                                  2422