              Compiling Bayesian Networks Using Variable Elimination                     ∗

                                Mark Chavira     and  Adnan Darwiche
                                     Computer Science Department
                                 University of California, Los Angeles
                                     Los Angeles, CA 90095-1596
                                    {chavira,darwiche}@cs.ucla.edu

                    Abstract                          sometimes run more efﬁciently by using a more structured
                                                      representation like ADDs [R.I. Bahar et al., 1993], afﬁne
    Compiling Bayesian networks has proven an ef-     ADDs  [Sanner and McAllester, 2005], representations com-
    fective approach for inference that can utilize both posed of confactors [Poole and Zhang, 2003], sparse repre-
    global and local network structure. In this pa-   sentations [Larkin and Dechter, 2003], and a collection of
    per, we deﬁne a new method of compiling based     others. Such a representation makes use of local structure
    on variable elimination (VE) and Algebraic Deci-  to skip certain arithmetic operations and to represent factors
    sion Diagrams (ADDs). The approach is impor-      more compactly. However, the effectiveness of these ap-
    tant for the following reasons. First, it exploits lo- proaches has been sometimes limited in practice, because the
    cal structure much more effectively than previous overhead they incur may very well outweigh any gains.
    techniques based on VE. Second, the approach al-
                                                        There has been less work to answer multiple queries si-
    lows any of the many VE variants to compute an-
                                                      multaneously using VE as is done by the jointree algorithm
    swers to multiple queries simultaneously. Third,
                                                      [Jensen et al., 1990; Lauritzen and Spiegelhalter, 1988];but
    the approach makes a large body of research into
                                                      see [Cozman, 2000] for an exception. Theoretical results
    more structured representations of factors relevant
                                                      in [Darwiche, 2000] demonstrate that by keeping a trace,
    in many more circumstances than it has been pre-
                                                      one can use VE as a basis for compilation. This algorithm,
    viously. Finally, experimental results demonstrate
                                                      which we will refer to as tabular compilation, will compile
    that VE can exploit local structure as effectively
                                                      a Bayesian network ofﬂine into an arithmetic circuit (AC).
    as state–of–the–art algorithms based on condition-
                                                      Tabular compilation runs in time and space exponential in
    ing on the networks considered, and can sometimes
                                                      treewidth; the resulting AC has size exponential in treewidth;
    lead to much faster compilation times.
                                                      and once ofﬂine compilation is complete, the AC can be used
                                                      online to answer many queries simultaneously in time lin-
1  Introduction                                       ear in its size. However, the applicability of this approach
                                                      is limited, because tabular compilation provides no practical
                 [                                ]
Variable elimination Zhang and Poole, 1996; Dechter, 1996 advantage over jointree.
(VE) is a well–known algorithm for answering probabilistic
queries with respect to a Bayesian network. The algorithm An important observation is that when compiling using
runs in time and space exponential in the treewidth of the net- VE, we need not use tables, but can instead use one of the
work. Advantages of VE include its generality and simplicity. more structured representations of factors. This observation
In this paper, we consider two aspects of VE. The ﬁrst is how is seemingly innocent, but it has two critical implications.
to effectively utilize local structure in the form of determin- First, the usual cost incurred when using structured repre-
ism [Jensen and Andersen, 1990] and context–speciﬁc inde- sentations gets pushed into the ofﬂine phase, where it can be
pendence [Boutilier et al., 1996] to perform inference more afforded. Second, the usual advantage realized when using
efﬁciently. The second consideration is how to answer mul- more structured representations means that the size of the re-
tiple queries simultaneously. For example, given particular sulting AC will not necessarily be exponential in treewidth.
evidence, we would like to be able to simultaneously com- The smaller AC translates directly into more efﬁcient online
pute probability of evidence and a posterior marginal on each inference, where efﬁciency matters most. We see that the pro-
network variable. And we would like to be able to repeat this posed approach is important for four key reasons. First, it
process for many different evidence sets.             exploits local structure much more effectively than previous
  Many proposals have been extended to utilize local struc- techniques based on VE. Second, the approach allows any of
ture in the context of VE. Although the standard version of the the many VE variants to compute answers to multiple queries
algorithm uses tables to represent factors, the algorithm can simultaneously. Third, the approach makes a large body of
                                                      research into more structured representations of factors rele-
  ∗This work has been partially supported by Air Force grant vant in many more circumstances than it has been previously.
#FA9550-05-1-0075-P00002 and JPL/NASA grant #1272258. Finally, experimental results demonstrate that on the consid-

                                                IJCAI-07
                                                  2443ered networks, VE can exploit local structure as effectively as      X           XY      Zf(.)
state–of–the–art algorithms based on conditioning as applied
                                                                                 x1  y1  z1  0.9
to a logical encoding of the networks, and can sometimes lead                    x   y   z   0.1
to much faster compilation times.                                                 1   1   2
                                                                  Z       Y      x   y   z   0.9
  To demonstrate these ideas, we propose a new method                             1   2   1
                                                                                 x   y   z   0.1
of compiling based on VE and algebraic decision diagrams                          1   2   2
(ADDs). We will refer to this method of compiling as ADD             Z           x2  y1  z1  0.1
compilation. It is well known that ADDs can represent                            x2  y1  z2  0.9
the initial conditional probability distributions (factors) of a                 x2  y2  z1  0.5
Bayesian network more compactly than tables. However, it is    .1 .9      .5     x2  y2  z2  0.5
not clear whether ADDs will retain this advantage when pro-
ducing intermediate factors during the elimination process, Figure 1: An ADD over variables X, Y, Z and the corre-
especially in the process of compilation which involves no sponding table it represents. Dotted edges point to low–
evidence. Note that the more evidence we have, the smaller children and solid edges point to high–children.
the ADDs for intermediate factors.
  Experimental results will demonstrate several important
points with respect to ADD compilation. The ﬁrst point deals 2.2 Elimination using ADDs
with AC sizes. ADD elimination (no compilation) can out-
perform tabular elimination (no compilation) when there are An ADD is a graph representation of a function that maps in-
                                                                                                [
massive amounts of local structure, but can dramatically un- stantiations of Boolean variables to real–numbers R.I. Bahar
                                                               ]
derperform tabular elimination otherwise. However, ADD et al., 1993 . Figure 1 depicts an ADD and the corresponding
compilation produces ACs that are much smaller than those table (function) it represents. In the worst case, an ADD has
produced by tabular compilation, even in many cases where the same space complexity as a table representing the same
there are lesser amounts of local structure. Second, because function. Moreover, the factor operations of multiplication
of the smaller AC size, online inference time is capable of and summing–out are implemented on ADDs in polynomial
outperforming jointree by orders of magnitude. Finally, ADD time. There are four points that are important with respect
compilation can be much faster in some cases than a state–of– to ADDs. First, because multiplication and summing–out are
the–art compilation technique, which reduces the problem to available for ADDs, we can use ADDs during VE instead of
logical inference [Darwiche, 2002].                   tables. Second, because ADDs can leverage local structure,
                                                      an ADD can be much smaller than the corresponding table.
  This paper is organized as follows. We begin in Section 2
                                                      Third, for the same reason, when applied to ADDs, the fac-
by reviewing VE, elimination using ADDs, and compilation
                                                      tor operations of multiplication and summing–out can result
into ACs. Next, Section 3 shows how a trace can be kept dur-
                                                      in far fewer arithmetic operations on numbers than the same
ing elimination employing ADDs to compile a network into
                                                      factor operations acting on tables. Finally, the constants in-
an AC. We then discuss in Section 4 implications for com-
                                                      volved in using ADDs are much larger than those involved
piling using structured representations of factors. We present
                                                      in using tables, so much so that elimination using ADDs will
experimental results in Section 5 and conclude in Section 6.
                                                      often take much longer than elimination using tables, even
                                                      when the ADD performs fewer arithmetic operations on num-
2  Background                                         bers, and so much so that we may run out of memory when
In this section, we brieﬂy review VE, elimination using using ADDs, even in cases where tabular elimination does
ADDs, and compilation into ACs.                       not. We will discuss later how we can deal with many–valued
                                                      variables within an ADD.
2.1  Variable Elimination
                                                      2.3  Compiling into ACs
Variable elimination is a standard algorithm for computing
probability of evidence with respect to a given a Bayesian The notion of using arithmetic circuits (ACs) to perform
                                                                                          [
network [Zhang and Poole, 1996; Dechter, 1996]. For space probabilistic inference was introduced in Darwiche, 2000;
                                                          ]
reasons, we mention only a few points with regard to this al- 2003 . With each Bayesian network, we associate a cor-
gorithm. First, the algorithm acts on a set of factors. Each responding multi-linear function (MLF) that computes the
                                                      probability of evidence. For example, the network in Fig-
factor involves a set of variables and maps instantiations of                A    B                 C
those variables to real–numbers. The initial set of factors ure 2—in which variables and are Boolean, and has
are the network’s conditional probability distributions (usu- three values—induces the following MLF:
ally tables). Elimination is driven by an ordering on the vari- λ λ λ θ θ θ   + λ  λ  λ  θ θ  θ      +
                                                          a1 b1 c1 a1 b1 c1|a1,b1 a1 b1 c2 a1 b1 c2|a1,b1
ables called an elimination order. During the algorithm, two ...
factor operations are performed many times: factors are mul- λ λ θ  θ θ       + λ  λ  λ  θ θ  θ
                                                        λa2  b2 c2 a2 b2 c |a ,b a2 b2 c3 a2 b2 c |a ,b
tiplied and a variable is summed out of a factor. These factor          2 2 2                   3 2 2
operations reduce to performing many multiplication and ad- The terms in the MLF are in one-to-one correspondence
dition operations on real–numbers. Although tables are the with the rows of the network’s joint distribution. Assume
most common representation for factors, any representation that all indicator variables λx have value 1 and all parameter
that supports multiplication and summing–out can be used. variables θx|u have value Pr(x|u). Each term will then be a

                                                IJCAI-07
                                                  2444                           +                                      Multiply(α  : SymADD, α    : SymADD)
 A       B                                            Algorithm 1           1              2            :
                       *      *                       returns SymADD.
            Compile
                            
                       a  a  a  a                      1: swap α1 and α2 if Pos(α1) >Pos(α2)
     C              +  1  1  2   2 +
                                                       2: if cache(α1,α2) = null then
                 *       *   *       *                 3:  return cache(α1,α2)
          +        *    +     *   +        +           4: else if α1 and α2 are leaf nodes then
                                                   5:  α ← new ADD leaf node
       *  * *  b1 b1 * *  *  b2 b2 * * * *  * *
                                                       6:  Ac(α) ← new AC ∗ node with children Ac(α1) and Ac(α2)
                                                       7: else if Pos(α1)=Pos(α2) then
                                                         α ←
      c1|a1b1 c |a b                              8:      new internal node
             2 1 2  c     c      c   c |a b c2|a2b2
                     2     1      3  1 2 1                 Var(α) ← Var(α1)
c |a b c |a b                                  9:
  2 1 1  3 1 1 c1|a1b2 c |a b c |a b c |a b c |a b c |a b
                     3 1 2  2 2 1 3 2 1  3 2 2  1 2 2 10:  Lo(α) ← Multiply(Lo(α1),Lo(α2))
                                                      11:  Hi(α) ← Multiply(Hi(α1),Hi(α2))
  Figure 2: A Bayesian network and a corresponding AC. 12: else
                                                      13:  α ← new internal node
                                                      14:  Var(α) ←  Var(α1)
                                                      15:  Lo(α) ← Multiply(Lo(α1),α2)
product of probabilities which evaluates to the probability of 16: Hi(α) ← Multiply(Hi(α1),α2)
the corresponding row from the joint. The MLF will add all 17: end if
                                  1.0
probabilities from the joint, for a sum of . To compute the 18: cache(α1,α2) ← α
probability Pr(e) of evidence e, we need a way to exclude 19: return α
certain terms from the sum. This removal of terms is accom-
plished by carefully setting certain indicators to 0 instead of
1
 , according to the evidence.                         stant n. For each variable X in the network, we also construct
  The fact that a network’s MLF computes the probability of an indicator ADD, which acts as a placeholder for evidence
evidence is interesting, but the network MLF has exponential to be entered during the online inference phase. Assuming
size. However, if we can factor the MLF into something small     X                   x     x
                                           Pr(e)      that variable is binary with values 1 and 2, the indicator
enough to ﬁt within memory, then we can compute  in   ADD for  X will consist of one internal node, labeled with
time that is linear in the size of the factorization. The factor- variable X, and having two sink children. The child corre-
ization will take the form of an AC, which is a rooted DAG
                                                      sponding to x1(x2) will be a sink labeled with a pointer to a
(directed acyclic graph), where an internal node represents                                     λ   (λ )
                                                      single–node AC that is itself labeled with indicator x1 x2 .
the sum or product of its children, and a leaf represents a con- Figure 3(a) shows a simple Bayesian network and Figure 3(b)
stant or variable. In this context, those variables will be indi- shows the corresponding symbolic ADDs.
cator and parameter variables. An example AC is depicted in
Figure 2. We refer to this process of producing an AC from a Working with symbolic ADDs requires a modiﬁcation to
network as compiling the network.                     the standard ADD operations, but in a minimal way. We ﬁrst
  Once we have an AC for a network, we can compute Pr(e) point out that ADD operations are typically implemented re-
by assigning appropriate values to leaves and then comput- cursively, reducing an operation over two ADDs into opera-
ing a value for each internal node in bottom-up fashion. The tions over smaller ADDs, until we reach boundary conditions:
value for the root is then the answer to the query. We can ADDs that correspond to sinks. It is these boundary condi-
also compute a posterior marginal for each variable in the tions that need to be modiﬁed in order to produce symbolic
network by performing a second downward pass [Darwiche, ADD operations. For example, Algorithm 1 speciﬁes how
2003]. Hence, many queries can be computed simultaneously to multiply two symbolic ADDs. When applied to an internal
                                                                           Pos  Lo     Hi
in time linear in the size of the AC. Another main point is that ADD node, the functions , ,and return the position
this process may then be repeated for as many evidence sets of the variable labeling the node in the ADD variable order,
                                                                                  false
as desired, without recompiling.                      the low–child (corresponding to  ), and the high–child
                                                      (corresponding to true), respectively. When applied to an
                                                      ADD sink, the functions Posand Ac return ∞ and a pointer
3  ADD Compilation                                    to the AC that labels the sink, respectively. Finally, cache is
We have seen how ADDs can be used in place of tables during the standard computed table used in ADD operations. The
VE and how compilation produces an AC from a network. In main observation is that the algorithm is identical to that for
this section, we combine the two methods to compile an AC multiplying normal ADDs (see [R.I. Bahar et al., 1993]), ex-
using a VE algorithm that employs ADDs to represent factors. cept for Line 6. When multiplying normal ADDs, this line
  The core technique behind our approach is to work with would label the newly created sink with a constant that is the
ADDs whose sinks point to ACs (their roots) instead of point- product of the constants labeling α1 and α2. When multi-
ing to constants. We will refer to these ADDs as symbolic plying symbolic ADDs, we instead label the new sink with a
ADDs.  Given a Bayesian network, we convert each condi- pointer to an AC that represents an analogous multiplication.
tional probability table (CPT) into a symbolic ADD called This AC will consist of a new multiply node having children
a CPT ADD. To do so, we ﬁrst convert the CPT into a nor- that are the ACs pointed to by α1 and α2. The algorithm for
mal ADD and then replace each constant n within a sink by a summing–out a variable from a symbolic ADD is constructed
pointer to an AC consisting of a single node labeled with con- from the algorithm for summing–out a variable from a normal

                                                IJCAI-07
                                                  2445                                                                      X

                                XYPr(y|x)              X         Y             X          Y
                         X
                                x1  y1    0.0
           X   Pr(x)            x1  y2    1.0
           x    0.1             x   y     0.5
            1            Y       2   1
           x2   0.9             x2  y2    0.5
                                                     .1   .9   0    1   .5   x1 x2    y1 y2

                                                     CPT ADD     CPT ADD     Indicator  Indicator
                                                      for X        for Y    ADD for X  ADD for Y
                           (a)                                          (b)

              Figure 3: (a) A simple Bayesian network and (b) symbolic ADDs representing the network.

Algorithm 2 Compile(N : Bayesian Network): returns AC. dundant AC nodes. Each entry is an AC node that has been
                                                      constructed, indexed the node’s label and its children. Before
 1: Ψ ← the set of indicator and CPT ADDs for N       we ever construct a new AC node, we ﬁrst consult the unique
 2: π ← ordering on the variables (π(i) is ith variable in order) table to see if a similar node has been constructed before. If
 3: for i in 1 ...number of variables in N do         this is the case, we simply point to the existing node instead
 4:  P ←{α  ∈ Ψ:α  mentions variable π(i)}           of constructing a duplicate node. As is standard practice, we
     Ψ ← (Ψ − P ) ∪{Σ        α}
 5:                  π(i) α∈P                         also use a unique table for the ADD nodes.
 6: end for
   β ←       α                                          Multi–valued variables: When a network variable X has
 7:      α∈Ψ
 8: return Ac(β)                                      more than two values, we deﬁne its ADD variables as follows.
                                                                         X                       V
                                                      For each value xi of , we create ADD variable xi .We
                                                      translate instantiations of X in the straightforward way. For
                                                                                    V    = true    V   =
                                                      example, we translate X = x2 as x2       and  xi
ADD in an analogous way, by modifying the code that labels false i =2                                V
                                                           for     . Moreover, we construct an ADD over xi
newly created sinks. In this case, instead of labeling with a                               V
constant representing a sum, we instead label with a pointer which enforces the constraint that exactly one xi can be true.
                                                      We multiply this ADD into every CPT and indicator ADD
to an new addition node.                                          X
  We now describe the compilation process in Algorithm 2. that mentions . Finally, when eliminating (summing–out) a
                                                      multi–valued variable X, we sum out all corresponding ADD
Line 1 begins by representing the Bayesian network using      V
symbolic ADDs as described previously. We then gener- variables xi simultaneously.
ate an ordering on the variables using a minﬁll heuristic on Converting a CPT into an ADD: The straightforward
Line 2. Lines 3–7 perform variable elimination in the stan- way to construct the ADD of a CPT is to construct an ADD
dard way, using the multiply and sum–out operations of sym- for each row of the CPT and then add the resulting ADDs.
bolic ADDs. Afterward, we are left with a trivial symbolic This method was extremely inefﬁcient for large CPTs (over a
ADD: a sink labeled with a pointer to an AC. This AC is a thousand parameters). Instead, we construct a tree–structured
factorization of the network MLF and a compilation of the ADD whose terminals correspond to the CPT rows, and then
given network! Each internal node of the AC is labeled with use the standard reduce operation of ADDs to produce a DAG
a multiplication or addition operation, and each leaf is labeled structure.
with a constant or an indicator variable. The AC represents a Simpliﬁcations: In certain cases, simpliﬁcations are pos-
history or trace of all arithmetic operations performed during sible. For example, when multiplying two ADD nodes, if one
the elimination process.                              of the nodes α is a sink labeled with a pointer to an AC node
  For the above procedure to scale and produce the results which which is itself labeled constant 0, then we can simply
we report later, it has to be augmented by a number of key return α without doing more work. Similar simpliﬁcations
techniques that we describe next.                     exist for constant 1 and for the summing–out operation.
  ADD variable order: ADDs require a ﬁxed variable order Figure 4 depicts the elimination process when applied to
which is independent of the elimination order we have dis- the symbolic ADDs of Figure 3(b) using elimination order
cussed. For that, we use the reverse order used in the elimina- Y,X. The ﬁrst step is to multiply the set of symbolic ADDs
tion process. This ensures that when a variable is eliminated involving Y , which consists of Y ’s CPT ADD and Y ’s indi-
from an ADD, it appears at the bottom of the ADD. The sum– cator ADD. Figure 4(a) shows the result. Observe that each
out operation on ADDs is known to be much more efﬁcient ADD sink points to an AC that represents the multiplication
when the summed out variable appears at the bottom.   of two ACs, one pointed to by a sink from Y ’s CPT ADD and
  Unique table: We maintain a cache, called a unique table one pointed to by a sink from Y ’s indicator ADD. Also ob-
in the ADD literature, to ensure that we do not generate re- serve that when one of the ACs involved in the multiplication

                                                IJCAI-07
                                                  2446is a sink labeled with 0 or 1, the resulting AC is simpliﬁed.          Tabular     ADD
Figure 4(b) next shows the result of summing–out Y from the Network  Time (ms) Time (ms) Improvement
symbolic ADD in Figure 4(a). Here, each ADD node labeled   alarm           31       360        0.086
    Y                                                      barley         307     14,049       0.022
with  and its children have been replaced with a sink which bm-5-3       4,892      658        7.435
points to an addition node. A simpliﬁcation has also occurred. diabetes   949     33,220       0.029
Figure 4(c) shows the result of multiplying symbolic ADDs  hailﬁnder       48       515        0.093
involving variable X in two steps: ﬁrst Figure 4(b) by X’s link          1,688     2,658       0.635
indicator ADD and then the result by X’s CPT ADD. Finally, mm-3-8-3      2,166      843        2.569
Figure 4(d) shows the result of summing–out X from Fig-    mildew          72     92,602       0.001
ure 4(c). At this point, since there is only one remaining sym- munin1    155      1,255       0.124
bolic ADD, we have our compilation.                        munin2         204      3,170       0.064
                                                           munin3         350      5,049       0.069
                                                           munin4         406      4,361       0.093
4  Implications                                            pathﬁnder       51      5,213       0.010
There has been much research into using alternatives to tabu- pigs         69       597        0.116
lar representations of factors in VE. The motivation for such st-3-2      186       362        0.514
research is that tabular elimination makes use only of global tcc4f        29       153        0.190
                                                           water           76      1,015       0.075
network structure (its topology). It may therefore miss oppor-
tunities afforded by local structure to make inference more ef- Table 1: Time to perform VE (no compilation) using tables
ﬁcient. Structured representations of factors typically exploit and using ADDs.
local structure to lessen the space requirements required for
inference and to skip arithmetic operations on numbers (mul-
tiplications and additions) required for inference. In some overhead is incurred during the ofﬂine phase, each arithmetic
cases, structured representations of factors have been success- operation saved makes the size of the resulting AC smaller,
ful in performing inference when tabular elimination runs out yielding more efﬁcient online inference. These observations
of memory, and in some cases, structured representations al- make research into structured representations of factors ap-
low for faster inference than tabular elimination. However, plicable in many more circumstances than it has been pre-
the time and space overhead involved in using more struc- viously. Another major advantage of adding compilation to
tured representations of factors may very well outweigh the elimination is that one can compute online answers to many
beneﬁts. This is particularly true when there are not exces- queries simultaneously and very quickly as is done by the
sive amounts of local structure present in the network. Ta- jointree algorithm; see [Darwiche, 2003] for details.
ble 1 demonstrates the performance of elimination (no com-
pilation) using both tables and ADDs applied to the set of net-
works we will be considering later in the paper. All ADD op- 5 Experimental Results
erations were performed using the publicly available CUDD In this section, we present experimental results of applying
ADD package (http://vlsi.colorado.edu/∼fabio/CUDD). We ADD compilation and compare ADD compilation to tabu-
see that in only two cases does ADD elimination outperform lar compilation, Ace compilation, and jointree. All ADD
tabular elimination. Both of the networks involved, bm-5- operations were performed using the CUDD ADD package
3 and mm-3-8-3, contain massive amounts of determinism. (http://vlsi.colorado.edu/∼fabio/CUDD), modiﬁed to work
In the remaining cases, ADD elimination is actually slower with symbolic ADDs. Ace (http://reasoning.cs.ucla.edu/ace)
than tabular elimination, often by an order of magnitude or is a state–of–the–art compiler that encodes Bayesian net-
more. The large number of examples where structured repre- works as logical knowledge bases and then applies condi-
sentations of factors are slower than tabular elimination have tioning to the result. Ace has been shown to be extremely
limited their applicability.                          effective on networks having large amounts of local structure
  The main observation behind this paper is that even when [Chavira et al., May 2006; 2005], able to compile many net-
elimination using a more structured representation of factors works with treewidths in excess of one–hundred. Ace was
is slower than tabular elimination, structured representations also shown in [Chavira and Darwiche, 2005] to perform on-
become very powerful in the context of compilation. Compi- line inference orders of magnitude more efﬁciently than join-
lation is an ofﬂine process, and so extra time can be afforded. tree when applied to networks having treewidth small enough
In fact, the combination of structured representations with for jointree to work and having lesser amounts of local struc-
compilation is ideal, since the very purpose of compilation is ture. On these networks, Ace online inference is very efﬁ-
to spend time ofﬂine in an effort to identify opportunities for cient, but some compile times are in excess of one thousand
savings that can be used repeatedly during online inference. seconds. The networks with which we experimented are a su-
Hence, in the context of compilation, the disadvantages of perset of these networks from [Chavira and Darwiche, 2005].
using structured representations are much less severe (virtu- Experiments ran on a 2GHz Core Duo processor with 2GB
ally disappearing) and the advantages are retained.1 Although of memory, using the networks in Table 2.2 To make the
                                                      experiments as fair as possible, for each network, we ﬁrst
  1Note that the use of more structured representations of factors
is usually asymptotically no worse than the use of tabular represen- 2Because Ace was not available for the test platform, Ace compi-
tations, which is particularly true for ADDs.         lation was carried out on a 1.6GHz Pentium M with 2GB of memory.

                                                IJCAI-07
                                                  2447