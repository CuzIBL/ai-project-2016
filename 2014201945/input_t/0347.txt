         Unsupervised Dimensionality Estimation and Manifold Learning in
                         high-dimensional Spaces by Tensor Voting

                              Philippos Mordohai and Gerard´     Medioni
                                    University of Southern California
                                email: {mordohai,medioni}@iris.usc.edu


                    Abstract                          orientation of each point [Medioni et al., 2000]. The votes are
                                                      also tensors and their accumulation is a process that is con-
    We address dimensionality estimation and nonlin-  siderably more powerful than the accumulation of scalars or
    ear manifold inference starting from point inputs vectors in terms of both the types of structure that can be de-
    in high dimensional spaces using tensor voting.   scribed, as well as its robustness to noise. A point casts a vote
    The proposed method operates locally in neigh-    to a neighboring point that carries an estimate of the orienta-
    borhoods and does not involve any global com-     tion of the receiver given the orientation of the voter. Even in
    putations. It is based on information propagation the case of unoriented inputs, meaningful votes can be cast as
    among neighboring points implemented as a voting  a function of the relative position of the two points. The result
    process. Unlike other local approaches for man-   of voting is a new tensor at each point that encompasses the
    ifold learning, the quantity propagated from one  weighted contribution of the point’s neighbors.
    point to another is not a scalar, but is in the form
    of a tensor that provides considerably richer infor- The dimensionality d of the manifold at each point in a D-
    mation. The accumulation of votes at each point   dimensional space is indicated by the maximum gap in the
    provides a reliable estimate of local dimensionality, eigenvalues of the tensor, while the top d eigenvectors span
    as well as of the orientation of a potential manifold the normal space of the manifold. The ﬁrst property makes di-
    going through the point. Reliable dimensionality  mensionality estimation a procedure with no parameters that
    estimation at the point level is a major advantage require tuning. These estimates are reliable without averag-
    over competing methods. Moreover, the absence of  ing, as shown is Section 4, and allow us to handle data with
    global operations allows us to process signiﬁcantly varying dimensionality. This is arguably one of the most im-
    larger datasets. We demonstrate the effectiveness portant contributions of our approach. Moreover, the pres-
    of our method on a variety of challenging datasets. ence of outliers, boundaries, intersections or multiple mani-
                                                      folds pose no additional difﬁculties, due to the unsupervised
                                                      learning ability of our method. Furthermore, non-manifolds,
1  Introduction                                       such as hyper-spheres, can be inferred since we do not at-
A number of algorithms for manifold learning from unor- tempt to estimate a global “unfolding”.
ganized points have been recently proposed in the machine We do not attempt to estimate a mapping from the input
learning literature. These include kernel PCA [Scholkopf¨ space to the embedding space, as in [Brand, 2003] [Teh and
et al., 1998], locally linear embedding (LLE) [Roweis and Roweis, 2003]. Even though certain advantages can be de-
Saul, 2000], Isomap [Tenenbaum et al., 2000] and charting rived from such a mapping, it is not always feasible, as in the
[Brand, 2003], that are brieﬂy described in Section 2. They case of hyper-spheres or multiple manifolds, and it is not nec-
aim at reducing the dimensionality of the input space in a way essary for many tasks, including interpolation. Valid paths on
that preserves some geometric or statistic properties of the the manifold can be followed using the estimated tangent sub-
data. Isomap, for instance, attempts to preserve the geodesic spaces at the input points by projecting the desired direction
distances between all points as the manifold is “unfolded” on them and moving on the projection.
and mapped to a space of lower dimension. A common as-  Since all operations are local, time complexity is
sumption is that the desired manifold consists of locally lin- O(Dnlogn) where n is the number of points, enabling us to
ear patches. We relax this assumption by only requiring that handle datasets with orders of magnitude more points than the
manifolds be smooth almost everywhere. Smoothness in this current state of the art without incurring unreasonable com-
context is the property of the manifold’s orientation to vary putational cost. In terms of storage, the requirements at each
gradually as one moves from point to point. This property is point are O(D2). Our current implementation is probably
encoded in the votes that each point casts to its neighbors. limited to D in the order of a few hundreds. As all manifold
  The representation of each point is in the form of a second learning methods, we operate under the assumption that the
order, symmetric, nonnegative deﬁnite tensor whose eigen- data are randomly sampled from the manifold in a way that
values and eigenvectors fully describe the dimensionality and is close to uniform, or at least does not favor certain parts ordirections of the manifold.                           ifold learning by enforcing local isometry. The lengths of
  The paper is organized as follows: the next section brieﬂy the sides of triangles of neighboring points are preserved dur-
reviews related work; Section 3 describes the tensor voting ing the embedding. These constraints can be expressed in
framework; Section 4 contains experimental results on classic terms of pairwise distances and the optimal embedding can
datasets; ﬁnally, Section 5 concludes the paper and discusses be found by semi-deﬁnite programming. The method is more
the directions of our future work.                    computationally demanding than LLE and Isomap, but can
                                                      reliably estimate the underlying dimensionality of the inputs
2  Related Work                                       by locating the largest gap between the eigenvalues of the
                                                      Gram matrix of the outputs. Similarly to our approach, this
In this section we brieﬂy present recent approaches for learn- estimate does not require a threshold, as do approaches that
ing low dimensional embeddings from points in high dimen- estimate the residual error as a function of the dimensionality
sional spaces. Most of them are extensions of linear tech- of the ﬁtted model.
niques, such as Principal Component Analysis (PCA) and  An algorithm that computes a mapping, and not just an
Multi-Dimensional Scaling (MDS), based on the assump- embedding, of the data is described in [Brand, 2003]. The
tion that nonlinear manifolds can be approximated by locally intrinsic dimensionality of the manifold is estimated by ex-
linear patches. In contrast to other methods, Scholkopf¨ et amining the rate of growth of the number of points contained
al. [1998] propose kernel PCA that attempts to ﬁnd linear in hyper-spheres as a function of the radius. Linear patches,
patches using PCA in a space of typically higher dimension- areas of curvature and noise can be discriminated using the
ality than the input space. Correct kernel selection can reveal proposed measure. Afﬁne transformations that align the co-
the low dimensional structure of the input data. For instance a ordinate systems of the linear patches are computed at the
second order polynomial kernel can “ﬂatten” quadratic man- second stage. This deﬁnes a global coordinate system for the
ifolds.                                               embedding and thus a mapping between the input space and
  Roweis and Saul [2000] address the problem by Locally the embedding space.
Linear Embedding (LLE). Processing involves computing re-
construction weights for each point from its neighbors. The Finally, we brieﬂy review approaches that estimate intrin-
same weights should also reconstruct the point from its neigh- sic dimensionality without any attempts to learn local struc-
bors in the low dimensional embedding space, since the ture. One such approach was presented in [Bruske and Som-
neighborhoods are assumed to be linear. The dimensional- mer, 1998]. An optimally topology preserving map (OTPM)
ity of the embedding however has to be given as a parameter is constructed for a subset of the data, which is produced after
since it cannot always be estimated from the data [Saul and vector quantization. Then PCA is performed for each node of
Roweis, 2003]. LLE assures that local neighborhood struc- the OTPM under the assumption that the data are locally lin-
ture is preserved during dimensionality reduction.    ear. The average of the number of signiﬁcant singular values
  Isomap Tenenbaum et al. [2000] approximates geodesic at the nodes is the estimate of the intrinsic dimensionality.
distances between points by graph distances. Then, MDS Kegl´ [2005] estimates the capacity dimension of the mani-
is applied on the geodesic instead of Euclidean distances to fold, which does not depend on the distribution of the data and
compute an embedding that forces nearby points to remain is equal to the topological dimension. An efﬁcient approxi-
nearby and faraway points to remain that way. A variation of mation based on packing numbers is proposed. The algorithm
Isomap that can be applied to data with intrinsic curvature, takes into account dimensionality variations with scale and is
but known distribution, and a faster alternative that only uses based on a geometric property of the data, rather than succes-
a few landmark point for distance computations have been sive projections to increasingly higher-dimensional subspaces
proposed in [de Silva and Tenenbaum, 2003]. Isomap and its until a certain percentage of the data is explained. Costa and
variants are limited to convex datasets.              Hero [2004] estimate the intrinsic dimension of manifold and
  Belkin and Niyogi [2003] compute the graph Laplacian of the entropy of the samples using geodesic-minimal-spanning
the adjacency graph of the input data, which is an approxima- trees. The method is similar to Isomap and thus produces a
tion of the Laplace-Beltrami operator on the manifold, and single global estimate. Levina and Bickel [2005] compute
exploit its locality preserving properties that were ﬁrst ob- maximum likelihood estimates of dimensionality by examin-
served in the ﬁeld of clustering. The Laplacian eigenmaps ing the number of neighbors included in spheres whose radius
algorithm can be viewed as a generalization of LLE, since is selected in such a way that the density of the data can be
the two become identical when the weights of the graph are assumed constant, and enough neighbors are included. These
chosen according to the criteria of the latter. The dimension- requirements cause an underestimation of the dimensionality
ality of the manifold however cannot be reliably estimated when it is very high.
from the data, as in most of the work reviewed here. Donoho The difference between our approach and those of [Bruske
and Grimes [2003] propose a similar scheme which computes and Sommer, 1998][Brand, 2003] [Kegl,´ 2005] [Weinberger
the Hessian instead of the Laplacian. The authors claim that and Saul, 2004] [Costa and Hero, 2004] and [Levina and
the Hessian is better suited for detecting linear patches on the Bickel, 2005] is that it produces reliable dimensionality es-
manifold. The major contribution of this approach is that it timates at the point level, which do not have to be averaged
proposes a global method, which, unlike Isomap, can be ap- over the entire dataset. We are also able to estimate the local
plied to non-convex datasets.                         orientation of the manifold without being restricted to simple
  Weinberger and Saul [2004] address the problem of man- linear models.3  Tensor Voting
In this section we ﬁrst review the tensor voting framework
[Medioni et al., 2000], which can infer structures from sparse
and noisy data via a local voting process. Results have been
published mostly in 2- and 3-D domains, but the framework
can be extended to higher dimensions, as in [Tang et al.,
2001]. We begin by describing the basics of the framework:  (a) Example tensors  (b) Vote generation
data representation and voting. Then, we present the contri-
bution of this paper to the methodology, which is an efﬁcient Figure 1: Tensor Voting. (a) The shape of the tensor indicates
implementation, in terms of both space and time, of tensor the type of structure at the location, while its size indicates the
voting in high-dimensional spaces. We are able to operate in conﬁdence of this information. The top tensor has a strong
spaces where that seemed impractical or impossible based on preference of orientation and has higher conﬁdence than the
the formulation of [Medioni et al., 2000].            bottom one, which has no preference for any orientation. (b)
                                                      Vote generation as a function of the relative position of a stick
3.1  Data Representation                              voter and the receiver and the orientation of the voter.
The representation at each point is in the form of a second
order, symmetric, nonnegative deﬁnite tensor. The tensor can
                                                      where λd are the eigenvalues in descending order and eˆd are
also be viewed as a matrix or an ellipsoid. It represents the the corresponding eigenvectors. The tensor simultaneously
normal space at the point. For instance, a hyper-plane has
                                                 T    encodes all possible types of structure. The conﬁdence we
one normal ~n, which can be encoded in tensor form as ~n~n . have in the type that has d normals is encoded in the differ-
A structure with a normal space of rank d has d normals and
                                                      ence λd − λd+1.
is represented by a tensor of the form:
                       X      T                       3.2  The Voting Process
                   T =    ~nd~n                 (1)
                              d                       The tensor voting framework was designed to enforce con-
                        d
                                                      straints, such as proximity, co-linearity and co-curvilinearity
A point without orientation information can be equivalently with human perception in 2- and 3-D in mind. These con-
viewed as one having all possible normals and is encoded as straints still hold in high dimensional spaces as long as one is
the identity matrix. A tensor in this form represents an equal looking for structures that are smooth almost everywhere. In
preference for all orientations and is called a “ball tensor”, this section we describe how information is propagated from
since the corresponding ellipsoid is a sphere or hyper-sphere. one point, the voter, to another, the receiver.
On the other hand, a tensor that contains only one orientation During the voting process, each point casts votes to each of
is called a “stick tensor”. Stick tensors represent the hyper- its neighboring points. The votes are also second-order, sym-
plane of a D-dimensional space, which has one normal and metric, nonnegative deﬁnite tensors. They encode the orienta-
D − 1 tangents.                                       tion the receiver would have, if the voter and receiver were in
  Depending on the type of structure the point belongs to, the same structure. We begin by examining the case of a voter
it has a different number of normals and tangents. The ten- that is associated with a stick tensor of unit length. The mag-
sor’s eigensystem encodes this information. Eigenvectors that nitude of a vote cast by the stick tensor decays with respect
belong to the tangent space correspond to zero eigenvalues, to the length of the smooth circular path connecting the voter
while those that belong to the normal space correspond to and receiver. The circle was selected since it maintains con-
nonzero eigenvalues (typically equal to 1). Therefore, points stant curvature. It degenerates to a straight line if the vector
with known orientation can be encoded in this representa- connecting the voter and receiver is orthogonal to the normal
tion by appropriately constructed tensors, as in (1). Points of the voter. The orientation of the vote is towards the center
with unknown orientation are represented by identity matri- of the circle deﬁned by the two points and the orientation of
ces (ball tensors) See Fig. 1(a) for example tensors in 3-D. the voter. The vote is generated according to the following
  On the other hand, given a second order, symmetric, non- equation and then transformed to the appropriate coordinate
negative deﬁnite tensor, the type of structure encoded in it system.
can be found by examining its eigensystem. Any tensor that
has these properties can be decomposed as in the following
                                                                      2   2           
equation:                                                          −( s +cκ ) −sin(2θ)
                                                         (s, l, θ) = e σ2                [−sin(2θ) cos(2θ)]
                                                        S                      cos(2θ)
         D
      X         T
  T =      λdeˆdeˆd =                                                           θk~vk        2sin(θ)
                                                                           s =       ,   κ =          (3)
      d=1                                                                      sin(θ)          k~vk
      = (λ  − λ )ˆe eˆT + (λ − λ )(ˆe eˆT +e ˆ eˆT )
          1    2  1 1     2   3   1 1   2 2           s is the length of the arc between the voter and receiver, and
                     T      T          T
        + .... + λD(ˆe1eˆ1 +e ˆ2eˆ2 + ... +e ˆdeˆd ) = κ is its curvature (see Fig. 1(b)), σ is the scale of voting, and
   D−1                d
X                  X      T         T         T       c is a constant. No vote is generated if the angle θ is more
      [(λd −λd + 1)    eˆdeˆd ]+λD(ˆe1eˆ1 +...+e ˆdeˆd ) than 45o. Also, the ﬁeld is truncated to the extend where the
d=1                k=1                                magnitude of the vote is more than 3% of the magnitude of
                                                (2)   the voter.  Since tensor voting is a function of the position of the voter to ~vn. We can exploit this by deﬁning a new basis for the
and the receiver and the orientation of the voter, vote gener- normal space of the voter that includes ~vn. Then, the vote is
ation by a stick tensor occurs in a 2-D subspace deﬁned by constructed as the tensor addition of the votes cast by stick
the vector ~v that connects the two points and the normal of tensors parallel to the new basis vectors. Among those votes,
the voter. Therefore, stick voting is fully deﬁned by Fig. 1(b) only the one generated by the stick tensor parallel to ~vn is
and Eq. 3, regardless of the dimension of the input space. not parallel to the normal space of the voter. See Fig. 2 for an
According to [Medioni et al., 2000], the votes cast by other illustration in 3-D. This scheme is applicable without changes
types of tensors were pre-computed and stored in look-up ta- in the case of ball voters. Since two unoriented points deﬁne
bles, called voting ﬁelds. The presence of a normal space a straight line in any space, the vote from a ball tensor should
of rank more than one is simulated by a rotating stick ten- have D −1 normals and one tangent. Since ~vt = ~0, the tensor
                                                                                             o
sor that spans the space. Since no closed form solution ex- parallel to ~vn does not cast a vote due to the 45 cut-off rule.
ists, the integration is numerically approximated. This pro- The remaining D − 1 tensors in the new basis cast votes that
cess becomes impractical as the dimensionality of the space are equal in magnitude and span the space orthogonal to ~v,
increases. Space requirements for storing D D-dimensional and thus represent a straight line connecting the two points.
                                         D
voting ﬁelds with k samples per axis are O(Dk ). At the Tensors with unequal eigenvalues are decomposed before
same time, the advantage of re-using pre-computed votes van- voting according to Eq. 2. Then, each component votes sep-
ishes as the dimensionality increases. We propose instead a arately and the vote is weighted by λd − λd+1, except the
novel, efﬁcient way to directly compute votes. Since integra- ball component whose vote is weighted by λD. This imple-
tion is infeasible, we seek an approximate solution.  mentation of tensor voting is more efﬁcient in terms of space,
                                                      since there is no need to store voting ﬁelds, and in terms of
3.3  Efﬁcient Voting Scheme                           time since we have devised a direct method for computing
The new scheme, proposed here, generates votes that contain votes that replaces the numerical integration of [Medioni et
the orientation information that should be communicated to al., 2000]. The new computation requires at most D compu-
the receiver, but not the uncertainty. The latter, in the orig- tations of a stick vote according to Eq. 3, followed by the
inal framework, was conveyed by the ball component of the addition of all the votes, instead of integration over D dimen-
vote that was due to the integration of votes cast by the rotat- sions. Experiments in 2- and 3-D, that cannot be included
ing stick tensor. For instance, according to [Medioni et al., here due to space limitations, validate the accuracy of the new
2000], a voting tensor that encodes a curve in 3-D (that has vote generation scheme.
two normals and a tangent) casts a vote that has a dominant
curve component and also some uncertainty. Here, we pro- 3.4 Vote Analysis
pose to propagate only the curve orientation at the receiver Votes are accumulated at each point by tensor addition, which
given the orientation of the voter. The uncertainty in the new is equivalent to the addition of matrices. After voting is com-
formulation comes only from the accumulation of votes that pleted, the eigensystem of the new tensor is analyzed and the
are not perfectly aligned.                            tensor is decomposed as in Eq. 2. In 3-D, the inference of
  Let ~v be the vector connecting the voting and receiving the type of structure that a point belongs is accomplished as
points. It can be decomposed into ~vt in the tangent space follows. The difference between the two largest eigenvalues
of the voter (null in the case of a ball voter) and ~vn in the encodes surface conﬁdence, with a surface normal given by
normal space. The new vote generation process is based on
                                                      ~e1. The difference between the second and third eigenvalue
the observation that curvature in Eq. 3 is not a factor when encodes curve conﬁdence, with a the normals to the curve
θ is zero, or in other words, if the voting stick is orthogonal
                                                      being ~e1 and ~e2. The smallest eigenvalue encodes junction
                                                      conﬁdence. Outliers receive little and inconsistent support
                                                      from their neighborhood and can be identiﬁed by their low
                                                      eigenvalues. Generalization to higher dimensions is straight-
                                                      forward, considering that there are D manifold types in D
                                                      dimensions. The dimensionality d is estimated by ﬁnding the
                                                      maximum difference λd − λd+1.

                                                      4   Experimental Results
                                                      In this section we present experimental results in dimension-
Figure 2: New scheme for vote generation. The voter here ality estimation and local structure inference. All inputs con-
is a tensor with two normals in 3-D. The vector connecting sist of un-oriented points and are encoded as ball tensors.
the voter and receiver is decomposed into ~vn and ~vt that lie
in the normal and tangent space of the voter respectively. A
new basis that includes ~vn is deﬁned for the normal space Swiss Roll The ﬁrst experiment is on the “Swiss Roll”
and each basis component casts a stick vote. Only the vote dataset, which is available at http://isomap.stanford.edu/. It
generated by the orientation parallel to ~vn is not parallel to contains 20, 000 points on a 2-D manifold in 3-D (Fig. 3).
the normal space. Tensor addition of the stick votes produces We perform a simple evaluation of the quality of the orien-
the combined vote.                                    tation estimates by projecting the nearest neighbors of each        Figure 3: The “Swiss Roll” dataset in 3-D                (a) Input            (b) 1-D points

point on the estimated tangent space and measuring the per-
centage of the distance that has been recovered. The accuracy
of dimensionality estimation at each point, the percentage of
recovered distances for the 8 nearest neighbors and execution
times on a Pentium 4 at 2.8 GHz, as a function of σ, can
be seen in Table 1. Performance is the same at boundaries.    (c) 2-D points          (d) 3-D points
The number of votes cast by each point ranges from 187 for
σ2 = 50 to 5440 for σ2 = 1000. The accuracy is high for a
large range of σ.                                     Figure 4: Data of varying dimensionality in 4-D. The ﬁrst
                                                      three axes of the input and the classiﬁed points are shown.
      σ2        Correct    Dist.     Time
                Dim. (%)   Rec. (%)  (sec)            and embedded in a 50- to 150-D space while uniform noise
      50        93.10      91.75     11               was added to all dimensions. The accuracy of dimensionality
      100       99.24      93.07     22               estimation after tensor voting can be seen in Table 2.
      200       99.91      93.21     50
      300       99.96      93.20     81                 Intrinsic Linear    Quadratic Space     Dim.
      500       99.95      93.18     144                Dim.      Mappings  Mappings  Dim.      Est. (%)
      700       99.88      93.13     202                4         10        6         50        93.6
      1000      99.67      93.02     307                3         8         6         100       97.4
                                                        4         10        6         100       93.9
Table 1: Rate of correct dimensionality estimation and exe- 3     8         6         150       97.3
cution times as functions of σ for the “Swiss Roll” dataset
                                                      Table 2: Rate of correct dimensionality estimation for high
                                                      dimensional data
Structures with varying  dimensionality The  second
dataset is in 4-D and contains points sampled from three
structures: a line, a 2-D cone and a 3-D hyper-sphere. The 5 Conclusions
hyper-sphere is a structure with three degrees of freedom. It We have presented an approach to manifold learning that of-
cannot be unfolded unless we remove a small part from it. fers certain advantages over the state-of-the-art. First, we are
Figure 4(a) shows the ﬁrst three dimensions of the data. The able to obtain accurate estimates of the intrinsic dimension-
dataset contains a total 135, 864 points, voting is performed
     2                                                ality at the point level. Moreover, since the dimensionality is
with σ = 200 and takes 2 hours and 7 minutes. Figures 4(c- found as the maximum gap in the eigenvalues of the tensor at
d) show the points classiﬁed according to their dimensional- the point, no thresholds are needed. In most other approaches
ity. Performing the same analysis as above for the accuracy the dimensionality has to be provided, or, at best, an average
of the tangent space estimation, 91.04% of the distances of intrinsic dimensionality is estimated for the entire dataset, as
the 8 nearest neighbors of each point lie on the tangent space, in [Bruske and Sommer, 1998] [Brand, 2003] [Kegl,´ 2005]
even though both the cone and the hyper-sphere have intrin- [Weinberger and Saul, 2004] [Costa and Hero, 2004]. Sec-
sic curvature and cannot be accurately approximated by linear ond, even though tensor voting on the surface looks like a
models. All the methods presented in Sec. 2 would fail for local covariance estimation, the fact that the votes are tensors
this dataset because of the presence of structures with differ- and not scalars reveals more information for each point. The
ent dimensionalities and because the hyper-sphere cannot be properties of the tensor representation, which can handle the
unfolded.                                             simultaneous presence of multiple orientations, allow the reli-
                                                      able inference of the normal and tangent space at each point.
Data in high dimensions The datasets for this experiment Due to its unsupervised nature, tensor voting is very robust
were generated by sampling a few thousand points with low against outliers, as demonstrated in [Medioni et al., 2000] for
intrinsic dimensionality (3 or 4) and mapping them to a the 2- and 3-D case. This property holds in higher dimen-
medium dimensional space (14- to 16-D) using linear and sions, where random noise is even more scattered. Lack of
quadratic functions. The generated points were then rotated space did not allow us to include outlier rejection results here.