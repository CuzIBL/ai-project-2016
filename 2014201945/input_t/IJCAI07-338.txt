                Team Programming in Golog under Partial Observability

        Alessandro Farinelli                Alberto Finzi  ∗              Thomas Lukasiewicz     †
      DIS, Universit`adiRoma            Institut f¨ur Informations-       DIS, Universit`adiRoma
           “La Sapienza”                   systeme, TU Wien                    “La Sapienza”
         00198 Rome, Italy                1040 Vienna, Austria               00198 Rome, Italy


                    Abstract                          Guestrin et al., 2003]), which allow for (i) compactly repre-
                                                      senting decision-theoretic planning problems without explic-
    In this paper, we present the agent programming   itly referring to atomic states and state transitions, (ii) exploit-
    language TEAMGOLOG, which is a novel approach     ing such compact representations for efﬁciently solving large-
    to programming a team of cooperative agents under scale problems, and (iii) nice properties such as modularity
    partial observability. Every agent is associated with (parts of the speciﬁcation can be easily added, removed, or
    a partial control program in Golog, which is com- modiﬁed) and elaboration tolerance (solutions can be easily
    pleted by the TEAMGOLOG interpreter in an opti-   reused for similar problems with few or no extra cost).
    mal way by assuming a decision-theoretic seman-     However, DTGolog is designed only for the single-agent
    tics. The approach is based on the key concepts of framework. That is, the model of the world essentially con-
    a synchronization state and a communication state, sists of a single agent that we control by a DTGolog program
    which allow the agents to passively resp. actively and the environment summarized in “nature”. But there are
    coordinate their behavior, while keeping their be- many applications where we encounter multiple agents that
    lief states, observations, and activities invisible to cooperate with each other. For example, in robotic rescue,
    the other agents. We show the usefulness of the ap- mobile agents may be used in the emergency area to acquire
    proach in a rescue simulated domain.              new detailed information (such as the locations of injured
                                                      people in the emergency area) or to perform certain rescue
1  Introduction                                       operations. In general, acquiring information as well as per-
                                                      forming rescue operations involves several and different res-
During the recent years, the development of controllers for cue elements (agents and/or teams of agents), which cannot
autonomous agents has become increasingly important in AI. effectively handle the rescue situation on their own. Only the
One way of designing such controllers is the programming cooperative work among all the rescue elements may solve
approach, where a control program is speciﬁed through a lan- it. Since most of the rescue tasks involve a certain level of
guage based on high-level actions as primitives. Another way risk for humans (depending on the type of rescue situation),
is the planning approach, where goals or reward functions are mobile agents can play a major role in rescue situations, es-
speciﬁed and the agent is given a planning ability to achieve a pecially teams of cooperative heterogeneous mobile agents.
goal or to maximize a reward function. An integration of both Another crucial aspect of real-world environments is that
approaches has recently been proposed through the seminal they are typically only partially observable, due to noisy and
                [                  ]
language DTGolog Boutilier et al., 2000 , which integrates inaccurate sensors, or because some relevant parts of the en-
                                  [          ]
explicit agent programming in Golog Reiter, 2001 with vironment simply cannot be sensed. For example, especially
                                              [
decision-theoretic planning in (fully observable) MDPs Put- in the robotic rescue domain described above, every agent has
          ]
erman, 1994 . It allows for partially specifying a control pro- generally only a very partial view on the environment.
gram in a high-level language as well as for optimally ﬁlling
in missing details through decision-theoretic planning, and it The practical importance of controlling a system of coop-
can thus be seen as a decision-theoretic extension to Golog, erative agents under partial observability by a generalization
where choices left to the agent are made by maximizing ex- of DTGolog has already been recognized in recent works
pected utility. From a different perspective, it can also be by Ferrein et al. [2005] and Finzi and Lukasiewicz [2006].
seen as a formalism that gives advice to a decision-theoretic A drawback of these two works, however, is that they are
planner, since it naturally constrains the search space. implicitly centralized by the assumption of a global world
                                                      model resp. the assumption that every agent knows the belief
  DTGolog has several other nice features, since it is closely
related to ﬁrst-order extensions of decision-theoretic plan- states, observations, and actions of all the other agents (and so
                                                      [                                         ]
ning (see especially [Boutilier et al., 2001; Yoon et al., 2002; Ferrein et al., 2005; Finzi and Lukasiewicz, 2006 have no
                                                      explicit communication between the agents), which is very
  ∗Alternate address: DIS, Universit`a di Roma “La Sapienza”. often not possible or not desirable in realistic applications.
  †Alternate address: Institut f¨ur Informationssysteme, TU Wien. In this paper, we present the agent programming language

                                                IJCAI-07
                                                  2097TEAMGOLOG, which is a novel generalization of DTGolog •Dssa  is the set of successor state axioms [Reiter, 2001].
for controlling a system of cooperative agents under partial For each ﬂuent F (x, s), it contains an axiom F (x, do(a,
observability, which does not have such centralization as- s)) ≡ ΦF (x, a, s),whereΦF (x, a, s) is a formula with free
sumptions. It is thus guided by the idea of truly distributed variables among x, a,ands. These axioms specify the truth
acting in multi-agent systems with a minimal interaction be- of the ﬂuent F in the next situation do(a, s) in terms of the
tween the agents. The main contributions are as follows: current situation s, and are a solution to the frame problem
                                                      (for deterministic actions). For example,
• We introduce the agent programming language TEAM-
GOLOG  for controlling a system of cooperative (middle-size) at(o, x, y, do(a, s)) ≡ a = moveTo(o, x, y) ∨
agents under partial observability. We deﬁne a decision-the- at(o, x, y, s) ∧¬∃x,y (a = moveTo(o, x,y))
oretic semantics of TEAMGOLOG, which are inspired by de-
                                                      may express that the object o is at (x, y) in do(a, s) iff it is
centralized partially observable MDPs (Dec-POMDPs) [Nair           s                                  s
et al., 2003; Goldman and Zilberstein, 2004].         moved there in , or already there and not moved away in .
                                                      •Dap   is the set of action precondition axioms. For each
• We introduce the concepts of a synchronization state and a
                                                      action a, it contains an axiom Poss(a(x),s) ≡ Π(x, s),
communication state, which are used to coordinate the agents,
                                                      which characterizes the preconditions of a. For example,
taking inspiration from artiﬁcial social systems [Shoham and                            
                                                      Poss(moveT o(o, x, y),s) ≡¬∃o (at(o ,x,y,s)) may ex-
Tennenholtz, 1995]: The behavior of each agent is encoded
                                                      press that it is possible to move the object o to (x, y) in s iff
in advance in its domain theory and program, and depends on         
                                                      no other object o is at (x, y) in s.
the online trace of synchronization and communication states.
• We deﬁne an interpreter for TEAMGOLOG, and provide a  Golog is an agent programming language that is based on
number of theoretical results around it. In particular, we show the situation calculus. It allows for constructing complex ac-
that the interpreter generates optimal policies.      tions from the primitive actions deﬁned in a basic action the-
                                                      ory AT , where standard (and not so standard) Algol-like con-
• We describe a ﬁrst prototype implementation of the TEAM-
                                                      structs can be used, in particular, (i) action sequences: p1; p2;
GOLOG  interpreter. We also provide experimental results
                                                      (ii) tests: φ?; (iii) nondeterministic action choices: p1|p2;
from the rescue simulation domain, which give evidence of (iv) nondeterministic choices of action argument: πx (p(x));
the usefulness of our approach in realistic applications. and (v) conditionals, while-loops, and procedures.
  Notice that detailed proofs of all results in this extended
abstract are provided in the full version of this paper. 3 Team Golog under Partial Observability
                                                      We now introduce the agent programming language TEAM-
2  The Situation Calculus and Golog                   GOLOG, which is a generalization of Golog for programming
The situation calculus [McCarthy and Hayes, 1969; Reiter, teams of cooperative agent under partial observability.
2001] is a ﬁrst-order language for representing dynamic do- Our approach is based on the key concepts of a synchro-
mains. Its main ingredients are actions, situations,andﬂu- nization state and a communication state, which allow the
ents.Anaction is a ﬁrst-order term of the form a(u),where agents to passively resp. actively coordinate their behavior,
a is an action name, and u are its arguments. For example, while keeping their belief states, observations, and activities
moveTo(r, x, y) may represent the action of moving an agent invisible to the other agents. Here, the synchronization state
r to the position (x, y).Asituation is a ﬁrst-order term en- is fully observable by all the agents, but outside their con-
coding a sequence of actions. It is either a constant symbol or trol. The communication state is a multi-dimensional state,
of the form do(a, s),wherea is an action and s is a situation. containing one dimension for each agent, which is also fully
The constant symbol S0 is the initial situation and represents observable by all the agents. But every agent may change its
the empty sequence, while do(a, s) encodes the sequence ob- part of the communication state whenever necessary, which
tained from executing a after the sequence of s.Forexam- encodes its explicit communication to all the other agents.
ple, do(moveTo(r, 1, 2),do(moveTo(r, 3, 4),S0)) stands for Since both the synchronization state S and the commu-
executing moveTo(r, 1, 2) after executing moveTo(r, 3, 4) nication state C are fully observable by all the agents, they
in S0.Aﬂuent   represents a world or agent property that can be used to condition and coordinate the behavior of the
may change when executing an action. It is a predicate sym- agents. At the same time, each agent can keep its belief state,
bol whose most right argument is a situation. For example, observations, and actions invisible to the other agents. We
at(r, x, y, s) may express that the agent r is at the position thus realize a maximally distributed acting of the agents. The
(x, y) in the situation s. In the situation calculus, a dynamic TEAMGOLOG program of each agent encodes the agent’s be-
                                                                         S    C
domain is encoded as a basic action theory AT =(Σ, DS0 , havior conditioned on and , and thus on the current situa-
Dssa, Duna, Dap),where:                               tion. Hence, TEAMGOLOG programs bear close similarity to
                                                      social laws in artiﬁcial social systems [Shoham and Tennen-
• Σ is the set of foundational axioms for situations.
                                                      holtz, 1995]. The basic idea behind such systems is to formu-
•Duna  is the set of unique name axioms for actions,saying late a mechanism, called social law, that minimizes the need
that different action terms stand for different actions. for both centralized control and online resolution of conﬂicts.

•DS0  is a set of ﬁrst-order formulas describing the initial There are many real-world situations where we encounter
state of the domain (represented by S0). E.g., at(r, 1, 2,S0) such a form of coordination. E.g., the trafﬁc law “right has
may express that the agent r is initially at the position (1, 2). precedence over left” regulates the order in which cars can

                                                IJCAI-07
                                                  2098                                                              0
pass a street cross. In most cases, this law is sufﬁcient to Qi (c, s, si,ai)=Ri(c, s, si,ai)
make the cars pass the street cross without any further inter- Qn(c, s, s ,a )=R (c, s, s ,a )+
                                                              i   i  i   i    i i
action between the car drivers. Only in exceptional cases,                          
                                                                                Pi(ci,si,oi|c, s, si,ai) ·
such as the one where on each street a car is approaching the        
                                                               c ∈C s ∈S s ∈Si oi∈Oi
                                                                       i       n−1    
cross or when a car has a technical defect, some additional       Pc (c−i,s |c, s) · V (c ,s,si)
                                                                    i           i
communication between the car drivers is necessary. Sim-      m                m
                                                             Vi (c, s, si)= min Qi (c, s, si,ai) ,
ilarly, a soccer team can ﬁx in advance the behavior of its              ai∈Ai
team members in certain game situations (such as defense or
attack), thus minimizing the explicit communication between        Figure 1: Q-andV -Functions
the members during the game (which may be observed by
                                                                                ,   ,               n ≥
the adversary). In these two examples, the synchronization a domain theory DT i =(AT i ST i OT i) consists of 2
                                                            I  { ,...,n}                 i ∈ I
state encodes the situation at the street cross resp. the game agents = 1 , and for each agent : a basic action
situation, while the communication state encodes the explicit theory AT i,astochastic theory ST i,andanoptimization the-
communication. The correct behavior of the car drivers resp. ory OT i, where the latter two are deﬁned below.
                                                                                                A
soccer players is encoded by trafﬁc laws resp. the strategy The ﬁnite nonempty set of primitive actions is parti-
                                                                                             A ,...,A
ﬁxed by the team in their training units and before the game. tioned into nonempty sets of primitive actions 1 n of
                                                             ,...,n
  In the rest of this section, we ﬁrst deﬁne a variant of Dec- agents 1 , respectively. We assume a ﬁnite nonempty
                                                                      O
POMDPs, which underlies the decision-theoretic semantics set of observations , which is partitioned into nonempty sets
                                                                   O  ,...,O           ,...,n
of TEAMGOLOG   programs. We then deﬁne the domain the- of observations 1    n of agents 1    , respectively.
                                                                                       i ∈ I
ory and the syntax of TEAMGOLOG programs.               A  stochastic theory ST i for agent is a set of ax-
                                                      ioms that deﬁne stochastic actions for agent i.Werepresent
3.1  Weakly Correlated Dec-POMDPs                     stochastic actions through a ﬁnite set of deterministic actions,
We consider the following variant of Dec-POMDPs for n ≥ 2 as usual [Finzi and Pirri, 2001; Boutilier et al., 2000].When
agents, which essentially consist of a transition function be- a stochastic action is executed, then with a certain proba-
tween global states (where every global state consists of bility, “nature” executes exactly one of its deterministic ac-
a communication state for each agent and a synchroniza- tions and produces exactly one possible observation. As un-
tion state) and a POMDP for each agent and each global derlying decision-theoretic semantics, we assume the weakly
state, where every agent can also send a message to the oth- correlated Dec-POMDPs of Section 3.1, along with the re-
                                                                                                 s
ers by changing its communication state. A weakly cor- lational ﬂuents that associate with every situation acom-
                                                                    c         j ∈ I                    z
related Dec-POMDP  (I,S,(Ci)i∈I ,P,(Si)i∈I , (Ai)i∈I , munication state j of agent , a synchronization state ,
                                                                     s         i
(Oi)i∈I , (Pi)i∈I , (Ri)i∈I ) consists of a set of n ≥ 2 agents and a local state i of agent , respectively. The communi-
I = {1,...,n}, a nonempty ﬁnite set of synchronization cation and synchronization properties are visible by all the
states S, a nonempty ﬁnite set of communication states Ci agents, the others are private and hidden. We use the pred-
                                                                    a, s, n, o, μ
for every agent i ∈ I, a transition function P : C × S → icate stochastic(    ) to encode that when executing
                                                                       a              s
PD(C  × S), which associates with every global state, con- the stochastic action in the situation , “nature” chooses the
                                                                       n                        o
sisting of a joint communication state c ∈ C = ×i∈I Ci and deterministic action producing the observation with the
                                                                μ                              a
a synchronization state s ∈ S, a probability distribution over probability . Here, for every stochastic action and situa-
                                                          s            n, o, μ                 a, s, n, o, μ
C × S, and for every agent i ∈ I: (i) a nonempty ﬁnite set of tion ,thesetofall( ) such that stochastic( )
local states Si, a nonempty ﬁnite set of actions Ai, (ii) a non- is a probability function on the set of all deterministic com-
                                                             n                o   a   s
empty ﬁnite set of observations Oi, (iii) a transition function ponents and observations of in . We also use the no-
                                                                a, s, n, o                     μ
Pi: C × S × Si × Ai → PD(Ci × Si × Oi), which associates tation prob(  ) to denote the probability such that
                                                                a, s, n, o, μ            a
with every global state (c, s) ∈ (C, S), local state si ∈ Si,and stochastic( ). We assume that and all its nature
                                                             n                                          a
action ai ∈ Ai a probability distribution over Ci × Si × Oi, choices have the same preconditions. A stochastic action
and (iv) a reward function Ri: C × S × Si × Ai → R,which is indirectly represented by providing a successor state ax-
                                                                                       n
associates with every global state (c, s) ∈ C × S, local state iom for every associated nature choice . The stochastic ac-
                                                          a                        s                o
si ∈ Si, and action ai ∈ Ai a reward Ri(c, s, si,ai) to agent i. tion is executable in a situation with observation ,de-
                                                                 a ,s         a, s, n, o >        n
  The q-andv-functions for agent i ∈ I of a ﬁnite-horizon noted Poss( o ),iffprob(   )  0 for some  .The
                                                                                     i ∈ I
value iteration are deﬁned in Fig. 1 for n>0 and m ≥ 0, optimization theory OT i for agent speciﬁes a reward
                                                                               i
     Pc ·|c, s                   P ·|c, s   c        and a utility function for agent . The former associates with
where  i (    ) is the conditioning of ( ) on i,and
                                                   every situation s and action a, a reward to agent i ∈ I, denoted
c  denotes c without c . That is, an optimal action of agent i
 −i                i                                  reward (i, a, s). The utility function maps every reward and
in the global state (c, s) and the local state si when there are n
                                n                     success probability to a real-valued utility utility(v, pr).We
stepstogoisgivenbyargmina   ∈A Q (c, s, si,ai). Notice
                           i  i i                     assume utility(v, 1) = v and utility(v, 0) = 0 for all v.An
that these are the standard deﬁnitions of q-andv-functions,
                                                      example is utility(v, pr)=v · pr. The utility function suit-
adapted to our framework of local and global states.
                                                      ably mediates between the agent reward and the failure of
3.2  Domain Theory                                    actions due to unsatisﬁed preconditions.
TEAMGOLOG   programs are interpreted relative to a domain Example 3.1 (Rescue Domain) We consider a rescue domain
theory, which extends a basic action theory by stochastic where several autonomous mobile agents have to localize
actions, reward functions, and utility functions. Formally, some victims in the environment and report their positions

                                                IJCAI-07
                                                  2099to a remote operator. We assume a team of three heteroge- set of pairs (s, μ) consisting of an ordinary situation s and a
neous agents a1, a2,anda3 endowed with shape recognition real μ ∈ (0, 1] such that (i) all μ sum up to 1, and (ii) all situa-
(SH), infrared (IF), and CO2 sensors, respectively. A vic- tions s in b are associated with the same joint communication
tim position is communicated to the operator once sensed state and the same synchronization state. Informally, every b
and analyzed by all the three sensing devices. Each agent represents the local belief of agent i ∈ I expressed as a proba-
ai can execute one of the actions goToi(pos ), analyze i(pos , bility distribution over its local states, along with unique joint
typei),andreportToOp i(pos ). The action theory AT i is de- communication and synchronization states. The probability
scribedbytheﬂuentsat i(pos ,s), analyzed i(pos , typei,s), of a ﬂuent formula φ(s) (uniform in s) in the belief state b,
and reported i(x, s), which are accessible only by agent ai. denoted φ(b),isthesumofallμ such that φ(s) is true and
E.g., the successor state axiom for at i(pos ,s) is   (s, μ) ∈ b. In particular, Poss(a, b),wherea is an action, is
                                                      deﬁned as the sum of all μ such that Poss(a, s) is true and
    at i(pos, do(a, s)) ≡ a = goToi(pos ) ∨ at i(pos ,s) ∧
                                                    (s, μ) ∈ b,andreward (i, a, b) is deﬁned in a similar way.
                     ¬∃pos (a = goToi(pos ))
                                                        Given a deterministic action a and a belief state b of agent
and the precondition axiom for the action analyze i is given i ∈ I,thesuccessor belief state after executing a in b,de-
                   ,     ,s ≡        ,s                                            
by Poss(analyze i(pos typei) ) at i(pos ).Asforthe    noted do(a, b), is the belief state b = {(do(a, s),μ/Poss(a,
global state, the communication state is deﬁned by the ﬂuent b)) | (s, μ) ∈ b, Poss(a, s)}. Furthermore, given a stochas-
       ,s       i
csi(data ),where  is the agent, and data is the shared info, tic action a, an observation o of a, and a belief state b of
                   , ,   ,s            a
e.g., cs1(atVictim((2 2) IF) ) means that 1 detected a agent i ∈ I,thesuccessor belief state after executing a in b
                 ,                                                                                      
victim in position (2 2) through the IF sensor. Other global and observing o, denoted do(ao,b), is the belief state b ,
                 p                           p                                                
data are repVictim ( ) (victim reported in position )and where b is obtained from all pairs (do(n, s),μ· μ ) such that
         p          p                                                          
noVictim( ) (position was inspected and there is no vic- (s, μ) ∈ b, Poss(a, s),andμ = prob(a, s, n, o) > 0 by nor-
                                     s           s
tim). Some synchronization states are start( ) and reset ( ) malizing the probabilities to sum up to 1.
standing for starting resp. resetting the rescue session. In The probability of making the observation o after executing
ST i, we deﬁne the stochastic versions of the actions in AT i,         a                    b       i ∈ I
                                ,                     the stochastic action in the local belief state of agent ,
e.g., goToS i(pos ) and analyzeS i(pos typei). Each of these denoted prob(a, b, o),isdeﬁnedasthesumofallμ · μ such
can fail resulting in an empty action, e.g.,          that (s, μ) ∈ b and μ = prob(a, s, n, o) > 0.
     prob(goToS i(pos ),s,goToi(pos ), obs(succ)) = 0.9,
     prob(goToS i(pos ),s,nop, obs(fail)) = 0.7 .     Example 3.2 (Rescue Domain cont’d) Suppose that agent a1
                                                      is aware of its initial situation, and thus has the initial be-
In OT i, we provide a high reward for a fully analyzed victim
                                                      lief state {(S0, 1)}. After executing the stochastic action
correctly reported to the operator, a low reward for the analy-
                                                      goToS 1(1, 1) and observing its success obs(succ), the be-
sis of a detected victim, and a (distance-dependent) cost is as-
                                                      lief state of a1 then changes to {(S0, 0.1), (do(goTo (1,
sociated with the action goTo. Since two agents can obstacle                                         1
                                                      1),S0), 0.9)} (here, prob(goToS 1(pos ),s,goTo1(pos ),
each other when operating in the same location, we penalize
                                                      obs(succ)) = 0.9,andgoToS 1(pos ) is always executable).
the agents analyzing the same victim at the same time. More
precisely, we employ the following reward:
                                                      3.4  Syntax
 reward (i, a, s)=r =def ∃p, t (a = analyzei(p, t) ∧
  (detVictim(p, s) ∧ (¬conﬂicts i(a, s) ∧ r =50∨      Given the actions speciﬁed by a domain theory DT i,apro-
          (a, s) ∧ r = 10) ∨¬    (p, s) ∧ r = − 10) ∨
  conﬂicts i             detVictim                    gram p in TEAMGOLOG   for agent i ∈ I has one of the fol-
  a = reportToOp i(p) ∧ fullyAnalyzed(p, s) ∧ r = 200 ∨                 φ             p, p ,p
                                                   lowing forms (where is a condition, 1 2 are programs,
  a = goTo (p) ∧∃p (at i(p ,s) ∧ r = − dist(p ,p))) ,
         i                                            and a, a1,...,an are actions of agent i):
where conﬂicts i is true if another agent communicates the
                                                        1. Deterministic or stochastic action: a.Doa.
analysis of the same location in the global state; detVic-
tim(p, s) is true if at least one agent has discovered a vic- 2. Nondeterministic action choice: choice(i: a1|···|an).
tim in p, i.e., csi(atVictim(p, t),s) for some i and t;and Do an optimal action among a1,...,an.
             p, s
fullyAnalyzed(  ) means that all the analysis has been  3. Test action: φ?.Testφ in the current situation.
performed, i.e., cs1(atVictim(p, SH ),s) ∧ cs2(atVictim(p,
IF),s) ∧ cs3(atVictim(p, CO 2),s). Notice that the action 4. Action sequence: p1; p2.Dop1 followed by p2.
      p has a cost depending on the distance between start-
goToi( )                                                5. Nondeterministic choice of two programs: (p1 | p2).
ing point and destination, hence, in a greedy policy, the agent
                                                          Do p1 or p2.
should go towards the closest non-analyzed victim and ana-
lyze it. However, given the penalty on the conﬂicts, the agents 6. Nondeterministic choice of an argument: πx (p(x)).
are encouraged to distribute their analysis on different victims Do any p(x).
taking into account the decisions of the other agents.  7. Nondeterministic iteration: p.Dop zero or more times.

3.3  Belief States                                      8. Conditional: if φ then p1 else p2.
We next introduce belief states over situations for single 9. While-loop: while φ do p.
agents, and deﬁne the semantics of actions in terms of transi-
tions between belief states. A belief state b of agent i ∈ I is a 10. Procedures, including recursion.

                                                IJCAI-07
                                                  2100Example 3.3 (Rescue Domain cont’d) The following code • Stochastic ﬁrst program action and h>0:
represents an incomplete procedure explorei of agent i:           
                                                            G([a ;p ],b,h,π,v, pr)=def
            (      ,                                               l         
        proc explore i                                         ∃ (    G([aoq ; p ],b,h,aoq ; πq , vq, pr ) ∧
          πx(     (x);                                             q=1                        q
             gotoS i                                             π = a ;   q =1   l    o     π ∧
               (    )    [       (x,    );                           oq for    to do if q then q
           if obs succ then analyzeS i type i                    v =   l  v ·    (a, ,o ) ∧
            if obs(succ) ∧ fullyAnalyzed(x) then                      q=1 q prob   b q
                                                                    =   l     ·    (a, ,o )) .
             reportToOp i(repVictim (x))]);                      pr     q=1 pr q prob b q
          explore i) .
                                                      Here, o1,...,ol are the possible observations of the sto-
          i
Here, agent ﬁrst has to decide where to go. Once the posi- chastic action a. The generated policy is a conditional plan in
                  i
tion is reached, agent analyzes the current location deploy- which every such observation oq is considered.
ing one of its sensing devices. If a victim is detected, then the
                                                      •                                     h>
position of the victim is communicated to the operator. Nondeterministic ﬁrst program action and 0:
                                                                                 
                                                            G([choice (i: a1|···|an);p ],b,h,π,v, pr)=def
                                                              ∃ ( n  G([a ; p],b,h,a ; π , v , ) ∧
4   TEAMGOLOG      Interpreter                                    q=1    q         q  q  q prq
                                                                 k =                   (v ,  ) ∧
In this section, we ﬁrst specify the decision-theoretic seman-      argmax q∈{1,...,n} utility q prq
                                                                 π = π ∧ v = v ∧  =    ) .
tics of TEAMGOLOG programs in terms of an interpreter. We            k      k   pr  pr k
then provide theoretical results about the interpreter.
                                                      4.2  Theoretical Results
4.1  Formal Speciﬁcation                              The following result shows that the TEAMGOLOG interpreter
We now deﬁne the formal semantics of a TEAMGOLOG pro- indeed generates an optimal H-step policy π along with its
gram p for agent i ∈ I relative to a domain theory DT.We expected utility U to agent i ∈ I for a given TEAMGOLOG
associate with every TEAMGOLOG program p, belief state b, program p, belief state b, and horizon H ≥ 0.
and horizon H ≥ 0, an optimal H-step policy π along with its
                                                      Theorem 4.1 Let p be a TEAMGOLOG   program for agent
expected utility U to agent i ∈ I. Intuitively, this H-step pol-
                                                      i ∈ I w.r.t. a domain theory DT i,letb be a belief state, and
icy π is obtained from the H-horizon part of p by replacing
                                                      let H ≥ 0 be a horizon. Then, the optimal H-step policy π of
every nondeterministic action choice by an optimal action.
                                                      p in b along with its expected utility U to agent i ∈ I is given
  Formally, given a TEAMGOLOG program p for agent i ∈ I
                                                      by DT i |= G(p, b, H, π,  v, pr
) and U = utility(v, pr).
relative to a domain theory DT, a horizon H ≥ 0,andastart
belief state b of agent i, we say that π is an H-step pol- The next result gives an upper bound for the number of
icy of p in b with expected H-step utility U to agent i iff leaves in the evaluation tree, which is polynomial when the
DT |= G(p, b, H, π,  v, pr
) and U = utility(v, pr),where horizon is bounded by a constant. Here, n is the maximum
the macro G(p, b, h, π,  v, pr
) is deﬁned by induction on the among the maximum number of actions in nondeterministic
different constructs of TEAMGOLOG. The deﬁnition of G for action choices, the maximum number of observations after
some of the constructs is given as follows (the complete deﬁ- actions, the maximum number of arguments in nondetermin-
nition is given in the full version of this paper):   istic choices of an argument, and the number of pairs consist-
• Null program (p = nil) or zero horizon (h =0):      ing of a synchronization state and a communication state.
                                                                      p
    G(p, b, h, π, v, pr)=def π = stop ∧v, pr = 0, 1 . Theorem 4.2 Let be a TEAMGOLOG program for agent
                                                      i ∈ I w.r.t. a domain theory DT i,letb be a belief state, and
Intuitively, p ends when it is null or at the horizon end. let H ≥ 0 be a horizon. Then, computing the H-step policy π
• Stochastic ﬁrst program action with observation and h>0: of p in b along with its expected utility U to agent i ∈ I via G
                                                               O n3H
                                                     generates (    ) leaves in the evaluation tree.
   G([ao ; p ],b,h,π,v,pr)=def (Poss(ao,b)=0∧
     π =stop ∧v, pr = 0, 1) ∨ (Poss(ao,b) > 0 ∧
     ∃ ( l  G(p,  (a ,b),h−1,π , v , ) ∧           5   Rescue Scenario
         q=1     do  o        q   q prq
        π = ao ; for q =1to l do if oq then πq ∧      Consider the rescue scenario in Fig. 2. We assume that three
                          l
        v =      (i, ao,b)+    vq ·   (ao,b,oq ) ∧    victims have already been detected in the environment, but
           reward          q=1   prob
          =     (a ,b) · l     ·    (a ,b,o ))) .     not completely analyzed: in position (3, 7), the presence of
        pr  Poss  o      q=1 pr q prob o  q
                                                      Alice was detected by a1 through the SH sensor; in position
Here, ∃(F ) is obtained from F by existentially quantifying (7, 7), agent a2 discovered Bob through IF, and a3 analyzed
all free variables in F . Moreover, o1,...,ol are the different him through the CO2 sensor; ﬁnally, in position (4, 2), victim
pairs of a joint communication state and a synchronization Carol was detected by a2 with IF. We assume that this infor-
state that are compatible with ao,andprob(ao,b,oq) is the mation is available in the global state, that is, the properties
probability of arriving in such oq after executing ao in b.In- cs1(atVictim((3, 7), SH ),s), cs3(atVictim((7, 7), IF),s),
                       
formally, suppose p =[ao ; p ],whereao is a stochastic action cs2(atVictim((7, 7), CO 2),s),andcs2(atVictim((4, 2),
with observation. If ao is not executable in b,thenp has only IF),s) hold in the communication state of the agents. As for
the policy π = stop along with the expected reward v =0and the local state, we assume the belief states b1 = {(s1,1, 0.8),
the success probability pr =0. Otherwise, the optimal exe- (s1,2, 0.2)}, b2 = {(s2, 1)},andb3 = {(s3, 1)}, with at 1(3,
             
cution of [ao ; p ] in b depends on that one of p in do(ao,b). 6,s1,1), at 1(3, 5,s1,2), at 2(7, 7,s2),andat 3(3, 7,s3).

                                                IJCAI-07
                                                  2101