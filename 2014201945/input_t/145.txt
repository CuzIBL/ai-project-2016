 SMDP Homomorphisms: An Algebraic Approach to Abstraction in Semi-Markov 
                                         Decision Processes 
                             Balaraman Ravindran and Andrew G. Barto 
                                    Department of Computer Science 
                                       University of Massachusetts 
                                         Amherst, MA, U. S. A. 
                                       {ravi | barto} @cs.umass.edu 

                     Abstract                            Our objective in this article is to introduce the concept of 
                                                       an SMDP homomorphism and argue that it provides a unified 
    To operate effectively in complex environments     view of key issues essential for a rigorous treatment of ab•
    learning agents require the ability to selectively ig• straction for stochastic dynamic decision processes. The con•
    nore irrelevant details and form useful abstractions. cept of a homomorphism between dynamic systems, some•
    In this article we consider the question of what con• times called a "dynamorphism" [Arbib and Manes, 1975], 
    stitutes a useful abstraction in a stochastic sequen• has played an important role in theories of abstract automata 
    tial decision problem modeled as a semi-Markov     [Hartmanis and Stearns, 1966], theories of modeling and 
    Decision Process (SMDPs). We introduce the no•     simulation [Zeigler, 1972], and it is frequently used by re•
    tion of SMDP homomorphism and argue that it pro•   searchers studying model checking approaches to system val•
    vides a useful tool for a rigorous study of abstrac• idation [Emerson and Sistla, 1996]. Although those study•
    tion for SMDPs. We present an SMDP minimiza•       ing approximation and abstraction methods for MDPs and 
    tion framework and an abstraction framework for    SMDPs have employed formalisms that implicitly embody 
    factored MDPs based on SMDP homomorphisms.         the idea of a homomorphism, they have not made explicit use 
    We also model different classes of abstractions that of the appropriate homomorphism concept. We provide what 
    arise in hierarchical systems. Although we use the we claim is the appropriate concept and give examples of how 
    options framework for purposes of illustration, the it can be widely useful as the basis of the study of abstraction 
    ideas are more generally applicable. We also show  in a dynamic setting. 
    that the conditions for abstraction we employ are a 
                                                         Informally, the kind of homomorphism we consider is a 
    generalization of earlier work by Dietterich as ap•
                                                       mapping from one dynamic system to another that eliminates 
    plied to the options framework. 
                                                       state distinctions while preserving the system's dynamics. We 
                                                       present a definition of homomorphisms that is appropriate for 
                                                       SMDPs. In earlier work [Ravindran and Barto, 2002] we de•
1 Introduction                                         veloped an MDP abstraction framework based on our notion 
The ability to form abstractions is one of the features that al• of an MDP homomorphism. This framework extended the 
lows humans to operate effectively in complex environments. MDP minimization framework proposed by [Dean and Gi-
We systematically ignore information that we do not need for van, 1997] and enabled the accommodation of redundancies 
performing the immediate task at hand. While driving, for arising from symmetric equivalence of the kind illustrated in 
example, we may ignore details regarding clothing and the Figure 1. While we can derive reduced models with a smaller 
state of our hair. Researchers in artificial intelligence (AI), in state set by applying minimization ideas, we do not neces•
particular machine learning (ML), have long recognized that sarily simplify the description of the problem in terms of the 
applying computational approaches to operating in complex number of parameters required to describe it. But MDPs of•
and real-world domains requires that we incorporate the abil• ten have additional structure associated with them that we 
ity to handle and form useful abstractions. In this article we can exploit to develop compact representations. By inject•
consider the question of what constitutes a useful abstraction. ing structure in the definition of SMDP homomorphism we 
Since this is a difficult problem when stated in general terms, can model abstraction schemes for structured MDPs. In this 
much of the work in this field is specialized to particular paper we present one simple example of such an abstraction 
classes of problems or specific modeling paradigms. In this scheme and mention other factored abstraction methods that 
work we will focus on Markov decision processes (MDPs), a can be modeled by suitable structured homomorphisms. 
formalism widely employed in modeling and solving stochas• In the second part of the paper we extend the notion of 
tic sequential decision problems and semi-Markov decision SMDP homomorphism to hierarchical systems. In particu•
processes (SMDPs), an extension to MDPs recently em•   lar, we apply homomorphisms in the options framework in•
ployed in modeling hierarchical systems [Sutton et ai, 1999; troduced by [Sutton et aly 1999] and show that this facili•
Dietterich, 2000; Parr and Russell, 1997].             tates employing different abstractions at different levels of 


PROBABILISTIC PLANNING                                                                                1011                                                                        gives the probability of executing action 
                                                       in state s. The value of a state-action pair under policy 
                                                         is the expected value of the sum of discounted future re•
                                                       wards starting from state s, taking action a, and following 
                                                       thereafter. When the SMDP has well defined terminal states, 
                                                       we often do not discount future rewards. In such cases an 
                                                       SMDP is equivalent to an MDP and we will ignore the tran•
Figure 1: (a) A symmetric gridworld problem. The goal state is sition times. The action-value function, corresponding 
G and there are four deterministic actions. This gridworld is sym•
metric about the NE-SW diagonal. For example, states A and B are to a policy is the mapping from state-action pairs to their 
equivalent since for each action in A, there is an equivalent action in values. The solution of an MDP is an optimal policy, that 
B. Taking action E, say, in state A is equivalent to taking action N uniformly dominates all other possible policies for that MDP. 
in state B, in the sense that they go to equivalent states that are each Let B be a partition of a set X. For any 
one step closer to the goal, (b) An equivalent reduced model of the denotes the block of B to which belongs. Any function 
gridworld in (a). The states A and B in the original problem corre• from a set X to a set Y induces a partition (or equivalence 
spond to the single state in the reduced problem. A solution relation) on X, with if and only if, 
to this reduced gridworld can be used to derive a solution to the full 
problem.                                               3 SMDP Homomorphisms 
                                                       A homomorphism from a dynamic system to a dynamic 
a hierarchy. We also argue that homomorphisms allow us system is a mapping that preserves 's dynamics, while 
to model aspects of symbolic AI techniques in represent• in general eliminating some of the details of the full system 
ing higher level task structure. Finally, we show that the One can think of as a simplified model of that 
SMDP homomorphism conditions generalize the "safe" state is nevertheless a valid model of with respect to the as•
abstraction conditions for hierarchical systems introduced by pect's of state that it preserves. The specific definition 
[Dietterich, 2000].                                    of homomorphism that we claim is most useful for MDPs and 
  After introducing some notation (Section 2), we define SMDPs is as follows: 
SMDP homomorphisms and discuss modeling symmetries 
(Section 3). Then we present a brief overview of our model 
minimization framework (Section 4), discuss abstraction in 
factored MDPs (Section 5) and abstraction in hierarchical 
systems (Section 6). We conclude with some discussion of 
directions for future research (Section 7). 


                                                       We call the homomorphic image of M under and we 
                                                       use the shorthand to denote The surjection 
                                                         maps states of to states of and since it is gener•
                                                       ally many-to-one, it generally induces nontrivial equivalence 
                                                       classes of states s of M: . Each surjection recodes the 
                                                       actions admissible in state s of to actions admissible in 
                                                       state f(s) of This state-dependent recoding of actions is 
                                                       a key innovation of our definition, which we discuss in more 
  A discrete time semi-Markov decision process (SMDP) is a 
                                                       detail below. Condition (1) says that the transition probabili•
generalization of an MDP in which actions can take variable 
                                                       ties in the simpler SMDP are expressible as sums of the 
amounts of time to complete. As with an MDP, an SMDP 
                                                       transition probabilities of the states of that / maps to that 
is a tuple where S, A and are the sets of 
                                                       same state in This is the stochastic version of the stan•
states, actions and admissible state-action pairs; 
                                                       dard condition for homomorphisms of deterministic systems 
               is the transition probability function with 
                                                       that requires that the homomorphism commutes with the sys•
            being the probability of transition from state 
s to state s' under action in N time steps, and        tem dynamics [Hartmanis and Stearns, 1966]. Condition (2) 
         is the expected discounted reward function, with says that state-action pairs that have the same image under 
          being the expected reward for performing action h have the same expected reward. An MDP homomorphism 
                                                       is similar to an SMDP homomorphism except that the condi•
a in state s and completing it in N time steps.1 
  A stochastic policy, is a mapping from to the real in• tions (1) and (2) apply only to the states and actions and not 
terval [0,1] with For any                              to the transition times. 
                                                         The state-dependent action mapping allows us to model 
   1 We are adopting the formalism of [Dietterich, 2000]. symmetric equivalence in MDPs and SMDPs. For example, 


1012                                                                             PROBABILISTIC PLANNING                                                        state set is necessarily simpler and hence might not lead to 
                                                       a simpler problem representation. Many classes of problems 
                                                       that are modeled as MDPs have some inherent structure. We 
                                                       define structured forms of homomorphisms that can exploit 
                                                       this structure in deriving simpler problem representations. 
                                                         Factored MDPs are a popular way to model structure in 


  The set of all automorphisms of an SMDP M, denoted 
by AutM, forms a group under composition of homomor-
phisms. This group is the symmetry group of M. In the 
gridworld example of Figure 1, the symmetry group consists 
of the identity map on states and actions, a reflection of the 
states about the NE-SW diagonal and a swapping of actions 
N and E and of actions S and W. Any subgroup of the sym•
metry group of an SMDP induces an equivalence relation on 
$, which can also be induced by a suitably defined homo-
morphism [Ravindran and Barto, 2001]. Therefore we can 
model symmetric equivalence as a special case of homomor-
phic equivalence.                                      where Pre denotes the parents of node s[ in the 2-TBN 
                                                       corresponding to action a and each of the Prob 
4 Minimization                                         is given by a conditional probability table (CPT) associated 
                                                       with node The reward function may be similarly repre•
The notion of homomorphic equivalence immediately gives 
                                                       sented. 
us an SMDP minimization framework. In [Ravindran and 
Barto, 2001] we extended the minimization framework of   Structuring the state space representation allows us to con•
Dean and Givan LDean and Givan, 1997; Givan et a/., 2003] sider morphisms that are structured, i. e. surjections from one 
to include state-dependent action recoding and showed that structured set to another. An example of a structured mor-
if two state-action pairs have the same image under a homo- phism is a simple projection onto a subset of features. Here 
morphism, then they have the same optimal value. We also the state set of the homomorphic image is described by a sub•
showed that when M' is a homomorphic image of an MDP   set of the features, while the rest are ignored. We introduce 
                                                       some notation, after [Zeigler, 1972], to make the following 
My a policy in Ml can induce a policy in M that is closely 
related. Specifically a policy that is optimal in M' can induce 
an optimal policy in M. Thus we can solve the original MDP 
by solving a homomorphic image. It is easy to extend these 
results to SMDP models. 
  Thus the goal of minimization is to derive a minimal image 
of the SMDP, i.e., a homomorphic image with the least num•
ber of admissible state-action pairs. We also adapted an exist•
ing minimization algorithm to find minimal images employ•
ing state-action equivalence. Employing state-dependent ac•
tion recoding allows us to achieve greater reduction in model 
size than possible with Dean and Givan's framework. For ex•
ample, the gridworld in Figure 1(a) is minimal if we do not 
consider state-dependent action mappings. 
5 Abstraction in Structured MDPs 
SMDP homomorphisms can also be used to model various 
SMDP abstraction frameworks. The definition of homomor-
phism in Section 3 assumed a monolithic representation of The first condition implies that the subset F should be cho•
the state set. While we can derive an equivalent MDP model sen such that the features chosen are sufficient to describe the 
with a smaller it does not follow that the description of the block transition dynamics of the system. In other words, the 


PROBABILISTIC PLANNING                                                                                1013 subgraph of the 2-TBN described by the projection should in• where is the initiation set of the option, 
clude all the incoming arcs incident on the chosen nodes. The is the termination function and M o is the option SMDP. 
second condition requires that all the parents and the incom• The state set of Mo is a subset of 5 and constitutes the do•
ing arcs to the reward node are also included in the subgraph. main of the option. The action set of Mo is a subset of A 
To find such homomorphic projections, we just need to work and may contain other options as well as "primitive" actions 
back from the reward node including arcs and nodes, until in A. The reward function of Mo is chosen to reflect the 
we reach the desired subgraph. Such an algorithm will run in sub-goal of the option. The transition probabilities of Mo 
time polynomial in the number of features. It is evident that are induced by P and the policies of lower level options. We 
the space of such simple projections is much smaller than assume that the lower-level options are following fixed poli•
that of general maps and in general may not contain a ho- cies which are optimal in the corresponding option SMDPs. 
momorphism reducing a given MDP. Without suitable con• The option policy is obtained by solving Mo> treating it as 
straints, often derived from prior knowledge of the structure an episodic task with the possible initial states of the episodes 
of the problem, searching for general structured homomor- given by and the termination of each episode determined by 
phisms results in a combinatorial explosion. Abstraction al• the option's termination function 
gorithms developed by Boutilier and colleagues can be mod• For example, in the gridworld task shown in Figure 2(a), 
eled as converging to constrained forms of structured mor- an option to pick up the object and exit room 1 can be defined 
phisms assuming various representations of the CPTs—when as the solution to the problem shown in 2(b), with a suitably 
the space of morphisms is defined by boolean formulae of defined reward function. The domain and the initiation set of 
the features [Boutilier and Dearden, 1994], when it is defined the option are all states in the room and the option terminates 
by decision trees on the features [Boutilier et al, 1995] and on exiting the room with or without the object. 
when it is defined by first-order logic formulae [Boutilier et 
a/., 2001].                                            6.2 Option Specific Abstraction 
                                                       The homomorphism conditions (1) and (2) are very strict and 
6 Abstraction in Hierarchical Systems                  frequently we end up with trivial homomorphic images when 
In the previous section we showed that SMDP homomor-   deriving abstractions based on a non-hierarchical SMDP. But 
phisms can model various abstraction schemes in "flat" it is often possible to derive non-trivial reductions if we re•
MDPs and SMDPs. SMDP homomorphisms are a convenient    strict attention to certain sub-problems, i.e., certain sub-goal 
and powerful formalism for modeling abstraction schemes in options. In such cases we can apply the ideas discussed in 
hierarchical systems as well. Before we explore various ab• Sections 4 and 5 to an option SMDP directly to derive abstrac•
straction approaches we first introduce a hierarchical archi• tions that are specific to that option. The problem of learning 
tecture that supports abstraction.                     the option policy is transformed to the usually simpler prob•
                                                       lem of learning an optimal policy for the homomorphic im•
6.1 Hierarchical Markov Options                        age. 
Recently several hierarchical reinforcement learning frame•
                                                       6.3 Relativized Options 
works have been proposed [Parr and Russell, 1997; Sutton et 
al, 1999; Dietterich, 2000] all of which use the SMDP for• Consider the problem of navigating in the gridworld environ•
malism. In this article the hierarchical framework we adopt is ment shown in Figure 2(a). The goal is to reach the central 
the options framework [Sutton et ai, 1999], though the ideas corridor after collecting all the objects in the environment. 
developed here are more generally applicable. Options are No non-trivial homomorphic image exists of the entire prob•
actions that take multiple time steps to complete. They are lem. But there are many similar components in the problem, 
usually described by: the policy to follow while the option is namely, the five sub-tasks of getting the object and exiting 
executing, the set of states in which the option can begin exe• roomt. We can model these similar components by a "par•
cution, and the termination function fi which gives the prob• tial" homomorphic image—where the homomorphism con•
ability with which the option can terminate in a given state. ditions are applicable only to states in the rooms. One such 
The resulting systems are naturally modeled as SMDPs with partial image is shown in Figure 2(b). Employing such an ab•
the transition time distributions induced by the option poli• straction lets us compactly represent a related family of op•
cies. We present an extension to the options framework that tions, in this case the tasks of collecting objects and exiting 
readily facilitates modeling abstraction at multiple levels of each of the five rooms, using a single option MDP. We refer 
the hierarchy using SMDP homomorphisms.                to this compact option as a relativized option. Such abstrac•
  We consider the class of options known as Markov options, tions are an extension of the notion of relativized operators 
whose policies satisfy the Markov property and that terminate introduced by [Iba, 1989]. Formally we define a relativized 
on achieving a certain sub-goal. In such instances it is possi• option as follows: 
ble to implicitly define the option policy as the solution to an 
option MDP, or an option SMDP if the option has access to 
other options. Accordingly we define a hierarchical Markov 
sub-goal option: 
Definition: A hierarchical Markov sub-goal option of an 
SMDP 


1014                                                                             PROBABILISTIC PLANNING Figure 2: (a) A simple rooms domain with similar rooms and usual 
stochastic gridworld dynamics. The task is to collect all 5 objects 
(black diamonds) in the environment and reach the central corridor. Figure 3: Comparison of performance of learning agents employ•
The shaded squares are obstacles, (b) The option MDP correspond• ing regular and relativized options on the task shown in Figure 2. 
ing to a get-object-and-leave-room option. 

                                                      object-shape and similarly for the move-object and place-
Here the state set of where So is the                 object options. A relativized option with this partial image 
domain of the option, and the admissible state-action set is and partial homomorphism as the option MDP and option 
//(ty). Going back to our example in Figure 2(a), we can now homomorphism, respectively, captures the desired conceptual 
define a single get-object-and-leave-room relativized option structure. Executing the optimal policy for this option results 
using the option MDP of Figure 2(b). The policy learned in in action sequences of the form: grasp, move, place, — De•
this option MDP can then be suitably lifted to M to provide pending on the object-shape these abstract actions get bound 
different policy fragments in the different rooms. Figure 3 to different lower level options. While techniques in sym•
demonstrates the speed-up in performance when using a sin• bolic Al have long been able to model such higher level task 
gle relativized option as opposed to five regular options. In structure, the reinforcement learning community lacked an 
this experiment the option policies and the higher level pol• obvious mechanism to do the same. Our work gives us new 
icy were learned simultaneously.2                     tools to model conceptual task structure at various temporal 
                                                      and spatial scales. 
6.4 Modeling Higher Level Structure 
SMDP homomorphisms have the power to model a broader  6.5 Relation to MaxQ Safe State-abstraction 
class of abstractions, those not supported by the base level 
                                                      [Dictterich, 2000] introduced safe state-abstraction condi•
dynamics of the system but which can be induced at some 
                                                      tions for the MaxQ architecture, a hierarchical learning 
intermediate levels of the hierarchy. For example, consider 
                                                      framework related to the options framework. These condi•
a multi-link assembly line robot arm that needs to move ob•
                                                      tions ensure that the resulting abstractions do not result in any 
jects from one assembly line to another. The state of the sys•
                                                      loss of performance. He assumes that the sub-problems at dif•
tem is described by the joint angles and velocities, object- ferent levels of the hierarchy are specified by factored MDPs. 
shape, object-orientation, and Boolean variables indicating The MaxQ architecture employs a form of value function de•
whether the object is firmly-grasped and whether the object composition and some of the safe abstraction conditions ap•
is at-target-location. The primitive actions are various joint ply only to this form of decomposition. The following condi•
torques. Depending on the configuration of the arm and the tion is more universal and applies to the hierarchical Markov 
shape and orientation of the object, this task requires different options framework as well: 
sequences of actions, or policies, even though conceptually 
the higher level task has the same "structure"—grasp object, 
move object, and place object. 
  We can model this higher level task structure using a rel•
ativized option. First, we define suitable grasp-object op•
tions, such as grasp-cylinder, grasp-block etc. , and simi• Condition (i) states that the transition probability can be 
lar options for moving and placing objects. Then we form expressed as a product of two probabilities, one of which de•
a partial homomorphic image of the resulting SMDP—the scribes the evolution of a subset of the features and depends 
state set of the image is described by the two Boolean fea• only on that subset. Condition (ii) states that if two states 
tures and the action set consists of grasp, move and place project to the same abstract state, then they have the same 
actions. The partial homomorphism, which applies only to immediate reward. From equations 1-4, it is evident that the 
the admissible state-option pairs of the SMDP, consists of above conditions are equivalent to the SMDP homomorphism 
                                                      conditions if we restrict our attention to simple projection ho•
                                                      momorphisms and do not consider action remapping. Thus 
                                                      the SMDP homomorphism conditions introduced here gener•
  2In [Ravindran and Barto, 2002] we have reported more detailed alize Dietterich's safe state-abstraction condition as applica•
experiments in this setting.                          ble to the hierarchical Markov option framework. 


PROBABILISTIC PLANNING                                                                               1015 