         Improving Embeddings by Flexible Exploitation of Side Information

             Ali Ghodsi                     Dana Wilkinson                  Finnegan Southey
       University of Waterloo            University of Waterloo                 Google Inc.


                    Abstract                          While it would be expensive to have a human examine and
                                                      label the entire set, it might be feasible to have a human ex-
    Dimensionality reduction is a much-studied task in amine a small subset and provide information on how pairs of
    machine learning in which high-dimensional data   images relate to each other. Similarly, some expensive exper-
    is mapped, possibly via a non-linear transforma-  iment might yield useful similarity information for a subset
    tion, onto a low-dimensional manifold. The result- of the data. This paper will show how two kinds of such side
    ing embeddings, however, may fail to capture fea- information can be used in a preprocessing step for embed-
    tures of interest. One solution is to learn a distance ding techniques, leading to embeddings that capture the tar-
    metric which prefers embeddings that capture the  get properties. The method improves over earlier work in this
    salient features. We propose a novel approach to  area by using standard ”off-the-shelf” optimization methods
    learning a metric from side information to guide the and by allowing more ﬂexibility in the side information used.
    embedding process.                                  The ﬁrst kind of side information identiﬁes pairs of points
    Our approach admits the use of two kinds of side  that belong to the same class or pairs that belong to differ-
    information. The ﬁrst kind is class-equivalence in- ent classes. Note that this information is about the class-
    formation, where some limited number of pairwise  equivalence/inequivalence of points but does not give the ac-
    “same/different class” statements are known. The  tual class labels. Consider four points, x1,x2,x3, and x4.
    second form of side information is a limited set of Given side information that x1 and x2 are in the same class,
    distances between pairs of points in the target met- and that x3 and x4 also share a class, we cannot be certain
    ric space. We demonstrate the effectiveness of the whether the four points fall into one or two classes.
    method by producing embeddings that capture fea-    The second kind of side information takes the form of par-
    tures of interest.                                tial information about the similarity of points in the form of
                                                      distances. For some pairs of points, one may have distances
                                                      corresponding to an informative metric space, obtained by
1  Introduction                                       some expensive measurement. Molecular conformation prob-
Many machine learning approaches use distances between lems are a good example. Some of the distances between
data points as a discriminating factor. In some cases, the pairs of atoms in a given molecule can be determined by nu-
plain Euclidean distance between points is meaningful and clear magnetic resonance spectroscopy, but the procedure is
such methods can work well. Such distances are the starting costly and certainly not guaranteed to provide all such dis-
point of many popular dimensionality-reduction techniques, tances. Determining the rest of the distances can help in clas-
such as MDS [Cox and Cox, 2001], LLE [Roweis and Saul, sifying the conformation of the molecule (see [Crippen and
2000],Isomap[Tenenbaum  et al., 2000], LEM [Belkin and Havel, 1988] for more details).
Niyogi, 2003],andSDE[Weinberger and Saul, 2004].Often,  These two kinds of side information can be used to learn
however, this distance does not capture the distinction one is a new distance metric. The distances between points in this
trying to characterize. Approaches such as kernelized meth- new space can then be used with any embedding technique.
ods address this issue by mapping the points into new spaces We start by showing how class-equivalence side information
where Euclidean distances may be more useful. An alterna- can be used to learn such a metric and show the effect of us-
tive approach is to construct a new distance metric over the ing this new metric with a variety of embedding techniques.
points and use it in place of Euclidean distances, as explored We then show how multiple applications of this method can
by Xing et al. [Xing et al., 2003]. In this approach, some be used to combine several learned distance metrics together
amount of side information is employed to learn a distance into one in order to capture multiple attributes in the embed-
metric that captures the desired distinction.         ding. We also discuss how the class-equivalence approach
  In some situations it may be possible to obtain a small can be kernelized, allowing for nonlinear transformations of
amount of information regarding the similarity of some points the metric. Finally, we formulate a method for using the sec-
in a particular data set. Consider a large collection of images. ond type of side information, where we have partial infor-

                                                IJCAI-07
                                                   810mation about desirable target distances between some pairs The optimization problem then becomes
of points. Experimental results demonstrate the value of this
                                                              min L(A) s.t. A  0 and Tr(A)=1         (1)
preprocessing step.                                            A
1.1  Related Work                                       The ﬁrst constraint (positive semideﬁniteness) ensures a
                                                      Euclidean metric. The second excludes the trivial solution
Our use of class equivalence relations to learn a metric fol- where all distances are zero. The constant in this constraint is
    [               ]
lows Xing et al., 2003 . In that work, a new distance met- arbitrary, affecting only the scale of the space. This objective
ric is learned by considering side information. Xing et al. will be optimized using standard semideﬁnite programming
used side information identifying pairs of points as “similar”. software and so it must be converted to a linear objective.
They then construct a metric that minimizes the distance be- Expanding the loss function
tween all such pairs of points. At the same time, they at-              
                                                                                      T
tempt to ensure that all “dissimilar” points are separated by L(A)=           (xi − xj) A(xi − xj) −
some minimal distance. By default, they consider all points
                                                                      (xi,xj )∈S
not explicitly identiﬁed as similar to be dissimilar. They              
                                                                                      T
present algorithms for optimizing this objective and show re-                 (xi − xj ) A(xi − xj )
sults using the learned distance for clustering, an application       (xi,xj )∈O
in which an appropriate distance metric is crucial. Other work
on learning distance metrics has been primarily focused on each squared distance term must be converted. We start
                                                      by observing that vec(XYZ)=(ZT   ⊗  X)vec(Y ),where
classiﬁcation, with side information in the form of class la- ()
bels [Globerson and Roweis, 2006; Weinberger et al., 2006; vec simply rearranges a matrix into a vector by concate-
                                                      nating columns and ⊗ is the Kronecker product. Note that
Goldberger et al., 2005].                                      T                         T
                                                      (xi − xj) A(xi − xj)=vec((xi  − xj ) A(xi − xj )) be-
                                                      cause the left-hand side is a scalar. Using this and the fact
2  Learning a Metric from Class-Equivalence           that (aT ⊗ bT )=vec(baT )T , we can rewrite the squared
   Side Information                                   distance terms as
                                                                            T
We will start with the simpler similar/dissimilar pair case, for-   (xi − xj ) A(xi − xj )
malizing this notion of side information and stating an objec-                 T
tive that will be optimized using standard semideﬁnite pro-     =   vec((xi − xj ) A(xi − xj))
                                                                             T           T
gramming software. The use of this kind of side information     =((xi   − xj ) ⊗ (xi − xj) )vec(A)
allows one to select a characteristic for distinction. For ex-
                                                                =      ((x − x )(x − x )T )T  (A)
ample, one may have several images of faces. One sensible           vec  i    j  i    j    vec
                                                                          T                    T
cluster is by presence or absence of a beard. Another is by the =   vec(A) vec((xi − xj )(xi − xj) )
presence or absence of glasses. Different indications of simi-
                                                        The linear loss function is then
larity allow the capturing of either distinction. The following    
                                                                               T                    T
takes the same basic approach as [Xing et al., 2003] but of- L(A)=       vec(A) vec((xi − xj )(xi − xj) ) −

fers a simpler optimization procedure using “off-the-shelf”      (xi,xj )∈S
optimization methods instead of their iterative method.            
                                                                               T                    T
                                                                         vec(A) vec((xi − xj)(xi − xj ) )

2.1  Derivation                                                  (xi,xj )∈O
                       t      n                                          ⎡
Given a set of t points, {xi} ⊆ R , we identify two kinds
                       i=1                                                  
of class-related side information. First, a set class-equivalent       T ⎣                           T
                                                             =   vec(A)           vec((xi − xj)(xi − xj ) )
(or similar) pairs of points
                                                                          (x ,x )∈S
                                                                            i j                  ⎤
       S :(xi,xj ) ∈S   if xi and xj are similar
                                                                     
                                                                                               T ⎦
and, second, a set of class-inequivalent (dissimilar)pairs       −         vec((xi − xj)(xi − xj) )
                                                                   (x ,x )∈O
     O  :(xi,xj) ∈O    if xi and xj are dissimilar                   i j
  We then wish to learn a matrix A that induces a distance This form, along with the two constraints from (1), can
metric D(A) over the points                           be readily submitted to an SDP solver to optimize the matrix
                                                     A1. Aside from this convenient form, this formulation has
   (A)                                T
 D    (xi,xj)=xi − xj A =   (xi − xj) A(xi − xj)    other advantages over that used by Xing et al., especially with
                                                      respect to the side information we can convey.
where A  0.                                            Xing et al. require at least one dissimilar pair in order to
  We deﬁne the following loss function, which, when mini- avoid the trivial solution where all distances are zero. The
mized, attempts to minimize the squared induced distance be- constraint on the trace that we employ means that we need
tween similar points and maximize the squared induced dis- not place any restrictions on pairings. Side information can
tance between dissimilar points                       consist of similar pairs only, dissimilar pairs only, or any com-
                               
                          2                   2       bination of the two; the method still avoids trivial solutions.
  L(A)=          xi − xjA −         xi − xjA
                                                         1
          (xi,xj )∈S          (xi,xj )∈O                 We use the MATLAB SDP solver SeDuMi [Sturm, 1999]

                                                IJCAI-07
                                                   811Furthermore, in the absence of speciﬁc information regarding cover them. Moreover, two different characteristics (beards
dissimilarities, Xing et al. assume that all points not explicitly and glasses) have been captured within the same data set,
identiﬁed as similar are dissimilar. This information may be even when using only small quantities of class-equivalence
misleading, forcing the algorithm to separate points that may, side information. Finally, the preprocessor is effective with a
in fact, be similar. The formulation presented here allows one wide range of embedding techniques.
to specify only the side information one actually has, parti-
tioning the pairings into similar, dissimilar, and unknown.         MDS               Equivalence−informed MDS
2.2  Results
Once a metric has been learned, the new distances can be used
with any embedding technique. To demonstrate the beneﬁts
of this approach, we generated embeddings with and without
the preprocessing step, informing the preprocessed embed-   2nd  dimension        2nd  dimension
dings about some characteristic of interest, and then exam-
ined the result. To show that the side information truly in-
forms the embedding, two sets of informed embeddings were

generated from the same data set, each with side information      1st dimension         1st dimension
pertaining to two different characteristics. Structure in the Figure 1: MDS and Equivalence-Informed MDS with
resulting embeddings that captures the two different charac- (un)bearded distinction (all equivalent pairs)
teristics within a single data set is evidence that the method
works as intended.
  Embeddings were generated using a variety of techniques           MDS               Equivalence−informed MDS
including MDS, Isomap, LEM, LLE, and SDE. The data
set consisted of 200 images of faces and two distinctions
are identiﬁed by side information: faces with beards vs.
faces without beards and faces with glasses vs. faces with-
out glasses. In one set of experiments (which we will call

all-similar), all similar pairs were identiﬁed but no dissim- 2nd  dimension      2nd  dimension
ilar pairs. The second set (which we will call ﬁve-pairs),
simulating a situation where labelling is expensive, identi-
ﬁes only four similar pairs and one dissimilar pair. The pairs
were selected at random. Techniques using the preprocessor
                                                                  1st dimension         1st dimension
are labelled as “Equivalence-Informed” (e.g., Equivalence-
Informed MDS). In each case, a two-dimensional embedding Figure 2: MDS and Equivalence-Informed MDS  with
is shown. The two classes are marked with X and O, respec- glasses/no glasses distinction (4 equivalent/1 inequiv. pairs)
tively. Additionally, a subset of the images are displayed on

the plot (plots with all images are unreadable). Some plots         Isomap           Equivalence−informed Isomap 
have been omitted due to space constraints.
  Inspection reveals that, in general, the informed versions
of these embeddings manage to separate the data based on
the target property, whereas the uninformed versions are typ-
ically chaotic. Even when separation is poor in the informed
embedding, there is usually much more structure than in the
                                                            2nd  dimension        2nd  dimension
uninformed embedding. Equivalence-Informed MDS (Fig-
ures 1 and 2, ﬁve-pairs beards and all-pairs glasses omitted)
offers mixed results, especially with ﬁve-pairs for glasses vs.
no glasses (Figure 2), but MDS is a comparatively crude em-
bedding technique in any case. Isomap with all-pairs works         1st dimension         1st dimension
very well (Figure 3, glasses omitted) and with ﬁve-pairs it still Figure 3: Isomap and Equivalence-Informed Isomap with
manages to group the two classes but does not separate them (un)bearded distinction (all equivalent pairs)
well (Figure 4, glasses omitted). LEM separates well with
all-pairs (Figure 5, glasses omitted) but shows weak separa- Finally, Figures 9 and 10 are provided to give a numerical
tion in the ﬁve-pairs case (Figure 6, glasses omitted). The notion of the improvement offered by informed embeddings.
same effective grouping and varying separation occurs for The plots show the misclassiﬁcation rate of k-means cluster-
LLE (ﬁgures omitted). Finally, SDE groups well, but is weak ing in recovering the true classes of the images, after applying
in separation in all cases (Figures 7 and 8, glasses omitted). each of the uninformed and informed embeddings (indicated
  These results show that the side information can be effec- by, e.g., ”MDS” and ”Inf. MDS”). The informed methods
tively exploited to capture characteristics of interest where achieve a lower error rate than their uninformed counterparts
uninformed embedding techniques fail to automatically dis- by better separating the data (except 4-pairs SDE).

                                                IJCAI-07
                                                   812             Isomap            Equivalence−informed Isomap          SDE               Equivalence−informed SDE
      2nd  dimension        2nd  dimension                  2nd  dimension        2nd  dimension


            1st dimension         1st dimension                   1st dimension         1st dimension
Figure 4: Isomap and Equivalence-Informed Isomap with Figure 7:   SDE  and  Equivalence-Informed SDE with
(un)bearded distinction (4 equivalent/1 inequivalent pairs) (un)bearded distinction (all equivalent pairs)

              LEM               Equivalence−informed LEM            SDE               Equivalence−informed SDE
      2nd  dimension        2nd  dimension                  2nd  dimension        2nd  dimension


            1st dimension         1st dimension                   1st dimension         1st dimension
Figure 5:  LEM  and  Equivalence-Informed LEM  with   Figure 8:   SDE  and  Equivalence-Informed SDE with
(un)bearded distinction (all equivalent pairs)        (un)bearded distinction (4 equivalent/1 inequivalent pairs)

3  Multiple-Attribute Metric Learning with            ﬁnes a rank k linear subspace onto which we can project
                                                                                                   ¯
   Class-Equivalence Side Information                 the data. Applying singular value decomposition to A, one
                                                      can form a matrix U from the k eigenvectors corresponding
In some cases, there may be more than one distinction to cap- to the largest eigenvalues. The ﬁnal transformation is then
ture in data (e.g., glasses vs. no glasses and beards vs. no Aˆ = UUT , which is an orthonormal basis combining the dis-
beards). The method above can be extended to construct a tinctions drawn by each kind of side information.
distance metric using multiple sets of side information, each
corresponding to a different criterion. Multiple metrics are 3.1 Results
learned and then combined to form a single metric.
                                                      We demonstrate the effectiveness of this method by creating
  Suppose there are k different sets of side information over
                                                      a single embedding that captures two distinctions. We re-
the same set of points. Using the optimization described
                                                      peat the earlier experiments but using both the (un)bearded
above, k transformations, A1, ···,Ak can be learned. From
                                                      and glasses/no glasses criteria together. Results were gen-
each Ai, the dominant eigenvector vi can be taken and a new
                                                      erated for all the embedding methods and their multiple-
matrix assembled where each column is one of these eigen-
        ¯                                             attribute, equivalence-informed versions but some are omit-
vectors, A =[v1   ··· vk  ]. This combined matrix de-
                                                      ted for space. In all cases, the side information consisted of
                                                      all similar pairs and no dissimilar pairs.
              LEM               Equivalence−informed LEM The informed embeddings tend to discover the four distinct
                                                      categories of faces (combinations of the presence and absence
                                                      of beards and glasses). Informed MDS (Figure 11) separates


                                                        0.5                      0.5
                                                        0.4                      0.4

      2nd  dimension        2nd  dimension              0.3                      0.3
                                                        0.2                      0.2
                                                        Error  Rate              Error  Rate
                                                        0.1                      0.1
                                                         0                        0

                                                          MDS     LLE LEM SDE       MDS     LLE LEM SDE
                                                             Isomap                    Isomap
                                                           Inf. MDS Inf. LLE Inf. LEM Inf. SDE Inf. MDS Inf. LLE Inf. LEM Inf. SDE
            1st dimension         1st dimension               Inf. Isomap              Inf. Isomap
Figure 6:  LEM  and  Equivalence-Informed LEM  with   Figure 9: Clustering Error for beards vs. no beards (Left: all
(un)bearded distinction (4 equivalent/1 inequivalent pairs) equivalent pairs Right: 4 equiv/1 inequiv pairs)

                                                IJCAI-07
                                                   813  0.5                      0.5                                      LLE               Multi−attribute−informed−LLE
  0.4                      0.4
  0.3                      0.3
  0.2                      0.2
 Error  Rate               Error  Rate
  0.1                      0.1
   0                        0

    MDS     LLE LEM SDE      MDS     LLE LEM SDE
       Isomap                   Isomap
     Inf. MDS Inf. LLE Inf. LEM Inf. SDE Inf. MDS Inf. LLE Inf. LEM Inf. SDE
        Inf. Isomap              Inf. Isomap
                                                            2nd  dimension        2nd  dimension
Figure 10: Clustering Error for glasses vs. no glasses (Left:
all equivalent pairs Right: 4 equiv/1 inequiv pairs)

these quite well, as do Informed Isomap (Figure 12) and In-        1st dimension         1st dimension
formed LLE (Figure 13). Results with Informed LEM (ﬁgure Figure 13: LLE and Multi-Attribute Informed LLE with
omitted) are roughly grouped. Finally, Informed SDE (Figure (un)bearded and (no)glasses attributes (all equivalent pairs)

14) does has trouble separating two categories, but a lot of        SDE               Multi−attribute−informed−SDE
useful structure is present. In all cases, the informed embed-
dings have distinct structure that the uninformed embeddings
fail to capture. The only feature the uninformed embeddings
seem to capture is the relative darkness of the images.


              MDS              Multi−attribute−informed−MDS
                                                            2nd  dimension        2nd  dimension


                                                                   1st dimension         1st dimension


      2nd  dimension        2nd  dimension            Figure 14: SDE and Multi-Attribute Informed SDE with
                                                      (un)bearded and (no)glasses attributes (all equivalent pairs)

                                                      similarity measure between any two data points. In this sec-
            1st dimension         1st dimension       tion, we show how to learn a metric in the feature space im-
Figure 11: MDS and Multi-Attribute Informed MDS with  plied by a kernel, allowing our use of side information to be
(un)bearded and (no)glasses attributes (all equivalent pairs) extended to nonlinear mappings of the data.
                                                        Conceptually, the points are mapped into a feature space
  These results show that the multiple-attribute approach can by some nonlinear mapping Φ() and a metric is learned in
capture the desired properties in a single embedding, while that space. Actually applying the mapping is often undesir-
the uninformed versions fail to capture either of the properties able (e.g., the feature vectors may have inﬁnite dimension),
of interest. Again, the method is straightforward to apply and so we employ the well known kernel trick, using some ker-
works with a variety of embedding techniques.         nel K(xi,xj ) that computes inner products between feature
                                                      vectors without explicitly constructing them.
                                                        The squared distances in our objective have the form (xi −
4  Kernelizing Metric Learning                           T
                                                      xj) A(xi − xj ). Because A is positive semideﬁnite, it can
Not uncommonly, nonlinear transformations of the data are
                                                      be decomposed into A = WWT  . W can then be expressed
required to successfully apply learning algorithms. One ef-                               W  = Xβ
ﬁcient method for doing this is via a kernel that computes a as a linear combination of the data points, , via the
                                                      kernel trick. Rewriting the squared distance

             Isomap            Multi−attribute−informed−Isomap            T
                                                                  (xi − xj) A(xi − xj)
                                                                          T     T
                                                               =(xi   − xj) WW    (xi − xj)
                                                                          T     T  T
                                                               =(xi   − xj) Xββ   X  (xi − xj )
                                                                    T      T      T   T      T
                                                               =(xi   X − xj X)ββ  (X  xi − X  xj)
                                                                     T       T   T     T      T
      2nd  dimension        2nd  dimension                     =(X    xi − X  xj) A(X   xi − X xj )

                                                      where A  =  ββT , we have now expressed the distance in
                                                      terms of the matrix to be learned, A, and inner products be-
                                                      tween data points, which can be computed via the kernel, K.
            1st dimension         1st dimension       The optimization of A proceeds as in the non-kernelized ver-
Figure 12: Isomap and Multi-Attribute Informed Isomap with sion but with one additional constraint. The rank of A must
(un)bearded and (no)glasses attributes (all equivalent pairs) be t because β is t by n and A =ββT . However, this con-

                                                IJCAI-07
                                                   814