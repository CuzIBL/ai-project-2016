              Learning and Multiagent Reasoning for Autonomous Agents

                           IJCAI-07 Computers and Thought Paper
                                              Peter Stone
                                   Department of Computer Sciences
                                   The University of Texas at Austin
                                         pstone@cs.utexas.edu

                    Abstract                          the other agents will have opposing or uncorrelated goals. In
                                                      both cases, when dealing with other agents, it is beneﬁcial
    One goal of Artiﬁcial Intelligence is to enable the from the points of view of robustness and ﬂexibility for an
    creation of robust, fully autonomous agents that can agent to be able to learn or adapt how it interacts with them.
    coexist with us in the real world. Such agents will As such, learning, interaction, and their combination are
    need to be able to learn, both in order to correct and key necessary capabilities on our path towards creating robust
    circumvent their inevitable imperfections, and to autonomous agents. To enable these capabilities, we need to
    keep up with a dynamically changing world. They   combine 1) basic, algorithmic research on machine learning
    will also need to be able to interact with one an- and multiagent systems with 2) application-oriented research
    other, whether they share common goals, they pur- threads aimed at studying complete agents in speciﬁc, com-
    sue independent goals, or their goals are in direct plex environments, with the ultimate goal of drawing general
    conﬂict. This paper presents current research di- lessons from the speciﬁc implementations.
    rections in machine learning, multiagent reasoning, This paper illustrates this research methodology, drawing
    and robotics, and advocates their uniﬁcation within heavily on concrete examples from my own research, and
    concrete application domains. Ideally, new theoret- suggests research directions for the future aimed at creating
    ical results in each separate area will inform prac- fully autonomous agents, including robots, capable of learn-
    tical implementations while innovations from con- ing and interacting. It represents my own perception of the
    crete multiagent applications will drive new theo- important and interesting research topics in the ﬁeld as of
    retical pursuits, and together these synergistic re- 2007. It is not intended to be comprehensive – there are cer-
    search approaches will lead us towards the goal of tainly other AI areas that are active, interesting, and likely to
    fully autonomous agents.                          be fruitful. My ambition for the paper is that it will inspire
                                                      some of my current and future colleagues to take an interest
1  Introduction                                       in the problems presented here, and perhaps that it will inspire
Much of the past research in Artiﬁcial Intelligence has others to formulate opposing arguments that entirely different
focused either on high-level reasoning from abstract, un- topics are more worthy of pursuit than those advocated here.
grounded representations or on interpreting raw sensor data The remainder of this paper is organized as follows. First,
towards building grounded representations. However, neither in Sections 2 and 3, learning and multiagent reasoning are
of these foci taken alone is sufﬁcient for deploying practical, considered separately. Next, in Section 4, related issues on
real-world AI systems. In recent years, an active contingent the path towards robust physical agents (robots) that can learn
within the ﬁeld has focused on creating complete autonomous and interact are addressed.
agents: those that sense their environment, engage in high- Although Sections 2–4 treat agent components, the essence
level cognitive decision-making, and then execute their ac- of creating complete autonomous agents is putting all the
tions in the environment.                             pieces together. Indeed, Daphne Koller organized her 2001
  As the ﬁeld progresses in this direction, individual au- Computers and Thought lecture around the notion of three
tonomous agents, either in software or physically embodied, conceptual bridges connecting representation, reasoning, and
are becoming more and more capable and prevalent. Multi- learning as a way of mitigating her observation that:
agent systems consisting of homogeneous or similar agents In AI, as in many communities, we have the ten-
are also becoming better understood. However, to success- dency to divide a problem into well-deﬁned pieces,
fully interact in the real world, agents must be able to reason and make progress on each one. But as we make
about their interactions with heterogeneous agents of widely progress, the problems tend to move away from
varying properties and capabilities.                      each other. [Koller, 2001]
  Some of these other agents will have similar or identical In Section 5, I argue that creating fully functional agents in
goals, in which case there may be some advantage to explic- complex application environments is another excellent way
itly coordinating their activities. On the other hand, some of to combat such fragmentation. Finally, Section 6 concludes.

                                                IJCAI-07
                                                   132Learning                                             sign choices can result in estimates that diverge from the
                                                      optimal value function [Baird, 1995] and agents that per-
Machine Learning has been an active area within AI for many form poorly. Even for reinforcement learning algorithms with
years. Indeed Tom Mitchell’s Computers and Thought paper guaranteed convergence [Baird and Moore, 1999; Lagoudakis
more than two decades ago refers to “a resurgence of inter- and Parr, 2003], achieving high performance in practice re-
est in machine learning” due to advances in general prob- quires ﬁnding an appropriate representation for the function
                                          [
lem solving and knowledge-based expert systems Mitchell, approximator. As Lagoudakis and Parr observe,
1983]. Since then, supervised learning methods, for both
classiﬁcation and regression, have matured to the point of The crucial factor for a successful approximate al-
enabling a general purpose toolkit that can be used produc- gorithm is the choice of the parametric approxima-
tively by experts and novices alike [Witten and Frank, 1999]. tion architecture(s) and the choice of the projection
Similarly, unsupervised learning algorithms for data cluster- (parameter adjustment) method. [Lagoudakis and
ing have advanced nicely. But from the point of view of au- Parr, 2003]
tonomous agents, it is the relatively recent development of Nonetheless, representational choices are typically made
reinforcement learning (RL) algorithms, designed to learn manually, based only on the designer’s intuition.
action selection from delayed reward in sequential decision
                                   1                    Despite an early emphasis in the ﬁeld on “tabula rasa”
making problems, that is most signiﬁcant.             learning, it is becoming increasingly accepted that the process
  Unlike classical supervised and unsupervised learning of designing agents for complex, real-world domains will al-
where the learner must be supplied with training data so that it ways require some manual input of this sort. The key to suc-
can learn a function from features to the target set, the premise cess will be limiting the requirements for human knowledge
of RL matches the agent paradigm exactly: the learner gath- to tasks and representations for which humans have good in-
ers its own training data by interacting with the environment tuitions. As referred to above, the existence of a general
so that it can learn a policy mapping states to actions. An RL purpose toolkit that can be used without expert knowledge
agent repeatedly takes actions that both move it to a new state of the underlying algorithms suggests that supervised learn-
in its environment and lead to some immediate reward signal. ing methods are mature enough for use in robust, complex
The learner must explicitly tradeoff between exploration and systems. That is not yet the case for RL. Despite the ele-
exploitation in an effort to maximize the long-term reward gant theory that accompanies TD methods, most notably that
that it will receive.                                 Q-learning converges to an optimal value function if every
  One common approach to reinforcement learning relies on state is visited inﬁnitely often [Watkins, 1989], and despite
the concept of value functions, which indicate, for a particular a limited number of successes that have been reported in
policy, the long-term value of a given state or state-action pair. large-scale domains [Tesauro, 1994; Crites and Barto, 1996;
Temporal difference (TD) methods [Sutton, 1988],which Stone et al., 2005], crucial, somewhat unintuitive decisions
combine principles of dynamic programming with statistical about representations need to be made based on a deep un-
sampling, use the immediate rewards received by the agent derstanding of the underlying algorithms and application do-
to incrementally improve both the agent’s policy and the es- main.
timated value function for that policy. Hence, TD methods The remainder of this section outlines directions for re-
enable an agent to learn during its “lifetime” from its individ- search that will help current reinforcement learning, and other
ual experience interacting with the environment.2     machine learning, methods scale up to the point that they can
  For small problems, the value function can be represented become core components of fully autonomous agents in real-
as a table. However, the large, probabilistic domains which world tasks. Sections 2.1 and 2.2 deal with automatically
arise in the real world usually require coupling TD methods adjusting knowledge representations used for learning. Sec-
with a function approximator, which represents the mapping tions 2.3 and 2.4 address ways in which humans can provide
from state-action pairs to values via a more concise, parame- intuitive knowledge to learning agents, either by providing
terized function and uses supervised learning methods to set a subtask decomposition or by suggesting related tasks for
its parameters. Many different methods of function approx- knowledge transfer.
imation have been used successfully, including CMACs, ra-
dial basis functions, and neural networks [Sutton and Barto, 2.1 Adaptive Function Approximation
    ]
1998 . However, using function approximators requires mak- In order to address the observation of Lagoudakis and Parr
ing crucial representational decisions (e.g. the number of hid- above, Whiteson and Stone [2006] automate the search for
den units and initial weights of a neural network). Poor de- effective function approximators for RL agents by applying

  1                                                   optimization techniques to the representation problem. In
   This is not to say that classical learning is irrelevant to agents. that research, we propose using evolutionary methods [Gold-
An autonomous agent may well incorporate learned predictions as a berg, 1989] for optimizing the representation because of
part of its world model. For example an autonomous bidding agent
may use regression to predict the future closing price of an auction, their demonstrated ability to discover effective representa-
and then use this information to evaluate its potential bids [Stone et tions [Gruau et al., 1996; Stanley and Miikkulainen, 2002].
al., 2003].                                           Synthesizing evolutionary and TD methods results in a new
  2TD methods contrast with policy search methods which learn approach called evolutionary function approximation,which
based on the results of holistic policy evaluations rather than from automatically selects function approximator representations
individual action effects.                            that enable efﬁcient individual learning. Thus, this method

                                                IJCAI-07
                                                   14evolves individuals that are better able to learn.Thisbi- gests a knowledge-transfer framework, in which we analyze
ologically intuitive combination has been applied to com- learned policies in one environment to discover abstractions
putational systems in the past [Hinton and Nowlan, 1987; that might improve learning in similar environments.
Ackley and Littman, 1991; Boers et al., 1995; French and We introduce an efﬁcient algorithm that discovers local re-
Messinger, 1994; Gruau and Whitley, 1993; Nolﬁ et al., gions of the state space in which the same action would be op-
1994] but never, to our knowledge, to aid the discovery of timal regardless of the value of a given set of state variables.
good TD function approximators.                       For example, you would take the same route to work in New
  Speciﬁcally, we use NeuroEvolution of Augmenting    York no matter what the weather is in London. This algorithm
Topologies (NEAT)  [Stanley and Miikkulainen, 2002]   depends on determining what set of actions is optimal in each
to select neural network function approximators for Q- state; we give two statistical tests for this criterion that trade
learning [Watkins, 1989], a popular TD method. The result- off computational and sample complexity. We then general-
ing algorithm, called NEAT+Q, uses NEAT to evolve topolo- ize this learned structure to a similar environment, where an
gies and initial weights of neural networks that are better able agent can ignore each set of state variables in the correspond-
to learn, via backpropagation, to represent the value estimates ing learned region of the state space.
provided by Q-learning.                                 We must take care when we apply our discovered abstrac-
  In experimental evaluation, we test Q-learning with a series tions, since the criteria we use in discovery are strictly weaker
of manually designed neural networks and compare the re- than those given in other work on safe state abstraction [Li et
sults to NEAT+Q and regular NEAT, which learns direct rep- al., 2006]. Transferring abstractions from one domain to an-
resentations of policies. The results demonstrate that evolu- other may also introduce generalization error. To preserve
tionary function approximation can signiﬁcantly improve the convergence to an optimal policy, we encapsulate our state
performance of TD methods, thus providing a much-needed abstractions in temporal abstractions,oroptions, which con-
practical approach to selecting TD function approximators, strue sequences of primitive actions as constituting a single
automating a critical design step that is typically performed abstract action [Sutton et al., 1999]. In contrast to previ-
manually.                                             ous work with temporal abstraction, we discover abstract ac-
  Though that research takes a step towards automating the tions intended just to simplify the state representation, not to
choice of representations for learning, there is still much achieve a certain goal state. RL agents equipped with these
room for future work, including extending it to use dif- abstract actions thus learn when to apply state abstraction the
ferent policy search methods such as PEGASUS [Ng and  same way they learn when to execute any other action.
Russell, 2000] and policy gradient methods [Sutton et al., The fact that abstractions are the building blocks for hi-
2000]; and perhaps more importantly, optimizing function erarchical RL suggests the recursive application of our ab-
approximators other than neural networks, such as CMACs straction discovery technique to create hierarchies of tempo-
and radial basis functions. Research on feature selection ral abstractions that explicitly facilitate state abstractions, as
to adapt the inputs to the representation [Fawcett, 1993; in MAXQ task decompositions [Dietterich, 2000]. This pos-
Whiteson et al., 2005] is also in this space of adaptive rep- sibility highlights the need for robust testing of optimal ac-
resentations.                                         tions, since each application of our method adds new poten-
                                                      tially optimal actions to the agent. Learning MAXQ hierar-
2.2  Learned Abstractions                             chies based on our method is a natural direction for future
Another approach to adjusting problem representation is state research.
abstraction, which maps two distinct ground states to a sin-
gle abstract state if an agent should treat the ground states in 2.3 Layered Learning
exactly the same way. The agent can still learn optimal behav- Hierarchies in general are powerful tools for decomposing
ior if the environment satisﬁes a particular condition, namely complex control tasks into manageable subtasks. As a case
that each action has the same abstract outcome (next state and in point, mammalian biology is a composition of hierarchi-
reward) for all primitive states that are mapped to the same cally organized components, each able to perform specialized
abstract state: ground states that are grouped together must subtasks. These components span many levels of behavior
share the same local behavior in the abstract state space [Dean ranging from individual cells to complex organs, culminat-
and Givan, 1997; Ravindran and Barto, 2003].However,this ing in the complete organism. Even at the purely behavioral
cited research only applies in a planning context, in which the level, organisms have distinct subsystems, including reﬂexes,
domain model is given, or if the user manually determines the visual system, etc. It is difﬁcult to imagine a monolithic
that the condition holds and supplies the corresponding state entity that would be capable of the range and complexity of
abstraction to the RL algorithm.                      behaviors that mammals exhibit.
  Jong and Stone [2005] propose an alternative condition for As covered in the previous section, initial steps have
state abstraction that is more conducive to automatic discov- been made towards learning hierarchies automatically in rel-
ery. Intuitively, if an agent can behave optimally while ignor- atively simple domains. However, in complex tasks, the hi-
ing a certain variable of the state representation, then it might erarchies still need to be deﬁned manually [Brooks, 1986;
be able to learn successfully while ignoring that state vari- Gat, 1998]. Layered learning [Stone and Veloso, 1998;
able. Recognizing that discovering such qualitative structure 2000a] is a hierarchical paradigm that similarly requires a
tends to require more time than learning an optimal behav- given task decomposition, but that then relies on learning the
ior policy [Thrun and Schwartz, 1995], this approach sug- various subtasks necessary for achieving the complete high-

                                                IJCAI-07
                                                   15level goal. Layered learning is a bottom-up paradigm by the choice of machine learning method depends on the sub-
which low-level behaviors (those closer to the environmen- task.
tal inputs) are trained prior to high-level behaviors.
                                                      Principle 4
  The layered learning approach is somewhat reminiscent of
Rodney Brooks’ subsumption architecture as summarized in The key deﬁning characteristic of layered learning is that each
his Computers and Thought paper [Brooks, 1991]. The sub- learned layer directly affects the learning at the next layer. A
sumption architecture layers control modules, allowing high- learned subtask can affect the subsequent layer by:
level controllers to override lower-lever ones. Each control • constructing the set of training examples;
level is capable of controlling a robot on its own up to a
                                                        • providing the features used for learning; and/or
speciﬁed level of functionality. In order to focus on learning
and to move quickly to high-level behaviors, layered learning • pruning the output set.
abandons the commitment to have every layer be completely Layered learning was originally applied in a complex,
able to control a robot. Instead, many situation-speciﬁc (but multi-agent learning task, namely simulated robot soccer in
as general as possible) behaviors are learned which are then the RoboCup soccer server [Noda et al., 1998].Anexten-
managed by higher-level behaviors. Nevertheless, the idea of sion that allows for concurrent training of multiple layers was
building higher levels of functionality on top of lower levels also implemented in simulation [Whiteson and Stone, 2003].
is retained.                                          As described below in Section 4.1, layered learning has re-
  Table 1 summarizes the principles of the layered learning cently been applied successfully on physical robots. In all
paradigm which are described in detail in this section. these cases, the subtask decomposition is supplied manually
                                                      and if relatively intuitive to construct. Nonetheless, discover-
 1. A  mapping directly from inputs to outputs is not ing ways to automate this decomposition, perhaps by leverag-
    tractably learnable.                              ing the abstraction discovery work described in Section 2.2,
 2. A hierarchical task decomposition is given.       is an important future goal.
 3. Machine learning exploits data to train and/or adapt. 2.4 Transfer Learning
    Learning occurs separately at each level.         A particularly topical area of AI research in 2007 is trans-
 4. The output of learning in one layer feeds into the next fer learning: leveraging learned knowledge on a source task
    layer.                                            to improve learning on a related, but different, target task.
                                                      Transfer learning can pertain to classical learning, but it is
     Table 1: The key principles of layered learning. particularly appropriate for learning agents that are meant to
                                                      persist over time, changing ﬂexibly among tasks and envi-
                                                      ronments. Rather than having to learn each new task from
Principle 1                                           scratch, the goal is to enable an agent to take advantage of its
Layered learning is designed for domains that are too com- past experience to speed up new learning.
plex for learning a mapping directly from the input to the For a reinforcement learning agent, there are several ways
output representation. Instead, the layered learning approach in which the source and target may differ in a transfer prob-
consists of breaking a problem down into several task lay- lem. For example, source and target tasks with the following
ers. At each layer, a concept needs to be acquired. A ma- differences have been studied in the literature:
chine learning (ML) algorithm abstracts and solves the local • Transition function: Effects of agents’ actions dif-
concept-learning task.                                    fer [Selfridge et al., 1985]
Principle 2                                             • Reward structure: Agents have different goals [Singh,
Layered learning uses a bottom-up incremental approach to 1992]
hierarchical task decomposition. Starting with low-level sub- • State space: Agents’ environments differ [Konidaris and
tasks, the process of creating new ML subtasks continues un- Barto, 2006; Fernandez and Veloso, 2006]
til the high-level tasks, that deal with the full domain com- •
plexity, are reached. The appropriate learning granularity and Initial state: Agents start in different locations over
subtasks to be learned are determined as a function of the time [Asada et al., 1994]
speciﬁc domain. The task decomposition in layered learning • Actions: Agents have different available actions [Maclin
is not automated. Instead, the layers are deﬁned by the ML et al., 2005; Taylor et al., 2005; Soni and Singh, 2006]
opportunities in the domain.                            • State variables: Agents’ state descriptions differ [Maclin
Principle 3                                               et al., 2005; Taylor et al., 2005; Soni and Singh, 2006]
Machine learning is used as a central part of layered learning In the most general case, the source and target can differ
to exploit data in order to train and/or adapt the overall sys- in all of these ways. For such cases, Taylor et al. [2005] in-
tem. ML is useful for training functions that are difﬁcult to troduce a method for transferring the learned value function
ﬁne-tune manually. It is useful for adaptation when the task in a source RL task to seed learning in the target. The key
details are not completely known in advance or when they technical challenge is mapping a value function in one repre-
may change dynamically. Like the task decomposition itself, sentation to a meaningful value function in another, typically

                                                IJCAI-07
                                                   16larger, representation, despite the fact that state-action values
are inherently task-speciﬁc.
  Past research conﬁrms that if two tasks are closely related
the learned policy from one task can be used to provide a good
initial policy for the second task. For example, Selfridge et
al. [1985] showed that the 1-D pole balancing task could be
made harder over time by shortening the length of the pole
and increasing its mass; when the learner was ﬁrst trained
on a longer and lighter pole it could more quickly learn to
succeed in the more difﬁcult task with the modiﬁed transition
function. In this way, the learner is able to reﬁne an initial
policy for a given task.
  We consider the more general case where tasks are related
but distinct in that their state and/or action spaces differ. To
use the source policy πS as the initial policy for a TD learner
in the target task, we must transform the value function so
that it can be directly applied to the new state and action
space. We introduce the notion of a behavior transfer func-
tional ρ(πS)=πT  that will allow us to apply a policy in
the target task. The policy transform functional, ρ, needs to
modify the source policy and its associated value function so
that it accepts the states in the target task as inputs and allows
for the actions in the target task to be outputs, as depicted in Figure 1: ρ is a functional which transforms a value function
Figure 1.                                             Q from one task so that it is applicable in a second task that
  Deﬁning ρ to do this modiﬁcation in such a way that πT has different state and action spaces.
is a good starting point for learning in the target is the key
technical challenge to enable general behavior transfer. Cur-     3
                         ρ                            is appropriate. For example, some domains require MAS,
rent results indicate that such ’s do exist, at least for some such as those in which agents represent people or organiza-
tasks [Taylor et al., 2005]. However, automating the discov- tions with different (possibly conﬂicting) goals and propri-
ery of the inter-task mapping between the state variables and etary information; having multiple agents may speed up com-
actions in the source and target tasks remains an open chal- putation via parallelization; MAS can provide robustness via
lenge, as does automatically selecting the source and target redundancy; MAS may be more scalable as a result of mod-
tasks themselves.                                     ularity; and they can be useful for their elucidation of funda-
                                                      mental problems in the social sciences and life sciences [Cao
3  Multiagent Reasoning                               et al., 1997], including intelligence itself [Decker, 1987].Re-
                                                      garding this last point, as Weiß [1996] put it: “Intelligence
In addition to learning, a second essential capability of robust, is deeply and inevitably coupled with interaction.” In fact, it
fully autonomous agents is the ability to interact with other has been proposed that the best way to develop intelligent ma-
agents: multiagent reasoning. As argued in the introduction, chines might be to start by creating “social” machines [Daut-
to successfully interact in the real world, agents must be able enhahn, 1995]. This theory is based on the socio-biological
to reason about their interactions with heterogeneous agents theory that primate intelligence ﬁrst evolved because of the
of widely varying properties and capabilities. Once we have need to deal with social interactions [Minsky, 1988].
a single complete agent that is able to operate autonomously Multiagent systems differ from single-agent systems in that
for extended periods of time in the real world, it is inevitable several agents exist that model each other’s goals and ac-
that soon after we will have many. And they will need to tions. In the fully general multiagent scenario, there may be
interact with one another.                            direct interaction among agents (communication). Although
  Though more recent to our ﬁeld than Machine Learning, this interaction could be viewed as environmental stimuli, we
in the past decade Multiagent Systems (MAS) has begun to present inter-agent communication as being separate from the
come to the forefront. As with any methodology, two impor- environment.
tant questions about MAS are:                           From an individual agent’s perspective, multiagent systems
  • What advantages does it offer over the alternatives? and differ from single-agent systems most signiﬁcantly in that the
                                                      environment’s dynamics can be affected by other agents. In
  • In what circumstances is it useful?               addition to the uncertainty that may be inherent in the domain,
                                                      other agents intentionally affect the environment in unpre-
It would be foolish to claim that MAS should be used when dictable ways. Thus, all multiagent systems can be viewed as
designing all complex systems. Like any approach, there are having dynamic environments. Figure 2 illustrates the view
some situations for which it is particularly appropriate, and
others for which it is not. In a survey of the ﬁeld, Stone and 3Other takes on the same issue appear elsewhere [Bond and
Veloso [2000b] summarized the circumstances in which MAS Gasser, 1988; Sycara, 1998].

                                                IJCAI-07
                                                   17