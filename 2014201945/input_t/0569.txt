                An MCMC Approach to Solving Hybrid Factored MDPs

                    Branislav Kveton                             Milos Hauskrecht
                Intelligent Systems Program               Department of Computer Science
                  University of Pittsburgh                     University of Pittsburgh
                    bkveton@cs.pitt.edu                           milos@cs.pitt.edu


                    Abstract                          Patrascu, 2002]. However, these methods are exponential in
                                                      the treewidth of the discretized constraint space, which limits
    Hybrid approximate linear programming (HALP)      their application to real-world problems. In addition, the ε-
    has recently emerged as a promising framework for grid discretization is done blindly and impacts the quality of
    solving large factored Markov decision processes  the approximation. Monte Carlo methods [de Farias and Roy,
    (MDPs) with discrete and continuous state and ac- 2004; Hauskrecht and Kveton, 2004] offer an alternative to
    tion variables. Our work addresses its major com- the ε-grid discretization and approximate the constraint space
    putational bottleneck – constraint satisfaction in in HALP by its ﬁnite sample. Unfortunately, the efﬁciency of
    large structured domains of discrete and continuous Monte Carlo methods is heavily dependent on an appropriate
    variables. We analyze this problem and propose a  choice of sampling distributions. The ones that yield good ap-
    novel Markov chain Monte Carlo (MCMC) method      proximations and polynomial sample size bounds are closely
    for ﬁnding the most violated constraint of a relaxed related to the optimal solutions and rarely known a priori [de
    HALP. This method does not require the discretiza- Farias and Roy, 2004].
    tion of continuous variables, searches the space of To overcome the limitations of the discussed constraint sat-
    constraints intelligently based on the structure of isfaction techniques, we propose a novel Markov chain Monte
    factored MDPs, and its space complexity is linear Carlo (MCMC) method for ﬁnding the most violated con-
    in the number of variables. We test the method on a straint of a relaxed HALP. The method directly operates in
    set of large control problems and demonstrate im- the domains of continuous variables, takes into account the
    provements over alternative approaches.           structure of factored MDPs, and its space complexity is pro-
                                                      portional to the number of variables. Such a separation ora-
1  Introduction                                       cle can be easily embedded into the ellipsoid or cutting plane
Markov decision processes (MDPs) [Bellman, 1957; Puter- method for solving linear programs, and therefore constitutes
man, 1994] offer an elegant mathematical framework for a key step towards solving HALP efﬁciently.
solving sequential decision problems in the presence of un- The paper is structured as follows. First, we introduce hy-
                                                                          [                ]
certainty. However, traditional techniques for solving MDPs brid MDPs and HALP Guestrin et al., 2004 , which are our
are computationally infeasible in real-world domains, which frameworks for modeling and solving large-scale stochastic
are factored and contain both discrete and continuous state decision problems. Second, we review existing approaches to
and action variables. Recently, approximate linear program- solving HALP and discuss their limitations. Third, we com-
ming (ALP) [Schweitzer and Seidmann, 1985] has emerged pactly represent the constraint space in HALP and formulate
as a promising approach to solving large factored MDPs [de an optimization problem for ﬁnding the most violated con-
Farias and Roy, 2003; Guestrin et al., 2003]. This work fo- straint of a relaxed HALP. Fourth, we design a Markov chain
cuses on hybrid ALP (HALP) [Guestrin et al., 2004] and ad- to solve this optimization problem and embed it into the cut-
dresses its major computational bottleneck – constraint satis- ting plane method. Finally, we test our HALP solver on a
faction in the domains of discrete and continuous variables. set of large control problems and compare its performance to
  If the state and action variables are discrete, HALP in- alternative approaches.
volves an exponential number of constraints, and if any of the
variables are continuous, the number of constraints is inﬁnite. 2 Hybrid factored MDPs
To approximate the constraint space in HALP, two techniques Factored MDPs [Boutilier et al., 1995] allow a compact rep-
have been proposed: ε-HALP [Guestrin et al., 2004] and resentation of large stochastic planning problems by exploit-
Monte Carlo constraint sampling [de Farias and Roy, 2004; ing their structure. In this section, we review hybrid factored
Hauskrecht and Kveton, 2004]. The ε-HALP formulation re- MDPs [Guestrin et al., 2004], which extend this formalism to
laxes the continuous portion of the constraint space to an ε- the domains of discrete and continuous variables.
grid, which can be compactly satisﬁed by the methods for A hybrid factored MDP with distributed actions (HMDP)
discrete-state ALP [Guestrin et al., 2001; Schuurmans and [Guestrin et al., 2004] is a 4-tuple M = (X, A, P, R), whereX =  {X1, . . . , Xn} is a state space represented by a set of which is stationary and deterministic [Puterman, 1994]. The
state variables, A = {A1, . . . , Am} is an action space repre- policy is greedy with respect to the optimal value function
sented by a set of action variables, P (X′ | X, A) is a stochas- V ∗, which is a ﬁxed point of the Bellman equation [Bellman,
tic transition model of state dynamics conditioned on the pre- 1957; Bertsekas and Tsitsiklis, 1996]:
ceding state and action choice, and R is a reward model as-
                                              1
signing immediate payoffs to state-action conﬁgurations . ∗ x       x a            x′ x a   ∗ x′  x′
                                                      V  ( )=sup  R( , )+γ      P (  | ,  )V (  )d C . (3)
State variables: State variables are either discrete or contin-                 ′
                                                               a           ′  x                    
                                                                           x    C
                                                                            D Z
uous. Every discrete variable Xi takes on values from a ﬁnite              X
domain Dom(Xi). Following Hauskrecht and Kveton 2004,                                              
we assume that every continuous variable is bounded to the 3 Hybrid ALP
[0, 1] subspace. The state is represented by a vector of value Value iteration, policy iteration, and linear programming are
assignments x = (xD, xC ) which partitions along its discrete the most fundamental dynamic programming (DP) methods
and continuous components xD and xC .                 for solving MDPs [Puterman, 1994; Bertsekas and Tsitsiklis,
Action variables: The action space is distributed and repre- 1996]. However, their computational complexity grows expo-
sented by action variables A. The composite action is deﬁned nentially in the number of used variables, which makes them
by a vector of individual action choices a = (aD, aC ) which unsuitable for factored MDPs. Moreover, they are built on the
partitions along its discrete and continuous components aD assumption of ﬁnite support for the optimal value function
and aC .                                              and policy, which may not exist if continuous variables are
Transition model: The transition model is given by the con- present. Recently, Feng et al. 2004 showed how to solve gen-
ditional probability distribution P (X′ | X, A), where X and eral state-space MDPs by performing DP backups of piece-
X′ denote the state variables at two successive time steps. We wise constant and piecewise linear value functions. This ap-
assume that the model factors along X′ as P (X′ | X, A) = proximate method has a lower scale-up potential than HALP,
  n      ′       ′
  i=1 P (Xi | Par(Xi)) and can be compactly represented by but does not require the design of basis functions.
a dynamic Bayesian network (DBN) [Dean and Kanazawa,  Linear value function: Value function approximation is a
Q                              ′    X   A
1989]. Usually, the parent set Par(Xi) ⊆ ∪ is a small standard approach to solving large factored MDPs. Due to
subset of state and action variables which allows for a local its favorable computational properties, linear value function
parameterization of the model.                        approximation [Bellman et al., 1963; Roy, 1998]:
Parameterization of transition model: One-step dynamics
                                                                      w x            x
of every state variable is described by its conditional proba-      V  ( ) =    wifi( )
                   ′        ′       ′                                         i
bility distribution P (Xi | Par(Xi)). If Xi is a continuous                  X
variable, its transition function is represented by a mixture of has become extremely popular in recent research [Guestrin et
beta distributions [Hauskrecht and Kveton, 2004]:     al., 2001; Schuurmans and Patrascu, 2002; de Farias and Roy,
                                                      2003; Hauskrecht and Kveton, 2004; Guestrin et al., 2004].
           ′         Γ(αj + βj ) αj −1    βj −1
P (x | Par(Xi))=  πij          x    (1−x)     , (1)   This approximation restricts the form of the value function
                     Γ(α )Γ(β )                         w
                j       j    j                                                     w                  x
               X                                      V   to the linear combination of | | basis functions fi( ),
                                                      where w is a vector of tunable weights. Every basis function
where πij is the weight assigned to the j-th component of
                                                      can be deﬁned over the complete state space X, but often is
the mixture, and α = φα (Par(X′)) and β = φβ (Par(X′))
               j    ij      i      j    ij      i     restricted to a subset of state variables X .
are arbitrary positive functions of the parent set. The mixture                         i
of beta distributions provides a very general class of transition 3.1 HALP formulation
functions and yet allows closed-form solutions to the integrals
                                                      Various methods for ﬁtting of the linear value function ap-
in HALP. If X′ is a discrete variable, its transition model is
            i                                         proximation have been proposed and analyzed [Bertsekas and
parameterized by |Dom(X′)| nonnegative discriminant func-
                      i                               Tsitsiklis, 1996]. We adopt a variation on approximate linear
          θ       ′  [                ]
tions θj = φij(Par(Xi)) Guestrin et al., 2004 :       programming (ALP) [Schweitzer and Seidmann, 1985], hy-
                                                      brid ALP (HALP) [Guestrin et al., 2004], which extends this
                   ′          θj
        P (j | Par(Xi)) =        ′   .          (2)   framework to the domains of discrete and continuous vari-
                          |Dom(Xi)|
                          j=1      θj                 ables. The HALP formulation is given by:

Reward model: The reward functionP is an additive function minimizew   w  α
  x a           x  a                                                     i i
R( , ) =    j Rj( j , j ) of local reward functions deﬁned           i
                                    X     A                         X
on the subsets of state and action variables j and j.                       x  a     x  a        x a
Optimal valueP function and policy: The quality of a pol-  subject to: wiFi(  , ) − R( , ) ≥ 0 ∀  , ;
                                                                     i
icy is measured by the inﬁnite horizon discounted reward            X
    ∞   t
E[  t=0 γ rt], where γ ∈ [0, 1) is a discount factor and rt where αi denotes basis function relevance weight:
is the reward obtained at the time step t. This optimality cri-
                                                 ∗
terionP guarantees that there always exists an optimal policy π  αi =       ψ(x)fi(x) dxC ,           (4)
                                                                          x
                                                                      xD   C
  1General state and action space MDP is an alternative name for      X  Z
a hybrid MDP. The term hybrid does not refer to the dynamics of the ψ(x) is a state relevance density function weighting the qual-
model, which is discrete-time.                        ity of the approximation, and Fi(x, a) = fi(x) − γgi(x, a)is the difference between the basis function fi(x) and its dis- Monte Carlo constraint sampling: Monte Carlo methods
counted backprojection:                               approximate the constraint space in HALP by its ﬁnite sam-
                                                      ple. De Farias and Van Roy 2004 analyzed constraint sam-
                        ′          ′   ′
     gi(x, a) =      P (x | x, a)fi(x ) dx .    (5)   pling for discrete-state ALP and bounded the sample size for
                    ′                  C
                ′  x
               x    C
                D Z                                   achieving good approximations by a polynomial in the num-
               X                                      ber of basis functions and state variables. Hauskrecht and
We say that the HALP is relaxed if only a subset of the con-
                                                      Kveton 2004 applied random constraint sampling to solve
straints is satisﬁed.
                                                      continuous-state factored MDPs and later reﬁned their sam-
  The HALP formulation reduces to the discrete-state ALP
                                                      pler by heuristics [Kveton and Hauskrecht, 2004].
[Schweitzer and Seidmann, 1985; Schuurmans and Patrascu,
                                                        Monte Carlo constraint sampling is easily applied in con-
2002; de Farias and Roy, 2003; Guestrin et al., 2003] if the
                                                      tinuous domains and can be implemented in a space propor-
state and action variables are discrete, and to the continuous-
                                                      tional to the number of variables. However, proposing an efﬁ-
state ALP [Hauskrecht and Kveton, 2004] if the state vari-
                                                      cient sampling procedure that guarantees a polynomial bound
ables are continuous. The quality of this approximation
                                                      on the sample size is as hard as knowing the optimal policy
was studied by Guestrin et al. 2004 and bounded with re-
                    w                                 itself [de Farias and Roy, 2004]. To lower a high number of
spect to minw kV ∗ − V k    , where k·k    is a max-
                      ∞,1/L          ∞,1/L            sampled constraints in a relaxed HALP, Monte Carlo sam-
norm weighted by the reciprocal of the Lyapunov function plers can be embedded into the cutting plane method. A tech-
  x         L  x
L( ) =   i wi fi( ). The integrals in the objective function nique similar to Kveton and Hauskrecht 2004 yields signiﬁ-
(Equation 4) and constraints (Equation 5) have closed-form cant speedup with no drop in the quality of the approximation.
solutionsP if the basis functions and state relevance densities
are chosen appropriately [Hauskrecht and Kveton, 2004]. For
example, the mixture of beta transition model (Equation 1) 4 MCMC constraint sampling
yields a closed-form solution to Equation 5 if the basis func- To address the deﬁciencies of the discussed constraint satis-
       ′
tion fi(x ) is a polynomial. Finally, we need an efﬁcient con- faction techniques, we propose a novel Markov chain Monte
straint satisfaction procedure to solve HALP.         Carlo (MCMC) method for ﬁnding the most violated con-
                                                      straint of a relaxed HALP. Before we proceed, we compactly
3.2  Constraint satisfaction in HALP                  represent the constraint space in HALP and formulate an op-
If the state and action variables are discrete, HALP involves timization problem for ﬁnding the most violated constraint in
an exponential number of constraints, and if any of the vari- this representation.
ables are continuous, the number of constraints is inﬁnite.
To approximate such a constraint space, two techniques have 4.1 Compact representation of constraints
been proposed recently: ε-HALP [Guestrin et al., 2004] and Guestrin et al. 2001 and Schuurmans and Patrascu 2002
Monte Carlo constraint sampling [de Farias and Roy, 2004; showed that the compact representation of constraints is es-
Hauskrecht and Kveton, 2004].                         sential in solving ALP efﬁciently. Following their ideas, we
ε-HALP:  The ε-HALP formulation relaxes the continuous deﬁne violation magnitude τ w(x, a):
portion of the constraint space to an ε-grid by the discretiza-
                       X      A                          w
tion of continuous variables C and C . The new constraint τ (x, a) = −  wi[fi(x) − γgi(x, a)] + R(x, a) (7)
space spans discrete variables only and can be compactly sat-        i
                                                                    X
isﬁed by the methods for discrete-state ALP [Guestrin et al.,                           w
2001; Schuurmans and Patrascu, 2002]. For example, Schu- to be the amount by which the solution violates the con-
                                                      straints of a relaxed HALP. We represent τ w(x, a) compactly
urmans and Patrascu 2002 search for the most violated con-                            X     A
straint with respect to the solution w(t) of a relaxed ALP: by an inﬂuence diagram (ID), where and are decision
                                                      nodes and X′ are random variables. The ID representation is
                                                                                 X′  X A
               (t)                                    built on the transition model P ( | , ), which is already
  arg min     w  [fi(x) − γgi(x, a)] − R(x, a)  (6)                                                   X
      x,a      i                                      factored and contains dependencies among the variables ,
         "  i                              #          X′     A
          X                                              , and . We extend the diagram by three types of reward
and add it to the linear program. If no violated constraint is nodes, one for each term in Equation 7: Rj = Rj(xj , aj ) for
       (t)
found, w  is an optimal solution to the ALP.          every local reward function, Hi = −wifi(x) for every basis
                                                                              ′
  The space complexity of both constraint satisfaction meth- function, and Gi = γwifi(x ) for every backprojection. The
ods [Guestrin et al., 2001; Schuurmans and Patrascu, 2002] construction is completed by adding arcs that represent the
is exponential in the treewidth of the constraint space. This dependencies of the reward nodes on the variables. Finally,
                                                                   w
                                                                     x a        ′
is a serious limitation because the cardinality of discretized we verify that τ ( , ) = EP (x |x,a)[ i(Hi+Gi)+ j Rj].
variables grows with the resolution of the ε-grid. Roughly, if Therefore, the decision that maximizes the expected utility in
the discretized variables are replaced by binary, the treewidth the ID corresponds to the most violatedP constraint. P
increases by a multiplicative factor of log2(1/ε + 1), where We conclude that any algorithm for solving IDs can be
(1/ε + 1) is the number of discretization points in a single used to ﬁnd the most violated constraint. Moreover, special
dimension. Therefore, even problems with a relatively small properties of the ID representation allow its further simpliﬁ-
treewidth are intractable for small values of ε. In addition, the cation. If the basis functions are chosen conjugate to the tran-
ε-grid discretization is done blindly and impacts the quality sition model (Section 3.1), we obtain a closed-form solution
of the approximation.                                 to EP (x′|x,a)[Gi] (Equation 5), and thus the random variablesX′                                                          z      z∗
   can be marginalized out of the diagram. This new repre- where −i and −i are the assignments to all variables but Zi
sentation contains no random variables and is know as a cost in the original and proposed states. If Z is a discrete variable,
                                                                                      i    ∗
                                                                                  (     −             )
network [Guestrin et al., 2001].                      its conditional p(z∗ | z ) = p z1,...,zi 1,zi ,zi+1,...,zn+m
                                                                     i   −i        p(z1,...,zi−1,zi,zi+1,...,zn+m)
                                                                                 zi
4.2  Separation oracle                                can be derived in a closed form.P If Zi is a continuous variable,
                                                      a closed form of its cumulative density function is not likely
To ﬁnd the most violated constraint in the cost network, to exist. To allow sampling from its conditional, we embed
we use the Metropolis-Hastings (MH) algorithm [Metropo- another MH step within the original chain. In the experimen-
lis et al., 1953; Hastings, 1970] and construct a Markov tal section, we use the Metropolis algorithm with the accep-
chain whose invariant distribution converges to the vicinity                              ∗ z
                                                                           ∗           p(zi | −i)
            w                                         tance probability A(zi, z ) = min 1, z    , where zi
of arg maxz τ (z), where z = (x, a) and Z = X ∪  A                         i            p(zi| −i)
                                                           ∗
is a joint set of state and action variables. The Metropolis- and zi correspond to the original andn proposed valueso of Zi.
Hastings algorithm accepts the transition from a state z to a Note that sampling from both conditionals can be performed
                                                                    w
proposed state z∗ with the acceptance probability A(z, z∗) = in the space of τ (z) and locally.
         ∗     ∗
       p(z )q(z|z )        ∗                            Finally, we get a non-homogenous Markov chain with the
             ∗    , where z   z is a proposal distribu-                                        −  ∗
min  1, p(z)q(z |z)     q(   | )                                                            1/Tt 1  z
                                                                            z z∗           p     (zi | −i)
                                                      acceptance probability A( , ) = min 1, 1/T −1 z
tion nand p(z) is a targeto density. Under mild restrictions on                            p  t  (zi| −i)
p(z) and q(z∗ | z), the chain always converges to the target that converges to the vicinity of the mostn violated constraint.o
density p(z) [Andrieu et al., 2003]. In the rest of this sec- A similar chain was derived by Yuan et al. 2004 and applied
tion, we discuss the choice of p(z) and q(z∗ | z) to solve our to ﬁnd the maximum a posteriori (MAP) conﬁguration of ran-
optimization problem.                                 dom variables in Bayesian networks.
Target density: The violation magnitude τ w(z) is turned
                                         w            4.3  Constraint satisfaction
into a density by the transformation p(z) = exp[τ (z)]. Due
to its monotonic character, p(z) retains the same set of global If the MCMC oracle converges to a violated constraint (not
           w                                  w
maxima as τ (z), and thus the search for arg maxz τ (z) necessarily the most violated) in a polynomial time, it guar-
can be performed on p(z). To prove that p(z) is a density, we antees that the ellipsoid method can solve HALP in a poly-
                                                      nomial time [Bertsimas and Tsitsiklis, 1997]. However, con-
show that it has a normalizing constant z p(z) dzC ,
                                     D  zC
where z and z  are the discrete and continuous components vergence of our chain within an arbitrary precision requires
      D      C                    P    R              an exponential number of steps [Geman and Geman, 1984].
of the vector z = (zD, zC ). As the integrand zC is restricted
                    Z                                 Even if this bound is too weak to be of practical interest,
to the ﬁnite space | C |, the integral is proper as long as
               [0, 1]                                 it suggests that the time complexity of ﬁnding a violated
  z is bounded, and therefore it is Riemann integrable and
p( )                                                  constraint dominates the time complexity of solving HALP.
ﬁnite. To prove that z is bounded, we bound w z . Let
                 p( )                    τ ( )        Therefore, the search for violated constraints should be per-
     denote the maximum one-step reward in the HMDP.
Rmax                                                  formed efﬁciently. Convergence speedups that directly ap-
If the basis functions are of unit magnitude, can be typi-
                                      wi              ply to our work include hybrid Monte Carlo (HMC) [Du-
cally bounded by            −1     , and consequently
              |wi| ≤ γ(1 − γ) Rmax                    ane et al., 1987], slice sampling [Higdon, 1998], and Rao-
 w  z      w          −1         . Therefore, z  is
|τ ( )| ≤ (| | γ(1 − γ) + 1)Rmax             p( )     Blackwellization [Casella and Robert, 1996].
bounded and can be treated as a density function.
                      z                                 To evaluate the MCMC oracle, we embed it into the cutting
  To ﬁnd the mode of p( ), we adopt the simulating an- plane method for solving linear programs, which results in a
               [                    ]
nealing approach Kirkpatrick et al., 1983 and simulate a novel approach to solving HALP. As the oracle is not guar-
non-homogeneous Markov chain whose invariant distribution anteed to converge to the most violated constraint, we run the
         1/Tt z
equals to p ( ), where Tt is a decreasing cooling schedule cutting plane method for a ﬁxed number of iterations rather
with limt→∞ Tt = 0. Under weak regularity assumptions on than having the same stopping criterion as Schuurmans and
  z   ∞ z
p( ), p ( ) is a probability density that concentrates on the Patrascu 2002 (Section 3.2). Our MCMC solver differs from
                       z
set of global maxima of p( ) [Andrieu et al., 2003]. If the the Monte Carlo solver in that it samples constraints based on
cooling schedule decreases such that Tt ≥ c/ log2(t + 2), their potential to improve the existing solution, which sub-
where c is a problem-speciﬁc constant independent of t, the stitutes for an unknown problem-speciﬁc sampling distribu-
                                       w z
chain converges to the vicinity of arg maxz τ ( ) with the tion. Comparing to the ε-HALP method, the MCMC oracle
probability converging to 1 [Geman and Geman, 1984]. How- directly operates in the domains of continuous variables and
ever, this schedule can be too slow in practice, especially for its space complexity is linear in the number of variables.
a high initial temperature c. Following the suggestion of Ge-
man and Geman 1984, we overcome this limitation by select- 5 Experiments
ing a smaller value of c than is required by the convergence
criterion. As a result, convergence to the global optimum The performance of the MCMC solver is evaluated against
         w
arg maxz τ (z) is no longer guaranteed.               two alternative constraint satisfaction techniques: ε-HALP
Proposal distribution: We take advantage of the factored and uniform Monte Carlo sampling. Due to space limitations,
character of Z and adopt the following proposal distribution our comparison focuses on two irrigation-network problems
[Geman and Geman, 1984]:                              [Guestrin et al., 2004], but our conclusions are likely to gen-
                                                      eralize across a variety of control optimization tasks. The
                 p(z∗ | z )  if z∗ = z                irrigation-network problems are challenging for state-of-the-
    q(z∗ | z) =     i   −i      −i   −i  ,
                 0           otherwise                art MDP solvers due to the factored state and action spaces
               ½ Ring topology                              n = 6                n = 12                  n = 18
                                     OV     Reward  Time   OV     Reward   Time   OV      Reward   Time
                                1/4  24.3 35.1 ± 2.3  11   36.2 54.2 ± 3.0   43   48.0  74.5 ± 3.4   85
                 ε-HALP    ε =  1/8  55.4 40.1 ± 2.4  46   88.1 62.2 ± 3.4  118  118.8  84.9 ± 3.8  193
                               1/16  59.1 40.4 ± 2.6 331   93.2 63.7 ± 2.8  709  126.1  86.8 ± 3.8 1 285
                                 10  63.7 29.1 ± 2.9  43   88.0 51.8 ± 3.6   71  113.9  57.3 ± 4.6  101
                 MCMC      N =   50  69.7 41.1 ± 2.6 221  111.0 63.3 ± 3.4  419  149.0  84.8 ± 4.2  682
                                250  70.6 40.4 ± 2.6 1 043 112.1 63.0 ± 3.1 1 864 151.8 86.0 ± 3.9 2 954
                 MC                  51.2 39.2 ± 2.8 1 651 66.6 60.0 ± 3.1 3 715  81.7  83.8 ± 4.3 5 178

 Ring-of-rings topology                     n = 6                n = 12                  n = 18
                                     OV     Reward  Time   OV     Reward   Time   OV      Reward   Time
                                1/4  28.4 40.1 ± 2.7  82   44.1 66.7 ± 2.7  345   59.8  93.1 ± 3.8  861
                 ε-HALP    ε =  1/8  65.4 48.0 ± 2.7 581  107.9 76.1 ± 3.8 2 367 148.8 104.5 ± 3.5 6 377
                               1/16  68.9 47.1 ± 2.8 4 736 113.1 77.6 ± 3.7 22 699 156.9 107.8 ± 3.9 53 600
                                 10  68.5 45.0 ± 2.7  69   99.9 67.4 ± 3.8  121  109.1  39.4 ± 4.1  173
                 MCMC      N =   50  81.1 47.4 ± 2.9 411  131.5 76.2 ± 3.7  780  182.7 104.3 ± 4.1 1 209
                                250  81.9 47.1 ± 2.5 1 732 134.0 78.2 ± 3.6 3 434 185.8 106.7 ± 4.1 5 708
                 MC                  55.6 43.6 ± 2.9 2 100 73.7 74.8 ± 3.9 5 048  92.1 102.0 ± 4.2 6 897
Figure 1: Comparison of HALP solvers on two irrigation-network topologies of varying sizes (n). The solvers are compared by
the objective value of a relaxed HALP (OV), the expected discounted reward of a corresponding policy, and computation time
(in seconds). The expected discounted reward is estimated by the Monte Carlo simulation of 100 trajectories. The ε-HALP and
MCMC solvers are parameterized by the resolution of ε-grid (ε) and the number of iterations (N).

(Figure 1). The goal of an irrigation network operator is to the informative search for violated constraints in the MCMC
select discrete water-routing actions AD to optimize contin- solver. Third, the quality of the MCMC policies (N = 250)
uous water levels XC in multiple interconnected irrigation is close to the ε-HALP ones (ε = 1/16), but does not surpass
channels. The transition model is parameterized by beta dis- them signiﬁcantly. In the irrigation-network problems, the ε-
tributions and represents water ﬂows conditioned on the op- HALP policies (ε = 1/16) are already close to optimal, and
eration modes of regulation devices. The reward function is therefore hard to improve. Even if the MCMC solver reaches
additive and given by a mixture of two normal distributions higher objective values than the ε-HALP solver, the policies
for each channel. The optimal value function is approximated may not improve due to the suggestive relationship between
                                                                             ∗    w
by a linear combination of four univariate piecewise linear our true objective minw kV − V k∞,1/L and the objective
                                                                           w
basis functions for each channel [Guestrin et al., 2004]. The of HALP minw kV ∗ − V k [Guestrin et al., 2004].
                             x                                                1,ψ
state relevance density function ψ( ) is uniform. A compre- Finally, the computation time of the ε-HALP solver is se-
hensive description of the irrigation-network problems can be riously affected by the topologies of tested networks, which
found in Guestrin et al. 2004.                        can be explained as follows. For a small ε and large n, the
  The ε-HALP solver is implemented with the method of time complexity of formulating cost networks grows approx-
Schuurmans and Patrascu 2002 as described in Section 3.2. imately by the rates of (1/ε + 1)2 and (1/ε + 1)3 for the
The Monte Carlo solver uniformly generates one million con- ring and ring-of-rings topologies, respectively. The ε-HALP
straints and establishes a baseline for the comparison to an solver spends a signiﬁcant amount of time by formulating
uninformatively behaving sampling method. The chain of cost networks, which makes its decent time complexity on
the MCMC oracle is simulated for 500 steps from the ini- the ring topology (quadratic in 1/ε + 1) deteriorate on the
tial temperature c = 0.2, which yields a decreasing cooling ring-of-rings topology (cubic in 1/ε + 1). A similar cross-
schedule from T0 = 0.2 to T500 ≈ 0.02. These parameters topology comparison of the MCMC solver shows that its
were chosen empirically to demonstrate the characteristics of computation times differ only by a multiplicative factor of 2.
our approach rather than to maximize the performance of the This difference is due to the increased complexity of sampling
MCMC oracle. All experiments were performed on a Dell    ∗  z
                                                      p(zi | −i), which is caused by more complex local depen-
Precision 340 workstation with 2GHz Pentium 4 CPU and dencies, and not the treewidth of the ring-of-rings topology.
1GB RAM. All linear programs were solved by the simplex
method in the LP SOLVE package. The results of the exper-
iments are reported in Figure 1.                      6   Conclusions
  Based on our results, we draw the following conclusions. Development of scalable algorithms for solving large factored
First, the MCMC solver (N = 250) achieves the highest ob- MDPs is a challenging task. The MCMC approach presented
jective values on all problems. Higher objective values can be in this paper is a small but important step in this direction. In
interpreted as closer approximations to the constraint space in particular, our method overcomes the limitations of existing
HALP since the solvers operate on relaxed versions of HALP. approaches to solving HALP and works directly with con-
Second, the quality of the MCMC policies (N = 250) sur- tinuous variables, generates constraints based on their poten-
passes the Monte Carlo ones while both solvers consume ap- tial to improve the existing solution, and its space complexity
proximately the same computation time. This result is due to is linear in the number of variables. Moreover, the MCMC