                         Faster Heuristic Search Algorithms for Planning 
                                   with Uncertainty and Full Feedback 

                             Blai Bonet                                      Hector Geffner 
                 Computer Science Department                          Departamento de Tccnologfa 
             University of California, Los Angeles                1CREA - Universitat Pompeu Fabra 
                  Los Angeles, CA 90024, USA                            Barcelona 08003, Espafia 
                    bonet@cs.ucla.edu                            hector.geffner@tecn.upf.es 


                        Abstract                               that comprises a loop in which states reachable from s0 and 
                                                               the greedy policy that have Bellman residuals greater than a 
     Recent algorithms like RTDP and LAO* combine 
                                                               given 6 (we call them 'inconsistent' states), are found and up•
     the strength of Heuristic Search (HS) and Dynamic 
                                                               dated until no one is left. FIND-and-REViSE makes explicit 
     Programming (DP) methods by exploiting knowl-
                                                               the basic idea underlying HS/DP algorithms, including RTDP 
     edge of the initial state and an admissible heuris•
                                                               and LAO*. We prove the convergence, complexity, and op-
     tic function for producing optimal policies without 
                                                               timality of FIND-and-REViSE, and introduce the second al•
     evaluating the entire space. In this paper, we in•
                                                               gorithm, HDP, that builds on it, and adds a labeling mech•
     troduce and analyze three new HS/DP algorithms. 
                                                               anism for detecting solved states based on Tarjan's efficient 
     A first general algorithm schema that is a simple 
                                                               strongly-connected components procedure [Tarjan, 1972]. A 
     loop in which 'inconsistent' reachable states (i.e., 
                                                               state is solved when it reaches only 'consistent' states, and 
     with residuals greater than a given c) are found and 
                                                               solved states are skipped in all future searches. HDP termi•
     updated until no such states are found, and serves 
                                                               nates when the initial state is solved. HDP inherits the con•
     to make explicit the basic idea underlying HS/DP 
                                                               vergence and optimality properties of the general FlND-and-
     algorithms, leaving other commitments aside. A 
                                                               REVISE schema and is strongly competitive with existing ap•
     second algorithm, that builds on the first and adds 
                                                               proaches. The third algorithm, HDP(i), is like HDP, except 
     a labeling mechanism for detecting solved states 
                                                               that while HDP computes a value function by enforcing its 
     based on Tarjan's strongly-connected components 
                                                               consistency over all reachable states (i.e., reachable from s
     procedure, which is very competitive with existing                                                                 0 
                                                               and the greedy policy), HDP(I) enforces consistency over the 
     approaches. And a third algorithm, that approx•
                                                               likely' reachable states only. We show that this approxima•
     imates the latter by enforcing the consistency of 
                                                               tion, suitably formalized, can lead to great savings in time and 
     the value function over the likely' reachable states 
                                                               memory, with no much apparent loss in quality, when transi•
     only, and leads to great time and memory savings, 
                                                               tions have probabilities that differ greatly in value. 
     with no much apparent loss in quality, when transi•
     tions have probabilities that differ greatly in value.      Our motivation is twofold: to gain a better understanding 
                                                               of HS/DP methods for planning with uncertainty, and to de•
                                                               velop more effective HS/DP algorithms for both optimal and 
1 Introduction                                                 approximate planning. 
Heuristic search algorithms have been successfully used for 
computing optimal solutions to large deterministic problems    2 Preliminaries 
(e.g., iKorf, 1997]). In the presence of non-determinism and   2.1 Model 
feedback, however, solutions are not action sequences but 
policies, and while such policies can be characterized and     We model non-deterministic planning problems with full 
computed by dynamic programming (DP) methods [Bellman,         feedback with state models that differ from those used in the 
1957; Howard, 1960], DP methods take all states into account   classical setting in two ways: first, state transitions become 
and thus cannot scale up to large problems [Boutilier et al,   probabilistic; second, states are fully observable. The result•
1999]. Recent algorithms like RTDP iBarto et al, 19951 and     ing models are known as Markov Decision Processes (MDPS) 
LAO* [Hansen and Zilberstein, 2001], combine the strength      and more specifically as Stochastic Shortest-Path Problems 
of Heuristic Search and Dynamic Programming methods by         iBertsekas, 1995], and they are given by:1 
exploiting knowledge of the initial state SQ and an admissi•
ble heuristic function (lower bound) h for computing opti•
mal policies without having to evaluate the entire space. In 
this paper we aim to contribute to the theory and practice 
of Heuristic Search/Dynamic Programming methods by for•
mulating and analyzing three new HS/DP algorithms. The 
first algorithm, called FlND-and-REVlSE is a general schema 


SEARCH                                                                                                               1233  M6. positive action costs 0, and                              point equation: 
 M7. fully observable states.                                                                                          (2) 
 Due to the presence of full feedback (M7) and the standard 
 Markovian assumptions, the solution of an MDP takes the       also known as Bellman's equation. For stochastic short•
form of a function mapping states s into actions a A(s).       est path problems like M1-M8 above, the border condition 
 Such a function is called a policy. A policy assigns a prob•  V(s) = 0 is also needed for goal states s G. Value it•
ability to every state trajectory starting in state            eration solves (2) by plugging an initial guess for V* in the 
 so which is given by the product of the transition probabili• right-hand side of (2) and obtaining a new guess on the left-
ties where If we further assume that                           hand side. In the form of VI known as asynchronous value 
actions in goal states have no costs and produce no changes    iteration [Bertsekas, 19951, this operation can be expressed 
                                           the expected cost   as: 
associated with a policy R starting in state SQ is given by the                                                       (3) 
weighted average of the probability of such trajectories times 
their cost An optimal solution is a policy 
                                                               where V is a vector of size |S| initialized arbitrarily (normally 
7r* that has a minimum expected cost for all possible initial 
                                                               to 0) and where the equality in (2) is replaced by assignment. 
states. An optimal solution is guaranteed to exist provided the 
                                                               The use of expression (3) for updating a state value in V is 
following assumption holds [Bertsekas, 19951: 
                                                               called a state update or simply an update. In standard (syn•
M8. the goal is reachable from every state with non-zero       chronous) value iteration, all states are updated in parallel, 
     probability.                                              while in asynchronous value iteration, only a selected subset 
                                                               of states is selected for update at a time. In both cases, it is 
Since the initial state s0 of the system is fixed, there is in 
principle no need to compute a complete policy but a partial   known that if all states are updated infinitely often, the value 
policy prescribing the action to take in the states that can be function V converges eventually to the optimal value func•
                                                               tion. From a practical point of view, value iteration is stopped 
reached following the policy from s0. Traditional dynamic 
programming methods like value or policy iteration compute     when the Bellman error or residual defined as the difference 
complete policies, while recent heuristic search DP methods    between left and right in (2): 
like RTDP and LAO* compute partial policies. They achieve 
this by means of suitable heuristic functions h(s) that provide 
admissible estimates (lower bounds) of the expected cost to 
reach the goal from any state s.                               over all states s is sufficiently small. In the discounted MDP 
                                                               formulation, a bound on the policy loss (the difference be•
2.2 Dynamic Programming                                        tween the expected cost of the policy and the expected cost 
                                                               of the optimal policy) can be obtained as a simple expres•
Any heuristic or value function h defines a greedy policy 7T/t: sion of the discount factor and the maximum residual. In 
                                                               stochastic shortest path models, no similar closed-form bound 
                                                        (1)    is known, although such bound can be computed [Bertsekas, 
                                                               1995]. Thus, one can execute value iteration until the max•
                                                               imum residual becomes smaller than a given then if the 
where the expected cost from the resulting states ,s' is as•
                                                               bound on the policy loss is higher than desired, the same pro•
sumed to be given by We call the greedy action 
                                                               cess can be repeated with a smaller and so on (see 
in s for the value function h. If we denote the optimal (ex•
                                                               [Hansen and Zilberstein, 2001] for a similar idea). For these 
pected) cost from a state s to the goal by it is well 
                                                               reasons, we will take as our basic task below, the computa•
known that the greedy policy 7r/  is optimal when h is the op•
                              t                                tion of a value function V(s) with residuals no greater than a 
timal cost function, i.e. h =                                  given parameter  
   While due to the possible presence of ties in (1), the greedy 
                                                                 One last definition and a few known results before we pro•
policy is not unique, we will assume throughout the paper 
                                                               ceed. We say that cost function V is monotonic iff 
that these ties are broken systematically using an static order•
ing on actions. As a result, every value function V defines a                                                         (4) 
unique greedy policy and the optimal cost function K* 
defines a unique optimal policy We define the relevant         for every s S. Notice that a monotonic value function 
states as the states that are reachable from SQ using this opti• never decreases when updated, and moreover, must increase 
mal policy; they constitute a minimal set of states over which by more than when updated in a state whose residual 
the optimal value function needs to be defined.2               is greater than As in the deterministic setting, a non•
   Value iteration (vi) is a standard dynamic programming      monotonic cost function can be made monotonic by simply 
method for solving MDPs and is based on computing the op•      taking the value V(s) to be the max between V(s) and the 
timal cost function V* and plugging it into the greedy policy  right-hand side of Bellman's equation. The following results 
(1). This optimal cost function is the only solution to the fixed are well known. 
                                                               Theorem 1 a) The optimal values V* (s) of a model MJ-M8 
   2This definition of 'relevant states* is more restricted than the 
one in [Barto et al, 1995] that includes the states reachable from so are non-negative and finite; b) the monotonicity and admis•
by any optimal policy.                                         sibility of a value function are preserved through updates. 


1234                                                                                                            SEARCH    start with a lower bound function 
   repeat 
       FIND a state s in the greedy graph Gv with  
       REVISE V at s 
   until no such state is found 
   return V 

               Algorithm 1: FlND-and-REVlSE 

 3 Find-and-Revise 
 The FlND-and-REVlSE schema is a general asynchronous VI        Figure 1: A graph and its strongly-connected components. 
 algorithm that exploits knowledge of the initial state and an 
 admissible heuristic for computing optimal or nearly optimal 
                                                                 Due to the presence of cycles in the greedy graph, bottom-
 policies without having to evaluate the entire space. Let us 
                                                               up algorithms common in AO* implementations cannot be 
 say that a value function V -consistent (inconsistent) over 
                                                               used. Indeed, if s is reachable (in the greedy graph) from 
 a state ,s when the residual over s is no greater (greater) than f, 
                                                               a descendant s' of s, then bottom-up approaches will be un•
 and that V itself is -consistent when it is -consistent over all 
                                                               able to label either state as solved. A labeling mechanism that 
 the states reachable from s  and the greedy policy Then 
                           0                                   works in the presence of cycles is presented in [Bonet and 
 FlND-and-REVlSE computes an -consistent value function by 
                                                               Geffner, 2003] for improving the convergence of RTDP. Basi•
 simply searching for inconsistent states in the greedy graph 
                                                               cally, after each RTDP trial, an attempt is made to label the last 
 and updating them until no such states are left; see Alg. 1. 
                                                               unsolved state .s in the trial by triggering a systematic search 
   The greedy graph refers to the graph resulting from 
                                                               for inconsistent states from .s. If one such state is found, it 
 the execution of the greedy policy starting in s ; i.e., s  is 
                                                0       0      is updated, and a new trial is executed. Otherwise, the state 
 the single root node in Gv,and for every non-goal state s in 
                                                               s and all its unsolved descendants are labeled as solved, and 
 6Vi its children are the states that may result from executing a new cycle of RTDP trials and labeling checks is triggered. 
 the action in s.                                              Here we take this idea and improve it by removing the need 
   The procedures FIND and REVISE are the two parameters of    of an extra search for label checking. The label checking will 
 the FlND-and-REVlSE procedure. For the convergence, opti-     be done as part of the FIND (DFS) search with almost no over•
 mality, and complexity of FlND-and-REVlSE, we assume that     head, exploiting Tarjan's linear algorithm for detecting the 
 FIND searches the graph systematically, and REVISE of V at    strongly-connected components of a directed graph iTarjan, 
 s updates V at s (and possibly at some other states), both    1972], and a correspondence between the strongly-connected 
 operations taking 0(|S|) time.                                components of the greedy graph and the minimal collections 
 Theorem 2 (Convergence) For a planning model MI-M8            of states that can be labeled at the same time. 
 with an initial value function h that is admissible and         Consider the (directed) greedy graph and the relation 
 monotonia FlND-a/w/-REVlSE yields an value between pairs of states s and that holds when 
function in a number of loop iterations no greater than        or when s is reachable from and is reachable from s 
                          where each iteration has time com-   in Gv. The strongly-connected components of Gv are the 
                                                               equivalence classes defined by this relation and form a parti•
                                                               tion of the set of states in Gv. For example, for the greedy 
 Theorem 3 (Optimality) For a planning model Ml -MS with       graph in Fig. 1, where 2 and 4 are terminal (goal) states, the 
 an initial admissible and monotonia value function, the value components are and 
function computed by FIND-and-REVlSE approaches the op-        C\ — Tarjan's algorithm detects the strongly-
 timal value function over all relevant states as e goes to 0. connected components of a directed graph in time 
                                                               while traversing the graph depth-first, where n stands for the 
 4 Labeling                                                    number of states in Gv) and e for the number of 
                                                               edges. 
 We consider next a particular instance of the general FIND-     The relationship between labeling and strongly-connected 
 and-REViSE schema in which the FIND operation is carried      components in Gv is quite direct. Let us say first that a 
 out by a systematic Depth-First Search that keeps track of    component C is e-consistent when all states s C are 
 the states visited. In addition, we consider a labeling scheme consistent, and that a component C is solved when every state 
 on top of this search that detects, with almost no overhead,  s C is solved. Let's then define as the graph whose 
 when a state is solved, and hence, when it can be skipped     nodes are the components of and whose directed edges 
 in all future searches. A state s is defined as solved when   are when some state in is reachable from some 
 the value function V is e-consistent over s and over all states state in C. Clearly, is an acyclic graph as two compo•
 reachable from s and the greedy policy Clearly, when this     nents which are reachable from each other will be collapsed 
 condition holds no further updates are needed in s or the states into the same equivalence class. In addition, 
 reachable from 5. The resulting algorithm terminates when 
 the initial state s0 is solved and hence when an e-consistent   1. a state s is solved iff its component C is solved, and 
 value function has been obtained.                                  furthermore, 


 SEARCH                                                                                                             1235   2. a component C is solved iff C is consistent and all com•
     ponents are solved. 
The problem of labeling states in the cyclic graph Gy can 
thus be mapped into the problem of labeling the components 
in the acyclic graph Gy, which can be done in bottom up 
fashion. 
  From Fig. 1 is easy to visualize the component graph asso•
ciated to the greedy graph. Thus, if 2 is the only inconsistent 
state, for example, we can label the components C\ and Ci 
as solved, while leaving C3 and C4 unsolved. 
  The code that simultaneously checks in depth-first fashion 
the consistency of the states and the possibility of labeling 
them is shown in Alg. 2. We call the resulting algorithm, 
HDP. HDP inherits its convergence and optimality properties 
from the FiND-and-REViSE schema and the correctness of the 
labeling mechanism. 
  We do not have space to explain HDP code in detail, yet it 
should be clear to those familiar with Tarjan's algorithm; in 
particular, the use of the state visit number, S.IDX, and the 
'low-link' number, s.LOW, for detecting when a new compo•
nent has been found. The flag flag and the (normal) propa•
gation of the visit numbers prevent a component from being 
labeled as solved when it is inconsistent or can reach an in•
consistent component. 
Theorem 4 (Correctness) The value function computed by 
HDP for a planning model M1-M8, given an initial admissible 
and monotonic value function, is t-consistent. 

5 Experimental Results 
We now evaluate the performance of HDP in comparison with 
other recent Heuristic Search/DP algorithms such as the sec•
ond code for LAO* in [Hansen and Zilberstein, 2001], that we 
call Improved LAO* (ILAO*), and Labeled RTDP (LRTDP), a 
recent improvement of RTDP that accelerates its convergence 
[Bonet and Geffner, 2003]. We use parallel Value Iteration as 
the baseline. We've implemented all these algorithms in C++ 
and the experiments have been run on a Sun Fire-280R with 
750 MHz and 1Gb of RAM. 
  The domain that we use for the experiments is the racetrack 
from [Barto et al, 1995]. The states are tuples               of Table 1, including number of states, optimal cost, and per•
that represent the position and speed of the car in the x, y  centage of states that are relevant. 
dimensions. The actions are pairs a — (ax, ay) of instan•
                                                                As heuristic, we follow [Bonet and Geffner, 2003], and use 
taneous accelerations where Uncer•
                                                              the domain independent admissible and monotonic heuristic 
tainty in this domain comes from assuming that the road is 
                                                              hmini obtained by replacing the expected cost in Bellman 
'slippery' and as a result, the car may fail to accelerate or 
                                                              equation by the best possible cost. The total time spent com•
desaccelerate. More precisely, an action a = (ax, ay) has 
                                                              puting heuristic values is roughly the same for the different 
its intended effect with probability 1 - p, while with prob•
                                                              algorithms (except VI), and is shown separately in the fifth 
ability p the action effects correspond to those of the action 
                                                              row in the table, along with its value for s . The experiments 
   = (0,0). Also, when the car hits a wall, its velocity is set                                       0
                                                              are carried with three heuristics: 
to zero and its position is left intact (this is different than in 
                                                              and h = 0. 
[Barto et al., 1995] where for some reason the car is moved 
to the start position).                                         The results are shown in Table 1. HDP dominates the other 
  We consider the track large-b from [Barto et al, 1995],     algorithms over all the instances for while LRTDP 
                                                              is best (with one or two exceptions) when the weaker heuris•
h- track from [Hansen and Zilberstein, 2001],3 and five 
other tracks (squares and rings of different size). Informa•  tics and 0 are used. Thus, while HDP seems best for 
tion about these instances can be found in the first three rows exploiting good heuristic information over these instances, 
                                                              LRTDP bootstraps more quickly (i.e., it quickly computes a 

   3TakeT, n from the source code of LAO*.                    good value function). We hope to understand the reasons for 


1236                                                                                                           SEARCH Table 1: Problem data and convergence time in seconds for the different algorithms with different heuristics. Results for 
e — 10 -3 and probability p = 0.2. Faster times are shown in bold. 

these differences in the future.                              from i = 0, as the code for HDP(O) corresponds exactly to the 
                                                              code for HDP, except that the possible successors of a state a 
6 Approximation                                               in the greedy graph are replaced by the plausible successors. 
                                                                 HDP(i) computes lower bounds that tend to be quite tight 
HDP, like FIND-and-REViSE, computes a value function V by     over the states that can be reached with plausibility no smaller 
enforcing its consistency over the states reachable from So   than i. At run time, however, executions may contain 'sur•
and the greedy policy ny. The final variation we consider,    prising* outcomes, taking the system 'out' of this envelope, 
that we call HDP(I), works in the same way, yet it enforces   into states where the quality of the value function and its cor•
the consistency of the value function V only over the states  responding policy, are poor. To deal with those situations, 
that are reachable from so and the greedy policy with some    we define a version of HDP(i), called HDP(t,j), that inter•
minimum likelihood.                                           leaves planning and execution as follows. HDP(i, j) plans 
  For efficiency, we formalize this notion of likelihood, using from s = so by means of the HDP(i) algorithm, then exe•
a non-negative integer scale, where 0 refers to a normal out• cutes this policy until a state trajectory with plausibility mea•
come, 1 refers to a somewhat surprising outcome, 2 to a still sure greater than or equal to j, and leading to a (non-goal) 
more surprising outcome, and so on. We call these measures    state s' is obtained. At that point, the algorithm replans from 
plausibilities, although it should be kept in mind, that 0 refers s' with HDP(i), and the same execution and replanning cycle 
to the most plausible outcomes, thus 'plausibility greater than is followed until reaching the goal. Clearly, for sufficiently 
f, means 'a plausibility measure smaller than or equal to i.y large j, HDP(i,j) reduces to HDP(i), and for large i, HDP(i) 
  We obtain the transition plausibilities Ka(s'\s) from the   reduces to HDP. 
corresponding transition probabilities by the following dis•     Table 2 shows the average cost for HDP(i,j) for i = 0 
cretization:                                                  (i.e., most plausible transitions considered only), and several 
                                                              values for j (replanning thresholds). Each entry in the ta•
                                                       (5)    ble correspond to an average over 100 independent execu•
                                                              tions. We also include the average cost for the greedy policy 
with when Plausibilities are                                  with respect to as a bottom-line reference for the fig•
thus formalized': the most plausible next states have always  ures. Memory in the table refers to the number of evaluated 
plausibility 0. These transition plausibilities are then com• states. As these results show, there is a smooth tradeoff be•
bined by the rules of the K calculus iSpohn, 1988] which is   tween quality (average cost to the goal) and time (spent in 
a calculus isomorphic to the probability calculus (e.g. [Gold- initial planning and posterior replannings) as the parameter 
szmidt and Pearl, 1996]). The plausibility of a state trajectory j vary. We also see that in this class of problems the hmtn 
given the initial state, is given by the sum of the transition heuristic delivers a very good greedy policy. Thus, further 
plausibilities in the trajectory, and the plausibility of reach• research is necessary to assess the goodness of HD?(i,j) and 
ing a state, is given by the plausibility of the most plausible the hmin heuristic. 
trajectory reaching the state. 
  The HDP(i) algorithm, for a non-negative integer z, com•    7 Related Work 
putes a value function V by enforcing its consistency over 
the states reachable from So with plausibility greater than   We have built on IBarto et al., 1995] and [Bertsekas, 1995], 
or equal to i. HDP(i) produces approximate policies fast by   and more recently on [Hansen and Zilberstein, 2001] and 
pruning certain paths in the search. The simplest case results [Bonet and Geffner, 2003]. The crucial difference between 


SEARCH                                                                                                             1237 