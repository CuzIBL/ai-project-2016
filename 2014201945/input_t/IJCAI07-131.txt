        State Similarity Based Approach for Improving Performance in RL                        ∗

                       Sertan Girgin1,2  and  Faruk Polat1   and  Reda Alhajj2,3
  Middle East Technical University1          University of Calgary2             Global University3
   Dept. of Computer Engineering          Dept. of Computer Science        Dept. of Computer Science
   {sertan,polat}@ceng.metu.edu.tr     {girgins,alhajj}@cpsc.ucalgary.ca         rhajj@gu.edu.lb


                    Abstract                          ically either by ﬁnding sub-goal states and generating corre-
                                                      sponding options that solve them [Stolle and Precup, 2002;
    This paper employs state similarity to improve rein- Menache et al., 2002; Mannor et al., 2004; Simsek et al.,
    forcement learning performance. This is achieved  2005] or by identifying frequently occurring patterns over
    by ﬁrst identifying states with similar sub-policies. the state, action and reward trajectories [McGovern, 2002;
    Then, a tree is constructed to be used for locat- Girgin et al., 2006]. Since different instances of the same
    ing common action sequences of states as derived  or similar subtasks would probably have different subgoals
    from possible optimal policies. Such sequences are in terms of state representation, with subgoal based meth-
    utilized for deﬁning a similarity function between ods they will be discovered and treated separately. Although
    states, which is essential for reﬂecting updates on sequence based methods are capable of combining different
    the action-value function of a state onto all similar instances of the same subtask, multiple abstractions may be
    states. As a result, the experience acquired during generated for each of them.
    learning can be applied to a broader context. Effec-
                                                        TAA based methods try to solve the solution sharing prob-
    tiveness of the method is demonstrated empirically.
                                                      lem inductively. Based on the fact that states with similar pat-
                                                      terns of behavior constitute the regions of state space corre-
1  Introduction                                       sponding to different instances of similar subtasks, the notion
                                                      of state equivalence can be used as a more direct mechanism
Reinforcement learning (RL) is the problem faced by an agent of solution sharing. By reﬂecting experience acquired on one
that must learn behavior through trial-and-error interactions state to all similar states, connections between similar sub-
with a dynamic environment, by gaining percepts and rewards tasks can be established implicitly. In fact, this reduces the
from the world and taking actions to affect it. In most of the repetition in learning, and consequently improves the perfor-
realistic and complex domains, the task that an agent tries mance. State equivalence is closely related with model min-
to solve is composed of various subtasks and has a hierar- imization in Markov Decision Processes (MDPs), and var-
                                             [
chical structure formed by the relations between them Barto ious deﬁnitions exist. [Givan et al., 2003] deﬁne equiva-
                  ]
and Mahadevan, 2003 . Each of these subtasks repeats many lence of states based upon stochastic bisimilarity, such that
times at different regions of the state space. Although all in- two states are said to be equivalent if they are both action
stances of the same subtask, or similar subtasks, have almost sequence and optimal value equivalent. Based on the no-
identical solutions, without any (self) guidance an agent has tion of MDP homomorphism, [Ravindran and Barto, 2001;
to learn these solutions independently by going through sim- 2002] extended equivalence over state-action pairs, which as
ilar learning stages again and again. This situation affects the a result allows reductions and relations not possible in case of
learning process in a negative way, making it difﬁcult to con- bisimilarity. Furthermore, they applied state-(action) equiva-
verge to optimal behavior in reasonable time.         lence to the options framework to derive more compact op-
  The main reason of this problem is the lack of connec- tions without redundancies, called relativized options. While
tions, that would allow to share solutions, between simi- relativized options are deﬁned by the user, instances are gen-
lar subtasks scattered throughout the state space. One pos- erated automatically. [Zinkevich and Balch, 2001] also ad-
sible way to build connections is to use temporally ab- dressed how symmetries in MDPs can be used to acceler-
stract actions (TAAs), which generalize primitive actions ate learning by employing equivalence relations on the state-
and last for a period of time [Sutton et al., 1999; Diet- action pairs; in particular for the multi-agent case, where
terich, 2000; Barto and Mahadevan, 2003]. TAAs can be permutations of features corresponding to various agents are
included in the problem deﬁnition, which requires extensive prominent, but without explicitly formalizing them.
domain knowledge and becomes more difﬁcult as the com-
plexity of the problem increases, or be constructed automat- In this paper, following homomorphism notion, we pro-
                                                      pose a method to identify states with similar sub-policies
  ∗This work was supported by the Scientiﬁc and Technological without requiring a model of the MDP or equivalence rela-
Research Council of Turkey under Grant No. 105E181(HD-7). tions, and show how they can be integrated into RL frame-

                                                IJCAI-07
                                                   817work to improve the learning performance. Using the col-
                                                       s1      s2      s3      s4     s1      s2     sΥ
lected history of states, actions and rewards, traces of policy a1/1 a2/0 a1/2           a1/1    a2/0
fragments are generated and then translated into a tree form
to efﬁciently identify states with similar sub-policy behavior a /0           a /0   a /0           a  /c
based on the number of common action-reward sequences. 1                       1      1              Υ
Updates on the action-value function of a state are then re-       (a)                       (b)
ﬂected to all similar states, expanding the inﬂuence of new
experiences. The proposed method can be treated as a meta- Figure 1: (a) Markov Decision Process M, and (b) its homo-
heuristic to guide any underlying RL algorithm. We demon- morphic image M . Directed edges indicate state transitions.
strate the effectiveness of this approach by reporting test re- Each edge label denotes the action causing the transition be-
sults on two domains, namely various versions of the taxi tween connected states and the associated expected reward.
cab and the room doorway problems. Further, the proposed
method is compared with other RL algorithms, and a substan-                                       
tial level of improvement is observed on different test cases. S, A, Ψ,T,R and M = S ∪{sΥ},A ∪{aΥ}, Ψ ,T ,R
                                                                          s
Also, we present how the performance of out work compares be two MDPs, such that Υ is an absorbing state with a single
                                                      admissible action aΥ which, once executed, transitions back
with hierarchical RL algorithms, although the approaches are                                
different. The tests demonstrate the applicability and effec- to sΥ with certainty. A surjection h :Ψ→ Ψ ∪{(sΥ,aΥ)}
                                                                                                M    M  
tiveness of the proposed approach.                    is a partial MDP homomorphism (pMDPH) from   to   ,
  The rest of this paper is organized as follows: In Section 2, if for all states that do not map to sΥ, state-action pairs that
we brieﬂy describe the standard RL framework of discrete have the same image under h also have the same block tran-
                                                                      M                            2
time, ﬁnite MDPs, deﬁne state equivalence based on MDP sition behavior in and the same expected reward .Un-
                                                                                                      3
homomorphisms, and show how they can be used to improve der h, M is called the partial homomorphic image of M .
the learning process. Our approach to how similar states can Two state-action pairs (s1,a1) and (s2,a2) in M are said to
be identiﬁed during the learning process is presented in Sec- be equivalent if h(s1,a1)=h(s2,a2); states s1 and s2 are
                                                                                    ρ : A  →  A
tion 3. Experimental results are reported and discussed in equivalent if there exists a bijection s1 s2 such that
                                                      ∀a ∈  A   h(s ,a)=h(s   ,ρ(a))
Section 4. Section 5 is conclusions.                         s1 ,  1          2     . The set of state-action
                                                      pairs equivalent to (s, a), and the set of states equivalent to s
2  Background and Motivation                          are called the equivalence classes of (s, a) and s, respectively.
                                                      [Ravindran and Barto, 2001] proved that if h above is a com-
In this section, we brieﬂy overview the background necessary plete homomorphism, then an optimal policy π∗ for M can
to understand the material introduced in this paper. We start be constructed from an optimal policy for M 4. This makes it
by deﬁning MDP and related RL problem.                possible to solve an MDP by solving one of its homomorphic
                  S, A, Ψ,T,R       S
  An MDP is a tuple            ,where   is a ﬁnite set images which can be structurally simpler and easier to solve.
       A                     Ψ ⊆ S×A
of states, is a ﬁnite set of actions, is the set of ad- However, in general such a construction is not viable in case
                      T :Ψ×   S → [0, 1]
missible state-action pairs,          is a state tran- of pMDPHs, since the optimal policies would depend on the
                    ∀(s, a) ∈ Ψ,     T (s, a, s)=1
sition function such that        s∈S             ,   rewards associated with absorbing states.
and R :Ψ→ is a reward function.  R(s, a) is the imme-   For example, consider the deterministic MDP M given in
diate expected reward received when action a is executed in Fig. 1(a). If the discount factor, γ, is set as 0.9, then optimal
    s 1                   π :Ψ→   [0, 1]                                             
state . A (stationary) policy,        , is a mapping  policy at s2 is to select action a2; M given in Fig. 1(b) is a
that deﬁnes the probability of selecting an action in a par- partial homomorphic image of M 5 and one can easily deter-
                            s           π  V π(s)               ∗                ∗
ticular state. The value of a state under policy , ,is mine that Q (s2,a1)=1and Q (s2,a2)=9∗  c,wherec is
the expected inﬁnite discounted sum of rewards that the agent the expected reward for executing action aΥ at the absorbing
                       s           π                                                       
will gain if it starts in state and follows . In particular, if state. Accordingly, the optimal policy for M at s2 is to select
                  a        s               π
the agent takes action at state andthenfollows ,there- a1 if c<1/9,anda2 otherwise, which is different than the
sulting sum is called the value of the state-action pair (s, a) optimal policy for M. This example demonstrates that unless
and denoted Qπ(s, a). The objective of an agent is to ﬁnd an
              ∗                                       reward functions are chosen carefully, it may not be possible
optimal policy, π , which maximizes the state value function to solve a given MDP by employing the solutions of its par-
for all states. Based on the experience in the form of sam- tial homomorphic images. However, if equivalence classes of
ple sequences of states, actions, and rewards collected from state-action pairs and states are known, they can be used to
on-line or simulated trial-and-error interactions with the en- speed up the learning process. Let (s, a) and (s,a) be two
vironment, RL methods try to ﬁnd a solution to an MDP by equivalent state-action pairs in a given MDP M based on the
approximating optimal value functions. Detailed discussions partial homomorphic image M  of M under the surjection h.
of various approaches can be found in [Kaelbling et al., 1996;
                                                         2                                   
Sutton and Barto, 1998; Barto and Mahadevan, 2003].      h((s, a)) = (f(s),gs(a)),wheref : S → S ∪ Υ is a sur-
                                                                           
  RL problems, in general, inherently contain subtasks that jection and {gs : As → Af(s)|s ∈ S} is a set of state dependent
need to be solved by the agent. These subtasks can be repre- surjections.
                                                         3
sented by simpler MDPs that locally preserve the state tran- For a formal deﬁnition see [Ravindran and Barto, 2001; 2002]
                                                         4∀ ∈  −1( ) ∗(   )=  ∗ ( ( ) ) | −1( )|
sition and reward dynamics of the original MDP. Let M =    a  gs  a , π s, a  πM f s ,a / gs a
                                                         5
                                                         Under h deﬁned as f(s1)=s1,f(s2)=s2,f(s3)=f(s4)=
  1                 (  )                                       (  )=       ( )=          (  )=
   s, a is used instead of s, a , wherever possible.  sΥ, g{s1,s2} a1 a1,gs2 a2  a2,g{s3,s4} a1 aΥ

                                                IJCAI-07
                                                   818Suppose that while learning the optimal policy, the agent se- ity between any two states based on the number of common
lects action a at state s which takes it to state t with an imme- action-reward sequences.
                                                                                 
diate reward r, such that t is mapped to a non-absorbing state Given any two states s and s in S,letΠs,i be the set of
                                                                                            
under h.LetΩ be the set of states in M which are accessible π-histories of state s with length i,andςi(s, s ) calculated as
                     
from s by taking action a , i.e. probability of transition is   
                                                                  i              
                                       M    t                         |{σ ∈ Π  |∃σ ∈ Π  ,AR   = AR   }|
non-zero, and are mapped to the same state in as under            j=1       s,j       s ,j  σ      σ
h                   t ∈ Ω                             ςi(s, s )=              
 . By deﬁnition, for any  , the probability of experienc-                        i  |Π   |
ing a transition from s to t upon taking action a is equal to                 j=1   s,j
that of from s to t upon taking action a. Also, since (s, a) and
(s,a)                   R(s, a)=R(s,a)            be the ratio of the number of common action-reward se-
      are equivalent, we have             , i.e., both quences of π-histories of s and s with length up to i to the
state-action pairs have the same expected reward, and since number of action-reward sequences of π-histories of s with
t is mapped to a non-absorbing state, independent of the re-                               
                                                      length up to i. ςi(s, s ) will be close to 1, if s is similar to s
ward assigned to the absorbing state, they have the same pol- in terms of state transition and reward behavior; and close to
icy. Therefore, for any state t ∈ Ω, o = s,a,r,t can
                                     Q(s,a)         0, in case they differ considerably from each other. Note that,
be regarded as a virtual experience tuple and can be  even for equivalent states, the action-reward sequences will
updated similar to Q(s, a) based on observation o, i.e., pre-
                 a                            s     eventually deviate and follow different courses as the subtask
tending as if action transitioned the agent from state to that they are part of ends. As a result, for i larger than some
state t with an immediate reward r.
                                                      threshold value, ςi would inevitably decrease and no longer
  The method described above assumes that equivalence be a permissible measure of the state similarity. On the con-
classes of states and corresponding pMDPHs are already
                                                      trary, for very small values of i, such as 1 or 2, ςi may over
known. If such information is not available prior to learn- estimate the amount of similarity since number of common
ing, then, for a restricted class of pMDPHs, it is still possible action-reward sequences can be high for short action-reward
to identify states that are similar to each other with respect to sequences. Also, since optimal value of i depends on the
state transition and reward behavior based on the history of subtasks of the problem, to increase robustness, it is neces-
events, as we show in the next section. Experience gathered sary to take into account action-reward sequences of various
on one state, then, can be reﬂected to similar states to improve lengths. Therefore, the maximum value or weighted average
the performance of learning.                                  
                                                      of ςi(s, s ) over a range of problem speciﬁc i values, kmin
                                                      and kmax, can be used to combine the results of evaluations
3  Finding Similar States                             and approximately measure the degree of similarity between
For the rest of the paper, we will restrict our attention to states s and s, denoted by ς(s, s).Onceς(s, s) is calcu-
direct pMDPHs in which, except the absorbing state and lated, s is regarded as equivalent to s if ς(s, s) is over a
its associated action, the action sets of an MDP and its given threshold value τsimilarity . Likewise, by restricting the
homomorphic image are the same and there is an iden-  set of π-histories to those that start with a given action a in the
tity mapping on the action component of state-action tu- calculation of ς(s, s), similarity between state-action pairs
ples6.LetM   =  S, A, Ψ,T,R be a given MDP. Start-  (s, a) and (s,a) can be measured approximately.
ing from state s, a sequence of states, actions and rewards If the set of π-histories for all states are available in ad-
σ =  s1,a2,r2,...,rn−1,sn, such that s1 = s and each  vance, then equivalent states can be identiﬁed prior to learn-
ai, 2 ≤   i ≤  n  − 1, is chosen by following a pol-  ing. However, in general, the dynamics of the system is not
icy π is called a π-history of s with length n. ARσ = known in advance and consequently such information is not
a1r1a2 ...an−1rn−1 is the action-reward sequence of σ, accessible. Therefore, using the history of observed events
and the restriction of σ to Σ ⊆ S, denoted by σΣ,isthe (i.e. sequence of states, actions taken and rewards received),
longest preﬁx s1,a2,r2,...,ri−1,si of σ, such that for all the agent must incrementally store the π-histories of length
j =1..i, sj ∈ Σ and si+1 ∈/ Σ. If two states of M are up to kmax during learning and enumerate common action-
equivalent under a direct pMDPH h, then the set of images of reward sequences of states in order to calculate similarities
π-histories restricted to the states that map to non-absorbing between them. For this purpose, we propose an auxiliary
states under h, and consequently, the list of associated action- structure called path tree, which stores the preﬁxes of action-
reward sequences are the same 7. This property of equiva- reward sequences of π-histories for a given set of states. A
lent states leads to a natural approach to calculate the similar- path tree P = N,E is a labeled rooted tree, where N is the

  6                                                   set of nodes, such that each node represents a unique action-
   gs(a)=a,foreverys that maps to a non-absorbing state.
                                                    reward sequence; and e =(u, v, a, r) ∈ E is an edge
  7Let (s, a) and (s ,a) be two state-action pairs that are equiv-
                                                     from u to v with label a, r, indicating that action-reward
alent to each other based on partial homomorphic image M =
                                                  sequence v is obtained by appending a, r to u, i.e., v = uar.
S ∪{sΥ},A  ∪{aΥ}, Ψ ,T ,R  of M = S, A, Ψ,T,R un-
der direct pMDPH h. Consider a π-history of state s of length The root node represents the empty action sequence. Fur-
                                                                       u             s, ξ∈S ×R
n,andletΣh  ⊆  S be the inverse image of S under h,and thermore, each node holds a list of         tuples,
                                                                     s               π
σΣh = s1,a2,r2,...,rk−1,sk,k ≤ n be the restriction of σ on stating that state has one or more -histories starting with
Σh. The image of σΣh under h, denoted by F (σΣh ), is obtained
                                                                                                   Σ
by mapping each state si to its counterpart in S , i.e., F (σΣh )= exists a π-history σ of s such that the image of its restriction to h
                                                                      (   )      (  )=   (   )
h(s1),a2,r2,h(s2),a3,...,rk−1,h(sk). By deﬁnition, F (σΣh ) is under h is equal to F σΣh , i.e., F σΣh F σΣh ; and further-
                                 
                                                                =
a π-history of h(s) in M , and since s and s are equivalent, there more ARσ ARσΣ .
                                                              Σh        h

                                                IJCAI-07
                                                   819Algorithm 1 Q-learning with equivalent state update.  ery s, s pair that co-exist in the tuples list of a node at that
                                                                                      
 1: Initialize Q arbitrarily (e.g., Q(·, ·)=0)        level, ς(s, s ) is compared with K(s, s )/κ(s) and updated
 2: Let T be the path tree initially containing root node only. if the latter one is greater. Note that, since eligibility values
 3: repeat                                            stored in the nodes of the path tree is a measure of occurrence
 4:   Let s be the current state                      frequencies of corresponding sequences, it is possible to ex-
 5:   repeat                            for each step tend the similarity function by incorporating eligibility val-
 6:      Choose and execute a from s using policy derived from
                                                     ues in the calculations as normalization factors. This would
   Q with sufﬁcient exploration. Observe r and the next state s ;
                                                     improve the quality of the metric and also result in better dis-
   append s, a, r, s  to observation history.
                                                    crimination in domains with high degree of non-determinism,
 7:      ΔQ(s, a)=α(r + γ maxa∈A  Q(s ,a) − Q(s, a))
                                s                     since the likelihood of the trajectories will also be taken into
 8:      for all (t, at) similar to (s, a) do
 9:        Let t be a state similar to s.           consideration. In order to simplify the discussion, we opted
                                                     to omit this extension. Once ς values are calculated, equiv-
10:        ΔQ(t, at)=α(r   +  γ maxa  ∈A  Q(t ,at ) −
                                    t  t                                           ς           τ
   Q(t, at))                                          alent states, i.e., state pairs with greater than similarity ,
11:      end for                                      can be identiﬁed and incorporated into the learning process.
12:      s = s                                       The proposed method applied to Q-learning algorithm is pre-
13:   until s is a terminal state                     sented in Alg. 1.
14:   Using the observation history, generate the set of π-histories
   of length up to kmax and add them T .
15:   Traverse T and update eligibility values of the tuples in each 4 Experiments
   node; prune tuples with eligibility value less than ξthreshold.
                                                     We applied the similar state update method described in Sec-
16:   Calculate ς and determine similar states s, s such that tion 3 to Q-learning and compared its performance with dif-
    (  )
   ς s, s >τsimilarity.                               ferent RL algorithms on two test domains: a six-room maze8
17:   Clear observation history.                      and various versions of the taxi problem9 (Fig. 2(a)). Also, its
18: until a termination condition holds
                                                      behavior is examined under various parameter settings, such
                                                      as maximum length of π-histories and eligibility decay rate.
                                                      In all test cases, the initial Q-values are set to 0, and 
-greedy
action sequence σu. ξ is the eligibility value of σu for state s,
                                                      action selection mechanism, where action with maximum Q-
representing its occurrence frequency. It is incremented every                    1 − 

time a new π-history for state s starting with action sequence value is selected with probability and a random action
                                                      is selected otherwise, is used with 
 =0.1. The results are
σu is added to the path tree, and gradually decremented oth-
                                                      averaged over 50 runs. Unless stated otherwise, the path tree
erwise. A π-history h = s1a2r2 ...rk−1sk can be added to a
                                                      is updated using an eligibility decay rate of ξdecay =0.95
path tree by starting from the root node, following edges ac-                   ξ        =0.1
cording to their label. Let nˆ denote the active node of the path and an eligibility threshold of threshold ,andinsim-
                                 i =1..k − 1          ilarity calculations the following parameters are employed:
tree, which is initially the root node. For ,ifthere  k    =3  k    =5      τ        =0.8
is a node n such that nˆ is connected to n by an edge with label min , max ,and similarity .
                                                        Fig. 2(b) shows the total reward obtained by the agent un-
ai,ri, then either ξ of the tuple s1,ξ in n is incremented
                                                      til goal position is reached in six-room maze problem when
or a new tuple s1, 1 is added to n if it does not exist, and nˆ
                                                      equivalent state update is applied to Q-learning and Sarsa(λ)
is set to n. Otherwise, a new node containing tuple s1, 1 is
created, and nˆ is connected to this node by an edge with label algorithms. Based on initial testing, we used a learning rate
                                                      and discount factor of α =0.125,andγ =0.90, respectively.
ai,ri. The new node becomes the active node.
                                                      For the Sarsa(λ) algorithm, λ is taken as 0.98. State simi-
  After each episode, or between two termination conditions
                                                      larities are calculated after each episode. As expected, due to
such as reaching reward peaks in case of non-episodic tasks,
                                                      backward reﬂection of received rewards, Sarsa(λ)converges
using the consecutive fragments of the observed sequence of
                                                      much faster compared to Q-learning. The learning curve of
states, actions and rewards, a set of π-histories of length up
  k
to max are generated and added to the path tree using the 8The agent’s task is to move from a randomly chosen position in
procedure described above. Then, the eligibility values of the top left room to the goal location in the bottom right room us-
tuples in the nodes of the tree are decremented by a factor ing primitive actions that move the agent to one of four neighboring
of 0 <ξdecay < 1, called eligibility decay rate, and tuples cells. Each action is non-deterministic and succeeds with probabil-
with eligibility value less than a given threshold, ξthreshold, ity 0.9, or moves the agent perpendicular to the desired direction
are removed to keep the tree of manageable size and focus with probability 0.1. The agent only receives a reward of 1 when it
the search on recent and frequently used action-reward se- reaches the goal location and a negative reward of −0.01 otherwise.
quences. Using the generated path tree, ς(s, s) can be cal- 9The agent tries to transport a passenger between predeﬁned lo-
culated incrementally for all s, s ∈ S by traversing it in cations on a n × n grid world using six primitive actions: move
breadth-ﬁrst order and keeping two arrays κ(s) and K(s, s). to one of four adjacent squares, pickup or dropoff the passenger.
κ(s) denotes the number of nodes containing s,andK(s, s) Should a move action cause the agent hit to wall/obstacle, the posi-
                                                     tion of the agent will not change. The movement actions are non-
denotes the number of nodes containing both s and s in their            0 2
                 ς(s, s) K(s, s)  κ(s)              deterministic and with . probability agent may move perpendic-
tuple lists. Initially ,       ,and     are set to 0. ular to the desired direction. An episode ends when the passenger
At each level of the tree, the tuple lists of nodes at that level                                    +20
                                                     is successfully transported and the agent receives a reward of .
are processed, and κ(s) and K(s, s ) are incremented accord- There is an immediate negative reward of −10 if pickup or drop-off
ingly. After processing level i, kmin ≤ i ≤ kmax,forev- actions are executed incorrectly, and −1 for any other action.

                                                IJCAI-07
                                                   820                                        5                                  0
                                        0                                -100
                                       -5
                                                                         -200
                                      -10
                                                                         -300
                                      -15
                     4 AB-20                                             -400

                                    reward                  q          reward                  q
                     3                -25                                -500
                                                      sarsa(0.98)        -600            sarsa(0.9)
                     2                -30              q w/ equ.                           smdp-q
                                      -35       sarsa(0.98) w/ equ.      -700             q w/ equ.
                     1                -40                                -800
                     0 CD                0  100  200  300  400  500  600  700  0  200  400  600  800  1000
                       01234                        episode                            episode
                 (a)                                 (b)                                (c)

      0                                 0                                  0
   -1000                              -100                               -100
   -2000                              -200                               -200
   -3000                              -300                               -300
   -4000                              -400                               -400

 reward -5000                       reward -500         q              reward -500             q
                                                     er (q)
   -6000                 q            -600                               -600                0.95
                  sarsa (0.90)                    er (sarsa)                                 0.75
   -7000            q w/ equ.         -700        q w/ equ.              -700                0.50
   -8000                              -800                               -800
        0    500   1000   1500  2000      0   200   400  600  800  1000      0   200   400  600  800  1000
                  episode                           episode                            episode
                 (d)                                 (e)                                (f)

    6000                                0                                 30000
                       0.95                                                                   3,4
    5000               0.75           -100                                25000               3,5
                                      -200
    4000               0.50                                               20000               3,6
                                      -300                                                    3,7
    3000                              -400                                15000

                                    reward -500            3,4
    2000                                                   3,5            10000

 avg.  tree size                      -600                             avg.  tree size
    1000                                                   3,6             5000
                                      -700                 3,7
      0                               -800                                   0
        0   200  400  600  800  1000      0   200   400  600  800  1000        0   200  400  600  800  1000
                  episode                           episode                             episode
                 (g)                                 (h)                                (i)

Figure 2: (a) Six-room maze and 5 × 5 Taxi problems, (b) Results for the six-room maze problem; Results for the (c) 5 × 5
and (d) 12 × 12 taxi problems; (e) Experience replay with the same number of state updates; Effect of ξdecay and kmin,max on
5 × 5 taxi problem: (f,h) Reward obtained, and (h,i) average size of the path tree.

Q-learning with equivalent state update indicates that, start- and symmetries more effectively Q-learning with equivalent
ing from early states of learning, the proposed method can state update performs better in the long run. The learning
effectively utilize state similarities and improve the perfor- curves of algorithms with equivalent state update reveals that
mance considerably. Since convergence is attained in less the proposed method is successful in identifying similar states
than 20 episodes, the result obtained using Sarsa(λ) with which leads to an early improvement in performance, which
equivalent state update is almost indistinguishable from that allows the agent to learn the task more efﬁciently. The results
of Q-learning with equivalent state update.           for the larger 12 × 12 taxi problem presented in Fig. 2(d)
                                                      demonstrate that the improvement becomes more evident as
  The results of the corresponding experiments in 5 × 5 taxi
                                                      the complexity of the problem increases.
problem showing the total reward obtained by the agent is
presented in Fig. 2(c). The initial position of the taxi agent, The proposed idea of using state similarities and then up-
location of the passenger and its destination are selected ran- dating similar states leads to more state-action value, i.e., Q-
domly with uniform probability. α is set to 0.05,andλ is value, updates per experience. It is known that remember-
taken as 0.9 in Sarsa(λ) algorithm. State similarities are com- ing past experiences and reprocessing them as if the agent re-
puted every 5 episodes starting from the 20th episode in order peatedly experienced what it has experienced before, which
to let the agent gain experience for the initial path tree. Simi- is called experience replay [Lin, 1992], speed up the learn-
lar to the six-room maze problem, Sarsa(λ) learns faster than ing process by accelerating the propagation of rewards. Ex-
regular Q-learning. In SMDP Q-learning [Bradtke and Duff, perience replay also results in more updates per experience.
1994], in addition to primitive actions, the agent can select In order to test whether the gain of the equivalent state up-
and execute hand-coded options, which move the agent from date method in terms of learning speed is simply due to the
any position to one of predeﬁned locations using minimum fact that more Q-value updates are made or not, we compared
number of steps. Although SMDP Q-learning has a very steep its performance to experience replay using the same number
learning curve in the initial stages, by utilizing abstractions of updates. The results obtained by applying both methods

                                                IJCAI-07
                                                   821