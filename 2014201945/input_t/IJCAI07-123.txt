                               Occam’s Razor Just Got Sharper

                                Saher Esmeir    and  Shaul Markovitch
      Computer Science Department, Technion—Israel Institute of Technology, Haifa 32000, Israel
                                  {esaher, shaulm}@cs.technion.ac.il


                    Abstract                          accuracy. While accepting the ﬁrst interpretation, Domingos
                                                      questioned the second one.
    Occam’s razor is the principle that, given two hy-  Webb  [1996] presented C4.5X, an extension to C4.5 that
    potheses consistent with the observed data, the sim- uses similarity considerations to further specialize consistent
    pler one should be preferred. Many machine learn- leaves. Webb reported an empirical evaluation which shows
    ing algorithms follow this principle and search for that C4.5X has a slight advantage in a few domains and ar-
    a small hypothesis within the version space. The  gued that these results discredit Occam’s thesis.
    principle has been the subject of a heated debate   Murphy and Pazzani [1994] reported a set of experiments
    with theoretical and empirical arguments both for in which all possible consistent trees were produced and their
    and against it. Earlier empirical studies lacked suf- accuracy was tested. Their ﬁndings were inconclusive. They
    ﬁcient coverage to resolve the debate. In this work found cases where larger trees had, on average, better accu-
    we provide convincing empirical evidence for Oc-  racy. Still, they recommend using Occam’s principle when
    cam’s razor in the context of decision tree induc- no additional information about the concept is available. The
    tion. By applying a variety of sophisticated sam- major limitation of their work is the exhaustive enumeration
    pling techniques, our methodology samples the ver- of the version space. Such an approach is only applicable to
    sion space for many real-world domains and tests  domains with very few features.
    the correlation between the size of a tree and its ac- In this work we present an alternative approach that per-
    curacy. We show that indeed a smaller tree is likely forms statistical testing of Occam’s thesis on a sample of
    to be more accurate, and that this correlation is sta- the version space. This approach allows us to use high-
    tistically signiﬁcant across most domains.        dimensional domains and complex concepts. One problem
                                                      with random sampling of the version space is the rarity of
1  Introduction                                       small trees in the sample. We therefore use, in addition to
Occam’s razor, attributed to the 14th-century English logi- random sampling, biased sampling methods based on mod-
                                                                                   [
cian William of Ockham, is the principle that, given two hy- ern anytime induction algorithms Esmeir and Markovitch,
                                                          ]
potheses consistent with the observed data, the simpler one 2004 . These methods produce samples with much higher
should be preferred. This principle has become the basis for concentrations of small trees.
many induction algorithms that search for a small hypothesis The major contribution of this work is to provide convinc-
within the version space [Mitchell, 1982]. Several studies at- ing empirical evidence for Occam’s razor in the context of
tempted to justify Occam’s razor with theoretical and empiri- classiﬁcation trees. Furthermore, the various sampling tech-
cal arguments [Blumer et al., 1987; Quinlan and Rivest, 1989; niques we applied help to better understand the space of con-
Fayyad and Irani, 1990]. But a number of recent works have sistent decision trees and how top-down induction methods
questioned the utility of Occam’s razor, and provided theoret- explore it. Note that this empirical demonstration of the util-
ical and experimental evidence against it.            ity of Occam’s principle does not pretend to provide a philo-
  Schaffer [1994] proved that no learning bias can outper- sophical proof for Occam’s thesis.
form another bias over the space of all possible learning tasks.
This looks like theoretical evidence against Occam’s razor. 2 Occam’s Empirical Principle
Rao et al. [1995], however, argued against the applicability In the context of machine learning, the widely accepted in-
of this result to real-world problems by questioning the valid- terpretation of Occam’s razor is that given two consistent
ity of its basic assumption about the uniform distribution of hypotheses, the simpler one is likely to have a lower er-
possible learning tasks.                              ror rate. Fayyad and Irani [1990] have formally deﬁned
  Domingos [1999] argued that the disagreement about the this notion: for two decision trees T1 and T2 and a ﬁxed ,
utility of Occam’s razor stems from the two different inter- 0 <<1, T1 is likely to have a lower error rate than T2 if
pretations given to it: the ﬁrst is that simplicity is a goal in Pr {P (T1,) <P(T2,)} > 0.5,whereP (T,) is the prob-
and of itself, and the second is that simplicity leads to better ability that T has an error rate greater than .

                                                IJCAI-07
                                                   768      Procedure TDIDT(E,A)                            on a subset of the consistent trees—the trees obtainable by
        If E = ∅                                      top-down induction, denoted by TDIDT A(E). Under the
           Return Leaf(nil)                           TDIDT scheme, the set of examples is partitioned into sub-
        If ∃c such that ∀e ∈ E Class(e)=c             sets by testing the value of an attribute and then each subset is
           Return Leaf(c)                             used to recursively build a subtree. The recursion stops when
        a ←  CHOOSE-ATTRIBUTE(A, E)                   all the examples have the same class label. Figure 1 formal-
        V  ← domain(a)                                izes the basic procedure for top-down induction.
        Foreach vi ∈ V *                                While TDIDT  A(E)  is a strict subset of DTA(E),we
           Ei ←{e  ∈ E | a(e)=vi}                     claim that the trees in DTA(E) that are not in TDIDT A(E)
           Si ← TDIDT(Ei,A−{a})                       are not interesting for the purpose of model learning. There
        Return Node(a, {vi,Si|i =1...|V |})         are two types of trees in DTA(E) − TDIDT A(E):
                                                        1. A tree containing a subtree with all leaves marked with
      *Whena  is numeric, a cutting point is chosen and   the same class. Obviously, such a subtree could have
      and a is not ﬁltered out when calling SID3.         been replaced by a single node marked with the class.
                                        E               2. A tree with an internal node that has no associated ex-
Figure 1: Top-down induction of decision trees. stands for            E
the training set and A stands for the set of attributes.  amples from  . The subtree rooted at this node is not
                                                          supported by training examples and is therefore not in-
                                                          teresting for induction.
  Fayyad and Irani [1990] also provided theoretical support
for favoring smaller trees. They showed that under a set of Note that including the above trees could unjustly distort
assumptions, given two trees T1 and T2 consistent with the the results. In the ﬁrst case, larger trees that are logically
observed data, T1 is likely to have a lower error rate than equivalent will be included, arbitrarily weakening the nega-
T2 if T1 has fewer leaves. Berkman and Sandholm [1995], tive correlation. In the second case, the extra branches are not
however, have questioned this set of assumptions and argued supported by any training example. Thus, their leaves must
that the opposite conclusion can be drawn from it.    be labeled randomly, lowering the accuracy and hence arbi-
  The main challenge we face in this work is to empirically trarily strengthening the negative correlation.
test the validity of Occam’s razor in decision tree induction. While we restrict the deﬁnition of Occam’s empirical prin-
We therefore deﬁne the Occam’s empirical principle:   ciple to consistent hypotheses, in our experiments we also
                                                      examine its applicability to pruned TDIDT A(E) trees. This
              E         E
Deﬁnition 1 Let train and test be a training and a testing allows us to draw conclusions for noisy datasets as well.
set respectively. Let H be a set of hypotheses consistent with
Etrain. We say that H satisﬁes Occam’s empirical principle 3.2 Sampling Techniques
with respect to Etrain and Etest if, for any h1,h2 drawn from
                                                      Our goal is to sample the TDIDT A(E) space in order to
H × H,
                                                   test Occam’s empirical principle. Our ﬁrst proposed sam-
                                
P  Acc (h1,Etest) ≥ Acc (h2,Etest) |h1|≤|h2| ≥ 0.5,   pling technique uses TDIDT with a random selection of the
                                                      splitting attribute (and cutting point, where the attribute is nu-
where |h| is the size of hypothesis h and 0 ≤ Acc(h, E) ≤ 1 meric). We refer to this method as the Random Tree Gen-
is the accuracy of h on a test set E.                 erator (RTG ). Observe that although it has no bias with re-
                                                      spect to generalization quality, RTG does not uniformly sam-
3  Sampling the Version Space                         ple TDIDT  A(E). For example, if the concept was a1 and
                                                      the attributes were {a1,a2,a3}, the probability of construct-
Given a learning problem, we would like to produce all ing the smallest tree (with a single split) is much higher than
the possible trees consistent with the observations and test that of constructing a speciﬁc large tree. We will later show
whether their size and accuracy are correlated. Such an ex- that the non-uniform sampling should not affect the validity
haustive enumeration, however, is not practical for most real- of our conclusions.
world domains. Therefore, in what follows we propose sam- One problem with random sampling is the rarity of small
pling as an alternative. First we deﬁne the population, i.e., the trees. Many induction methods, however, are likely to con-
space of decision trees we sample from, and then we describe centrate on small trees. Theoretically, the correlation could
3 different sampling techniques, each of which focuses on a have been statistically signiﬁcant when sampling the TDIDT
different part of the sampled space.                  space but not when sampling a subspace consisting of small
                                                      trees. To test this hypothesis we need a sample of small
3.1  Deﬁning the Version Space                        trees. Such a sample could be obtained by repeatedly in-
Given a set of attributes A, the hypothesis class we deal with voking RTG and keeping the smaller trees. Nevertheless,
is the set of decision trees over A, denoted by DTA.Let the number of RTG invocations needed to obatin a reason-
E be a set of examples. Because Occam’s razor is applica- able number of small trees is prohibitively high. Another al-
ble only to hypotheses that can explain the observations, we ternative is to use ID3. Repeated invocations of ID3,how-
limit our discussion to the version space—the set of all trees ever, result in similar trees that can vary only due to differ-
consistent with E, denoted by DTA(E). Furthermore, since ent tie-breaking decisions. Esmeir and Markovitch [2004] in-
most decision tree learners build a tree top-down, we focus troduced SID3, a stochastic version of ID3 that is designed

                                                IJCAI-07
                                                   769     Procedure SID3-CHOOSE-ATTRIBUTE(E,A)             to improve with the increase in r. Figure 3 lists the procedure
       Foreach a ∈ A                                  for attribute selection as applied by LSID3. Because our goal
          p (a) ← gain-1(E,a)                         is to sample small trees and not to always obtain the small-
       If ∃a such that entropy-1(E,a)=0               est tree, we use LSID3(r =1) as a sampler. Observe that
          a∗ ← Choose attribute at random from        LSID3  is stochastic by nature, and therefore we do not need
              {a ∈ A | entropy-1(E,a)=0}              to randomize its decisions.
       Else
          a∗ ← Choose attribute at random from A;     4   Experimental Results
                             a
              for each attribute , the probability    We tested Occam’s empirical principle, as stated in Deﬁnition
              of selecting it is proportional to p (a)
              a∗                                      1, on 20 datasets, 18 of which were chosen arbitrarily from
       Return                                         the UCI repository [Blake and Merz, 1998], and 2 which are
                                                      artiﬁcial datasets that represent hard concepts: XOR-5 with 5
          Figure 2: Attribute selection in SID3       additional irrelevant attributes, and 20-bit Multiplexer. Each
                                                      dataset was partitioned into 10 subsets that were used to cre-
    Procedure LSID3-CHOOSE-ATTRIBUTE(E,A,r)
        r                                             ate 10 learning problems. Each problem consisted of one sub-
      If  =0                                          set serving as a testing set and the union of the remaining 9
        Return ID3-CHOOSE-ATTRIBUTE(E, A)             as a training set, as in 10-fold cross validation. We sampled
      Foreach a ∈ A
                                                      the version space, TDIDT A(E), for each training set E us-
        Foreach vi ∈ domain(a)
           E  ←{e  ∈ E | a e   v }                    ing the three methods described in Section 3 and tested the
            i            ( )=   i                     correlation between the size of a tree (number of leaves) and
           mini ←∞
                 r                                    its accuracy on the associated testing set. The size of the sam-
           Repeat  times                              ple was ten thousand for RTG and SID3, and one thousand
             T ←  SID3(Ei,A−{a})
             min  ←      min  , |T |                  for LSID3 (due to its higher costs). We ﬁrst present and dis-
                 i  min (   i   )                    cuss the results for consistent trees and then we address the
        total ←    |domain(a)| min
             a     i=1          i                     problem of pruned, inconsistent trees.
      Return a for which totala is minimal
                                                      4.1  Consistent Decision Trees
         Figure 3: Attribute selection in LSID3       Figure 4 plots size-frequency curves for the trees obtained by
                                                      each sampling method for three datasets: Nursery, Glass, and
to sample the version space semi-randomly, with a bias to Multiplexer-20 (for one fold out of the 10). Each of the three
smaller trees. In SID3, instead of choosing an attribute that methods focuses on a different subspace of TDIDT A(E),
maximizes the information gain, we choose the splitting at- with the biased sampling methods producing samples consist-
tribute semi-randomly. The likelihood that an attribute will ing of smaller trees. In all cases we see a bell-shaped curve,
be chosen is proportional to its information gain.1 However, indicating that the distribution is close to normal. Recall that
if there are attributes that decrease the entropy to zero, then RTG does not uniformly sample TDIDT A(E): a speciﬁc
one of them is picked randomly. The attribute selection pro- small tree has a better chance of being built than a speciﬁc
cedure of SID3 is listed in Figure 2.                 large tree. The histograms indicate, however, that the fre-
  For many hard learning tasks such as parity concepts, quency of small trees in the sample is similar to that of large
ID3’s greedy heuristic fails to correctly estimate the useful- trees (symmetry). This can be explained by the fact that there
ness of the attributes and can mislead the learner to produce are more large trees than small trees. To further verify this,
relatively large trees. In such cases, SID3 can, in theory, we compared the distribution of the tree size in an RTG sam-
produce signiﬁcantly smaller trees. The probability for this, ple to that of all trees, as reported in [Murphy and Pazzani,
however, is low and decreases as the number of attributes in- 1994] (Mux-11 dataset). The size-frequency curves for the
creases. To overcome this problem, we use a third sampling full space and the sampled space were found to be similar.
technique that is based on the recently introduced LSID3 Occam’s empirical principle states that there is a nega-
algorithm for anytime induction of decision trees [Esmeir tive correlation between the size of a tree and its accuracy.
and Markovitch, 2004]. LSID3 adopts the general TDIDT To test the signiﬁcance of the correlation, we used the non-
scheme, and invests more time resources for making better parametric Spearman correlation test on each of the samples.2
split decisions. For every candidate split, LSID3 attempts to Spearman’s coefﬁcient ρ measures the monotonic association
estimate the size of the resulting subtree were the split to take of two variables, without making any assumptions about their
place, and favors the one with the smallest expected size. The frequency distribution. For a paired sample of X and Y , ρ is
                                                                              2
estimation is based on a biased sample of the space of trees deﬁned as 1 − 6 di/(n(n − 1)),wheredi is the differ-
rooted at the evaluated attribute. The sample is obtained using ence in the statistical rank of xi and yi. There is a special
SID3. LSID3 is parameterized by r, the sample size. When correction for this formula in the presence of ties.
r is greater, the sample is larger and the resulting estimate is Table 4.1 lists summarizing statistics for the RTG, SID3,
expected to be more accurate. Therefore, LSID3 is expected and LSID3 samplers. The validity of Occam’s empirical prin-

  1SID3 ensures that attributes with gain of zero will have a posi- 2All statistics were computed using the The R Project package
tive probability to be selected.                      [R Development Core Team, 2005].

                                                IJCAI-07
                                                   770                   0.25                     0.16                     0.3
                                RTG                RTG                 RTG
                                SID3        0.14                       SID3
                    0.2                            SID3             0.25
                               LSID3        0.12  LSID3                LSID3
                                                                     0.2
                   0.15                     0.1
                                            0.08                    0.15
                    0.1                     0.06
                 Frequency               Frequency                Frequency  0.1
                                            0.04
                   0.05                                             0.05
                                            0.02
                     0                       0                       0
                        1000  2000  3000  4000  5000  25  50  75  100  125  150  175  0  100  200  300  400  500
                            Tree size               Tree size                Tree size
           Figure 4: Frequency curves for the Nursery (left), Glass (middle), and Multiplexer-20 (left) datasets


                             RTG          √             SID3        √            LSID3        √
        DATASET     ACC.SIZE         ρ         ACC.SIZE         ρ         ACC.SIZE        ρ

        BREAST-W   92.9±2.8 128±14  -0.1  7  93.1±2.7  108±11  -0.1 8    94.3±1.6  77±4  -0.1 5
        BUPA       59.7±8.0 213±10   01063.4±7.3        94±6    0761.9±7.4         69±3   03
        CAR        72.8±4.6 647±77  -0.8 10  79.7±5.8  520±95  -0.9 10   91.9±1.1 285±13  03
        CLEVELAND  51.6±6.9 188±7    0650.2±7.2        134±7    0746.1±7.4         98±5  -0.1 7
        CORRAL     73.3±22.9 15±3   -0.1  9  81.6±19.6  10±2   -0.2 10   89.8±8.3  7±1   0.2  NA
        GLASS      55.6±9.9 135±8   -0.1 10  62.3±9.3   57±5   -0.2 10   68.0±8.4  39±3  -0.1 9
        HUNGERIAN  72.8±7.4 125±10  -0.1  9  73.3±7.2   65±6   -0.1 8    69.9±7.5  47±3  -0.1 8
        IRIS       88.9±7.3  39±9   -0.3 10  92.8±4.5   12±2   -0.1 8    93.8±2.7  8±0   -0.1 7
        MONKS-1    91.1±4.6 203±42  -0.5 10  97.0±3.8  113±55  -0.7 10  100.0±0.0  28±4  NA   NA
        MONKS-2    77.8±4.4 294±9   -0.3 10  75.5±4.6  289±8   -0.4 10   77.7±3.1 259±3  0.2  0
        MONKS-3    88.3±5.4 171±46  -0.6 10  96.0±2.5   77±33  -0.5 10   96.7±0.4  38±2  0.1  NA
        MUX-20     55.7±6.3 388±14  -0.1 10  56.6±6.5  249±13  -0.2 10   86.1±11.8 89±35 -0.9 10
        NURSERY    77.2±5.9 3271±551 -0.9 10 93.1±1.8 1583±295 -0.8 10   98.1±0.5 656±54 -0.4 10
        SCALE      72.2±4.2 394±11   0.1  0  71.7±4.1  389±11  0.1  0    70.1±3.8 352±5  0.1  3
        SPLICE     61.1±3.7 1977±101 -0.6 10 60.5±4.2 1514±112 -0.7 10   89.3±1.8 355±23 -0.5 10
        TIC-TAC    72.3±4.8 468±34  -0.4 10  80.2±4.5  311±30  -0.4 10   87.7±3.0 166±11 -0.1 9
        VOTING     89.2±5.8  52±12  -0.3 10  92.8±4.3   26±5   -0.2 9    94.5±3.1  15±2  -0.2 8
        WINE       78.6±10.2 73±12  -0.3 10  90.6±6.7   13±3   -0.2 9    91.7±4.3  7±1   -0.1 6
        XOR-5      50.7±10.6 136±8  -0.1 10  51.9±11.8 108±11  -0.4 10   96.5±7.7  39±11 -0.8 10
        ZOO        90.0±7.4  24±5   -0.2 10  91.8±6.2   18±4   -0.2 9    94.2±3.5  11±1  -0.1 NA

Table 1: Testing Occam’s empirical principle using different sampling methods that produce consistent trees. For each method
we report the accuracy, tree size, and Spearman’s correlation coefﬁcient (ρ) averaged over all 10 partitions. We also report the
number of times (out of 10) that a negative correlation was found to be statistically signiﬁcant with p =0.95.

ciple, tested by Spearman’s method, is listed in the rightmost pothesis could not be rejected is higher than in the previous
column. For each sampling method, for each dataset, we samplers. One possible reason for this phenomenon is that
count how many times, out of the 10 folds, the null hypoth- LSID3 samples trees from a much tighter size-range. Hence,
esis H0 (which states that the variables are not correlated) there is an increased probability for ﬁnding two trees, T 1 and
can be rejected at an α =5%signiﬁcance level, against the T 2, with the size and accuracy of T 1 being greater than T2.
alternative hypothesis that the correlation is negative. As indicated by the frequency curves, the different sam-
  The results indicate that when random sampling (RTG )in pling methods cover different portions of the space, but some-
used, Occam’s empirical principle, as measured by Spear- times overlap. An interesting question is whether the conclu-
man’s test, is valid for almost all problems, except for Scale. sions hold if we analyze all samples together. Note that it is
The results for the SID3 sampling method indicate that even not statistically valid to merge the samples of RTG, SID3,and
when focusing on smaller trees, simplicity is beneﬁcial as a LSID3 due to their different distributions. Nevertheless, we
bias for accuracy. Again, except for the Scale dataset, there is measured the correlation statistics for the combined samples
a strong inverse correlation between size and accuracy. The and found that the results are very similar, with a strong neg-
numbers indicate that the correlation is weaker than the RTG ative correlation between the size of a tree and its accuracy.
case, yet still signiﬁcant across most domains.         To illustrate the correlation between the size of a tree and
  The LSID3 samples focus on very small trees. In several its accuracy, we grouped the trees from a single run into bins,
cases, LSID3 could reach the smallest tree possible. Again according to their size, and calculated the average accuracy
the negative correlation was found to be signiﬁcant for most for each bin. We did this for each of the sampling methods.
domains.3 However, the number of cases where the null hy- Bins with less than 5 observations were discarded. Figure 5
                                                      plots the results for the Nursery, Glass, and Multiplexer-20
  3Note that in some cases, signiﬁcance tests could not be com- datasets. The error bars represent conﬁdence intervals, with
puted due to the large number of ties (indicated by NA in the table). α =5%.

                                                IJCAI-07
                                                   771                   0.95                     0.96                    0.988
                                                                    0.986
                    0.9                     0.94
                                                                    0.984
                   0.85
                                            0.92                    0.982
                    0.8                                              0.98
                                            0.9
                   0.75                                             0.978
                 Accuracy                Accuracy  0.88           Accuracy  0.976
                    0.7
                                                                    0.974
                                            0.86
                   0.65                                             0.972
                    0.6                     0.84                     0.97
                       2000  3000  4000       1000  1500  2000  2500   550  600  650  700  750  800
                            Tree size               Tree size                Tree size
                    0.8                     0.95                     0.9
                                            0.9
                   0.75                                             0.85
                                            0.85
                    0.7
                                            0.8                      0.8
                   0.65                     0.75
                                                                    0.75
                    0.6                     0.7
                 Accuracy                Accuracy  0.65           Accuracy  0.7
                   0.55
                                            0.6
                                                                    0.65
                    0.5                     0.55
                   0.45                     0.5                      0.6
                     100  110  120  130  140  150  160  40  45  50  55  60  65  70  75  80  34  36  38  40  42  44  46  48
                            Tree size               Tree size                Tree size
                    0.7                     0.85                     1
                                            0.8                     0.95
                   0.65                                              0.9
                                            0.75
                                                                    0.85
                    0.6
                                            0.7                      0.8
                                                                    0.75
                   0.55                     0.65
                 Accuracy                Accuracy                 Accuracy  0.7
                                            0.6
                    0.5                                             0.65
                                            0.55                     0.6
                   0.45                     0.5                     0.55
                     320  340  360  380  400  420  440  160  180  200  220  240  260  280  300  20  40  60  80  100  120  140  160
                            Tree size               Tree size                Tree size
Figure 5: Correlation between size and accuracy using RTG (left-most), SID3 (middle), and LSID3 (right-most). The upper
graphs represent the results for the Nursery dataset, the graphs in the middle row stand for the Glass dataset and the lower
graphs stand for the Multiplexer-20 dataset.

  Again, the graphs show a strong correlation between size 5 Conclusions
and accuracy, conﬁrming Occam’s empirical principle. For Occam’s razor states that given two consistent hypotheses, the
the Nursery and Multiplexer-20 datasets the correlation is simpler one should be preferred. This principle has been the
strong for all 3 samplers. For Glass, the correlation is weaker subject for a heated debate, with theoretical and empirical ar-
when the trees are very small. These graphs, which represent guments both for and against it. In this work we provided
each size range in its own bin, indicate that the positive sup- convincing empirical evidence for the validity of Occam’s
port we showed for Occam’s principle is not a result of a size principle with respect to decision trees. We state Occam’s em-
bias in our sampling methods.                         pirical principle, which is well-deﬁned for any learning prob-
                                                      lem consisting of a training set and a testing set, and show ex-
4.2  Pruned Decision Trees                            perimentally that the principle is valid for many known learn-
Formally, Occam’s empirical principle, as deﬁned in Section ing problems. Note that our study is purely empirical and
2, is applicable only to consistent hypotheses. Most decision does not attempt to reach an indisputable conclusion about
tree learners, however, do not necessarily produce consistent Occam’s razor as an epistemological concept.
models because they output a pruned tree in attempt to avoid Our testing methodology uses various sampling techniques
overﬁtting the data. This is usually done in two phases: ﬁrst to sample the version space and applies Spearman’s correla-
a tree is grown top-down and then it is pruned. In our second tion test to measure the monotonic association between the
set of experiments, we examine whether taking simplicity as size of a tree and its accuracy. Our experiments conﬁrm Oc-
a bias in the ﬁrst stage is beneﬁcial even when the tree is later cam’s empirical principle and show that the negative correla-
pruned. Therefore, we measure the correlation between the tion between size and accuracy is strong. Although there were
size of unpruned trees and their accuracy after pruning. several exceptions, we conclude that in general, simpler trees
  Table 4.2 summarizes the results. The same statistics were are likely to be more accurate. Observe that our results do not
measured with a single change: the accuracy was measured contradict those reported by Murphy and Pazzani [1994],but
after applying error-based pruning [Quinlan, 1993].Asinthe complement them: we do not claim that a smaller tree is al-
case of consistent trees, examining the overall correlation be- ways more accurate, but show that for many domains smaller
tween the size of a tree and its accuracy indicates that for most trees are likely to be more accurate.
datasets the inverse correlation is statistically signiﬁcant. We view our results as strong empirical evidence for the
  Table 4.2 also gives the percentages of pruned leaves. utility of Occam’s razor to decision tree induction. It is im-
While the trees produced by RTG were aggressively pruned, portant to note that the datasets used in our study do not nec-
the percentage of pruned leaves in LSID3 was relatively low. essarily represent all possible learning problems. However,
This is due to the stronger support for the decisions made at these datasets are frequently used in machine learning re-
the leaves of smaller consistent trees.               search and considered typical tasks for induction algorithms.

                                                IJCAI-07
                                                   772