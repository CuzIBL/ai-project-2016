         Maximum Margin Coresets for Active and Noise Tolerant Learning

                 Sariel Har-Peled∗   and  Dan Roth†                   Dav Zimak†‡
                 Department of Computer Science                        Yahoo! Inc.
            University of Illinois at Urbana-Champaign              Santa Clara, CA
                  {sariel, danr}@uiuc.edu                    davzimak@yahoo-inc.com


                    Abstract                          data when using a hypothesis class with unbounded complex-
                                                      ity. Unfortunately, there is no guarantee that the size of the
    We study the problem of learning large margin half- support set will be small. Additionally, the running time of
    spaces in various settings using coresets and show the SVM algorithm to ﬁnd an exact solution to the large mar-
    that coresets are a widely applicable tool for large gin problem is O(m3) time and O(m2) space using m exam-
    margin learning. A large margin coreset is a sub- ples and is infeasible for large datasets.
    set of the input data sufﬁcient for approximating   Most practical algorithms, such as chunking [Vap82],de-
    the true maximum margin solution. In this work,   composition [OFG97], and sequential minimal optimiza-
    we provide a direct algorithm and analysis for con-
                              1                       tion (SMO) [Pla98], reduce the problem to manageable sub-
    structing large margin coresets . We show various tasks are heuristic solutions which may converge slowly.
    applications including a novel coreset based analy- Furthermore, the running time remains crucially dependent
    sis of large margin active learning and a polynomial on the number of support vectors in the solution. Alterna-
    time (in the number of input data and the amount of tively, recent work has focused on on-line approaches to ap-
    noise) algorithm for agnostic learning in the pres- proximate the maximum-margin algorithms [Kow00; Gen01;
    ence of outlier noise. We also highlight a simple LL02]. On-line algorithms are iterative solutions that add ex-
    extension to multi-class classiﬁcation problems and amples to the large margin solution based on various condi-
    structured output learning.                       tions – all related to the relative margin of the example under
                                                      consideration. As a result, they can bound the number of ex-
1  Introduction                                       amples necessary to guarantee a large margin separation.
                                                        In this paper we relax the requirement that we ﬁnd the
Large margin techniques are the basis for both practical al- unique maximum margin separation. Speciﬁcally, we ﬁnd
gorithms and theoretic analysis in machine learning. Algo- an approximate maximum margin separation – a hyperplane
rithmically, the most notable example is the support vector that separates all of the input data with margin larger than
machine (SVM) [Vap95] that ﬁnds a maximum-margin sep- (1 − )ρ∗,whereρ∗ is best achievable. We use the coreset
aration of a given data set. The SVM has proven very suc- method ﬁrst described in [BC03] and extended to the maxi-
cessful in practice and theoretically, a large margin separation mum margin setting in [TKC05].Acoreset for a maximum
implies good generalization performance [KS94].       margin separating hyperplane is a subset, C ∈ D of exam-
  The SVM  has a simple representation and a straightfor- ples such that the maximum margin hyperplane on C is an
ward implementation — ﬁnd the set of support vectors that approximate maximum margin separating hyperplane on D.
uniquely deﬁne the maximum margin separation. Amazingly, In some sense, it captures all of the necessary information for
this approach simultaneously allows the classiﬁer to be rep- the approximation just as the set of support vectors does for
resented with a (potentially small) subset of the input data the true maximum margin separating hyperplane.
and, through the use of kernel functions, to utilize an arbi- This paper studies the running time of a simple coreset al-
trarily powerful hypothesis space. If a small support set can gorithm for binary and structured-output classiﬁcation and
be found, then one can guarantee high performance on unseen the use of the coreset as an analysis tool for active learning
  ∗                                                   and noise-tolerant learning in the agnostic setting. In previ-
   Work on this paper was partially supported by a NSF CAREER
award CCR-0132901.                                    ous work, the coreset was constructed as a reduction to ﬁnd-
  †Work on this paper was partially supported by NSF grants ITR- ing a coreset for a different problem – the minimum enclos-
                                                      ing ball [BC03]. In Section 3, we show a direct algorithm for
IIS-0085980 and IIS-9984168.                                                                   2
  ‡                                                                              |C|  O  R/ρ    /
   Much of this work was done while at the University of Illinois. ﬁnding a coreset of size at most = (( ∗) ) in time
                                                      O  nd|C|  |C|T |C|        R     ρ
  1Throughout the paper, we often present only proof sketches ( +    (  )) where  and  ∗ measure the size of
due to lack of space. The full proofs appear in Technical Report the example set and the quality of the maximum-margin clas-
UIUCDCS-R-2006-2784 available from the University of Illinois siﬁer and T (|C|) is the time to run an SVM black-box on |C|
Computer Science Department.                          examples. This improves previous bounds by a factor 1/ and

                                                IJCAI-07
                                                   836provides an explicit running time to the algorithm.       Algorithm CORESET SVM
  In Section 4  we  analyze one of the  most effec-            INPUT:
tive active learning algorithms based on the maximum-            S =((x1,y1),...,(xm,ym)),
margin principal [TK02] and give a running time and a            Approximation parameter  ∈(0, 1)
  −                                                                           d           m
(1    )-approximation guarantee. We show that in time            where S ∈  R  ×{−1,  1}
  d|C|           1
O        |C|         |C|T |C|                                   UTPUT            h ∈H
 ( e  ln   +lnδ   +     (  )) one can compute a core-         O      : A classiﬁer
                                 2  2
set C of size at most |C| = O((R/ρ∗) / ) such that with  begin
                                                                  C     x ,y
high conﬁdence, 1 − δ, the classiﬁer produced will have large  Set  =((  1  1))
                                                                  i    ...T
margin and small, e, error. In Section 5, we analyze learn-   For =1
ing with outlier noise. Roughly speaking, a set of outliers is   Set hi = SVM(C)
a small subset of the input data such that, if removed would     Set ρi = ρ(hi,C)
                                                                 x   ,y                     ρ h , x,y
yield the correct maximum-margin classiﬁer. Thus if we as-       ( min min) = argmin(x,y)∈S\C ( i   )
                                                                  ρ h , x  ,y   <    −  ρ
sume there are k outliers, the best maximum margin classi-       if ( i min min) (1    ) i
                                                                   C   C    x  ,y
ﬁer is well-deﬁned. We show a polynomial time algorithm              =     ( min min)
for learning the maximum margin separation in this setting.      else
Finally, in Section 6 we highlight an important connection         return hi
between the coreset algorithm and recent work for learning     Return SVM(C)
SVM  for structured output [THJA04]. Indeed, we can view  end
SVMstruct as a coreset algorithm.
                                                           Figure 1: Maximum-margin learning via coresets.
2  Preliminaries
We are given D = {(x1,y1),...,(xM ,yM )}, a labeled train- For the latter task, there exists a coreset algorithm that runs
ing set of cardinality M drawn from some distribution DX ,Y , in time linear in the number of points [BC03]. In this sec-
where xm ∈Xare the examples in a inner-product space  tion, we provide very similar results with a slight (factor 1/)
and ym ∈Yare labels. For most of the paper, we assume a
                     d                                improvement by providing a direct algorithm and analysis.
real valued input, X = R , and binary output, Y = {−1, 1}. In Figure 1, the simpliﬁed coreset algorithm is presented
However, in Section 6 we note an important extension to the for learning binary labeled data in a noise-free setting. The
structured output domain.                             coreset C is built iteratively. At each step, we construct the
  In this paper, we use the maximum margin principle to true maximum margin classiﬁer, hi = SVM(C) and use it to
discover a hypothesis h ∈Hrepresented by a halfspace  ﬁnd the example with the smallest (or negative) margin. This
h x                  y w  · x       w ∈ Rd
 ( ) = argmaxy∈{−1,1} (     ),where        .Thebi-    example is then added to the coreset and the process repeats.
nary margin (or geometric margin), ρ(w, x,y)=y(w · x), It is possible to tell that hi is a (1 − )-approximation by ob-
for an example, x, is deﬁned as the distance from the exam- serving the ratio between the margin on the coreset, ρ(hi,C),
ple to the discriminating hyperplane w · x =0. Notice that a and the margin on the entire data set, ρ(hi,D). If this ratio
negative margin is indicative of a misclassiﬁed example. The is small enough then the margin of hi on the entire data set is
margin of hypothesis h is ρ(h, D)=min(x,y)∈D ρ(w, x,y). sufﬁciently large and the algorithm halts.
  Therefore, given a sample, D, the maximum margin hy-
pothesis (hyperplane) is
                                                      Lemma 3.1  Let ρ∗ = ρ(L(D),D)   be the optimal margin
                                                                         d           M
         L(D) =   argmax     min  y(w · x)            for data set D ∈{R   ×{−1,  1}}  of size M. Given a
                w∈RD ,||w||=1 (x,y)∈D                 parameter,  ∈ (0, 1), one can compute a coreset C of size
                                                      |C| = O((R/ρ∗)2/) in time O(nd|C| + |C|T (|C|)),where
is the uniform length hyperplane with maximum margin over
                                                      R =max(x,y)∈D  ||x||.
the data.
                                                        Proof sketch: We show using a simple geometric argument
               −                              h
Deﬁnition 2.1 ((1 )-Approximation) A hypothesis  is   that each time an example is added to the coreset, the margin
    −                                        h∗
a (1    )-approximation to the optimal hypothesis if  on the next integration decreases by at least a constant factor.
ρ(h, D) ≥ (1 − )ρ(h∗,D).
                                                      That is,                      
                                                                                  α2
                                                                     ρ    ≤    −   i   ρ ,
Deﬁnition 2.2 (Maximum margin coreset) Amaximum                        i+1    1   R2    i             (1)
margin coreset is a set of examples C = C(, ρ) ⊂ D such                         8
   h   L  C       −                 h∗   L  D
that =   ( ) is a (1 )-approximation to =   ( ).      where αi =  ρi − ρ(hi, xi) measures how much the added
                                                      example violates the current margin guess using the current
3  Coreset Learning Algorithm                         hypothesis hi. Then, it can be shown that the decreasing se-
                                                      quence of margins will be smaller than ρi ≤ (1 + )ρ after at
Large margin coresets were ﬁrst introduced in [TKC05] to   64R2
form the Core Vector Machine (CVM). In that work, they re- most ρ2 steps. Once the margin is small enough, we show
duced ﬁnding a maximum margin hyperplane to ﬁnding the that it quickly decreases and outputs a (1 − )-approximation
minimum enclosing ball of a set of points around the origin. to the maximum margin classiﬁer.

                                                IJCAI-07
                                                   837  At each step, the algorithm in Figure 1 adds the example we think this work helps support our claim that coreset-based
with the smallest margin to the coreset. However, the algo- algorithms can be practical.
rithm is easily modiﬁed such that any example with margin
small enough can sufﬁce. Speciﬁcally, if we know that each 4 Maximum Margin Active Learning
example (x,y) added to the coreset is such that ρ(hi, x,y) <
(1−)ρi, but not necessarily the example with minimum mar- In active learning, the learner is presented with a set of un-
                                                                  U    {x ,...,x }
gin, the algorithm still converges, but with a larger coreset. labeled data, = 1 m and an oracle, ORACLE :
                                                      X→{−1,    1} that provides a label to any example x ∈ U
                                                      consistent with a large margin hypothesis. The goal is to
Corollary 3.2 Let ρ∗ = ρ(L(D),D) be the optimal margin
                   d          M                       learn exactly this maximum margin separation using a lim-
for data set D ∈{R  ×{−1,  1}}   of size M. Given a
                                                      ited number of oracle queries.
parameter,  ∈ (0, 1), one can compute a coreset C of size
              2  2                                      Recently, iterative algorithms for active learning SVM
|C| = O((R/ρ∗) / ) in time O(nd|C| + |C|T (|C|)),where
                                                      have been proposed [TK02; CCS00].  After presenting a
R =max(x,y)∈D  ||x||.
                                                      slight modiﬁcation of these algorithms using coresets and in-
  Proof sketch: If, rather than add the minimum margin ex- troducing an explicit stopping criteria, we show that the al-
ample at each step, we add any example with small enough gorithm converges quickly to a (1 − )-approximation of the
margin, ρ(hi, x,y) <ρi, then we can simplify the simple true maximum margin hypothesis that exists given all labels
geometric proof sketch for Lemma 3.1 and show that    (with high probability).
                               
                             2                       4.1  Coreset Active Learning Algorithm
              ρi+1 ≤  1 − ρi      ρi.
                            8R2                       In Figure 2, the active learning algorithm from [TK02] is
                                                      adapted by adding a veriﬁcation stage. The algorithm runs
Then, similarly to Lemma 3.1, we use this decreasing se-
                                                      in iterations, where at each step, the unlabeled example that
quence to prove the result.
                                                      is closest to the decision boundary is added to the coreset.
3.1  Related Work                                     However, if there are no examples near the decision bound-
                                                      ary (i.e. they are further than the current large-margin guess),
Central to the coreset algorithms presented here, is the idea we may think that all labels are classiﬁed correctly and thus
of iteratively building a working set of examples by carefully the algorithm can halt. Of course, since the labels are un-
selecting examples to add at each step. In the online learning known, it is possible that there are still a large number of
literature, this idea has appeared in work related to the coreset misclassiﬁed examples. At this point the algorithm enters a
approach.                                             veriﬁcation phase, VERIFY(), where examples are sampled
  Indeed, even the perceptron algorithm can be viewed as uniformly at random from the entire data set according to
building a working set. At each iteration, an example, UNIFORMRANDOM     and labeled using ORACLE .
 x ,y                           y  w  · x <                            ()                       ()
( i i), is added to the working set if i( i i) 0.The    It is important to note that the algorithm presented in Fig-
hypothesis is a linear sum of elements in the set.    ure 2 repeatedly cycles over the unlabeled dataset to ﬁnd the
  Various approximate algorithms for online learning have single example closest to the decision boundary. This is eas-
also been proposed. In [Kow00], Kowalczyk proposed a  ily modiﬁed to two important cases when the number of ex-
perceptron-like update rule, with various criteria for choos- amples is very large (i.e. m>>|C|) or when there is an
ing which example to update. One of them is exactly the inﬁnite stream of examples (i.e. m = ∞). In these cases, any
minimum  margin approach used in coresets. After only example, x where |ρ(hi, x,y)| < (1 − )ρi can be added to
O R2    R
 ( 2 log 2 ) updates, the algorithm would converge to a the coreset and the algorithm can halt after enough examples
(1 − )-approximate classiﬁer — a result very similar to are seen without making a mistake. Indeed Lemma 4.2 below
ours, modulo 1/ and log R terms. At roughly the same applies to these more general cases.
as Kowalczyk’s algorithm, two additional algorithms were
proposed: the Relaxed Online Maximum  Margin Algo-    4.2  Analysis
rithm (ROMMA) and the Approximate Large Margin Algo-  Algorithm 2 seeks the true maximum margin hypothesis of
rithm (ALMAp). ROMMA is an online algorithm that learns
    −                                                the data that would be found if all of the labels were known.
a (1   )-approximate maximum margin separation. Both  Here, we show that with high probability (1 − δ), it ﬁnds a
have similar selection criteria, and perform similarly in prac- (1−a)-approximation to this hypothesis. Unfortunately, it is
                                        O  R2
tice. ALMAp also provides a mistake bound of ( 2ρ2 ). impossible to guarantee error free learning (see Section 4.3),
  Recently a new algorithm, SVM-Perf, was introduced and so we must accept a small, e, prediction error.
                                1
implemented [Joa06] with similar O( 2 ) bounds. SVM-Perf The analysis follows from two facts. First, there exists a
is presented as a cutting-plane algorithm, where at each iter- coreset that can be constructed by adding examples that lie
ation a cutting-plane is found that represents a ﬁxed subset close to the decision boundary. Any example with very small
of the training examples. The coreset methods are a special margin (< (1 − a)ρi) will improve the approximation ir-
case of the cutting-plane algorithm where each cutting plane respective of the actual label. Second, the veriﬁcation stage
is described by a single example. Indeed, this very clever al- will halt either because it has found an example with very
gorithm converges very fast. Many experimental results are small (i.e. negative) margin that will improve the coreset or
presented that show the fast convergence time in practice, and because enough examples have been seen with no mistakes.

                                                IJCAI-07
                                                   838    Algorithm ACTIVE CORESET  SVM                     Using Lemma 4.1 and Corollary 3.2, we bound the running
                                        d  m
        INPUT:DataU   =(x1,...,xm)  ∈  R              time of this algorithm to converge to an approximate solution.
                               d
        OUTPUT: A classiﬁer h : R →{−1,  1}
                                                      Lemma 4.2  Let D  =  ORACLE(U), be the entire labeled
    begin                                                         ρ    ρ L D  ,D
        Set C = ∅                                     data set and ∗ =  (  ( )   ) be the optimal margin for
                                                      data set D of size M. Given parameters, δ, e,a ∈ [0, 1],
        Repeat until Halt                                                                           2  2
                                                                             C             O   R/ρ∗  /
          Set hi = SVM(C)                             one can compute a coreset of size at most (( ) a)
              ρ   ρ h ,C                                     O  d|C|   |C|    1    |C|T |C|         R
          Set  i = ( i  )                             in time  ( e  ln   +lnδ   +      (  )),where    =
          x                  ρ h , x,y                           ||x||
            min =argminx∈U\C  ( i    )                max(x,y)∈D    . The total number of calls to ORACLE()
                       |ρ h , x ,y | <  −   ρ                     |C|          1
          if miny∈{−1,1} ( i  min )   (1   ) i        is less than O(  ln |C| +ln  ), and T(m) is the running
            y              x                                       e           δ
             min = ORACLE( min)                      time of the SVM for m examples.
            C  = C   (xmin,ymin)
                                                                                                    2  2
          else                                          Proof: The coreset will be at most |C| = O((R/ρ∗) /a)
            (xv,yv)=VERIFY(U    \ C, hi)              as a result of Corollary 3.2 by noticing that each time an ex-
            if xv = NULL                              ample, (x,y) is added to the coreset, ρ(hi, x,y) < (1 − )ρi
               return hi and Halt                     —  either because an unlabeled example is added in Line (1)
            else                                     in Algorithm 2 (a) or because a labeled example is added
               C = C   (xv,yv)                        in Line (2) in Algorithm 2 (b). In the former we know that
          else                                        ρi −|ρ(hi, x,y)| >ρi, and in the latter we know that
            Return SVM(C)  and Halt                   ρ(hi, x,y) < 0.
    end                                                 Since we know   that at most |C| examples will be
              (a) ACTIVE CORESET SVM                  added to the coreset, and at each iteration, the margin
                                                      of the current working hypothesis decreases we can use
        Algorithm VERIFY                              Lemma 4.1 to bound the total number of Oracle queries in
                                                       1 ln |C| +ln1  to ensure 1−δ conﬁdence that the classiﬁer
            INPUT:                                    e           δ
                  U    x  ,...,x  ∈{Rd}m              has at most e mistakes. Therefore, the total number of calls
              Data  =(   1     m)                                                      1           1
                            d                         to ORACLE() is at most O(|C| + |C|  ln |C| +ln )=
              A classiﬁer h : R →{−1, 1}                                             e          δ
                                                      O  |C| 1   |C|    1  .
            OUTPUT:                                     (   e ln   +lnδ  )  The running time follows since
              (x, y) ∈ Rd ×{−1, 1} or NULL            SVM must be run each time an example is added to the core-
        begin                                         set.
            for i =1...T do
              x = UNIFORMRANDOM(U)                    4.3  Related Work
              y = ORACLE(x)                           Previously, the efﬁcacy of maximum margin active learning
              if ρ(h, x,y) < 0                        algorithms were explained because by choosing the example
                Return (x,y)                          closest to the decision boundary, the version space will be ap-
            Return NULL                               proximately halved [TK02]2. More precisely, it was argued
        end                                           that because at each iteration the version space is an intersec-
                     (b) VERIFY                       tion of half-spaces in the kernelized feature space. If we as-
                                                      sume that each example is of constant size (i.e. ||x|| =1)then
                                                      the hypothesis with maximum margin separation is a point in
Figure 2: (a) Active learning using coresets. Abusing nota-
    U \ C     x ∈ U| x,         x    ∈ C              the version space at the center of the largest enclosed ball in
tion,      =(        (  ORACLE(  ))     ). (b) Verify this polytope. Therefore, by choosing an example with small
procedure. UNIFORMRANDOM(U)   returns a random exam-
                                x                     margin, it is hoped that it comes close to bisecting the en-
ple from the unlabeled set. ORACLE( ) returns the correct closed ball and also the version space.
label for x and UNIFORMRANDOM(U)  selects an example
    U                                                   If one could guarantee that the version space was in-
from  uniformly at random.                            deed halved at each iteration, then the algorithm would con-
                                                      verge quickly to the true maximum margin hypothesis [TK02;
In the latter case, we can apply the following lemma, adapted FS97] Unfortunately, no such guarantee can be made, either
from [KLPV87; Ang87] that shows learning from member- in practice or in theory, thus the “halving” argument falls
ship queries can be used to give PAC-bounds.          short to adequately explain the practical success of choosing
                                                      the minimum absolute margin example at each iteration.
                                                        In addition, as ﬁrst presented in [Das05] a zero-error active
Lemma 4.1  If L is a conservative on-line algorithm with learning algorithm is impossible without requesting the label
              M
mistake bound     and  access to an  example oracle      2                     ||x|| =1
                         i.i.d.             D            In [TK02] it is assumed that  for all examples. While
ORACLE() drawing examples   from distribution ,then relaxing this assumption does cause a different view of the versions
           M    M      1
after at most  ln +lnδ   calls to ORACLE(), with con- space and changes the motivation, it does not affect their results. In-
ﬁdence 1 − δ, L produces a hypothesis with expected error deed, here, we propose an alternate justiﬁcation that does not depend
less than  on examples drawn from D.                 on the version space.

                                                IJCAI-07
                                                   839                                                         Algorithm OUTLIER SVM
                                                             INPUT:                               
                                                                                  M      d          M
                                                               Data D =((xm,ym))1   ∈  R  ×{−1,  1}
                                                             OUTPUT:
                                                               A classiﬁer h : Rd →{−1, 1}
                                                         begin
                                                             Set R =max(x,y)∈D ||x||
                                                             For i =0, 1, 2,...
                                                               ρ   R/  i
           (a)                     (b)                           =    2     

                                                                          R2
                                                               Set c = 32  2
                                                                         ρ        
Figure 3: Impossibility of Zero-Error Approximation: (a)                           D
                                                               For each subset Ds ∈ c
Active learning nightmare – all examples are positive, with a    hs = SVM(C)
single negative. (b) Nightmare in 3D – 2 negative points, one
                                                               hmax =argmaxs  ρk(hs,D)
below origin, a second on the plane of the circle.             if ρk(hmax,D) >ρ
                                                                 Return hmax and Halt.
of all examples. To see this, we consider a sample of exam- end
ples spread at a constant interval on the surface of a circle in        (a) OUTLIER SVM
R2. See Figure 3 for an illustration. The concept represented
by Figure 3(a) is one where a single example is negative and    Algorithm SIMPLE OUTLIER SVM
the rest are positive. The maximum margin separation thus           INPUT:
                                                                                        M
separates a single example from the rest. Consider any algo-          Data D =((xm,ym))1
rithm that computes the maximum margin separation of any            OUTPUT:
labeled subset of this data. Unless the single negative exam-         A classiﬁer h : Rd →{−1, 1}
ple is included in the labeled subset, then there is no hope of begin
achieving an approximate large-margin separation that cor-          Set R =max(x,y)∈D ||x||
rectly classiﬁes all examples in the data set. Therefore, if an     For i =0, 1, 2,...
adversary controls the oracle, by simply answering “+” to ev-                 i
                                                                      Set c =2            
ery query until the ﬁnal query, the learner is forced to ask the                         D
                                                                      For each subset Ds ∈
label of every example.                                                                   c
                                                                        hs = SVM(Ds)
                                                                      hmax =argmaxs  ρk(hs,D)
5  Agnostic Learning with Outlier Noise                                                   R2
                                                                      if ρk(hmax,D) >  32 c
Learning in the presence of noise is of great interest. While           Return hmax and Halt.
there are many deﬁnitions of noise, such as attribute, label,   end
and malicious noise, we consider a very general model, out-
lier noise. One can think of outlier noise in the following          (b) SIMPLE OUTLIER SVM
way: without the “noisy” examples, a “clean” function could
be learned. Thus if the noisy examples could be identiﬁed Figure 4: (a) Approximate maximum-margin learning with
a priori, we could learn the true maximum margin classiﬁer. outlier noise. ρk(h, D) returns the margin of the example
In some sense, many types of noise can be viewed as outlier in D that is smaller than the margin of all but k examples
noise, so the analysis presented in this section can be widely (i.e. the k +1-th smallest margin). (b) Simple algorithm. An
applied. We show a polynomial time algorithm for learning alternate description of the algorithm that simply doubles the
an approximate maximum margin hyperplane in the presence size of the sub-sample sets at each iteration.
of outliers.
                                                      expected coreset size. This provides an extremely simple
Deﬁnition 5.1 (Outlier Set) Consider a data set D =   polynomial-time algorithm for learning with noise.
        M
{(xi,yi)}i=m of binary (y ∈{−1, 1})examples. Forany
set of outliers, V ⊆ D, we can consider the maximum mar-
              D                                    Lemma 5.2  Let ρk =  ρ(L(D  \ Vk),D \ Vk) be the op-
gin hyperplane, h = L(D ), on the examples D = D \ V .                        D         M       k
                       k           V         k        timal margin for data set  of size   with   outliers.
Then an outlier set of size is a subset, k,ofsize that Given a parameter, , one can compute a separating hy-
achieves maximum margin on the remaining data
                                                      perplane with margin (1 − )ρk on D \ Vk in polynomial
                                                          O  dT c M c+1   M         c   O  R/ρ   2/
         Vk =argmaxρ(L(D     \ V ),D\ V ).            time  (  ( )     log   ),where  =   ((   k)   ). and
              V ∈D,|V |=k                             R =max(x,y)∈D  ||x||.

  Because a coreset exists for the “clean” data and we know Proof sketch:Because the clean data, D\Vk), contains a core-
                                                                           2
that there are at most k (or a ﬁxed fraction) outliers, we set of size c = O((R/ρk) /) from Lemma 3.1, it sufﬁces to
avoid exponential running time by subsampling based on the ﬁnd this set and observe that there are at most k outliers. It

                                                IJCAI-07
                                                   840