     Coping with exceptions in multiclass ILP problems using possibilistic logic

                    Mathieu Serrurier                               Henri Prade
            UPS, IRIT, 118 route de Narbonne,            UPS, IRIT, 118 route de Narbonne,
                  31062 Toulouse, France                       31062 Toulouse, France
                      serrurie@irit.fr                              prade@irit.fr


                    Abstract                          B ∪ D ∪ H  |= E.

    The handling of exceptions in multiclass problems   We extend propositional possibilistic logic [Dubois et al.,
    is a tricky issue in inductive logic programming  1994] to the ﬁrst-order case. Possibilistic logic is sound and
    (ILP). In this paper we propose a new formalization complete for refutation, using an extended resolution rule,
    of the ILP problem which accounts for default rea- with respect to a semantics in terms of a complete plausibil-
    soning, and is encoded with ﬁrst-order possibilistic ity preorder on the interpretations (encoded by a possibility
    logic. We show that this formalization allows us to distribution) [Dubois et al., 1994]. S denotes a set of Her-
    handle rules with exceptions, and to prevent an ex- brand interpretations. A possibility distribution can be de-
    ample to be classiﬁed in more than one class. The ﬁned on Herbrand interpretations as well. The possibilistic
    possibilistic logic view of ILP problem, can be eas- degree of a formula φ is the maximum possibility level of
    ily handled at the algorithmic level as an optimiza- its models and is denoted Π(φ). Necessity is the dual no-
    tion problem.                                     tion of possibility. It refers to the possibility degrees of the
                                                      counter-models of a formula :N(φ) = 1 − Π(¬φ). A possi-
1  Introduction                                       bilistic ﬁrst-order formula is a pair (φ, α) where φ is a ﬁrst-
                                                      order formula and α ∈]0, 1]. It is understood as a constraint
The handling of exceptions is a serious bottleneck in ILP due on an unknown necessity measure of the form N(φ) ≥ α.
to the formalization in ﬁrst-order logic. Indeed, when a rule Given a set K of possibilistic formulas, the α-cut of K is
has some exceptions, i.e. some examples are misclassiﬁed by
                                                      Kα  =  {φ|(φ, β) ∈ K, β ≥ α}. Given T a set of classi-
it, there is no way to compensate these exceptions by means cal ﬁrst-order formulas, T is minimal w. r. t. a formula φ
of another rule. So, a hypothesis accumulates all the excep- iff T |= φ and ∀ψ ∈ T, T − ψ 6|= φ. The following def-
tions of the rules that appear in it. Moreover, when dealing inition avoids the drowning problem (i.e., consequences are
with more than two classes, a rule with some exceptions may lost when their implicants are taken in inconsistent sets of
prevent another one to perform the right classiﬁcation. A formulas, although they don’t contribute to inconsistency) by
proper handling of exceptions can be made by adding to the
                                                      checking if it exists a proof of φ in Kα, which is free from
standard ILP setting a logical constraint that expresses that inconsistencies at level α.
an example can be only classiﬁed into one class. Thus, hav-
ing some exceptions may lead to inconsistency. In contrast Deﬁnition 1 Given K a set of possibilistic ﬁrst-order formu-
                                                                           ′         ′              ′
with ﬁrst-order logic, default reasoning encoded in possibilis- las, K |=π (φ, α) iff ∃K ⊂ Kα, K 6|= ⊥ such as K min-
                                                                            ′′
tic logic is well-suited for dealing with inconsistency. In this imal w. r. t. φ and ∄K minimal w. r. t. ⊥ such that
                                                        ′    ′′
paper, we extend the ILP settings in a possibilistic way in or- K ⊂ K ⊆ Kα.
der to handle inconsistency due to exceptions.
                                                      In practice, this deﬁnition makes sure that, if two logical con-
                                                      sequences are inconsistent, the one which has the highest ne-
2  Preliminaries                                      cessity degree is preferred. As already advocated in the in-
Stated in the general context of ﬁrst-order logic, the task troduction, we propose to use possibilistic logic in ILP for a
of ILP is to ﬁnd a non-trivial set of formulas H such that better handling of exceptions.
B ∪  H  |=  E given a background theory B and a set
of examples E of the form C(x, y) where x denotes the 3   Possibilistic ILP
identiﬁcation key of an example and y a class. E, B and H
here denote sets of Horn clauses. In order to treat multiple Given H a standard ILP hypothesis, let NH denote a func-
classiﬁcation of an example as inconsistency, we reformulate tion (called priority function) which at each rule in H asso-
the ILP problem, by adding the following classiﬁcation ciates a priority level, to be understood as a necessity de-
constraint D ≡ ∀X, Y, Z C(X, Y ) ∪ C(X, Z) → Y = Z,   gree. This gives birth to a possibilistic hypothesis Hp =
as follows : given B, D and E, the goal is to ﬁnd H such as {(h, NH (h)); h ∈ H}. Let Bp and Dp be composed by allformulas that appear respectively in B and D, with 1 as ne- (switching the order of rules two by two while accuracy in-
cessity level. The priority function NE over the examples is creases). Our algorithm is denoted as Pilp. Since Pilp is a
deduced from Hp as follows :                          non deterministic algorithm, the results that are presented are
                                                      average results on 50 running steps with the same settings.
                  , by convention, if α >
                 1                 6 ∃   0            The value in the brackets are standard derivations for the 50
                 such that Dp ∪ Bp ∪ Hp |=π (e, α)
                                                     results. For each experiment, two results are shown : Pilp
e ∈ E, NE(e) =                                      avg is the average result for the 50 tests, and Pilp max is the
                 max  α >  , D   B    H       e, α ,
                     {    0   p ∪  p ∪  p |=π (  )}   best result found in the 50 tests. The experimentation is made
                  otherwise.
                                                     with the ﬁnite element MESH Design dataset because it rep-
                                              (1)
                                                      resents a typical hard ILP multiple class problem. It describes
Then, given D, B and E, the goal of possibilistic ILP is to
                                                      the structure of a mesh with unary and binary predicates. The
ﬁnd H , composed by a classical hypothesis H and an asso-
     p                                                dataset contains 277 examples that describe 13 classes. The
ciated priority function N such that B D H      E
                     H           p ∪  p ∪  p |=π p    dataset is split in 5 sub datasets denoted by A, B, C, D and E.
with E     e, N  e   e   E . Note that, if it exists such
      p = {(   E ( )); ∈  }                           The test of the accuracy of the algorithm is made by testing on
H , this hypothesis will be correct and complete. This en-
 p                                                    one subset a hypothesis induced from the other sub datasets.
larges the scope of classical ILP by learning sets of default
                                                      Results are shown in the previous table. The results for the
rules in a framework that handles exceptions, the one of pos-
                                                      algorithms other than Pilp can be found in [Kietz, 2002]. The
sibilistic logic.
                                                      results are clearly in the favor of Pilp algorithm which in-
Any possibilistic hypothesis, even it contains some rules with
                                                      creases the number of examples covered by the most effective
exceptions, can be completed in order to be correct and com-
                                                      algorithm up to 10% in average and 20% at maximum.
plete by adding the misclassiﬁed examples with 1 as necessity
level. Then, the possibilistic ILP problem can be reformu-
lated as an optimization problem :                    5   Conclusion
Given D, B and E, the goal of possibilistic ILP is to ﬁnd In this paper, we have proposed a new ILP formalization for
Hp that maximizes the accuracy (i.e. the proportion of well- dealing with exceptions in a multiple class problem. In or-
classiﬁed examples). This deﬁnition of possibilistic ILP prob- der to do that, we have extended possibilistic propositional
lem is fully in agreement with the paradigm of the minimiza- logic to a ﬁrst-order setting. Then, by treating exceptions as
tion of the empirical risk. Since here, necessity levels are inconsistency, we have reformulate the ILP problem in ﬁrst-
only used for obtaining an ordering of the formulas in H, this order possibilistic logic. In this reformulation, the ILP prob-
induces equivalence classes of priority functions NH . Two lem is turned in an optimization problem. This formalization
                         ′                            allows our algorithm to learn sets of default rules. In this
priority functions NH and NH , belong to the same equiva-
                       ′                              context, it may exist a correct and complete hypothesis which
lence class, i.e. NH ≡ NH if and only if ∀h1, h2 ∈ H if
                       ′         ′                    contains rules that have some exceptions. Possibilistic ILP is
NH (h1) > NH (h2) then NH (h1) > NH (h2).
                                                      more ﬂexible and more general than ﬁrst-order decision lists
Proposition 1 Given H, ﬁnding the class of equivalence
                                                      [Mooney and Califf, 1995] and allows us to correctly cope
of priority functions such the accuracy is maximal is NP-
                                                      with recursive hypotheses. Experiments have proved that an
complete with respect to the number of formulas in H.
                                                      implementation of possibilistic ILP may be very effective for
It shows that, although using a possibilistic rather than the propositional or for relational learning and can compete with
classical setting is always more effective, ﬁnding the best pri- the best machine learning algorithms.
ority function over formulas in H may be computationally
very costly. Note that choosing a particular ordering based References
on the conﬁdence or the support degrees of the rules is not
                                                      [                ]
optimal in general. It suggests to use heuristics for inducing Dubois et al., 1994 D. Dubois, J. Lang, and H. Prade. Pos-
hypotheses together with their priority function.        sibilistic logic. In D. Gabbay et al., editor, Handbook of
                                                         Logic in Artiﬁcial Intelligence and Logic Programming,
4  Experimentations                                      vol. 3, pages 439–513. Oxford University Press, 1994.
                                                      [Kietz, 2002] J. U. Kietz. Learnability of description logic
             A    B     C      D     E
                                            P            programs. In S. Matwin and C. Sammut, editors, Proceed-
  Maximum    52   38    28     57    89     277          ings of the 12th Inter. Conference of Inductive Logic Pro-
  Foil       17   5     7      9     5      43           gramming, ILP 2002, pages 117–132. Springer, 2002.
  Indigo     21   14    9      18    33     95
  Cilgg      26   12    10     18    37     103       [Mooney and Califf, 1995] R. Mooney and M. E. Califf. In-
  Pilp avg   23   11.6  9.5    18.3  55.3   117.8        duction of ﬁrst-order decision lists: results on learning the
             [0]  [1.3] [0.5]  [3.6] [1.5]  [5.03]       past tense of english verbs. Journal of Artiﬁcial Intelli-
  Pilp max   23   13    10     23    57     125          gence Research, 3:1–24, 1995.
                                                      [Serrurier et al., 2004] M. Serrurier, H. Prade, and
In order to learn possibilistic logic rules, we learn a hypothe-
                                                         G. Richard.  A  simulated annealing framework for
sis directly together with its priority function. The hypotheses
                                                         ilp. In R. Camacho, R. King, and A. Srinivasan, editors,
are learnt by using a stochastic exploration of the hypothesis
                                                         Proceedings of ILP 2004, pages 288–304. LNAI 3194,
space as explained in [Serrurier et al., 2004]. The priority is
                                                         Springer, 2004.
found by using a greedy algorithm based on the 2-opt method