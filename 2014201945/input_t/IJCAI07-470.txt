         Semantic Smoothing of Document Models for Agglomerative Clustering 
                            Xiaohua Zhou, Xiaodan Zhang, Xiaohua Hu
                                           Drexel University
                             College of Information Science & Technology
            xiaohua.zhou@drexel.edu, xzhang@ischool.drexel.edu, thu@ischool.drexel.edu


                    Abstract                          analyze the underlying reasons of its poor performance and
                                                      propose a solution.
    In this paper, we argue that the agglomerative
                                                        Steinbach et al. (2000) argue that the agglomerative
    clustering with vector cosine similarity measure
                                                      hierarchical clustering perform poorly because the nearest
    performs poorly due to two reasons. First, the
                                                      neighbors of a document belong to different classes in many
    nearest neighbors of a document belong to different
    classes in many cases since any pair of documents cases. According to their examination on the data, each class
                                                      has a “core” vocabulary of words and remaining “general”
    shares lots of “general” words. Second, the sparsity
                                                      words may have similar distributions on different classes.
    of class-specific “core” words leads to grouping
                                                      Thus, two documents from different classes may share many
    documents with the same class labels into different
                                                      general words (e.g. stop words) and will be viewed similar
    clusters. Both problems can be resolved by suitable
                                                      in terms of vector cosine similarity. To solve this problem,
    smoothing of document model and using Kullback-
    Leibler divergence of two smoothed models as      we should “discount” general words and “emphasize” more
                                                      importance on core words in a vector. Besides, we think the
    pairwise document distances. Inspired by the recent
                                                      poor performance of the agglomerative clustering can also
    work in information retrieval, we propose a novel
                                                      be attributed to the sparsity of core words in a document. A
    context-sensitive semantic smoothing method that
                                                      document is often short and contains very few number of
    can automatically identifies multiword phrases in a
                                                      core words. Thus, two documents from the same class may
    document and then statistically map phrases to
    individual document terms. We evaluate the new    share few core words and be falsely grouped into different
                                                      clusters when using vector cosine similarity metric. To solve
    model-based similarity measure on three datasets
                                                      this problem, we should assign reasonable positive counts to
    using complete linkage criterion for agglomerative
                                                      “unseen” core words if its related topical terms occur in the
    clustering and find out it significantly improves the
                                                      document.
    clustering quality over the traditional vector cosine
    measure.                                            Discounting seen words and assigning reasonable counts
                                                      to unseen words are two exact goals of the probabilistic
                                                      language model smoothing. In this paper, we view the
1 Introduction                                        calculation of pairwise document similarity as a process of
                                                      document model smoothing and comparison. As usual, we
Document clustering algorithms can be categorized into use the Kullback-Leibler divergence distance function to
agglomerative and partitional approaches according to the measure the difference of two models (i.e. word probability
underlying clustering strategy (Kaufman and Rousseeuw, distributions). So the problem is reduced to obtaining a good
1990). The agglomerative approaches initially assign each smoothed language model for each document in the corpus.
document into its own cluster and repeatedly merge pairs of The language modeling approach to information retrieval
most similar clusters until only one cluster is left. The (IR) has been received much attention in recent years due to
partitional approaches iteratively re-estimate the cluster its mathematical foundation and empirical effectiveness. In
model (or the cluster centroid) and reassign each document a nugget, the language modeling approach to IR is to
into the closest cluster until no document is moved any smooth document models (Lafferty and Zhai, 2001). To the
longer. In comparison with partitional approaches, the best of our knowledge, the document model smoothing has
agglomerative approach does not need initialization and not been studied in the context of agglomerative clustering.
gives very intuitive explanation of why a set of documents In this paper, we adapt the existing smoothing methods used
                                                 2
are grouped together. However, it suffers from the O(n ) in language modeling IR to the context of agglomerative
clustering time and performs poorly in general in terms of clustering and hypothesize that document model smoothing
cluster quality (Steinbach et al., 2000). In this paper, we will


                                                IJCAI-07
                                                  2922can significantly improve the quality of the agglomerative (2005) adopted compound terms for text classification.
hierarchical clustering.                              However, compound terms are not used for smoothing
  In IR, a simple but effective smoothing strategy is to purpose in their work. Instead, compound terms are directly
interpolate document models with a background collection working as features in conjunction with single-word
model. For example, Jelinek-Mercer, Dirichlet, Absolute features. In our previous work (Zhou et al., 2006), we
discount (Zhai and  Lafferty, 2001) and  Two-stage    proposed a context-sensitive semantic smoothing method
smoothing (Zhai and Lafferty, 2002) are all based on this for language modeling IR. The method decomposes a
strategy. In document clustering, TF-IDF score is often used document into a set of weighted context-sensitive topic
as the dimension values of document vectors. The effect of signatures and then statistically maps topic signatures into
TF-IDF scheme is roughly equivalent to the background individual terms; a topic signature is defined as a pair of two
model smoothing. However, a potentially more significant concepts which are semantically and syntactically related to
and effective smoothing method is what may be referred to each other. The topic signature is similar to a compound
as semantic smoothing where context and sense information term in the sense that both have constant and unambiguous
are incorporated into the model (Lafferty and Zhai, 2001). meanings in most cases. For instance, if “star” and “movie”
The first trial of semantic smoothing may be dated back to forms a topic signature, its context may be highly related to
latent semantic indexing (LSI, Deerwester et al., 1990) “entertainment”, but rarely to “war”. The extraction of
which projects documents in corpus into a reduced space concepts and concept pairs, however, relies on domain
where document semantics becomes clear. LSI explores the ontology, which is impractical for many public domains.
structure of term co-occurrence and can solve synonymy. To overcome this limitation, we propose the use of
However, it brings noise while reducing the dimensionality multiword phrases (e.g. “star war”, “movie star”) as topic
because it is unable to recognize the polysemy of a same signatures in this paper. Same as a concept pair, a multiword
term in different contexts. In practice, it is also criticized for phrase is often unambiguous. Furthermore, multiword
the lack of scalability and interpretability.         phrases can be extracted from a corpus by existing statistical
  Berger and Lafferty (1999) proposed a kind of semantic approaches without human knowledge. Last, documents are
smoothing approach referred to as the statistical translation often full of multiword phrases; thus, it is robust to smooth a
language model which statistically mapped document terms document model through statistical translation of multiword
onto query terms.                                     phrases in a document to individual terms.
                                                        We evaluate the new model-based document similarity
           =
     p(q | d) t(q | w) p(w | d)               (1)    metric on three datasets using agglomerative clustering with
              w                                       complete linkage criterion (Kaufman and Rousseeuw,
where      is the probability of translating the document
      t(q | w)                                        1990). The experiment results show that the KL-divergence
term w to the query term q and p(w | d) is the maximum similarity metric performs consistently better than the vector
likelihood estimator of the document model. With term cosine metric. Moreover, the KL-divergence metric with
translations, a document containing “star” may be returned semantic smoothing significantly outperforms with simple
for the query “movie”. Likewise, a document with the  background smoothing. The result of the agglomerative
dimension of “star” but not “movie” may be merged into a clustering with semantic smoothing is comparable to that of
cluster of “entertainment” together with a document   the K-Means partitional clustering on three testing datasets.
containing “movie” but not “star”. However, like the LSI,

this approach also suffers from the context-insensitivity  Vt                     Vd             Vw
problem, i.e., unable to incorporate contextual information
                                                          t
into the model. Thus, the resulting translation may be fairly 1                    D1             w1
general and contain mixed topics. For example, “star” can t2
be either from the class of “entertainment” (movie star) or                        D2             w2
                                                          t
from the class of “military” (star war).                   3
                                                                                   D3             w3
  Unlike Berger and Lafferty (1999) who estimated word    t4

translation probabilities purely based on word distributions t5                    D4             w4
in a corpus, Cao et al. (2005) constrained word relationships

with human knowledge (i.e. relationships defined in   Figure 1. Illustration of document indexing. VBtB, VBdB and VBwB
WordNet) in order to reduce noise. They further combined are phrase set, document set and word set, respectively.
linearly such a semantic-constrained translation model with
a smoothed unigram  document model. However, their    2 Document Model Smoothing
model still did not solve the context-insensitivity problem in
essence.                                              2.1 Semantic Smoothing of Document Model
  Compound terms often play an important role for a   Suppose we have indexed all documents in a given
machine to understand the meaning of texts because they collection C with terms (individual words) and topic
usually have constant and unambiguous meanings. Bai et al. signatures (multiword phrases) as illustrated in Figure 1.


                                                IJCAI-07
                                                  2923The translation probabilities from a topic signature tk to any the experiment, the four parameters are set to 1, 1, 4, and
individual term w, denoted as p(w|tk), are also given. Then 0.75, respectively.
we can easily obtain a document model below:
                                                      Table 1. Examples of phrase-word translations. The three phrases
            =
    pt (w | d)  p(w | tk ) pml (tk | d)      (2)     are automatically extracted from the collection of 20-newsgroup by
              k                                       Xtract. We list the top 20 topical words for each phrase.
The likelihood of a given document generating the topic  Arab Country     Nuclear Power     Gay People
                                                         Term    Prob.     Term    Prob.   Term     Prob.
signature tk can be estimated with
                                                       Arab      0.061 nuclear     0.084 gay        0.078
                     c(t ,d)                           country   0.048 power       0.046 homosexual 0.052
                  =    k
         pml (tk | d)                         (3)      Israel    0.046 plant       0.039 sexual     0.046
                    c(ti ,d)                          Jew       0.037 technology  0.030 church     0.027
                     i                                 Israeli   0.032 air         0.026 persecute  0.023
                                                       Jewish    0.024 fuel        0.025 friend     0.023
where c(t ,d) is the frequency of the topic signature tBiB in a
        i                                              Palestine 0.020 fossil      0.025 Abolitionist 0.019
given document d.                                      1948      0.018 reactor     0.024 parent     0.019
  We refer to the above model as translation model after Syria   0.018 steam       0.024 Society    0.019
Berger and Lafferty’s work (1999). As we discussed in the expel  0.016 contaminate 0.024 lesbian    0.019
introduction, the translation from multiword phrase to terror    0.015 water       0.024 themselves 0.019
individual term would be very specific. Thus, the translation Iraq 0.015 cold      0.023 lover      0.018
model not only weakens the effect of “general” words, but Davidsson 0.014 Cool     0.022 lifestyle  0.018
                                                       War       0.013 Tower       0.022 emotion    0.018
also relieves the sparsity of class-specific “core” words. homeland 0.013 industry 0.022 thier      0.018
However, not all topics in a document can be expressed by Egypt  0.013 radioactive 0.020 repress    0.018
topic signatures (i.e., multiword phrases). If only translation Zionist 0.013 boil 0.020 affirm     0.018
model is used, there will be serious information loss. A legitimism 0.012 site     0.019 Ministry   0.017
natural extension is to interpolate the translation model with Kaufman 0.012 built 0.019 straight   0.017
a unigram language model below:                        rejoinder 0.012 temperature 0.018 preach     0.017

    p (w | d) = (1 ) p (w | d) + p(w | C)  (4)       For each phrase t BkB, we have a set of documents (D BkB)
     b              ml                                containing that phrase. Intuitively, we can use this document

Here _ is a coefficient accounting for the background set D B kB to estimate the translation probabilities for tBkB, i.e.,

                                                      determining the probability of translating the given phrase tB B
collection model p(w | C) and pml (w | d) is a maximum                                                  k
likelihood estimator. In the experiment, _ is set to 0.5. We to terms in the vocabulary. If all terms appearing in the
refer to this unigram model as simple language model or document set center on the sub-topic represented by tB kB,we
                                                      can simply use the maximum likelihood estimator and the
baseline language model. We use Jelinek-Mercer smoothing
                                                      problem is as simple as term frequency counting. However,
on the purpose of further discounting “general” words.
                                                      some terms address the issue of other sub-topics while some
  The final document model for clustering use is described
                                                      are background terms of the whole collection. We then use a
in equation (5). It is a mixture model with two components:
                                                      mixture language model to remove noise. Assuming the set
a simple language model and a translation model.

                                                      of documents containing tBkB is generated by a mixture
            =            + 
    pbt (w | d) (1- ) pb (w | d) pt (w | d )   (5)    language model (i.e., all terms in the document set are either
                                                      translated by the given topic signature model p(w | ) or
The translation coefficient (_) is to control the influence of                                       tk
two components in the mixture model. With training data, generated by the background collection model p(w | C) ), we
the translation coefficient can be trained by optimizing the have:
clustering quality.                                       p(w | ,C) = (1  ) p(w | ) + p(w | C)  (6)
                                                                tk                tk
2.2 Topic Signature Extraction and Translation        where _ is a coefficient accounting for the background noise
                                                      and  denotes parameter set of translation probabilities for
Zhou et al (2006) implemented topic signatures as concept  tk

pairs and developed an ontology-based approach to extract tBkB. Under this mixture language model, the log likelihood of

concepts and concept pairs from documents. However, for generating the document set DBkB is:
many domains, ontology is not available. For this reason,
                                                          log p(D | ,C) = c(w, D )log p(w | ,C)  (7)
we propose the use of multiword phrases as topic signatures      k  tk            k          tk
and employ Xtract (Smadja, 1993) to identify phrases in                    w
documents. Xtract is a kind of statistical extraction tool with where c(w, Dk ) is the document frequency of term w in D B k.B,

some syntactic constraints. It is able to extract noun phrases i.e., the cooccurrence count of w and t B kB in the whole
frequently occurring in the corpus without any external collection. The translation model can be estimated using the

knowledge. Xtract uses four parameters, strength (kB0B), peak EM algorithm (Dempster et al., 1977). The EM update

z-score (kB1B), spread (UB0B), and percentage frequency (T), to formulas are:
control the quantity and quality of the extracted phrases. In


                                                IJCAI-07
                                                  2924                       (n)                         dataset normalized by the arithmetic mean of the maximum
                  (1   ) p (w | t )
    pˆ (n) (w) =                k              (8)    possible entropies of the empirical marginals, i.e.,
             (1  ) p (n) (w | ) + p(w | C)
                           tk
                             (n)                                           I(X ;Y )
       +            c(w, D ) pˆ (w)                       NMI(X  ,Y ) =                            (12)
    p (n 1) (w | ) =     k                   (9)                           +
              tk               (n)                                     (logk  logc) / 2
                  c(wi  , Dk ) pˆ (wi )
                   i                                  where X is a random variable for cluster assignments, Y is a
In the experiment, we set the background coefficient _=0.5. random variable for the pre-existing labels on the same data,
We also truncate terms with extremely small translation k is the number of clusters, and c is the number of pre-
probabilities for two purposes. First, with smaller number of existing classes. Regarding the details of computing I(X; Y),
translation space, the document smoothing will be much please refer to (Banerjee and Ghosh, 2002). NMI ranges
more efficient. Second, we assume terms with extremely from 0 to 1. The bigger the NMI is the higher quality the
small probability are noise (i.e. not semantically related to clustering is. NMI is better than other common extrinsic
the given topic signature). In detail, we disregard all terms measures such as purity and entropy in the sense that it does
with translation probability less than 0.001 and renormalize not necessarily increase when the number of clusters
the translation probabilities of the remaining terms. increases.
                                                        We take complete linkage criterion for agglomerative
2.3 The KL-Divergence Distance Metric                 hierarchical clustering. The two document similarity metrics
After estimating a language model for each document in the are the traditional vector cosine and the Kullback-Leibler
corpus with context-sensitive semantic smoothing, we use divergence proposed in this paper. For cosine similarity, we
the Kullback-Leibler divergence of two language models as try three different vector representations: term frequency
the distance measure of the corresponding two documents. (tf), normalized term frequency (i.e., tf divided by the vector
Given two probabilistic document models p(w|d1) and   length), and TF-IDF. For KL-divergence metric, we use
p(w|d2), the KL-divergence distance of p(w|d1) to p(w|d2) is document models with semantic smoothing as described in
defined as:                                           equation (5) and test 11 translation coefficients (_) ranging
                                                      from 0 to 1. When _=0, it actually uses simple background
            =                  p(w | d1 )            smoothing.
     (d1, d 2 )   p(w | d1 ) log             (10)
                wV             p(w | d 2 )             In order to compare with the partitional approach, we also
where V is the vocabulary of the corpus. KL-divergence implement a basic K-Means using cosine similarity metric
distance will be a non-negative score. It gets the zero value on three vector representations (TF, NTF, and TF-IDF). The
if and only if two document models are exactly same.  calculation of the cluster centroid uses the following
However, KL-divergence is not a symmetric metric. Thus, formula:
we define the distance of two documents as the minimum of                     1
two KL-divergence distances. That is,                               centroid =  d                  (13)
                                                                             C  dC
              =             
    dist(d1 ,d 2 ) min{ (d1 ,d 2 ), (d 2 ,d1 )} (11)
                                                      where C  is the corpus. Since the result of K-Means
  The calculation of KL-divergence involves scanning the clustering varies with the initialization. We run ten times
vocabulary, which makes the solution computationally  with random  initialization and average the results. For
inefficient. To solve this problem, we truncate terms with its various vector representations, each run has the same
distribution probability less than 0.001 while estimating initialization.
document model using the equation (5) and renormalize the
probabilities of remaining terms. Because we keep terms 3.2 Datasets
with high probability values in document models, it makes We conduct clustering experiments on three datasets: TDT2,
almost no difference in clustering results.           LA Times (from TREC), and 20-newsgroups (20NG). The
                                                      TDT2 corpus has 100 document classes, each of which
3 Experiment Settings and Result Analysis             reports a major news event. LA Times news are labeled with
                                                      21 unique section names, e.g., Financial, Entertainment,
3.1 Evaluation Methodology                            Sports, etc. 20-Newsgroups dataset is collected from 20
Cluster quality is evaluated by three extrinsic measures, different Usenet newsgroups, 1,000 articles from each.
purity (Zhao and Karypis, 2001), entropy (Steinbach et al., We index 9,638 documents in TDT2 that have a unique
2000) and normalized mutual information (NMI, Banerjee class label, 21,623 documents from top ten sections of LA
and Ghosh, 2002). Due to the space limit, we only list the Times, and all 19,997 documents in 20-newsgroups. For
result of NMI, an increasingly popular measure of cluster each document, we index its title and body content with
                                                      both multiword phrases and individual words, and ignore
quality. The other two measures are consistent with NMI on
                                                      other sections including Meta data. A list of 390 stop words
all runs. NMI is defined as the mutual information between
                                                      is used. In the testing stage, 100 documents are randomly
the cluster assignments and a pre-existing labeling of the
                                                      picked from each class of a given dataset and merged into a


                                                IJCAI-07
                                                  2925big pool for clustering. For each dataset, we create five such general, NMI will increase with the increase of translation
random pools and average the experimental results. The ten coefficient till the peak point (around 0.8 in our case) and
classes selected from TDT2 are 20001, 20015, 20002,   then go downward. In our experiment, we only consider
20013, 20070, 20044, 20076, 20071, 20012, and 20023.  phrases appearing in more than 10 documents as topic
The ten sections selected from LA Times are Entertainment, signatures in order to obtain a good estimate of translation
Financial, Foreign, Late Final, Letters, Metro, National, probabilities. Moreover, not all topics in a document can be
Sports, Calendar, and View. All 20 classes of 20NG are expressed by multiword phrases. Thus, the phrase-based
selected for testing.                                 semantic smoothing will cause information loss. We
                                                      interpolate the translation model with a unigram language
Table 2. Statistics of three datasets                 model to make up the loss. Now it is easy to understand
 Dataset Name          TDT2     LA Times  20NG        why the NMI goes downward when the influence of the
 # of indexed docs     9,638    21,623    19,997      semantic smoothing is too high. Actually, LSI also causes
 # of words            37,994   63,510    140,269     information loss when the dimensionality reduction is too
 # of phrases          8,256    9,517     10,902      aggressive; but there is no mechanism to recover the loss. In
 Avg. doc length (word) 240     103       193         this sense, the semantic smoothing approach is more flexible
 Avg. doc length (phrase) 21    10        10          than LSI.

 # of classes          10       10        20              0.8
                                                          0.7
3.3 Experiment Results and Analysis                       0.6
The Dragon Toolkit [Zhou et al., 2006] is used to conduct 0.5
                                                          0.4
                                                         NMI
clustering experiments. The translation coefficient (_) in 0.3
equation (5) is trained over TDT2 dataset by maximizing the 0.2
NMI of clustering. The optimal value _=0.8 is then applied 0.1
to two other datasets. The NMI result of the agglomerative 0.0
hierarchical clustering with complete linkage criterion is    0   0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
                                                                    Translation Coefficient (?) TDT2
listed in Table 3. When the vector cosine measure is used as                                    20NG
pairwise document similarity, the TF-IDF scheme performs      Model-based Agglomoerative Clustering LATimes
slightly better than the TF scheme. As we discussed before,
the heuristic TF-IDF weighting scheme can discount    Figure 2. The variance of the cluster quality with the translation
“general” words and strengthen “specific” words in a  coefficient (_) which controls the influence of semantic smoothing
document vector. Thus, it can improve the agglomerative
clustering quality. The KL-divergence similarity measure Steinbach et al. (2000) reported that K-Means performed
with background smoothing of document models (i.e., _=0) as good as or better than agglomerative approaches. Our
consistently outperforms the cosine measure on both TF and experiment also repeated this finding. Using vector cosine
TF-IDF schemes. As expected, the KL-divergence measure similarity measure, the complete linkage algorithm performs
with context-sensitive semantic smoothing significantly significantly worse than the K-Means (see table 3 and 4).
improves the quality of the agglomerative clustering on all However, with semantic smoothing of document models,
three datasets. After semantic smoothing, the class-  the result of complete linkage clustering is comparable to
independent general words will be dramatically weakened that of K-Means on three representation schemes (TF, Norm
and the class-specific “core” words will be strengthened TF, and TF-IDF). This is also a kind of indication that
even if it does not appear in the document at all. Thus, the semantic smoothing of document models is very effective in
distance of intra-class documents will be decreased while improving agglomerative clustering approaches.
the distance of inter-documents will be increased, and hence
improve the clustering quality.                       Table4. NMI results of the regular K-Means clustering. K is the
                                                      number of true classes listed in table 2.
Table 3. NMI results of the agglomerative hierarchical clustering Dataset TF     NTF          TF-IDF
with complete linkage criterion
                                                       TDT2        0.792         0.805        0.790
                 Cosine           KL-Divergence
  Dataset                                              LA Times    0.194         0.197        0.166
            TF/NTF    TF-IDF   background Semantic
                                                       20NG        0.197         0.161        0.374
 TDT2      0.369     0.484     0.550      0.743
 LA Times  0.059     0.054     0.104      0.202       4 Conclusions and Future Work
 20NG      0.135     0.135     0.155      0.216
                                                      The quality of agglomerative hierarchical clustering often
  To see the robustness of the semantic smoothing method, highly depends on pairwise document similarity measures.
we show the performance curve in Figure 2. Except for the The density of class-independent “general” words and the
point of _=1, the semantic smoothing always improve the sparsity of class-specific “core” words in documents make
cluster quality over the simple background smoothing. In the traditional vector cosine a poor similarity measure for


                                                IJCAI-07
                                                  2926