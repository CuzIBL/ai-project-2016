               Hierarchical Hidden Markov Models for Information Extraction 

           Marios Skounakis+t                              Mark Craven**                                Soumya Ray** 
           marios@cs.wisc.edu                        craven@biostat.wisc.edu                           sray@cs.wisc.edu 

                      * Department of Computer Sciences                    Department of Biostatistics & Medical Informatics 
                            University of Wisconsin                                    University of Wisconsin 
                          Madison, Wisconsin 53706                                    Madison, Wisconsin 53706 

                           Abstract 
                                                                           Here we report the identification of an integral membrane 
     Information extraction can be defined as the task                     ubiquitin-conjugating enzyme. This enzyme, UBC6, local•
     of automatically extracting instances of specified                    izes to the endoplasmic reticulum, with the catalytic domain 
     classes or relations from text. We consider the case                  facing the cytosol. 
     of using machine learning methods to induce mod•
     els for extracting relation instances from biomedi•
     cal articles. We propose and evaluate an approach 
     that is based on using hierarchical hidden Markov                        subcellular-localization(UBC6,endoplasmic reticulum) 
     models to represent the grammatical structure of 
     the sentences being processed. Our approach first                  Figure 1: An example of the information extraction task. The top of 
     uses a shallow parser to construct a multi-level rep•              the figure shows part of a document from which we wish to extract 
     resentation of each sentence being processed. Then                 instances of the SUbcellular-localization relation. The bottom of 
     we train hierarchical HMMs to capture the regu•                    the figure shows the extracted tuple. 
     larities of the parses for both positive and negative 
     sentences. We evaluate our method by inducing 
     models to extract binary relations in three biomedi•               role. In contrast, the task we consider here is extracting infor•
     cal domains. Our experiments indicate that our ap•                 mation from abstracts of biological articles [Hirschman et a/., 
     proach results in more accurate models than several                2002]. In this domain, it is important that the learned models 
     baseline HMM approaches.                                           are able to represent regularities in the grammatical structure 
                                                                        of sentences. 
                                                                           In this paper, we present an approach based on using hier•
1 Introduction                                                          archical hidden Markov models (HHMMs) [Fine etal, 1998] 
In many application domains, there is the potential to greatly          to extract information from the scientific literature. Hierar•
increase the utility of on-line text sources by using automated         chical hidden Markov models have multiple "levels" of states 
methods for mapping selected parts of the unstructured text             which describe input sequences at different levels of granu•
into a structured representation. For example, the curators of          larity. In our models, the top level of the HMMs represent 
genome databases would like to have tools that could accu•              sentences at the level of phrases, and the lower level of the 
rately extract information from the scientific literature about         HMMs represent sentences at the level of individual words. 
entities such as genes, proteins, cells, diseases, etc. For this        Our approach involves computing a shallow parse of each 
reason, there has been much recent interest in developing               sentence to be processed. During training and testing, the 
methods for the task of information extraction (IE), which              hierarchical HMMs manipulate a two-level description of the 
can be defined as automatically recognizing and extracting in•          sentence parse, instead of just processing the sentence words 
stances of specific classes of entities and relationships among         directly. We evaluate our approach by extracting instances of 
entities from text sources.                                             three binary relations from abstracts of scientific articles. Our 
  Machine learning methods often play a key role in IE sys•             experiments show that our approach results in more accurate 
tems because it is difficult and costly to manually encode              models than several baseline approaches using HMMs. 
the necessary extraction models. Hidden Markov models                      An example of a binary relation that we consider in our 
(HMMs) [Leek, 1997; Bikel et al, 1999; Freitag and McCal-               experiments is the subcellular-localization relation, which 
lum, 2000], and related probabilistic sequence models [Mc-              represents the location of a particular protein within a cell. 

Callum et at, 2000; Lafferty et al.y 2001], have been among             We refer to the domains of this relation as PROTEIN and 
the most accurate methods for learning information extrac•              LOCATION. We refer to an instance of a relation as a tuple. 
tors. Most of the work in learning HMMs for information ex•             Figure 1 provides an illustration of our extraction task. The 
traction has focused on tasks with semi-structured and other            top of the figure shows two sentences in an abstract, and the 
text sources in which English grammar does not play a key               bottom of the figure shows the instance of the target relation 


INFORMATION EXTRACTION                                                                                                                 427  subcellular-localization that we would like to extract from 
 the second sentence. This tuple asserts that the protein UBC6 
 is found in the subcellular compartment called the endoplas•
 mic reticulum. In order to learn models to perform this task, 
 we use training examples consisting of passages of text, an•
 notated with the tuples that should be extracted from them. 
    In earlier work [Ray and Craven, 2001], we presented 
 an approach that incorporates grammatical information into 
 single-level HMMs. The approach described in this paper ex•
 tends the earlier work by using hierarchical HMMs to provide 
 a richer description of the information available from a sen•
 tence parse. 
    Hierarchical HMMs originally were developed by Fine et 
 al. (1998), but the application of these models to information 
 extraction is novel, and our approach incorporates several ex•
 tensions to these models to tailor them to our task. Bikel el 
 al. (1999) developed an approach to named enlily recognition 
 that uses HMMs with a multi-level representation similar to 
 a hierarchical HMM. In their models, the top level represents           Figure 2: Input representation for a sentence which contains a 
 the classes of interest (e.g. person name), and the bottom              subcellular-localization tuple: the sentence is segmented into 
 level represents the words in a sentence being processed. Our           typed phrases and each phrase is segmented into words typed with 
 approach differs from theirs in several key respects: (i) our in•       part-of-speech tags. Phrase types and labels arc shown in column 
 put representation for all sentences being processed is hierar•         (a). Word part-of-speech tags and labels arc shown in column (b). 
 chical, (ii) our models represent the shallow phrase structure          The words of the sentence are shown in column (c). Note the group•
 of sentences, (iii) we focus on learning to extract relations           ing of words in phrases. The labels (PROTEIN, LOCATION) are 
                                                                         present only in the training sentences. 
 rather than entities, (iv) we use null models to represent sen•
 tences that do not describe relations of interest, and (v) we 
 use a discriminative training procedure. Miller el al. (2000)           subcellular-localization relation and its annotated segments. 
 developed an information-extraction approach that uses a lex-           The sentence is segmented into typed phrases and each phrase 
 icalized, probabilistic context-free grammar (LPCFG) to si•             is segmented into words typed with part-of-speech tags. 
 multaneously do syntactic parsing and semantic information              For example, the second phrase segment is a noun phrase 
 extraction. The genre of text that we consider here, however,           (NP.SEGMENT) that contains the protein name UBC6 (hence 
 is quite different from the news story corpus on which avail•           the PROTEIN label). Note that the types are constants that 
 able LPCFGs have been trained. Thus it is not clear how well            are pre-defined by our representation of Sundance parses, 
this intriguing approach would transfer to our task.                     whereas the labels are defined by the domains of the particu•
                                                                         lar relation we are trying to extract. 
2 Sentence Representation 
                                                                         3 Hierarchical HMMs for Information 
In most previous work on HMMs for natural language tasks, 
the passages of text being processed have been represented                    Extraction 
as sequences of tokens. A hypothesis underlying our work is              A schematic of one of our hierarchical HMMs is shown in 
that incorporating sentence structure into the learned models            Figure 3. The top of the figure shows the positive model, 
will provide better extraction accuracy. Our approach is based           which is trained to represent sentences that contain instances 
on using syntactic parses of all sentences to be processed.              of the target relation. The bottom of the figure shows the null 
In particular, we use the Sundance system [Riloff, 1998] to              model, which is trained to represent sentences that do not 
obtain a shallow parse of each given sentence.                           contain relation instances (e.g. off-topic sentences). At the 
   The representation we use in this paper does not incorpo•             "coarse" level, our hierarchical HMMs represent sentences 
rate all of the information provided by the Sundance parser.             as sequences of phrases. Thus, we can think of the top level 
Instead our representation provides a partially "flattened",             as an HMM whose states emit phrases. We refer to this HMM 
two-level description of each Sundance parse tree. The top               as the phrase HMM, and its states phrase states. At the "fine" 
level represents each sentence as a sequence of phrase seg•              level, each phrase is represented as a sequence of words. This 
ments. The lower level represents individual tokens, along               is achieved by embedding an HMM within each phrase state. 
with their part-of-speech (POS) tags. In positive training ex•           We refer to these embedded HMMs as word HMMs and their 
amples, if a segment contains a word or words that belong to             states as word states. The phrase states in Figure 3 are de•
a domain in a target tuple, the segment and the words of in•             picted with rounded rectangles and word states are depicted 
terest are annotated with the corresponding domain. We iefer             with ovals. To explain a sentence, the HMM would first fol•
to these annotations as labels. Test instances do not contain            low a transition from the START state to some phrase state qi, 
labels - the labels are to be predicted by the learned IE model.         use the word HMM of qi to emit the first phrase of the sen•
   Figure 2 shows a sentence containing an instance of the               tence, then transition to another phrase state qj, emit another 


428                                                                                                       INFORMATION EXTRACTION                                                                          Figure 4: Architectures of the word HMMs for the subcellular-
                                                                         localization relation. Bold text within states denotes domain labels. 
                                                                         For states with implicit empty labels, italicized text within paren•
                                                                         theses denotes the position of the state's emissions relative to the 
 Figure 3: Schematic of the architecture of* a hierarchical HMM for 
                                                                         domain words. The figure shows (a) the structure of the embedded 
 the subcellular-localization relation. The top part of the figure 
                                                                         HMMs for phrase states without labels, (b), phrase states with one 
 shows the positive model and the bottom part the null model. Phrase 
                                                                         label and (c) phrase states with two labels. 
 states are depicted as rounded rectangles and word states as ovals. 
 The types and labels of the phrase states are shown within rectangles 
 at the bottom right of each state. Labels are shown in bold and states  HMM shown in Figure 4(b) can explain the phrase "the endo•
 associated with non-empty label sets are depicted with bold borders. 
                                                                         plasmic reticulum " by following a transition from the START 
 The labels of word states are abbreviated for compactness. 
                                                                         state to the (before) state, emitting the word "the", transition•
                                                                         ing to the LOCATION state, emitting the words "endoplas-

 phrase using the word HMM of q3 and so on until it moves                mic" and "reticulum" with the LOCATION label and then 
 to the END state of the phrase HMM. Note that only the word             transitioning to the END state. In order for a phrase state to 
 states have direct emissions.                                           emit a whole phrase, as given by the input representation, 
   Like the phrases in our input representation, each phrase             and not sequences of words that are shorter or longer than 
 state in the HMM has a type and may have one or more labels.            a phrase, we require that the embedded word HMM transi•
 Each phrase state is constrained to emit only phrases whose             tion to the end state exactly when it has emitted all the words 
 type agrees with the state's type. We refer to states that have         of a given phrase. Thus word HMMs will always emit se•
 labels associated with them as extraction states, since they            quences of words that constitute whole phrases and transi•
 are used to predict which test sentences should have tuples             tions between phrase states occur only at phrase boundaries. 
extracted from them.                                                        The standard dynamic programming algorithms that are 
   The architectures of the word HMMs are shown in Fig•                  used for learning and inference in HMMs - Forward, Back•
ure 4. We use three different architectures depending on the             ward and Viterbi [Rabiner, 1989] - need to be slightly mod•
 labels associated with the phrase state in which the word               ified for our hierarchical HMMs. In particular, they need to 
HMM is embedded. The word HMMs for the phrase states                     (i) handle the multiple-levels of the input representation, en•
with empty label sets (Figure 4(a)) consist of a single emit•            forcing the constraint that word HMMs must emit sequences 
ting state with a self-transition. For the extraction states of          of words that constitute phrases, and (ii) support the use of 
the phrase HMM, the word HMMs have a specialized archi•                  typed phrase states by enforcing agreement between state and 
tecture with different states for the domain instances, and for          phrase types. 
the words that come before, between and after the domain                    The Forward algorithm for our hierarchical HMMs is de•
instances (Figures 4(b) and 4(c)). All the states of the word            fined by the recurrence relationships shown in Table 1. The 
HMMs can emit words of any type (part-of-speech). That is,               first three equations of the recurrence relation provide a 
they are untyped, in contrast to the typed phrase states. The            phrase-level description of the algorithm, and the last three 
word states are annotated with label sets, and are trained to            equations provide a word-level description. Notice that the 
emit words with identical label sets. For example, the word              third equation describes the linkage between the phrase level 


INFORMATION EXTRACTION                                                                                                                  429  Tabic 1: The left side of the table shows the Forward-algorithm recurrence relation for our hierarchical HMMs. The right side of the table 
 defines the notation used in the recurrence relation. 

 and the word level. The Backward and Viterbi algorithms re•   the null models (shown in Figure 3) as its two submodels, 
 quire similar modifications, but we do not show them due to   with shared START and END states. 
 space limitations.                                              Once a model has been trained, we can use the Viterbi al•
   As illustrated in Figure 2, each training instance for our  gorithm to predict tuples in test sentences. We extract a tuple 
 HMMs consists of a sequence of words, segmented into          from a given sentence if the Viterbi path goes through states 
 phrases, and an associated sequence of labels. For a test in• with labels for all the domains of the relation. For example, 
stance, we would like our trained model to accurately pre•     for the SUbcellular-localization relation, the Viterbi path for 
dict a sequence of labels given only the observable part of the a sentence must pass through a state with the PROTEIN la•
sentence (i.e. the words and phrases). We use a discrimina•    bel and a state with the LOCATION label. This process is 
tive training algorithm [Krogh, 1994] that tries to find model illustrated in Figure 6. 
parameters, 0, to maximize the conditional likelihood of the 
labels given the observable part of the sentences:             4 Hierarchical HMMs with Context Features 

                                                        (1)    In this section we describe an extension to the hierarchical 
                                                               HMMs presented in the previous section that enables them to 
                                                               represent additional information about the structure of sen•
Here sl is the sequence of words/phrases for the Zth instance, tences within phrases. We refer to these extended HMMs 
and c1 is the sequence of labels for the instance. This training as Context hierarchical HMMs (CHHMMs). Whereas the 
algorithm will converge to a local maximum of the objective    hierarchical HMMs presented earlier partition a sentence s 
function. We initialize the parameters of our models by first  into disjoint observations where each is a word, a 
doing standard generative training. We then apply Krogh's      CHHMM represents s as a sequence of overlapping obser•
algorithm which involves iterative updates to the HMM pa•      vations . Each observation consists of a window of 
rameters. To avoid overfitting, we stop training when the ac•  three words, centered around together with the part-
curacy on a held-aside tuning set is maximized.                of-speech tags of these words. Formally, is a vector 
   In order for this algorithm to be able to adjust the param•                                            where is the 
eters of the positive model in response to negative instances  part-of-speech tag of word 8ij. Note that and 
and vice-versa, we join our positive and null models as shown  share although these features 
in Figure 5. This combined model includes the positive and     are located in different positions in the two vectors. Figure 7 
                                                               shows the vectors emitted for the phrase "the endoplasmic 
                                                               reticulum" by a word HMM in the CHHMM. 
                                                                 Using features that represent the previous and next words 
                                                               allows the models to capture regularities about pairs or triplets 
                                                               of words. For instance, a CHHMM is potentially able to 
                                                               learn that the word "membrane " is part of a subcellular loca•
                                                               tion when found in "plasma membrane" while it is not when 
                                                               found in "a membrane". Furthermore, by using features that 
                                                               represent the part-of-speech of words, the models are able to 
Figure 5: Architecture of the combined model. The positive and null learn regularities about groups of words with the same part 
models refer to the models in Figure 3.                        of speech in addition to regularities about individual words. 


430                                                                                         INFORMATION EXTRACTION                                 Extracted tuples: subcellular-localization(MAS20, mitochondria) 
                                                          subcellular-localization(MAS22, mitochondria) 

Figure 6: Example of the procedure for extracting tuples of the subcellular-localization relation from the sentence fragment "...MAS20 
and MAS22 are found in the mitochondria...". The top of the figure shows how the most likely path explains the sentence fragment. Bold 
transitions between states denote the most likely path. Dashed lines connect each state with the words that it emits. The table shows the label 
sets that are assigned to the phrases and the words of the sentence. The extracted tuples arc shown at the bottom of the figure. 


                                                                         where Ek(oijkIqa,b) is the probability of word state qa,b 
                                                                         emitting an observation whose A;-th feature is Oij^-
                                                                            Note that the features employed by the representation of 
                                                                         Equation 2 are clearly not conditionally independent. Con•
                                                                         secutive words are not independent of one another and cer•
                                                                         tainly the part-of-speech tag of a word is not independent of 
                                                                         the word itself. However, we argue that the discriminative 
Figure 7: Generation of the phrase "the endoplasmic reticulum" by        training algorithm we use [Krogh, 1994] can compensate in 
a word HMM in a CHHMM. The bold arcs represent the path that             part for this violation of the independence assumption. 

generates the phrase. The vector observations oi,J emitted by each 
state are shown in the rectangles above the model and are connected      5 Empirical Evaluation 
with dotted arcs with the emitting state. The word that would be 
emitted by each state of the equivalent HHMM is shown in boldface.       In this section we present experiments testing the hypothesis 
                                                                         that our hierarchical HMMs are able to provide more accurate 
                                                                         models than HMMs that incorporate less grammatical infor•
                                                                         mation. In particular we empirically compare two types of 
The advantages of this representation are especially realized            hierarchical HMMs with three baseline HMMs. 
when dealing with an out-of-vocabulary word; in this case                   • Context HHMMs: hierarchical HMMs with context 
part-of-speech tags and neighboring words may be quite in•                     features, as described in the previous section. 
formative about the meaning and use of the out-of-vocabulary 
                                                                            • HHMMs: hierarchical HMMs without context features. 
word. For example, an out-of-vocabulary adjective will rarely 
be a protein, since proteins are usually nouns.                             • Phrase HMMs: single-level HMMs in which states are 
                                                                              typed (as in the phrase level of an HHMM) and emit 
   Because the number of possible observations for a given                    whole phrases. These HMMs were introduced by Ray 
word state in a CHHMM is very large (all possible vectors                     and Craven (2001). Unlike hierarchical HMMs, the 
representing sequences of three words and their POS tags), to                 states of Phrase HMMs do not have embedded HMMs 
model the probability of an observation our CHHMMs                            which emit words. Instead each state has a single multi•
assume that the features are conditionally independent given                  nomial distribution to represent its emissions, and each 
the state. Under this assumption, the probability of the obser-               emitted phrase is treated as a bag of words. 


INFORMATION EXTRACTION                                                                                                                  431 