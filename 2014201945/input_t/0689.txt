                       Semi-Supervised Regression with Co-Training

                                     Zhi-Hua Zhou     and  Ming Li
                          National Laboratory for Novel Software Technology
                               Nanjing University, Nanjing 210093, China
                                   {zhouzh, lim}@lamda.nju.edu.cn


                    Abstract                            A prominent achievement in this area is the co-training
                                                      paradigm proposed by Blum and Mitchell [1998], which
    In many practical machine learning and data min-  trains two classiﬁers separately on two sufﬁcient and redun-
    ing applications, unlabeled training examples are dant views, i.e. two attribute sets each of which is sufﬁcient
    readily available but labeled ones are fairly expen- for learning and conditionally independent to the other given
    sive to obtain. Therefore, semi-supervised learn- the class label, and uses the predictions of each classiﬁer on
    ing algorithms such as co-training have attracted unlabeled examples to augment the training set of the other.
    much attention. Previous research mainly focuses    Dasgupta et al. [2002] have shown that when the require-
    on semi-supervised classiﬁcation. In this paper, a ment of sufﬁcient and redundant views is met, the co-trained
    co-training style semi-supervised regression algo- classiﬁers could make few generalization errors by maxi-
    rithm, i.e. COREG, is proposed. This algorithm    mizing their agreement over the unlabeled data. Unfortu-
    uses two k-nearest neighbor regressors with differ- nately, such a requirement can hardly be met in most sce-
    ent distance metrics, each of which labels the unla- narios. Goldman and Zhou [2000] proposed an algorithm
    beled data for the other regressor where the label- which does not exploit attribute partition. This algorithm re-
    ing conﬁdence is estimated through consulting the quires using two different supervised learning algorithms that
    inﬂuence of the labeling of unlabeled examples on partition the instance space into a set of equivalence classes,
    the labeled ones. Experiments show that COREG     and employs cross validation technique to determine how to
    can effectively exploit unlabeled data to improve label the unlabeled examples and how to produce the ﬁnal
    regression estimates.                             hypothesis. Although the requirement of sufﬁcient and re-
                                                      dundant views is quite strict, the co-training paradigm has al-
                                                      ready been used in many domains such as statistical parsing
1  Introduction                                       and noun phrase identiﬁcation [Hwa et al., 2003][Pierce and
In many practical machine learning and data mining appli- Cardie, 2001][Sarkar, 2001][Steedman et al., 2003].
cations such as web user proﬁle analysis, unlabeled training It is noteworthy that previous research mainly focuses on
examples are readily available but labeled ones are fairly ex- classiﬁcation while regression remains almost untouched. In
pensive to obtain because they require human effort. There- this paper, a co-training style semi-supervised regression al-
fore, semi-supervised learning methods that exploit unlabeled gorithm named COREG, i.e. CO-training REGressors, is
examples in addition to labeled ones have attracted much at- proposed. This algorithm employs two k-nearest neighbor
tention.                                              (kNN) regressors, each of which labels the unlabeled data
  Many current semi-supervised learning methods use a gen- for the other during the learning process. In order to choose
erative model for the classiﬁer and employ Expectation- appropriate unlabeled examples to label, COREG estimates
Maximization (EM) [Dempster et al., 1977] to model the the labeling conﬁdence through consulting the inﬂuence of
label estimation or parameter estimation process. For ex- the labeling of unlabeled examples on the labeled examples.
ample, mixture of Gaussians [Shahshahani and Landgrebe, The ﬁnal prediction is made by averaging the regression es-
1994], mixture of experts [Miller and Uyar, 1997], and naive timates generated by both regressors. Since COREG utilizes
Bayes [Nigam et al., 2000] have been respectively used as the different distance metrics instead of requiring sufﬁcient and
generative model, while EM is used to combine labeled and redundant views, its applicability is broad. Moreover, experi-
unlabeled data for classiﬁcation. There are also many other mental results show that this algorithm can effectively exploit
methods such as using transductive inference for support vec- unlabeled data to improve regression estimates.
tor machines to optimize performance on a speciﬁc test set The rest of this paper is organized as follows. Section 2
[Joachims, 1999], constructing a graph on the examples such proposes the COREG algorithm. Section 3 presents an anal-
that the minimum cut on the graph yields an optimal labeling ysis on the algorithm. Section 4 reports on the experimental
of the unlabeled examples according to certain optimization results. Finally, Section 5 concludes and raises several issues
functions [Blum and Chawla, 2001], etc.               for future work.2  COREG                                              and h2 can be diverse through instantiating them with differ-
                                                      ent p values. Such a setting can also bring another proﬁt, that
Let L = {(x1, y1), · · · , (x|L|, y|L|)} denote the labeled ex- is, since it is usually difﬁcult to decide which p value is better
ample set, where xi is the i-th instance described by d at- for the concerned task, the functions of these regressors may
tributes, yi is its real-valued label, i.e. its expected real- be somewhat complementary to be combined.
valued output, and |L| is the number of labeled examples;
                                                                               Ã               !1/p
let U denote the unlabeled data set, where the instances are                     Xd
                                                                                              p
also described by the d attributes, whose real-valued labels Minkowskyp(xr, xs) =   |xr,l − xs,l|     (1)
are unknown, and |U| is the number of unlabeled examples.                        l=1
  Two regressors, i.e. h1 and h2, are generated from L, each In order to choose appropriate unlabeled examples to la-
of which is then reﬁned with the help of unlabeled exam- bel, the labeling conﬁdence should be estimated such that the
ples that are labeled by the latest version of the other re- most conﬁdently labeled example can be identiﬁed. In classi-
gressor. Here the kNN regressor [Dasarathy, 1991] is used ﬁcation this is relatively straightforward because when mak-
as the base learner to instantiate h1 and h2, which labels a ing classiﬁcations, many classiﬁers can also provide an esti-
new instance through averaging the real-valued labels of its mated probability (or an approximation) for the classiﬁcation,
k-nearest neighboring examples.                       e.g. a Naive Bayes classiﬁer returns the maximum posteriori
  The use of kNN regressor as the base learner is due to the hypothesis where the posterior probabilities can be used, a
following considerations. First, the regressors will be reﬁned BP neural network classiﬁer returns thresholded classiﬁcation
in each of many learning iterations. If neural networks or re- where the real-valued outputs can be used, etc. Therefore, the
gression trees are used, then in each iteration the regressors labeling conﬁdence can be estimated through consulting the
have to be re-trained with the labeled examples in addition to probabilities of the unlabeled examples being labeled to dif-
the newly labeled ones, the computational load of which will ferent classes. For example, suppose the probability of the
                   k
be quite heavy. Since NN is a lazy learning method which instance a being classiﬁed to the classes c1 and c2 is 0.90
does not hold a separate training phase, the reﬁnement of the and 0.10, respectively, while that of the instance b is 0.60 and
kNN regressors can be efﬁciently realized. Second, in order 0.40, respectively. Then the instance a is more conﬁdent to
to choose appropriate unlabeled examples to label, the label- be labeled (to class c1).
ing conﬁdence should be estimated. In COREG the estimation Unfortunately, in regression there is no such estimated
utilizes the neighboring properties of the training examples, probability that can be used directly. This is because in con-
which can be easily coupled with the kNN regressors.  trast to classiﬁcation where the number of class labels to be
  It is noteworthy that the initial regressors should be diverse predicted is ﬁnite, the possible predictions in regression are
because if they are identical, then for either regressor, the inﬁnite. Therefore, a key of COREG is the mechanism for
unlabeled examples labeled by the other regressor may be estimating the labeling conﬁdence.
the same as these labeled by the regressor for itself. Thus, Heuristically, the most conﬁdently labeled example of a
the algorithm degenerates to self-training [Nigam and Ghani, regressor should be with such a property, i.e. the error of
2000] with a single learner. In the standard setting of co- the regressor on the labeled example set should decrease the
training, the use of sufﬁcient and redundant views enables the most if the most conﬁdently labeled example is utilized. In
learners be different. Previous research has also shown that other words, the most conﬁdently labeled example should be
even when there is no natural attribute partitions, if there are the one which makes the regressor most consistent with the
sufﬁcient redundancy among the attributes then a fairly rea- labeled example set. Thus, the mean squared error (MSE)
sonable attribute partition will enable co-training to exhibit of the regressor on the labeled example set can be evaluated
advantages [Nigam and Ghani, 2000]. While in the extended ﬁrst. Then, the MSE of the regressor utilizing the information
co-training algorithm which does not require sufﬁcient and provided by (xu, ˆyu) can be evaluated on the labeled exam-
redundant views, the diversity among the learners is achieved ple set, where xu is an unlabeled instance while ˆyu is the
through using different learning algorithms [Goldman and real-valued label generated by the original regressor. Let ∆u
Zhou, 2000]. Since COREG does not assume sufﬁcient and denote the result of subtracting the latter MSE from the for-
redundant views and different learning algorithms, the diver- mer MSE. Note that the number of ∆u to be estimated equals
sity of the regressors should be sought from other channels. to the number of unlabeled examples. Finally, (xu, ˆyu) as-
  Here the diversity is achieved through utilizing different sociated with the biggest positive ∆u can be regarded as the
distance metrics. In fact, a key of kNN learner is how to most conﬁdently labeled example.
determine the distances between different instances. The Since repeatedly measuring the MSE of the kNN regres-
Minkowsky distance shown in Eq. 1 is usually used for this sor on the whole labeled example set in each iteration will be
purpose. Note that different concrete distance metrics can be time-consuming, considering that kNN regressor mainly uti-
generated through setting different values to the distance or- lizes local information, COREG employs an approximation.
der, p. Roughly speaking, the smaller the order, the more That is, for each xu, COREG identiﬁes its k-nearest neighbor-
robust the resulting distance metric to data variations; while ing labeled examples and uses them to compute the MSE. In
the bigger the order, the more sensitive the resulting distance detail, let Ω denote the set of k-nearest neighboring labeled
metric to data variations. Therefore, the vicinities identiﬁed examples of xu, then the most conﬁdently labeled example

for a given instance may be different using the Minkowsky is identiﬁed through maximizing the value of ∆xu in Eq. 2,
                                                                                              0
distance with different orders. Thus, the kNN regressors h1 where h denotes the original regressor while h denotes the                                                      regression estimates. In order to simplify the discussion, here
  Table 1: Pseudo-code describing the COREG algorithm the effect of the pool U 0 is not considered as in [Blum and
                                                      Mitchell, 1998]. That is, the unlabeled examples are assumed
 ALGORITHM:  COREG                                    as being picked from the unlabeled example set U directly.
 INPUT: labeled example set L, unlabeled example set U, In each learning iteration of COREG, for each unlabeled ex-
       number of nearest neighbors k,                 ample xu, its k-nearest neighboring labeled examples are put
       maximum number of learning iterations T ,      into the set Ω. As mentioned before, the newly labeled exam-
       distance orders p1, p2                         ple should make the regressor become more consistent with
 PROCESS:                                             the labeled data set. Therefore, a criterion shown in Eq. 3 can
   L  ← L; L  ←  L                                    be used to evaluate the goodness of xu, where h is the origi-
    1       2                                                           0
   Create pool U 0 by randomly picking examples from U nal regressor while h is the one reﬁned with (xu, yˆu). If the
                                                      value of ∆ is positive, then utilizing (x , yˆ ) is beneﬁcial.
   h1 ← kNN(L1, k, p1); h2 ← kNN(L2, k, p2)                    u                        u  u
   Repeat for T rounds:                                          X                     X
                                                              1                 2   1            0    2
    for j ∈ {1, 2} do                                   ∆u =         (yi − h(xi)) −        (yi − h (xi))
                    0                                        |L|                   |L|
      for each xu ∈ U do                                         xi∈L                  xi∈L
         ˆyu ← hj(xu)                                                                                 (3)
         Ω ← Neighbors(xu, k, Lj)                       In the COREG  algorithm, the unlabeled example which
          0                                           maximizes the value of ∆ is picked to be labeled. There-
         hj ← kNN(Lj  ∪ {(xu, ˆyu)}, k, pj)                                 xu
                P               2   ¡          ¢2     fore, the question is, whether the unlabeled example chosen
         ∆   ←      ((y − h (x )) −  y − h0 (x ) )
          xu          i    j  i       i   j  i        according to the maximization of ∆ will result in a positive
                xi∈Ω                                                                xu
      end of for                                      ∆u value or not.
                                                        First, assume that (x , yˆ ) is among the k-nearest neigh-
      if there exists an ∆xu > 0                                          u  u
                                                      bors of some examples in Ω, and is not among the k-nearest
      then x˜j ← arg max ∆xu ; y˜j ← hj(˜xj)
                     0
                 xu∈U                                 neighbors of any other examples in L. In this case, it is obvi-
                            0    0
           πj ← {(x˜j, y˜j)}; U ← U − πj              ous that utilizing (xu, yˆu) will only change the regression es-
      else πj ← ∅                                     timates on the examples in Ω, therefore Eq. 3 becomes Eq. 4.
    end of for                                        Comparing Eqs. 2 with 4 it can be found that the maximiza-
    L1 ←  L1 ∪ π2; L2 ← L2 ∪ π1
                                                      tion of ∆xu also results in the maximization of ∆u.
    if neither of L1 and L2 changes then exit
                                                                X                   X
    else                                                      1                2  1            0    2
                                                        ∆u =        (yi − h(xi)) −      (yi − h (xi)) (4)
      h1 ← kNN(L1, k, p1); h2 ← kNN(L2, k, p2)                k                   k
      Replenish U 0 by randomly picking examples from U        xi∈Ω                 xi∈Ω
  end of Repeat                                         Second, assume that (xu, yˆu) is not among the k-nearest

                   ∗      1                           neighbors of any example in Ω. In this case, the value of ∆xu
 OUTPUT: regressor h (x) ← 2 (h1(x) + h2(x))
                                                      is zero, therefore (xu, yˆu) won’t be chosen in COREG.
                                                        Third, assume that (xu, yˆu) is among the k-nearest neigh-
reﬁned regressor which has utilized the information provided bors of some examples in Ω as well as some examples
                                                      in L −  Ω, and assume these examples in L  − Ω  are
by (xu, ˆyu). Note that ˆyu = h(xu).                    0  0        0   0
             X                                        (x1, y1), · · · , (xm, ym). Then Eq. 3 becomes Eq. 5.
      ∆   =     ((y − h(x ))2 − (y − h0(x ))2)  (2)
       xu          i      i      i      i                        1 X
                                                           ∆  =       ((y − h(x ))2 − (y − h0(x ))2)+
            xi∈Ω                                            u    k       i      i      i       i
                                                                  x ∈Ω
  The pseudo code of COREG is shown in Table 1, where              i ³          ´    ³          ´
                                                          1    X        0     0  2     0       0 2
the function kNN(Lj, k, pj) returns a kNN regressor on the          ( y  − h(x )   −  y  − h0(x ) )   (5)
                                                         m              q     q        q       q
labeled example set Lj, whose distance order is pi. The learn- q∈{1,···,m}
ing process stops when the maximum number of learning it-
                                                        Maximizing ∆   will maximize the ﬁrst sum term of Eq. 5,
erations, i.e. T , is reached, or there is no unlabeled exam-       xu
                                                      but whether it can enable ∆ be positive should also refer the
ple which is capable of reducing the MSE of any of the re-                   u
                                                      second sum term. Unfortunately, the value of this sum term
gressors on the labeled example set. According to Blum and
                                                      is difﬁcult to be measured except that the neighboring rela-
Mitchell [1998]’s suggestion, a pool of unlabeled examples
                                                      tionships between all the labeled examples and (x , yˆ ) are
smaller than U is used. Note that in each iteration the unla-                                   u   u
                                                      evaluated. Therefore, there may exist cases where the unla-
beled example chosen by h won’t be chosen by h , which
                       1                   2          beled example chosen according to the maximization of ∆
is an extra mechanism for encouraging the diversity of the                                             xu
                                                      may decrease ∆ , which is the cost COREG takes for using
regressors. Thus, even when h and h are similar, the exam-          u
                         1     2                      ∆    that can be more efﬁciently computed to approximate
ples they label for each other will still be different. xu
                                                      ∆u. Nevertheless, experiments show that in most cases such
                                                      an approximation is effective.
3  Analysis                                             It seems that using only one regressor to label the unlabeled
This section attempts to analyze whether the learning process examples for itself might be feasible, where the unlabeled ex-

of COREG  can use the unlabeled examples to improve the amples can be chosen according to the maximization of ∆xu .While considering that the labeled example set usually con- from the original ones. For example, on 3-d Mexican Hat two
tains noise, the use of two regressors can be helpful to reduce new attributes, i.e. x3 and x4, are constructed from x1 + x2
overﬁtting.                                           and x1 − x2, and then a kNN regressor is built on x1 and x2
  Let Λ denote the subset of noisy examples in L. For the while the other is built on x3 and x4. In each iteration, each
unlabeled instance xu, either of the regressors h1 and h2 will kNN regressor chooses the unlabeled example which maxi-

identify a set of k-nearest neighboring labeled examples for mizes the value of ∆xu in Eq. 2 to label for the other regres-
xu. Assume these sets are Ω1 and Ω2, respectively. Since h1 sor. The ﬁnal prediction is made by averaging the regression
and h2 use different distance orders, Ω1 and Ω2 are usually estimates of these two reﬁned regressors. Besides, a kNN re-
different, and therefore Ω1 ∩ Λ and Ω2 ∩ Λ are also usually gressor using only the labeled data is tested as a baseline for
different. Suppose xu is labeled by h1 and then (xu, h1(xu)) the comparison, which is denoted by LABELED.
is put into L1, where h1(xu) suffers from Ω1 ∩ Λ. For an- All the kNN regressors used in SELF, ARTRE, and LA-
other unlabeled instance xv which is very close to xu, its BELED employ 2nd-order Minkowski distance, and the k
                                                                                  0
k-nearest neighbors identiﬁed by h1 will be very similar to value is set to 3. The same pool, U , as that used by COREG is
Ω1 except that (xu, h1(xu)) has replaced a previous neigh- used in each iteration of SELF and ARTRE, and the maximum
bor. Thus, h1(xv) will suffer from Ω1 ∩ Λ more seriously number of iterations is also set to 100.
than h1(xu) does. While, if the instance xu is labeled by h2 One hundred runs of experiments are carried out on each
and (xu, h2(xu)) is put into L1, then h1(xv) will suffer from data set. In each run, the performance of all the four algo-
Ω1 ∩ Λ only once, although xu is still very close to xv. rithms, i.e. COREG, SELF, ARTRE, and LABELED, are eval-
                                                      uated on randomly partitioned labeled/unlabeled/test splits.
4  Experiments                                        The average MSE at each iteration is recorded. Note that
Experiments are performed on ten data sets listed in Table 2 the learning processes of the algorithms may stop before the
where “# attribute” denotes the number of input attributes. maximum number of iterations is reached, and in that case,
These data sets have been used in [Zhou et al., 2002] where the ﬁnal MSE is used in computing the average MSE of the
the detailed descriptions of the data sets can be found. Note following iterations.
that the input attributes as well as the real-valued labels have The improvement on average MSE obtained by exploiting
been normalized to [0.0, 1.0].                        unlabeled examples is tabulated in Table 3, which is com-
                                                      puted by subtracting the ﬁnal MSE from the initial MSE and
            Table 2: Experimental data sets           then divided by the initial MSE.
          Data set        # attribute Size
                                                         Table 3: Improvement on average mean squared error
          2-d Mexican Hat    1      5,000
                                                            Data set         SELF   ARTRE   COREG
          3-d Mexican Hat    2      3,000
          Friedman #1        5      5,000                   2d Mexican Hat   9.2%    12.8%   19.6%
          Friedman #2        4      5,000                   3d Mexican Hat   3.9%     3.7%    5.7%
          Friedman #3        4      3,000                   Friedman #1     -1.8%    -4.0%    0.5%
          Gabor              2      3,000                   Friedman #2     -1.3%    -4.3%    2.1%
          Multi              5      4,000                   Friedman #3     -0.9%    -3.6%    0.0%
          Plane              2      1,000                   Gabor            4.0%     3.8%    9.0%
          Polynomial         1      3,000                   Multi           -1.9%    -4.4%    1.4%
          SinC               1      3,000                   Plane           -3.8%    -3.5%    -1.6%
                                                            Polynomial      15.1%    17.4%   22.0%
                                                            SinC            13.0%    16.4%   26.0%
  For each data set, 25% data are kept as the test set, while
the remaining 75% data are partitioned into the labeled and
unlabeled sets where 10% (of the 75%) data are used as la- Table 3 shows that SELF and ARTRE improve the regres-
beled examples while the remaining 90% (of the 75%) data sion estimates on only ﬁve data sets, while COREG improves
are used as unlabeled examples.                       on eight data sets. Moreover, Table 3 discloses that the im-
  In the experiments, the distance orders used by the two provement of COREG is always bigger than that of SELF and
kNN regressors in COREG are set to 2 and 5, respectively, ARTRE. These observations tell that COREG can effectively
the k value is set to 3, the maximum number of iterations T is exploit unlabeled examples to improve regression estimates.
set to 100, and the pool U 0 contains 100 unlabeled examples For further studying the compared algorithms, the average
randomly picked from the unlabeled set in each iteration. MSE of different algorithms at different iterations are plotted
  A self-training style algorithm is tested for comparison, in Fig 1, where the average MSE of the two kNN regressors
which is denoted by SELF. This algorithm uses a kNN re- used in COREG are also depicted. Note that in each subﬁg-
gressor and in each iteration, it chooses the unlabeled exam- ure, every curve contains 101 points corresponding to the 100

ple which maximizes the value of ∆xu in Eq. 2 to label for learning iterations in addition to the initial condition, where
itself. Moreover, a co-training style algorithm, denoted by only 11 of them are explicitly depicted for better presentation.
ARTRE, is also tested. Since the experimental data sets are Fig. 1 shows that COREG can exploit unlabeled examples
with no sufﬁcient and redundant views, here an artiﬁcial re- to improve the regression estimates on most data sets, except
dundant view is developed through deriving new attributes that on Friedman #3 there is no improvement while on Plane             −3
            x 10

                                                                      0.072
           5.4                          0.06
                                                                      0.071

           5.2                          0.059                         0.07

                                                                      0.069
           5
                                        0.058
                                                                      0.068

           4.8                                                        0.067
                                        0.057

          Mean  Squared Error          Mean  Squared Error           Mean  Squared Error 0.066
           4.6
                                        0.056                         0.065

           4.4                                                        0.064
                                        0.055
                                                                      0.063
            0   20  40   60  80  100      0   20  40   60  80  100      0   20  40   60  80  100
                     Iterations                    Iterations                    Iterations
               (a) 2-d Mexican Hat           (b) 3-d Mexican Hat            (c) Friedman #1


                                        0.13
          0.08                                                        0.041

          0.075                         0.125
                                                                      0.04

          0.07                          0.12

                                                                      0.039
          0.065
                                        0.115

          0.06
                                                                      0.038
                                        0.11
                                                                     Mean  Squared Error
         Mean  Squared Error           Mean  Squared Error
          0.055
                                        0.105                         0.037
          0.05
                                         0.1
          0.045                                                       0.036

            0   20  40   60  80  100      0   20  40   60  80  100      0   20  40   60  80  100
                     Iterations                    Iterations                    Iterations
                (d) Friedman #2               (e) Friedman #3                  (f) Gabor

                                                                         −3
                                                                        x 10
         0.0565                                                        4.8

          0.056                         0.318
                                                                       4.6
         0.0555
                                        0.316
          0.055
                                                                       4.4
         0.0545                         0.314

          0.054                         0.312                          4.2
         0.0535
                                        0.31
          0.053                                                        4
         Mean  Squared Error           Mean  Squared Error
                                                                      Mean  Squared Error

         0.0525                         0.308
                                                                       3.8
          0.052                         0.306
         0.0515
                                                                       3.6
                                        0.304
            0   20  40   60  80  100      0   20  40   60  80  100      0   20  40   60  80  100
                     Iterations                    Iterations                    Iterations
                   (g) Multi                     (h) Plane                   (i) Polynomial

                            −3
                           x 10

                          4.6

                          4.4

                          4.2

                          4

                          3.8
                         Mean  Squared Error
                          3.6

                          3.4

                          3.2
                           0   20  40   60  80  100
                                    Iterations
                                  (j) SinC                       Legend

           Figure 1: Comparisons on average mean squared error of different algorithms at different iterations

the performance is degenerated. While, SELF and ARTRE de- the ﬁnal regression estimates of COREG are signiﬁcantly bet-
generate the regression estimates on ﬁve data sets, i.e. Fried- ter than these of ARTRE on almost all the data sets except
man #1 to #3, Multi, and Plane. Moreover, the average MSE on Friedman #1 where the latter is better. Furthermore, the
of the ﬁnal prediction made by COREG is almost always the ﬁnal regression estimates of COREG are signiﬁcantly better
best except on Friedman #1 where ARTRE is slightly better than these of SELF and LABELED on almost all the data sets
and on Plane where LABELED is the best while all the semi- except on Plane where LABELED is better and on Fried-
supervised learning algorithms degenerate the performance. man #3 where there is no signiﬁcant difference. These re-
These observations disclose that COREG is apparently the sults of t-tests conﬁrm that COREG is the strongest among
best algorithm in the comparison.                     the compared algorithms, which can effectively exploit unla-
                                                      beled data to improve the regression estimates.
  Pairwise two-tailed t-tests with 0.05 signiﬁcance level re-
veal that the ﬁnal regression estimates of COREG are signif-
icantly better than its initial regression estimates on almost 5 Conclusion
all the data sets except that on Plane the latter is better while This paper proposes a co-training style semi-supervised re-
on Friedman #3 there is no signiﬁcant difference. Moreover, gression algorithm COREG. This algorithm employs two k-