                                 Transfer in Learning by Doing

        Bill Krueger, Tim Oates, Tom Armstrong                Paul Cohen, Carole Beal
         University of Maryland Baltimore County         USC Information Sciences Institute
                CSEE, 1000 Hilltop Circle                       4676 Admiralty Way
                   Baltimore, MD 21250                       Marina del Rey, CA 90292
           {wkrueg1,oates,arm1}@cs.umbc.edu                    {cohen,cbeal}@isi.edu

                    Abstract                          learner learns ﬁnite state machines and uses its current ma-
                                                      chine to generate its next move.
    We develop two related themes, learning proce-      The experiments in this paper involve a sequence of three
    dures and knowledge transfer. This paper intro-   games. In the ﬁrst game, the child and the adult each have two
    duces two methods for learning procedures and one stacks of cards, one in which all of the cards are face-up and
    for transferring previously-learned knowledge to a another in which all of the cards are face-down. Let’s denote
    slightly different task. We demonstrate by experi- these stacks by the owner (A for adult or C for child) and
    ment that transfer accelerates learning.          whether they are face-up (U) or face-down (D). Therefore, the
                                                      four stacks are AU, AD, CU, CD. Each player must ﬂip over the
                                                      card on top of their face-down stack and move it to the top of
1  Introduction                                       their face-up stack. Each player does this as fast as they can,
Procedures are interesting for many reasons: Infants exer- and can ignore the other player’s actions for the purposes of
cise procedures (called “circular reactions” by Piaget) almost this game.


from birth. These gradually develop in complexity, adding


                                                                                ¡ ¢ £


                                                                  ¤ ¥ ¦ § ¨ £ ¥ © ¨ ¡   


                                                                                      ¡ ¢ £


new elements, and eventually incorporate objects in the in-                           


                                                                    £ ¥ © ¨ ¡   


fant’s environment. Piaget believed infants learn much about


                                                                   ¡ ¢ £                 ¢ © ¨ £ ¥ © ¨ 
   
the world by executing procedures. Older students do, as            


well; indeed, the literature on expert/novice differences can


                                                                    ¢ © ¨ £ ¥ © ¨ ¡      ¡ ¢ £


                                                                             ¥ ¦ § ¨ £ ¥ © ¨ 
   


be summarized in a single phrase: Novices know “how,” ex-                    ¤


                                                                              ¥ © ¨ 
   
perts also know “why.” Novices can run statistical tests but                  £
not understand why they are appropriate, they can follow
recipes but not understand the underlying chemistry and gas- Figure 1: A state machine for Game 2 that yields perfect play.
tronomy.
  There are advantages to learning fact-like knowledge  In the second game, the players take turns ﬂipping and
alongside procedures. One can do useful work with proce- moving cards, as shown in Figure 1. The two nodes
dures even if one doesn’t completely understand them. This on the left specify constraints on what the child observes,
means learning is grounded in activity and is gradual over namely, the action that is currently being taken by the
the life of the agent. By exercising procedures the learner adult. To be in the lower-left state in the machine, the adult
produces occasional failures as well as the context one needs must currently be ﬂipping over the top card on her face-
to gradually learn both new procedures and non-procedural down stack (FLIP(TOP(AD))). When this happens, the
knowledge. The latter includes the facts and reasons we call action executed by the child is WAIT. That is, when the
“understanding,” or, less colloquially, the conditioning vari- adult does FLIP(TOP(AD)), the child does WAIT at the
ables that affect the probabilities that procedures will suc- same time. This leads to a state in which the adult does
ceed. In social environments, there often is a human to help MOVE(TOP(AD), TOP(AU)), which also results in the
the learner correct missteps before a procedure goes com- child waiting. Then, when the adult executes a WAIT action
pletely wrong. This minimizes the credit-assignment prob- (the constraint in the upper-right state), the child ﬂips the card
lem.                                                  on top of her face-down stack, and the adult continues to wait
  Our intention is to have a machine learn a sequence of (the lower-right state) as the child does MOVE(TOP(CD),
increasingly-difﬁcult games, starting with extremely easy TOP(CU)). Some time later, the adult ﬂips the card on top
ones. All the games are two-person card games. One player of her face-down stack, and the cycle repeats.
makes mistakes, the other is a competent player who offers The third game is just like the second game, except when
corrections as necessary. For convenience we call these play- either player turns over a card and the color of that card
ers the learner or child (C) and the adult (A). Correct play matches that of the card atop the other player’s face-up stack,
for the games can be modeled by ﬁnite state machines, so the both players say “Squawk”.2  Learning the State Machines                        3   Experiments
                                                      Figure 2 shows two machines that were intermediate steps
                                                      on the path to learning the turn taking game. In the top ma-
Our goal is to learn procedures, and, speciﬁcally, perfect-play chine, there is a single state from which the child can WAIT,
state machines such as those we described in the previous MOVE(CD CU), or FLIP(CD). Edges are labeled with two
section. A useful distinction can be drawn between learning counts - the number of times the transition was taken and the
a state machine and learning a state machine given a related number of times that transition resulted in a NO. This machine
state machine. We are most interested in the second case, does not yield perfect play, so the state will be split. The ob-
which we present as a kind of transfer learning. Even so, servable feature that most reduces entropy in the distribution
we begin our discussion of learning with two methods for of negative feedback is the feedback received for the move
learning state machines de novo. These are Bayesian Model just made.
Merging (BMM)  [Stolcke, 1994], a well-known method for
learning HMMs; and State Splitting (SS), which also learns
HMMs, though by splitting rather than merging states.
  The training data in both approaches was a sequence of ob-
servations such as MOVE(TOP(CD), TOP(CU)), the sym-
bol SQUAWK, and the special symbol NO. The BMM and SS
algorithms rely on NO to indicate that the child has done
something that is not allowed in perfect play. Incorrect play
is generated by the learner’s incorrect machine in the active
learning regime. Passive learning requires training data that Figure 2: Machines produced by SS when learning the turn
includes mistakes and adult admonitions (i.e., NO) and cor- taking game.
rections. For this purpose we build simulators of child and
adult play.                                             The bottom machine in Figure 2 shows the result of split-
                                                      ting on this feature and pruning away any transitions that al-
  BMM is a top-down approach in which an initial HMM is ways cause a NO. This machine correctly captures the fact that
constructed with one state for every observation, and obser- if the adult corrects the child, the only legal action is to WAIT
vations are greedily merged to maximize the posterior likeli- while the adult repairs the incorrect action, which will result
hood of the data using a description length prior on HMMs. It in a YES. Residual feedback non-determinism exists in the
is incremental in the sense that new observations can be added top state due to errors in action ordering (e.g., ﬂipping twice
during the merging process. SS is a bottom-up approach that in a row). Therefore, that state will be split next.
starts with one state that matches all observations, and re- In a second set of experiments, the child chose actions ran-
peatedly splits states on observable features of the world in domly from a set of known actions until a model was devel-
an effort to more accurately predict negative feedback from oped (using BMM) that could guide action selection. Ini-
the teacher. Features are chosen that minimize entropy in the tially, therefore, the child would frequently choose an incor-
distribution of feedback (positive or negative) over all states. rect action. Learning early on with no model is tedious and
  Regardless of whether the HMM was learned via BMM   time-consuming as it requires heavy exploration. The second
or SS, it can be used for action selection (game play) and to game, the simple turn-taking game, was still easy to learn.
parse new observations. We are interested in whether learning Only 10 simulation time steps were required to learn the opti-
a series of related, yet increasingly difﬁcult, games requires mal model for that game when starting from scratch. Without
less effort in total than learning the most difﬁcult game de using this model to bias the model learned for the third game,
novo. For example, is it easier to learn our three games in 53 time steps were required to achieve the optimal model for
sequence, using the machine learned for game n to bias the the more complex game. However, when the game 2 model
learning of game n + 1, than to learn to play Squawk with no was used to bias the learning of game 3, only an additional
prior machine to serve as bias?                       17 time steps were needed to learn game 3, making a total of
                                                      27 time steps. That is, game 3 took in total half as long to
  When using BMM to learn a new game biased by an exist- learn when game 2 was learned as an intermediate step in the
ing HMM, new states and transitions are added for the new learning process.
observations, but old and obsolete state transitions are not re-
moved. Instead, as more and more data are collected for the Acknowledgments
new game, the probabilities of obsolete state transitions that
are no longer traversed will tend to zero. This is not accept- This work was supported by DARPA (contract number: 53-
able, as we want to quickly reﬁne the model to work with the 4540-0016; project name: Learning by Doing).
new game while harnessing as much information as possible
from previous experience. To achieve this we preserve model References
structure but eliminate the bias of old probability parameters [Stolcke, 1994] Andreas Stolcke. Bayesian Learning of
when moving from one game to the next. In BMM terminol-  Probabilistic Language Models. PhD thesis, University
ogy, this means that we reset Viterbi counts to be uniform and of California, Berkeley, 1994.
small when starting to learn a new game.