       Dynamically Weighted Hidden Markov Model for Spam Deobfuscation

           Seunghak Lee                      Iryoung Jeong                    Seungjin Choi
      Department of Chemistry           Department of Computer       Department of Computer Science
         POSTECH, Korea                    Science Education                 POSTECH, Korea
        boy3@postech.ac.kr              Korea University, Korea           seungjin@postech.ac.kr
                                       crex@comedu.korea.ac.kr

                    Abstract                          and its effect is limited. Other approaches involve various
                                                      search strategies. The Viterbi decoding is a time-consuming
    Spam deobfuscation is a processing to detect obfus- task for HMMs which contain a large vocabulary (e.g., 20,000
    cated words appeared in spam emails and to convert words). The beam search algorithm [Jelinek, 1999] speeds up
    them back to the original words for correct recog- the Viterbi decoding by eliminating improper state paths in
    nition. Lexicon tree hidden Markov model (LT-     the trellis with a threshold. However, it still has a limitation
    HMM) was recently shown to be useful in spam      in performance gain if a large number of states are involved
    deobfuscation. However, LT-HMM suffers from a     (e.g., 100,000 states) since the number of states should still
    huge number of states, which is not desirable for be taken into account in the algorithm.
    practical applications. In this paper we present    Spam deobfuscation is an important pre-processing task
    a complexity-reduced HMM, referred to as dy-      for content-based spam ﬁlters which use words of contents in
    namically weighted HMM (DW-HMM) where the         emails to determine whether an incoming email is a spam or
    states involving the same emission probability are
                                                      not. For instance, the word ”viagra” indicates that the email
    grouped into super-states, while preserving state containing such word is most likely a spam. However, spam-
    transition probabilities of the original HMM. DW- mers obfuscate words to circumvent spam ﬁlters by inserting,
    HMM   dramatically reduces the number of states
                                                      deleting and substituting characters of words as well as by in-
    and its state transition probabilities are determined troducing incorrect segmentation. For example, ”viagra” may
    in the decoding phase. We illustrate how we con-  be written as ”vi@graa” in spam emails. Some examples of
    vert a LT-HMM to its associated DW-HMM. We
                                                      obfuscated words in real spam emails are shown in Table 1.
    conﬁrm the useful behavior of DW-HMM in the       Thus, an important task for successful content-based spam ﬁl-
    task of spam deobfuscation, showing that it signiﬁ- tering is to restore obfuscated words in spam emails to origi-
    cantly reduces the number of states while maintain- nal ones. Lee and Ng [Lee and Ng, 2005] proposed a method
    ing the high accuracy.                            of spam deobfuscation based on a lexicon tree HMM (LT-
                                                      HMM), demonstrating promising results. Nevertheless, the
1  Introduction                                       LT-HMM suffers from a large number of states (e.g., 110,919
Large vocabulary problems in hidden Markov  models    states), which is not desirable for practical applications.
(HMMs) have been addressed in various areas such as hand-
writing recognition [A. L. Koerich and Suen, 2003] and
                                                        Table 1: Examples of obfuscated words in spam emails.
speech recognition [Jelinek, 1999]. As the vocabulary size
increases, the computational complexity for recognition and
                                                              con. tains forwa. rdlook. ing sta. tements
decoding dramatically grows, making the recognition system
                                                              contains forwardlooking statements
impractical. In order to solve the large vocabulary problem,
various methods have been developed, especially in speech     th’e lowest rates in thkhe u.s.
and handwriting recognition communities.                      the lowest rates in the us
  For example, one approach is to reduce the lexicon size     D1scla1merBelow:
by using the side information such as word length and word    Disclaimer Below
shape. However, this method reduces the global lexicon to
only its subset so that the true word hypothesis might be dis- In this paper, we present a complexity-reduced structure of
carded. An alternative approach is to reduce a search space in HMM as a solution to the large vocabulary problem. The
the large lexicon, since the same initial characters are shared core idea is to group states involving the same emission
by the lexical tree, leading to redundant computations. Com- probabilities into a few number of super-states, while pre-
pared to the ﬂat lexicon where whole words are simply cor- serving state transition probabilities of the original HMM.
rected, these methods reduce the computational complexity. The proposed complexity-reduced HMM is referred to as dy-
However, the lexical tree still has a large number of nodes namically weighted HMM (DW-HMM), since state transition

                                                IJCAI-07
                                                  2523probabilities are dynamically determined using the data struc-         P (y|s4)=P (y|q8).
ture which contains the state transition probabilities of the
                                                      In the DW-HMM, the number of states is reduced from 8 to
original HMM in the decoding phase. We illustrate how
                                                      4.
to construct a DW-HMM, given a LT-HMM, reducing the
                                                        The data structure Φ (as shown in the right of the bottom in
number of states dramatically. We also explain conditions
                                                      Figure 1) is constructed, in order to preserve the state transi-
for equivalence between DW-HMM and LT-HMM. We apply
                                                      tion probabilities deﬁned in the original HMM. Each node
DW-HMM    to the task of spam deobfuscation, emphasizing
                                                      in Φ is labeled by the super-state which it belongs to and
its reduced-complexity as well as performance.
                                                      state transition probabilities are stored, following the origi-
                                                      nal HMM.
2  Dynamically Weighted Hidden Markov
   Model                                                                          q4
A hidden Markov model is a simple dynamic Bayesian net-                     q2
work that is characterized by initial state probabilities, state                  q5
transition probabilities, and emission probabilities. Notations
                                                                      q1                  s8q8
for HMM are as follows:
                                                                                  q6
 1. Individual hidden states of HMM are denoted by
                                                                            q3
    {q1,q2,q3,...,qK },whereK is the number of states.
                                                K
    The hidden state vector at time t is denoted by qt ∈ R .                      q7
 2. Observation symbols belong to a  ﬁnite alphabet,
    {y ,y ,...,y }       M                                                     HMM
      1  2     M  ,where   is the number of distinct ob-                                   P (q4|q2)
    servation symbols. The observation data at time t is de-
                                                                                            s2
            y ∈ RM                                                                  P (q2|q1)     P (q8|q4)
    noted by t      .                                                                             P (q8|q5)
                                                                  s                   s2
                                        q        q                 2                       P (q5|q2) P (q8|q6)
 3. The state transition probability from state i to state j                                      P (q |q )
                                                                              Π(q )                  8 7
               P qj|qi                                                           1          s3
    is deﬁned by (   ).                                   s1              s4
                                                                                s          P (q |q ) s
 4. The emission probability of observation symbol yi,                           1           6 3     4
                                                                                    P (q |q ) s2
    given state qj is denoted by P (yi|qj).                       s3                   3 1
                                    q                                                 s3
 5. The initial state probability of state i is denoted by                                 P (q7|q3)
      q
    Π( i).                                                                                  s3
With these deﬁnitions, HMM is characterized by the joint dis- DW-HMM                        Φ
tribution of states q1:T = {q1,...,qT } and observed sym-
bols y1:T = {y1,...,yT }, which is of a factorized form Figure 1: Transition diagrams for the 8-state HMM, its asso-
                                                      ciated 4-state DW-HMM, and the data structure ,areshown.
                            T                                                               Φ
P q   , y        q  P y |q     P  q |q   P y  |q .    The original HMM contains 8 different states with 4 different
 ( 1:T  1:T )=Π(  1) ( 1  1)     ( t t−1)  ( t t)     emission probabilities. The nodes with the same emission
                            t=2
                                                      probabilities are colored by the same gray-scale. DW-HMM
  We ﬁrst illustrate the DW-HMM with a simple example. contains 4 different super-states and the data structure Φ is
Figure 1 shows a transition diagram of a 8-state HMM and its constructed in such a way that the transition probabilities are
associated DW-HMM  that consists of four super-states, s1, preserved for the DW-HMM.
s2, s3,ands4. In this example, the original 8-state HMM
(as shown in the top in Figure 1) has four distinct emission The trellis of the 4-state DW-HMM is shown in Figure 2.
probabilities, i.e.,                                  Transition probabilities are determined by searching Φ using
                     P  y|q ,
                       (  1)                          a hypothesis, s1:t. To show the relation between DW-HMM
            P (y|q2)=P (y|q4)=P  (y|q6),              and HMM, we deﬁne that super-state sequence of DW-HMM,
                                                      s1:T , is corresponding to state sequence of HMM, q1:T ,if
            P (y|q3)=P (y|q5)=P  (y|q7),
                     P (y|q8).
                                                                 P (y|qi)=P (y|si),  1 ≤ i ≤ T.
  The states involving the same emission probability, are
                                                      Thus, in Figure 2, the hypothesis of the 8-state HMM cor-
grouped into a super-state, denoted by s in the DW-HMM
                                                      responding to s1:4 = {s1,s2,s3,s4} is the state sequence,
(as shown in the left of the bottom in Figure 1). In such a
                                                      q1:4 =  {q1,q2,q5,q8}. Given an observation sequence,
case, we construct a DW-HMM containing 4 super-states, s1,
                                                      y1:4 = {y1, y2, y3, y4}, the joint probabilities of the state
s2, s3,ands4, where the following is satisﬁed:
                                                      sequence and the observation sequence are as follows:
                 P (y|s1)=P (y|q1),
                                                                                  4
       P (y|s2)=P (y|q2)=P  (y|q4)=P (y|q6),
                                                      P (q  , y )=Π(q   )P (y |q )   P (q |q  )P (y |q ),
       P y|s     P y|q    P  y|q    P y|q ,               1:4 1:4       1    1  1        t t−1    t  t
         (  3)=   (   3)=   (  5)=   (   7)                                       t=2

                                                IJCAI-07
                                                  2524                                                                                    y
           Observation  y1                 y2                  y3                    4

                         Π(s1)
                        s1                 s1                  s1                   s1
                                P (s2|s1)
                        s2                 s2                  s2                   s2
                                                     P (s |s )
           Super-states                                3 2
                        s3                 s3                  s3                   s3
                                                                         P (s4|s3)
                        s4                 s4                  s4                   s4

           Hypothesis  {s1}{s1,             s2}            {s1, s2, s3}{s1,      s2, s3, s4}


                                      P (s2|s1)
                         s2                 s2                  s2                   s2
                    s2                  s2                  s2                   s2
                Π(s1)                                                                     P (s4|s3)
                         s3                 s3                  s3                   s3
                                                             P (s |s )
           Φ    s1             s4   s1             s4  s1      3 2     s4   s1              s4
                         s2                 s2                  s2                   s2

                     s3                 s3                  s3                   s3
                         s3                 s3                  s3                   s3
                        t =1               t =2                t =3                 t =4

Figure 2: Trellis of the 4-state DW-HMM converted from the 8-state HMM. Transition probabilities are determined by searching
a data structure, Φ, with a hypothesis, s1:t. The search process at a time instance is represented by the thick line in Φ.


                                                      {y1, y2,...,yT } of HMMs equals that of the corresponding
                                                                        s      {s , s ,...,s }
                           4                         super-state sequence 1:T = 1  2     T  and the same
                                                      observation sequence y1:T of converted DW-HMMs as
P (s1:4, y1:4)=Π(s1)P (y1|s1) P (st|st−1)P (yt|st).
                           t=2                        follows:
     q                                     s
Since 1:4 is the state sequence corresponding to 1:4,the          P (q1:T , y1:T )=P (s1:T , y1:T ).
emission probabilities involved in the joint probabilities are The state transition probability of DW-HMM is deﬁned as
equal as follows:                                     follows:

                 P (y|q1)=P (y|s1),                                    
                                                                         0       if s1:t does not match
                 P y|q2   P  y|s2 ,
                  (    )=   (   )                         P (st|st−1)=           any paths in Φ
                 P (y|q5)=P (y|s3),                                      ω(s1:t) otherwise,

                 P (y|q8)=P (y|s4).                   where  s1:t is state sequence decoded so far, s1:t =
                                                      {s , s ,...,s  , s } ω s
The DW-HMM    searches Φ with the hypothesis, s1:t, deter- 1 2    t−1  t .  ( 1:t) is a weight function which
mining the transition probabilities as follows:       returns transition probabilities from Φ. The weight function,
                                                      ω(s1:t), traces a hypothesis, s1:t,inΦ, and returns a state
                                                                        P  s |s
                  Π(s1)=Π(q1),                        transition probability, ( t t−1) that is stored in a node vis-
                                                      ited by st. For example, in Figure 2, ω(s1:3) determines the
                P (s2|s1)=P (q2|q1),
                                                      state transition probability, P (s3|s2) at t =3. The thick line
                P (s3|s2)=P (q5|q2),                  in Φ shows that ω(s1:3) traces the hypothesis s1:3,andre-
                                                                                P  s |s
                P (s4|s3)=P (q8|q5).                  turns the transition probability, ( 3 2), stored in the node,
                                                      s3, which is the same as the value of P (q5|q2) as shown in
By the above equalities, the 8-state HMM and the 4-state DW- Figure 1. If a node contains several state transition probabil-
HMM have the same joint probability as follows:       ities, we choose the correct one according to the node vis-
                                                            t −
            P  q  , y     P  s  , y  .                ited at   1. It should be noted that DW-HMM determines
              ( 1:4 1:4)=   ( 1:4 1:4)                the transition probability by searching the data structure of Φ
  In searching of a probability, P (st|st−1), we assume that whereas HMMs ﬁnd them in the transition probability matrix.
state sequence s1:t is unique in Φ. The constraint is not so DW-HMMs converted from HMMs have the following
strong in that if there are several state sequences which have characteristics. First, DW-HMMs are useful for HMMs
exactly the same emission probabilities for each state, it is which have a very large state transition structure and com-
very possible that the model has redundant paths. Therefore, mon emission probabilities among a number of states, such
it is usual when the constraint is kept in HMMs.      as LT-HMMs. When we convert a HMM    to its associated
  The  joint probability of state sequence q1:T  =    DW-HMM, the computational complexity signiﬁcantly de-
{q1, q2,...,qT } and an observation sequence y1:T =   creases because the number of states is dramatically reduced.

                                                IJCAI-07
                                                  2525Second, there is no need to maintain a transition probability of the HMM.
matrix since the weight function dynamically gives transition
probabilities in the decoding phase. Third, DW-HMMs are so
ﬂexible that it is easy to add, delete, or change any states in
the state transition structure since only the data structure, Φ,                          s4
                                                                     s1
needs to be updated. Fourth, the speed and accuracy are con-                         s1
ﬁgurable by using beam search algorithm and N-best search
[Jelinek, 1999] respectively, thus securing a desirable perfor-
                                                                         s
mance.                                                            s2      3          s2     s3
2.1  Conversion algorithm
To convert a HMM  to DW-HMM, we should make a set      Figure 3: Representation of self-transitions of HMMs in Φ.
of super-states, S, from the states of HMM which repre-
sent unique emission probabilities. When S consists of
a small number of super-states and the original HMM’s 2.2  Conditions for equivalence
state transition structure is large, the DW-HMM is more In the conversion process, DW-HMM conserves transition
efﬁcient than HMMs. For example, a LT-HMM is a good   and emission probabilities. However, there is a difference
candidate to convert to a DW-HMM, because it only has a between HMM and its associated DW-HMM when we use
few states with unique emission probabilities and contains straightforward Viterbi algorithm. Figure 4 illustrates trellis
large trie dictionary. The following explains the algorithm of
                                                      of DW-HMM   at t =3and the state s1, the only state path
converting a HMM to DW-HMM.
                                                      {s1,s2,s1} that can propagate further since {s1,s3,s1} has
                                                      a smaller probability than {s1,s2,s1}. Therefore, the path
                                                      {s1,s3,s1} is discarded in a purging step of the Viterbi algo-
Algorithm: Conversion of HMM to DW-HMM                rithm. However, in case of HMM shown in Figure 5, at t =3
                                                      and the state q1, both state paths {q1,q2,q1} and {q1,q3,q1}
                                                      can propagate to the next time instant because there are two
 1. Make a set of super-states which have unique emission
                                                      q1 states.
    probabilities, S = {s1,s2,...,sK }, from a HMM. In
    this step, the number of states of HMM is reduced as To ensure that the results of DW-HMM and HMM are the
    shown in Figure 1.                                same, we adapt N-best search for DW-HMMs and we choose
                                                      the best path at the last time instant in the trellis. Figure 6
                                                      shows that the two-best search makes two hypotheses at s1
 2. If there are any loops (self-transitions) in the HMM, and t =3. Both state paths {s1,s2,s1} and {s1,s3,s1} are
    add additional super-states. For example, if si has kept to propagate further.
    a loop, an additional sj state is made. It makes it
    possible to distinguish between self-transitions and
                                                                       s1      s1     s1
    non-self-transitions.
                                                                       s2      s2     s2
 3. Construct the DW-HMM   associated with the HMM
                                                                       s       s      s
    using super-states in S. State transitions are made                 3      3       3
    between super-states if there exists a state transition in
                                                                       t =1   t =2    t =3
    the HMM, from which the super-states are made. For
    example, in Figure 1, the 4-state DW-HMM has a state          Figure 4: Trellis for DW-HMM.
    transition from s2 to s3 because q2 may transition to q5
    in the 8-state HMM.

                                                                       q1      q1     q1
 4. Make a data structure, Φ, to deﬁne a weight function,
    ω(s1:t), which gives the transition probability of the             q1      q1     q1
    DW-HMM.    Φ contains the transition structure of the              q       q      q
    HMM   and stores transition probabilities in each node.             2       2      2
    In making Φ, self-transitions in the HMM are changed               q3      q3     q3
    as shown in Figure 3. The super-state that has a loop
    transitions to an additional super-state made from step            t =1   t =2    t =3
    2 and it transitions to states where the original state
    goes. The structure of Φ and the state transition struc-        Figure 5: Trellis for HMM.
    ture of the HMM may be different due to self-transitions.
                                                        Figure 7 illustrates the case when two-best search is useful
 5. Deﬁne emission probabilities of the DW-HMM’s super- in LT-HMM. We denote the physical property of the states
    states which are the same as of the corresponding states in the nodes, showing which states have the same emission

                                                IJCAI-07
                                                  2526                 s1     s1      s1                    the same observation symbols as the model of Lee and Ng.
                                                      Transition probabilities of the DW-HMM are determined in
                 s1     s1      s
                                 1                    the decoding phase by a weight function equipped with Φ,
                 s2     s2      s2                    which is a data structure of lexicon tree containing transition
                                                      probabilities of the LT-HMM. Null transitions are allowed
                 s      s       s
                  2      2       2                    and their transition probabilities are also determined by the
                 s3     s3      s3                    weight function. It recovers deletion of characters in the in-
                                                      put data. 1
                 s3     s3      s3                      The set of individual hidden super-states of the DW-HMM
                                                      converted from the LT-HMM is as follows:
                 t =1   t =2   t =3
                                                        S   {s ,s ,s ,...,s }
         Figure 6: Trellis for two-best search.           =   1  2 3      55 ,

                                                      where s1 is an initial state and states in {s2,...,s27} are
probability. q1 and qN are the initial and ﬁnal state of the LT- match states. States in {s28,...,s54} are insert states and s55
HMM, respectively. Assuming that there exist two hypothe- is the ﬁnal state. A match state is the super-state representing
ses at t =4, {q1,a,b,a}, {q1,a,c,a},theLT-HMMallow    the letters of the English alphabet and an insert state is the
them to propagate further. However, the DW-HMM cannot state stems from the self-transitions of the LT-HMM. Since
preserve both at t =4if we use one-best search. Since the self-transitions of the LT-HMM reﬂect insertion of characters
states labeled ”a” in the LT-HMM are grouped into a super- in obfuscated words, the state is named an insert state. The
state, one hypothesis should be selected at t =4. Thus, ﬁnal state is the state which represents the end of words.
in case of the DW-HMM, if the answer’s state sequence is Transition probability of the DW-HMM is as follows:
{q ,a,c,a,c,i,a,q }                     {q ,a,b,a}                     
  1             N , and only one hypothesis, 1    ,                      0       if s1:t does not match
          t
is chosen at =4, we will fail to ﬁnd the answer. We address P (st|st−1)=         apreﬁxinΦ
the problem with N-best search. If we adapt two-best search,             ω(s1:t) otherwise.
              {q ,a,b,a} {q ,a,c,a}
two hypotheses, 1       ,  1      , are able to propa-  The structure of Φ is made using lexicon tree and each node
gate further, thereby preserving the hypothesis for the answer. of Φ has the transition probability of the LT-HMM.
  HMM and converted DW-HMM give exactly the same re-                     s            s
                                   N                    Here, a hypothesis 1:t starts from 1 which is the initial
sults if we adapt the N-best search where is the maximum state decoded. The DW-HMM converted from the LT-HMM
number of states of HMMs that compose the same super-state has only one initial state and it may appear many times in
and propagate further at a time instant. However, for practi-
           N                                          the hypothesis since the input data is a sentence. To reduce
cal purposes, does not need to be large. Our application the computational cost in searching Φ, we can use the sub-
for spam deobfuscation shows that the DW-HMM works well          s
     N                                                sequence of 1:t to search Φ since it is possible to reach the
when   is just two.                                   same node of Φ if a subsequence starts from the initial state
                                                      at any time instant.
                                                        We deobfuscate spam emails by choosing the best path us-
                 baca                                 ing decoding algorithms given observation characters. For
                                                      example, given the observation characters, ”vi@”, the best
          a                                           state sequence, {s1,s23,s10,s2}, is chosen which represents
                 cacia                                ”via”.
                                                        We deﬁne emission probabilities as follows:
  q1                                           qN

                                                        P (y |st) when st is a match state
                                                            t  
                                                                 ρ   if y is a corresponding character
          zebra                                                   1      t
                                                            =    ρ2  if yt is a similar character
                                                                 ρ3  otherwise
Figure 7: Transition diagram of LT-HMM. By using two-best P y |s     s
                                                          ( t ⎧t) when t is a insert state
search, its associated DW-HMM is able to keep two states        σ     y
                                                             ⎨⎪  1  if t is a corresponding character,
labeled ”a” at a time instance.                                     a similar character,
                                                           =
                                                             ⎩⎪     or not a letter of the English alphabet
                                                                σ2  otherwise
3  Application
                                                        P (yt|st) when st is a ﬁnal state
A lexicon tree hidden Markov model(LT-HMM) for spam        
                                                              ψ1  if yt is a white space
deobfuscation was proposed by Lee and Ng [Lee and Ng,
                                                         =    ψ2  if yt is not a letter of the English alphabet
    ]
2005 . It consists of 110,919 states and 70 observation sym-  ψ3  otherwise.
bols, such as the English alphabet, the space, and all other
standard non-control ASCII characters. We transform the LT- 1More than one consecutive character deletion is not considered
HMM to the DW-HMM which consists of 55 super-states and due to the severe harm in readability.

                                                IJCAI-07
                                                  2527