        Planning for Weakly-Coupled Partially Observable Stochastic Games

                                    AnYuan Guo        Victor Lesser
                                  University of Massachusetts Amherst
                                    Department of Computer Science
                              140 Governor’s Drive, Amherst, MA 01003
                                    {anyuan, lesser}@cs.umass.edu


                                                           0
1  Introduction                                         • si ∈ ∆(Si) is the initial state distribution of agent i.
Partially observable stochastic games (POSGs) provide a • Ai is the action set of agent i. A = A1 × A2 × . . . × An
powerful framework for modeling multi-agent interactions. denotes the joint action set.

While elegant and expressive, the framework has been shown          Ai
                                                        • Bi : S → 2   is the available action function that maps
to be computationally intractable [Bernstein et al., 2002]. An a joint state to the set of available actions for agent i.
exact dynamic programming algorithm for POSGs has been    B  (s) ⊂ A for all s ∈ S .
developed recently, but due to high computational demands,  i      i           i
it has only been demonstrated to work on extremely small • Pi : Si × Ai × Si → [0, 1] is the transition function
problems. Several approximate approaches have been devel- of agent i. The joint transition function is completely
                                                                            0    0
oped [Emery-Montemerlo et al., 2004; Nair et al., 2003], but factored, where P ((s1 . . . sn) | (s1 . . . sn), (a1 . . . an))
                                                            Qn       0
they lack strong theoretical guarantees. In light of these the- = i=1 Pi(si|si, ai).
oretical and practical limitations, there is a need to identify • Ri : Si × Ai → < is the reward function for agent i.
special classes of POSGs that can be solved tractably.    This states that the rewards of the agents depend only on
  One dimension along which computational gain can be     the local state and the action taken by agent i.
leveraged is by exploiting the independence present in the
problem dynamics. In this paper, we examine a class of  In our model, each agent has a complete view of its own
POSGs where the agents only interact loosely by restricting local state. A local policy for each agent is a mapping from
one another’s available actions. Speciﬁcally, rather than hav- the local state and the available action set to an action in that
ing ﬁxed action sets, each agent’s action set is a function of set. A joint policy is a tuple of local policies, one for each
the global state. The agents are independent from one another agent.
otherwise, i.e. they have separate transition and reward func- Deﬁnition 1 A local policy πi, for agent i of an n-agent
tions that do not interact. This class of problems arises fre- POSG with state-dependent action sets, is a mapping from
quently in practice. Many real world domains are inhabited pairs of local states and local action sets hsi, Ai(s)i to
by self-interested agents that act to achieve their individual actions in the current local action set Ai(s), where s =
goals. They may not affect each other in any way, except for hs1, . . . , si, . . . , sni.
occasionally putting restrictions on what each other can do.
                                                        Here is an example of a POSG with state-dependent ac-
  The best-known solution concepts in game theory are that
                                                      tion sets. Two autonomous rovers are exploring an unknown
of computing Nash equilibrium and performing strategy elim-
                                                      terrain and collecting rock samples. There is a river with a
ination. Our work addresses both of these solution concepts.
                                                      narrow bridge on it that only allows one rover to pass at a
First, we show that ﬁnding a Nash equilibrium where each
                                                      time. To simplify the illustration, we will assume the rovers
agent achieves reward at least k is NP-hard. Another contri-
                                                      have two states {on land, on bridge}, and three actions each
bution of this work is that we present an exact algorithm for
                                                      {move, get on bridge, pick up rock}. The rovers receive re-
performing iterated elimination of dominated strategies. It is
                                                      ward for the number of rocks picked up. The state-dependent
able to solve much larger problems than exact algorithms for
                                                      action set is used to constrain the rover’s actions such that
the general class by exploiting problem structure.
                                                      only one is allowed on the bridge at a time. For exam-
                                                      ple, the available actions for rover 1 are speciﬁed as follows,
2  POSGs with State-Dependent Action Sets             B1(hs1 = on land, s2 = on landi) = {move, get on bridge,
In this section, we will present a formal deﬁnition of our pick up rock}, B1(hs1 = on land, s2 = on bridgei) = {move,
model.  An  n-agent POSG  with state-dependent action pick up rock}, and analogously for rover 2.
                          0
sets can be deﬁned as h{Si}, {si }, {Ai}, {Bi}, {Pi}, {Ri}i,
where,                                                3   Complexity

  • Si is the state space of agent i. S = S1 × S2 × . . . × Sn We state our decision problem, denoted as POSG-NE, as
    denotes the joint state set.                      follows: given a POSG with state-dependent action sets,            0
G =  h{Si}, {si }, {Ai}, {Bi}, {Pi}, {Ri}i, does there exist                 before     after
a Nash equilibrium where all agents have expected utility at      # states  pruning    pruning
least k?                                                               6    972,000      13
                                                                       8   34,992,000   168
Theorem 1 POSG-NE is NP-hard even in problems involv-                             8
ing 2 agents and a horizon of 2.                                       9    2.1 ×10      69
                                                                      12   4.5 ×1010    774
                                                                                  13
4  Iterated Action Elimination Algorithm                              16   5.9 ×10      2,198
We present an algorithm that performs iterated elimination of Table 2: Average size of the policy space before and after
dominated strategies. The algorithm is able to work directly action elimination for problems with 3 constrained states.
with the compact representation of a POSG without ﬁrst con-
verting it to the double exponentially large normal form rep-
resentation. In this algorithm, ﬁrst, we ﬁrst ﬁx the action sets Once we ﬁx the available action sets of each agent at all the
of each agent at each state to the optimistic and pessimistic states, the agents no longer depend on each other in any way.
action sets. The idea behind this is that although an agent Our model decomposes to a set of MDPs. For each agent, we
cannot predict exactly what actions will be available at each solve two MDPs, one using the optimistic action sets and the
state, it can ﬁnd out what actions would be available in the other using the pessimistic action sets. The solutions will pro-
best and worst case scenario. In the best case, none of the ac- vide us with an upper and a lower bound on the actual value
tions that can be restricted by the state of the other agents are, function of each agent. With these upper and lower bounds on
and in the worst case, all of the actions that can be restricted the expected values of each state, we can now derive bounds
are in fact unavailable.                              on the expected action values. At each state, dominated ac-
                                                      tions are pruned. We iterate the action elimination procedure
                                                      until no more actions can be pruned. Since the number of
 For each agent i:
                                                      actions at each state is ﬁnite, the procedure will eventually
   1. For each state sk, compute the optimistic and pes- converge.
     simistic action sets. Here s = hs1, s2, . . . , sni and
      i    i                                          Theorem 2  The iterated action elimination algorithm corre-
     s =  sk.                                         sponds to the iterated elimination of very weakly dominated
                       i    [
                  Aopt(sk) =   Bi(s)                  strategies.
                             s
                        i   \                         5   Experiments
                  Apes(sk) =   Bi(s)
                             s                        We tested the algorithm on a simpliﬁed 2-agent autonomous
                                                      rover exploration scenario. The size of each agent’s local
   2. Compute the value functions of the 2 MDPs that cor-
                                                      state space varies from 6 to 16. We introduced up to three
     respond to alternately ﬁxing the available action sets
                                                      constrained states in each agent’s state space. 100 trials were
     to the A  and A  .
            opt     pes                               run for each size of state space and number of constraints.
                    "                          #      Table 2 shows detailed results for problems with three con-
                              X
     U(s) =   max    R(s, a) +    P (s0|s, a)U(s0)    strained states. For problems with one and two constrained
            a∈A   (s)
                opt            s0                     states, the ﬁnal policy space ranges from 3 to 106 and 8 to
                    "                          #      252 respectively. In all three cases, the iterated action elimi-
                              X                       nation algorithm is able to reduce the size of the policy space
      L(s) =  max     R(s, a) +   P (s0|s, a)L(s0)
            a∈A   (s)                                 by several orders of magnitude.
                pes            s0
   3. For each action in each state, derive upper and lower References
     bounds on the action value from the value function [Bernstein et al., 2002] D. S. Bernstein, R. Givan, N. Immer-
     bounds U(s) and L(s).                               man, and S. Zilberstein. The complexity of decentralized
                            X                            control of Markov decision processes. Mathematics of Op-
         Q  (s, a) = R(s, a) +  P (s0|s, a)U(s)
           U                                             erations Research, 27:4, November 2002.
                             s0
                                                      [Emery-Montemerlo et al., 2004] R. Emery-Montemerlo,
                            X      0
         QL(s, a) = R(s, a) +   P (s |s, a)L(s)          G. Gordon, J. Schneider, and S. Thrun. Approximate
                             s0                          solutions for partially observable stochastic games with
                                    0                    common payoffs. In Proceedings of the Third Joint Con-
   4. At each state, eliminate all actions a whose action ference on Autonomous Agents and Multiagent Systems,
     value is dominated by another action a, QL(s, a) ≥
            0                                            2004.
     QU (s, a ).
                                                      [Nair et al., 2003] R. Nair, M. Tambe, M. Yokoo, D. Pyna-
   5. Repeat steps 1 to 4 until no more actions can be   dath, and S. Marsella. Taming decentralized POMDPs:
     eliminated at any state.                            Towards efﬁcient policy computation for multiagent set-
                                                         tings. In Proceedings of the Eighteenth International Joint
    Table 1: The iterated action elimination algorithm.  Conference on Artiﬁcial Intelligence, 2003.