                       Stereotype    Extraction     with  Default   Clustering

                               Julien Velcin and Jean-Gabriel   Ganascia
                                        LIP6, Universite´ Paris VI
                                        8 rue du Capitaine Scott
                                        75015  PARIS,  FRANCE


                    Abstract                          ﬁrst on artiﬁcial data sets and secondly with a real data case
                                                      generated from newspaper articles.
    The concept of stereotype seems to be really
    adapted when wishing to extract meaningful de-
    scriptions from data, especially when there is a high 1 Conceptual clustering with sparse data
    rate of missing values. This paper proposes a log- 1.1 Dealing with missing values
    ical framework called default clustering based on This paper proposes a clustering method that deals with high
    default reasoning and local search techniques. The rates of missing values. But contrary to algorithms such as k-
    ﬁrst experiment deals with the rediscovering of ini- modes (categorical version of k-means) or EM, that can eas-
    tial descriptions from artiﬁcial data sets, the second ily lead to local optima, we have chosen to achieve the clus-
    one extracts stereotypes of politicians in a real case tering using a combinatorial optimization approach, like in
    generated from newspaper articles. It is shown that [Figueroa et al., 2003] or [Sarkar and Leong, 2001]. Note that
    default clustering is more adapted in this context our goal is not only to cluster examples but also and mainly
    than the three classical clusterers considered.   to describe the cluster in a way that is simple and easy to
                                                      understand. The problem can thus be stated as ﬁnding read-
Introduction                                          able, understandable, consistent and rich descriptions from
                                                      the data.
Conceptual clustering [Michalski, 1980] is a fundamental
machine learning task that is applied in various areas such 1.2 Overview of default logic
as image analysis, analytical chemistry, biology, sociology. During the eighties, there were many attempts to model de-
It takes a set of object descriptions as input and creates a ductive reasoning when missing information exists. A lot of
classiﬁcation scheme. The conceptual descriptions of clus- formalisms were developed to encompass the inherent difﬁ-
ters are of particular interest to reason about the categories culties of such models, especially their non-monotony: close-
themselves, to compare different data sets and to predict new world assumption, circumscription, default logic, etc. Since
observations. This work focuses on the extraction of such our goal is to deal with missing values, it seems natural to
conceptual descriptions, especially in the speciﬁc context of take advantage of this work. The default logic formalism, in-
missing data.                                         troduced by R. Reiter in 1980, was chosen because it seemed
  Automatic inductive techniques have to deal with missing to correspond well to our problem.
information, due to voluntary omissions, human error, bro- This logic for default reasoning is based on the notion of
ken equipment [Newgard and Lewis, 2002]. In the context of default rule, through which it is possible to infer new formu-
sparse data, i.e. with a huge amount of missing values, the las when the hypotheses are not inconsistent with the current
concept of stereotype seems more appropriate than the usual context. More generally, a default rule always has the fol-
one of prototype to describe data clusters. Therefore, our goal lowing form: A : B1, B2 . . . Bn/C where A is called the
is to extract stereotype sets that represent the data sets as well prerequisite, Bi the justiﬁcations and C the conclusion. This
as possible. By analogy to default logic [Reiter, 1980], which default rule can be interpreted as follows: if A is known to be
is a speciﬁc logic for default deduction, we make use of de- true and if it is consistent to assume B1, B2 . . . Bn then con-
                                                      clude C. For instance, let us consider the default rule below
fault subsumption, which is a speciﬁc logic for default induc- related to the experiments of the last section:
tion, to build such stereotypes.
  Section 1 presents a new approach to conceptual clustering politician(X) ∧ introducedAbroad(X) : ¬diplomat(X)
when missing information exists. Section 2 proposes a gen-                 traitor(X)
eral framework in the attribute-value formalism. The new no- This rule translates an usual way of reasoning for many peo-
tion of default subsumption is introduced, before seeing how ple living in France at the end of the 19th century. It states
the concept of stereotype makes it possible to name clusters. that the conclusion traitor(X) can be derived if X is a
A stereotype set extraction algorithm based on local search politician who is known to be introduced abroad and that we
techniques is then presented. Section 3 concerns experiments, cannot prove that he is a diplomat.  The key idea here is to use similar observations and their which signiﬁes that d0 ∧ d00 entails d. The exact deﬁnition
descriptions to infer new information instead of default rules, follows:
                                                                                0                      0
but the underlying mechanism is the same. The following Deﬁnition 1 d subsumes d by default (noted d ≤D d )
                                                                                             0
subsection explains the transition from default logic to default iff ∃ dc such that dc 6=⊥ and d ≤ dc and d ≤ dc where
                                                           0                   0
induction.                                            t ≤ t stands for t subsumes t in the classical sense. dc is a
                                                      minorant of d and d0 in the subsumption lattice.
1.3  Default clustering                                 To illustrate our deﬁnition, here are some descriptions
E. Rosch saw the categorization itself as one of the most im- based on binary attributes that can be compared with respect
portant issues in cognitive science [Rosch, 1975]. She in- to the default subsumption:
troduced the concept of prototype as the ideal member of d1 = {(T raitor = yes), (Internationalist = yes)}
a category. Whereas categorization makes similar observa- d2 = {(T raitor = yes), (Connection with jews = yes)}
tions ﬁt together and dissimilar observations be well sepa- d3 = {(P atriot = yes)}
rated, clustering is the induction process in data mining that d1 ≤D d2 and d2 ≤D d1 because ∃ dc such that d1 ≤ dc
actually build such categories. More speciﬁcally, conceptual and d2 ≤ dc: dc = {(T raitor = yes), (Internationalist =
clustering is a machine learning task deﬁned by R. Michalski yes), (Connection with jews = yes)}.
[Michalski, 1980] which does not require a teacher and uses However, considering that a patriot cannot be an interna-
an evaluation function to discover classes that have appropri- tionalist and vice-versa, i.e. ¬((Patriot=yes) ∧ (Internation-
ate conceptual descriptions. Conceptual clustering was prin- alist=yes)), which was an implicit statement for many people
cipally studied in a probabilistic context (see, for instance, D. living in France at the end of the 19th century, d1 does not
Fisher’s Cobweb algorithm [Fisher, 1987]) and rarely on re- subsume d3 by default, i.e. ¬(d1 ≤D d3).
ally sparse data sets. For instance, the experiments done by Property 1 The notion of default subsumption is more gen-
                                                                                                    0
P.H. Gennari do not exceed 30% of missing values [Gennari, eral than classical subsumption since, if d subsumes d , i.e.
                                                            0                0                     0
1990].                                                d ≤  d , then d subsumes d by default, i.e. d ≤D d . The
  As seen above, default logic is a logic for deduction de- converse is not true.
pending on background knowledge. This paper proposes a  Property 2 The default subsumption relationship is sym-
                                                                      0        0      0
new technique called default clustering which uses a similar metrical, i.e. ∀d ∀d if d ≤D d then d ≤D d.
principle but for induction when missing information exists. Note that the notion of default subsumption may appear
The main assumption is the following: if an observation is strange for people accustomed to classical subsumption be-
grouped with other similar observations, you can use these cause of the symmetrical relationship. As a consequence, it
observations to complete unknown information in the original does not deﬁne an ordering relationship on the description
fact if it remains consistent with the current context. Whereas space D. The notation ≤D may be confusing with respect
default logic needs implicit knowledge expressed by default to this symmetry, but it is relative to the underlying idea of
rules, default clustering only uses information available in the generality.
data set. The next section presents this new framework. It
shows how to extract stereotype sets from very sparse data: 2.2 Concept of stereotype
ﬁrst it extends classical subsumption (see section 2.1), next In the literature of categorization, Rosch introduced the con-
it discusses stereotype choice (see section 2.2), and ﬁnally it cept of prototype [Rosch, 1975; 1978] inspired by the fam-
proposes a local search strategy to ﬁnd the best solution (see ily resemblance notion of Wittgenstein [Wittgenstein, 1953]
section 2.4).                                         (see [Shawver, 2004] for an electronic version and [Narboux,
                                                      2001] for an analysis focused on family resemblance). Even
2  Logical  framework                                 if our approach and the original idea behind the concept of
                                                      prototype have several features in common, we prefer to refer
This section presents the logical framework of default clus- to the older concept of stereotype that was introduced by the
tering in the attribute-value formalism (an adaptation to con- publicist W. Lippman [Lippman, 1922]. For him, stereotypes
ceptual graphs can be found in [Ganascia and Velcin, 2004]). are perceptive schemas (a structured association of character-
The description space is noted D, the descriptor space (i.e. istic features) shared by a group about other person or object
the values the attributes can take) V and the example set E. categories. These simplifying and generalizing images about
The function δ maps each example e ∈ E to its description reality affect human behavior and are very subjective. Below
δ(e) ∈ D.                                             are three main reasons to make such a choice.
                                                        First of all, the concept of prototype is often misused in
2.1  Default subsumption                              data mining techniques. It is reduced to either an average ob-
Contrary to default logic, the problem here is not to deduce, servation of the examples or an artiﬁcial description built on
but to induce knowledge from data sets in which most of the the most frequent shared features. Nevertheless, both of them
information is unknown. Therefore, we put forward the no- are far from the underlying idea in family resemblance. Es-
tion of default subsumption, which is the equivalent for sub- pecially in the context of sparse data, it seems more correct to
sumption of the default rule for deduction. Saying that a de- speak about a combination of features found in different ex-
scription d ∈ D subsumes d0 ∈ D by default means that there ample descriptions than about average or mode selection. The
exists an implicit description d00 such that d0 completed with second argument is that the notion of stereotype is often de-
d00, i.e. d0 ∧ d00, is more speciﬁc than d in the classical sense, ﬁned as an imaginary picture that distorts the reality. Our goalis precisely to generate such pictures even if they are caricat- 2.4 Stereotype extraction
ural of the observations. Finally, these speciﬁc descriptions In this paper, default reasoning is formalized using the no-
are better adapted for fast classiﬁcation (we can even say dis- tions of both default subsumption and stereotype set. Up to
crimination) and prediction than prototypes. This last feature now, these stereotype sets were supposed to be given. This
is closely linked to Lippman’s deﬁnition.             section shows how the classiﬁcation can be organized into
  In order to avoid ambiguities, we restrict the notion of such sets in a non-supervised learning task. It can be summa-
stereotype to a speciﬁc description d ∈ D associated to (we rized as follows. Given:
can say “covering”) a set of descriptions D ⊂ D. How-   1. An example set E.
ever, the following subsection does not deal just with stereo-
                                                        2. A description space D.
types but with stereotype sets to cover a whole description set.
                                                        3. A description function δ: E −→ D which associates a
The objective is therefore to automatically construct stereo-
                                                      description δ(e) ∈ D to each example belonging to the train-
type sets, whereas most of the studies focus on already ﬁxed
                                                      ing set E.
stereotype usage [Rich, 1979; Amossy and Herschberg Pier-
                                                      The function of a non-supervised learning algorithm is to or-
rot, 1997]. Keeping this in mind, the space of all the possible
                                                      ganize the initial set of individuals E into a structure (for
stereotype sets is browsed in order to discover the best one,
                                                      instance a hierarchy, a lattice or a pyramid). In the present
i.e. the set that best covers the examples of E with respect
                                                      case, the structure is limited to partitions of the training set,
to some similarity measure. But just before addressing the
                                                      which corresponds to searching for stereotype sets as dis-
search itself, we should consider both the relation of relative
                                                      cussed above. These partitions may be generated by (n + 1)
cover and the similarity measure used to build the categoriza-
                                                      stereotypes S = {s , s , s . . . s }: it is sufﬁcient to asso-
tion from stereotype sets.                                             ∅  1  2    n
                                                      ciate to each si the set Ei of examples e belonging to E and
2.3  Stereotype sets and relative cover               covered by si relative to S. The examples that cannot be cov-
                                                      ered by any stereotype are put into the E cluster and associ-
Given an example e characterized by its description d =                                 ∅
                                                      ated to s∅.
δ(e) ∈ D, consider the following statement: the stereotype To choose from among the numerous possible partitions,
s ∈ D can cover e if and only if s subsumes d by default. It which is a combinatorial problem, a non-supervised algo-
means that in the context of missing data each piece of infor- rithm requires a function for evaluating stereotype set rele-
mation is so crucial that even a single contradiction prevents vance. Because of the categorical nature of data and the pre-
the stereotype from being a correct generalization. Further- vious deﬁnition of relative cover, it appears natural to make
more, since there is no contradiction between this example
                                                      use of the similarity measure Msim. This is exactly what we
and its related stereotype, the stereotype may be used to com- do by introducing the following evaluation function h :
plete the example description.                                                                     E
                                                        Deﬁnition  3   E   being an  example  set, S   =
  In order to perform the clustering, a very general similarity
                                                      {s∅, s1, s2 . . . sn} a stereotype set and CS the function that
measure Msim has been deﬁned, which counts the number associates to each example e its relative cover, i.e. its closest
of common descriptors of V belonging to two descriptions,
                                                      stereotype with respect to Msim and S, the evaluation func-
ignores the unknown values and takes into account the default tion h is deﬁned as follows:
subsumption relationship:                                  E
                    +
 Msim: D×D     →  N
                                                                 hE(S) =  X  Msim(δ(e), CS (e))
       (di, dj ) 7→ Msim(di, dj ) = |{v ∈ d/d = di ∧ dj }|
                                                                          e∈E
                      if di ≤D dj and
                  Msim(di, dj ) = 0 if ¬(di ≤D dj ),    While k-modes and EM algorithms are straightforward, i.e.
  where di ∧ dj is the least minorant of di and dj in the sub- each step leads to the next one until convergence, we re-
sumption lattice.                                     duce here the non-supervised learning task to an optimization
  Let us now consider a set S = {s∅, s1, s2 . . . sn} ⊂ D of problem. This approach offers several interesting features:
stereotypes. s∅ is the absurd-stereotype linked to the set E∅. avoiding local optima (especially with categorical and sparse
Then, a categorization of E can be calculated using S with an data), providing “good” solutions even if not the best ones,
affectation function that we called relative cover:   better control of the search. In addition, it is not necessary to
  Deﬁnition 2 The relative cover of an example e ∈ E, with specify the number of expected stereotypes that is also dis-
respect to a set of stereotypes S = {s∅, s1, s2 . . . sn}, noted covered during the search process.
CS(e), is the stereotype si if and only if:             There are several methods for exploring such a search
  • si ∈ S,                                           space (hill-climbing, simulated annealing, etc.). We have
                                                      chosen the meta-heuristic called tabu search which improves
  • MS (δ(e), si) > 0,
                                                      the local search algorithm. Remember that the local search
  • ∀k ∈ [1, n], k 6= i, Msim(δ(e), si) > Msim(δ(e), sk). process can be schematized as follows: 1. An initial solution
  It means that an example e ∈ E is associated to the most Sini is given (for instance at random). 2. A neighborhood P
similar and “covering-able” stereotype relative to the set S. is calculated from the current solution Si with the assistance
If there are two competitive stereotypes with an equal higher of permitted movements. These movements can be of low
score or if there is no covering stereotype, then the example is inﬂuence (enrich one stereotype with a descriptor, remove a
associated to the absurd-stereotype s∅. In this case, no com- descriptor from another) or of high inﬂuence (add or retract
pletion can be calculated for e.                      one stereotype to or from the current stereotype set). 3. The  Sc ← {s∅} ; the current solution                      A new constraint called cognitive cohesion is now deﬁned.
  Sb ← Sc ; the best up-to-now solution
  T ← ∅ ; the list that contains tabu-attributes      It veriﬁes cohesion within a cluster, i.e. an example set Ej ⊂
  for i ← 1 to NStep do {
                                                      E, relative to the corresponding stereotype sj ∈ S. Cognitive
     let Sc be {s∅, s1, s2 . . . sk }
     P ← ∅ ; initialize the neighborhood              cohesion is veriﬁed if and only if, given two descriptors v1
     for all Ai ∈/ T do {                             and v2 ∈ V  of sj , it is always possible to ﬁnd a series of
        for m ← 1 to k do {                           examples that makes it possible to pass by correlation from
          for all v ← (Ai = Vij ) do
          if v does not belong to a stereotype of Sc { v1 to v2. Below are two example sets with their covering
             S ← {s1 . . . sm−1, sm ∪ v, sm+1 . . . sk} stereotype. The example on the left veriﬁes the constraint,
             P ← P ∪ S }                              the one on the right does not.
          if v belongs to a stereotype of Sc {
              0
             s ← sm \ v                                        s1 : a0 , b1 , d5 , f0 , h0 s2 : a0 , b1 , d5 , f0 , h0
                0                      0
             if s 6= ∅ then S ← {s1 . . . sm−1, s , sm+1 . . . sk } e1 : a0, ?, ?, ?, h0 e0 : a0, b1, ?, ?, ?
             else S ← {s1 . . . sm−1, sm+1 . . . sk }          e2 : a0, b1, ?, ?, ? e8 : ?, ?, ?, f0, ?
             if S 6= ∅ then P ← P ∪ S }                        e6 : ?, ?, d5, ?, ? e9 : a0, b1, ?, ?, ?
        }                                                      e8 : ?, b1, d5, f0, ? e51: ?, ?, d5, ?, h0
        for all v ← (Ai = Vij )                                e42: a0, ?, d5, ?, ? e98: ?, ?, d5, ?, h0
        if v does not belong to a stereotype of Sc do {
          sn ← {v}
          S ← {s1, s2 . . . sm, sn}                   Hence, with s2 it is never possible to pass from a0 to d5,
          P ← P ∪ S }                                 whereas it is allowed by s1 (you begin with e2 to go from a0
     }                                                to b , and then you use e to go from b to d ). It means that,
     SN ← argmax S∈P (hE (S))                            1                 8          1    5
     T is updated, depending on the chosen attribute Ai in the case of s1, you are always able to ﬁnd a “correlation
        that permits to pass from Sc to SN .          path” from one descriptor of the description to another, i.e.
     Sc ← SN
     if hE (Sc) > hE (Sb) then Sb ← Sc }              examples explaining the relationship between the descriptors
  return Sb                                           in the stereotype.
                                                      3   Experiments
        Figure 1: The default clustering algorithm    This section presents experiments performed on artiﬁcial data
                                                      sets. This is followed by an original comparison in a real
best movement, relative to the evaluation function hE, is cho- data case using three well-known clusterers. Default clus-
sen and the new current solution Si+1 is computed. 4. The tering was implemented in a Java program called PRESS
process is iterated a speciﬁc number of times NStep and the (Programme de Reconstruction d’Ensembles de Ster´ eotypes´
best up-to-now discovered solution is recorded. Then, the Structures´ ). All the experiments for k-modes, EM and Cob-
solution is the stereotype set Sb that best maximizes hE in web were performed using the Weka platform [Garner, 1995].
comparison to all the crossed sets.
  As in almost all local search techniques, there is a trade- 3.1 Validation on artiﬁcial data sets
off between exploitation, i.e. choosing the best movement, These experiments use artiﬁcial data sets to validate the ro-
and exploration, i.e. choosing a non optimal state to reach bustness of our algorithm. The ﬁrst step is to give some con-
completely different areas. The tabu search extends the basic trasted descriptions of D. Let us note ns the number of these
local search by manipulating short and long-term memories descriptions. Next, these initial descriptions are duplicated
which are used to avoid loops and to intelligently explore the nd times. Finally, missing data are artiﬁcially simulated by
search space. This meta-heuristic is detailed in [Glover and removing a percentage p of descriptors at random from these
Laguna, 1997] and its application to clustering can be found ns × nd artiﬁcial examples. The evaluation is carried out
in [Al-Sultan, 1995]. Note that only the short-term memory by testing different clusterers on these data and comparing
was used at this stage of our work.                   the discovered cluster representatives with the initial descrip-
                                                      tions. We verify what we call recovered descriptors, i.e. the
2.5  Default clustering algorithm                     proportion of initial descriptors that are found. This paper
Fig. 1 is the main frame of the default clustering algorithm. presents the results obtained with ns = 5 and nd = 50 over
It is based on a very basic version of tabu search that tries 50 runs. The number of examples is 250 and the descriptions
to maximize our function hE. Ai denotes the ith attribute are built using a langage with 30 binary attributes. The tabu-
and E is the example set. Sc and Sb stands respectively for list length is equal to 10 and NStep to 100. Note that the ﬁrst
the current and for the best solution. NStep is the maximal group of experiments are placed in the Missing Completely
number of iterations, P the current neighborhood and T the At Random (MCAR) framework.
tabu-list that contains tabu-attributes. If an attribute Ai is in Fig. 2 shows ﬁrstly that the results of PRESS are very good
the tabu-list then no descriptor (Ai = Vij ) can be used to with a robust learning process. The stereotypes discovered
calculate the neighborhood of the current solution.   correspond very well to the original descriptions up to 75%
  A “no-redundancy” constraint has been added in order to of missing descriptors. In addition, this score remains good
obtain a perfect separation between the stereotypes. In the (nearly 50%) up to 90%. Whereas Cobweb seems stable rel-
context of sparseness, it seems really important to extract ative to the increase in the number of missing values, the re-
contrasted descriptions which are used to quickly classify the sults of EM rapidly get worse above 80%. Those obtained us-
examples, as does the concept of stereotype introduced by ing k-modes are the worst, although the number of expected
Lippman.                                              classes has to be speciﬁed.                                                                               k-Modes       EM
                                                                          (1) (2)  (3) (4) (1)  (2)
                                                                  n       6    6   6   6    2   2
                                                              ex. contradiction 27 0 27 0  48   0
                                                              av. contradiction 42 0 44 0  56   0
                                                                 hE       .89 .60  .74 .50 .85 .66
                                                               redundancy 70  63   0   0   17   7
                                                              cog. cohesion × ×    ×   ×   ×    ×
                                                                      EM          Cobweb      PRESS
                                                                    (3)  (4) (1) (2)  (3) (4)
                                                               n     2   2   2    2   2   2     6
                                                             ex. cont. 48 0  56   0   57  0     0
                                                             av. cont. 56 0  52   0   51  0     0
                                                              hE    .83  .65 .82 .56  .68 .46  .79
                                                              red.   0   0   72  55   0   0     0
                                                            cog. coh. ×  ×   ×   ×    ×   ×     X

                                                             Figure 3: Comparative results on Le Matin.
      Figure 2: Proportion of recovered descriptors.
                                                      with its representative. This facet of conceptual clustering is
                                                      very important, especially in the sparse data context.
3.2  Studying social misrepresentation                  Secondly, we check if the cognitive cohesion constraint
The second part of the experiments deals with real data ex- (see 2.5) is veriﬁed. The rate of descriptor redundancy is also
tracted from a newspaper called “Le Matin” at the end of the considered. These two notions are linked to the concept of
19th century in France. The purpose is to automatically dis- stereotype and to the sparse data context.
cover stereotype sets from events related to the political dis- Finally, we consider the degree of similarity between the
order in the ﬁrst ten days of September 1893. The results of examples and their covering representatives. This corre-
PRESS are compared to those of the three clusterers k-modes, sponds to the notion of compactness within clusters, but with-
EM  and Cobweb. It should be pointed out that our interest out penalizing the stereotypes with many descriptors. The
focuses on the cluster descriptions, which we call representa- function hE seems really adapted to give an account of rep-
tives to avoid any ambiguity, rather than on the clusters them- resentative relevance. In fact, we used a version of hE nor-
selves.                                               malized between 0 and 1, by dividing by the total number of
  The articles linked to the chosen theme were gathered and descriptors.
represented using a language with 33 attributes. The terms of
this language, i.e. attributes and associated values, were ex- 3.4 Results
tracted manually. Most of the attributes are binary, 4 accept Fig. 3 gives the results obtained from the articles published
more than two values and 4 are ordinals. The number of ex- in Le Matin. Experiments for the k-modes algorithm were
tracted examples is 63 and the rate of missing data is nearly carried out with N = 2 . . . 8 clusters, but only N = 6 re-
87%, which is most unusual.                           sults are presented in this comparison. The rows of the ta-
                                                      ble show the number n of extracted representatives, the two
3.3  Evaluation of default clustering
                                                      scores concerning contradiction, the result of hE, the redun-
In order to evaluate PRESS, a comparison was made with dancy score and whether or not the cognitive cohesion con-
three classical clusterers: k-modes, EM and Cobweb. Hence, straint is veriﬁed. The columns represent each type of experi-
a non-probabilistic description of the clusters built by these ment (k-modes associated with techniques (1) to (4), EM and
algorithms was extracted using four techniques: (1) using the Cobweb as well, and ﬁnally our algorithm PRESS).
most frequent descriptors (mode approach); (2) the same as Let us begin by considering the contradiction scores.
(1) but forbidding contradictory features between the exam- They highlight a principal result of default clustering: us-
ples and their representative; (3) dividing the descriptors be- ing PRESS, the percentage of examples having contradictory
tween the different representatives; (4) the same as (3) but features with their representative is always equal to 0%. In
forbidding contradictory features. Two remarks need to be contrast, the descriptions built using techniques (1) and (3)
made. Firstly, the cluster descriptions resulting from k-modes (whatever the clusterer used) possess at least one contradic-
correspond to technique (1). Nevertheless, we tried the other tory descriptor with 27% to 57% of the examples belonging
three techniques exhaustively. Secondly, representatives re- to the cluster. Furthermore, around 50% of the descriptors
sulting from extraction techniques (3) and (4) entail by con- of these examples are in contradiction with the covering de-
struction a redundancy rate of 0%. The comparison was made scription, and that can in no way be considered as a negligible
according to the following three points:              noise. This is the reason why processes (1) and (3) must be
  The ﬁrst approach considers the contradictions between an avoided, especially in the sparse data context, when building
example and its representative. The example contradiction is such representatives from k-modes, EM or Cobweb cluster-
the percentage of examples containing at least one descriptor ing. Hence, we only consider techniques (2) and (4) in the
in contradiction with its covering representative. In addition, following experiments.
if you consider one of these contradictory examples, average Let us now study the results concerning clustering qual-
contradiction is the percentage of descriptors in contradiction ity. This quality can be expressed thanks to the compactness