  An Analysis of Laplacian Methods for Value Function Approximation in MDPs

                                             Marek Petrik
                                    Department of Computer Science
                                      University of Massachusetts
                                          Amherst, MA 01003
                                          petrik@cs.umass.edu

                    Abstract                          cently, a new framework for automatically constructing the
                                                      basis was proposed in [Mahadevan, 2005]. This approach
    Recently, a method based on Laplacian eigenfunc-  is based on analysis of the neighborhood relation among the
    tions was proposed to automatically construct a ba- states. The analysis is inspired by spectral methods, often
    sis for value function approximation in MDPs. We  used in machine-learning tasks. We describe this framework
    show that its success may be explained by drawing in more detail in Section 2.
    a connection between the spectrum of the Lapla-     In the following, we use v to denote the value function,
    cian and the value function of the MDP. This expla- and r to denote the reward vector. The matrix I denotes an
    nation helps us to identify more precisely the con- identity matrix of an appropriate size. We use n to denote the
    ditions that this method requires to achieve good number of states of the MDP.
    performance. Based on this, we propose a modi-      An important property of many VFA methods is that they
    ﬁcation of the Laplacian method for which we de-  are guaranteed to converge to an approximately optimal so-
    rive an analytical bound on the approximation er- lution. In methods based on approximate policy iteration the
    ror. Further, we show that the method is related  maximal distance of the optimal approximate value function
    the augmented Krylov methods, commonly used to    vˆ from the optimal value function v∗ is bounded by [Munos,
    solve sparse linear systems. Finally, we empiri-  2003]:
    cally demonstrate that in basis construction the aug-
                                                                    ∗           2γ
    mented Krylov methods may signiﬁcantly outper-         lim sup v − vˆμ ≤       sup vk − v˜kμ ,
                                                                             (1 − γ)2             k
    form the Laplacian methods in terms of both speed       k→∞                       k
    and quality.                                      where vk and v˜k are true and approximated value at step k
                                                      given the current policy. The norm ·μ denotes a weighted
                                                      quadratic norm with distribution μ. The distribution μ is ar-
1  Introduction                                       bitrary, and μk depends on μ and the current transition ma-
Markov Decision Processes (MDP) [Puterman, 2005] are a trix [Munos, 2003]. Similar bounds hold for algorithms based
widely-used framework for planning under uncertainty. In on Bellman residual minimization, when vk − v˜k is re-
                                                               r − (I − γP   )˜v       P
this paper, we focus on the discounted inﬁnite horizon prob- placed by      πk k  , where πk is a transition
lem with discount γ such that 0 <γ<1. We also assume  matrix of the current policy. In addition, these bounds hold
ﬁnite state and action spaces. While solving this problem re- also for the max norm [Bertsekas and Tsitsiklis, 1996]. In the
quires only polynomial time, many practical problems are too following, we refer to the value function vk and its approx-
large to be solved precisely. This motivated the development imation v˜k as v and v˜ respectively, without using the index
of approximate methods for solving very large MDPs that are k.
sparse and structured.                                  The main focus of the paper is the construction a good basis
  Value Function Approximation (VFA) is a method for ﬁnd- for algorithms that minimize v − v˜2 in each iteration. We
ing approximate solutions for MDPs, which has received a focus only on the quadratic approximation bound, because
lot of attention [Bertsekas and Tsitsiklis, 1996]. Linear ap- the other bounds are related. Notice that ·∞ ≤·2.
proximation is a popular VFA method, because it is simple to The paper is organized as follows. Section 2 describes
analyze and use. The representation of the value function in the spectral methods for VFA in greater detail. The main
linear schemes is a linear combination of basis vectors. The contribution of the paper is an explanation of the good per-
optimal policy is usually calculated using approximate value formance of spectral methods for VFA and its connection to
iteration or approximate linear programming [Bertsekas and methods for solving sparse linear systems. This is described
Tsitsiklis, 1996].                                    in Section 3, where we also propose two new alternative al-
  The choice of the basis plays an important role in solving gorithms. In Section 4, we show theoretical error bounds of
the problem. Usually, the basis used to represent the space one of the methods, and then in Section 6 we demonstrate
is hand-crafted using human insight about some topologi- that our method may signiﬁcantly outperform the previously
cal properties of the problem [Sutton and Barto, 1998]. Re- proposed spectral methods.

                                                IJCAI-07
                                                  25742  Proto-Value Functions                              3   Analysis
In this section, we describe in greater detail the proto-value In this section we show that if we use the actual transition
function methods proposed in [Mahadevan, 2005]. These matrix instead of the random walk Laplacian of the adjacency
methods use the spectral graph framework to construct a ba- matrix, the method may be well justiﬁed and analyzed. We
                                                                                                P
sis for linear VFA that respects the topology of the problem. assume in the following that the transition matrix and the
                                                                    r
The states of the MDP for a ﬁxed policy represent nodes reward function are available.
                                                                                                 P
of an undirected weighted graph (N,E). This graph may   It is reasonable to explain the performance using instead
                                                         L                              P
be represented by a symmetric adjacency matrix W , where of r, because in the past applications was usually very
                                                              I −L
Wxy represents the weight between the nodes. A diagonal similar to r. This is because the transition in these prob-
matrix of the node degrees is denoted by D and deﬁned as lems were symmetric, and the adjacency matrix was based
                                                                                                  L
Dxx  =        Wxy. There are several deﬁnitions of the on a random policy. Notice that the eigenvectors of r and
          y∈N                                         I − L                         λ                 L
graph Laplacian, with the most common being the normal-    r are the same. Moreover, if is an eigenvalue of r,
                                     − 1   − 1        then 1 − λ is an eigenvalue of I − Lr.
ized Laplacian, deﬁned for W as L = I − D 2 WD 2 . The
advantage of the normalized Laplacian is that it is symmet- 3.1 Spectral Approximation
ric, and thus its eigenvectors are orthogonal. Its use is closely
                                          −1          Assuming that P is the transition matrix for a ﬁxed Markov
related to the random walk Laplacian Lr = I − D W and
                                                      policy, we can express the value function as [Puterman,
the combinatorial Laplacian Lc = D − W , which are com-
                                                      2005]:
monly used to motivate the use of the Laplacian by making a                         ∞
connection to random walks on graphs [Chung, 1997].               v =(I − γP)−1r =     (γP)ir.       (3.1)
  A function f on a graph (N,E) is a mapping from each                              i=0
        R
vertex to . One well-deﬁned measure of smoothness of a The second equality follows from the Neumann series expan-
function on a graph is its Sobolev norm. The bottom eigen-
        L                                             sion. In fact, synchronous backups in the modiﬁed policy it-
vectors of c may be seen as a good approximation to smooth eration calculate this series adding one term in each iteration.
functions, that is, functions with low Sobolev norm [Chung, We assume that the transition matrix is diagonalizable. The
1997].                                                analysis for non-diagonalizable matrices would be similar,
  The application of the spectral framework to VFA is but would have to use Jordan decomposition. Let x1 ...xn
straightforward. The value function may be seen as a func- be the eigenvectors of P with corresponding eigenvalues
tion on the graph of nodes that correspond to states. The edge λ1 ...λn. Moreover, without loss of generality, xj2 =1
weight between two nodes is determined by the probability of for all j. Since the matrix is diagonalizable, x1 ...xn are
transiting either way between the corresponding states, given linearly independent, and we can decompose the reward as:
a ﬁxed policy. Usually, the weight is 1 if the transition is pos-
                                                                             n
sible either way and 0 otherwise. Notice that these weights               r =    c x ,
must be symmetric, unlike the transition probabilities. If we                     j j
assume the value function is smooth on the induced graph, the                 j=1
spectral graph framework should lead to good results. While                  i      i
                                                      for some c1 ...cn. Using P xj = λ xj,wehave
the adjacency matrix with edge weights of 1 was usually used                        j
before, [Mahadevan, 2005] also discusses other schemes.    n  ∞             n     1         n
                                                        v =       (γλ )ic x =            c x =     d x .
  While the approach is interesting, it suffers from some            j  j j       1 − γλ  j j       j j
problems. In our opinion, the good performance of this      j=1 i=0           j=1       j      j=1
method has not been sufﬁciently well explained, because the
                                                        Considering a subset U of eigenvectors xj as a basis will
construction of the adjacency matrix is not well motivated.
                                                      lead to the following bound on approximation error:
In addition, the construction of the adjacency matrix makes                      
the method very hard to analyze. As we show later, using the           v − v˜2 ≤   |dj|.           (3.2)
actual transition matrix instead of the adjacency matrix leads                   j/∈U
to a better motivated algorithm, which is easy to derive and
analyze.                                              Therefore, the value function may be well approximated by
  The requirement of the value function being smooth was considering those xj with greatest |dj|. Assuming that all cj
partially resolved by using diffusion wavelets in [Mahade- are equal, then the best choice to minimize the bound (3.2) is
van and Maggioni, 2005; Maggioni and Mahadevan, 2006]. to consider the eigenvectors with high λj. This is identical to
Brieﬂy, diffusion wavelets construct a low-order approxima- taking the low order eigenvectors of the random walk Lapla-
tion of the inverse of a matrix. An disadvantage of using the cian, as proposed in the spectral proto-VFA framework, only
wavelets is the high computational overhead needed to con- that the transition matrix is used instead. Using the analysis
struct the inverse approximation. The advantage is, however, above, we propose a new algorithm in Subsection 3.3.
that once the inverse is constructed it may be reused for dif-
ferent rewards as long as the transition matrix is ﬁxed. Thus, 3.2 Krylov Methods
we do not compare our approach to diffusion wavelets due to There are also other well-motivated base choices besides the
the different goals and computational complexities of the two eigenvectors. Another choice is to use some of the vectors
methods.                                              in the Neumann series (3.1). We denote these vectors as

                                                IJCAI-07
                                                  2575       i
yi = P r for all i ∈0, ∞). Calculating the value func-  Require: P , r, k - number of eigenvectors in the ba-
tion by progressively adding all vectors in the series, as done sis, l - total number of vectors
in the modiﬁed policy iteration, potentially requires an in- Let z1 ...zk be the top real eigenvectors of P
ﬁnite number of iterations. However, since the linear VFA  zk+1 ← r
methods consider any linear combination of the basis vec-  for i ← 1 ...l+ k do
tors, we need at most n linearly independent vectors from the if i>k+1then
        {y }∞                                    y
sequence  i i=0. Then it is preferable to choose those i       zi ← Pzi−1
with small i, since these are simple to calculate.           end if
  Interestingly, it can be shown that we just need y0 ...ym−1 for j ← 1 ...(i − 1) do
                                m
to represent any value function, where is the degree of the    zi ← zi −zj,zizj
minimal polynomial [Ipsen and Meyer, 1998] of (I − γP).      end for
Moreover, even taking fewer vectors than that may be a good  if zi≈0 then
choice in many cases. To show that y0 ...ym−1 vectors are      break
sufﬁcient to precisely represent any value function, let the end if
                    p(A)=     m  α Ai                              1
minimal polynomial be:        i=0 i  . Then let              zi ←     zi
                                                                  zi
                     1 m−1                                end for
                B =        α   Ai.
                     α       i+1
                      0 i=0                           Figure 1: Augmented Krylov method for basis construction.
By algebraic multiplication, we have BA = I. Having this,
the value function may be represented as:             may contain complex numbers, what would require a revi-
                                                      sion of the approximate policy iteration algorithms. If these
               1 m−1                m−1
     v = Br =        α   (I − γP)ir =    β y ,        issues were resolved, this may be a viable alternative to other
              α       i+1                  i i
               0 i=0                  i=0             methods.
                                                        The second algorithm we propose is to use the vectors from
for some βi. A more rigorous derivation may be found for the augmented Krylov method, that is, to combine the vectors
example in [Ipsen and Meyer, 1998; Golub and Loan, 1996]. in the Krylov space with a few top eigenvectors. A pseudo-
The space spanned by y0 ...ym−1 is known as Krylov space code of the algorithm is in Figure 1. The algorithm calculates
(or Krylov subspace), denoted as K(P, r). It has been previ- an orthonormal basis of the augmented Krylov space using a
ously used in a variety of numerical methods, such as GM- modiﬁed Gram-Schmidt method.
RES, or Lancoz, and Arnoldi [Golub and Loan, 1996].Itis Using the Krylov space eliminates the problems with non-
also common to combine the use of eigenvectors and Krylov diagonalizable transition matrices and complex eigenvectors.
space, what is known as an augmented Krylov methods [Saad, Another reason to combine these two methods is to take ad-
1997]. These methods actually subsume the methods based vantage of approximation properties of both methods. Intu-
on simply considering the largest eigenvectors of P . We dis- itively, Krylov vectors capture the short-term behavior, while
cuss this method in Subsection 3.3.                   the eigenvectors capture the long-term behavior. Though,
                                                      there is no reliable decision rule to determine the right num-
3.3  Algorithms                                       ber of augmenting eigenvectors, it is preferable to keep their
In this section we propose two algorithms based on the pre- number relatively low. This is because they are usually more
vious analysis for constructing a good basis for VFA. These expensive to compute than vectors in the Krylov space.
algorithms deal only with the selection of the basis and they
can be arbitrarily incorporated into any algorithm based on
approximate policy iteration.                         4   Approximation Bounds
  The ﬁrst algorithm, which we refer to as the weighted spec- In this section, we brieﬂy present a theoretical error bound
tral method, is to form the basis from the eigenvectors of the guaranteed by the augmented Krylov methods. The bound
transition matrix. This is in contrast with the method pre- characterizes the worst-case approximation error of the value
sented in [Mahadevan, 2005], which uses any of the Lapla- function, given that the basis is constructed using the current
cians. Moreover, we propose to choose the vectors with great- transition matrix and reward vector. We focus on bounding
est |dj| value, in contrast to choosing the ones with largest λj. the quadratic norm, what also implies the max norm.
The reason is that this leads to minimization of the bound in We show the bound for r − v˜ + γPv˜2. This bound ap-
(3.2).                                                plies directly to algorithms that minimize the Bellman resid-
  A practical implementation of this algorithm faces some ual [Bertsekas and Tsitsiklis, 1996], and it also implies:
major obstacles. One issue is that there is no standard eigen-
                                                                             −1            −1
vector solver that can efﬁciently calculate the top eigenvec- v − v˜∞ = (I − γP) r − (I − γP) (I − γP)˜v∞
tors with regard to |dj| of a sparse matrix. However, such a           1
                                                                 ≤        r − (I − γP)˜v .
method may be developed in the future. In addition, when the         1 − γ              2
transition matrix is not diagonalizable, we need to calculate
the Jordan decomposition, which is very time-consuming and This follows from the Neumann series expansion of the in-
unstable. Finally, some of the eigenvectors and eigenvalues verse and from P ∞ =1.

                                                IJCAI-07
                                                  2576  In the following, we denote the set of m Krylov vectors as where PU is the least squares projection matrix to U. Note
Km  and the chosen set of top eigenvectors of P as U.We that p(A)PU r − AUy =0, since U is an invariant space
also use E(c, d, a) to denote an ellipse in the set of complex of A. Moreover, (I − PU ) ∗ r is perpendicular to U. The
numbers with center c, focal distance d, and major semi-axis theorem then follows from the approximation by Chebyshev
a. The approximation error for a basis constructed for the cur- polynomials as described above, and the deﬁnition of matrix-
rent policy may be bounded as the following theorem states. valued function approximation [Golub and Loan, 1996].
                (I − γP)=XSX−1
Theorem 4.1. Let                  be a diagonalizable   This bound shows that it is important to choose an invari-
matrix. Further, let                                                                                 P
                                −1                    ant subspace that corresponds to the top eigenvalues of .It
            φ =maxXTX             x2,                             U
                x∈U ⊥x =1                           also shows that should be chosen to be to the highest de-
                       2                              gree perpendicular to the remaining eigenvectors. Finally, the
     T
where  is a diagonal matrix with 1 in place of eigenvalues of bound also implies that the approximation precision increases
             |U|
eigenvectors in . The approximation error using the basis with lower γ, as this decreases the size of the ellipse to cover
K   ∪ U
  m     is bounded by:                              the eigenvalues.
                                   a1       a
                              Cm(    )  Cm(  )
 r − (I − γP)˜v ≤ max   κ(X)     d1 ,φ    d   ,
                2                  c1       c
                              Cm(    )  Cm(  )        5   Experiments
                                   d1       d
                                                      In this section we demonstrate the proposed methods on the
where Cm is the Chebyshev polynomial of the ﬁrst kind of de-
    m      κ(X)=X    X−1                 a, c, d  two-room problem similar to the one used in [Mahadevan,
gree  , and           2      2. The parameters        2005; Mahadevan and Maggioni, 2005]. This problem is a
and a1,c1,d1 are chosen such that E(c1,d1,a1) includes the
     n−|U|             (I−γP)      E(c, d, a)         typical representative of some stochastic planning problems
lower      eigenvalues of     , and        includes   encountered in AI.
all its eigenvalues.                                    The MDP we use is a two-room grid with a single-cell
  The value of φ depends on the angle between the invari- doorway in the middle of the wall. The actions are to move
ant subspace U and the other eigenvectors of P . If they are in any of the four main direction. We use the policy with an
perpendicular, such as when P is symmetric, then φ =0. equal probability of taking any action. The size of each room
Proof. To simplify the notation, we denote A =(I − γP). is 10 by 10 cells. The problem size is intentionally small
First, we show the standard bound on approximation by a to make explicit calculation of the value function possible.
Krylov space [Saad, 2003], ignoring the additional eigenvec- Notice that because of the structure of the problem and the
                                                      policy, the random walk Laplacian has identical eigenvectors
tors. In this case, the objective is to ﬁnd such w ∈ Km that
minimizes the following:                              as the transition matrix in the same order. This allows us to
                  n               n                 evaluate the impact of choosing the eigenvectors in the order
                           i                 i        proposed by the weighted spectral method.
 r − Aw2 = r −    w(i)A r2 =     −w(i)A  r2,
                  i=1              i=0                  We used the problem with various reward vectors and dis-
                                                      count factors, but with the same transition structure. The
where w(0) = −1. Notice that this deﬁnes a polynomial in
A            r                                  w     reward vectors are synthetically constructed to show possi-
  multiplied by with the constant factors determined by . ble advantages and disadvantages of the methods. They are
Let Pm denote the set of polynomials of degree at most m
             p ∈P          p(0) = 1                   shown projected onto the grid in Figure 2. Vector “Reward 1”
such that every   m satisﬁes      . The minimization  represents an arbitrary smooth reward function. Vectors “Re-
problem may be then expressed as ﬁnding a polynomial p ∈
P                p(A)r                              ward 2” and “Reward 3” are made perpendicular to the top
 m  that minimizes      2. This is related to the value 40 and the top 190 eigenvectors of the random walk Lapla-
                                    [
of the polynomial on complex numbers as Golub and Loan, cian respectively.
    ]
1996 :                                                  A lower discount rate is generally more favorable to Krylov
           p(A)2 ≤ κ(X)max|p(λi)|,
                          i=1,...,n                   methods, because the value is closer to the value of the reward
where λi are the eigenvalues of A. A more practical bound received in the state. This can also be seen from Theorem 4.1,
                                                             γ
may be obtained using Chebyshev polynomials, as for exam- because shrinks the eigenvalues. Therefore, we use prob-
ple in [Saad, 2003], as follows:                      lems that are generally less favorable to Krylov methods, we
                                                                                                  γ =0.9
                                          C  ( a )    evaluate the approach using two high discount rates,
                                            m d           γ =0.99
 min   max  |p(λi)|≤ min    max   |p(λ)|≤     c  ,    and        .
 p∈P  i=1,...,n      p∈P                  C  (  )
    m                   m λ∈E(c,d,a)        m d         In our experiments, we evaluate the Mean Squared Er-
where the ellipse E(c, d, a) covers all eigenvalues of A. ror (MSE) of the value approximation with regard to the num-
  In the approximation with eigenvectors, the minimization ber of vectors in the basis. The basis and the value function
may be expressed as follows:                          were calculated based on the same policy with equiprobable
                                                      action choice. We obtained the true value function by explic-
 min r − Av˜2 =minmin   r − Aw − AUq2  =                               −1
  v              w∈Km q∈U                             itly evaluating (I − γP) r. Notice, however, that the true
   =minmin      p(A)r + AUq2 =                      value does not need to be used in the approximation, it is only
       p∈Pm q∈U                                       required to determine the approximation error. We compared
   =minmin      p(A)(I − PU )r + p(A)PU r − AUq2    the following 4 methods:
       p∈Pm q∈U
                                                      Laplacian The technique of using the eigenvectors of the
   =minp(A)(I     − PU )r2.
       p∈Pm                                               random walk Laplacian of the adjacency matrix, as pro-

                                                IJCAI-07
                                                  2577                           Reward 1                Reward 2                Reward 3

                   10                     1000                     400

                                                                   200
                                           500
                    5                                               0
                  Reward
                                         Reward
                                            0                    Reward
                                                                  −200

                    0                     −500                    −400
                   10                       10                     10
                                       20                     20                      20
                        5
                                  10            5                      5
                                                          10                     10
                       Y   0 0   X
                                                Y  0 0   X             Y   0 0   X

                                Figure 2: Reward vectors projected onto the grid

                           Reward 1                Reward 2                Reward 3
                   10                       10                     10
                  10                      10                      10
                                Laplacian
                                WL
                   0                        0                      0
                  10            Krylov    10                      10
                                KA

                   −10                      −10                    −10
                  10                      10                      10
                 MSE                     MSE                     MSE


                   −20                      −20                    −20
                  10                      10                      10


                   −30                      −30                    −30
                  10                      10                      10
                     0       100     200     0       100     200     0       100     200
                           Vectors                  Vectors                 Vectors

         Figure 3: Mean squared error of each method, using discount of γ =0.9. The MSE axis is in log scale.

    posed in [Mahadevan, 2005]. The results of the normal- constructing the invariant space of eigenvectors. Using Mat-
    ized Laplacian are not shown because they were practi- lab on a standard PC, it took 2.99 seconds to calculate the 50
    cally identical to the random walk Laplacian.     top eigenvectors, while it took only 0.24 second to calculate
WL  This is the weighted spectral method, described in Sub- the ﬁrst 50 vectors of the Krylov space for “Reward 1”.
    section 3.3, which determines the optimal order of the
    eigenvectors to be used based on the value of dj. 6   Discussion
Krylov This is the augmented Krylov method with no eigen-
                                                      We presented an alternative explanation of the success of
    vectors, as described in Subsection 3.3.
                                                      Laplacian methods used for value approximation. This ex-
KA  This is the augmented Krylov method with 3 eigenvec- planation allows us to more precisely determine when the
    tors, as proposed in Figure 1. The number of eigenvec- method may work. Moreover, it shows that these meth-
    tors was chosen before running any experiments on this ods are closely related to augmented Krylov methods. We
    domain.                                           demonstrated on a limited problem set that basis constructed
  The results for γ =0.9 are in Figure 3 and for γ =0.99 from augmented Krylov methods may be superior to one con-
are in Figure 4. “Reward 3” intentionally violates the smooth- structed from the eigenvectors. In addition, both the weighted
ness assumption, but it is the easiest one to approximate for spectral method and the augmented Krylov method do not as-
all methods except the Laplacian. In the case of “Reward 1” sume the value function to be smooth. Moreover, calculating
and γ =0.99, the weighted spectral method and the random vectors in the Krylov space is typically cheaper than calculat-
walk Laplacian outperform the ordinary Krylov method for ing the eigenvectors.
the ﬁrst 10 vectors. However, with additional vectors, both An approach for state-space compression of Partially Ob-
Krylov and augmented Krylov methods signiﬁcantly outper- servable MDPs (POMDP) that also uses Krylov space was
form them.                                            proposed in [Poupart and Boutilier, 2002]. While POMDPs
  Our results suggest that Krylov methods may offer superior are a generalization of MDPs, the approach is not practical
performance compared to using the eigenvectors of the Lapla- for large MDPs since it implicitly assumes that the number of
cian. Since these methods have been found very effective in observations is small. This is not true for MDPs, because the
many sparse linear systems [Saad, 2003], they may perform number of observations is equivalent to the number of states.
well for basis construction also in other MDP problems. In In addition, the objectives of this approach are somewhat dif-
addition, constructing a Krylov space is typically faster than ferent, though the method is similar.

                                                IJCAI-07
                                                  2578