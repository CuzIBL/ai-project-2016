                   Planning for Gene Regulatory Network Intervention

                                    Daniel Bryce & Seungchan Kim
                           Department of Computer Science and Engineering
                             Arizona State University, Brickyard Suite 501
                               699 South Mill Avenue, Tempe, AZ 85281
                                     {dan.bryce, dolchan}@asu.edu

                    Abstract                          thousands of genes from living tissue in terms of mRNA con-
                                                      centrations (the products of gene transcription used to code
    Modeling the dynamics of cellular processes has re- proteins). Correlations between observed gene activity lev-
    cently become a important research area of many   els help describe regulatory inﬂuences. Predictor functions
    disciplines. One of the most important reasons    characterize the regulatory inﬂuences and provide a dynamic
    to model a cellular process is to enable high-
                                                      model (e.g., when g1 and g2 are highly active, g3 becomes
    throughput in-silico experiments that attempt to  inactive). The GRN state models the activity levels of genes
    predict or intervene in the process. These ex-    and the predictor functions describe possible next states. Out-
    periments can help accelerate the design of thera- side interventions (e.g., using RNA interference to suppress a
    pies through their cheap replication and alteration. gene’s activity level) alter predictor functions to control the
    While some techniques exist for reasoning with cel- dynamics of the GRN, providing a natural planning problem.
    lular processes, few take advantage of the ﬂexible In a plan, each action models either a possible intervention or
    and scalable algorithms popularized in AI research. non-intervention that will change the state of the GRN.
    In this domain, where scalability is crucial for feasi-
    ble application, we apply AI planning based search  There are a number of GRN features that affect our choice
    techniques and demonstrate their advantage over   of AI planning model. First, intervention plans need only fo-
    existing enumerative methods.                     cus on horizons long enough to ensure the GRN will naturally
                                                      transition to nominal states. Biological knowledge and com-
                                                      putational simulation of GRNs [Kim et al., 2002] tell us that
1  Introduction                                       cellular processes, left to their own, transition to (or through)
The cell maintains its functions via various interconnections stable attractor states. These states represent common cellu-
and regulatory controls among genes and proteins. Therefore, lar phenomena such as the cell cycle, division, etc. However,
it is critical, for understanding the living cell, to unravel how some states are consistent with disease, such as the metastasis
such cellular components as genes and proteins interact with of cancer. From abnormal states, planning interventions pro-
each other. Mathematical modeling and computational simu- vides a method to push the evolution of the cell toward nomi-
lation of cellular systems, especially gene regulatory systems, nal attractor states. Second, the cell has an inherently hidden
has been crux of computational systems biology, or biomed- state because full observations are prohibitively costly and we
ical science in general. Once a model is constructed, it can have limited accessability [Datta et al., 2004]. Planning with
be used to predict the behavior of a cellular system under un- partial observability is important because biologists analyz-
usual conditions, to identify how a disease might develop, ing cellular processes cannot be expected to understand, nor
and/or how to intervene such development to prohibit cells obtain complete state information. Third, cellular processes
from reaching undesirable states. Such models can aid biol- are commonly viewed as stochastic [Rao et al., 2002]. Genes
ogists by quickly ruling out or conﬁrming simple hypotheses are typically regulated many different ways, meaning GRNs
before expensive and time-consuming wet lab experiments. must allow for the probabilistic selection of predictor func-
  In this paper, we address the problem of planning to in- tion for each gene. Because state transitions are stochastic,
tervene in cellular processes, focusing on gene regulatory provide only partial observations, and are only relevant over
networks (GRNs). A GRN describes a cellular process by a limited planning horizon, we formulate our model of GRN
a set of genes and their regulatory inﬂuences over each intervention as decision theoretic planning in a ﬁnite horizon
other. GRNs focus solely on genes (omitting proteins or partially observable Markov decision process (POMDP).
other molecules) to model the high level behavior of many This work is not the ﬁrst to address the problem of plan-
genes versus the low level behavior of a much smaller sys- ning interventions for GRNs, but it is the ﬁrst to apply AI
tem. Because practical GRN models are typically learned planning techniques. Existing research [Datta et al., 2004]
from micro-array data [Kim et al., 2000], they are situated enumerates all possible plans and then uses a dynamic pro-
at the right level of granularity for automated parametriza- gramming algorithm to ﬁnd the optimal ﬁnite horizon plan.
tion. Micro-array experiments measure the activity level of Our planner is based on the AO* algorithm [Nilsson, 1980],

                                                 IJCAI07
                                                  1834which uses upper bounds on plan quality (reward) for prun- Horizon 0  Horizon 1   Horizon 2    Horizon 3
ing. We evaluate our planner on several formulations of a ran-
                                                         Initial Belief 1.0 g1, g2 1.0 g1, g2 1.0 g1, g2 10
                                                                                -1          0
dom GRN with different reward functions to see the beneﬁt 0.25 g1, g2 0.5
pruning. We also evaluate on the WNT5A GRN [Weeraratna              No Intervention g2 Intervention No Intervention
                                                       0.25 g1, g2  Observe g2  Observe g2  Observe g2
et al., 2002],which[Datta et al., 2004] use for intervention      0
                                                       0.25 g1, g2
planning. WNT5A is a gene that plays a signiﬁcant role in          No Intervention No Intervention No Intervention
                                                                   Observe g2
the development of melanoma and is known to induce the 0.25 g1, g2 0.5          Observe g2  Observe g2
                                                                                0           0
metastasis of melanoma when highly active. On the WNT5A              1.0 g1, g2  1.0 g1, g2   1.0 g1, g2 10
problems studied by [Datta et al., 2004], our planner performs
signiﬁcantly better than the enumeration algorithm of [Datta
et al., 2004]. Our study formalizes this interesting planning        Figure 1: Example Plan.
domain as a ﬁnite horizon POMDP and shows the gains of
applying relatively standard AI techniques.             The quality of the plan is the expected reward of all possi-
  We start by describing a simple example of planning in- ble plan branches. The ﬁrst branch, taken with 0.5 probability
terventions in a GRN. We then brieﬂy deﬁne the components has a reward of 9 (because we had to intervene), and the sec-
of a ﬁnite horizon POMDP and a GRN intervention prob- ond branch, taken with 0.5 probability has a reward of 10.
lem. With these deﬁnitions, we map the intervention prob- Thus, the total expected reward is 9.5.
lem to the POMDP. The solution to the POMDP is a condi-
tional plan, for which we deﬁne the semantics and provide
two solution algorithms – AO* and enumeration [Datta et al., 3 Decision Theoretic Planning
2004]. We compare both algorithms on several GRNs, ei- This section describes the basics of the ﬁnite horizon POMDP
ther randomly generated or learned from actual micro-array model, formally deﬁnes intervention planning, and details the
experiments. We end with related work, a conclusion, and formulation of intervention planning in the POMDP.
discussion of future work.
                                                      POMDP Model:    Unlike most work on POMDPs, that ﬁnd
                                                      a policy over the belief space, we ﬁnd a policy (conditional
2  Intervention Planning Example                      plan) rooted at an initial situation. The ﬁnite horizon POMDP
To clarify intervention planning, consider a small two gene problem P is deﬁned as the tuple S, A, T, R, Ω,O,h,bI ,
network where each gene g is either active (g)orinactive where S is a set of states, A is a set of actions, T : S × A∪
¬                                  {      }
( g). There are two predictor functions fg1 ,fg2 that de- {⊥} × S ∪{⊥}→[0, 1] is a transition function, R : S × A ∪
scribe the GRN dynamics, and an intervention to suppress g2 {⊥} × ∪{⊥}→R
                                                           S            is the transition reward function, Ω is a
described by the predictor function fg2 . Using the predictor set of observations, O : S ×A×Ω → [0, 1] is the observation
functions, we can deﬁne the following two actions:    function, h is the planning horizon, and bI : S → [0, 1] is the
 Action    No Intervention     g2 Intervention        initial belief state. We overload the symbol ⊥ to denote both
 Reward    0                   -1                     the terminal action and state signifying the end of the plan.
                 →¬    ¬  →          →¬   ¬   →
 Effects   fg1 : g2 g1, g2  g1 fg1 : g2 g1, g2  g1
               ¬  →¬      →       →¬
 (Predictors) fg2 : g2 g2, g2 g2 fg2 : g2             Intervention Planning Problem: Given our deﬁnition of the
 Observation g2 / ¬g2          g2 / ¬g2               POMDP model, in the following we show how an interven-
Intervening to suppress g2 incurs a reward of -1 by rewrit- tion planning problem maps to the POMDP. The intervention
                                                     problem is deﬁned by the tuple G             O  ,
ing the predictor function fg2 with fg2 . We also assume that                      ,Dom,F,X,W,Y,     ,h
we can observe g2 but not g1. The goal of the intervention where G is a set of genes, Dom is the set of activity levels for
problem associates a reward of 10 with activating gene g1. genes, F is a set of predictor functions, X is a set of inter-
While the actions are deterministic (for the sake of example ventions, W is an initial situation, Y is a goal description, O
simplicity), we can describe stochastic actions by allowing a is a set of observations, and h is the horizon. The genes and
probabilistic choice of predictor function for each gene. their activity levels describe states of the POMDP, the pre-
  Assume that we start with the initial belief state where each dictor functions and interventions describe actions, interven-
GRN state is equally likely, as depicted in Figure 1. The ﬁg- tions and the goal description deﬁne the reward function, and
ure shows a horizon three plan to achieve the goal of activat- the initial situation, observations, and horizon map directly to
ing g1. The ﬁrst action in the plan is to not intervene; we will their POMDP counterparts.
let the genes affect each other, and then observe g2. Not inter- States: Each gene g ∈Ghas an activity level from the do-
vening leads to the belief state { {¬ 1 2} { 1 ¬ 2}}.
                          0.5  g ,g  , 0.5 g , g      main Dom  of values, of which we will only illustrate boolean
Using the observation of whether 2 or ¬ 2 holds, the plan
                             g     g                  domains {g,¬g}, active or inactive. A state s : G→Dom of
branches to belief states consistent with the appropriate ob- the gene network maps each gene to a value d ∈ Dom.The
servation. Since we do not know which value of 2 will occur,
                                       g              entire set of GRN states deﬁnes the POMDP states S.
we plan for both branches. In the ﬁrst branch, we apply our
g2 intervention action to suppress g2. In the following step, Predictor Functions and Interventions: Given a state of the
we do not intervene and having g2 inactive will activate g1, gene network, the predictor functions F describe states reach-
leading to a state satisfying the goal. In the second branch, able after one step. Interventions re-write predictor functions
we can apply the non intervention action indeﬁnitely because in F for speciﬁc genes to ensure the gene network transitions
the goal is satisﬁed and will remain satisﬁed.        to speciﬁc states. Thus, each possible action in the POMDP

                                                 IJCAI07
                                                  1835is described by a set of predictor functions. A non interven- algorithms to ﬁnd plans. The ﬁrst algorithm is AO* [Nils-
tion simply uses F to describe the action, but an interven- son, 1980], and the second Datta is based on a competing
tion action x ∈ X replaces predictor functions in F to get a approach [Datta et al., 2004] from the GRN literature.
new set Fx. Each intervention x ∈ X is a set of predictors
{         }                                           Conditional Plans: A solution to the problem P is a con-
 fg1 ,fg2 , ... , allowing us to deﬁne                            P
                                                     ditional plan of horizon h, described by a partial func-
              ∪  \{  |   ∈      ∈       }
       Fx = F   x  fg fg   F, fg  x, g = g .          tion P : V →  A ∪{⊥}over the belief state space graph
  Each predictor function fg is deﬁned as the mapping fg : G =(V,E). A subset of the vertices b ∈ V (which are belief
      
Dom|G  | → Dom  from activity levels of genes in G ⊆G states) are mapped to a “best” action a, denoted P(b)=a.
                                                                  ∈                      o
to the activity level of gene g. The interventions described in Each edge e E directed from b to ba is mapped to an
                                                              ∈                        ∈
this work contain a single predictor function fg where G = ∅, action a A and an observation o Ω, and denoted
                                                           o            P                 o
meaning that irrespective of the state g has its activity level set e(b, ba)=(a, o).If (b)=a and e(b, ba)=(a, o),then
                                                      P  o
deterministically.                                      (ba) is the action to execute after executing a and receiving
  Since each gene may be predicted by several predictor observation o. Throughout our discussion, we assume that
functions, where each state transition selects one probabilis- the horizon is a feature of every state to ensure that the graph
tically, we assign a weight w(fg) to each. While we assume G is acyclic. Belief states where the horizon is equal to h
the predictor functions are given, we can learn them from have a single available action ⊥ to signify the end of the plan,
micro-array experiments. In the empirical analysis we evalu- leading to a terminal ⊥.
                                                          P                                 o
ate GRNs where the predictor functions and weights are both If (b)=a, and there exists an edge e(b, ba)=(a, o) then
generated randomly and from real micro-array experiments. the successor belief state bo is deﬁned
                                                                            a
  Each action aF deﬁned by the set of predictor functions F                             
                                                                   ba(s )=     b(s)T (s, a, s )
(similarly Fx) describes the transitionP function probability              s∈S
                                            ( )                                  
                              f ∈F :f (s)=s (g) w fg              o
                    |            g P g                             ba(s )=αba(s  )O(s ,a,o),
  T (s, aF ,s)=Pr(s F, s)=              w(f )    .
                            ∈G      fg ∈F  g
                           g                          where α is a normalization constant. If for all s ∈ S,
                                                       o  
Observations: After each step, whether by intervention or ba(s )=0because no observation is consistent with the be-
non intervention, we can observe the state of the gene net- lief state ba, then the belief state is not added to the graph.
work. The set O⊆Gdeﬁnes which genes are observable      The expected reward q(a, b) of a plan that starts with action
(by genetic markers, physiology, etc.). The set of observa- a at belief state b is the sum of current and future rewards:
                     |O|                                                
tions Ω={o|o  ∈ Dom    } is deﬁned by all joint activity                                      
               O                                            q(a, b)=        b(s)T (s, a, s )R(s, a, s )+
levels of genes in . In this work, we assume that observa-           s∈S s∈S
                                                                                    V  o
tions are perfect and the same for each action, meaning that            ba(s )O(s ,a,o) (ba),
if a state s and observation o agree on the activity level of        o∈Ω
each gene, then the probability of the observation is one (i.e., where the expected reward for a belief state is V(b)=
O(s, a, o)=1), otherwise zero (i.e., O(s, a, o)=0). Obser-
                       ≤          ≤                   maxa∈A  q(a, b). Terminal vertices are assigned the expected
vations can be noisy (i.e., 0 O(s, a, o) 1) in general. goal reward
Rewards:  The goal Y is a function describing desirable                     
                                                                   q(⊥,b)=     b(s)R(s, ⊥, ⊥).
states. We assume that the goal maps states to real values                  s∈S
Y : Dom|G| →  R. The reward function for terminal actions
                                     ⊥  ⊥
and goal states is deﬁned by the goal R(s, , )=Y (s). AO* Algorithm: We solve the ﬁnite horizon POMDP prob-
We assume that the reward associated with actions is -1 for   AO*
                                                     lem with     search [Nilsson, 1980] in the space of belief
intervention actions (i.e., R(s, aF ,s)=−1) and 0 for non       AO*
                           x                         states. The   algorithm, listed in Figure 2, takes the plan-
intervention (i.e., R(s, aF ,s)=0).                   ning problem as input, and iteratively constructs the belief
Initial Situation: The initial situation W is a distribution space graph G rooted at bI . The algorithm involves three it-
over GRN states W : Dom|G| → [0, 1]. This mapping to the erated steps: expand the current plan with the ExpandPlan
POMDP initial situation is straight-forward, bI (s)=W (s). routine (line 3), collect the ancestors Z’ of new vertices Z
                                                      (line 4), and compute the current best partial plan (line 5).
  Since the formulation has a distinct initial belief state, we The algorithm ends when it expands no new vertices. In the
do not explore more traditional POMDP value or policy itera- following we brieﬂy describe the sub-routines used by AO*.
tion techniques for computing a ﬁnite horizon policy. Rather, The ExpandPlan routine recursively walks the current
we use the knowledge of the initial belief state to guide ex- plan to ﬁnd unexpanded vertices (lines 2-5). Upon ﬁnding
pansion of a conditional plan. By searching forward from the a vertex to expand, it generates all successors of the vertex
initial belief state, it is possible to focus plan construction on (lines 7-18). Generating successors involves assigning the ⊥
reachable belief states.                              action if the vertex it is at the max horizon (line 10) or con-
                                                      structing the vertices reached by all action and observation
4Search                                               combinations (lines 12-17). Notice that each vertex has its
This section describes our approach to solving a ﬁnite hori- value initialized with an upper bound,
zon POMDP. We start by deﬁning the semantics of condi-
                                                        Rmax  =maxR(s, a, s)h     +max(0,R(s,  ⊥, ⊥)),
tional plans and our search space. We follow with two search    s,s∈S,a∈A

                                                 IJCAI07
                                                  1836 AO*(P )                                               AddAncestors(Z)
  1: expanded(bI)=FALSE                                 1: Z’ = Z
                                                                          o             o
  2: repeat                                             2: while ∃b s.t. e(b, bP( )) ∈ E and bP( ) ∈ Z do
          ExpandPlan(     ,0)                                               b             b
  3:   Z=               bI                              3:   Z’ = Z’ ∪b
  4:   Z’ = AddAncestors(Z)                             4: end while
  5:   Update(Z’)                                       5: return Z’
  6: until |Z| =0
                                                       Update(Z)
 ExpandPlan
             (b,hzn)                                    1: while |Z| > 0 do
                                                                                              
  1: if expanded(b) then                                2:   Remove b ∈ Zs.t.¬∃b ∈ Zwheree(b, b ) ∈ E
              o    ∈
  2:   for e(b, bP(b)) E do                             3:   Backup(b)
             ExpandPlan    o
  3:     Z’ =            (bP(b), hzn+1)                 4: end while
  4:     Z=Z∪   Z’                                     Backup(b)
  5:   end for                                          1: for all a ∈ A ∪{⊥}do
  6: else                                               2:   Compute q(a, b)
  7:   expanded(b)=TRUE
             ∪{  }                                      3: end for
  8:   Z = Z    b                                       4: V(b)=   max   q(a, b)
  9:   if hzn == h then                                           a∈A∪{⊥}
 10:     E = E ∪{e(b, ⊥)=(⊥,  ⊥)}                       5: P(b) = argmax q(a, b)
 11:   else                                                       a∈A∪{⊥}
 12:     for a ∈ A, o ∈ Ω do
                 ∪{  o}                                                     AO*
 13:       V = V    ba                                              Figure 3:   Subroutines.
                 ∪{      o        }
 14:       E = E    e(b, ba)=(a, o)
                    o
 15:       expanded(ba)=FALSE                          Datta(P  )
 16:       V(bo)=Rmax
              a                                         1: ExpandPlanD(b    ,0)
 17:     end for                                                           I
                                                        2: Update(V)
 18:   end if
 19: end if                                            ExpandPlanD(b,hzn)
 20: return Z                                           1: if hzn == h then
                                                        2:   E = E ∪{e(b, ⊥)=(⊥,  ⊥)}
           Figure 2: AO* Search Algorithm.              3: else
                                                        4:   for a ∈ A, o ∈ Ω do
                                                                      ∪{ o}
                                                        5:     V = V     ba
on its expected reward. The upper bound plays a role in prun-         ∪{     o        }
                                                        6:     E = E    e(b, ba)=(a, o)
ing vertices from consideration in the search.                 ExpandPlanD    o
                                                        7:                   (ba, hzn+1)
  After expanding the current plan, ExpandPlan returns  8:   end for
the set of expanded vertices Z. In order for Update     9: end if
(Figure 3) to ﬁnd the best plan, given the new vertices,
AddAncestors   adds to Z’ every ancestor vertex of a ver-     Figure 4: Datta Enumeration Algorithm.
tex in Z. The resulting set of vertices consists of every ver-
                                          Update
tex whose value (and best action) can change after .                          Datta
The Update routine iteratively removes vertices from Z that the entire graph G, as would .
have no descendent in Z and calls Backup until no vertices
remain in Z. The Backup routine computes the value of a
vertex and sets its best action. The reason Update chooses Enumeration Algorithm: In order to compare our planner
vertices with no descendent in Z is to ensure each vertex has to the work of [Datta et al., 2004], we provide a description
its value updated with the updated values of its children. of their algorithm, we call Datta, in Figure 4. Unlike the
  AO*  can often avoid computing the entire belief space iterative AO*, Datta consists of two steps: expand G with
graph G, leading to signiﬁcant savings in problems with large ExpandPlanD, and then update each vertex v ∈ V with
horizons. By initializing vertices with an upper bound on Update.TheExpandPlanD   routine recursively expands
their value it is possible to ignore vertices that have a consis- G by either reaching a terminal vertex at the horizon (line 2),
tently lowest upper bound. For example in Backup,ifthere or generating and recursing on each child of a vertex (lines 4-
exists an action whose q-value is always greater than the al- 8). Following ExpandPlanD, Update computes the best
ternative actions, then the best action will never be set to one action for each vertex in V .
of the alternatives. Further, because the alternative actions Unlike AO*, Datta is unable to prune vertices from ex-
are never considered best, ExpandSolution will never ex- pansion, making it insensitive to the reward function. While
pand them. As we will explore in the empirical evaluation, the result of the two algorithms is identical, the time and
the reward function has a signiﬁcant effect on the number of space required can be very different. We implemented both
vertex expansions. In the worst case, it is possible to expand algorithms within our planner and demonstrate their effec-

                                                 IJCAI07
                                                  1837                                                                 AO*
                 Expansions      -10             Expansions                       Expansions      AO*
                                 -1                              Datta                            Datta
                                 1                               Max   1.E+08                     Max
      1.E+11                     10   1.E+10                           1.E+07
      1.E+10                     Datta
      1.E+09                     Max  1.E+08                           1.E+06
      1.E+08                                                           1.E+05
      1.E+07                          1.E+06
      1.E+06                                                           1.E+04
      1.E+05                          1.E+04                           1.E+03
      1.E+04                                                           1.E+02
                                                                      #  Belief States

     #  Belief States 1.E+03
                                      #  Belief States 1.E+02
      1.E+02                                                           1.E+01
      1.E+01                                                           1.E+00
                                      1.E+00
      1.E+00                                                                  24681012
            24681012                        2  4  6   8 10 12  14 16
                   Horizon                           Horizon                          Horizon

 Figure 5: Number of Expanded Vertices for random GRN (left), WNT5A intervention (center), PIRIN intervention (right).

                                                                 AO*                              AO*
                 Total Time      -10              Total Time                      Total Time
                                 -1                              Datta                            Datta
      1.E+04
                                 1    1.E+03                          1.E+03
                                 10
                                 Datta
      1.E+03
                                      1.E+02                          1.E+02

      1.E+02
                                                                      Time(s)
     Time(s)
                                      Time(s)                         1.E+01
                                      1.E+01
      1.E+01

                                                                      1.E+00
                                      1.E+00
      1.E+00                                                                 24681012
            24681012                        246810121416
                                                                                      Horizon
                    Horizon                          Horizon

    Figure 6: Total Planning Time(s) for random GRN (left), WNT5A intervention (center), PIRIN intervention (right).

tiveness on the GRN intervention planning problems. We negative reward of one. This illustrates the ability of AO* to
also implemented a straight-forward version of the [Datta et prune the search space in comparison with enumeration. In
al., 2004] algorithm that does not make use of several efﬁ- the WNT5A GRN, we reproduce two intervention problems
ciency improvements within the planner, such as using ADDs studied by [Datta et al., 2004]. The ﬁrst directly intervenes
[Bryant, 1986] for compact action and belief state represen- to suppress WNT5A (which happens to be the goal) and ob-
tation, as well as duplicate belief state detection.  serves the PIRIN gene. The second attempts to indirectly
                                                      control WNT5A by PIRIN (a predictor gene of WNT5A)
5  Empirical Evaluation                               intervention. Both WNT5A problems use the same reward
To test the feasibility of using our planner to solve GRN inter- function, assigning interventions a negative reward of one and
vention problems, we experiment with a random GRN and the the goal (activating WNT5A) a negative reward of three. We
WNT5A GRN    [Weeraratna et al., 2002; Datta et al., 2004]. use negative reward for the goal to maintain consistency with
                                                         [               ]
The following table summarizes the features of the GRNs: the Datta et al., 2004 model. Thus plans avoid activating
                                                      WNT5A, which is equivalent to pursuing suppression. Both
                  |G| |Dom|  |F | |X| |O|
          Random   7    2    14   3   1               WNT5A networks use the initial belief state where each gene
          WNT5A    7    2    14   1   1               is set to an activity level with probability proportional to its
Both networks use seven genes (each with two activity lev- observed frequency in the data.
els), two predictor functions per gene, each with two genes Our planner is implemented in C++ and ran on a 2.8GHz
as predictors. The predictor functions were selected ran- P4 Linux machine with 1GB of RAM. Each experiment
domly in the random GRN and learned from micro-array data was given a twenty minute time limit. For our plan-
(measurements of mRNA concentrations in a cell that indi- ner binary and gene regulatory network encodings, several
cate gene activity levels) in the WNT5A GRN. In order to of which we did not have space to describe, please visit:
learn predictor functions, we use a coefﬁcient of determina- http://verde.eas.asu.edu/GRN.
tion (COD) statistic to measure the strength of correlation be- Random GRN: The leftmost plot in Figure 5 depicts the
tween predictor and target genes from normalized and dis- number of expanded vertices (including terminal actions) in
cretized data [Kim et al., 2000]. We use the two predic- AO*, Datta, and the maximum possible (Max). The results
tor functions with highest COD for each gene, setting their for AO* are indexed by a number indicating the reward as-
weight equal to the COD.                              sociated with the goal, since Datta is insensitive to reward.
  Within both GRNs we study several intervention problems. Max represents the number of vertices expanded in a search
In the random GRN, we vary the goal W to assign differ- tree (versus a graph), similar to the original implementation
ent rewards to terminal states, while assigning interventions a of [Datta et al., 2004]. Because we implemented the Datta

                                                 IJCAI07
                                                  1838