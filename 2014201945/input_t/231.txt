               Modular self-organization for a long-living autonomous agent 

                                                Bruno SCHERRER 
                                              scherrer@loria.fr 
                                                   LORIA, BP 239 
                                            54506 Vandceuvre-les-Nancy 
                                                        France 

                        Abstract                                 This paper proposes an algorithm by which the organiza•
                                                              tion of an agent in functional modules is automatically com•
     The aim of this paper is to provide a sound frame•       puted. The most significant aspect of our work is that the 
     work for addressing a difficult problem: the auto•       number m of modules is fewer than the number n of tasks to 
     matic construction of an autonomous agent's mod•         be performed. Therefore, the approach we propose involves 
     ular architecture. We briefly present two appar•         a high-level clustering process, in which the n tasks need to 
     ently uncorrelated frameworks: Autonomous plan•          be "properly" spread over the m modules. 
     ning through Markov Decision Processes and Ker•             Section 1 introduces what we consider as the theoretical 
     nel Clustering. Our fundamental idea is that the         foundation for modelling a mono-task autonomous agent: 
     former addresses autonomy whereas the latter al•         Markov Decision Processes. Section 2 presents the Kernel 
     lows to tackle self-organizing issues. Relying           Clustering approach: we consider this approach as a theoret•
     on both frameworks, we show that modular self-           ical basis for addressing self-organization. Finally, Section 3 
     organization can be formalized as a clustering prob•     combines both domains in order to propose a modular self-
     lem in the space of MDPs. We derive a modular            organizing algorithm. 
     self-organizing algorithm in which an autonomous 
     agent learns to efficiently spread n planning prob•
     lems over m initially blank modules with rn < n.         1 Modelling A Mono-Task Autonomous Agent 
                                                              Markov Decision processes [Puterman, 1994] provide the 
                                                              theoretical foundations of challenging problems such as plan•
Introduction                                                  ning under uncertainty and reinforcement learning [Sutton 
This paper addresses the problem of building a long-living    and Barto, 1998]. They stand for a fundamental model for se•
autonomous agent; by long-living, we mean that this agent     quential decision making and they have been applied to many 
has a large number of relatively complex and varying tasks    real worls problems [Sutton, 1997]. This section describes 
to perform. Biology suggests some ideas about the way an•     this formalism and presents a general scheme for approach•
imals deal with a variety of tasks: brains are made of spe•   ing difficult problems (that is problems in large domains). 
cialized and complementary areas/modules; skills arc spread     A Markov Decision Process (MDP) is a controlled stochas•
over modules. On the one hand, distributing functions and     tic process satisfying the Markov property with rewards (nu•
representations has immediate advantages: parallel process•   merical values) assigned to state-control pairs. Formally, an 
ing implies reaction speed-up; a relative independence be•    MDP is a four-tuple (S, A, T, R) where S is the state space, 
tween modules gives more robustness. Both properties might    A is the action space, T is the transition function and R. is the 
clearly increase the agent's efficiency. On the other hand, the reward function. T is the state-transition probability distribu•
fact of distributing a system raises a fundamental issue: how tion conditioned by the control: 
does the organization process of the modules happen during 
the life-time ?                                                                                                      (0 
  There has been much research about the design of modu•
                                                                            is the instantaneous reward for taking action 
lar intelligent architectures (e.g. [Theocharous et al, 2000J 
                                                                  A in state S. 
[Hauskrechtef a/., 1998] [Kaelbling, 1993]). It is neverthe•
less very often the (human) designer who decides the way        The usual MDP problem consists in finding an optimal pol•
modules are connected to each other and how they behave       icy, that is a mapping : S A from states to actions, that 
with respect to the others. Few works study the construc•     maximises the following performance criterion, also called 
tion of these modules. To our knowledge, there are no ef•     value function of policy TT: 
fective works about modular self-organisation except for re•
active tasks (stimulus-response associations) (e.g. [Jacobs et                                                       (2) 
al., 1991] [Digney, 1996]). 


1440                                                                                                  POSTER PAPERS It is shown that there exists a unique optimal value function Given a set of kernels a data point x is naturally 
V*; once V* is computed, an optimal policy can immediately    associated to its most representative kernel L{x), i.e. the one 
be derived (e.g. see [Puterman, 1994]).                       that is the closest according to distance d: 
  In brief, solving an MDP problem amounts to computing 
the optimal value function. Well-known algorithms for doing                                                          (3) 
so are Value Iteration and Policy Iteration (see [Puterman,   Conversely, a set of kernels naturally induces a 
1994]). Their temporal complexity dramatically grows with     partition of the data set into m classes  
the number of states [Littman et al, 1995], so they can only  each class corresponding to a kernel: 
be applied in small domains. 
  In large domains, it is impossible to solve an MDP ex•                                                             (4) 
actly, so one usually adresses a complexity/quality compro-     Given a data space, a data set, a kernel space and a distance 
mis through an approximation scheme. Ideally, an approxi•     d(), the goal of the Kernel Clustering problem is to find the 
mation scheme for MDPs should consist of a set of tractable   set of kernels that minimizes the distortion D 
algorithms for                                                for the data set which is defined as follows: 
  • computing an approximate optimal value function 
  • evaluating (an upper bound of) the approximation error                                                           (5) 
  • improving the quality of approximation (by reducing the 
    approximation error) while constraining the complexity.   A general procedure for suboptimally solving this problem 
                                                              is known as the Dynamic Cluster algorithm [Diday, 19731, 
The first two points are the fundamental theoretical bases for 
                                                              which we present in an online version in algorithm 2. It is 
sound approximation. The third one is often interpreted as 
a learning process and corresponds to what most Machine 
Learning researchers study. For convenience, we respectively 
call these three procedures Appproximate(), Error () and 
Learn(). Then, the practical use of an approximate scheme 
can be sketched by algorithm 1. One successively applies the 


                                                              a very intuitive process: for each piece of data x, one finds 
                                                              its most representative kernel L, and one updates L so that 
Lcarn() procedure in order to minimize the approximation      it gets even more representative of x. Little by little, one 
error; when this is done, one can compute a good approxi•     might expect that such a procedure will minimize the global 
mate optimal value function.                                  distortion and eventually give a good clustering. 

2 Kernel Clustering                                           3 Modular Self-Organization 
Before addressing the problem of modular self-organization,   This final section shows how the Kernel Clustering paradigm 
we need to present the Kernel Clustering paradigm. In [Di-    can be used to formalize a modular self-organization prob•
day, 1973], the author introduces the Kernel Clustering ap•   lem in the MDP framework, the algorithmic solution of which 
proach as an abstract generalization of vector quantization.  will be given by the on-line Dynamic Cluster procedure (al•
Indeed, the author argues that, in general, a clustering prob• gorithm 2). 
lem is based on three elements:                                 If one carefully compares the general learning scheme we 
                                                              have described in order to address a large state space MDP 
  • a set of data points taken from a data space A" 
                                                              (algorithm 1) and the on-line Dynamic Cluster procedure (al•
  • a set of kernels taken from a kernel space                gorithm 2), one can see that the former is a specific case of 
                                                              the latter. More precisely, algorithm 1 solves a simple Kernel 
  • A distance measure between any data                       Clustering problem where 
    point and any kernel. The smaller the distance d(x, L),     • the data space is the space of all possible MDPs and the 
    the more L is representative of the point x.                  data set is a unique task corresponding to an MDP M 


POSTER PAPERS                                                                                                      1441    • the kernel space is the space of all possible approxima•  problem in the space of MDPs. A natural algorithmic solu•
     tions and there is one and only one kernel:               tion to this clustering problem (algorithm 3) uses an on-line 
                                                               version of the Dynamic Cluster algorithm (algorithm 2). Due 
   • the distance d is the Error() function. 
                                                               to lack of space, we could not show any experimental evalu•
   It is then straightforward to extend this simple clustering ation; interested readers will find some in [Scherrer, 2003bl 
 problem to a more general one (with n tasks/data points and   and [Scherrer, 2003a]. 
 m approximate models/kernels). Given a set of m approxi•
 mate models }, an MDP is naturally associ•                    References 
 ated to the approximate model that makes the small•           [Diday, 1973J E. Diday. The dynamic clusters method 
 est error:                                                       and optimization in non hierarchical-clustering. In 
                                                                  SpringerVerlag, editor, 5th Conference on optimization 
                                                        (6)       technique, Lecture Notes in Computer Science 3, pages 
                                                                  241-258,1973. 
 As before, a set of approximate models nat•
                                                               [Digney, 1996] B. Digney. Emergent hierarchical control 
 urally induces a partition of any set of n MDPs 
                                                                  structures: Learning reactive hierarchical relationships in 
 into m classes each class corresponding to an 
                                                                  reinforcement environments, 1996. 
 approximate model: 
                                                               [Hauskrecht et al., 1998] M. Hauskrecht, N. Meuleau, L. P. 
                                                        (7)       Kaelbling, T. Dean, and C. Boutilier. Hierarchical solu•
                                                                  tion of Markov Decision Processes using macro-actions. 
   The transpositon of the on-line Dynamic Cluster Algorithm      In Uncertainty in Artificial Intelligence, pages 220-229, 
into the MDP framework (algorithm 3) therefore allows to          1998. 
find a set of m approximate models that globally minimize 
the approximation error for n MDPs:                            [Jacobs et al, 1991] R. Jacobs, M. Jordan, and A. Barto. 
                                                                  Task decomposition through competition in a modular 
                                                                  connectionist architecture: The what and where vision 
                                                                  tasks. Cognitive Science, 15:219-250, 1991. 
                                                               [Kaelbling, 1993] L. P. Kaelbling. Hierarchical learning in 
                                                                  stochastic domains: Preliminary results. In International 
                                                                  Conference on Machine Learning, pages 167-173, 1993. 
                                                               [Littmanefa/., 1995] M. L. Littman, T. L. Dean, and L. P. 
                                                                  Kaelbling. On the complexity of solving Markov decision 
                                                                  problems. In Proceedings of the Eleventh Annual Con•
                                                                 ference on Uncertainty in Artificial Intelligence (UAI-95), 
                                                                  pages 394-402, Montreal, Quebec, Canada, 1995. 
                                                               [Puterman, 1994] M. Puterman. Markov Decision Processes. 
                                                                  Wiley, New York, 1994. 
                                                               [Scherrer, 2003a] B. Scherrer. Apprentissage de 
                                                                  representation et auto-organisation modulaire pour 
                                                                  un agent autonome. PhD thesis, Universite Henri Poincare 
                                                                  - Nancy 1, January 2003. 
                                                               [Scherrer, 2003b] Bruno Scherrer. Modular Self-
                                                                  Organization for a long-living autonomous agent. 
                                                                 Technical report, INR1A, April 2003. 
                                                               [Sutton and Barto, 1998] R.S. Sutton and A.G. Barto. Rein•
                                                                 forcement Learning, An introduction. BradFord Book. The 
                                                                 MIT Press, 1998. 
order to efficiently solve n tasks, or, as we might say, it self-
organizes the m modules in order to improve the resolution     [Sutton, 1997] Richard S. Sutton. On the significance of 
of the n tasks.                                                   markov decision processes. In ICANN, pages 273-282, 
                                                                  1997. 
Conclusion                                                     lTheocharous et al.,2000] G. Theocharous, K. Rohani-
                                                                 manesh, and S. Mahadevan. Learning and planning with 
In this paper, we have described a general scheme for ad•        hierarchical stochastic models for robot navigation. In 
dressing large state space Markov Decision Processes. We         ICML 2000 Workshop on Machine Learning of Spatial 
have then showed how such an approach could be extended          Knowledge, Stanford University, July 2000. 
to an interesting problem: modular self-organization. Indeed, 
we have formalized modular self-organization as a clustering 


1442                                                                                                   POSTER PAPERS 