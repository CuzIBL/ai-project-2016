                             Learning User Clicks in Web Search

                  Ding Zhou, Levent Bolelli, Jia Li, C. Lee Giles, Hongyuan Zha
                           Department of Computer Science and Engineering
                                        Department of Statistics
                            College of Information Sciences and Technology
                     The Pennsylvania State University, University Park, PA 16802

                    Abstract                          search scenario. The vast amount of documents quickly result
                                                      in very high dimensional space and hence sparsiﬁes the prob-
    Machine learning for predicting user clicks in Web- lem space (or equivalently leads to a lack of observations ),
    based search offers automated explanation of user which effects model training. In order to reduce the problem
    activity. We address click prediction in the Web  space, some previous work ﬁrst classiﬁes the large document
    search scenario by introducing a method for click collection into a small number of categories so as to predict
    prediction based on observations of past queries  topic transition in Web search [Shen et al., 2005].However,
    and the clicked documents. Due to the sparsity of since the prediction of user clicks requires the granularity of
    the problem space, commonly encountered when      a single document, we do not consider issues in document
    learning for Web search, new approaches to learn  clustering but rather work on the full space of the document
    the probabilistic relationship between documents  collection.
    and queries are proposed. Two probabilistic models
    are developed, which differ in the interpretation of Consider the problem of learning from Web search logs for
    the query-document co-occurrences. A novel tech-  click prediction. We deﬁne the task as learning the statistical
    nique, namely, conditional probability hierarchy, relationship between queries and documents. The primary
    ﬂexibly adjusts the level of granularity in parsing assumption is that the clicks by users indicate their feedback
    queries, and, as a result, leverages the advantages on the quality of query-document matching. Hypothesize that
    of both models.                                   the vocabulary of query in terms of words remains stable over
                                                      a period of time. Denote this by Σ.Ideally,ifwewereableto
                                                      collect sufﬁcient instances for every combination of words (
1  Introduction                                       2Σ ) and the clicked documents with these queries, we would
Predicting the next click of a user has gained increasing im- be able to estimate the probability of future clicks based on
portance. A successful prediction strategy makes it possible past observations. The feasibility of the approach, however,
to perform both prefetching and recommendations. In addi- relies on the assumptions that the training data exhausts all
tion, the measurement of the likelihood of clicks can infer a the possible queries.
user’s judgement of search results and improve ranking. However, since the number of different queries in Web
  As an important goal of Web usage mining [Srivastava et search explodes with emerging concepts in user knowledge
al., 2000], predicting user clicks in Web site browsing has and randomness in the formation of queries, the tracking of
been extensively studied. Browse click prediction typically all possible queries becomes infeasible both practically and
breaks down the process into three steps: (1) clean and pre- computationally. For example, Fig. 1 illustrates the increase
pare the Web server log data; (2) extract usage patterns; and in the number of distinct queries submitted to CiteSeer over a
(3) create a predictive model. There have been a variety of period of three months. The linear growth of distinct queries
techniques for usage pattern extraction, user session cluster- over time indicates that we can hardly match even a small
ing [Banerjee and Ghosh, 2001; Gunduz and Ozsu, 2003], fraction of the new queries exactly with the old queries. As a
page association rule discovery [Gunduz and Ozsu, 2003], result, prediction cannot be performed for new queries, yield-
Markov modeling [Halvey et al., 2005], and implicit rele- ing low predictability. Furthermore, many new documents are
vance feedback [Joachims, 2002].                      being clicked even after having accumulated documents that
  Contrary to the rich research in click prediction in Web site were clicked on over considerably long time. Learning of the
browsing, the prediction of user clicks in Web search has not relationship between the complete queries and the documents
been well addressed. The fundamental difference between is consequently a highly challenging problem.
predicting click in Web search and Web site browsing is the An alternative naive solution to the lack of training in-
scale of the problem space. The modeling of site browsing stances is to break down queries into words. An observation
typically assumes a tractable number of pages with a reason- of a query and the corresponding clicked document ( query-
able number of observations of the associations among these document pair) is transformed into several independent ob-
pages. This assumption, however, no longer holds in the Web servations of word and document, i.e. word-document pair.

                                                IJCAI-07
                                                  1162             5
           x 10
           9                                          (query, clicked URL) pair is used to construct a bipartite
           8  Number of different queries
              Number of different words
           7                                          graph and queries that result in clicking to the same URL are
           6                                          clustered. The content features in the queries and the docu-
           5                                          ments are discarded. Joachims [Joachims, 2002] introduces
           4

          Observations 3                              a supervised learning method that trains a retrieval function
           2                                          based on click-through data that takes the relative positions of
           1

           0                                          clicks in a rank as training data set. We pursue a different ap-
           0   2    4   6    8   10   12
                    Observation time (weeks)          proach that seeks to learn the statistical relationship between
                                                      queries and user actions in this paper.
Figure 1: Number of distinct queries and words in CiteSeer
over 12 weeks.                                        3   Problem Statement
                                                      Descriptions of the problem is formalized. Let ℵ be the doc-
This solution can predict unknown queries as long as the new ument full set. {di} = D ∈ℵdenotes the set of docu-
query contains some known words. However, this solution ments that have ever been shown to users in search results
suffers in prediction accuracy due to the discarding of word and C ⊆ D is the document set that has been clicked on.
proximity information in existing queries.            Σ={wi}    denotes the word vocabulary. The query set is
                                                                                           Σ
  We ﬁrst propose two probabilistic models, namely full Q = {qj},whereqj = {wj1, ..., wjk}∈2 . Our observa-
model and independent model, in order to capture the ideas tion of Web search log is abstracted as a sequence of query-
behind the above intuition, which interpret query-document document pairs: Γ ⊆ Q × C. The posterior probability for
pairs differently. The full model tends to achieve high pre- observing each click on a certain document d is P (d|q, dq),
diction accuracy if sufﬁcient training instances are provided, where dq represents the document list returned with query q.
but it cannot be applied when new queries are encountered. The problem is to predict P (d|q, dq, Γ), which measures
On the other hand, the independent model yields high pre- the probability of user clicking on document d for query q
dictability while suffering in accuracy. In order to tradeoff when presented with the result list dq. The prediction of
the prediction accuracy with predictability, we suggest a new user click for query q becomes: dˆ=argmaxd P (d|q, dq, Γ),
conditional probability hierarchy technique that combines the where d ∈ dq and Γ is the observation of query-document
two models. We are not aware of any previous work study- click so far.
ing the click prediction problem for Web search on such large
scale search logs. In addition, as a by-product of the new 4 Probabilistic Models
combination approach, n-grams of words are discovered in-
crementally.                                          The two probabilistic models we propose are for acquiring
                                                      estimations of P (d|q) for each d in dq given query q.De-
                                                      pending on interpretations of a click on d for q, two types of
2  Related Work                                       models are introduced, namely, the independent model and
User click prediction is based on understanding the naviga- the full model.
tional pattern of users and, in turn, modeling observed past
behavior to generate predictive future behavior. Usage pattern 4.1 Independent model
modeling has been achieved traditionally by session cluster- When we observe a query-document pair d, q,howdowe
ing, Markov models, association rule generation, collabora- interpret it? The independent model we propose ﬁrstly as-
tive ﬁltering and sequential pattern generation.      sume each word in q is independent of each other. Formally,
  Markov model based prediction methods generally suffer the independent model we deﬁne interprets an instance d, q
from high order, usually needing clustering of user clicks to as observing d and given d, observing the words w1, ..., wk
reduce the limitations stemming from high state-space com- independently.
plexity and decreased coverage. On the other hand, lower Let us consider how P (d|q) is estimated under the indepen-
order Markov models are not very accurate in predicting dent model. In the case where the query consists of k words
user’s browsing behavior, since these models keep a small q =[w1, ..., wk] and the clicked document is d, we measure
window to look back in the history, which is not sufﬁcient the probability P (d|w1, ..., wk) as:
to correctly discriminate observed patterns [Deshpande and
Karypis, 2004]. Markov models seem to be more suitable for                     P (d, w1, ..., wk)
                 [                ]                           P (d|w1, ..., wk)=
mobile applications Halvey et al., 2005 where the number                        P (w1, ..., wk)
of states is low due to the few links a user can navigate.
                                                                        P d P w  |d ...P w |d
  User session clustering [Banerjee and Ghosh, 2001; Gun-               ( ) (  1 )    ( k )
                ]                                                    =                                (1)
duz and Ozsu, 2003 identiﬁes users’ navigational paths from               d∈D P (d, w1, ..., wk)
web server logs and deﬁnes session similarity metrics based
                                                                            k
on similarity of the paths and time spent on each page on a             P (d)    P (wi|d)
                                                                 =           i=1                    (2)
path. Clustering is employed on the similarity graph which                P  d   k  P w |d
are utilized for predicting users’ requests.                          d∈D  ( )   i=1 (  i )
  Beeferman and Berger [Beeferman and Berger, 2000] pro- Eq. 1 is obtained using the Bayes formula. The transition
posed query clustering based on click-through data. Each from Eq. 1 to Eq. 2 assumes the conditional independence

                                                IJCAI-07
                                                  1163                                                                         P(d’|a,b,c,d,e)
between wi and wj according to the model deﬁnition. P (d) is                                 L4
the marginal probability of document d, which is proportional                       P(d’|c,d,e)
to the number of occurrences of d. The above derivations                                     L3
                                                                 P(d’|a,b) P(d’|c,d)
show that for the purpose of calculation, each d, q can be                                 L2
broken down to a collection of independent instances d, wi,
     i    , ..., k                                                                           L1
where  =1      .                                                     ab       cde
  As indicated in Eq. 2, the estimation requires calculation of
P (d) and P (wi|d). Since we are able to keep track of P (d)
                                                      Figure 2: CPH: Hierarchical combination of conditional
easily, the computation for Eq. 2 then transforms to P (wi|d).                                
  §                                   P w  |d         probabilities at different levels. To estimate P (d |a, b, c, d, e)
In  6, we discuss the Bayesian estimation of ( i ) to ad-          d           a, b, c, d, e P d|a, b
dress the sparsity in training data.                  for document    and query           ,   (     ) and
                                                      P (d|c, d, e) are combined. P (d|a, b) is the combination of
                                                                                       
4.2  Full model                                       P (d |a), P (d |b) and n(d ,a,b),wheren(d ,a,b) denotes the
                                                      number clicks on d with queries containing a, b.
While the independent model treats each query as a set of
independent single words, the full model reﬂects the other ex-
treme where all words within a query are treated as a group. with the CPH. A query q = a, b, c, d, e consists of ﬁve
In our deﬁnition, the full model treats the q in an instance individual words a, b, c, d and e. Suppose we already have
                                                                                    α
d, q as a singleton. The combination of words in q is re- the full model estimation of P (d |w) for w = a, b, c, d and
                                                                       
garded as an entity.                                  e.WesettheP    (d |w)’s at L1 as equivalent to full model
  The full model emphasizes a query as a group and hence estimation:
yields high prediction accuracy, provided that a large amount
of queries have been observed with large supports. However,            h         α         β
                                                           L1 :  P (d |w) = P (d |w) = P (d |w)       (3)
as noted before, the number of different queries grows so
quickly that the full model always suffers the lack of train- where w = a, b, c, d, e. Note that at the single word level, full
ing data in practice. In the next section, we will introduce the model and independent model are equivalent. Thus we have
method to combine the full model and the independent model P (d|w)α = P (d|w)β .
in order to address the sparsity issue.                 Then we arrive at the combination of probabilities in level
                                                      L1 to L2:
5  Probability Hierarchy
                                                                    h                β            α
Ideally, if we are able to observe all future queries with the L2 : P (d |a, b) =(1− λ)P (d |a, b) + λP (d |a, b) (4)
                                                                    h                β            α
documents being clicked in the log ﬁle, especially with de- L2 : P (d |c, d) =(1− λ)P (d |c, d) + λP (d |c, d) (5)
cent number of instances, the full model alone can be sufﬁ-
cient. However, new queries keep emerging which dramat- where λ is the shrinkage rate that we set according to our
ically enlarges the problem space if we simply learn from conﬁdence with the full model in this case. Note that λ can
the co-occurrence d, q. In order to address such sparsity vary for every case and is tunable according to observation
issues, we introduce this new conditional probability hier- supports of long units.
                                                                                       h
archy (CPH) method, which recursively generates multiple Similarly, the estimation P (d |c, d, e) at L3 is expressed
intermediate combinations between the full and independent as:
models. The shrinkage rate is tunable according to the sup-        h                   β             α
                                                      L3  P  d |c, d, e    − λ P d |c, d, e  λP d |c, d, e (6)
ports of the full model.                                 :  (       ) =(1     ) (       )  +   (       )
                                                      where the independent model estimation of P (d|c, d, e)β is
5.1  Hierarchical shrinkage                           the combination of P (d|c, d)h and P (d|e)h by setting:
The conditional probability hierarchy (CPH) we propose
combines the full model and the independent model.Itstarts                P d P c, d|d hP e|d h
                                                          P d|c, d, e β   ( )  (     )  (   )   .
with treating a query as a set of independent words, i.e. ob- (    )  =                 h      h   (7)
                                                                            P (d )P (c, d|d ) P (e|d )
taining P (d|wj). Hierarchically, words are merged into n-                d
     1
grams  (or units) of increasing length, getting P (d|uk).The Finally, the estimation P (d|a, b, c, d, e)h becomes:
ﬁnal estimation of P (d|q) is the hierarchical combination
across several levels.                                                                         h
  Fig.  2   illustrates an example   estimation  of    L4 :                       P (d |a, b, c, d, e) =
                                                                              β                 α
P (d |a, b, c, d, e). Suppose we have a document d and     (1 − λ)P (d |a, b, c, d, e) + λP (d |a, b, c, d, e) (8)
               u   u
a word sequence   (  can be either a single word of a                          β
multiple word query ). Let P (d|u)α denote the estimation of where, again, P (d |a, b, c, d, e) is estimated using indepen-
P d|u                      P  d|u β                   dent model on P (d|a, b)h and P (d|c, d, e)h.
 (   ) under the full model and ( ) under independent                                          
model.WeuseP   (d|u)h to denote the estimation of P (d|u) In general, for a query q, and document d ,theCPH
                                                      P (d|q)h is:
  1The n-gram (or unit) in our case is referred to as a word se-
                                                                   h              β          α
quence of length n.                                          P (d |q) =(1−  λ)P (d |q) + λP (d |q) .  (9)

                                                IJCAI-07
                                                  1164  Then P (d|q)β is estimated using:                    The only question left for the Bayesian estimation of
                                                      P (θ|x) is the parametrization of the conjugate prior P (θ),
                            h      h               i.e. the determination of α and β for P (θ). Assume each
                  P (d )P (ql|d ) P (qr|d )
     P d|q β                           .            document is equally likely to be clicked on2.Wewantto
       (   ) =      P d P q |d hP q |d h    (10)
                  d (  ) ( l  )  ( r  )              set expectation E(P (θ)) as 1/m,wherem is the number of
                                               h     distinct documents in the whole collection. Since we have
where ql ⊕ qr = q,i.e.ql and qr areasplitofq, P (ql|d ) is               α            α       1
                   h            α                  E(Beta(α, β)) =       ,weset         =   , obtaining
obtained using P (d ,ql) /P (d ), P (d |q) is the full model           (α+β)        (α+β)     m
                                                            β
estimation. The structure of the tree reﬂects a particular re- α = m−1 . The Bayesian estimation for P (d|w) becomes:
cursive parsing of the queries into smaller word combinations
                                                                                    β
and ultimately single words. With the structure of the tree                       m−1  + x
                        h                                          θˆ  P d|w
ﬁxed, the probability P d|q can be computed recursively             =   (   )=    β                  (13)
                    (  )                                                             + β + n
by a bottom-up procedure. Eq. 9 and Eq. 10 illustrate the re-                   m−1
cursion. We refer to the tree structure, exempliﬁed in Fig. 2, where, again, x is the number of clicks on d and m repre-
as the conditional probability hierarchy (CPH).       sents the number of candidate documents. n is the number
  The construction of the tree has much to do with the over- of times that w has been clicked on with any document. We
all performance. Note that we only use 2-way tree here. A need α, β > 0. In experiments, we tune β.
greedy algorithm is used to produce the hierarchy. In particu-
lar, we iteratively search for the binary adjacent units (can be 7 Experiments
a word) with largest support in each level and feed the merged
units to the higher level.                            For evaluation, we study the property of our approach from
  Now we need to determine the parameter λ in Eq. 9. In- three perspectives: (a) prediction accuracy; (b) query seg-
tuitively, λ should depend on the number of instances that mentation quality and (c) prediction power, i.e. predictability.
                                                      The accuracy in prediction is evaluated using both MLE and
w1, ..., wk appear together. It is natural to give higher weight
to the full model when there are many such observations since Bayesian estimation. The query segmentation quality exam-
the full model tends to be more accurate. Accordingly, we ines the semantic structure in queries.
                 n(w ,...,w )
weight λ as: λ =    1   k  ,whereα>0.Alargeα
                α+n(w1,...,wk )                      7.1  Data preparation
indicates a higher trust in previous estimations and a slower We apply our click model to the search environment in Cite-
update rate.                                          Seer (citeseer.ist.psu.edu), a popular online search engine and
                                                      digital library. We collect the Apache access logs at one Cite-
6  Bayesian estimation                                Seer server over 90 days period. There are in total 56,452,022
So far, we have seen that both models depend on obtaining es- requests in this period.
timation for the probability P (d|q) from the observations of We remove the robots by their agent ﬁeld in Apache
d, q or d, w. In the estimation for the independent model, logs and time constratints. We further identify the queries
the estimation of P (wi|d) is required but boils down to the performed at CiteSeer obtaining a total of 886,957 distinct
estimation of P (d|wi) using Bayesian formula. For brevity, queries and 1,826,817 query-click pairs. There are in all
we only focus on deriving the estimation for P (d|w).We 510,409 distinct documents ever shown in search results,
apply Bayesian estimation due to the lack of sufﬁcient obser- 301,052 of which have been clicked on. For each query and
vations. Let P (d|w)=θ ∼B(θ). We need to estimate θ.Let the document being clicked, we collect the ﬁrst 20 documents
n be the number of times that w has been clicked on with any from which this document was picked.
document, x be the number of clicks on d.
  We assume users carry out queries and clicks indepen- 7.2 Evaluation metrics
dently. Then P (x|θ) is a binomial distribution. If we use the Two important quantitative metrics for evaluation are (a) pre-
beta distribution Beta(α, β) as the conjugate prior for θ,we diction accuracy and (b) predictability.
will easily see that P (θ|x) also follows the beta distribution We deﬁne a prediction accuracy in evaluation as propor-
and the beta distribution is parametrized as:         tional to number of “correct” predictions of clicked docu-
                                                      ments from the candidate list. Formally, we have prediction
                                                                              nc
          P θ|x  ∼ Beta α   x, n  β − x                                  ρ1             nc
            (  )       (  +     +      )       (11)   accuracy deﬁned as:   = ns ,where   is the number of
                                                                         n
  Now P (θ|x) ∼ Beta(α + x, n + β − x), we obtain the es- correct prediction and s is the size of tested sample queries.
timation of θ, conditioned on x, as the expectation of P (θ|x): For each query, the original returned list of documents are
                                                      provided as candidates.
             1                                         We deﬁne the predictability metric as the measurement of
                               α + x
       θˆ =     P (θ|x)θdθ =          .        (12)   the models’ robustness to new queries. Consider when the
             0               α + β + n                model estimates the P (d|q) as 0 for all candidate d’s, the fail-
                                                      ure of prediction happens. We denote this percentage as Pf .
                 θˆ
  The estimation of in Eq. 12 will serve as our estimation Quantitatively, the predictability of a model equals to 1 − Pf .
for P (d|w) in the problem. Eq. 12 gets around the sparsity
issue in training data and is capable to provide non-zero value 2We may change the assumption to take into consideration the
for P (d|w) even with no previous observation of d, w. ranking of documents in the result list.

                                                IJCAI-07
                                                  1165                                                         0.7
                  0.75

                   0.7                                   0.65

                  0.65                                   0.6
                   0.6
                                                         0.55
                  0.55

                   0.5                                   0.5

                  0.45                                   0.45
                  Prediction  accuracy                                       Hierarchy

                   0.4                                  Prediction  accuracy
                                    Hierarchy            0.4                 Full
                                    Full
                  0.35              Independent                              Independent
                                                         0.35
                   0.3
                   10k  100k 300k 500k 700k  900k         10k  100k 300k 500k 700k 900k
                             Size of training set                 Size of training set
                  (a) Maximum likelihood estimation.          (b) Bayesian estimation.

                                Figure 3: Prediction accuracy w.r.t. training size.

7.3  Prediction accuracy
                                                             Table 1: Prediction accuracy w.r.t. β setting.
We train the independent model, full model, and the CPH
                                                               β    Full model Independent model Hierarchy
model over different sized subsets of the collection of query- 1      0.56       0.47       0.61
click pairs. For each round of testing, we randomly choose     5      0.57       0.47       0.62
1,000 query-clicks complementary set of training. In each      10     0.58       0.46       0.61
test, we evaluate the accuracy using the two metrics deﬁned    100    0.57       0.44       0.59
above. Since we will study the predictability later, the accu-
racy we show here is for predictable query-click pairs.
  Fig. 3(a) and Fig. 3(b) give the experimental evaluation on Table 2: Hierarchies discovered in sample queries.
the accuracy for our three models w.r.t. training size. Sub-
sets of the whole collection with sizes from 10K to 900K 1             [ tutorial, [target, tracking] ]
query-document pair instances are experimented. Fig. 3(a) 2        [ [machine, learning], [search, engine] ]
presents the accuracy of prediction using MLE for P (d|w). 3       [ partial, [ [least, square], regression ] ]
Comparatively, in Fig. 3(b), we present the accuracy compar- 4        [ [k, means], [cluster, analysis] ]
ison using Bayesian estimation measurement. In both ﬁgures, 5        [ [markov, chain], [monte, carlo] ]
                                                         6            [ [spectral, clustering], jordan ]
the shrinkage rate λ for CPH model is set to 0.6 so that we
                                                         7    [ [energy, efﬁcient], [matrix, multiplication], fpgas ]
give full model higher weight in combination. For Bayesian
                     β                                   8           [ distributed, [cache, architecture] ]
estimation prediction, the is set to 5.                  9            [ [ [code, red], worm ], paper ]
  We are able to see that the full model usually outperforms 10 [[dynamic, [channel, allocation]], [cellular, telephone]]
independent model in terms of prediction accuracy, usually
by 15%. MLE works better in prediction than Bayesian es-
timation but the MLE leads to lower predictability ( we will
discuss the predictability in Sec. 7.5. ). The performance of CPH model w.r.t. setting of β for priors in hierarchy. As can
the new CPH technique is slightly lower than full model in be seen, the accuracy remains relatively stable but the smaller
MLE but better than full model in Bayesian estimation. The β gives higher accuracy.
CPH technique gains signiﬁcantly higher accuracy in predic-
tionthantheindependent model.                         7.4  Query segmentation
  In Fig. 4, we show the impacts of the setting of shrinkage
rate λ on the accuracy and predictability. We use the training In this section, we present the quality of query segmentation
set sized 500K. As expected from Eq. 9, the accuracy tilts up formed in discovered query hierarchies. A nice segmentation
as λ increases and the predictability goes down.      of queries detects the n-grams in queries that provides the ba-
                                                      sis for the full model. Due to the limit of space, we present

          1                                           10 randomly picked queries issued to CiteSeer and their seg-
                          Predictability
                          Prediction accuracy
         0.8                                          mentations performed in the CPH.
                                                        The discovered hierarchies in queries are presented by
         0.6
                                                      nesting square brackets in Table 2. We are able to see, from
         0.4
         Percentage                                   the limited sample, that the hidden structure of words in plain
         0.2                                          text queries are well discovered using the n-gram frequency.

          0                                           With proper segmentations, discovered n-grams in queries are
          0.1     0.3    0.5    0.7    0.9
                     Shrinkage rate                   feed to full models, and the hierarchical structures are fol-
                                                      lowed while evaluating Eq. 9 for probabilistic hierarchy.As
Figure 4: Prediction accuracy and power w.r.t. shrinkage rate. we have seen in Sec. 7.3, the use of n-grams for full model
                                                      boosts prediction accuracy of independent model. We will
  The sensitivity of β for Bayesian estimation of three mod- see in Sec. 7.5 that the probabilistic hierarchy improves in
els are also tested. In Table. 1, we present the accuracy of predictability from full model as well.

                                                IJCAI-07
                                                  1166