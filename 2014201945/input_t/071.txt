Data Clustering: Principal Components, Hopfield and Self-Aggregation Networks* 

                                                   Chris H.Q. Ding 
                           NERSC Division, Lawrence Berkeley National Laboratory 
                        University of California, Berkeley, CA 94720. chqding@lbl.gov 


                        Abstract                               well-motivated clustering objective functions. A further point 
                                                               is the recognition that spectral graph clustering is embedded 
     We present a coherent framework for data cluster•
                                                               in the scaled PCA thus leading to self-aggregation networks. 
     ing. Starting with a Hopfield network, we show 
                                                                 This paper combines above three threads and develop a 
     the solutions for several well-motivated cluster•
                                                               coherent framework for clustering. We begin with two-way 
     ing objective functions are principal components.         clustering in and generalize to multi-way clustering in §3. 
     For MinMaxCut objectives motivated for ensuring 
     cluster balance, the solutions are the nonlinearly        2 Two-way clustering 
     scaled principal components. Using scaled PC A, 
     we generalize to multi-way clustering, construct•         We start by formulate two-way clustering as a Hopfield net•
     ing a self-aggregation network, where connection          work and derive PCA as cluster indicator vectors. 
     weights between different clusters are automati•          2.1 Hopfield Network and PCA 
     cally suppressed while connection weights within 
     same clusters are automatically enhanced.                 Given n data points and properly defined similarity or associ•
                                                               ation 0 between points i, j, we form a network 
                                                               (a weighted graph) G with as the connection between 
1 Introduction 
                                                               nodes We wish to partition it into two clusters Cj,c2. 
Principal component analysis (PCA) is widely adopted as an     The result of clustering can be represented by an indicator 
effective unsupervised dimension reduction method. PCA is      vector q, where 
extended in many different directions [Hastie and Stuetzle, 
1989; Kramer, 1991; Lee and Seung, 1999; Scholkopf et al,                                                              (1) 
1998; Collins et a/., 2001]. 
   The main justification is that PCA uses singular value de•  Consider the clustering objective, 
composition (SVD) which is the best low rank approxima•

tion in L2 norm to original data due to Eckart-Young theo•
rem. However, this results alone is inadequate to explain the 
effectiveness of PCA. Here, we provide a new derivation of 
PCA based on optimizing suitable clustering objective func•
                                                               ity of c1, and analogously for s(c2, c2). 
tions and show that principal components are actually cluster    Now we propose a min-max clustering principle: data 
indicator vectors in clustering. 
                                                               points are grouped into clusters such that the overlap s(c1, c2) 
   Hopfield networklHopfield, 1982] provide a convenient       between different clusters are minimized while within-cluster 
framework of our study. In particular, the self-aggregation    similarities are maximized 
network proposed in this work uses Hebb rule to encode pat•                                                            (3) 
tern vectors. One feature of Hopfield associative-memory 
networks is that it can be adopted to solve hard combinato•    These conditions can be simultaneously satisfied by maximiz•
rial problems[Haykin, 1998 2nd ed].                            ing the energy (objective function) J1. Using Hopfield model 
  Another thread of this work is the spectral graph partition• [Hopfield, 1982], the solution is obtained by the update rule 
ing [Fiedler, 1973; Pothen et al., 1990; Hagen and Kahng, 
1992; Shi and Malik, 2000; Ding et al, 2001a; 2001b; 
Ng et al., 2001; Meila and Shi, 2001], which uses Laplacian 
matrix of a graph. This arises naturally for balancing the clus• or in vector form If one relaxes q(i) 
ters (see Our approach differs from others mainly in           from discrete indicators to continuous values in (-1,1), the 
                                                               solution q is given by 
   * Work supported by Department of Energy (Office of Science, 
through a LBNL LDRD) under contract DE-AC03-76SF00098.                                                                 (4) 


LEARNING                                                                                                              479 Since the matrix entries in W are non-negative, the first prin• The solutions to this equation are the singular value decom•
cipal eigenvector qi has all positive (or all negative) entries. position (SVD) of B: {f;} are left singular vectors and {g,} 
Thus the desired solution is q2.                               are right singular vectors of the SVD of £?, 
2.2 Hopfield network for bipartite graph 
                                                                                                                      (9) 
Here we extend the Hopfield networks for clustering bipar•
tite graph. An example of bipartite graph is a m x n term-
document association matrix , where each row rep•             Again, since the matrix entries in W are non-negative, the 

resents a word, each column represents a document, and btJ    first principal components have entries of same sign; 
the counts of co-occurrence of row rt and column We thus the desired solutions are 112, V2. Note that this is also the 
show that the solution for clustering indicators is precisely SVD employed in LSI. We summarize these results in 
the Latent Semantic Indexing[Deerwester el al, 19901          Theorem 1. Principal components are solutions for clus•
   We wish to partition the r-type nodes of R into two parts  tering indicators for clustering undirect graphs and bipartite 

R1, R2 and simultaneously partition the otype nodes of C into graphs under appropriate Hopfield network models. 
two parts based on the clustering principle of mini•
mizing between-cluster association and maximizing within-     2.3 Principal component clustering 
cluster association. We use indicator vector f to determine   In the objective J1, we may explicitly enforce a balance of 
how to split R into = 1, or -1 depending on                   clusters From Eq.l, we need to minimize 
                  We use g to determine how to split C into   Thus we consider the clustering objective, 
                   -1 depending on 
   (For presentation purpose, we index the nodes such that                                                          (10) 
nodes within same cluster are indexed contiguously. The 
clustering algorithms presented are independent to this as•
                                                              where is a parameter and e — We adjust u 
sumption. Bold face lower case letters are vectors. Matri•
                                                              to control the levele of balance between The principal 
ces are denoted by upper case letters.) Thus we may write 
                                                              eigenvector q1 of is the desired indicator 
                                                              vector. 
                                                                 What is the reasonable choice of, A natural choice is to 

                                                              set the average of wiJ. With this 
                                                              choice, the modified weight matrix satisfies the sum-to-zero 
                                                       (5) 
                                                              condition: 

It is convenient to convert the bipartite graph into an undi•                                                       (11) 
rected graph. We follow standard procedure and combine the 
two types nodes to one by setting 
                                                                 This condition can be further refined by centering each col•
                                                       (6)    umn and each rows, such that 
                                                                                                                    (12) 
This induces an undirected graph G, whose adjacency matrix 
is the symmetric weight matrix W. 
  Consider the following objective function,                  where 
                                                                                                                    (13) 
                                                       (7) 
                                                              (here column and row sums, 
                                                              and w are standard notations in statistics.) Now the desired 
where , the sum of association                                cluster indicator vector is the principal eigenvector of 

between R1 and C2, and S(R1,C2) is the sum of association 
between R1 and c2. S(RI,C2) and S(R2,CI) should be mini•
                                                                The fully centered W has a useful property that all eigen•
mized. S(R1 , C\) is the sum of association within cluster 1 (see 
                                                              vectors of W with nonzero eigenvalues have the sum-to-zero 
Fig.l), and s(R2,c2) is the sum of association within cluster 
                                                              property: = 0. This is because (1) q0 = e is an 
2. S(R1 , C1) and S(R2, C2) should be maximized. These condi•
                                                              eigenvector of W with (2) all other eigenvectors are 
tions are simultaneously satisfied by maximizing J2. 
  We can write down the Hopfield network update rule for q.   orthogonal to qo, i.e, 
If one relaxes q{i) from discrete indicators to continuous val• The sum-to-zero condition = 0 does not necessar•
ues, the solution q satisfies Eq.(4). Now utilizing the explicit ily imply that the sizes of the two cluster, should be 
structures of W and q, we have                                equal. In fact, the cluster indicator vector should be refined to 

                                                       (8)                                                          (14) 


480                                                                                                          LEARNING (in this paper, all vectors are implicitly normalized to 1 using To overcome this problem, we seek to prevent either 

 L2 norm). Correspondingly, the clustering objective becomes   s(c1,C2) or s(c2,c2) become very small. We optimizelDing 
                                                               et a/., 2001b] 

                                                                                                                     (18) 
                                                      (15) 
Clearly, the first two terms represent the average within-
clustcr similarities which are maximized, and the 3rd term 
represent average betwcen-cluster similarities which are min•
imized. The factor encourages cluster balance, since                                                                 (19) 
                               is reached when = 
      We summarize these results in 
Theorem 2. Solution for clustering objective is given by 
qi; and solution for clustering objective 
   We call these clustering schemes with objective functions 
          as principal component clustering methods because 
                                                                                                                     (20) 
of the use of principal components. 
   All above discussions apply to bipartite graphs. For exam•  which can be written as 
ple, J1 becomes 
                                                                                                                     (21) 
                                                      (16) 
                                                               Again, the desired solutions is the eigenvector associated 
                                                               with the second largest eigenvalue. Note that by comparing 
                                                               Eq.(21) with Eq.(4), the net effect for cluster balance is di•
where are sizes of row clusters , — total                      agonal scaling. This diagonal scaling, however, leads to an 
row size); and are sizes of column clusters + important feature of self-aggregation (see §3.2). 
            column size). Again, the first two terms represent 
                                                               2.5 Cluster balance for bipartite graphs 
the average within-cluster similarities which are maximized, 
and the last two terms represent average between-cluster sim•  For balanced clustering of bipartite graphs, object function of 
ilarities which are minimized.                                 Eq.(7) becomes 
   We note that is partially motived by the classic scal•
ing (also called principal coordinate analysis) in statistics 
iBorg and Gronen, 1997]. Suppose we are given pairwise 
distances for a dataset, define the pairwise similarity as     Using the representation W and q of Eq.6, and similar deriva•
             , follow through the same centering procedures    tion [Ding, 2003; Zha et al, 2001 J, the solution for optimiza•

and solve for the eigenvectors of W. The classic scaling the•  tion of J4 is also given by Eq.21. Let Dr = diag(Be) and 
orem proves that if dtJ are the Euclidean distances, the first                  we have 
A" eigenvectors of W will recover the original coordinates. 
This theorem justifies the use of the K principal eigenvec•
tors as the coordinates in multidimensional scaling. However, 
our clustering approach does not emphasize the recovery of 
original coordinates, and the objective function is well-
motivated for clustering only. 


                                                      (17) 

Although are maximized, it often oc•
curs that one cluster is much larger than another, 
         or vice versa.                                        clustering balancing using MinMaxCut objective of Eq.(22) 
                                                               over the simple objective Eq.(7) is the scaling of the associ•
   'Since exp(rr) is monotonic, max J1 maxexp , which 
is minexp ]. This is approx•                                   ation matrix B in Eq.(25). However, with this scaling, the 
imately Eq.17.                                                 self-aggregation property emerges (see 3 A-way clustering 
The above mainly focus on 2-way clustering. Below we gen•
eralize to A'-way clustering, K > 2. The generalization is 
based on the key observation that the solution for cluster in•
dicator vectors (see §2.4 and §2.5) are scaled principal com• are the K eigenvectors with eigenvalue is a lin•
ponents, as we discuss next.                                  ear combination of K step functions, i.e., piece-wise constant 
                                                              function. Clearly, all data objects within the same cluster have 
3.1 Scaled principal components                               identical elements in q. The coordinate of object i in the K-
                                                              dim SPCA space is Thus objects 
Associations among data objects are mostly quantified by a 
                                                              within a cluster are located at (self-aggregate into) the same 
similarity metric. The scaled principal component approach 
                                                              point in SPAC space. 
starts with a nonlinear (non-uniform) scaling of W. Noting 
                                                                 Scaled principal components are not unique, since C could 
that W - D1/2(D'       ' WD' ' )D '  we apply spectral 
                       x 2   l 2 l 2t                         be any orthogonal matrix. However, 
decomposition on the scaled matrix 
                                                                                                                    (30) 


Note Eq.(27) is identical to Eq.(20), or Eq.(21), thus scaled 
principal components are cluster indicator vectors in Min-    Clusters overlap 
MaxCut (see §2.3).                                              Consider the case when overlaps among different clusters 
                                                              exist. The overlaps are treated as a perturbation. Theorem 
3.2 Self-aggregation network                                  3. At the first order, the K scaled principal components and 
In Hopfield networks, a pattern q1 is encoded into the net•   their eigenvalues have the form 
work as (the Hebb rule); multiple patterns are encoded 
additively. In our problem, a pattern is a cluster partitioning 
indicator vector. We define a self-aggregation network with 
the connection weights 

                   . Here we highlights several important 
properties of WSA and provides several example applica•
tions. (Self-aggregation is first studied in [Ding et al, 2002].) 
        has an interesting self-aggregation property enforced 
by within-cluster association (connectivity). To prove, we ap•
ply perturbation analysis by writing , 
where is the similarity matrix when clusters are well-        Several features of SPCA can be obtained from Theorem 3: 
separated (zero-overlap) and accounts for the overlap         Corollary 3.1. SA network has the same block di•
among clusters and is treated as a perturbationlDing et aL,   agonal form of Eq.(30), within the accuracy of Theorem 2. 
2001a]. This perturbation approach is standard in quantum     This assures that objects within the same cluster will self-
physics[Mathews and Walker, 1971].                            aggregate as in Theorem 1. 
                                                              Corollary 3.2. The first scaled principal component is qi = 
                                                                                                                 and qi 
                                                              are also the exact solutions to the original Eq.(20). 
                                                              Corollary 3.3. When K = 2, the second principal compo•
                                                              nent is 

                                                                                                                    (32) 

                                                                                                                    (33) 


482                                                                                                          LEARNING Note that this is precisely J3 of Eq.18, the clustering objective 
we started with: our clustering framework is consistent. 
Example 1. A dataset of 3 clusters with substantial ran•
dom overlap between the clusters. All edge weights are 1. 
The similarity matrix and results are shown in Fig.l, where 
nonzero matrix elements are shown as dots. The exact and 
approximate from Theorem 3 are close: 


WSA is much sharper than the original weight matrix W 
clearly due to self-aggregation: connections between differ•
ent clusters are substantially suppressed while connections 
within same clusters are substantially enhanced. 


Figure 1: Left: similarity matrix W. Diagonal blocks repre•
sent weights inside clusters and off-diagonal blocks represent 
overlaps between clusters. Right: Computed WSA-

Application 1. In DNA micro-array gene expression pro•
                                                              Figure 2: Gene expression profiles of cancerous and normal lym•
filing, responses of thousands of genes from tumor tissues    phoma tissues samples from Alizadeh et al. in original Euclidean 
are simultaneously measured. We apply SPCA framework          space (A), in scaled FCA space (B), and in scaled PCA space after 
to gene expression profiles of lymphoma cancer sampleslAl-    one iteration of Eq.34 (C). In all 3 panels, objects in original space 
izadeh ex al, 2000]. Three cancer and three normal subtypes   are shown in 2D-view spanned by the first two PCA components. 
are shown in Fig.2. This is a difficult case due to the large Cluster structures become clearer due to self-aggregation. The in•
variations of cluster sizes (the number of samples in each sub• sert in (C) shows the eigenvalues of the 1st and 2nd SA network. 
type are shown in parentheses in Fig.2B). Self-aggregation is 
evident in Figure 2B and 2C. The computed clusters corre•     Noise reduction 
spond quite well to the normal and cancer subtypes identified 
                                                                SA net has noises. For example, WSfii has sometimes nega•
by human experts. 
                                                              tive weights (WSA)ij whereas we expect them to be nonnega-
                                                              tive. However, by Corollaries 2.1 and 3.1, W  has a diagonal 
3.3 Dynamic aggregation                                                                                 SA
                                                              block structure and every elements in the block are identical 
The self-aggregation process can be repeated to obtain        (Eq.30) even when overlaps exit. This property allows us to 
sharper clusters. W& is the low-dimensional projection that   interpret as the probability that two objects i, j 
contains the essential cluster structure. Combining this struc• belong to the same cluster: 
ture with the original similarity matrix, we obtain a new sim•
ilarity matrix containing sharpened cluster information: 
                                                              To reduce noise in the dynamic aggregation, we set 
                                                      (34) 
                                                                                                                    (35) 
where, the weight matrix at f-th                              where and we chose= 0.8. Noise reduction 
iteration, Setting is crucial for enforcing the cluster       is an integral part of SA net. In our experiments, final re•
structure.                                                    sults are insensitive to The above dynamic aggregation 
  Applying SA net on W^ leads to further aggregation (see     repeats self-aggregation process and forces data objects move 
Figure 2C). The eigenvalues of the 1st and 2nd SA net are     towards the attractors, which are the desired clusters and their 
shown in the insert in Figure 1C. As iteration proceeds, a clear principal eigenvalues approach 1 (see insert in Fig.2C). Usu•
gap is developed, indicating that clusters becoming more sep• ally, after one or two iterations the cluster structure becomes 
arated.                                                       evident. 


LEARNING                                                                                                            483 