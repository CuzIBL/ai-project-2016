                    Subtree Mining for Question Classiﬁcation Problem

          Minh Le Nguyen                  Thanh Tri Nguyen                    Akira Shimazu
     Japan Advanced Institute of      Japan Advanced Institute of       Japan Advanced Institute of
      Science and Technology            Science and Technology            Science and Technology
        nguyenml@jaist.ac.jp               t-thanh@jaist.ac.jp              shimazu@jaist.ac.jp


                    Abstract                          Thus, common words like ”what”, ”else”, etc. are considered
                                                      to be ”stop-words” and omitted as a dimension reduction step
    Question Classiﬁcation, i.e., putting the questions in the process of creating features in document. However,
    into several semantic categories, is very important these words are very important for question classiﬁcation.
    for question answering. This paper introduces a   Also, word frequencies play an important role in document
    new application of using subtree mining for ques- classiﬁcation while those frequencies are usually equal to 1
    tion classiﬁcation problem. First, we formulate   in a question, thus, they do not signiﬁcantly contribute to the
    this problem as classifying a tree to a certain la- performance of classiﬁcation. Furthermore, the shortness of
    bel among a set of labels. We then present a use of question makes the feature space sparse, and thus, makes the
    subtrees in the forest created by the training data to feature selection problem more difﬁcult and different from
    the tree classiﬁcation problem in which maximum   document classiﬁcation.
    entropy and a boosting model are used as classi-
    ﬁers. Experiments on standard question classiﬁ-     There are two main approaches for question classiﬁca-
    cation data show that the uses of subtrees along  tion. The ﬁrst approach does question classiﬁcation us-
                                                                                                [
    with either maximum entropy or boosting models    ing handcrafted rules which are reported in Voorhees,
                                                          ][             ][            ]
    are promising. The results indicate that our method 1999 Voorhees, 2000 Voorhees, 2001 . Although it is pos-
    achieves a comparable or even better performance  sible to manually write some heuristic rules for the task, it
    than kernel methods and also improves testing efﬁ- usually requires tremendous amount of tedious work. One
    ciency.                                           has to correctly ﬁgure out various forms of each speciﬁc type
                                                      of questions in order to achieve reasonable accuracy for a
                                                      manually constructed classiﬁcation program.
1  Introduction                                         The second approach applies machine learning to ques-
What a current information retrieval system can do is just tion classiﬁcation problems such as in [Radev et al, 2002],
”document retrieval”, i.e., given some keywords it only re- the machine learning tool Ripper has been utilized for this
turns the relevant documents that contain the keywords. problem. The author deﬁned 17 question categories, trained
  However, what a user really wants is often a precise an- and tested their question classiﬁcation on TREC dataset.
swer to a question. For instance, given the question ”Who The reported accuracy of their system is 70%, which is
was the ﬁrst American in space?”, what a user really wants not high enough for question classiﬁcation. Li [Li and
is the answer ”Alan Shepard”, but not to read through lots Roth, 2002] presented the use of SNoW method in which
of documents that contain the words ”ﬁrst”, ”American” and the good performance of their system depends on the fea-
”space” etc.                                          ture called ”RelWords” (related word) which are constructed
  In order to correctly answer a question, usually one needs semi-automatically but not automatically. The author also re-
to understand what the question was asked. Question Classi- ported several useful features for the question classiﬁcation
ﬁcation can not only impose some constraints on the plausible task including words, parts of speech, chunking labels, and
answers but also suggest different processing strategies. For named entity labels. In addition, they indicated that hierar-
instance, if the system understands that the question ”Who chical classiﬁcation methods is natural to the question classi-
was the ﬁrst American in space?” asks for a person name, ﬁcation problem. A question is classiﬁed by two classiﬁers,
the search space of plausible answers will be signiﬁcantly re- the ﬁrst one classiﬁes it into a coarse category, the second
duced. In fact, almost all the open-domain question answer- classiﬁes it into a ﬁne category.
ing systems include a question classiﬁcation module. The Zhang and Lee[Zhang and Lee, 2003] employed tree ker-
accuracy of question classiﬁcation is very important to the nel with a SVM classiﬁer and indicated that the syntactic in-
overall performance of the question answering system. formation as well as tree kernel is suitable for this problem.
  Question classiﬁcation is a little different from document However, the authors showed the performance only for the
classiﬁcation, because a question contains a small number of coarse category, and there was no report on classifying ques-
words, while a document can have a large number of words. tions to ﬁne categories.

                                                IJCAI-07
                                                  1695  On the other hand, the connection of data mining tech-
niques with machine learning problem recently shows that
data mining can help improve the performance of classiﬁer.
Morhishita [Morhishita, 2002] showed that the use of associ-
ation rule mining can be helpful for enhancing the accuracy
of the boosting algorithm. Berzal et al also [F. Berzal et al ,
2004] presented a family of decision list based on association
rules mined from training data and reported a good result in
the UCI data set [Li and Roth, 2002].
  Mining all subtrees can be helpful for re-ranking in the
syntactic parsing problem [Kudo et al, 2005] as well as en-
semble learning in semantic parsing [Nguyen et al, 2006]. Figure 1: Labeled ordered tree and subtree relation
Along with the success, the natural question is whether or not
subtree mining can be helpful for question classiﬁcation prob-
lem? This paper investigates the connection of subtree mining Decision stump functions are observed on the training data
to the maximum entropy and the boosting model. We formu- (the set of trees and their labels), then these functions are
late the question classiﬁcation as a tree classiﬁcation prob- incorporated to a maximum entropy model and a boosting
lem. We then present an efﬁcient method for utilizing sub- model.
trees from the forests created by the training data in the max-
imum entropy model (subtree-mem) and a boosting model 2.2  Boosting with subtree features
(subtree-boost) for the tree classiﬁcation problem.   The decision stumps classiﬁers for trees are too inaccurate
  The remainder of this paper is organized as follows: Sec- to be applied to real applications, since the ﬁnal decision
tion 2 formulates the tree classiﬁcation problem in which a relies on the existence of a single tree. However, accu-
maximum entropy model and a boosting model using subtree racies can be boosted by the Boosting algorithm[Schapire,
features are presented. Section 3 discusses efﬁcient subtree 1999] Boosting repeatedly calls a given weak learner to ﬁ-
mining methods. Section 4 shows experimental results and nally produce hypothesis f, which is a linear combination
Section 5 gives some conclusions and plans for future work. of K hypotheses produced by the prior weak learners, i,e.:
                                                                 K
                                                      f(x)=sgn(    k=1 αkh<tk,yk>(x))
                                                        A weak learner is built at each iteration k with differ-
2  Classiﬁer for Trees                                                         (k)     (k)    (k)
                                                      ent distributions or weights d =(di , ..., dL ) ,(where
Assume that each sentence in a question is parsed into a tree. N
                                                         (k)     (k)
A tree can be a sequence of word, a dependency tree, and a di =1,di  ≥ 0). The weights are calculated in such a
syntactic tree.                                       i=1
  The problem of question classiﬁcation is equivalent to clas- way that hard examples are focused on more than easier ex-
sifying a syntactic tree into a set of given categories. amples.
                                                                                L
2.1  Deﬁnition                                                                 
                                                                gain(<t,y>)=       yidih<t.y>(xi)
The tree classiﬁcation problem is to induce a mapping f(x):                    i=1
X  →{1,   2, ..., K}, from given training examples T =
        L
{xi,yi}i=1,wherexi ∈ X  is a labeled ordered tree and There exist many Boosting algorithm variants, how-
yi ∈{1, 2, ..., K} is a class label associated with each train- ever, the original and the best known algorithm is
ing data. The important characteristic is that the input exam- AdaBoost[Schapire, 1999]. For this reason we used the Ad-
ple xi is represented not as a numerical feature vector (bag-of- aboost algorithm with the decision stumps serving as weak
words) but a labeled ordered tree.                    functions.
  A labeled ordered tree is a tree where each node is asso-
ciated with a label and is ordered among its siblings, that is, 2.3 Maximum Entropy Model with subtree
there are a ﬁrst child, second child, third child, etc.    features
  Let t and u be labeled ordered trees. We say that t matches Maximum entropy models [Berger et al, 1996], do not
u,ort is a subtree of u (t ⊆ u ), if there exists a one-to-one make unnecessary independence assumptions. Therefore, we
function ψ from nodes in t to u, satisfying the conditions: are able to optimally integrate together whatever sources of
(1)ψ preserves the parent-daughter relation, (2)ψ preserves knowledge we believe to be potentially useful to the transfor-
the sibling relation, (3)ψ preserves the labels.      mation task within the maximum entropy model. The max-
  Let t and x be labeled ordered trees, and y be a class label imum entropy model will be able to exploit those features
(yi ∈{1, 2, ..., K}), a decision stump classiﬁer for trees is which are beneﬁcial and effectively ignore those that are ir-
given by:                                            relevant. We model the probability of a class c given a vector
                          1 t ⊆ x                     of features x according to the ME formulation:
            h<t,y>(x)=
                          0otherwise                                            n
                                                                            exp[  λifi(c, x)]
  Figure 1 shows an example of a labeled ordered tree and its                  i=0
                                                                   p(c|x)=                            (1)
subtree and non-subtree.                                                          Zx

                                                IJCAI-07
                                                  1696  Here Zx is the normalization constant, fi(c, x) is a fea- and di is a normalized weight assigned to xi.GivenT ﬁnd
ture function which maps each class and vector element to the optimal rule <t0,y0 > that maximizes the gain value.
a binary feature, n is the total number of features, and λi is The most naive and exhaustive method, in which we ﬁrst
a weight for a given feature function. These weights λi for enumerate all subtrees F and then calculate the gains for all
features fi(c, x) are estimated by using an optimization tech- subtrees, is usually impractical, since the number of subtrees
nique such as the L-BFGs algorithm [Liu and Nocedal, 1989]. is exponential to its size. The method to ﬁnd the optimal rule
  In the current version of maximum entropy model using can be modeled as a variant of the branch-and-bound algo-
subtrees, we deﬁne each feature function fi(c, x) using sub- rithm, and is summarized in the following strategies:
trees x and a class label c. The value of a feature function • Deﬁne a canonical search space in which a whole set of
is received as +1 if (c, x) is observed in the training data. subtrees of a set of trees can be enumerated.
Otherwise, it is received 0. Under the current framework of
                                                        •
maximum entropy model, all subtrees mined are incorporated Find the optimal rule by traversing this search space.
to the MEM as these feature functions mentioned above.  • Prune search space by proposing a criterion with respect
                                                          to the upper bound of the gain.
3  Efﬁcient subtree mining                              In order to deﬁne a canonical search space, we applied the
                                                                                              [         ]
Our goal is to use subtrees efﬁciently in the MEM or Boosting efﬁcient mining subtree method as described in Zaki, 2002 .
models. Since the number of subtrees within the corpus is After that, we used the branch-and-bound algorithm in which
large a method for generating all subtrees in the corpus seems we deﬁned the upper bound for each gain function in the pro-
intractable.                                          cess of adding a new subtree. To deﬁne a bound for each
  Fortunately, Zaki [Zaki, 2002] proposed an efﬁcient subtree, we based our calculation on the following theorem
                                                      [Morhishita, 2002]
method, rightmost-extension, to enumerate all subtrees from      ⊇       ∈{         }              
a given tree. First, the algorithm starts with a set of trees con- For any t t and y 1, 2, ..., K , the gain of <t,y >
sisting of single nodes, and then expands a given tree of size is bounded by μ(t)
(k .. 1) by attaching a new node to this tree to obtain trees                                          
                                                                            L                  L
of size k. However, it would be inefﬁcient to expand nodes
                                                      μ(t)=max  2         di −  yidi, 2       di +   yidi
at arbitrary positions of the tree, as duplicated enumeration is
                                                                 i|yi=+1,t∈xi i=1    i|yi=−1,t∈xi i=1
inevitable. The algorithm, rightmost extension, avoids such                                            (2)
duplicated enumerations by restricting the position of attach-
ment. To mine subtrees we used the parameters bellow:   Note that this equation is only used for binary classiﬁca-
  • minsup: the minimum frequency of a subtree in the data tion. To adapt it to the multi classiﬁcation problem one can
  •                                                   use some common strategies such as one-vs-all, one-vs-one,
    maxpt: the maximum depth of a subtree             and error correcting code to consider the multi classiﬁcation
  • minpt: the minimum depth of a subtree             problem as the combination of several binary classiﬁcations.
                                                      In this paper we focus on the use of one-vs-all for multi-class
  Table 1 shows an example of mining results.
                                                      problems. Hence the subtrees can be selected by using the
                                                      bound mentioned above along with binary classiﬁcation us-
        Table 1: Subtrees mined from the corpus       ing boosting.
          Frequency        Subtree                      We can efﬁciently prune the search space spanned by right
          4          (VP(VBZfeatures)(PP))            most extension using the upper bound of gain μ(t).Dur-
          7         (VP(VP(VBNconsidered)))           ing the traverse of the subtree lattice built by the recursive
          2          (VP(VP(VBNcredited)))            process of rightmost extension, we always maintain the tem-
          2           (VP(VP(VBNdealt)))              porally suboptimal gain δ among all gains calculated previ-
                                                                                                  
          5          (VP(VP(VBNinvented)))            ously. If μ(t) <delta, the gain of any super-tree t ∈ t is
                                                      no greater than delta , and therefore we can safely prune the
                                                      search space spanned from the subtree t. Otherwise, we can
  The subtree mining using the right most extension is used not prune this branch. The algorithm for selecting subtrees is
in both maximum entropy model and boosting model. The listed in Algorithm 1.
following subsection will describe how these subtrees can be Note that |t| means the length of a given subtree t. In addi-
used in MEM and Boosting.                             tion, Algorithm 1 is presented as in an algorithm for K classes
                                                      but in the current work we applied it to the binary class prob-
3.1  Subtree selection for Boosting                   lem.
The subtree selection algorithm for boosting is based on the After training the model using subtrees, we used the one-
decision stump function. We will eliminate a subtree if its vs-all strategy to obtain a label for a given question.
gain value is not appropriate for the boosting process. We
borrow the deﬁnition from [Kudo et al , 2004][Kudo et al, 3.2 Subtree selection for MEM
2005] as shown bellow:                                Count cut-off method
  Let T = {<x1,y1,d1 >, ..., < xL,yL,dL >} be training The subtree selection for maximum entropy model is con-
data, where xi is a tree and yi is a labeled associated with xi ducted based on a right most extension way. Each subtree is

                                                IJCAI-07
                                                  1697  Input: Let T = {<x1,y1,d1 >, ..., < xL,yL,dL >} be  experiments in previous work which was collected from four
  training data, where xi isatreeandyi is a labeled associated sources: 5,500 English questions published by USC, about
  with xi and di is a normalized weight assigned to xi. 500 manually constructed questions for a few rare classes,
                      0 0
  Output: Optimal rule <t ,y >                        TREC 8 and TREC 9 questions and also 500 questions from
  Begin                                               TREC 10. The standard data is used by almost previous
  Procedure project(t)                                work on question classiﬁcation [Zhang and Lee, 2003][Li and
  Begin                                               Roth, 2002]. The labels of each question is followed the two-
       ≤
  if μ(t) δ then                                      layered question taxonomy proposed by [Li and Roth, 2002],
     return 1
  end                                                 which contains 6 coarse grained category and 50 ﬁne grained
                                                     categories, as shown bellow. Each coarse grained category
  y =argmaxy∈{1,2,...,K} gain(<t,y>) if
           
  gain(<t,y >) >δthen                                 contains a non-overlapping set of ﬁne grained categories.
     <t0,y0 >=<t,y>                                     • ABBRS : abbreviation, expansion
     δ = gain(<t,y>)                                    • DESC : deﬁnition, description, manner, reason
  end
                                                       •
  for each t ∈{set of trees that are rightmost extension of t } do ENTY : animal, body, color, creation, currency, dis-
     s = single node added by right most extension        ease/medical, event, food, instrument, language, letter,
     if μ(t) ≤ δ continue                                 other, plant, product, religion, sport, substance, symbol,
            
     project(t );                                         technique, term, vehicle, word
  end                                                   • HUM: description, group, individual, title
  End (project(t)              
              L                                        •
  for t ∈ t|t ∈∪i=1 {t|t ⊆ xi} , |t| =1 do                LOC: city, country, mountain, other, state
            
     project(t );                                       • NUM: code, count, date, distance, money, order, other,
  end       
                                             percent, period, speed, temperature, size, weight
        0  0
  return t ,y                                           In order to obtain subtrees for our proposed methods, we
  End                                                 initially used the Chaniak parser [E. Charniak, 2001] to parse
          Algorithm 1: Find Optimal Rule              all sentences within the training and testing data. We obtained
                                                      a new training data consisting of a set of trees along with their
                                                      labels. We then mined subtrees using the right most extension
                                [         ]
mined using the algorithm described in Zaki, 2002 in which algorithm [Zaki, 2002]. Table 2 shows a running example
a frequency, a maximum length, and minimum length of a of using maximum entropy models with subtrees. To train
subtree is used to select subtrees. The frequency of a sub- our maximum entropy model we used the L-BFGS algorithm
tree can suitably be used for the count cut-off feature selec- [Liu and Nocedal, 1989] with the gaussian for smoothing.
tion method in maximum entropy models. Under the cur-
rent framework of maximum entropy model we used only the
simple feature selection method using count cut off method. Table 2: A running example of using maximum entropy
However, the results we gained in question classiﬁcation are model with subtrees
promising.
                                                       Features                                    Weight
Using boosting                                         ENTY (ADJP(ADVP))                          0.074701
In order to ﬁnd effective subtree features for maximum en- ENTY (ADJP(INof))                      0.433091
tropy model, we applied the subtree boosting selection as ENTY (ADJP(JJcelebrated))               0.084209
presented in the previous section. We obtained all subtree HUM (ADJP(JJcommon))                   0.003351
features after the running of the subtrees boosting algorithm HUM (ADJP(JJdead))                  0.117682
for performing on data set of one vs all strategy. In fact we ENTY:substance (NP(ADJP(RBSmost)(JJcommon))) 0.354677
obtained 50 data set for 50 class labels in the question classi- NUM:count (ADJP(PP(INfor)))      0.157310
ﬁcation data. These subtree features after applying the boost- NUM:count (NP(DTthe)(NNnumber))    0.810490
ing tree algorithm in each data set will be combined to our
set features for maximum entropy models. The advantage of To evaluate the proposed methods in question classiﬁca-
using the boosting subtrees with maximum entropy models tion, we used the evaluation method presented in [Li and
is that we can ﬁnd an optimization combination of subtree Roth, 2002][Zhang and Lee, 2003] as the formula below.
features under the maximum entropy principle using the ef-                 #of correct predictions
ﬁcient optimization algorithm such as the L-BFGS algorithm     accuracy =
[Liu and Nocedal, 1989]. In my opinion, the combination of                    #of predictions
subtree features using maximu entropy models might better We conducted the following experiments to conﬁrm our ad-
than the linear combination as the mechanism of Adaboost. vantage of using subtree mining for classiﬁcation with the
                                                      maximum entropy model and boosting model. The experi-
                                                      ments were done using signiﬁcant test with 95% conﬁdent
4  Experimental results                               interval. Table 3 shows the accuracy of our subtree maxi-
The experiments were conducted on a pentium IV at 4.3 GHz. mum entropy model (ST-Mem), subtree boosting model (ST-
We tested the proposed models on the standard data similarly Boost), the combination of subtree boosting and maximum

                                                IJCAI-07
                                                  1698entropy(ST-MB), and the tree kernel SVM (SVM-TK) for the
coarse grained category, respectively. It shows that the boost- Table 5: Question classiﬁcation accuracy using subtrees, un-
ing model achieved competitive results to that of SVM tree der the ﬁne grained category deﬁnition (total 50 labels).
kernel. The ST-MB outperforms the SVM-TK and achieve     Parameters         Subtree-boost Subtree-mem
the best accuracy.                                       maxpt=6 minsup=3       0.796         78.3
                                                         maxpt=4 minsup=2       0.826        80.50
                                                         maxpt=5 minsup=2       0.812        79.76
Table 3: Question classiﬁcation accuracy using subtrees, un-
der the coarse grained category deﬁnition (total 6 labels)
       ST-Mem    ST-Boost  ST-MB   SVM-TK             the boosting and maximum entropy model. So, the mathe-
       87.6        90.6     91.2      90.0            matical formulation for this relation is worth investigating.
                                                        The subtree mining approach to question classiﬁcation
Table 4: Question classiﬁcation accuracy using subtrees, un- relies on a parser to get the syntactic trees. However,
der the ﬁne grained category deﬁnition (total 50 labels). most parsers, including the Charniak’s parser we used in
     ST-Mem    ST-Boost  ST-MB    Mem    SVM          the above experiments, are not targeted to parse questions.
     80.5        82.6     83.6    77.6   80.2         Those parsers are usually trained on the Penn Treebank
                                                      corpus, which is composed of manually labeled newswire
  Table 4 shows the performance of subtree-mem, subtree- text. Therefore it is not surprising that those parsers can not
boost, and subtree-MB for question classiﬁcation. It also achieve a high accuracy in parsing questions, because there
shows the result of using word features for maximum entropy are only very few questions in the training corpus. In [Herm-
models (MEM) and support vector machine models (SVM). jakob, 2001], the accuracy of question parsing dramatically
The result of using SVM tree kernel is not reported in the improves when complementing the Penn Treebank corpus
original paper [Zhang and Lee, 2003]. The author did not [Marcus et al, 1993] with an additional 1153 labeled ques-
report the performance of SVM using tree kernel on the ﬁne tions for training. We believe that a better parser is beneﬁcial
grained category. Now on our paper we reported the perfor- to our approach.
mance of ﬁne grained category using subtrees under the max- Summarily, all experiments show that the subtree mined
imum entropy and the boosting model. We see that without from the corpus is very useful for the task of question classi-
using semantic features such as relation words we can obtain ﬁcation. It also indicated that subtrees can be used as feature
a better result in comparison with previous work. Table 4 in- functions in maximum entropy model and weak function as
dicates that the subtree-MB obtained the highest accuracy in in a boosting model. In addition, in the paper we give an al-
comparison with other methods since it utilizes both the ad- ternative method for using subtrees in question classiﬁcation.
vantage of subtree features selection using boosting and the As the results showed in our experiment, we can conclude
maximum entropy principle.                            that mining subtrees is signiﬁcantly contribution to the per-
  Since the subtree information and other semantic features formance of question classiﬁcation.
such as the named entity types and the relation words do not In the paper, we investigate the use of syntactic tree repre-
overlap, mixing these features together might improve the sentation which do not overlap the named entity types and the
performance of question classiﬁcation task.           relation words, so other tree structure representation with the
  The results also indicate that boosting model outperformed combination of syntactic and semantic information be bene-
maximum model using subtrees and the computational time ﬁcial to our approach. In future, we would like to investigate
of MEM is faster than that of boosting model.         the use of semantic parsing such as semantic role labelling to
  The training time of maximum entropy models is approx- the task.
imately 12 times faster than the boosting model. We needed
approximately 6 hours to ﬁnish all the training process of 5 Conclusions
boosting models for 5,500 training data examples. While us-
ing the maximum entropy model we needed only approxi- This paper proposes a method which allows incorporating
mately 0.5 hours. The computational times of combination subtrees mined from training data to the question classiﬁ-
subtree boosting features and maximum entropy models is cation problem. We formulate the question classiﬁcation as
comparable to the subtree boosting model.             the problem of classifying a tree into a preﬁxed categories.
  The testing using MEM is also approximately 5 times We then proposed the use of maximum entropy and booting
faster in comparison with that of boosting model. We guess model with subtrees as feature and weak functions by consid-
that the reason is the boosting model had to ﬁnd optimal rules ering subtree mining as subtree feature selection process.
during the training process. In addition, the larger number Experimental results show that our boosting model
of classes (50 labels) might affect to the computational time achieved a high accuracy in comparison to previous work
because of the strategy of one-vs-all method.         without using semantic features. The boosting model out-
  Table 5 depicted the results of boosting model and maxi- performed the maximum entropy model using subtrees but
mum model using subtrees in which the minsup and maxpt its computational times is slower more than 12 times in com-
mean the frequency and the maximum depth of a subtree, parison with that of the proposed maximum model. In ad-
respectively. This table reported that there is a relation be- dition, the combination of using boosting and maximum en-
tween the subtree mined parameters and the performance of tropy models with subtree features achieved substantially bet-

                                                IJCAI-07
                                                  1699