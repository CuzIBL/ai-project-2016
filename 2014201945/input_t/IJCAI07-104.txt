              Keep the Decision Tree and Estimate the Class Probabilities
                                  Using its Decision Boundary
        Isabelle Alvarez(1,2)             Stephan Bernard    (2)          Guillaume Deffuant    (2)
    (1) LIP6, Paris VI University         (2) Cemagref, LISC                (2) Cemagref, LISC
           4 place Jussieu              F-63172 Aubiere, France            63172 Aubiere, France
         75005 Paris, France         stephan.bernard@cemagref.fr      guillaume.deffuant@cemagref.fr
       isabelle.alvarez@lip6.fr

                    Abstract                          (except smoothing) induce a drastic change in the fundamen-
                                                      tal properties of the tree: Either the structure of the tree as a
    This paper proposes a new method to estimate the  model is modiﬁed, or its main objective, or its intelligibility.
    class membership probability of the cases classiﬁed The method we propose here aims at improving the class
    by a Decision Tree. This method provides smooth   probability estimate without modifying the tree itself, in or-
    class probabilities estimate, without any modiﬁca- der to preserve its intelligibility and other use. Besides the
    tion of the tree, when the data are numerical. It ap- attributes of the cases, we consider a new feature, the dis-
    plies a posteriori and doesn’t use additional train- tance from the decision boundary induced by the DT (the
    ing cases. It relies on the distance to the deci- boundary of the inverse image of the different class labels).
    sion boundary induced by the decision tree. The   We propose to use this new feature (which can be seen as
    distance is computed on the training sample. It   the margin of the DT) to estimate the posterior probabilities,
    is then used as an input for a very simple one-   as we expect the class membership probability to be closely
    dimension kernel-based density estimator, which   related to the distance from the decision boundary. It is the
    provides an estimate of the class membership prob- case for other geometric methods, like Support Vector Ma-
    ability. This geometric method gives good results chines (SVM). A SVM  deﬁnes a unique hyperplane in the
    even with pruned trees, so the intelligibility of the feature space to classify the data (in the original input space
    tree is fully preserved.                          the corresponding decision boundary can be very complex).
                                                      The distance from this hyperplane can be used to estimate the
1  Introduction                                       posterior probabilities, see [Platt, 2000] for the details in the
Decision Tree (DT) algorithms are very popular and widely two-class problem. In the case of DT, the decision boundary
used for classiﬁcation purpose, since they provide relatively consists in several pieces of hyperplanes instead of a unique
easily an intelligible model of the data, contrary to other hyperplane. We propose to compute the distance to this deci-
learning methods. Intelligibility is a very desirable property sion boundary for the training cases. Adapting an idea from
                                                      [               ]
in artiﬁcial intelligence, considering the interactions with the Smyth et al., 1995 , we then train a kernel-based density es-
end-user, all the more when the end-user is an expert. On the timator (KDE), not on the attributes of the cases but on this
other hand, the end-user of a classiﬁcation system needs addi- single new feature.
tional information rather than just the output class, in order to The paper is organized as follows: Section 2 discusses re-
asses the result: This information consists generally in con- lated work on probability estimate for DT. Section 3 presents
fusion matrix, accuracy, speciﬁc error rates (like speciﬁcity, in detail the distance-based estimate of the posterior proba-
sensitivity, likelihood ratios, including costs, which are com- bilities. Section 4 reports the experiment performed on the
monly used in diagnosis applications). In the context of de- numerical databases of the UCI repository, the comparison
cision aid system, the most valuable information is the class between the distance-based method and smoothing methods.
membership probability. Unfortunately, DT can only provide Section 5 discusses the use of geometrically deﬁned subsets
piecewise constant estimates of the class posterior probabil- of the training set in order to enhance the probability estimate.
ities, since all the cases classiﬁed by a leaf share the same We make further comments about the use of the distance in
posterior probabilities. Moreover, as a consequence of their the concluding section.
main objective, which is to separate the different classes, the
raw estimate at the leaf is highly biased. On the contrary, 2 Estimating Class Probabilities with a DT
methods that are highly suitable for probability estimate pro- Decision Trees (DT) posterior probabilities are piecewise
duce generally less intelligible models. A lot of work aims at constant over the leaves. They are also inaccurate. Thus they
improving the class probability estimate at the leaf: Smooth- are of limited use (for ranking examples, or to evaluate the
ing methods, specialized trees, combined methods (decision risk of the decision). This is the reason why a lot of work has
tree combined with other algorithms), fuzzy methods, ensem- been done to improve the accuracy of the posterior probabili-
ble methods (see section 2). Actually, most of these methods ties and to build better trees in this concern.

                                                IJCAI-07
                                                   654  Traditional methods consist in smoothing the raw condi- By convention, in a two-class problem, we choose one
                                r( | )=  k                                         Γ
tional probability estimate at the leaf p c x n , where k class (the positive class) to orient : If a case stands in the
is the number of training cases of the class label classiﬁed by positive class area, its algebraic distance will be negative.
the leaf, and n is the total number of training cases classiﬁed Deﬁnition 1 Algebraic distance to the DT (2-class problem)
by the leaf. The main smoothing method is the Laplace cor-
       L
rection p (and its variants like m-correction). The correction The algebraic distance of x is h(x)=−d(x, Γ) if c(x) is
 L( | )=  k+1
p  c x   n+C , where C is the number of classes, shifts the the positive class c and h(x)=+d(x, Γ) otherwise.
probability toward the prior probability of the class ([Cest-
                                                        This deﬁnition extends easily to multi-class problems. For
nik, 1990; Zadrozny and Elkan, 2001]). This improves the
                                                      each class c, we consider Γ , the decision boundary induced
accuracy of the posterior probabilities and keep the tree struc-             c
                                                      by the tree for class c: Γ is the inverse image of the class c
ture unchanged, but the probabilities are still constant over the          c
                                                      area. (We have Γ ⊂ Γ).
leaves. In order to bypass this problem, smoothing methods          c
can be used on unpruned trees. The great number of leaves Deﬁnition 2 Class-Algebraic distance to the DT
                                                                                       ( )=−     (  Γ )
allows the tree to learn the posterior probability with more The c-algebraic distance of x is hc x d x, c if
                                                       ( )=         ( )=+   (  Γ )
accuracy (see [Provost and Domingos, 2003]). The intelligi- c x c and hc x d x, c otherwise.
bility of the model is most reduced in this case, so specialized The c-algebraic distance measures the distance of a case to
building and pruning methods are developped for PET’s (see class c area. Actually, algebraic distance is a particular case
for instance [Ferri and Hernandez, 2003]).            of c-algebraic distance where c is the positive class.
  In order to produce smooth and case-variable estimate of These deﬁnitions apply to any decision tree. But in the
the posterior probabilities, Kohavi [1996] deploys a Naive case of axis-parallel DT (ADT) operating on numerical data,
Bayes Classiﬁer (NBC) at each leaf, using a specialized in- a very simple algorithm computes the algebraic distance
duction algorithm. Thus the partition of the space in NBTree (adapted from the distance algorithm in [Alvarez, 2004]). It
is different from classical partition, since its objective is to consists in projecting the case x onto the set F of leaves f
better verify the conditional independance assumption at each whose class label differs from c(x) (in a two class problem).
leaf. In the same idea, Zang and Su [2004] use NBC to eval- In a multi-class problem, each class c is considered in turn.
uate the choice of the attribute at each step. But the structure When c(x)=c, the set F contains the leaves whose labels
of the tree is essentialy different.                  differs from c; Otherwise it contains the leaves whose class
  Other methods obtain case-variable estimates of the poste- label is c. The nearest projection gives the distance.
rior probabilities by propagating a case through several paths Algorithm 1 AlgebraicDistance(x,DT,c)
                                      [
at each node (mainly fuzzy methods like in Umano et al., 0. d = ∞;
    ]     [            ]                  [
1994 or in Quinlan, 1993 ; Or, more recently, Ling and 1. Gather the set F of leaves f whose class c(f) veriﬁes:
        ]
Yan, 2003 ). These methods aim at managing the uncertainty  (c(f) Xor c(x)) = c;
both in the input and in the training database.       2. For each f ∈ F do: {
  Smyth, Gray and Fayyad [1995] propose to keep the struc-
                                                      3.   compute pf (x)=projectionOntoLeaf(x,f);
ture of the tree and to use a kernel-based estimate at each leaf. 4. compute d (x)=d(x, p (x));
They consider all the training examples but use only the at-        f           f
                                                      5.   if (df (x) <d) then d = df (x) }
tributes involved in the path of the leaf. The dimension of 6. Return h (x)=−sign(c(x)=c) ∗ d
the subspace is then at most the length of the path. But the    c
                                                                                       =(   )
resulting dimension is nevertheless far too high for this kind Algorithm 2 projectionOntoLeaf(x,f Ti i∈I )
of technique and the method cannot be used in practice. 1. y = x;
                                                                            {
  We propose here to reduce the dimension to 1, since KDE 2.Fori=1tosize(I) do:
                                                                                              }
is very effective in very low dimensions. We preserve the 3. if y doesn’t verify the test Ti then yu = b
structure of tree, and our method theoretically applies as soon where Ti involves attribute u with threshold value b
as it is possible to deﬁne a metric on the input space. 4. Return y
                                                      The projection onto a leaf is straightforward in the case of
3  Distance-based Probability Estimate for DT         ADT since the area classiﬁed by a leaf f is a hyper-rectangle
                                                      deﬁned by its tests. The complexity is in O(Nn) in the worst
3.1  Algebraic Distance as a New Feature              case where N is the number of tests of the tree and n the
We consider an axis-parallel DT (ADT) operating on numer- number of different attributes of the tree.
ical data: Each test of the tree involves a unique attribute. The distance to the decision boundary presents two main
We note Γ the decision boundary induced by the tree. Γ con- advantages, because it is a continuous function: it is relatively
sists of several pieces of hyperplanes which are normal to the robust to the uncertainty of the value of the attributes for new
axes. We also assume that it is possible to deﬁne a metric on cases, and to the noise in the training data, assuming that only
the input space, possibly with a cost or utility function. the thresholds of the tests are modiﬁed (not the attributes).
  Let x be a case, c(x) the class label assigned to x by the
tree, d = d(x, Γ) the distance of x from the decision bound- 3.2 Kernel Density Estimate (KDE) on the
ary Γ. The decision boundary Γ divides the input space into Algebraic Distance
different areas (possibly not connected areas) which are la- Kernel-based density estimation is a non-parametric tech-
beled with the name of the class assigned by the tree. nique widely used for density estimation. It is constantly im-

                                                IJCAI-07
                                                   655proved by researches on algorithms, on variable bandwidth
and bandwidth selection (see [Venables and Ripley, 2002] for
references).
  Univariate KDE basically sums the contribution of each
training case to the density via the kernel function K.To
estimate the density f(x) given a sample M = {x } ,
                                        i i∈[1,n]
            ˆ( )=   1  n   1   x−xi
one computes f x    n  i=1 b K   b  , where K is the
kernel function and b the bandwidth (b can vary over M).
  Many methods could be used in this framework to compute
the distance-based kernel probability estimate of the class
membership. (We could also use kernel regression estimate).
We have used very basic KDE for simplicity.
  We consider here the algebraic distance h(x) as the at-
tribute on which the KDE is performed. So we compute the
density estimate of the distribution of the algebraic distance,
from the set of observations h(x),x∈ S:
                                       
                   1         ( ) −  ( )
         ˆ                   h x   h xi
        f(h(x)) =        K                      (1)
                  nb              b                   Figure 1: Variation of Laplace and GDK probability estimate over
                     xi∈S                             a test sample from the wdbc database. DT errors are highlighted
  In order to estimate the conditional probabilities, we con-
sider the set S of training cases and its subset Sc of cases such 4 Experimental Study
that c(x)=c. We estimate the density of the two populations:
  ˆ
  f the density estimate of the distribution of the algebraic 4.1 Design and Remarks
distance to the decision boundary;                    We have studied the distance-based kernel probability esti-
  ˆ                                                   mate on the databases of the UCI repository [Blake and Merz,
  fc, the density estimate of the distribution of the algebraic
distance of points of class c.                        1998] that have numerical attributes only and no missing val-
  ˆ                    ˆ                              ues. For simplicity we have treated here multi-class problems
  f is computed on S and f on S . We then derive from the
                       c    c                         as 2-class problems. We generally chose as positive class the
Bayes rule, if pˆ(c) estimates the prior probability of class c:
                                                      class (or a group of class) with the lowest frequency in the
                        ˆ ( ( )) ∗ ˆ( )               database.
             ˆ( | ( )) = fc h x   p c
             p c h x        ˆ                   (2)     For each database, we divided 100 bootstrap samples into
                           f (h(x))
                                                      separate training and test sets in the proportion 2/3 1/3, re-
Deﬁnition 3 Distance-based kernel probability estimate specting the prior of the classes (estimated by their frequency
  pˆ(c|h(x)) is called the distance-based kernel (DK) proba- in the total database). It is certainly not the best way to build
bility estimate                                       accurate trees for unbalanced datasets or different error costs,
  The algorithm is straightforward. We note S the training but here we are not interested in building the most accurate
set used to build the decision tree DT.               trees, we just want to study the distance-based kernel proba-
                                                      bility estimate. For the same reason we grow trees with the
Algorithm 3 DistanceBasedProbEst(x,DT,c,S)
                                                      default options of j48 (Weka’s [Witten and Frank, 2000] im-
1. Compute the algebraic distance
                                                      plementation of C4.5) although in many cases different op-
    =   ( )=                   (      );
   y   h x   AlgebraicDistance x, DT, c               tions would build better trees. For unpruned trees we disabled
2. Compute the subset ( ) of from which the probability
                   S x    S                           the collapsing function. We used Laplace correction smooth-
density is estimated: Default value is ( )= ;
                               S x    S               ing method to correct the raw probability estimate at the leaf
3. Select S (x)={x ∈ S(x),c(x)=c};
         c                                          for pruned trees and unpruned trees. We also built NBTree on
          ˆ( )=  1              y−h(xi) ;
4. Compute f y       xi∈S(x) K                        the same samples.
                 nb             b   
          ˆ       1            y−h(xi)                  We used two different metrics in order to compute the dis-
5. Compute f (y)=          K           ;
           c      nb  xi∈Sc      b                    tance from the decision boundary, the Min-Max (MM) metric
6. Compute and Return pˆ(c|h(x)) from equation (2)    and the standard (s) metric. Both metrics are deﬁned with the
  Several possibilities can be considered for the set S(x) basic information available on the data: An estimate of the
used to compute the kernel density estimate, we discuss them range of each attribute i or an estimate of its mean Ei and
in section 5. The simplest method consists in using the whole of its standard deviation si. The new coordinate system is
sample S. The algebraic distance is taken into account glob- deﬁned by (3).
ally, without any other consideration concerning the location          −                      −
                                                            MM  =    xi  Mini           s = xi  Ei
of the cases. We call the corresponding conditional probabil- yi                  or   yi             (3)
                                                                   Maxi  − Mini               si
ity estimate pˆg the global distance-based kernel (GDK) prob-
ability estimate. Figure 1 shows how the GDK probability es- The parameters of the standard metric are estimated on each
timate varies over a test sample, compared with the Laplace training sample. (On each bootstrap sample for the Min-Max
probability estimate which is piecewise constant.     metric, to avoid multiple computation when an attribute is

                                                IJCAI-07
                                                   656outside the range). In practice, the choice of the metric should   Both      Both   Normal vs   DK vs.
be guided by the data source. (For instance, the accuracy of Dataset Red.-Error Normal Unpruned NBTree
most sensors is a function of the range, therefore it suggests
                                                         bupa   0.26±0.77  0.39±0.84 0.03±0.81 0.91±0.93
to use an adapted version of the Min-Max coordinate system.) glass 1.89±0.85 -0.0±0.7 -1.25±0.67 -1.02±0.78
  To compute the kernel density estimate of the algebraic dis- iono. 0.63±0.67 -2.2±0.53 -2.56±0.55 -3.02±0.64
tance we choose simplicity and used the standard algorithm iris 4.70±0.52  3.85±0.46 3.90±0.48 1.99±0.47
available in R [Venables and Ripley, 2002] with default op- thyroid 4.35±0.71 2.98±0.75 2.48±0.75 -0.57±0.75
tions, although the setting of KDE parameters (choice of the pendig. 0.46±0.05 0.40±0.04 0.28±0.03 0.65±0.05
kernel, optimal bandwidth, etc.) or specialized algorithms pima 2.78±0.58  0.31±0.41 0.41±0.44 0.69±0.54
(also available in some dedicated packages) would certainly sat 1.00±0.12  0.89±0.08 0.28±0.06 1.18±0.09
                                                                   ±          ±        ±         ±
give better results. We systematically used the default option segment. 7.95 1.06 5.35 0.86 5.34 0.89 3.77 0.71
                                                         sonar  2.66±0.95  2.07±0.78 1.88±0.80 -2.53±1.03
except for the bandwidth selection, since it is inappropriate      ±          ±        ±         ±
for using the Bayes rule in equation (2): We used the same vehicle 0.42 0.26 0.65 0.20 0.08 0.20 1.97 0.29
            ˆ      ˆ                                     vowel  4.19±0.52  3.09±0.30 2.79±0.28 2.13±0.32
bandwidth for f as for fc. We set it to a fraction τ of the range wdbc 3.75±0.28 2.24±0.23 2.17±0.23 2.20±0.21
of the algebraic distance . This is the only parameter we used wine 5.35±0.51 3.31±0.36 2.97±0.34 0.25±0.23
to control the KDE algorithm. More sophisticated methods
could obviously be used but the Bayes rule in equation (2) Table 1: Mean difference of the AUC obtained with global DK
should be reformulated if variable bandwidths are used. probability estimate (standard metric, τ =10%) and with Laplace
                                                      correction. GDK on pruned tree versus Laplace on pruned or un-
4.2  Results for the Global Distance-based Kernel     pruned tree. (Mean values and standard deviations are ×100. In-
     Probability Estimate                             signiﬁcant values are italic. Bad results are bold)
In order to compare our method with other methods of prob-
ability estimate, we used the AUC, the Area Under the Re- the AUC remains better. This is easily comprehensible: when
ceiving Operator Characteristic (ROC) Curve (see [Bradley, τ increases, the kernel estimate tends to erase sharp varia-
1997]). The AUC is widely use to assess the ranking ability tions. As a consequence, probability estimate cannot reach
of the tree. Although this method cannot distinguish between the extremes (0 or 1) easily. The Log loss metric is not shown
good scores and scores that are good probability estimates, it since it gives useless results (almost always inﬁnite).
doesn’t make any hypothesis on the true posterior probability,
so it is useful when the true posterior probability are unknown 5 Local estimate: Partition of the Space
(since it is clear that good probability estimates should pro-
duce on average better AUC). We also used Mean Squared In order to get more local estimate, the ﬁrst idea would be to
                                                      use the leaves to reﬁne the deﬁnition of the sets S(x) used to
Error and log-loss, although these methods make strong hy-                                            ( )
pothesis on the true posterior probability.           run the kernel density estimate (step 2 of algorithm 3). S x
  Table 1 shows the difference of the AUC from global would be simply the leaf that classiﬁes x. However, we’ll ar-
distance-based kernel (GDK) probability estimate and the gue on a simple example that this option shows severe draw-
Laplace correction. Apart from a few cases, it gives better backs, and that a deﬁnition based on the geometry of the DT
values than Laplace correction (within a 95% conﬁdence in- boundary should generally be preferred.
terval). From the intelligibility viewpoint, it is interresting to
note that GDK probability estimate on pruned tree is gener-           GD vs. Laplace  GD vs. NBTree
ally better than smoothing method on unpruned tree (which is Dataset  MSE    p-value   MSE   p-value
better than smoothing method on pruned tree). We also per-             ±                ±
formed a Wilcoxon unlateral paired-wise test on each batch of bupa -7.29 0.59 3.9e-16 -1.68 0.88 0.006
                                                            glass   0.79±3.56 0.017 4.19±3.52  0.114
100 samples. The tests conﬁrm exactly the signiﬁcant results           ±                ±
                                0 01                        iono.   1.88 0.27 2.2e-09 1.61 0.77 0.023
from Table 1 (with p-values always < . ).                   iris   -1.54±0.37 5.3e-4 -0.84±0.65 0.088
  The choice of the metric has a limited effect on the result thyroid 1.49±0.71 0.092 6.04±0.82 1.3e-10
in term of AUC. The difference is always of a different order pendig. -0.08±0.01 1.9e-08 -0.19±0.03 2.0e-07
except for vehicle, pima and especially glass for which it can pima -5.04±0.30 3.2e-18 -1.39±0.53 0.008
reach 10−2.                                                 sat    -0.07±0.03 0.024 -0.20±0.05 5.0e-4
  We used several bandwidths, with τ from 5 to 15% of the   segment. 0.68±0.39 0.017 -1.46±0.76 0.071
range of the algebraic distance: Results vary very smoothly sonar  -3.29±0.51 5.8e-09 0.34±1.52 0.491
                                                            vehicle 1.99±0.50 5.7e-04 1.63±0.56 0.007
with the bandwidth, Table 1 would present completely similar           ±                ±
results.                                                    vowel  -0.12 0.12  0.26 0.69 0.19 1.1e-4
                                                            wdbc   -2.63±0.22 3.8e-17 -2.00±0.40 4.6e-06
  Table 2 shows the comparison using Mean Squared-Error     wine   -1.72±0.31 6.2e-08 1.79±0.56 8.3e-4
(MSE) as a metric to measure the quality of the probability
                                 ( )=     ( ( | ) −
estimate. The error at test point x is se x c p c x
      2                                               Table 2: Mean MSE difference (and p-value of the corresponding
pˆ(x|c)) , where p(c|x) is the true conditional probability unilateral Wilcoxon paired test) between GDK probability estimate
(here p(c|x) is set to 1 when c(x)=c). se(x) is then summed (standard metric, τ =5%) and either Laplace correction (normal
over each test sample. The performance of the GDK proba- pruned tree) or NBTree. (Values are ×100 except p-values. In-
bility estimate diminishes quickly when τ increases, although signiﬁcant values are italic. Bad results for GDK are bold)

                                                IJCAI-07
                                                   6575.1  DT Boundary-based partition
The rationale behind our partition is to consider the groups of
points for which the distance is deﬁned by the same normal
to the decision surface, because these points share the same
axis deﬁning their distance, and they relate to the same linear
piece of the decision surface.
  Let {H1,H2, .., Hk} be the separators used in the nodes of
the tree, and S the learning sample. To partition the total sam-
ple S, we associate with each separator Hi a subset Si of S,
which comprises the examples x such that the projection of x
onto the decision boundary Γ belongs to Hi. Generically, the
projection p(x) of x onto Γ is unique, but p(x) can belong to
several separators. In that case we associate to x the separator
Hi deﬁning p(x) with the largest margin. We deﬁne S(x) as
Si. The KDE is then run on the h(xj),xj ∈ Si, as explained
previously.
5.2  Advantage of our partition
We now illustrate on a simple example the advantage of our
partition compared with the partition generated by the leaves. Figure 2: Top: Partition deﬁned by the three leaves of the
Suppose that the learning examples are uniformly distributed tree. Bottom: Partition into 2 subsets (light gray and gray),
in a square in a space x, y, and that the probability of class 1 deﬁned by the separators.
depends only on axis x, with the following pattern (Figure 2):
  • For x growing from −0.5 to 0, the probability of class 1
    grows smoothly from 0.1 to 0.9, with a sigmoid function
    centered on −0.25.                                  Table 3 shows the results of the comparison between local
  • For x growing from 0 to 0.5, the probability of class 1 distance-based kernel estimate and smoothing method at the
    decreases sharply from 1 to 0 around x =0.25.     leaf for some databases. Because of the partition of the space,
                                                      the results from the AUC comparison and MSE are not nec-
  A typical decision tree obtained from a sample generated essarily correlated, and MSE is globally better than for the
with this distribution is represented on ﬁgure 2 (top). It has global DK probability estimate.
three leaves, two yield class -1 and are deﬁned by a single
separation, and one yields +1 and is deﬁned by both separa-
tions (the gray area on the ﬁgure). The partition deﬁned by
the leaves thus generates 3 subsets.
  Figure 2 (bottom) shows the partition obtained with our
method. It includes only two subset, one for each separation.
Figure 3 shows the results obtained with a kernel estimation
(a simple Parzen window of width 0.1) on the leaf partition,
and on our partition, compared with the actual probability
function which was used to generate the sample. One can
notice that our partition gives a result which is very close to
the one that would be obtained with a kernel estimation on
the x axis. This is not the case for the leaf partition, which
introduces strong deviations from the original probability.
  The main explanation of these deviations is a side effect
which artiﬁcially increases the probability of 1 around the
border, in the case of the leaf partition. Actually, this is the
same bias as the one obtained when estimating directly the
probabilities on the leaves. Moreover, the leaf partition intro-
duces an artiﬁcial symmetry on both sides of the leaf, because
of the averaging on all the examples at similar distance of the
boundary. These problems are not met with our partition. We
claim that the problems illustrated in this example have good
chances to occur very frequently with the leaf partition. In
particular, the side effect due to the border of the leaf will al- Figure 3: Results with a kernel estimator of width 0.1 (Parzen
ways induce a distortion, which is avoided with our partition. window). Top: With the leaf partition. Bottom: With the
Artiﬁcial symmetries will always be introduced when a leaf separator partition. (In black the true probability, in gray the
includes several separators.                          estimate).

                                                IJCAI-07
                                                   658