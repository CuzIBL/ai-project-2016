          An Inference Model for Semantic Entailment in Natural Language
                  Rodrigo de Salvo Braz        Roxana Girju        Vasin Punyakanok
                                   Dan Roth          Mark Sammons
                                    Department of Computer Science
                               University of Illinois at Urbana-Champaign
                                        Urbana, IL, 61801, USA
              {braz, girju, punyakan, danr,                mssammon}@cs.uiuc.edu

  Semantic entailment is the problem of determining if the mean- Just resorting to general purpose knowledge representations
ing of a given sentence entails that of another. This is a fundamen- – FOL based representations, probabilistic representations or
tal problem in natural language understanding that provides a broad hybrids – along with their corresponding general purpose in-
framework for studying language variability and has a large number ference algorithms is not sufﬁcient.
of applications. We present a principled approach to this problem We have developed an integrated approach that provides
that builds on inducing re-representations of text snippets into a hi- solutions to all challenges mentioned above. We formally
erarchical knowledge representation along with a sound inferential deﬁne the problem of semantic entailment for Natural Lan-
mechanism that makes use of it to prove semantic entailment. guage and present a computational approach to solving it, that
                                                      consists of a hierarchical knowledge representation language
1  Introduction                                       into which we induce appropriate representations of the given
Semantic entailment is the task of determining, for example, text and required background knowledge, along with a sound
that the sentence: “WalMart defended itself in court today inferential mechanism that makes use of the induced repre-
against claims that its female employees were kept out of jobs sentation to determine entailment. Our inference approach is
in management because they are women” entails that “Wal- formalized as an optimization algorithm that we model as an
Mart was sued for sexual discrimination”.             integer linear programming problem. The preliminary evalu-
  Determining whether the meaning of a given text snippet ation of our approach is very encouraging and illustrates the
entails that of another or whether they have the same meaning signiﬁcance of some of the key contributions of this approach.
is a fundamental problem in natural language understanding 1.1 General Description of Our Approach
that requires the ability to abstract over the inherent syntac-
tic and semantic variability in natural language [Dagan and Given two text snippets S (source) and T (target) (typically,
Glickman, 2004]. This challenge is at the heart of many high but not necessarily, S consists of a short paragraph and T ,
level natural language processing tasks including Question a sentence) we want to determine if S|=T , which we read
Answering, Information Retrieval and Extraction, Machine as “S entails T ” and, informally, understand to mean that
Translation and others that attempt to reason about and cap- most people would agree that the meaning of S implies that
ture the meaning of linguistic expressions.           of T . Somewhat more formally, we say that S entails T
  Research in natural language processing in the last few when some representation of T can be “matched” (modulo
years has concentrated on developing resources that provide some meaning-preserving transformations to be deﬁned be-
multiple levels of text analysis (both syntactic and semantic), low) with some (or part of a) representation of S, at some
resolve various context sensitive ambiguities, and identify ab- level of granularity and abstraction.
stractions (from syntactic categories like POS tags to semantic The approach consists of the following components:
ones like named entities), and the text relational structures. KR: A Description Logic based hierarchical knowledge
  Indeed, several decades of research in natural language representation, EFDL, into which we re-represent the surface
processing and related ﬁelds have made clear that the use of level text representations, augmented with induced syntactic
deep structural, relational and semantic properties of text is a and semantic parses and word and phrase level abstractions.
necessary step towards supporting higher level tasks. How- KB: A knowledge base consisting of syntactic and seman-
ever, beyond these resources, in order to support fundamen- tic rewrite rules, written in EFDL.
tal tasks such as inferring semantic entailment between two Subsumption: An extended subsumption algorithm which
texts snippets, there needs to be a uniﬁed knowledge repre- determines subsumption between EFDL expressions (repre-
sentation of the text that (1) provides a hierarchical encod- senting text snippets or rewrite rules). “Extended” here means
ing of the structural, relational and semantic properties of the that the basic uniﬁcation operator is extended to support sev-
given text, (2) is integrated with learning mechanisms that eral word level and phrase level abstractions.
can be used to induce such information from raw text, and (3) First a set of machine learning based resources are used to
is equipped with an inferential mechanism that can be used induce the representation for S and T . The entailment algo-
to support inferences with respect to such representations. rithm then proceeds in two phases: (1) it incrementally gen-  S: Lung cancer put an end to the life of Jazz singer Marion Montgomery on Monday. S1’: Lung cancer killed Jazz singer Marion Montgomery on Monday.  

  S2’: Jazz singer Marion Montgomery died of lung cancer on Monday.       T: Singer dies of carcinoma. 

                                                                    H0
                                                                                      PHTYPE: VP     H0
                                  PHTYPE: VP      ARGM-TMP
                         ARG1                                             SRLTYPE: ARG0
                                N24                         SRLTYPE: ARGM-TMP      N’12
        SRLTYPE: ARG1                                                              N’2

                                                                           N’11
             N22                                            N23

                                  PHTYPE: VP                                          PHTYPE: VP
                                                                                   N’10       PHTYPE: PP
                                N18                                                N’2
                                                             PHTYPE: PP                     N’9
       PHTYPE: NP                         PHTYPE: PP                       N’8
                                                                             PHTYPE: NP
             N19                        N20                 N21              NETYPE: PROF.

                                                                                     PHTYPE: PP      H1
                                                                           N’6             N’7 PHTYPE: NP
                                          PHTYPE: PP
      PHTYPE: NP                                              PHTYPE: PP H1                    NETYPE: DISEASE
                                                  PHTYPE: NP
            N15
                                        N16                 N17 PHTYPE: NP  PHTYPE: NP
                                                  NETYPE: DISEASE                   ID    ID     N’5
                 ID                                             NETYPE: TIME NETYPE: PROF.
                                                 N13                      ID
                               ID                               N14
                      PHTYPE: NP          ID
          N11 PHTYPE: NP N12                                                 BEFORE BEFORE BEFORE     H2
                      NETYPE: PERSON
            NETYPE: PROF.                            ID    ID      ID      N’1    N’2    N’3     N’4

                       ID
     BEFORE BEFORE BEFORE BEFORE                 BEFORE              H2
                                   BEFORE BEFORE       BEFORE BEFORE      WORD: singer WORD: dies WORD: of WORD: carcinoma
         N2     N3     N4               N6      N7                        LEMMA: singer LEMMA: die POS: IN
   N1                           N5                   N8     N9     N10                         LEMMA: carcinoma
                                                                          POS: NN PHHEAD: VP   POS: NN
                                                                          PHHEAD: NP           PHHEAD: NP
  WORD: Jazz WORD: singer WORD: Marion WORD: Montgomery WORD: died WORD: of WORD: lung WORD: cancer WORD: on WORD: Monday
  LEMMA: Jazz LEMMA: singer POS: NN POS: NN LEMMA: die POS: IN LEMMA: lung LEMMA: cancer POS: IN POS: NNP
  POS: NN POS: NN    PHHEAD: NP PHHEAD: VP    POS: NN POS: NN     PHHEAD: NP
                                                    PHHEAD: NP

Figure 1: Example of Re-represented Source & Target pairs as concept graphs. The original source sentence S generated several alternatives
        ′                         ′
including S1 and the sentence in the ﬁgure (S2). Our algorithm was not able to determine entailment of the ﬁrst alternative (as it fails to match
                                            ′                                          ′
in the extended subsumption phase), but it succeeded for S2. The dotted nodes represent phrase level abstractions. S2 is generated in the ﬁrst
phase by applying the following chain of inference rules: #1 (genitives): “Z’s W → W of Z”; #2: “X put end to Y’s life → Y die of X”. In
the extended subsumption, the system makes use of WordNet hypernymy relation (“lung cancer” IS-A “carcinoma”) and NP-subsumption
rule (“Jazz singer Marion Montgomery’” IS-A “singer”). The rectangles encode the hierarchical levels (H0, H1, H2) at which we applied the
extended subsumption. Also note that in the current experiments we don’t consider noun plurals and verb tenses.

erates re-representations of the original representation of the line is a lexical-level matching based on bag-of-words repre-
source text S and (2) it makes use of an (extended) subsump- sentation with lemmatization and normalization (LLM).
tion algorithm to check whether any of the alternative repre-
sentations of the source entails the representation of the target Perform. Overall  Task [%]
T                                                                [%]   CD   IE   IR   MT   PP   QA   RC
 . The subsumption algorithm mentioned above is used in System  64.8   74.0 35.0 62.0 87.5 63.8 84.0 49.0
both phases in slightly different ways.                LLM      54.7   64.0 50.0 50.0 75.0 55.2 50.0 52.9
  Figure 1 provides a graphical example of the representa-
tion of two text snippets, along with an sketch of the extended Table 1: System’s performance obtained for each experiment on the
subsumption approach to decide the entailment.        Pascal corpora and its subtasks.
  Along with the formal deﬁnition developed here of seman-
tic entailment, our knowledge representation and algorithmic
approach provide a novel solution that addresses some of the Acknowledgement
key issues the natural language research community needs to We thank Dash Optimization for the free academic use of
address in order to move forward towards higher level tasks of their Xpress-MP software. This work was supported by
this sort. Namely, we provide ways to represent knowledge, the Advanced Research and Development Activity (ARDA)’s
either external or induced, at multiple levels of abstractions Advanced Question Answering for Intelligence (AQUAINT)
and granularity, and reason with it at the appropriate level. Program, NSF grant ITR-IIS-0085980, and ONR’s TRECC
                                                      and NCASSR programs.
2  Experimental Evaluation
Data. We tested our approach on the PASCAL challenge data References
set (http://www.pascal-network.org/Challenges/RTE/). As the sys- [Dagan and Glickman, 2004] I. Dagan and O. Glickman.
tem was designed to test for semantic entailment, the PAS- Probabilistic textual entailment: Generic applied model-
CAL data set is ideally suited, being composed of 276 source ing of language variability. In Learning Methods for Text
- target sentence pairs, indicating whether the source logi- Understanding and Mining, Grenoble, France, 2004.
cally entails the target. The set is split into various tasks: CD
(Comparable Documents), IE (Information Extraction), MT
(Machine Translation), PP (Prepositional Paraphrases), QA
(Question Answering), and RC (Reading Comprehension).
  In Table 1 we show the system’s performance. The base-