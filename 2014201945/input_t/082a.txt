                   An Adaptive Model of Decision-Making in Planning 


          Gregg Collins1 Lawrence Birnbaunv Bruce Krulwiclv 
      University of Illinois Yale University Yale University 
   Dept of Computer Science Dept. of Computer Science Dept. of Computer Science 
         Urbana, Illinois New Haven, Connecticut New Haven, Connecticut 
               61801 06520-2158 06520-2158 
     Collins@p.cs.uiuc.edu Birnbaum@yale.arpa Krulwich@yale.arpa 

                      Abstract 
                                                         be constructed to achieve particular and well-defined 
                                                          goals. Considerably less attention has been devoted to 
   Learning how to make decisions in a domain is a        those aspects of planning that we might term decision•
  critical aspect of intelligent planning behavior. The   making: Deciding whether (or when) to pursue a given 
  ability of a planner to adapt its decision-making to a  goal, or whether to use a given plan for the goal, or for 
  domain depends in part upon its ability to optimize     that matter whether even to consider the goal in the first 
  the tradeoff between the sophistication of its deci•    place. 
  sion procedures and their cost. Since it is difficult to 
  optimize this tradeoff on a priori grounds alone, we    To make such decisions correctly, a planner must know 
  propose that a planner start with a relatively simple   a great deal about the particular environment in which 
  set of decision procedures, and add complexity in re•   they arise. For example, a person's decision about whether 
  sponse to experience gained in the application of its   to turn around and look if he hears footsteps behind him 
  decision-making to real-world problems. Our model       depends crucially on the situation in which he finds 
  of this adaptation process is based on the explana•     himself: His decision is likely to be quite different on a 
  tion of failures, in that it is the analysis of bad     moderately populated suburban street in the middle of 
  decisions that drives the improvement of the deci•      the day than in an empty city street late at night. The 
  sion procedures. We have developed a test-bed          relevant characteristic of the environment, obviously, is 
  system for the implementation of planning models        the rate at which threats can be expected to arise, and in 
  employing such an approach, and have demon•            particular the threat of being victimized by a criminal. 
  strated the ability of such a model to improve its 
  procedure for projecting the effects of its moves in   One of the major issues confronting any planner is deter•
  chess.                                                 mining how much effort to expend on evaluating a given 
                                                         decision. Decision-making in general is subject to a trade•
1 Introduction                                           off between the sophistication of the procedure em•
                                                         ployed to make the decision, and the time and effort 
The ability to plan effectively involves many different   required to perform the analysis. A more sophisticated 
forms of reasoning: projecting the effects of actions in a decision-making procedure will, generally speaking, 
current or hypothetical situation, deciding which goal to either consider more alternatives, consider each alterna•
pursue from among the many that might be pursued at       tive in more depth, or do both. This requires more 
any given time, constructing sequences of actions that   reasoning, and is correspondingly more expensive. 
can achieve a given goal, determining whether to exe•    Whether a more sophisticated procedure is worth using 
cute such a sequence, and so on. By and large, most      depends on whether the gain from its superior perform•
research on planning in Al has concentrated on those     ance outweighs this extra cost. Consider, for example, 
aspects of planning that in vol ve reasoning about actions, the tradeoffs involved in deciding which goal to pursue: 
and more specifically, on how sequences of actions can   The planner must first determine which of a myriad 
                                                         possible goals—including goals to acquire resources, 
                                                         protect desired states, or improve the current situation, 
                                                         among others—are reasonable candidates, and then 
 This research was supported in part by the Defense Advanced 
1                                                        compare the expected benefits from each of these candi•
Research Projects Agency, monitored by the Air Force Office of 
                                                         dates. Obviously, the more goals the planner considers 
Scientific Research, under contract number F49620-88-C-0058, 
and in part by the Office of Naval Research under contract as reasonable candidates, the more likely it will be to find 
number N0014-85-K-010.                                   a better goal to pursue. On the other hand, the more goals 

                                                                           Collins, Birnbaum and Krulwich 511 the planner must compare in making its decision, the      This paper describes the application of our model to the 
more this procedure will cost.                            progressive adaptation of decision-making procedures 
                                                          to new planning environments. Such an application 
The key point about such tradeoffs in the sophistication  requires spelling out the assumptions involved in the 
of the decision-making process is that, like the decisions expectation that simple decision procedures will suffice, 
themselves, they are highly dependent upon the particu•   how those assumptions can fail, and how the procedures 
lar planning environment. For example, in a highly time-  can be improved upon the diagnosis of such failures. We 
limited situation (e.g., basketball or some other fast-   first discuss how these issues arise in a case study of the 
paced game), a planner simply cannot afford to consider   chess fork. We next briefly describe a program which 
very many candidate goals when trying to determine        implements some of our ideas, and then discuss its 
what it should do next. Thus, when confronted with a      application to a second case study, concerning the devel•
novel planning environment, a planner must be pre•        opment of a planner's ability to predict its opponent's 
pared to adapt its decision-making processes to that      move in deciding its own move in chess. 
environment, balancing the tradeoff between their so•
phistication and their cost.                              2 Case study: The fork 

It seems difficult to imagine how an agent might find the As our first case study in improving decision-making in 
roughly optimal level of decision-making sophistication   planning, we will consider how a novice chess planner 
on the basis of a priori reasoning alone. This suggests that can learn to deal with a fork—a situation in which the 
planners should be prepared to alter their decision•      planner's opponent confronts it with an attack on two 
making processes on the basis of experience within a      pieces simultaneously. Unless a move can be made that 
new planning environment. The approach we have taken,     will defend both pieces at once—or alternatively one 
therefore, is to start with a relatively simple model of  piece can be moved or defended in a way that at the same 
decision-making—one that ignores many potential al•       time poses a strong counter-threat to the opponent—one 
ternatives, and many of the factors that might bear on    of the pieces in question will surely be captured. The fork 
deciding among them—and to progressively make it          is something that all chess players eventually learn about, 
more sophisticated in response to poor decisions, i.e., de• usually as a result of being victimized by it. The question 
cisions to pursue goals and plans that ultimately fail. In is, what does a novice learn about the fork from this 
other words, our model of how a planner adapts its        experience? 
decision-making to new planning domains is failure-
driven (see, e.g., Sussman, 1975; Schank, 1982; Hayes-    A planner victimized by an opponent's tactic should 
Roth, 1983; Minton, 1984; Hammond, 1986). In such an      learn two things: how to avoid being victimized in the 
approach, when a plan fails, the goal of preventing a     future, and how to use the same tactic to victimize others. 
similar failure in the future leads to an attempt to explain We will concentrate here on the first of these, the ques•
the current failure.1 The explanation of the failure, in  tion of how a planner learns to avoid the fork. Since the 
turn, suggests ways in which similar failures might sub•  fork takes advantage of the planner's plan for defending 
sequently be avoided.                                     its pieces, we must begin by considering what the plan•
                                                          ner knows about this plan, and in particular, what he 
We have elsewhere presented our model of failure-         knows about the assumptions upon which its efficacy 
driven learning in planning domains (Birnbaum and         depends. 
Collins 1988;Collinsand Birnbaum, 1988a and 1988b). In 
our approach, the planner encodes a description of the    The plan that almost every novice chess player seems to 
intended functioning of its plans in explicit justification have for defending his pieces is this: Before each turn, 
structures. When the expectation that a plan will succeed check to see if any of the opponent's pieces is in position 
is violated, the planner can trace back through the justi• to move to the location of one of your pieces. If this is the 
fication structure to identify the culprit among the initial case, then you can prevent the execution of the threat by 
assumptions (see, e.g., deKleer, et ah, 1977; Doyle, 1979). moving the threatened piece, or by guarding it with 
By identifying which assumptions have failed, the plan•   another piece. This plan makes the assumption that a 
ner is able to arrive at a characterization of how the    one-move warning of a threat to a piece is sufficient to to 
particular si tuation brought about this failure, which can allow it to be defended, and this is generally the case. It 
then serve as the basis for a rule that suggests what to do is this assumption, however, that the fork takes advan•
when similar situations arise subsequently.               tage of: When two imminent threats are detected on the 
                                                          same turn, there is not time to block both of them. 

1Thus the approach can also be viewed as a form of explanation- Given that a novice planner understands that its plan 
based learning (see, e.g., Dejong and Mooney, 1986; Mitchell, 
Keller, and Kedar-Cabelli, 1986).                         depends upon this assumption, a plausible explanation 

512 Cognitive Models for how it comes to understand the fork can be con•      moves in advance is that it would cost too much to do so: 
structed. The fork results in the failure of the plan to The planner would be swamped by the need to respond 
protect materiel, since it results in the capture of a piece. to a massive number of threats, since in two moves most 
This failure can be traced to the violation of an underly• of the opponent's pieces would be able to move to at least 
ing assumption of the plan, namely that a one-move       one square currently occupied by one of the planner's 
warning of any threat would be sufficient.1 The violation pieces. Most of these "threats" would never develop, 
of this assumption was, in turn, brought about by the     and the planner would at best waste a great deal of time 
opponent's positioning of a piece so that it attacked two that could better be used plotting strategy. There is, in 
pieces simultaneously. The key point here is this: Such an other words, a tradeoff: By detecting threats early, the 
analysis not only explains the fork, it suggests the way to planner provides more time to deal with multiple threats, 
guard against it. The failure of the assumption that a one- but at the cost of being forced to consider an enormous 
move warning is sufficient implies the need to detect at  number of possible—but unlikely—threats. By detecting 
least one of the two threats earlier. Moreover, the fact that threats later, the planner runs the risk of not having 
the failure was brought about by the positioning of a    enough time to deal with multiple threats, but avoids 
piece to attack two pieces at once means that efforts at being swamped with too many low probability reports. 
earlier detection can be limited to situations where the  In this case, the tradeoff clearly is on the side of later 
opponent can, with a single move, produce a threat to    detection, and this is likely to be true in many domains. 
two pieces. Thus, understanding how the fork violates 
its assumptions enables a planner to determine how it     A key characteristic of the appropriate defense against 
can avoid being victimized by the fork in the future: It  the fork is that it avoids a blow-up in the number of 
must modify its plan for scanning the board to include a detected threats by looking only for cases in which one 
check for opponent's pieces that can be moved into        piece can be moved so as to attack two pieces at once. In 
position to attack two unguarded pieces of its own.       other words, by looking for forks in particular, rather 
                                                          than detecting all threats two moves in advance, the 
This explanation of how a novice planner learns from      planner focusses its attention narrowly enough to make 
experiencing the fork leaves us with a question, how•     the extra effort worthwhile. So this is what the planner 
ever: Why would the planner have assumed that a one-      really learns from the fork: Not that one move's warning 
move warning was enough in the first place? The most      might not be enough—it already knew that—but a 
obvious answer is that the planner was simply unable to  characterization of the circumstances in which it is not 
imagine a circumstance in which this would not be the     that is precise enough to allow them to be detected effi•
case. Unfortunately, this explanation does not hold up    ciently. 
upon further reflection. The idea that multiple threats 
pose a problem for defenses that involve detecting and    We can now provide a coherent account of how the fork 
blocking individual threats is well known to all human    is learned. The planner begins with the understanding 
planners: This is the fundamental reason, for example,    that threats to pieces will arise periodically in chess. The 
why "ganging up" allows a superior force to be defeated   standard strategy for dealing with threats in any domain 
by a collection of inferior forces in any competitive    is this: First, construct methods for detecting individual 
situation. It seems likely that any human planner would  instances of threats in advance. Second, construct meth•
have experienced this phenomenon, and grasped its        ods for blocking threats that can be executed when a 
significance, long before learning to play chess. But     threat is spotted. And third, ensure that all threats will be 
given the knowledge that detection-based plans for       detected in time for a blocking routine to be carried out. 
blocking threats are vulnerable to an overload of threats, 
plus the lack of any particular reason to believe that this In chess, first, the obvious method for detecting threats is 
problem would not arise in chess, it seems unlikely that  to determine which of the opponent's pieces could move 
the planner would nevertheless conclude that a one-       to locations occupied by the planner's pieces in some 
move warning would always be enough. To argue this        particular number of moves. Second, the obvious method 
would be to conclude that vulnerability to the fork is the for blocking these threats is to move the threatened piece 
result of a mistake—a mistake made consistently by       out of harm's way; a slightly more sophisticated plan is 
almost every person who ever learns the game of chess.    to guard the piece with another. And third, since any 
It would be difficult to explain why such an error in     piece can generally either be moved or guarded in a 
reasoning should be made by nearly everyone.              single turn, and since—all other things being equal—it is 
                                                         best to detect threats as late as possible, a planner can 
In fact, the answer lies not in the logic of the situation, but conclude that one move's warning will generally be 
in its economics. The reason for not detecting threats two enough. However, the planner cannot prove that one 
                                                          move will always be enough, and so is forced to make the 
 'See Collins and Birnbaum, 1988b, for a discussion of how this is assumption that this will generally be the case. By 
accomplished. 

                                                                           Collins, Birnbaum and Krulwich 513 making this assumption explicit, and by ensuring that it  this section. In addition, there are 22 chess-specific rea•
is monitored whenever a threat arises, the planner as•    soning rules. All of these rules, general as well as 
sures itself of being alerted to cases in which the assump• specific, have associated justification structures, as do all 
tion fails, as it does in the fork. So, while the planner is particular facts and expectations contained in the sys•
forced to accept the cost of a few failures as the price of tem's data base. These justification structures are con•
an efficient threat-detection strategy, by monitoring the structed and utilized by the rule applier, the failure 
assumptions it is forced to make, and analyzing the       explanation algorithm, and the rule patching mecha•
failures that occur, it is able to gradually improve its  nism. We devote the rest of this section to describing the 
performance by dealing with problematic cases one at a    program's application to a second case study, concern•
time.                                                     ing the development of a planner's ability to predict its 
                                                          opponent's move in deciding its own move in chess. 
In a sense, this explanation argues that a planner is 
deliberately courting failure as a shortcut to a better plan. The decision-making process within our planning model 
Viewed this way, the alternative would have been for the  is composed of three interleaved components: a decider, 
planner to reason out, a priori, the circumstances in which which determines what move to make, a projector, which 
multiple threats could occur, rather than waiting for     projects the results of a possible move in a given situ•
those circumstances to arise in practice. In effect, this ation, and an evaluator, which evaluates a situation from 
would mean inventing the fork before actually playing     a given player's perspective. Our planner starts with 
the game. While this is possible in principle, in practice extremely primitive methods for accomplishing each of 
the computational cost is likely to be extremely high—    these decision-making tasks. First, to decide what move 
high enough that it is almost certainly cheaper to wait for to make it simply projects the results of each move 
the fork to happen. We believe that similar explanations  available to it, evaluates each resulting situation, and 
underlie the acquisition of a wide range of decision•     chooses the move that yields the best result. To project 
making strategies employed by planners.                   the results of making a given move, the planner simply 
                                                          assumes that its opponent will behave as it would in any 
3 An implementation and a second case study               given situation; the projection method that implements 
                                                          this is shown in figure 1. Finally, to evaluate a given 
 In order to explore more concretely our failure-driven   situation, the planner computes the difference between 
approach to the adaptation of decision-making proc•       the total values of the two player's pieces remaining on 
esses in planning, we have constructed a test-bed—        the board. 
comprising mechanisms for inference and rule applica•
tion, justification maintenance, expectation handlers,    Consider now how a planner endowed with these primi•
failureexplanation, and rule patching—and implemented     tive decision-making methods would behave in the situ•
within it a simple model of decision-making and plan•     ation shown in figure 3. To such a planner, moving the 
ning for turn-taking games. This model, in turn, has been rook to take the knight looks like the best move. This 
used for exploring rudimentary planning and learning      decision is based on the erroneous expectation that its 
in tic-tac-toe and in chess.                              opponent will take the planner's knight. It forms this 
                                                          expectation because the projection method shown in 
(def-brule proj-factor-2                                  figure 1 simply evaluates the opponent's options in the 
  (world (move move) (player player) result (weight integer) 
        (opp-move move) (time time)                       original situation, i.e., without taking into account the 
        (better-move move) (better-goal goal)             effects of the planner's own move. The use of such an 
        (better-value integer) (orig-value integer) )     unsophisticated procedure considerably simplifies the 
 (project-factor proj-factor-1.5 world move player result weight) projection problem, because it obviates the need to re•
 (and (= world (world-at-time time)) 
    (decide (player-opponent player)                      compute the opponent's response individually for each 
     (possible-moves-at-time (player-opponent player) time) move contemplated by the planner. However, its valid•
    world simple-dec-factors opp-move)                    ity, and hence the validi ty of theexpectationsthe planner 
    (= result (world-after-move opp-move (world-after-move generates concerning the opponent's moves, depends 
        move world))) 
    (= weight 1)                                          on an assumption that nothing will occur that will enable 
   )                                                      the opponent to make a better move than those currently 
 [Justification herel)                                    available to it. This assumption, is an instance of what is 
                                                          sometimes referred to as an inertia assumption, namely an 
           Figure 1: Initial Projection Method            assumption that things will stay as they are as much as 
                                                          possible. In our planner, this assumption is itself justi•
The planning model is implemented in 14 fairly general    fied by a conjunction that says roughly "Nothing will 
rules, concerning the detection of threats and opportuni• happen to give him a better move because he can't do 
ties, making choices, forming and checking expecta•       anything to give himself one, I won't do anything to give 
tions, and the like; some typical rules are shown later in 

514 Cognitive Models                                           Figure 3: Game Board Sequence 

him one, and no outside forces will give him one."       pected. When this happens, the planner detects that its 
Because the planner's decision to take the knight is     expectation that the opponent will take the knight has 
justified by its projection that its opponent will take a failed. This then causes the planner to traverse the 
knight in response, it monitors the status of this expecta• justification structure for that expectation, checkingthe 
tion. The expectation, in turn, is justified by the assump• assumptions upon which it depends. 
tions underlying the projection method used to produce 
it.                                                       In particular, the planner will discover that its assump•
Applying rules in NEW-EXPECTATION-FAILURE-RULES          tion that the opponent will not have a better move has 
—Expectation failure:                                    been violated. A partial transcript of this process is 
 item=(move-to-make (move opponent move-take pawn 
                       (ro>loc 2 3) (rc->loc 3 4) knight) shown in figure 4, but it should be clear that the assump•
                   opponent (goal-unknown) 2)            tion in the justification that will fail is the inertia assump•
Traversing...                                            tion that no event will occur that will give the opponent 
Checking fact                                            a better move than we expect him to have, and in particu•
#(fact 1785: (project (move computer move-take rook 
                (move opponent move-take pawn            lar the assumption that no action of the program's will 
                      (rc->loc 2 3} (rc->loc 3 4) knight) give the opponent a better move. 

Checking b-rule #{b-rule proj-factor-2)                  As can be seen in figure 4, the system knows more than 
—> Valid: 
(not (exists (c event)                                   just the identity of the assumption that failed: It also 
       (move-to-make ?e.216 opponent ?better-goal.209 1))) knows how that assumption failed. The program's domain 
—> Valid:                                                knowledge allows it to determine that the opponent's 
(not (exists (e event) 
                                                         move was in fact a legal one, and a comparision of the jus•
       (and (extraneous-event ?e.2l7) 
        (event-enables-event ?e.217 ?better-move.208)))) tification for this belief with the program's prior expec•
—> Found a bug:                                          tation reveals that the event that enabled the opponent to 
(no (and (move-enables-move ?move.210 ?better-move.208)  make a better move than expected was in fact the com•
       (move-possible ?better-move.208 ?time.207)        puter's own move. Once the program discovers this, it 
       (move-legal ?better-move.208) 
       (evaluate (world-after-move ?better-move.208      must modify its reliance on the inertia assumption to 
                 (world-after-move ?move.210 ?world.211) take the problem into account, using the information 
               eval-factors                              provided in its justification structures and its analysis of 
               (player-opponent ?player.212) 
                                                         the failure. The program uses standard goal regression 
               ?better-value.213) 
       (evaluate (world-after-move ?opp-move.214         techniques (see, e.g., Dejong and Mooney, 1986; Mitchell 
                 (world-after-move ?move.210 ?world.21D) etal., 1986) todetermine just those aspectsof the situation 
               eval-factors                              that caused the expectation to fail. 
               (player-opponent ?player.212) 
               ?orig-value.215)                          The resulting generalized explanation for the failure is 
       (> ?better-valuc.213 ?orig-value.215)))           used to patch the method that predicts the opponent's re•
 *** Returning a BUG ***                                 sponse, as utilized by the projection component, so that 
                                                         it checks for opportunities presented to the opponent by 
     Figure 4: Traversing an Expectation's Justification the planner's own move, thus ensuring that this mistake 
                                                         will be avoided in the future (see figure5). The new rule 
Unfortunately, as we can see, the opponent is not going  says roughly the following: 'To see what the world will 
to make the move that the computer expected, and the     be like after making move M, see what you would do in 
move that he will make—namely taking the planner's       the opponent's situation at the current time, and assume 
rook—is a better move than the one the planner ex•


                                                                           Collins, Birnbaum and Krulwich 515 