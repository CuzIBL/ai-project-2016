    A Tighter Error Bound for Decision Tree Learning Using PAC Learnability
                     Chaithanya Pichuka, Raju S. Bapi, Chakravarthy Bhagvati,
                                  Arun K. Pujari, B. L. Deekshatulu
                           Department of Computer and Information Sciences
                                        University of Hyderabad
                                 Gachibowli, Hyderabad, 500046, India
                pssgrk438@yahoo.com;     {bapics, chakcs, akpcs, bldcs}@uohyd.ernet.in

                    Abstract                          SVM, Bayesian ML and kNN describe increasingly more
                                                      complex decision regions and hence have richer hypothesis
    Error bounds for decision trees are generally based representation capability. It is to be noted that although lim-
    on depth or breadth of the tree. In this paper, we ited empirical characterization of classiﬁers is possible as de-
    propose a bound for error rate that depends both on picted in Figure 1, theoretical characterization of classiﬁers,
    the depth and the breadth of a speciﬁc decision tree in general, is difﬁcult. In this paper we shall consider DTs for
    constructed from the training samples. This bound further theoretical and empirical analysis.
    is derived from sample complexity estimate based
    on PAC learnability. The proposed bound is com-     Several techniques have been proposed for estimating the
                                                                                           [
    pared with other traditional error bounds on sev- future error rate of a decision tree classiﬁer Kaariainen and
                                                                                  ]
    eral machine learning benchmark data sets as well Langford, 2005; Mansour, 2000 . There are two primary
    as on an image data set used in Content Based Im- classes of error estimation methods available. The ﬁrst class
    age Retrieval (CBIR). Experimental results demon- of methods utilize the empirical error on the training samples
    strate that the proposed bound gives tighter estima- in predicting the future error. Empirical error could be based
    tion of the empirical error.                      on the training set or on the test set or on both. For example,
                                                      we can use k-fold cross-validation on a dataset and then trans-
                                                      form the cross-validation estimate into an estimate or heuris-
1  Introduction                                       tic conﬁdence interval of the error of the ﬁnal decision tree
Computational learning theory (CoLT) deals with the char- learned from all examples. Another approach is the sample
acterization of the difﬁculty of learning problems and the compression bound which considers the test set after labelling
capabilities of machine learning algorithms [Vapnik, 1998; it, that is, it estimates the next label based on the training set
Vidyasagar, 1997]. The probably approximately correct and the labelled previous test set [Langford, 2005].Thereare
(PAC) framework is formulated to characterize classes of hy- several other methods such as Microchoice bound [Langford
potheses that can be reliably learned from a reasonable (poly- and Blum, 1999], Test set bound and Occam bounds [Lang-
nomial) number of randomly drawn training examples with a ford, 2005]. Microchoice bound inherently depends on the
reasonable amount of computation. Sample complexity speci- structure of the DT by calculating the choice spaces at every
ﬁes the number of training examples needed for a classiﬁer to node of the DT. Test set bound is a test set-based bound and
converge, with a high probability, to a successful hypothesis. is entirely characterized by the errors on the labelled test set.
Within the PAC framework, it is possible to derive bounds As the test set bound incorporates test set error directly, its es-
on sample complexity by combining the expression for the timate is usually good. Occam bound assumes the underlying
size of the hypothesis space and the empirical error [Mitchell, distribution to be Binomial and computes an estimate based
1997]. Thus characterization of the hypothesis space is an im- on the empirical errors observed on the training dataset.
portant issue in CoLT.                                  The second class of methods utilize structural aspects of
  In order to understand the representational capability of the classiﬁer in order to arrive at an estimate. For example, in
various classiﬁers, we ran several algorithms such as percep- the case of DTs, one could consider the depth or breadth of
tron, linear and polynomial support vector machines (SVM), the DT constructed on the training samples in the estimation
Bayes maximum likelihood (ML), decision tree (DT), oblique of future error.
decision tree, and k-nearest neighbor (kNN) classiﬁers on the In this paper we propose a structure-based error bound
standard Iris benchmark dataset. Iris is a four-dimensional for DTs using the PAC learning framework. The new error
dataset comprising three classes, of which two classes are not bound considers both depth and breadth of the DT learned
linearly separable. Figure 1 shows a two-dimensional projec- from training examples. We conducted several experiments
tion of the decision regions described by the trained classi- on benchmark datasets to compare the results of various error
ﬁers on the benchmark dataset. Perceptron, linear SVM and bound estimation methods and found that the proposed esti-
DT deﬁne elementary decision regions comprising linear and mation method works well. The rest of the paper is organized
axis-parallel rectangular surfaces. Oblique DT, polynomial as follows. Firstly, we describe both the structure-based and

                                                IJCAI-07
                                                  1011Figure 1: Decision Regions of Classiﬁers: Comparison of the hypotheses learned by various classiﬁcation methods on the Iris
machine learning benchmark dataset.

empirical error-based estimation methods. Experimental re- H if for all cC, distribution T over X, ε such that 0 <
                                                           1                          1
sults are presented subsequently. Discussion of the results is ε< 2 ,andδ such that 0 <δ< 2 , learner L will with
followed by conclusions.                              probability at least (1 − δ) output a hypothesis h ∈ H such
                                                                ( ) ≤                             1  1
                                                      that errorT h   ε, in time that is polynomial in ε , δ , n,
2  Classiﬁer Structure-Based Error Bounds             and size(c). This can also be represented mathematically as
                                                      follows.
In this section, before introducing classiﬁer structure based
bounds, we will describe PAC learning in detail and explore       [      ( )   ] ≤
                                                                P  errorT h >ε     δ
how generalization bounds could be formulated based on                                     (or)       (1)
PAC learning framework.                                           [      ( )   ] ≥ (1 − )
                                                                P  errorT h <ε         δ
2.1  PAC Learning and Error Bounds                      PAC-Learnability concept can be used to derive a general
                                                      lower bound on the size of the training set required by a
In this section we review the relation between Probably Ap-                m
                                                      learner. There are two possible scenarios. In the ﬁrst sce-
proximately Correct (PAC) learning framework and general-
                                                      nario, we assume that the learner is consistent,thatis,the
ization error bounds [Valiant, 1984; Mitchell, 1997].PAC
                                                      learner outputs hypotheses that are consistent with the tar-
tackles the questions related to the number of examples and
                                                      get concept on the training set. The probability that a single
the amount of computation required to learn various classes
                                                      hypothesis having true error greater than ε and is consistent
of target functions. PAC learning framework has two major
                                                      with the training set is at most (1 − ). And the probability
assumptions. One that assumes that the classiﬁcation error                           ε
                                                      that this hypothesis will be consistent with m individuals is at
is bounded by some constant, ε, that can be made arbitrarily    m
                                                      most (1 − ε) . If, instead of one, there are such hypotheses
small. The second assumption requires that the classiﬁer’s                                k
                                                      whose true error is greater than ε and are consistent with the
probability of failure is bounded by some constant, δ,that
                                                      training set, then the upper bound on the probability that at
can be made arbitrarily small. In short, we require that the
                                                      least one of these k hypotheses will be consistent with all the
classiﬁer probably learns a hypothesis that is approximately                               m
                                                        randomly drawn individuals is, ∗(1− ) . Using the fact
correct – hence it is called the Probably Approximately Cor- m                    k      ε
                                                      that  ≤|  |, we can write down the following inequality in
rect (PAC) learning framework. Deﬁning the error on entire k   H
                                                      Equation 2.
instance distribution (T ) of data as true error (errorT ) and
                      ( )                 (      )
the error on training set D as empirical error errorD ,         [      ( )    ] ≤  ∗ (1 − )m ≤
PAC learnability can be deﬁned formally as follows.            P errorT h >ε     H       ε     δ      (2)
  Deﬁnition [Mitchell, 1997]: Consider a concept class C We can rewrite this as shown in Equation 3 and solving for
deﬁned over a set of instances X of length n and a learner L m (the number of training examples needed by the learning
using hypothesis space H. C is PAC-Learnable by L using algorithm) results in the inequality shown in Equation 4.

                                                IJCAI-07
                                                  1012                                                        If we substitute this into Equation 7, then we get the expres-
                                (−mε)
         p[errorT (h) >ε] ≤ H ∗ e    ≤  δ       (3)   sion for error bound for decision tree using the depth feature
                                                      as shown in Equation 9.
                    1           1
               m ≥   (ln|H| + ln( ))            (4)
                    ε           δ
                                                               ( ) ≤       ( )+
  In the second scenario when the learner is agnostic,that errorT h  errorD h
is, the true error on the training set is not necessarily zero,    
then we can use Chernoff approximation to estimate the er-            ln2 ((2k − 1)(1 +  )+1+     ( 1 ))
ror bound in the PAC learning framework [Mitchell, 1997].             2m             log2n      ln δ
Analogous to Equation 4, the error bound for agnostic learner                                         (9)
can be derived and is shown in Equation 5.
                    1            1                    2.3  Breadth-Based Error Bound for DT (PAC II)
              m ≥     (ln|H| + ln( ))           (5)
                   2ε2           δ
  Now, we can use Equation 5 to derive the generalization In this method, computation of |H| is based on breadth of the
error bound as shown in Equation 6.                   tree and the approach is akin to the one shown in the lecture
                                                     notes of Guestrin [Guestrin, 2005]. We assume that the DTs
                    1             1                   are binary trees and use the idea that every k-leaves decision
             ε ≥       (ln|H| + ln( ))          (6)
                   2m             δ                   tree will have (k − 1)-leaves decision trees in it.
                                    (      ( ))
  We can write this in terms of true error errorT h and
           (      ( ))
training error errorD h as shown in Equation 7.       Approximation of |H| for PAC II
                          
                             1            1           Given n attributes,
  error (h) ≤ errorD (h)+      (ln|H| + ln( ))  (7)
       T                    2m             δ
                                                        Hk   =   Number of decision trees of leaves k
  Coming back to the original problem of deriving error H0   =2
bounds for DTs, Equation 7 can be used if only we know            k−1
                            |  |                                                                     (10)
the size of the hypothesis space, H . Thus for PAC based Hk  =   n    HiHk−i
error bounds, we need an estimate of |H|. There are three         i=1
                                                                  k−1      2k−1
ways of estimating |H| for DTs. The ﬁrst approach is to es- Hk = n   (k +1)
timate based on the depth of DT, the second method is to
estimate based on the breadth of DT, and the third possibility
is to use both of these measures. We propose a bound based The last step in Equation 10 is a rough (crude) approxi-
on the third approach wherein recursive estimation of |H| is mation to Hk from the previous step. If we substitute this
done. In the subsequent sub-sections, we describe the three into Equation 7, then we get the expression for error bound
structure-based error bounds for DTs.                 for decision tree using the breadth feature as shown in Equa-
                                                      tion 11. The crude approximation will be replaced by a better
2.2  Depth-Based Error Bound for DT (PAC I)           approximation obtained by recursive estimation in PAC(III).
Computation of |H| is based on depth of the tree and the
approach is akin to the one shown in the lecture notes of
Guestrin [Guestrin, 2005]. We assume that the DTs are bi-
                                                        error (h) ≤ errorD (h)+
nary trees and use the idea that every k-depth decision tree T
        ( − 1)                                              
will have k   -depth decision trees as its children.           1                                    1
                                                              2m ((n − 1)ln(n)+(2k − 1)ln(k +1)+ln( δ ))
Approximation of | | for PAC I
                 H                                                                                   (11)
Given n attributes,

 Hk        =   Number of decision trees of depth k
 H0        =2                                         2.4  Depth and Breadth-Based Error Bound for DT
 Hk+1      =(choices of root attribute)∗                   (PAC III)
                (possible left subtrees)∗
                (                     )
                 possible right subtrees              We propose this new approach for calculating error bounds
           =     ∗    ∗
 Hk+1          n  Hk   Hk                             for DTs. This approximation is very much similar to PAC II
                2k   2k −1
 Hk        =2∗      n                                 although the resulting bound will be tighter as we will show
 if Lk     =   log2Hk  and L0 =1                      empirically. The simple idea behind this approach is that it
 Lk+1      =   log2n +2Lk                             is important to consider several structural features of the DT
                 k
 So, Lk    =(2−      1)(1 + log2n)+1                  to get a closer approximation of |H| whichinturnleadstoa
                                                (8)   tighter estimate for the error bound.

                                                IJCAI-07
                                                  1013Approximation of |H| for PAC III                      3.2  Test Set Bound
Given n attributes,                                   In this bound assumptions about the error distribution are
                                                      made. In particular, classiﬁcation error distribution is mod-
 Hk       =   Number of decision trees of leaves k    elled as coin ﬂips distribution or the Binomial distribution as
 H0       =2                                          shown in Equation 16.
 H1       =   n ∗ H0 ∗ H0
 H2       =   n ∗ H1 ∗ H1                                                  k
                                                                   k            m    j        m−j
          .                                                   Bin(  ,cD)=     (    )c (1 − cD)       (14)
          .                                                                     j    D
          .                                                       m        i=1
                k−1
                              k−1         k             The expression in Equation 14 computes the probability
   k      =          i k−i =      ∗  k ∗
 H            n    H  H      n     F    H1            that m examples (coins) with error rate cD produce k or fewer
                i=1                                   errors. We can interpret the Binomial tail as the probability
 W here,                                                                                     k
              k−1                                    of an empirical error greater than or equal to m .Butweare
                                                      interested in error bound for a classiﬁer given the probability
 Fk       =       FiFk−i
                                                      of error δ and m, so we deﬁne Binomial tail inversion as given
              i=1
                                                      in Equation 15 which gives the largest true error such that the
 F1       =1(Initial condition)                                            k
                                                      probability of observing m or more errors is at least δ.
                                               (12)
                                                            ( k       ( )) =     {  :    ( k  ) ≥ }
  We have to substitute the expression for H into Equation 7 Bin , errorT h maxp  p  Bin    ,p   δ   (15)
to get error bound for DT using this approximation.          m                            m
                                                        Now, given the number of test instances m, test set bound
                                                      can be formulated as shown in Equation 16.
3  Empirical Error-Based Bounds                       Test Set Bound :
                                                             [      ( ) ≤    (         )] ≥ 1 −
In this section, we will describe generalization bounds for- P errorT h  Bin  errortest,δ      δ     (16)
mulated based on the empirical error observed on the dataset. One serious drawback of the test set bound is that it is
Unlike the structure-based error bounds discussed in the pre- possible that the test set and training sets are incompatible
vious section, bounds reviewed in this section explicitly in- and thereby introduce inaccuracies in the error bound estima-
corporate empirical errors observed on the datasets — train- tion [Kaariainen and Langford, 2005].
ing and test sets.
                                                      3.3  Occam’s Razor Bound
3.1  Microchoice Bound                                This is can be termed as a training set-based bound as it takes
                                                      the training set performance into consideration. This is rea-
Microchoice approach tries to estimate an aprioribound sonable as many learning algorithms implicitly assume that
on the true error based on the sequential trace of the algo- the train set accuracy behaves like the true error. Addition-
     [                      ]
rithm Langford and Blum, 1999 . When a learning algo- ally, in this bound we need to know the prior probability of
rithm is applied on training set of n instances, the algorithm the hypotheses.
successively makes a sequence of choices c1,...,cn from a
sequence of choice spaces, C1,...,Cn ﬁnally producing a Occam’s Razor Bound:
hypothesis, h ∈ H. Speciﬁcally, the algorithm looks at the For all priors P (h), over all classiﬁers h,forallδ ∈ (0, 1),
choice space C1 to produce choice c1. The choice c1 in turn
                                                          [      ( ) ≤    (      ( )    ( ))] ≥ 1 −
determines the next choice space C2. It is to be noted here P errorT h Bin errorD h ,δP h         δ  (17)
that at every stage the previous choices are eliminated from We obtain the Occam’s razor bound by negating the above
consideration at the next stage. The algorithm again looks at Equation 17.
the data to make the next choice c2 ∈ C2. This choice then
determines the next choice space C3, and so on. These choice [    ( ) ≥     (      ( )   ( ))]
spaces can be thought of as nodes in a choice tree, where P errorT h    Bin errorD  h ,δP h   <δ     (18)
each node in the tree corresponds to some internal state of It is very important to notice that the prior P (h) must be
the learning algorithm, and a node belonging to some choice selected before looking at the instances. We can relax the
space Ci.                                             Occam’s Razor bound with the entropy Chernoff bound to
  We can apply this Microchoice bound to decision tree by get a somewhat more tractable expression [Langford, 2005].
taking into account the choice spaces available at every node Chernoff Occam’s Razor Bound:
of the decision tree and then using PAC bound technique to For all priors P(h), over all classiﬁers h,forallδ ∈ (0, 1),
get the error bound as shown in Equation 13.                                 
                                                                                1      1         1
                       
                                                      P [err (h) ≥ errD(h)+      (ln(    )+ln(   ))] <δ
                         1                   1            T                  2m     P (h)      δ
error (h) ≤ errorD (h)+    (           Ci + ln( ))
     T                   2m                    δ                                                     (19)
                             i∈Nodes(DT)                The application of the Occam’s Razor bound is somewhat
                                               (13)   more complicated than the application of the test set bound.

                                                IJCAI-07
                                                  1014                 Table 1: Results of experiments for error bound calculating on 15 different data sets.

       Data Set     S     P(I)   P(II)   P(III)  MC      Occ    Test    Emp   ATT     B    D     E
        Weather     14    0.489  0.356   0.356   0.490  0.953   0.485   0.25    4     1     1   3.5
     Yellow-small-  16    0.521  0.409   0.409   0.473  0.889   0.263  0.263    5     1     1   4.2
        Adult-      20    0.462  0.362   0.362   0.425  0.793   0.427   0.05    5     1     1    1
        Adult+      20    0.462  0.362   0.362   0.425  0.793   0.427   0.05    5     1     1    1
     Yellow-small+  20    0.462  0.362  0.362   0.4294  0.793   0.388   0.04    5     1     1   0.8
     Contact Lenses 24    0.532  0.354   0.347   0.466  0.734   0.394  0.096    5    1.1   1.7  2.3
         Labor      57    0.368  0.220   0.220   0.394  0.424   0.294  0.071   17    0.7   1.5  4.1
        Monk1       432   0.374  0.211   0.167   0.296  0.288   0.190  0.138    7     5    4.9  59.9
        Monk2       432   0.423  0.271   0.206   0.325  0.288  0.188   0.131    7    7.2   5.2  56.6
        Monk3       432   0.374  0.212  0.168    0.276  0.274   0.255  0.106    7    4.5   4.9  46.2
     Voting-records 435   0.32    0.18   0.149   0.219  0.215   0.158  0.056   17    3.5   4.6  24.5
         CRX        690   0.419  0.129   0.107   0.150  0.150   0.139  0.104   16    3.5   5.5  72.3
       Tic-tac-toe  958   0.958  0.247   0.201   0.237  0.230   0.221  0.087   10    12.5  4.1   84
        Segment    1500   0.168  0.154   0.123   0.184  0.113   0.077  0.025   20    8.6   9.4  38.8
         CBIR       696   0.812  0.296   0.219   0.321  0.271   0.190  0.184   15    11    11   128
S: Size; P(I): PAC(I); P(II): PAC(II); P(III): PAC(III); MC: Microchoice; Occ: Occam’s Razor; Test: Test set; Emp: Empirical
      Error; ATT: Number of Attributes; B: Average Breadth of DTs; D: Average Depth of DTs; E: Average count of
                                           misclassiﬁcation errors

                                                      lustrate how various error bounds are computed. In this case
                                                      the breadth and depth are one each and the choice space size
                                                      at each node is also indicated in Figure 2. The number of
                                                      attributes, n,is5.
                                                        The  H  value computed for various error bounds in
                                                      Equation 20 are now  substituted in Equation 7 to ob-
                                                      tain the ﬁnal theoretical estimates of the error bound
                                                      under different schemes.   Taking  m   =14and
                                                      δ   =0.05, we obtain    the following values for er-
                                                      ror bounds:  PAC(I) = 0.462;  PAC(II) = 0.362;  and
                                                      PAC(III) = 0.362. For the Microchoice bound, we need to
                                                      take Equation 13 and substitute the choice-space sizes from
Figure 2: Example DT: corresponding to Adult- dataset the DT in Figure 2. Then the Microchoice error works out to
where breadth and depth are 1 and the quantity in parenthesis be 0.524.
denotes the size of the choice space at every node.
                                                      5   Empirical Results
4  Illustrative Example                               We have done experiments on 14 different machine learning
                           k   2k −1                  benchmark datasets from the UCI repository. The results re-
   PAC  − I :    Hk   =5∗     n     [Eq 9]
                           1   21−1                   ported are averaged over 10 experiments conducted on ran-
                 H1   =5∗     4    =20
                           k−1       2k−1             domly chosen subsets of the datasets. Training was done on
   PAC  − II :   Hk   =   n   (k +1)     [Eq 11]        %                                    %
                           0  2∗1−1                   66  of the data and testing on the remaining 33 . In the case
                 H1   =5(2)         =1∗  2=2          of image dataset from content based image retrieval (CBIR)
                           k−1                       system, due to the complexity of the experiment only one run
   PAC  − III :  Hk   =   n    HiHk−i [Eq 12]         was conducted. The results are summarized in Table 1. Ta-
                            i=1
                      =2[=1                   ]       ble reports the observed error rates (on training set and test
                 H1         k    , Init. Cond.       set) as empirical error. The goal of the experimentation is to
   Microchoice : H    =              Ci               see if the theoretical bounds estimated from various methods
                          i∈Nodes(DT)                 come close to the observed empirical error. The bold-faced
                 H    =14+8                           entries in Table 1 correspond to the best estimate of the true
                                               (20)   error. From the table, as expected we can see that PAC(III)
  Here we consider an  example experimental dataset,  always gives better estimate than PAC(I) and PAC(II). Depth
namely, the Adult- dataset. The DT structure obtained from based measures overestimate the size of the tree since they
one of the ten experiments on Adult- dataset is chosen to il- assume a complete binary tree and in reality, the DT may be

                                                IJCAI-07
                                                  1015