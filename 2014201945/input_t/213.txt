                                  Extending DTGOLOG with Options4 

                                    A. Ferrein, Ch. Fritz, and G. Lakemeyer 
                                          Department of Computer Science 
                                                    RWTH Aachen 
                                    {ferrein, fritz, gerhard}@cs.rwth-aachen.de 


 1 Introduction                                                briefly introduce MDPs and DTGOLOG. Section 3 describes 
Recently Boutilier et al. (2000) proposed the language DT-     options in more detail and how to map them into DTGOLOG. 
GOLOG which combines explicit agent programming with           We end with a discussion of experimental results. 
decision theory. The motivation is that a user often has some  2 Decision Theory and GOLOG 
idea about how to go about solving a particular problem yet at 
                                                               A fully observable MDP is represented as a tuple M = 
the same time does not want to or cannot commit in advance 
                                                                             where S is a finite set of states, A is a finite set 
to an exact course of action. Instead, certain choices are left 
                                                               of actions, Pr is a probability distribution 
to the agent running the program and determining an optimal 
                                                               [0,1] (Pr(s |A, s ) denotes the probability of the agent end•
action selection policy involves solving a Markov Decision                j     i
                                                               ing up in state S  after performing action A in state s , and 
Process (MDP) [Puterman, 1994]. In a sense, a DTGOLOG                          j                                    t
                                                                             is a bounded reward function. The objective 
program can be thought of as a factored representation of an 
                                                               is then to construct a policy that maximizes the 
MDP. As an example, Boutilier et al. consider a mail-delivery 
                                                               expected total reward over some horizon. A simple algorithm 
scenario where the task of delivering mail to a particular per•
                                                               for constructing optimal policies is value iteration (cf. [Put•
son is hand-coded and fixed, while the agent chooses the or•
                                                               erman, 1994]). 
der in which the various people are served according to some 
reward function. They note that this approach allows solving     DTGOLOG [Boutilier et al, 2000] can be thought of, 
problems which are much larger than those solvable using the   roughly, as a programming language which allows a user 
traditional dynamic-programming approach for MDPs.             to combine pre-defined primitive actions into complex pro•
                                                               grams using the usual constructs like sequence, if-then-else, 
   Nevertheless, it is not very difficult to find examples 
                                                               while, and recursive procedures. In addition there are nonde-
where DTGOLOG also shows poor computational behavior. 
                                                               terministic actions to model that an agent can choose among 
Roughly, the complexity is determined by the number of 
                                                               alternatives. The semantics is based on the situation calcu•
choices the agent needs to consider when computing a pol•
                                                               lus and, in particular, the so-called basic action theories de•
icy and the number of actions with uncertain outcomes (na•
                                                               scribed in [Reiter, 2001], which define when primitive ac•
ture's choices) that need to be considered along the way. 
                                                               tions are executable and how they change and do not change 
To better control this potential blow-up we were inspired by 
                                                               what is true in the world. As a special form of primitive ac•
work on so-called macro actions or options previously de•
                                                               tion DTGOLOG allows so-called stochastic actions, which 
veloped in the MDP framework [Hauskrecht et al, 1998; 
                                                               have probabilistic outcomes, just as in MDPs. For example, 
Sutton et al., 1999]. The idea is that the policies for cer•
                                                               it is possible to define an action r (move to the right), after 
tain subtasks like leaving a room are pre-computed and then 
                                                               which a robot has moved one step to the right with probabil•
simply used when working on the global policy. 
                                                               ity 0.9, but has moved to the left, up, or down with probability 
  This is, roughly, what we propose: for a given domain        0.1. MDP-style reward and cost functions are incorporated as 
identify subtasks and compute local policies using standard    well. Given a DTGOLOG program, the idea is then to com•
MDP techniques like value iteration. From those local          pute a policy in the sense that each nondeterministic choice 
policies generate two representations: (1) a (deterministic)   is resolved in a decision theoretic manner, that is, by choos•
DTGOLOG program directly encoding the local policy in          ing the action with maximal expected reward. Assuming that 
terms of what action to take while the local policy is ap•     there are not too many nondeterministic or stochastic actions, 
plicable and (2) so-called abstract stochastic actions which   a policy can be computed even in cases where an MDP repre•
only address the expected outcome of a local policy. It turns  sentation seems infeasible due to the size of the state space. 
out that, while both representations can be used to compute 
a global policy, the use of abstract stochastic actions may    3 Mapping Options into DTGOLOG 
result in an exponential speed-up.— In the next section we 
                                                               As in [Sutton et al., 1999], we define options as triples 
   *This work was partly supported by the German Science Foun•          where I is the set of initial states, is a policy, and 
dation (DFG), Grant No. LA 747/9-1, and a grant by the NRW Min•  is the set of terminating states. To illustrate options con•
istry of Education and Research (MSWF).                        sider the example in Figure 1 from [Hauskrecht et al, 1998]. 


1394                                                                                                   POSTER PAPERS                                                                east door is not a primitive action). Hence, in a final step we 
                                                               need to replace the abstract actions by the programs which we 
                                                               derived above. 
                                                               4 Experimental results 
                                                               Using our running example we conducted a number of exper•
        Figure 1: Mazc66 from [Hauskrecht et al, 1998].        iments, the goal being at different distances from the initial 
                                                               position (Fig.l shows the special case of distance 8). The re•
The task is to find an optimal policy to get from position S   sults are given in Figure 2. The x-axis depicts the initial dis•
to position G. Performing an action has cost 1, the goal po•   tance to the goal, the y-axis the running time. We compared 
                                                               three different approaches: (A) calculating the optimal policy 
sition has a high positive reward. The agent can perform the 
                                                               in DTGOLOG nondeterministically choosing only from the 
stochastic basic or primitive actions r,l,u,d succeeding with  primitive actions, (B) using a set of procedures like the one in 
probability 0.9. With probability 0.1 it will be in any other ad• the previous section for leaving each room towards a certain 
jacent position. For each room options arc defined to leave the neighboring room, choosing from primitive actions only in 
room through a certain door (one for each room/door combi•     the goal room, and (C) using options in the form of abstract 
nation). The gray dots correspond to the termination positions stochastic actions, choosing from primitive actions only in the 
of these options. For the left lower room in Figure 1 there are goal room. 

two options OE and ON (leave through the east or north door, 
respectively) with IE = and 

     = {(2,4), (4,2)}, and similarly for ON. (In order to 
mark state (4,2) in as success and (2,4) as failure, they are 
assigned positive and negative reward, respectively.) 
   Applying standard value iteration techniques, the follow•
ing can be computed for an option O: 
   1. the optimal policy that is, the most appropriate action 

Ai for each st J; 
   2. for each st I and Sj the probability to terminate 
the option with the outcome SJ when starting in si                 Figure 2: Runtimes of the three test programs in the maze. 
   These results can now be translated into a form suitable for 
DTGOLOG. Given I = {s1,..., sn), suppose that each st            Note that the y-axis of the diagram has a logarithmic scale. 
can be uniquely characterized by a logical formula (In our     The speed-up from (A) to (B) shows the benefit using DT•
example, <pi simply expresses the coordinates of the location  GOLOG to constrain the search space by providing fixed pro•
of .s\.) Then the policy can be translated in a straightfor•   grams for certain subtasks. Interestingly, (C), that is, using 
ward fashion into the following DTGOLOG program:               abstract actions, clearly outperforms (B). Roughly, this is be•
                                                               cause each abstract action has only two outcomes, whereas 
                                                               the corresponding program provides a very fine-grained view 
                                                               with a huge number of outcomes that need to be considered. 
                                                                 Taking the time of calculation of all options into account 
                                                               (here: 10.51 seconds) the use of options pays off at horizons 
where and senseState(O) is a so-called sens•                   greater than 5. Also, calculating options can be done off-line 
ing action. Roughly, executing senseState(O) establishes       and can be neglected in case of frequent reuse. 
the truth values of the so that they can be tested in the        We remark that while method (A) guarantees optimality, 
following while and if conditions. (The sensing actions are    this is not necessarily so for (B) ana (C), for essentially the 
necessary to account for the MDP assumption of full observ•    same reasons as in [Hauskrecht et ai, 1998]. Certainly in 
ability.) All in all, the program simply prescribes that the   the case of (C), this price seems worth the computational 
optimal action (according to ) should be executed as long as   gain. Finally, while we currently assume options as given, 
the agent is in one of the initial states of O.                Hauskrecht et al. (1998) discuss ways of automatically 
   Given (2.) above we can also generate for each st I generating options with good solution qualities, an issue we 
an abstract stochastic action o(si). For example, for the      intend to investigate in the future as well. 
option OE and state s1 = (2,2) we would define an ab•          References 
stract stochastic action with nature's choices 
and for leaving through the east or north door, respec•        [Boutilier et al, 2000] C. Boutilier, R. Reiter, M. Soutchanski, and 
                                                                  S. Thrun. Decision-theoretic, high-level agent programming in 
tively, together with the probabilities of these actually occur•  the situation calculus. In Proc. AAAI-2000, 2000. 
ring as computed in (2.) = 0.99898, 
                                                               [Hauskrecht et al., 1998] M. Hauskrecht, N. Meuleau, L. Kael-
   Just as options in the MDP framework can be treated like       bling, T. Dean, and C. Boutilier. Hierarchical solutions of MDPs 
                                                                  using macro-actions. In Proc. UAI98, 1998. 
primitive actions in MDPs that use these options, the transla•
tion into abstract stochastic actions in the DTGOLOG frame-    [Puterman, 1994] M. Puterman. Markov Decision Processes: Dis•
work has the same effect, that is, we can now write DT•           crete Dynamic Programming. Wiley, New York, 1994. 
GOLOG programs treating abstract stochastic actions just like  [Reiter, 2001] R. Reiter. Knowledge in Action. MIT Press, 2001. 
any other primitive actions. A global policy computed by DT•   [Sutton et al., 1999] R. Sutton, D. Precup, and S. Singh. Between 
GOLOG for such a program usually mentions abstract actions.       MDPs and semi-MDPs: A framework for temporal abstraction in 
These are not readily executable (leaving a room through the     reinforcement learning. Journal of Artificial Intelligence, 1999. 


POSTER PAPERS                                                                                                        1395 