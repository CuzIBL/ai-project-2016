                              Learning to Walk through Imitation 

         Rawichote Chalodhorn, David B. Grimes, Keith Grochow, and Rajesh P. N. Rao 
                                       Neural Systems Laboratory  
                           Department of Computer Science and Engineering 
                                       University of Washington 
                                    Seattle, WA 98195-2350 U.S.A. 
                        E-mail: {choppy,grimes,keithg,rao}@cs.washington.edu 

                    Abstract                          achieve a stable walking gait. Learning approaches such as 
                                                      reinforcement learning [Sutton and Barto, 1998]  are more 
    Programming a humanoid robot to walk is a chal-
    lenging problem in robotics. Traditional ap-      flexible and can adapt to environmental change but such 
    proaches rely heavily on prior knowledge of the   methods are typically not directly applicable to humanoid 
    robot's physical parameters to devise sophisticated robots due to the curse of dimensionality problem engen-
    control algorithms for generating a stable gait. In dered by the high dimensionality of the full-body joint space 
    this paper, we provide, to our knowledge, the first of the robot.  
    demonstration that a humanoid robot can learn to       
    walk directly by imitating a human gait obtained         Motion tracking         Low dimensional subspace 
    from motion capture (mocap) data. Training using                                     representation 
    human motion capture is an intuitive and flexible 
    approach to programming a robot but direct usage 

    of mocap data usually results in dynamically un-                      
    stable motion. Furthermore, optimization using          Kinematics mapping
    mocap  data in the humanoid full-body joint-space 
    is typically intractable. We propose a new model-
    free approach to tractable imitation-based learning  Learning of a complex task  Robot dynamics compensa-
                                                                                               
    in humanoids. We represent kinematic information                                        tion
    from human motion capture in a low dimensional 
    subspace and map motor commands in this low-
    dimensional space to sensory feedback to learn a 
    predictive dynamic model. This model is used 
    within an optimization framework to estimate op-
    timal motor commands that satisfy the initial kine- Figure 1. A framework for learning human behavior by imitation 
    matic constraints as best as possible while at the through sensory-motor mapping in reduced dimensional spaces. 
    same time generating dynamically stable motion. 
    We demonstrate the viability of our approach by       In this paper, we propose a model-free approach to 
    providing examples of dynamically stable walking  achieving stable gait acquisition in humanoid robots via 
    learned from mocap data using both a simulator    imitation. The framework for our method is shown in Figure 
    and a real humanoid robot.                        1. First, a motion capture system transforms Cartesian posi-
                                                      tion of markers attached to the human body to joint angles 
1 Introduction                                        based on kinematic relationships between the human and 
                                                      robot bodies. Then, we employ dimensionality reduction to 
Imitation is an important learning mechanism in many bio- represent posture information in a compact low-dimensional 
logical systems including humans [Rao and Meltzoff, 2003]. subspace. Optimization of whole-body robot dynamics to 
It is easy to recover kinematic information from human mo- match human motion is performed in the low dimensional 
tion using, for example, motion capture, but imitating the space. In particular, sensory feedback data are recorded 
motion with stable robot dynamics is a challenging research from the robot during motion and a causal relationship be-
problem.  Traditional model-based approaches based on tween actions in the low dimensional posture space and the 
zero-moment point (ZMP) [Vukobratovic and Borovac,    expected sensory feedback is learned. This learned sensory-
2004], [Kajita and Tani, 1996]  or the inverted pendulum motor mapping allows humanoid motion dynamics to be
model [Yamaguchi et al., 1996]  require a highly accurate optimized. An inverse mapping from the reduced space back 
model of robot dynamics and the environment in order to to the original joint space is then used to generate optimized 


                                                IJCAI-07
                                                  2084motion on the robot. We present results demonstrating that analysis (PCA) to parameterize the low-dimensional motion 
the proposed approach allows a humanoid robot to learn to subspace X . Although nonlinear dimensionality reduction 
walk based solely on human motion capture without the methods could be used (e.g., [MacDorman et al., 2004], 
need for a detailed physical model of the robot.      [Grochow et al., 2004]), we found the standard linear PCA 
                                                      method to be sufficient for the classes of motion studied in 
2  Human Motion Capture and Kinematic                 this paper.  
    Mapping                                             The result of linear PCA can be thought of as two linear 
                                                      operators C and C-1 which map from high to low and low to 
                                                      high dimensional spaces respectively. The low dimensional 
     
                                                      representation of the joint angle space of the HOAP-2 robot 
                                                      executing a walking gait (in the absence of gravity) is 
                                                      shown in Figure 3. 

                                                                        4


                                                             4                                  1

Figure 2. Kinematic mapping used in our approach (from left to 2
right: human body, human skeleton, robot skeleton, and robot 
body, respectively).                                         0

In this paper, we manually map the joint angle data from a  -2
motion capture system to a kinematic model for a Fujitsu    -4
HOAP-2 humanoid robot. To generate the desired motion       5

sequence for the robot, we capture example motions from a 3
                                                                                                2
human subject and map these to the joint settings of the ro-      0
                                                                                                -4
bot.  Initially, a set of markers is attached to the human sub-                           -2
                                                                                    0
ject and the 3-D positions of these markers are recorded for                   2
                                                                      -5 4
each pose during motion.  We use a Vicon optical system 
running at 120Hz and a set of 41 reflective markers.  These Figure 3. Posture subspace and example poses. A 3-dimensional 
recorded marker positions provide a set of Cartesian points reduced space representation of the postures of the HOAP-2 robot 
in the 3D capture volume for each pose.  To obtain the final during a walking motion. We applied linear PCA to 25 dimensions 
subject poses, the marker positions are then assigned as po- of joint angle data of the robot that mapped from a human kine-
sitional constraints on a character skeleton to derive the joint matic configuration as described in Section 2. Blue diamonds 
angles using standard inverse kinematics (IK) routines. along the function approximated trajectory represent different 
   As depicted in Figure 2, in order to generate robot joint robot postures during a single walking cycle. Red circles mark 
                                                      various example poses as shown in the numbered images.  
angles, we simply replace the human subject’s skeleton with  
a robot skeleton of the same dimensions.  For example, the We use a straightforward, standard linear PCA method to 
shoulders were replaced with three distinct 1-dimensional map between the low and high-dimensional posture spaces. 
rotating joints rather than one 3-dimensional ball joint.  The Vectors in the high-dimensional space are mapped to the 
IK routine then directly generates the desired joint angles on low-dimensional space by multiplication with the transfor-
the robot for each pose.  There are limitations to such a mation matrix C . The rows of C  consist of the eigenvec-
technique (e.g, there may be motions where the robot’s 
                                                      tors, computed via singular value decomposition (SVD), of 
joints cannot approximate the human pose in a reasonable 
                                                      the motion covariance matrix. SVD produces transformed 
way), but since we are interested only in classes of human 
                                                      vectors whose components are uncorrelated and ordered 
motion that the robot can handle, this method proved to be a 
                                                      according to the magnitude of their variance.  
very efficient way to generate large sets of human motion 
                                                        For example, let q =×21 1 vector of joint angles (the 
data for robotic imitation. 
                                                      high-dimensional space) and p =×31 vector in 3D space. 
3   Sensory-Motor Representations                     We can calculate p  in 3D space by using r=Cq, where r
                                                      is a 21× 1vector of all principal component coefficients of 
3.1 Low-Dimensional Representation of Postures        q  and C  is the 21× 21 transformation matrix.  We then 
Particular classes of motion such as walking, kicking, or pick the first three elements of r  (corresponding to the first 
reaching for an object are intrinsically low-dimensional. We three principal components) to be p . The inverse mapping 
apply the well known method of principal components   can be computed likewise using the pseudo inverse of C . 


                                                IJCAI-07
                                                  2085                                                                              ∑   (xˆˆ ii+1 × x  )
                                                                                 i
                                                                         zθ =               .     (1) 
3.2 Action Subspace Embedding                                                 ∑   (xˆˆ ii+1 × x  )
High-level control of the humanoid robot reduces to select-                      i
                                                                                                        i
ing a desired angle for each joint servo motor. As discussed Next, xθ is chosen to align with the maximal variance of x
previously, operations in the full space of all robot joint 
                                                      in a plane orthogonal to zθ . Finally, yθ is specified as or-
angles tend to be intractable. We leverage the redundancy of 
the full posture space and use the reduced dimensional sub- thogonal to xθ and zθ . The final embedded training data is 
space X  to constrain target postures. Any desired posture obtained by cylindrical conversion to (,r,h)φ where r is the 
(also referred to as an action) can be represented by a radial distance, h the height above the xyθθ−  plane, and 
point a ∈ X .  
                                                      φ  the angle in the xy− plane. The angle φ  can also be 
  A periodic movement such as walking is represented by a                θθ
loop in X as shown in Figure 4. In the general case, we con- interpreted as the phase angle of the motion.  
sider a non-linear manifold representing the action space Given the loop topology of the latent training points, one 
A ⊆ X . Non-linear parameterization of the action space can parameterize r  and h  as a function of φ . The embed-
allows further reduction in dimensionality. We embed a one ded action space is represented by a learned approximation 
dimensional representation of the original motion in the of the function: 
three dimensional posture space and use it for constructing a            []r,h= g (φ)                                     (2) 
constrained search space for optimization as discussed in 
Section 5. Using the feature representation of the set of ini- where 0 ≤≤φ 2π . Approximation of this function is per-
tial training examples x=ii Cz, we first convert each point formed by using a radial basis function (RBF) network. 
to its representation in a cylindrical coordinate frame. This 
is done by establishing a coordinate frame with three basis 4 Learning to Predict Sensory Consequences 
directions x,y,zθθθin the feature space. The zero point of 
the coordinate frame is the empirical mean μ of the data   of Actions 
points in the reduced space. We recenter the data around this A central component of our proposed framework is learning 
new zero point and denote the resulting data xˆ i .   to predict future sensory inputs based on actions and using
                                                      this learned predictive model for action selection. The goal 
                                                      is to predict the future sensory state of the robot, denoted 
                                                      by s  . In general, the state space S=Z ×Ρ  is the Cartesian 
     4                                                    t+1
                                                      product of the high-dimensional joint space Z  and the 
     2                                                space of other percepts Ρ . Other percepts could include, for 
                                                      example, a torso gyroscope, an accelerometer, and foot 
     0                                                pressure sensors as well as information from camera images. 
                                                      The goal then is to learn a function F : SA×  S that maps 
    -2                       xθ 
                       yθ   j                         the current state and action to the next state. For this paper, 
                        o                             we assume that F  is deterministic. 
    -4
    5                     zθ                              Often the perceptual state st  is not sufficient for pre-
                                                      dicting future states. In such cases, one may learn a higher 
                                                      order mapping based on a history of perceptual states and 
           0
                                              -4      actions, as given by an n-th  order Markovian function: 
                                        -2
                                 0
                          2
                -5 4                                               st = F (s t-n ,...,s t-1 ,a t-n ,...,a t-1 )          (3) 

Figure 4. Embedded action space of a humanoid walking gait. We use a radial basis function (RBF) approximator to 
Training data points in the reduced posture space (shown in blue- learn F from sensory-motor experience. In particular, the 
dots) are converted to a cylindrical coordinate frame relative to the RBF network approximates F by learning a function 
coordinate frame x,y,zθθθ. The points are then represented as a F' : α   β : 
function of the angle φ, which forms an embedded action space 
(shown in red-solid-curve). This action space represents a single  K
                                                                                        −1
gait cycle.                                                    β = ∑∑w  exp−− (αμ)(T      αμ −  ) ,       (4) 
                                                                       kk(              k      k)
                                                                   k
   We then compute the principal axis of rotation zθ :                                      μ      Σ−1
                                                      where K represents the number of kernels, k and k are 
                                                      the mean and inverse covariance of thek -th kernel respec-


                                                IJCAI-07
                                                  2086           tively. The output weight vector wk  scales the output of   The objective function used in this paper is a measure 
           each kernel appropriately, and the input and output are  of torso stability as defined by the following function of 
                                                           gyroscope signals:   
           α =[] s , s ,...,s ,a ,a ,...,a and β = s  respec-
                t t-1  t-n-1 t t-1 t-n-1      t+1           
           tively. For convenience, one can instead view the RBF as a 
                                                                         Γ++(ω) = λ ω222λ ω λ ω ,       (6) 
           time delay network [Lang et al., 1990] for which the input           xx    yy   zz
                    α                                       
           simplifies to  = []s,att. The previous state and action in-
                                                           where ω ,ω ,ω  refer to gyroscope signals in 
           puts are implicitly remembered by the network using recur- xyz
                                                                                            λ  λ λ
           rent feedback connections.                      the x, y, z axes respectively. The constants xyz, ,  allow 
              In this paper, we use a second-order (n = 2) RBF net- one to weight rotation in each axis differently. The objective 
           work with the state vector equal to the three-dimensional function (6) provides a measure of stability of the posture 
                          ≡ ω
           gyroscope signal ()stt. As discussed in the previous during motion. For our second-order predictive function F, 
           section, an action represents the phase angle, radius, and the optimization problem becomes one of searching for op-
                                           ≡∈χ  X
           height of the data in latent posture space ()at .  timal stable actions given by:  
                                                            
           5  Motion Optimization using the Learned                    χ*  = arg minΓ()F (ω , ω , χ ,χ )      (7) 
                                                                        ttt-tt-χ ∈        11
               Predictive Model                                               t
                                                                                     ⎡⎤φ
           The algorithm we present in this section utilizes optimiza-                 s
                                                                                   = ⎢⎥
           tion and sensory prediction to select optimal actions and                 ⎢⎥rs         (8) 
                                                                                     ⎢⎥
           control the humanoid robot in a closed-loop feedback sce-                 ⎣⎦hs
           nario. Figure 5 illustrates the optimization process.  To allow for efficient optimization, we restrict the search 
            One may express the desired sensory states that the robot space to a local region in the action subspace as given by: 
           should attain through an objective function Γ()s . Our algo-      φ   φφ≤+    ε
                                                                              tt-1< s -1  φ       (9) 
           rithm then selects actions aa**,..., such that the predicted       εε≤≤
                                tT                                         r-arsa r  r+  r      (10) 
           future states ss,..., will be optimal with respect to  Γ()s :       εε≤≤
                     tT                                                     h-ah   h sa h+ h     (11) 
                    *        Γ                                                      ε    π
                   assaat = arg min()F ( t ,..., t-n , t ,..., t-n ) .      (5) 0<   φ <2       (12) 
                           a
                            t                                                 [](r,h= gφ )       (13) 
                                                                                aa      s
                             Controller                     
                                                                                             φ
                                                           The phase motion command search range s begins after 
                              χ′            ωp
            ωmin                                           the position of the phase motion command at the previous 
                    Optimization
                                  Model Predictor          time step φ . The radius search range r begins from a 
                    Algorithm                                       t-1                     s
                                                           point in the action subspace embedding A that is defined by 
                                                           (12) in both positive and negative directions from ra  along 
                             χ                 ω
                                                a                        ε >
                                                           r  for the distance r 0 . The search range hs  is defined in 
                                  Humanoid Robot
                                            Gyroscope signal the same manner as r according to h and ε . In the ex-
                                                                            s           a    h
                                                           periments, the parameters εφ , ε and ε  were chosen to 
            ω                       χ                                               r     h
            min = minimum gyroscope signal = posture command
           ωa      = actual gyroscope signal               ensure efficiency while at the same time allowing a reason-
                                    χ′
           ωp      = predicted gyroscope signal = tentative posture command able range for searching for stable postures. An example of 
                                                           the search space for a walking motion is shown in Figure 6. 
                                                              Selected actions will only truly be optimal if the sen-
           Figure 5. Model predictive controller for optimizing posture stabil-
           ity. The optimization algorithm and the sensory-motor model pre- sory-motor predictor is accurate. We therefore periodically 
                                                           re-train the prediction model based on the new posture com-
           dictor produce the action ()a ≡∈χ X which is used for posture 
                              t                            mands generated by the optimization algorithm and the sen-
           control of the humanoid robot. The resulting gyroscope signal is sory feedback obtained from executing these commands. 
           fed back to the predictor for retraining. The optimization algorithm After three iterations of sensory-motor prediction learning, 
           utilizes a predicted gyroscope signal ω in order to optimize actions 
                                    p                      an improved dynamically balanced walking gait is obtained. 
           for posture stability.                          The trajectory of the optimized walking gait in the low di-
                                                           mensional subspace is shown in Figure 6.  
                                                              
                                                            


                                                      IJCAI-07
                                                       2087                                                   7)  Repeat steps 4 through 6 until a satisfactory gait is 
    4                              Original data      obtained. 
                                   Embed function
        Search space
    2                              Optimized data 6 Experimental Results 

                                                              Target pattern
    0

    -2                                              Final result   Scale 0.7 
                                                                           Scale 0.5 
                                                                                  Scale 0.3
    -4
   -5

                                                      4
                                                                                            5
       0                                              2

                                                      0
                                          4
                           0      2
          5         -2                                                                   0
            -4                                       -2

                                                     -4

Figure 6. Optimization result for a walking motion pattern in a 4 3
                                                               2   1
low-dimensional subspace based on an action subspace embed-            0   -1         -5
                                                                               -2   -3
ding. 
We summarize below the entire optimization and action Figure 7. Motion pattern scaling. The target motion pattern is 
selection process:                              scaled down until it can produce a stable motion to start the motion 
   1)  Use PCA to represent in a reduced 3D space the ini- optimization process. 
     tial walking gait data from human motion capture. 
   2)  Employ the non-linear embedding algorithm for pa- This section explains how the optimization methodology in 
     rameterization of the gait.                the previous section is used in conjunction with the mocap 
   3)  Start the learning process by projecting actions back data. From our study of the motion pattern in the reduced 
     to the original joint space and executing the corre- subspace, we found that we can scale up and down the mo-
     sponding sequence of servo motor commands in the tion pattern and get similar humanoid motion patterns ex-
     Webots HOAP-2 robot simulator [Webots, 2004]. cept for changes in the magnitude of motion. When we scale 
                                                down the pattern in the reduced subspace, it produces a 
   4)  Use the sensory and motor inputs from the previous 
                                                smaller movement of the humanoid robot, resulting in s-
     step to update the sensory-motor predictor as de-
                                                maller changes in dynamics during motion. Our strategy is 
     scribed in Section 4 where the state vector is given to scale down the pattern until we find a dynamically stable 
     by the gyroscope signal of each axis and the action motion and start learning at that point. We apply the motion 
                φ
     variables are ,r and h  in the low-dimensional sub- optimization method in Section 5 to the scaled-down pattern 
     space.                                     until its dynamic performance reaches an optimal point; 
   5)  Use the learned model to estimate actions according then we scale up the trajectory of the optimization result 
     to the model predictive controller framework de- toward the target motion pattern. In our experiments, we 
     scribed above (Figure 5).                  found that a scaling down of 0.3 of the original motion pat-
   6)  Execute computed actions and record sensory (gyro- tern is typically stable enough to start the learning process.  
     scope) feedback.                           Our final optimization result obtained using this procedure 
                                                is shown as a trajectory of red circles in Figure 7. It corre-
                                                sponds to about 80% of the full scale motion. 


                                           IJCAI-07
                                            2088