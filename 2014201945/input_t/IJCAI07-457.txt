                       A Fusion of Stacking with Dynamic Integration 

                                    Niall Rooney, David Patterson 
                          Northern Ireland Knowledge Engineering Laboratory 
                              Faculty of Engineering, University of Ulster 
                            Jordanstown, Newtownabbey, BT37 OQB, U.K. 
                                {nf.rooney, wd.patterson}@ulster.ac.uk 


                    Abstract                          important that the base learning models are sufficiently ac-
                                                      curate and diverse in their predictions [Krogh & Vedelsby, 
                                                      1995], where diversity in terms of regression is usually 
                                                      measured by the ambiguity or variance in their predictions. 
      In this paper we present a novel method that    To achieve this in terms of homogeneous learning, methods 
      fuses the ensemble meta-techniques of Stack-    exist that either manipulate, for each model, the training 
      ing and Dynamic Integration (DI) for regres-    data through instance or feature sampling methods or ma-
      sion problems, without adding any major         nipulate the learning parameters of the learning algorithm. 
                                                      Brown et al.[2004] provide a recent survey of such ensem-
      computational overhead. The intention of the 
                                                      ble generation methods. 
      technique is to benefit from the varying per-
                                                      Having generated an appropriate set of diverse and suffi-
      formance of Stacking and DI for different       ciently accurate models, an ensemble integration method is 
      data sets, in order to provide a more robust    required to create a successful ensemble. One well–known 
      technique. We detail an empirical analysis of   approach to ensemble integration is to combine the models 
      the technique referred to as weighted Meta –    using a learning model itself. This process, referred to as 
      Combiner (wMetaComb) and compare its per-       Stacking [Wolpert, 1992], forms a meta-model created from 
      formance to Stacking and the DI technique of    a meta-data set. The meta-data set is formed from the pre-
      Dynamic Weighting with Selection. The em-       dictions of the base models and the output variable of each 
      pirical analysis consisted of four sets of ex-  training instance. As a consequence, for each training in-
      periments where each experiment recorded        stance I =<xy, >  belonging to training set D , meta-
                                                                 =<ˆˆ            >                 ˆ
      the cross-fold evaluation of each technique for instance MIfxfxy1 (),..N (), is formed, where f i ()x is 
      a large number of diverse data sets, where      the prediction of base model ii,1..= Nwhere N is the size of 
      each base model is created using random fea-    the ensemble. To ensure that the predictions are unbiased, 
      ture selection and the same base learning al-   the training data is divided by a J -fold cross-validation 
      gorithm. Each experiment differed in terms of   process into J subsets, so that for each iteration of a cross-
      the latter base learning algorithm used. We     validation process, one of the subsets is removed from the 
      demonstrate that for each evaluation, wMeta-    training data and the models are re-trained on the remaining 
      Comb was able to outperform DI and Stack-       data. Predictions are then made by each base model on the 
      ing for each experiment and as such fuses the   held out sub-set.  The meta-model is trained on the accumu-
      two underlying mechanisms successfully.         lated meta-data after the completion of the cross validation 
                                                      process and each of the base models is trained on the whole 
                                                      training data. Breiman[1996b] showed the successful appli-
                                                      cation of this method for regression problems using a linear 
1 Introduction                                        regression method for the meta-model where the coeffi-
                                                      cients of the linear regression model were constrained to be 
The objective of ensemble learning is to integrate a number non-negative. However there is no specific requirement to 
of base learning models in an ensemble so that the generali- use linear regression as the meta-model [Hastie et al., 2001] 
zation performance of the ensemble is better than any of the and other linear algorithms have been used particularly in 
individual base learning models. If the base learning models the area of classification [Seewald, 2002, Džeroski, & 
are created using the same learning algorithm, the ensemble Ženko, 2004]. 
learning schema is homogeneous otherwise it is heterogene- A technique which appears at first very similar in scope to 
ous. Clearly in the former case, it is important that the base Stacking (and can be seen as a variant thereof) is that pro-
learning models are not identical, and theoretically it has posed by Tsymbal et al.[2003] referred to as Dynamic Inte-
been shown that for such an ensemble to be effective, it is gration. This technique also runs a J-fold cross validation 


                                                IJCAI-07
                                                  2844process for each base model, however the process by which 2 Methodology 
it creates and uses meta-data is different. Each meta-
instance consists of the following format In this section, we describe in detail the process of wMeta-
<>                                                    Comb. The training phase of wMetaComb is shown in Fig-
  x,Err1 ( x ),.., ErrN ( x ), y  where Erri () x  is the predic-
tion error made by each base model for each training in- ure 1..  
stance <>x, y . Rather than forming a meta-model based 
on the derived meta-data, Dynamic Integration uses a k- Algorithm : wMetaComb: Training Phase
nearest neighbour (k-NN) lazy model to find the nearest Input: D
                                                              ˆ SR ˆ DI ˆ
neighbours based on the original instance feature space Output: f , f , fii,1..= N
(which is a subspace in the meta-data) for a given test in- (* step 1 *) 

stance. It then determines the total training error of the base partition D  into J–fold partitions DiJ=1..
models recorded for this meta-nearest neighbour set. The initialise meta-data set MDSR =∅
level of error is used to allow the dynamic selection of one 
                                                         initialise meta-data set MD DI =∅
of the base models or an appropriate weighted combination For i = 1..J-2 
of all or a select number of base models to make a predic-  D   = DD\
tion for the given test instance. Tsymbal et al. [2003] de-   train  i
                                                            DD=
scribes three Dynamic Integration techniques for classifica-  test i
                                                                             ˆ
tion which have been adapted for regression problems        build N learning models fii,1..= N  using Dtrain
[Rooney et al., 2004]. In this paper we consider one variant             <>
                                                            For each instance x, y in Dtest
of these techniques referred to as Dynamic Weighting with 
                                                        Form meta-instance MI     SR : <>ffyˆˆ(),..,xx (),
Selection (DWS) which applies a weighting combination to                               1     N
                                                                                  DI <>
the base models based on their localized error, to make a   Form meta-instance MI   : xx,Err1 ( ),.., ErrN ( x ), y
prediction, but excludes first those base models that are   Add MI      SR to MDSR
deemed too inaccurate to be added to the weighted combina-   Add MI     DI to MD DI
tion.                                                     EndFor
A method of injecting diversity into a homogeneous ensem- Endfor
ble is to use random subspacing [Ho, 1998a, 1998b]. Ran- Build SR meta-model fˆ SR using the meta-data set MDSR
dom subspacing is based on the process whereby each base 
                                                         Build  DI meta-model fˆ DI using the meta-data set MD DI
model is built with data with randomly subspaced features. 
Tsymbal et al. [2003] revised this mechanism so that a vari- (* step 2 *) 
                                                         i=J 
able length of features are randomly subspaced. They shown 
                                                          D  = D
this method to be effective in creating ensembles that were test i

integrated using DI methods for classification.          Determine TotalErrSR and TotalErrDI using Dtest
It has been shown empirically that Stacking and Dynamic  (* step 3 *) 
Integration based on integrating models, generated using i = J-1 
                                                             =
random subspacing, are sufficiently different from each   DtrainDD\ i
                                                             =
other in that they can give varying generalization perform- DDtest i
ance on the same data sets [Rooney et al., 2004]. The main 
                                                          build N learning models fˆ  using D
computational requirement in both Stacking and Dynamic                      ii,1..= N train
                                                                      <>
Integration, is the use of cross-validation of the base models For each instance x, y in Dtest
to form meta-data. It requires very little additional computa-  Form meta-instance MI SR : <>ffyˆˆ(),..,xx (),
tional overhead to propose an ensemble meta-learner that                         1     N
                                                       Form meta-instance MI DI : <>xx,Err ( ),.., Err ( x ), y
does both in one cross validation of the ensemble base mod-                         1      N
els and as a consequence, is able to create both a Stacking  Add MI SR to MDSR
meta-model and a Dynamic Integration meta-model, with  Add MI     DI to MD DI
the required knowledge for the ensemble meta-learner to use EndFor
                                                                  ˆ
both models to form predictions for test instances. The focus Retrain the fii,1..= N  on D
of this paper is on the formulation and evaluation of such an Build SR meta-model fˆ SR using the meta-data set MDSR
ensemble meta-learner, referred to as weighted Meta-
                                                                         ˆ DI                 DI
Combiner (wMetaComb). We investigated whether wMeta-     Build DI meta-model f using the meta-data set MD
Comb will on average outperform both Stacking for regres- Figure 1. Training Phase of wMetaComb 
sion (SR) and DWS for a range of data sets and a range of 
base learning algorithms used to generate homogeneous The first step in the training phase of wMetaComb consists 
randomly subspaced models.                            of building SR and DI meta-models on J-1 folds of the 
                                                      training data (step 1). The second step involves testing the 
                                                      meta-models using the remaining training fold, as an indica-
                                                      tor of their potential generalization performance.  It is im-
                                                      portant to do this as even though models are not created 


                                                IJCAI-07
                                                  2845using the complete meta-data, they are tested using data that ||normError− normError
                                                           =          12
they are not trained on, in order to give a reliable estimate of T                   (3) 
                                                                      0.01
their test performance. The performance of the meta-models 
is recorded by their total absolute error, TotalError and  
                                              SR      wMetaComb forms a prediction for a test instance x by the 
TotalErrorDI within step 2. The meta-models are then re-
built using the entire meta-data completed by step 3. The following combination of the predictions made by the meta-
computational overhead of wMetaComb in comparison to  models: 
SR by itself centres only on the cost of training the addi-  
tional meta-model. However as the additional meta-model is  
based on a lazy nearest neighbour learner, this cost is trivial.     ˆˆSR+             DI
                                                         norm_ mw12 * f () x norm _ mw . f () x
The prediction made by wMetaComb is made by a simple   
weighted combination of its meta-models based on their 
total error performance determined in step 2. Denote 
TotalErr=< TotalErrorSR, TotalError DI > . The individual 3  Experimental Evaluation and Analysis 
weight for each meta-model is determined by finding the In these experiments, we investigated the performance of 
normalized error for each meta-model:                 wMetaComb in comparison to SR and DWS. The work was 
                                                      carried out based on appropriate extensions to the WEKA 
          =
normerrorii TotalError/  TotalError i                 environment, such as the implementation of DWS. The en-
                      i=1..2                          semble sets consisted of 25 base homogeneous models 
A normalized weighting  for each meta-model is given by: where the data for each base model was randomly sub-
                                                      spaced [Tsymbal et al., 2003]. Note that random subspacing 
         − normerrori                                 was a pseudo-random process so that a feature sub-set gen-
   mw=  e   T                      (1) 
     i                                                erated for each model i in an ensemble set is the same for all 
                                                      ensemble sets created using different base learning algo-
                                                      rithms and the same data set. The meta-learner for SR and 
                mw                                    the Stacking component within wMetaComb was based on 
   norm_ mw  =    i                (2)                the model tree M5 [Quinlan, 1992], as this has a larger hy-
           i     mw
                    i                                 pothesis space than Linear Regression. The number of near-
               i
                                                      est neighbours for DWS k-NN meta-learner was set to 15. 
                                                      The performance of DWS can be affected by the size of 
Equation 1 is influenced by a weighting mechanism that 
                                                       k for particular data sets, but for this particular choice of 
Wolpert & Macready [1996] proposed for combining base 
                                                      data sets, previous empirical analysis had shown that the 
models generated by stochastic process such as used in 
                                                      value of 15, gave overall a relatively strong performance. 
Bagging  [Breiman,1996a]. 
                                                      The value of J during the training phase of each meta-
The weighting parameter T determines how much relative 
                                                      technique was set to 10. 
influence to give to each meta-model based on their normal-
                                                      In order to evaluate the performance of the different ensem-
ized error e.g. suppose          normError = 0.4 and 
                                         1            ble techniques, we chose 30 data sets available from the 
          =
normError2  0.6 , then Table 1 gives the normalized   WEKA environment [Witten & Frank, 1999]. These data 
weight values for different values of T . This indicates that a sets represent a number of synthetic and real-world prob-
low value of T  will give large imbalance to the weighting, lems from a variety of different domains, have a varying 
whereas a high value of T  almost equally weights the meta- range in their attribute sizes and many contain a mixture of 
learners regardless of their difference in normalized errors. both discrete and continuous attributes. Data sets which had 
                                                      more than 500 instances, were sampled without replacement 
                                                      to reduce the size of the data set to a maximum of 500 in-
           T     norm_ mw1   norm_  mw2  
                                                      stances, in order to make the evaluation computationally 
           0.1 0.88          0.12 
                                                      tractable. Any missing values in the data set were replaced 
           1.0 0.55          0.45                     using a mean or modal technique. The characteristics of data 
           10.0 0.505        0.495                    sets are shown in Table 2. 
                                                      We carried out a set of four experiments, where each ex-
Table 1. The effect of the weighting parameter        periment differed purely in the base learning algorithm de-
                                                      ployed to generate base models in the ensemble. Each ex-
Based on these considerations, we applied the following periment calculated the relative root mean squared error 
heuristic function for the setting of T , which increases the (RRMSE) [Setiono et al., 2002] of each ensemble technique 
imbalance in the weighting dependent on how great the for each data set based on a 10 fold cross validation meas-
normalized errors differ.                             ure. 
                                                       


                                                IJCAI-07
                                                  2846                                                      of data sets where technique A outperformed technique B 
   Data set   Original  Data set Number of            significantly, losses is a count of the number of data sets 
              Data set size in  input                 where technique B outperformed technique A significantly 
              size    experi- Attributes 
                      ments   Con-   Dis-  Total      and ties a count where there was no significant difference. 
                              tinuous crete           The significance gain is the difference in the number of 
   abalone 4177  500 8               0 8              wins and the number of losses. If this zero or less, no gain 
   autohorse 205      205     15     10 25            was shown. If the gain is less than 3 this is indicative of 
   autoMpg 398        398     5      3 8              only a slight improvement. 
   autoprice 159      159     15     10 25 
   auto93 93 93 16 5 21                               We determined the average significance difference (ASD)
   bank8FM 8192 500           8      0     8          between the RRMSE of the two techniques, averaged over 
   bank32nh 8192 500          32     0     32          wins+ losses data sets only where a significant difference 
   bodyfat 252 252 14 0 14                            was shown. If technique A outperformed B as shown by the 
   breastTumor 286    286     1      8     9          significance ratio, the ASD gave us a percentage measure of 
   cal_housing 20640  500     8      0     8 
   cloud 108 108 4 2 6                                the degree by which A outperformed B (if technique A was 
   cpu 209 209 6 1 7                                  outperformed by B, then the ASD value is negative). The 
   cpu_act 8192 500 21               0 21             ASD by itself of course has no direct indication of the per-
   cpu_small 8192  500        12     0     12         formance of the techniques and is only a supplementary 
   echomonths 130     130     6      3     9          measure. A large ASD does not indicate that technique A 
   elevators 16559 500        18     0 18 
   fishcatch 158      158     5      2     7          performed considerably better than B if the difference in the 
   friedman1 40768  500       10     0     10         number of wins to losses was small. Conversely even if the 
   housing 506 500 13                0 13             number of wins was much larger than the number of losses 
   house_8L 22784 500         8      0     8          but the ASD was small, this reduces the impact of the result 
   house_16H 22784  500       16     0     16         considerably. This section is divided into a discussion of the 
   lowbwt 189 189 7                  2 9              summary of results for each individual base learning algo-
   ma-        209 209 6              0 6 
   chine_cpu                                          rithm then a discussion of the overall results.  
   pollution 60       60      15     0     15 
   pyrim 74 74 27 0 27                                3.1 Base Learner: k-NN 
   sensory 576  500 0                11 11 
   servo 167 167 4 0 4 
   sleep 62 62 7 0 7                                  Table 3 shows a summary of the results for the evaluation 
   strike 625 500 5 1 6                               when the base learning algorithm was 5-NN. Clearly all the 
   veteran 137 137 7                 0 7              ensemble techniques strongly outperformed single 5-NN 
                                                      whereby in terms of significance alone, wMetaComb shown 
Table 2. The data sets’ characteristics               the largest improvement with a gain of 22. Although SR 
                                                      showed fewer data sets with significant improvement, it had 
The RRMSE is a percentage measure. By way of compari- a greater ASD than wMetaComb. wMetaComb outper-
son of the ensemble approaches in general, we also deter- formed DWS with a gain of 7, and showed a small im-
mined the RRMSE for a single model built on the whole provement over SR, with a gain of 3. SR showed a signifi-
unsampled data, using the same learning algorithm as used cance gain of only 1 over DWS. 
in the ensemble base models. We repeated this evaluation 
four times, each time using a different learning algorithm to Technique 5-NN SR     DWS wMetaComb 
generate base models. The choice of learning algorithms   5-NN           0:11:19  0:20:10   0:22:8 
was based on a choice of simple and efficient methods in                     (-16.07) (-12.28)   (-14.47) 
order to make the study computationally tractable, but also SR       11:0:19    3:2:25 0:3:27  
to have representative range of methods with different learn-         (16.07)        (3.82) (-6.8) 
                                                          DWS        20:0:10 2:3:25     1:8:21  
ing hypothesis spaces in order to determine whether                   (12.28)  (-3.82)      (-5.28) 
wMetaComb was a robust method independent of the base     wMetaComb  22:0:8  3:0:27  8:1:21 
learning algorithm deployed.  The base learning algorithms           (14.47) (6.8)   (5.28) 
used for the four experiments were 5 nearest neighbours (5-
NN), Linear Regression (LR), Locally weighted regression 
[Atkeson et al., 1997] and the model tree learner M5. Table 3. Comparison of methods when base learning algorithm 
The results of the evaluation were assessed by performing a was 5-NN 
2 tailed paired t test (p=0.05) on the cross fold RRMSE for 
each technique in comparison to each other for each data 
set. The results of the comparison between one technique A 
and another B were then summarised in the form 
wins:losses:ties (ASD)  where wins is a count of the number 


                                                IJCAI-07
                                                  28473.2 Base Learner: LR                                  3.4 Base Learner: M5 
Table 4 shows that all ensembles techniques outperformed 
single LR, with wMetaComb showing the largest             Technique M5       SR     DWS wMetaComb 
improvement with a gain of 13 and an ASD of 11.99.        M5             3:1:26 1:6:23     0:8:22  
wMetaComb outperformed SR and DWS with a larger                               (4.4) (-3.45) (-4.51) 
improvement shown in comparison to DWS than to SR both    SR          1:3:26    2:6:22 0:11:19 
                                                                       (-4.4)        (-2.62)  (-4.48) 
in terms of significance and ASD. SR outperformed DWS     DWS         6:1:23 6:2:22    0:6:24  
with a significance gain of 7 and ASD of 5.1.                          (3.45)  (2.62)      (-3.53) 
                                                          wMetaComb   8:0:22 11:0:19 6:0:24 
                                                                       (4.51)  (4.48)  (3.53) 
    Technique LR      SR      DWS wMetaComb 
    LR          2:10:18 1:11:18      0:13:17 
                      (-11.38) (-5.56)  (-11.99)      Table 6. Comparison of methods when base learning algorithm 
    SR         10:2:18    11:4:15  0:5:25             was M5 
               (11.38)        (5.1)  (-5.58) 
    DWS        11:1:18  4:11:15   3:13:14             Table 6 shows a much reduced performance for the ensem-
               (5.56) (-5.1)         (-6.02)          bles in comparison to the single model, when the base learn-
    wMetaComb  13:0:17  5:0:25  13:3:14               ing algorithm was M5. In fact, SR showed no improvement 
               (11.99) (5.58) (6.02)                  over M5, and wMetaComb and DWS although they im-
                                                      proved on M5 in terms of significance did not show an ASD 
                                                      as large as for the previous learning algorithms. The reduced 
Table 4. Comparison of methods when base learning algorithm 
                                                      performance with M5 is indicative that in the case of certain 
was LR 
                                                      learning algorithms, random subspacing by itself is not able 
                                                      to generate sufficiently accurate base models, and needs to 
3.3 Base Learner: LWR                                 be enhanced using search approaches that improve the qual-
Table 5 shows that all ensembles techniques outperformed ity of ensemble members such as considered in [Tsymbal et 
single LWR, with wMetaComb showing the largest        al., 2005].  wMetaComb showed improvements over SR 
improvement with a significance gain of 16. wMetaComb and to a lesser degree both in terms of significance and ASD 
outperformed SR and DWS with a larger improvement     values. 
shown over DWS than SR in terms of significance and   Overall, it can be seen that wMetaComb gave the strongest 
ASD.  SR outperformed DWS with a significance gain of 4 performance of the ensembles for  3 out of 4 base learning 
and  an ASD of 6.24.                                  algorithms (5-NN, LR, M5) in comparison to the single 
                                                      model, in terms of its significance gain, and tied in compari-
                                                      son to DWS for LWR. For 3 out of the 4 learning algo-
   Technique LWR SR           DWS wMetaComb           rithms (LR, LWR, M5), wMetaComb also had the highest 
   LWR            1:9:20 0:16:14      0:16:14         ASD compared to the single model. wMetaComb outper-
                      (-13.81) (-12.28) (-12.28)      formed SR for all learning algorithms. The largest signifi-
   SR          9:1:20     6:2:22  0:5:25              cant improvement over SR was recorded with M5 with a 
                (13.81)       (6.24)  (-10.48)        significance gain of 11, and the largest ASD in comparison 
   DWS         16:0:14 2:6:22    0:9:21 
                (12.28) (-6.24)        (-8.28)        to SR was recorded with LWR of 10.48. The lowest signifi-
   wMetaComb   16:0:14 5:0:25  9:0:21                 cant improvement over SR was recorded with 5-NN, LR, 
                (15.91) (10.48)  (8.28)               and LWR with a significant gain of 5, and lowest ASD of 
                                                      4.48 with M5. 
                                                      wMetaComb outperformed DWS for all four base learning 
Table 5. Comparison of methods when base learning algorithm algorithms. The degree of improvement was larger in com-
was LWR                                               parison to DWS than SR. The largest significant improve-
Table 5 shows that all ensembles techniques outperformed ment over DWS was shown with LR with a significance 
single LWR, with wMetaComb showing the largest        gain of 10 and the largest ASD of 8.28 with LWR. The low-
improvement with a significance gain of 16. wMetaComb est improvement was shown with M5 both in terms of gain 
outperformed SR and DWS with a larger improvement     and ASD which were 6 and 3.53 respectively. In effect, 
shown over DWS than SR in terms of significance and   wMetaComb was able to fuse effectively the overlapping 
ASD.  SR outperformed DWS with a significance gain of 4 expertise of the two meta-techniques. This was seen to be 
and ASD of 6.24.                                      case independent of the learning algorithm employed. In 
                                                      addition, wMetaComb provided benefit whether overall for 
                                                      a given experiment, SR was stronger than DWS or not (as 


                                                IJCAI-07
                                                  2848