           Maintaining      Arc  Consistency     using   Adaptive    Domain     Ordering∗
                Chavalit Likitvivatanavong                         Yuanlin  Zhang
           Cork  Constraint Computation   Centre          Department  of Computer   Science
             University College  Cork, Ireland           Texas  Tech University, Texas, U.S.

                                James  Bowen    and Eugene   C. Freuder
                 Cork Constraint Computation   Centre, University College Cork,  Ireland

1  Introduction                                       Deﬁnition 1 (Negative Repeat) A  constraint   check
Solving a Constraint Satisfaction Problem (CSP) by Maintain- CXY (a, b) performed at time t during search is called a
ing Arc Consistency (MAC) [Sabin and Freuder, 1994] has negative repeat with respect to Y if and only if:
been one of the most widely-used methods. Since Arc Con- (1) CXY (a, b) = false, and
sistency (AC) is enforced at every node in a search tree, its (2) CXY (a, b) has been performed at time s where s < t, and
efﬁciency is critical to the whole algorithm. We propose a (3) b has been continuously present in the time interval (s, t).
new MAC algorithm based on AC-3.1 [Zhang and Yap, 2001]
in which the AC component is capable of starting from where 3 Adaptive Domain Ordering
it left off in its previous execution with low overhead. It has
                             2                        In this section we introduce a new MAC algorithm called
the following properties: (1) O(ed ) worst-case time com- Adaptive Domain Ordering (ADO). The key feature is the
plexity in any node and any branch of the search tree; (2) dynamic rearrangement of variable domains after each back-
O(ed) space complexity; (3) the ability to avoid a type of re- track. A pruned value is simply restored to the end of its do-
dundant constraint check called a negative repeat; (4) no re- main, rather than its initial position. This technique makes
computation and maintenance of its internal data structures the algorithm capable of avoiding all negative repeats as
upon backtrack.                                       well as requiring no maintenance on its internal data struc-
                                                      ture. Unlike AC-3.1, on which the constraint processing unit
2  Deﬁnitions                                         of ADO  is based, we associate the following two invari-
In this paper we consider CSPs with binary constraints and ants for last(X,a,Y). First, the safety invariant: there exists
use DX , n, e, and d to denote the current domain of variable no support of a in {c ∈ DY | c < last(X,a,Y)}. Sec-
X, the number of variables, the number of constraints, and ond, the prospect invariant: there exists a support of a in
the maximum domain size respectively. We also assume a do- {c ∈ DY | c ≥ last(X,a,Y)}. ADO is a propagation-oriented
main to be totally-ordered and adopt a random-access doubly backtrack search algorithm and behaves like MAC-3.1 for the
linked-list implementation. The terms head and tail denote most part. The main differences lie in the routines for re-
the boundary of a domain. A constraint check between value moving a value (remove), ﬁnding a support, and restoring
                                                              restore
a ∈ DX and value b ∈ DY is denoted by CXY (a, b).     a value (      ). When a value is removed, any last that
  A propagation-oriented backtrack search algorithm for points to it will be moved to the next value. When no sup-
CSPs is the standard depth-ﬁrst backtrack search framework port for a ∈ DX is found in DY , last(X,a,Y) will be made
augmented by some process that handles all constraint prop- to point to tail of Y . When a value is restored, all the last
agation and the maintenance of the internal data structures pointers that point to the tail will be made to point to the re-
involved. The search tree is deﬁned by associating each node stored value instead. It can be proved that, given any path in a
                                                      search tree, the worst-case aggregate complexity of remove
with a variable assignment of the algorithm. Node complexity 2
of the algorithm is the time complexity cost of the constraint is O(ed ). Due to space restriction we are not able to give de-
propagation performed at each node. Path complexity is the tailed proofs of the complexity cost and correctness of ADO.
aggregate cost for any path in the search tree, summing the An example trace of the algorithm is shown as follows.
cost of every node in succession, starting from the root, to a Example Consider DX = {a, b, c, d}, DY = {1, 2, 3, 4}
leaf. During search some constraint checks may be repeated and CXY ={(a, 1),(b, 1),(c, 2),(d, 4)} (allowed tuples). The
                                                                                    1
many times even though the constraint processing component result after the initial AC processing is shown in Figure (1.1):
is optimal in a single execution. We deﬁne the following last(X,a,Y)=1, last(X,b,Y)=1, last(X,c,Y)=2, and last(X,d,Y)=4.
type of redundant check called a negative repeat. Negative Suppose a and d are removed due to some external cause;
repeats can be troublesome for hard problems that require a their last values remain unchanged (1.2). Next, suppose 4 is
large amount of backtracking.                         removed. The result after AC processing is shown in (1.3). At
  ∗This work has received support from Science Foundation Ire- 1In truth, we only enforce directional AC on the arc (X,Y ) to
land under Grant 00/PI.1/C075.                        keep the example simple.   a  b c d     a  b c d     a  b c d     a  b c d     a  b c d      a b c d      a b c d      a b c d


   1  2 3 4     1  2 3 4     1  2 3 4     1  2 3 4     1  2 3 4      2 3 1 4      2 3 1 4      2 3 1 4
       u            u            u            u            u            u            u            u
     (1.1)        (1.2)        (1.3)        (1.4)        (1.5)        (1.6)        (1.7)        (1.8)

  X   buffer buffer buffer buffer buffer X buffer buffer buffer buffer buffer X buffer buffer buffer buffer buffer X buffer buffer buffer buffer buffer

 head a last b last c last d last tail head a last b last c last d last tail head a last b last c last d last tail head a last b last c last d last tail


  Y   buffer buffer buffer buffer buffer Y buffer buffer buffer buffer buffer Y buffer buffer buffer buffer buffer Y buffer buffer buffer buffer buffer

 head 4 last 2 last 3 last 1 last tail head 4 last 2 last 3 last 1 last tail head 1 last 2 last 3 last 4 last tail head 1 last 2 last 3 last 4 last tail
          (2.1)                     (2.2)                        (2.3)                     (2.4)
the next search level suppose 1 is removed. According to the for each value, thereby enforcing arc consistency; (2) the
algorithm, any last that points to 1 — including last(X,a,Y) safety invariant states that no support is skipped. Note that
—  will be shifted to 2 (1.4). Figure (1.5) shows the result because the problem is made arc consistent before the search
after the problem is made arc consistent. Now consider the starts, we can always expect a value to have a support. If a
network after backtrack. Since 1 is removed after 4, it must value is subsequently removed, it is only because all of its
be restored before. Figure (1.6) shows the result after 1 is re- supports are pruned; these supports are still in the original
stored. All the last pointers that pointed to tail are moved to domain. The existence of a support in the original domain is
1. Notice that b is restored as well because it was pruned at central in the correctness proofs, and without the AC prepro-
the same level as 1. Figure (1.7) shows the result after 4 is re- cessing before the search starts ADO would not be correct.
stored. Figure (1.8) shows the network after the search back-
tracks to the point where we started this example. Note how 4 Conclusions
the last pointers and the domain ordering differ from (1.1). 2 We have designed ADO to explore the theoretical limits of
  To make ADO efﬁcient, we need another data structure as- MAC and, as far as we know, it achieves the best results and
sociated with each value to account for the last pointers that outperforms all other existing MAC algorithms. Speciﬁcally,
                                                                  2
point to it. This is called buffer. Its elements can be preallo- ADO has O(ed ) worst-case time complexity in any node and
cated since a value has only one last pointer in any constraint. any branch of the search tree while using only O(ed) space.
We then make a last pointer refer to a value’s buffer instead This resolves the trade-off between the two traditional imple-
of the value itself — i.e. (a, X) ∈ buffer(b,Y) iff last(X,a,Y) mentations of MAC-3.1. The ﬁrst one records every change
points to buffer(b,Y). This allows a set of last pointers to be made to last so that after backtrack the algorithm can start
switched all at once just by rearranging related buffers. For from the exact same state. Although both node and path com-
example, consider Figure (2.1), a more detailed view of (1.5). plexity for this approach is O(ed2), its space complexity is
When 1 is restored, we want to move all the pointers from tail O(ed min(n, d)). The second approach resets last in every
to 1. This can be done simply by swapping the two buffers search node to keep the space at O(ed), but this comes at the
(2.2). As a result restore takes O(1) time.           expense of path complexity, which becomes O(ned2).
  We use a similar technique for remove. We compare the Regin´ [2004] also aims to create a maintenance-free MAC
buffer of the value to be removed with that of the next value algorithm that has the best features from the two implemen-
in the current domain (or tail if there is none) and swap both tations of MAC-3.1. The algorithm in [Regin,´ 2004] is both
buffers if the ﬁrst contains more elements. For an exam- node and path optimal while using O(ed) space. However,
ple, consider Figure (2.3), which is the detailed view of Fig- the last structure needs to be recomputed and updated after
ure (1.3). When value 1 is removed, its buffer size is com- each backtrack besides the normal restoration of pruned val-
pared with the buffer size of value 2. Since it has more ele- ues. It cannot avoid negative repeats. By contrast, ADO re-
ments, we move pointers from the buffer for value 2 into that quires no recomputation and no update of its internal structure
for value 1 and swap both buffers, which results in (2.4). and is able to avoid negative repeats.
  Given any path in a search tree, the worst-case aggregate
complexity of remove using buffer with the above techniques References
is O(ed lg d). We can further reduce the cost by represent- [R´egin, 2004] J.-C. R´egin. Maintaining arc consistency algorithms
ing data in a buffer as a rooted tree and having the root of during the search with an optimal time and space complexity.
the smaller tree point to that of the larger one. In fact, this is In CP-04 Workshop on Constraint Propagation and Implemen-
the union-by-rank operation for the disjoint-set union prob- tation, 2004.
lem. remove then takes O(1) time. However, locating the [Sabin and Freuder, 1994] D. Sabin and E. C. Freuder. Contradict-
value of a last pointer no longer takes constant time. By us- ing conventional wisdom in constraint satisfaction. In Proceed-
ing path compression, its worst-case aggregate complexity is ings of ECAI-94, pages 125–129, 1994.
         2
O(ed α(ed )) where α is the inverse Ackerman’s function, [Tarjan, 1975] R. E. Tarjan. Efﬁciency of a good but not linear set
which is almost constant [Tarjan, 1975].                union algorithm. Journal of ACM, 22(2):215–225, 1975.
               2
  ADO  has O(ed ) for node and path complexity since the [Zhang and Yap, 2001] Y. Zhang and R. H. C. Yap. Making AC-3
cost related to buffer is subsumed by the standard cost of es- an optimal algorithm. In Proceedings of IJCAI-01, pages 316–
tablishing arc consistency. Correctness follows because: (1) 321, 2001.
the prospect invariant states that there is a support somewhere