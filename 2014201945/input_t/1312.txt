              Sequential Genetic Search for Ensemble Feature Selection 

         Alexey Tsymbal              Mykola Pechenizkiy      Pádraig Cunningham   
    Dept. of Computer Science          Dept. of CS & ISs         Dept. of Computer Science  
       Trinity College Dublin      University of Jyväskylä     Trinity College Dublin    
         Dublin 2, Ireland         P.O. Box 35, Finland-40351         Dublin 2, Ireland     
         tsymbalo@cs.tcd.ie        mpechen@cs.jyu.fi    Padraig.Cunningham@cs.tcd.ie 

                  Abstract                      sets used to generate the base classifiers, it is possible to 
                                                promote diversity and produce base classifiers that tend to 
   Ensemble learning constitutes one of the main di-
                                                err in different sub-areas of the instance space. 
   rections in machine learning and data mining. En- Feature selection algorithms, including ensemble feature 
   sembles allow us to achieve higher accuracy, which selection, are typically composed of the following compo-
   is often not achievable with single models. One 
                                                nents [Aha and Bankert, 1995, Opitz, 1999]: (1) search 
   technique, which proved to be effective for con- strategy, that searches through the space of feature subsets; 
   structing an ensemble of diverse classifiers, is the and (2) fitness function, that inputs a feature subset and out-
   use of feature subsets. Among different approaches 
                                                puts a numeric evaluation. The search strategy’s goal is to 
   to ensemble feature selection, genetic search was find a feature subset maximizing this function. 
   shown to perform best in many domains. In this It is reasonable to include in the fitness function, explic-
   paper, a new strategy GAS-SEFS, Genetic Algo-
                                                itly or implicitly, both accuracy and diversity. One measure 
   rithm-based Sequential Search for Ensemble Fea- of fitness, which was proposed in [Opitz, 1999], defines 
   ture Selection, is introduced. Instead of one genetic fitness Fitness  of classifier i corresponding to feature subset 
   process, it employs a series of processes, the goal     i
                                                i to be proportional to classification accuracy acci and diver-
   of each of which is to build one base classifier. Ex- sity div  of the classifier: 
   periments on 21 data sets are conducted, comparing i
   the new strategy with a previously considered ge-       Fitnessi = acci + α ⋅ divi , (1) 
   netic strategy for different ensemble sizes and for 
                                                where α  reflects the influence of diversity. Diversity divi is 
   five different ensemble integration methods. The the contribution of classifier i to the total ensemble diver-
   experiments show that GAS-SEFS, although being sity, which can be measured as the average pairwise diver-
   more time-consuming, often builds better ensem- sity for all the pairs of classifiers including i. This fitness 
   bles, especially on data sets with larger numbers of function was also used in experiments in [Tsymbal et al., 
   features.                                    2003; 2005], and it is used in the experiments in this paper. 
                                                  In [Tsymbal et al., 2005] a genetic search-based strategy 
1 Introduction                                  GA has been introduced. It uses genetic search for evolving 
A popular method for creating an accurate model from a set the initial population built with random subspacing. GA was 
of training data is to construct a set (ensemble) of classifiers. shown to perform best on average with respect to the other 
It was shown that an ensemble is often more accurate than three strategies, and two diversity measures were best for 
any of the single classifiers in it. The integration of classifi- GA of the five considered measures: the kappa statistic, and 
ers is currently an active research area in the machine learn- the fail/non-fail disagreement. 
ing and neural networks communities [Dietterich, 1997]. In this paper, we introduce a new genetic search-based 
 Both theoretical and empirical research have demon- strategy for ensemble feature selection, GAS-SEFS, which, 
strated that a good ensemble should include diverse base instead of maintaining a set of feature subsets in each gen-
classifiers. Another important issue in creating an effective eration like in GA, consists in applying a series of genetic 
ensemble is the choice of the function for combining the processes, one for each base classifier, sequentially. 
predictions of the base classifiers. It was shown that increas- The paper is organized as follows. In Section 2 the task of 
ing the ensemble diversity is not enough to ensure increased ensemble feature selection is considered. In Section 3 we 
accuracy – if the integration method does not properly util- present two strategies for genetic ensemble feature selec-
ize the ensemble diversity, then no benefit arises from the tion, GA and GAS-SEFS, and two diversity measures. In 
integration [Brodley and Lane, 1996].           Section 4 different methods for ensemble integration are 
 One effective approach for generating an ensemble of di- reviewed. In Section 5 we present our experiments with the 
verse classifiers is the use of feature subsets, or ensemble two genetic strategies and conclude in the next section with 
feature selection [Opitz, 1999]. By varying the feature sub- a summary and assessment of further research topics.   2  Ensemble Feature Selection and Random        cluding in the fitness function penalty terms accounting for 
                                                 the number of features). The use of individual accuracy and 
    Subspacing                                   diversity as in (1) is an alternative solution to this problem. 
 The task of using an ensemble of models can be broken Another motivation for this alternative is the fact that over-
 down into two basic questions: (1) what set of models fitting at the level of the base classifiers is more desirable 
 should be generated?; and (2) how should the predictions of than overfitting of the ensemble itself. It was shown recently 
 the models be integrated? [Dietterich, 1997].   in several studies that an ensemble of overfitted members 
  One effective approach to ensemble generation is the use might often be better than an ensemble of non-overfitted 
of different subsets of features for each model. Finding a set members. For example, in [Street and Kim, 2001] pruning 
of feature subsets for constructing an ensemble is also trees resulted in decreased ensemble accuracy, even though 
known as ensemble feature selection [Opitz, 1999]. While the accuracy of the trees themselves increased. 
traditional feature selection algorithms have the goal of The Genetic Algorithm for ensemble feature selection 
finding the best feature subset that is suitable to both the (GA) [Tsymbal et al., 2005] is based on the GEFS strategy 
learning problem and the learning algorithm, the task of of Opitz [1999]. GEFS was the first genetic algorithm for 
ensemble feature selection has the additional goal of finding ensemble feature selection that explicitly used diversity in 
a set of feature subsets that will promote diversity among its fitness function. GA begins with creating an initial popu-
the base classifiers [Opitz, 1999].              lation with RS. Then, new candidate classifiers are produced 
  Ho [1998] has shown that simple random selection of fea- by crossover and mutation. After producing a certain num-
tures may be an effective technique for ensemble feature ber of individuals the process continues with selecting a new 
selection because the lack of accuracy in the ensemble subset of candidates randomly with a probability propor-
members is compensated for by their diversity. This tech- tional to fitness (so called roulette-wheel selection). The 
nique is called the random subspace method or simply Ran- process of creating new classifiers and selecting a subset of 
dom Subspacing (RS).                             them (a generation) continues a certain number of times. 
  With RS one may solve the small sample size problem, After a predefined number of generations, the fittest indi-
because the training sample size relatively increases in ran- viduals make up the population, which comprises the en-
dom subspaces. Ho [1998] shows that while most other semble. The representation of each individual is a constant-
classification methods suffer from the curse of dimensional- length string of bits, where each bit corresponds to a particu-
ity, this method does not. RS has much in common with lar feature. The crossover operator uses uniform crossover, 
bagging [Skurichina and Duin, 2001], but instead of sam- in which each feature of the two children takes randomly a 
pling instances, one samples features. Like bagging, RS is a value from one of the parents. The mutation operator ran-
parallel learning algorithm, that is, generation of each base domly toggles a number of bits in an individual. 
classifier is independent. This makes it suitable for parallel Instead of maintaining a set of feature subsets in each 
implementation that is desirable in some practical applica- generation of one genetic process, GAS-SEFS (Genetic Al-
tions. It was shown that, like in bagging, accuracy could be gorithm-based Sequential Search for Ensemble Feature Se-
only increased with the addition of new members, even lection) uses a series of genetic processes, one for each base 
when the ensemble complexity grew [Ho, 1998].    classifier, sequentially. Pseudo-code for GAS-SEFS is given 
  RS is used as a base in a number of ensemble feature se- in Figure 1. After each genetic process one base classifier is 
lection strategies, e.g. GEFS (Genetic Ensemble Feature selected into the ensemble. GAS-SEFS uses the same fitness 
Selection) [Opitz, 1999] and HC (Hill Climbing) [Cunning- function (1), but diversity is calculated with the base classi-
ham and Carney, 2000].                           fiers already formed by previous genetic processes instead 
                                                 of the members of current population. In the first GA proc-
 3  Two Strategies for Genetic Ensemble Fea-     ess, the fitness function has to use accuracy only. GAS-
                                                 SEFS uses the same genetic operators as GA. 
    ture Selection 
                                                 For I=1 to EnsembleSize 
                                                   For J=1 to 10 Population(J)=RSM(#Features); 
 3.1 GA and GAS-SEFS                               For J=1 to #Generations 
                                                     For K=1 to 10 CalculateFitness(Population(K)); 
 The use of genetic search has been an important direction in     For K=1 to 40 
 the feature selection research. Genetic algorithms have been     //randomly proportional to log(1+fitness) 
 shown to be effective global optimization techniques. The         (L,M)=Select2(Population); 
                                                         Offsprings(K)=CrossOver(L,M); 
 use of genetic algorithms for ensemble feature selection was     EndForK 
 first proposed in [Kuncheva, 1993] and further elaborated in      For K=1 to 10  Mutate1_0(Offsprings(20+K)); 
                                                             Mutate0_1(Offsprings(30+K)); 
 [Kuncheva and Jain, 2000]. As the fitness function in      For K=1 to 40 CalculateFitness(Offsprings(K)); 
 [Kuncheva, 1993; Kuncheva and Jain, 2000] the ensemble //randomly proportional to fitness 
 accuracy was used instead of the accuracy of the base classi- Population=Select10(Population+Offsprings); 
                                                   EndForJ 
 fiers. However, such a fitness function is biased towards   //according to fitness 
 some particular integration method (often simple voting). BaseClassifier(I)=Select1(Population);   
 Besides, as it was shown e.g. in [Kuncheva, 1993], such a EndForI 
 design is prone to overfitting, and some additional preven- Figure 1 Pseudo-code for the GAS-SEFS algorithm 
 tive measures are needed to be taken to avoid this (as in- GA has a number of peculiarities, which we use also in      l
                                                               N
 GAS-SEFS. Full feature sets are not allowed in RS nor may   ∑  ii         l
                                                                              N   N
 the crossover operator produce a full feature subset. Indi- Θ = i=1 , and Θ = ⎛ i* ⋅ *i ⎞ , (3) 
                                                          1             2  ∑⎜ N   N ⎟
 viduals for crossover are selected randomly proportional to  N            i=1⎝     ⎠
 log(1+fitness) instead of just fitness, which adds more di- where l is the number of classes and N is the total number of 
versity into the new population. The generation of children instances. Θ  estimates the probability that the two classifi-
identical to their parents is prohibited. To provide a better 1
                                                 ers agree, and Θ2 is a correction term, which estimates the 
diversity in the length of feature subsets, two different muta- probability that the two classifiers agree simply by chance 
tion operators are used (Mutate1_0 and Mutate0_1), one of (in the case where each classifier chooses to assign a class 
 which always deletes features randomly with a given prob-
                                                 label randomly). The pairwise diversity div_kappai,j is de-
 ability, and the other – adds features.         fined as follows: 
  Parameter settings for our implementation of GA and 
                                                                        Θ1 − Θ2
 GAS-SEFS include a mutation rate of 50%, a population      div _ kappai,j =  . (4) 
 size of 10, a search length of 40 feature subsets (the number          1− Θ2
 of new individuals produced by crossover and mutation), of We normalize this measure to vary from 0 to 1. 
 which 20 are offsprings of the current population of 10 clas-
 sifiers generated by crossover, and 20 are mutated off- 4 Integration of an Ensemble of Models 
 springs (10 with each mutation operator). 10 generations of 
                                                 The challenging problem of integration is to decide which of 
 individuals were produced, as our pilot studies have shown 
                                                 the classifiers to select or how to combine the results pro-
 that in most cases, with this configuration, the ensemble 
                                                 duced by the base classifiers. A number of selection and 
 accuracy does not improve after 10 generations, due to over-
                                                 combination approaches have been proposed. 
 fitting the training data. 
                                                   One of the most popular and simplest techniques used to 
  The complexity of GA does not depend on the number of 
                                                 combine the results of the base classifiers, is simple voting 
 features, and is O(S′⋅ N ) , where S′  is the number of 
                    gen                          (also called majority voting) [Bauer and Kohavi, 1999]. In 
individuals in one generation, and N  is the number of 
                              gen                the voting, the output of each base classifier is considered as 
 generations [Tsymbal et al., 2005]. The complexity of GAS-
                                                 a vote for that particular class value. The class value that 
 SEFS is O(S ⋅ S′⋅ N ) , where S is the number of base 
                gen                              receives the biggest number of votes is selected as the final 
 classifiers. In our experiments, on average, GA and GAS-
                                                 classification. Weighted Voting (WV), where each vote has 
 SEFS look through about 400 and 4000 feature subsets cor-
                                                 a weight proportional to the estimated generalization per-
 respondingly (given that the number of base classifiers is 
                                                 formance of the corresponding classifier, works usually bet-
 10, the number of individuals in a generation is 40, and the 
                                                 ter than simple voting [Bauer and Kohavi, 1999]. 
 number of generations is 10). 
                                                 A number of selection techniques have also been proposed 
                                                 to solve the integration problem. One of the most popular 
 3.2 Diversity measures used in the fitness function and simplest selection techniques is Cross-Validation Ma-
 The fail/non-fail disagreement measure and the kappa statis- jority (CVM, also called Single Best; we call it simply 
 tic were shown to provide the best performance for GA in Static Selection, SS, in our experiments) [Schaffer, 1993]. 
 [Tsymbal et al., 2005].                         In CVM, the cross-validation accuracy for each base classi-
  The fail/non-fail disagreement measure was defined in fier is estimated, and then the classifier with the highest 
[Skalak, 1996] as the percentage of test instances for which accuracy is selected. 
the classifiers make different predictions but for which one The described above approaches are static. They select 
of them is correct:                              one model for the whole data space or combine the models 
                        01   10                  uniformly. In dynamic integration each new instance to be 
                       N  + N                    classified is taken into account. Usually, better results can 
        div _ disi,j =              , (2) 
                  N11 + N 10 + N 01 + N 00       be achieved if integration is dynamic. 
       ab                                          We consider in our experiments three dynamic techniques 
where N  is the number of instances, classified correctly based on the same local error estimates: Dynamic Selection 
 (a=1) or incorrectly (a=0) by classifier i, and correctly (b=1) (DS), Dynamic Voting (DV), and Dynamic Voting with 
or incorrectly (b=0) by classifier j. The denominator in (2) is Selection (DVS) [Tsymbal and Puuronen, 2000]. They con-
 equal to the total number of instances. The fail/non-fail dis- tain two main phases. First, at the learning phase, the local 
 agreement varies from 0 to 1.                   classification errors of each base classifier for each instance 
  The kappa statistic was first introduced in [Cohen, 1960]. of the training set are estimated according to the 1/0 loss 
Let Nij be the number of instances, recognized as class i by function using cross validation. The learning phase finishes 
 the first classifier and as class j by the second one, Ni* is the with training the base classifiers on the whole training set. 
number of instances recognized as i by the first classifier, The application phase begins with determining k-nearest 
and N*i is the number of instances recognized as i by the neighbours for a new instance using a given distance metric. 
second classifier. Define then Θ1 and Θ2 as:     Then, weighted nearest neighbour regression is used to pre-
                                                 dict the local classification errors of each base classifier for 
                                                 the new instance.   After, DS simply selects a classifier with the least pre- stances were divided into two sets of approximately equal 
dicted local classification error. In DV, each base classifier size (a validation set and a test set). 70 test runs of were 
receives a weight that is proportional to its estimated local made on each data set for each search strategy and diversity.  
accuracy, and the final classification is produced as in WV. Four different ensemble sizes have been tested: 3, 5, 7, 
In DVS, the base classifiers with the highest local classifica- and 10. The ensemble size did not exceed 10 due to two 
tion errors are discarded (the classifiers with errors that fall main reasons: (1) limitation in computational resources, and 
into the upper half of the error interval) and locally (2) it was shown in experiments that for guided ensemble 
weighted voting (DV) is applied to the remaining classifiers. construction such as genetic search the biggest gain is 
                                                 achieved already with 10 base classifiers, and much less 
 5 Experimental Investigations                   classifiers are needed than with unguided ensemble con-
                                                 struction such as RS and bagging.  
 5.1 Experimental setup                            At each run of the algorithm, accuracies for the five types 
 The experiments are conducted on 21 data sets taken from of ensemble integration are collected: Static Selection (SS), 
 the UCI machine learning repository [Blake et al., 1999]. Weighted Voting (WV), Dynamic Selection (DS), Dynamic 
These data sets include real-world and synthetic problems, Voting (DV), and Dynamic Voting with Selection (DVS). 
vary in characteristics, and were previously investigated by We have collected ensemble characteristics for four num-
other researchers. The main characteristics of the data sets bers of generations: 1, 3, 5, and 10. 
are presented in Table 1.                          To reduce the number of possible combinations of pa-
                                                 rameters, we conducted a separate series of preliminary ex-
         Table 1 Data sets and their characteristics periments using the wrapper approach based on cross vali-
                                                 dation to select the best diversity coefficient α  and the 
                                 Features 
      Data set Instances Classes                 number of nearest neighbors in dynamic integration k as in 
                              Categ. Num.        [Tsymbal et al., 2005]. We have experimented with seven 
      Balance 625 3             0       4        values of α : 0, 0.25, 0.5, 1, 2, 4, and 8. Seven values: 1, 3, 
    Breast Cancer 286    2      9       0                        n
       Car      1728 4          6       0        7, 15, 31, 63, 127 ( 2 −1, n = 1,...,7 ) were used for k. From 
     Diabetes 768 2             0       8        the experimental results we could see that the best value of k 
   Glass Recognition 214 6      0       9        depended mostly only on the integration method used and 
    Heart Disease 270    2      0      13
     Ionosphere 351      2      0      34        on the data set. The best α ’s varied with the search strat-
     Iris Plants 150     3      0       4        egy, integration method, and data set used.  
       LED 300 10 7 0                              After, the experiments were repeated with the selected 
      LED17 300 10 24 0                          values of α  and k. Although the same data were used for 
    Liver Disorders 345  2      0       6
    Lymphography 148     4      15      3        the selection and for the later experiments, we believe that 
     MONK-1 432 2               6       0        this did not lead to overfitting due to the small number of 
     MONK-2 432 2               6       0        possible values for α  and k. 
     MONK-3 432 2               6       0
     Soybean 47 4               0 35               The test environment was implemented within the 
      Thyroid 215 3             0       5        MLC++ framework (the machine learning library in C++) 
    Tic-Tac-Toe 958      2      9       0        [Kohavi et al., 1996]. A multiplicative factor of 1 was used 
      Vehicle 846 4 0 18
      Voting 435 2 16 0                          for the Laplace correction in SB as in [Domingos and Paz-
       Zoo 101 7 16 0                            zani, 1997]. Numeric features were discretized into ten 
                                                 equal-length intervals (or one per observed value, whichever 
 As in [Tsymbal et al., 2003; 2005], we use Simple Bayes was less), as it was done in [Domingos and Pazzani, 1997]. 
 (SB) as the base classifier in the ensembles. It has been re- Although this approach was found to be slightly less accu-
 cently shown experimentally and theoretically that SB can rate than more sophisticated ones, it has the advantage of 
 be optimal even when the “naïve” feature-independence simplicity, and is sufficient for comparing different ensem-
 assumption is violated by a wide margin [Domingos and bles of SB classifiers with each other.  
 Pazzani, 1997]. Second, when SB is applied to the sub-
 problems of lower dimensionalities, the error bias of the 5.2 Experimental results 
 Bayesian probability estimates caused by the feature- To validate our findings, we divided all data sets into two 
 independence assumption becomes smaller. It also can eas- groups: with less than 9 features (10 data sets, group 1), and 
 ily handle missing feature values. Besides, it has advantages with greater than or equal to 9 features (11 data sets, group 
 in terms of simplicity, learning speed, classification speed, 2); and checked all the characteristics for these groups. The 
 and storage space. We believe that dependencies and con- ensemble accuracies were nearly the same for the two diver-
 clusions presented in this paper do not depend on the learn- sity measures, and the fail/non-fail disagreement was 
 ing algorithm used and would be similar for most known slightly better on average, so we present results for this di-
 learning algorithms.                            versity measure only here. In Figure 2 the ensemble accura-
  To evaluate GA and GAS-SEFS, we have used stratified cies for strategies GA and GAS-SEFS, over the two groups 
 random-sampling cross validation with 60 percent of in- of data sets and four ensemble sizes are shown averaged 
 stances in the training set. The remaining 40 percent of in- over the data sets for the best integration method (DVS). It  can be seen from the figure that GAS-SEFS builds even 5.3 Other interesting findings 
 more accurate ensembles than GA; especially for group 2 
 including data sets with larger numbers of features. Accu- Selected values of α  were different for different data sets, 
 racy grows with the ensemble size, but the growth flattens supporting findings in [Tsymbal et al., 2003; 2005]. In gen-
 as the number of base classifiers increases.    eral, for both strategies, α  for the dynamic integration 
                                                 methods is bigger than for the static ones (2.2 vs 0.8 on av-
                                                 erage). GAS-SEFS needs slightly higher values of α  than 
  0.840                                          GA (1.8 vs 1.5 on average). This can be explained by the 
                                                 fact that GAS-SEFS always starts with a classifier, which is 
  0.835                                          based on accuracy only, and the subsequent classifiers need 
                                                 more diversity than accuracy. 
  0.830                                            The number of selected features falls as the ensemble size 
                                             3   grows, and this is especially clear for GAS-SEFS, as the 
                                             5
  0.825 
                                             7   base classifiers need more diversity. As a rule, more features 
                                             10  are needed in the static integration methods than in the dy-
  0.820                                          namic ones to achieve better accuracy. GAS-SEFS results in 
                                                 slightly smaller feature subsets on average (48% vs 50% of 
  0.815                                          features for dynamic integration strategies). 
                                                   As it was also reported in [Tsymbal et al., 2005], the se-
  0.810                                          lected k-neighbourhood values for dynamic integration 
        GA_gr1  GAS-SEFS_gr1 GA_gr2 GAS-SEFS_gr2 change with the integration method. DS needs higher values 
 Figure 2 Ensemble accuracies for two strategies, two groups of of k. This can be explained by the fact that its prediction is 
data sets, and four ensemble sizes               based on only one classifier being selected, and thus, it is 
                                                 very unstable. Higher values of k provide more stability to 
 In Figure 3 ensemble accuracies are shown for the two DS. The average selected k is equal to 33 for DS, and it is 
 strategies, five integration methods, and four ensemble sizes only 14 for DV. For DVS, as a hybrid strategy, it is in be-
 on the Tic-Tac-Toe data set, as a representative of group 2 tween at 24. The selected values of k do not change signifi-
 including 958 instances and 9 features. This figure supports cantly with the change of the search strategy and the ensem-
 our previous findings. Besides, it could be seen that dy- ble size.  
 namic integration, expectedly, outperforms static integration Experimental results for both GA and GAS-SEFS show 
 both for GA and for GAS-SEFS. Accuracy grows with the that the static integration methods, SS and WV, and the dy-
 ensemble size and this growth is greater for the best integra- namic DS start to overfit the validation set already after 5 
 tion methods (DS and DVS in this case).         generations and show lower accuracies, whereas the accura-
                                                 cies of DV and DVS continue to grow up to 10 generations. 
  0.95                                           This shows the importance of selection of the appropriate 
                                                 integration method for the genetic strategies. 
  0.90

  0.85                                           6 Conclusions 
                                           3
                                           5     In our paper, we have considered two genetic search strate-
  0.80
                                           7     gies for ensemble feature selection. The new strategy, GAS-
                                           10    SEFS, consists in employing a series of genetic search proc-
  0.75
                                                 esses, one for each base classifier. It was shown in experi-
  0.70                                           ments that GAS-SEFS results in better ensembles having 
                                                 greater accuracy in many domains, especially for data sets 
  0.65                                           with relatively larger numbers of features. GAS-SEFS is 
      SS  WV  DS DV DVS SS  WV DS  DV DVS        significantly more time-consuming than GA. However, it 
Figure 3 Ensemble accuracies for GA (left) and GAS-SEFS (right) can be easily parallelized in a multiprocessor setting, and 
for five integration methods and four ensemble sizes on the Tic- one processor could be used for each offspring in the current 
Tac-Toe data set                                 generation. 
                                                   One of the reasons for the success of GAS-SEFS is the 
The difference between the two strategies is clearer for the fact that each of the core GA processes leads to significant 
best integration methods. The dependencies were the same overfitting of a corresponding ensemble member, and, as it 
for all the data sets, with sometimes lesser difference be- was shown before, an ensemble of overfitted members is 
tween the integration methods. For some data sets DV out- often better that an ensemble of non-overfitted members. 
performs DS, which supports the previous findings about In [Oliveira et al., 2003] it was shown that besides the use 
behaviour of the integration methods [Tsymbal and Puu- of weights to combine a number of objectives in the fitness 
ronen, 2000; Tsymbal et al., 2005].              function in genetic algorithms (as the use of α  in our case), 
                                                 another common approach that often gives better results for 