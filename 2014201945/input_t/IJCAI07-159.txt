                                    Kernel Matrix Evaluation

                                  Canh Hao Nguyen and Tu Bao Ho
                                     School of Knowledge Science
                          Japan Advanced Institute of Science and Technology
                            1-1 Asahidai, Nomi, Ishikawa, 923-1292, Japan
                                       {canhhao,bao}@jaist.ac.jp

                    Abstract                          The goodness of kernel function can only be seen from the
                                                      goodness of the kernel matrix K; therefore, measuring the
    We study the problem of evaluating the goodness of goodness of K is of primary interest in various contexts.
    a kernel matrix for a classiﬁcation task. As kernel
    matrix evaluation is usually used in other expensive There are many ways to measure the goodness of a ker-
    procedures like feature and model selections, the nel matrix (kernel function) for a classiﬁcation task, differ-
    goodness measure must be calculated efﬁciently.   ently reﬂecting the expected quality of the kernel function.
    Most previous approaches are not efﬁcient, except Commonly used measures are regularized risk [Sch¨olkopf
    for Kernel Target Alignment (KTA) that can be cal- and Smola, 2002], negative log-posterior [Fine and Schein-
    culated in O(n2) time complexity. Although KTA    berg, 2002] and hyperkernels [Ong et al., 2005]. These mea-
    is widely used, we show that it has some serious  sures do not give a speciﬁc value, but only assert certain cri-
    drawbacks. We propose an efﬁcient surrogate mea-  teria in form of regularities in certain spaces, for example,
    sure to evaluate the goodness of a kernel matrix  RKHS or hyper-RKHS. All of these kernel measures require
    based on the data distributions of classes in the fea- an optimization procedure for evaluation. Therefore, they
    ture space. The measure not only overcomes the    are prohibitively expensive to be incorporated into other ex-
    limitations of KTA, but also possesses other prop- pensive procedures like model and feature selections. Other
    erties like invariance, efﬁciency and error bound techniques like cross validation, leave-one-out estimators or
    guarantee. Comparative experiments show that the  radius-margin bound can also be considered as expected qual-
    measure is a good indication of the goodness of a ity functionals. Like the previously mentioned works, they
    kernel matrix.                                    require the whole learning process.
                                                        To be used in feature and model selection processes, good-
                                                      ness measures of kernel matrices must be efﬁciently calcu-
1  Introduction                                       lated. To the best of our knowledge, the only efﬁcient kernel
Kernel methods, such as Support Vector Machines, Gaussian goodness measure is Kernel Target Alignment [Cristianini et
Processes, etc., have delivered extremely high performance al., 2002]. Due to its simplicity and efﬁciency, KTA has been
in a wide variety of supervised and non-supervised learning used in many methods for feature and model selections. Some
tasks [Sch¨olkopf and Smola, 2002]. The key to success is that notable methods are learning conic combinations of kernels
kernel methods can be modularized into two modules: the [Lanckriet et al., 2004], learning idealized kernels [Kwok and
ﬁrst is to map data into a (usually higher dimensional) feature Tsang, 2003], feature selection [Neumann et al., 2005] and
space, which is a powerful framework for many different data kernel design using boosting [Crammer et al., 2002].
types; and the second is to use a linear algorithm in the fea- In this work, we ﬁrst analyze KTA to show that having
ture space, which is efﬁcient and has theoretical guarantees a high KTA is only a sufﬁcient condition to be a good ker-
[Shawe-Taylor and Cristianini, 2004]. While many linear al- nel matrix, but not a necessary condition. It is possible for a
gorithms can be kernelized, the major effort in kernel meth- kernel matrix to have a very good performance even though
ods is put into designing good mappings deﬁned by kernel its KTA is still low. We then propose an efﬁcient surrogate
        φ
functions :                                           measure based on the data distributions in the feature space
                    φ : X → H.                  (1)   to relax the strict conditions imposed by KTA. The measure
X is the original data space and H is the feature space, which is invariant to linear operators in the feature space and re-
is a Reproducing Kernel Hilbert Space (RKHS). While H tains several properties of KTA, such as efﬁciency and an er-
may be a many-dimensional space, the algorithm takes ad- ror bound guarantee. The measure is closely related to some
vantage of the kernel trick to operate solely on the kernel ma- other works on worst-case error bounds and distance metric
trix induced from data:                               learning. We show experimentally that the new measure is
                                                      more closely correlated to the goodness of a kernel matrix,
          K  = {φ(xi),φ(xj )}i=1..n,j=1..n.   (2)   and conclude ﬁnally with some future works.

                                                IJCAI-07
                                                   9872  Kernel Target Alignment                            (SVMs). Varying data (in the feature space) along the separat-
Kernel target alignment is used to measure how well a kernel ing hyperplane will not affect the margin. If data varies along
matrix aligns to a target (or another kernel) [Cristianini et al., the perpendicular direction of the separating hyperplane, the
2002]. Alignment to the target is deﬁned as the (normalized) margin can be changed. However, KTA does not take into
Frobenius inner product between the kernel matrix and the account variances in different directions.
covariance matrix of the target vector. It is interpreted as co- The second condition is too strict, and not applicable in
sine distance between these two bi-dimensional vectors. We many cases, because it requires the mapped vectors of differ-
ﬁrst introduce some notations.                        ent classes to be additive inverses of each other. Ideally, if
  Training example set {xi}i=1..n ⊂ X with the correspond- the kernel function maps all examples of a class to one vector
                      T              n                in the feature space, the kernel matrix should be evaluated as
ing target vector y = {yi}i=1..n ⊂{−1, 1} . Suppose that
                                                      optimal. This is the reason why having high KTA is only a
y1 = .. = yn+ =1and  yn++1 =  .. = yn++n− = −1; n+
examples belong to class 1, n− examples belong to class −1, sufﬁcient condition, but not a necessary condition. We use
n+ + n−  = n. Under a feature map φ, the kernel matrix is some examples to show this limitation quantitatively.
                                                                                               φ
deﬁned as:                                              Example 1  (best case): The kernel function maps all
                                                      examples of class 1 into vector φ+ ∈ H and all examples
        K = {k  =  φ(x ),φ(x )}         .
              ij       i    j   i=1..n,j=1..n   (3)   of class −1 into vector φ− ∈ H. Assume that φ+.φ+ =
                                                      φ  .φ  =1     φ  .φ  = α  −1   α<1           α
  Frobenius inner product is deﬁned as:                −   −     and  +  −     ,           .Forany   ,the
                                                      kernel matrix K should be evaluated as optimal, as it is in-
                         n n
             K, K∗  =        k  k∗ .                duced from an optimal feature map. However, its KTA value
                    F           ij ij           (4)   is:
                         i=1 j=1                                           n2 + n2 − 2n  n  α
                                                              A(K, y)=     +    −      + −     .
                                T                                                                     (6)
  The target matrix is deﬁned to be y.y . Kernel target align-             2    2           2
                                                                          n+ + n− +2n+n−α    ∗ n
ment of K is deﬁned as the normalized Frobenius inner prod-
uct:                                                                                             α
                               T                      KTA  values of these kernel matrices change as varies
                         K, y.y F                        1   −1
         A(K, y)=                       .            from   to   , and any value in that range can be the KTA
                                T    T          (5)
                     K, KF y.y ,y.y F             value of a kernel matrix of an optimal feature function. As
                                                                               2                        2
                                                      lim    A(K, y)=   (n+−n−)                  (n+−n−)
  The advantage, which makes KTA popular in many feature α→1               n2   , KTA ranges from   n2
and model selection methods, is that it satisﬁes several prop- to 1 in this case.
erties. It is efﬁcient as its computational time is O(n2).Itis Example 2 (worst case): The kernel function φ maps half
highly concentrated around its expected value, giving a high of the examples of each class into vector φ1 ∈ H and the
probability that a value is close to the true alignment value. other half into vector φ2 ∈ H. Assume that φ1.φ1 = φ2.φ2 =
It also gives some error bounds, meaning that when KTA is 1 and φ1.φ2 = α, −1  α  1.Foranyα,thekernelma-
high, one can expect that the error rate using the kernel must trix K should be evaluated very low, as it is induced from the
be limited to a certain amount.                       worst kernel function, which fuses the two classes together
  We claim that having a very high value A(K,y) is only a and has no generalization ability. However, its KTA value is:
sufﬁcient condition, but not a necessary condition, for K to be              (n  − n  )2 1+α
a good kernel matrix for a given task speciﬁed by the target                   +  −     2
                                                                   A(K, y)=             2  .          (7)
                     1                                                         2     1+α
y.As0    A(K, y)  1 ,whenA(K, y)=1, the two bi-                             n ∗   (  2 )
                 K     y.yT
dimensional vectors and    are linear. Up to a constant,                                     2
                                k   = y .y                                            (n+−n−)
the optimal alignment happens when ij  i  j.Thisis    As shown above, limα→1 A(K, y)=    n2   , which is the
equivalent to two conditions:
                                                      same limit as the best case. We2 can see that KTA of this
                                                                     0   (n+−n−)
 1. All examples of the same class are mapped into the same case ranges from to n2 . There is a similar caution
    vector in the feature space (kij =1for xi,xj in the same in [Meila, 2003]. As the best and the worst cases cover the
    class).                                           whole range of KTA values, any other case would coincide
 2. The mapped vectors of different classes in the feature with one of them. Therefore, KTA may mistake any case to
                                                      be either the best or the worst.
    space are additive inverse (kij = −1 for xi,xj in differ-
    ent classes).                                       Example 3 (popular case): Consider the case when a ker-
                                                      nel function maps all examples into an orthant in the feature
  It is a sufﬁcient condition that having a high K, one can space, inducing a kernel matrix with non-negative entries, i.e.
expect good performance, as classes are collapsed into a point kij  0. In this case:
in the feature space. Violating any of the conditions would                    
mean that KTA is penalized. However, it is not a necessary                        2     2
                                                                                 n+ + n−
condition as we analyze below.                                       A(K, y)            .            (8)
  The ﬁrst condition implies that the within-class variance                        n
penalizes KTA. However, there is a gap between the condi-
                                                      Proof can be derived by using the fact that kij  0 and the
tion and the concept of margin in Support Vector Machines Cauchy-Schwarz inequality [Gradshteyn and Ryzhik, 2000].
                                                                             n  =  n   A(K, y)  √1
  1In [Cristianini et al., 2002],itis−1  A(K, y). The inequality Take a special case when + −,    2 .Re-
here comes from the fact that K is positive semideﬁnite. call that KTA of a kernel matrix ranges from 0 to 1,andthe

                                                IJCAI-07
                                                   988                                                      calculate the within-class variance (of both classes) in the di-
                                                                                       e =  φ−−φ+
                                                      rection between class centers. Denote φ−−φ+ as the
                                                      unit vector in this direction. The total within-class variance
                                                      of two classes in the direction is:
                                                                        
                                                                          n+              2
                                                                          i=1φ(xi) − φ+,e 1
                                                                 var =[                     ] 2
                                                                               n  − 1
                                                                               +
                                                                      n                   2          (10)
                                                                            φ(xi) − φ−,e
                                                                      i=n++1                1
                                                                  +[                       ] 2 .
                                                                            n− −  1
Figure 1: Data variance in the direction between class centers. We calculate the ﬁrst term in equation (10), the total
                                                      within-class variance of the class +1 in the direction between
                                                      φ+ and φ−, denoted as var+.
higher the better. This example means that these types of ker-
                                                                        n+
nel functions will have bounded KTA values, no matter how           2                   2
good the kernel functions are. This is a drawback of KTA. (n+ − 1)var+ =   φ(xi) − φ+,e
Unfortunately, this type of kernel function is extensively used         i=1
                                                            n
in practice. Gaussian kernels [Shawe-Taylor and Cristianini,   + φ(x ) − φ ,φ  − φ 2
                                                         =    i=1    i    +   −    +                 (11)
    ]                                                                        2
2004 , and many other kernels deﬁned over discrete structures       (φ− − φ+)
[Gartner et al., 2004] or distributions [Jebara et al., 2004],etc. n
                                                               + (φ(x )φ  + φ2 − φ(x )φ  − φ φ  )2
fall into this category.                                 =    i=1    i  −    +      i  +    + −   .
                                                                                   2
  The above examples show that KTA can mistake any ker-                   (φ− − φ+)
                                                                                             P
                                                                          Pn+                 n
                                                                                                    φ(xj )
nel matrix to be the best or the worst. KTA is always bound                 j=1 φ(xj )        j=n++1
                                                        We substitute φ+ =         and φ− =
to some numbers for many popular kernel matrices with pos-                   n+                  n−
itive entries (e.g., Gaussian, and many kernels for discrete into equation (11), and then we deﬁne some auxiliary vari-
structures or distributions). KTA will disadvantage the use of ables as follows. For i =1..n+:
                                                                         Pn+             Pn+
such kernels.                                                              j=1 φ(xi)φ(xj ) j=1 kij
                                                        • ai = φ(xi)φ+ =              =
                                                                              n+           n+   ,
                                                                         Pn                 Pn
                                                                                 φ(xi)φ(xj )        kij
3  Feature Space-based Kernel Matrix                                       j=n++1             j=n++1
                                                        • bi = φ(xi)φ− =                 =            .
   Evaluation Measure                                                           n−              n−
                                                        For i = n+ +1..n:
In this section, we introduce a new goodness measure of                  Pn+             Pn+
                                                                           j=1 φ(xi)φ(xj ) j=1 kij
                                                        • ci = φ(xi)φ+ =              =
a kernel matrix for a given (binary classiﬁcation) task,                      n+           n+   ,
                                                                         P                  P
named Feature Space-based Kernel Matrix Evaluation Mea-                    n                  n
                                                                                 φ(xi)φ(xj )        kij
                                                                           j=n++1             j=n++1
sure (FSM). This measure should be computed on the ker- • di = φ(xi)φ− =        n         =     n     .
                                                                                 −          P    −
nel matrix efﬁciently, and overcome the limitations of KTA.         Pn+         Pn+           n
                                                                        a           b         i=n +1 ci
Our idea is to use the data distributions in the feature space. A =  i=1 i B =    i=1 i C =     +
                                                        Denote       n+   ,       n+  ,        n−     and
                                                           Pn
Speciﬁcally, two factors are taken into account: (1) the         di
                                                             n++1
                                                      D  =                 A =  φ+φ+  B  = C =  φ+φ−
within-class variance in the direction between class cen-     n−   . Hence,          ,                and
                                                                                     2
ters; and (2) relative positions of the class centers.These D = φ−φ−. Therefore, (φ− − φ+) = A + D − B − C.
factors are depicted in Figure 1. The ﬁrst factor improves
                                                      Plugging them into equation (11), then:
the ﬁrst condition of KTA, by allowing data to vary in cer-                 n+ (b − a + A − B)2
                                                           (n  − 1)var2 =   i=1  i   i          .
tain directions. The second factor solves the problem im-    +        +       A + D − B − C          (12)
posed by the second condition of KTA. Let var be the total
within-class variance (for both classes) in the direction be- We can use a similar calculation for the second term in equa-
                                                      tion (10):
tween class centers. Denote center of a classPn as the mean             
                                         + φ(x )                          n                      2
                                        i=1  i                                   (ci − di + D − C)
                                φ+ =                                2     i=n++1
of the class data in the feature space:  n+    and       (n  − 1)var  =                           .
      Pn                                                   −        −                                (13)
              φ(xi)                                                           A + D − B − C
        i=n++1
φ− =              . Our evaluation measure FSM is de-
           n−                                           The proposed kernel matrix evaluation measure FSM, as
ﬁned to be the ratio of the total within-class variance in the deﬁned in formula (9), is calculated as:
direction between the class centers to the distance between                     var+ + var−
                                                                FSM(K, y)=√                   .      (14)
the class centers:                                                              A + D − B − C
                              var                     FSM(K, y)    0, and the smaller FSM(K, y) is, the better
            FSM(K, y):=               .         (9)
                           φ− − φ+                  K  is.
                                                        One can easily see that ai, bi, ci, di, A, B, C and D can
3.1  FSM Calculation                                  be all calculated together in O(n2) time complexity (one pass
We show that the evaluation measure can be calculated using through the kernel matrix). Therefore, the evaluation measure
the kernel matrix efﬁciently with simple formulas. We ﬁrst is also efﬁciently calculated in O(n2) time complexity.

                                                IJCAI-07
                                                   9893.2  Invariance                                                 1
                         FSM(K, y)                             0.9
There are some properties of         regarding linear                                  1−KTA
operations that make it closer to SVMs than KTA is.                                    FSMerr
                                                               0.8                     error
  Theorem 1: FSM(K, y)   is invariant under translation,
rotation and scale in the feature space.                       0.7
  Sketch of proof: FSM(K, y) is scale invariant owing to       0.6
                    A  + D − B  − C   FSM(K, y)
its normalization factor            .            is            0.5
rotation and translation invariant, as it is built on the building
blocks of ai − bi and di − ci.                                 0.4
  The performance of SVMs and other kernel methods are         0.3
unchanged under rotation, translation and scale in the feature
                                                               0.2
space. Therefore, it is reasonable to ask for measures to have
these properties. However, KTA is neither rotation invariant   0.1
nor translation invariant.                                      0
                                                                30    60    90    120   150   180
3.3  Error Bound
                                                                                                β
We show that for any kernel matrix, it is possible to obtain a Figure 2: Results on synthetic data with different values.
training error rate, which is bounded by some amount propor-
tionate to FSM(K, y). This means that a low FSM(K, y)
value can guarantee a low training error rate. In this case, we scale, and relaxes the strict conditions of KTA by considering
can expect a low generalization error rate.           relative positions of classes in the feature space. For the ex-
  Theorem  2: There exists a separating hyperplane such amples in Section 2, FSM values of the best cases are always
                                                      0                                ∞
that its training error is bounded by:                 , those of the worst cases are always , and for the pop-
                                   2                  ular case, there are no bounds. This measure also has some
                         FSM(K, y)                    similarities with other works.
           FSMerr   :=                .        (15)
                       1+FSM(K, y)2                     The minimax probability machine (MPM) [Lanckriet et al.,
                                                      2002] controls misclassiﬁcation probability in the worst-case
  Proof: We use a one-tailed version of Chebyshev’s in- setting. The worst-case misclassiﬁcation probability is min-
       [          ]
equality Feller, 1971 . Consider the data distribution in the imized using Semideﬁnite Programming. The worst case is
direction between class centers. For each class, the data dis- determined by making no assumption rather than the mean
tribution has a mean of either φ+ or φ−, and the correspond-
           var    var                                 and the covariance matrix of each class. MPM is similar to
ing variance  + or   −. The following inequalities can our approach that our measure also shows the worst-case mis-
be derived:                                           classiﬁcation error. The difference is that MPM considers the
                                            1         data variance in all directions (by using the covariance ma-
  Pr(φ − φ−, −e  k.var−|φ ∈ class(−)) 
                                          1+k2        trix), while our measure takes only the variance in the direc-
                                           1          tion between the class centers. That is the reason why our
  Pr(φ − φ  ,e  k.var |φ ∈ class(+))      .
           +           +                1+k2          measure is a lightweight version of MPM and can be calcu-
                                               (16)   lated efﬁciently. A special case of MPM is when the direction
                                                      between class centers contains eigenvectors of both covari-
  The separating hyperplane, which takes e as its norm vec- ance matrices; the objective function that MPM optimizes is
tor and intersects the line segment between φ+ and φ− at the equivalent to our measure.
     h        |h−φ+| = var+
point such that |h−φ−| var− , has the formula:          In the context of nearest neighbor classiﬁcation, one of the
                                                      criteria of learning a good distance function is to transform
                     var−φ+  + var+φ−
       f(x)=e.x  − e.                  =0.     (17)   data such that examples from the same class are collapsed into
                        var+ + var−                   one point [Globerson and Roweis, 2006]. Our measure also
                                                      shows quantitatively how much a class collapses. However,
The error rate of this hyperplane for each class is bounded us-
                                        |φ −φ+|       the key difference is that in their method, the objective is the
ing the inequalities in equation (16) with k = − =
                                       var++var−      whole class collapses to one point, while our measure shows
    1
FSM(K,y) . Therefore, the total training error rate on both how the whole class collapses to a hyperplane. This differ-
classes is also bounded by:                           ence explains why their method is applied to nearest neighbor
                                                      classiﬁcation, while our measure is for kernel methods.
               1       FSM(K, y)2
                   =                 .
             1+k2     1+FSM(K, y)2             (18)
                                                      4   Experiments
3.4  Discussion                                       As the purpose of these measures is to predict efﬁciently how
The measure can be interpreted as the ratio of within-class good a kernel matrix is for a given task using kernel methods,
variance to between-class variance. It indicates how well the we compare the measures to the de facto standard of cross
two classes are separated. It is advantageous over KTA be- validation error rates of SVMs. We mimicked the model se-
cause it takes into account the within-class variances at a ﬁner lection process by choosing different kernels. We monitored

                                                IJCAI-07
                                                   990              australian            breast−cancer             diabetes                fourclass
       1                     0.7                      1                       1

                             0.6                     0.9
      0.8                                                                    0.8
                             0.5                     0.8

      0.6                    0.4                     0.7                     0.6

      0.4                    0.3                     0.6                     0.4
                             0.2                     0.5
      0.2                                                                    0.2
                             0.1                     0.4

       0                      0                      0.3                      0
       Lin.  Poly RBF   Tanh  Lin.  Poly  RBF   Tanh  Lin.  Poly  RBF   Tanh  Lin.  Poly  RBF   Tanh

               german                  heart                 ionosphere               vehicle
       1                      1                       1                       1

      0.9
                             0.8                     0.8                     0.8
      0.8
                                                                                      1−KTA
      0.7                    0.6                     0.6                     0.6      FSMerr
                                                                                      error
      0.6                    0.4                     0.4                     0.4
      0.5
                             0.2                     0.2                     0.2
      0.4

      0.3                     0                       0                       0
       Lin.  Poly RBF   Tanh  Lin.  Poly  RBF   Tanh  Lin.  Poly  RBF   Tanh  Lin.  Poly  RBF   Tanh

                                  Figure 3: Results on different UCI data sets.

the error rates (as baselines), KTA and FSM, to see if how different β, as we expected. FSMerr is also rather stable,
they reﬂect the baselines. We used 10-times 5-fold stratiﬁed varying similarly to error. 1-KTA changes dramatically from
cross validations to estimate the error rates.        0.936 down to 0.395. It can be concluded that KTA is too
  To facilitate visual inspection of the results, we instead sensitive to absolute positions of data in the feature space.
showed the following quantities: (a) 1−KTA,(b)FSMerr, This conﬁrms our claim about the limitations of KTA, and
deﬁned as error bound based on FSM measure in formula shows that our measure is more robust.
(15), and (c) error, cross validation error rates. The reason
is that all of these quantities range from 0 to 1 and relate to 4.2 Real Data
the expected error rates of the kernel function in some sense. We showed the advantage of our proposed measure over KTA
The smaller their values, the better the kernel. The ﬁrst two in real situations by selecting several popular data sets from
quantities are expected to be correlated to the third one. the UCI collection for experiments. Data names and experi-
                                                      mental results are displayed in Figure 3. Data was normalized
4.1  Synthetic Data                                   to [−1, 1]. Classes were grouped to make binary classiﬁcation
We generated synthetic data in R2, and used linear kernels to problems. We chose four types of kernel for model selection:
simulate different data distributions by different kernels in a linear kernels, polynomial kernels (degree 3 with scale 1),
feature space. For each data set parameterized by an angle RBF (Gaussian) kernels (default), and tan-hyperbolic kernels
                                                             2
β, we used two Gaussian distributions (ﬁxed variance in all (default) . As described above, we ran cross validations and
                                                 2
directions) for two classes centered at φ+ =(1, 0) ∈ R displayed the results of different kernels.
                                              2
for class +1 and at φ− =(cos(β),sin(β))   ∈ R   for     The following conclusions can be observed from the
class −1. Each class contains 500 training examples. The graphs:
variances of the Gaussian distributions are determined to be • RBF kernels give the (approximately) lowest cross vali-
               1
var+ = var− =  2  φ− −φ+ . This ensures that for any β, dation error rates in 7 out of 8 data sets. This shows that
any data set can be images of another after a linear operation. RBF kernels are usually a good choice. However, KTA
Therefore, these data sets with linear kernels are equivalent to makes RBF kernels the worst one as values of 1-KTA
one data set with different kernel functions. For any β<1, are the highest ones in 7 out of 8 cases. Our measure
the problems should have the same level of error rates (or agrees with the error rates in most cases, making it more
other measures) when using linear kernels, i.e. linear kernels reliable than KTA is.
should be evaluated at the same level of goodness. We run •
                       β                                  Similarly, Tanh kernels perform worst in most cases,
experiments with different values of 30, 60, 90, 120, 150 while KTA values are among the best ones.
and 180 (degrees) in turn. The results of 1-KTA, FSMerr
and error are shown in Figure 2.                         2Default parameter values are set by LIBSVM environment at
  From Figure 2, we can observe that error is stable across http://www.csie.ntu.edu.tw/˜ cjlin/libsvm/

                                                IJCAI-07
                                                   991