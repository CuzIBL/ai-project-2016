                                Collective    Object   Identiﬁcation

                                  Parag  Singla       Pedro  Domingos
                           Department  of Computer   Science and Engineering
                                       University of Washington
                                    Seattle, WA 98195-2350,   U.S.A.
                                  {parag, pedrod}@cs.washington.edu

  In many domains, the objects of interest are not uniquely
identiﬁed, and the problem arises of determining which ob- Record        Title    Author     Venue
servations correspond to the same object. For example, in b1 Object Identification using CRFs     Linda Stewart Proc. AAAI−05
information extraction and NLP we need to determine which b2 Object Identification using CRFs       Linda Stewart Proc. 20th AAAI
noun phrases refer to the same entity. When merging multiple b3 Learning Boolean Formulas Bill Johnson Proc. AAAI−05
databases, a problem of keen interest to many large scientiﬁc b4 Learning of Boolean Expressions William Johnson Proc. 20th AAAI
projects, businesses, and government agencies, we need to
determine which records represent the same entity and should
therefore be merged. This problem, ﬁrst placed on a ﬁrm sta- record−match node field−match node field−similarity node
tistical footing by Fellegi and Sunter [1969], is known by the
name of object identiﬁcation, record linkage, de-duplication        (a) A bibliography database
and others. Most approaches described to solve this problem

are variants of the original Fellegi-Sunter model, in which ob- Title(T)                 Title(T)
ject identiﬁcation is viewed as a classiﬁcation problem: given
                                                       Sim(Object Identification using CRFs, Sim(Learning Boolean Formulas, 
a vector of similarity scores between the attributes of two ob- Object Identification using CRFs) Learning of Boolean Expressions)
servations, classify it as “Match” or “Non-match.” A separate
match decision is made for each candidate pair, followed by
transitive closure to eliminate inconsistencies. Typically, a b1.T = b2.T?           b3.T = b4.T?
logistic regression model is used. We call this the standard
model.
                                                                    b1=b2?      b3=b4?
  Making match decisions separately ignores that informa-
tion gleaned from one match decision may be useful in others.
For example, if we ﬁnd that a paper appearing in Proc. IJCAI-
03 is the same as a paper appearing in Proc. 18th IJCAI,     b1.A = b2.A? b1.V = b2.V? b3.A = b4.A?
                                                                         b3.V = b4.V?
this implies that these two strings refer to the same venue,
which in turn can help match other pairs of IJCAI papers. In
this paper, we propose an approach which accomplishes this         Sim(Proc. IJCAI−03, Proc. 18th IJCAI )
propagation of information. Our approach makes decisions                  Venue(V)
collectively, performing simultaneous inference for all candi-
date match pairs, and allowing information to propagate from Sim(Linda Stewart, Linda Stewart) Sim(Bill Johnson, William Johnson)
one candidate match to another via the attributes (or ﬁelds) Author(A)                   Author(A)
they have in common. Our model is based on conditional
random ﬁelds [Lafferty et al., 2001]. We call our model the       (b) Collective model (fragment)
collective model. Figure 1(a) shows a four-record bibliog-
raphy database and 1(b) shows the corresponding graphical
representation for the candidate pairs (b1, b2) and (b3, b4) in Figure 1: Example of collective object identiﬁcation. For
the collective model. There are three types of nodes in the ﬁg- clarity, we have omitted the edges linking the record-match
ure. Record-match nodes are Boolean-valued and they corre- nodes to the corresponding ﬁeld-similarity nodes.
spond to asking the question “Is record bi the same as record
bj ?” Field-match nodes are also Boolean-valued and they ues are, according to a pre-deﬁned similarity measure. The
correspond to asking the question “Do ﬁeld values bi.F and values of these nodes can be directly computed from data,
bj .F , for the ﬁeld F, represent the same underlying property?” and hence they are also called the evidence nodes. Intu-
Field-similarity nodes are real-valued nodes taking values in itively, an edge between two nodes represents the fact their
the domain [0, 1] and they encode how similar two ﬁeld val-values directly inﬂuence each other. Note how dependencies
ﬂow through the shared ﬁeld-match node corresponding to Table 1: F-measures on Cora and BibServ before and after
                                                      transitive closure.
the venue ﬁeld. Inferring that b1 and b2 refer to the same un-
derlying paper will lead to the inference that the correspond- Model       Cora          BibServ
ing venue strings “Proc. IJCAI-03” and “Proc. 18th IJCAI”             Before   After  Before   After
refer to the same underlying venue, which in turn might pro- Standard 86.9%   84.7%   82.7%   68.5%
vide sufﬁcient evidence to merge b3 and b4. In general, our Collective 87.4%  88.9%   82.8%   73.6%
model can capture complex interactions between candidate   Combined   85.8%   89.0%   85.6%   76.0%
pair decisions, potentially leading to better object identiﬁca-
tion.
  For random ﬁelds where maximum clique size is two and
all non-evidence nodes are Boolean, the inference problem  100                      100
can be reduced to a graph min-cut problem, provided cer-   80                        80
tain constraints on the parameters are satisﬁed [Greig et al.,  60                   60
1989]. Our formulation of the problem satisﬁes these con-  40  standard              40  standard
                                                        F-measure collective     F-measure collective
straints. Since min-cut can be solved exactly in polynomial  20                      20
time, we have a polynomial-time exact inference algorithm   0                        0
                                                            50 100 200 300  400       0  0.2  0.4  0.6  0.8  1
for our model. We follow the standard approach of gradient      Number of Clusters         Distortion
descent to learn the parameters. Calculating the exact deriva-
tive is intractable as it involves an expectation over an expo- (a) F-measure vs. number (b) F-measure vs. distor-
nential number of conﬁgurations. We use a voted perceptron of clusters           tion level
algorithm [Collins, 2002], which approximates this expecta-
tion by the feature counts of the most likely conﬁguration,
which we ﬁnd using our polynomial-time inference algorithm Figure 2: Experimental results on semi-artiﬁcial data.
with the current parameters.
  Combining models is often a simple way to improve ac- varied, respectively.1 The collective model clearly dominates
curacy. We combine the standard and collective models us- the standard model over a broad range of number of clusters
ing logistic regression. For each record-match node in the and level of distortion.
training set, we form a data point with the outputs of the two In summary, determining which observations correspond
models as predictors, and the true value of the node as the to the same object is a key problem in information integra-
response variable. We then apply logistic regression to this tion, citation matching, natural language, vision, and other ar-
dataset. Notice that this still yields a conditional random ﬁeld. eas. We have developed a collective approach to this problem,
  We  performed experiments on real and semi-artiﬁcial where information propagates among related decisions via
databases, comparing the performance of (a) the standard shared ﬁeld values, and shown experimentally that it outper-
Fellegi-Sunter model using logistic regression, (b) the col- forms the standard one of making decisions independently.
lective model, and (c) the combined model.
  The ﬁrst set of experiments was on Cora database, which Acknowledgments
is a collection of 1295 different citations to computer science
                                                      This research was partly supported by ONR grant N00014-
research papers. We cleaned it up by correcting some labels
                                                      02-1-0408, by a gift from the Ford Motor Co., and by a Sloan
and ﬁlling in missing values. This cleaned version contains
                                                      Fellowship awarded to the second author.
references to 132 different research papers. We used the au-
thor, venue, and title ﬁelds. The second set of experiments
was done on the BibServ.org database, which is the result of References
merging citation databases donated by its users, Citeseer, and [Collins, 2002] M. Collins. Discriminative training methods for
DBLP. We experimented on the user-donated subset of Bib- hidden Markov models: Theory and experiments with perceptron
Serv, which contains 21,805 citations. Table 1 reports the algorithms. In Proc. EMNLP-02, pages 1–8, 2002.
results for these databases. The combined model gives the [Fellegi and Sunter, 1969] I. Fellegi and A. Sunter. A theory for
best performance on both Cora and BibServ, followed by the record linkage. Journal of the American Statistical Association,
collective model. Transitive closure helps these on Cora but 64:1183–1210, 1969.
hurts all models on BibServ (where recall was close to 100% [Greig et al., 1989] D. M. Greig, B. T. Porteous, and A. H. Seheult.
even without transitive closure). The best combined model Exact maximum a posteriori estimation for binary images. Jour-
outperforms the best standard model in F-measure by about nal of the Royal Statistical Society, Series B, 51:271–279, 1989.
2% on Cora and 3% on BibServ.                         [Lafferty et al., 2001] J. Lafferty, A. McCallum, and F. Pereira.
  To further observe the behavior of the algorithms, we gen- Conditional random ﬁelds: Probabilistic models for segmenting
erated variants of the Cora database by taking distinct ﬁeld and labeling sequence data. In Proc. 18th ICML, pages 282–289,
values from the original database and randomly combining 2001.
them to generate distinct papers. Figures 2(a) and 2(b) com-
pare the performance of the collective and standard models 1For clarity, we have not shown the curves for the combined
as number of clusters and level of distortion in the data are model, which are similar to the collective one’s.