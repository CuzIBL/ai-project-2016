       Efﬁciently Exploiting Symmetries in Real Time Dynamic Programming

                  Shravan Matthur Narayanamurthy         and  Balaraman Ravindran
                           Department of Computer Science and Engineering
                                 Indian Institute of Technology Madras
                            shravmn@cse.iitm.ac.in and ravi@cse.iitm.ac.in


                    Abstract                          ﬁnding a functionally equivalent smaller model using this ap-
                                                      proach forms the crux of the model minimization paradigm.
    Current approaches to solving Markov Decision     Identifying symmetrically equivalent situations frequently re-
    Processes (MDPs) are sensitive to the size of the sults in useful abstraction. Informally, a symmetric system
    MDP. When applied to real world problems though,  is one which is invariant under certain transformations onto
    MDPs exhibit considerable implicit redundancy,    itself. An obvious class of symmetries is based on geomet-
    especially in the form of symmetries. Existing    ric transformations such as, rotations, reﬂections and transla-
    model minimization methods do not exploit this re- tions. Existing work on model minimization of MDPs such as
    dundancy due to symmetries well. In this work,    [Givan et al., 2003] and [Zinkevich and Balch, 2001] do not
    given such symmetries, we present a time-efﬁcient handle symmetries well. They either fail to consider state-
    algorithm to construct a functionally equivalent re- action equivalence or do not provide speciﬁc algorithms to
    duced model of the MDP. Further, we present a     minimize an MDP, considering state-action equivalence.
    Real Time Dynamic Programming (RTDP) algo-          In this article we consider a notion of symmetries, in the
    rithm which obviates an explicit construction of the form of symmetry groups, as formalized in [Ravindran and
    reduced model by integrating the given symmetries Barto, 2002]. Our objective here is to present algorithms that
    into it. The RTDP algorithm solves the reduced    provide a way of using the symmetry information to solve
    model, while working with parameters of the origi- MDPs, thereby achieving enormous gains over normal solu-
    nal model and the given symmetries. As RTDP uses  tion approaches. First, we present a time-efﬁcient algorithm
    its experience to determine which states to backup, (G-reduced Image Algorithm) to construct a reduced model
    it focuses on parts of the reduced state set that are given the symmetry group. The reduced model obtained is
    most relevant. This results in signiﬁcantly faster functionally equivalent to the original model in that, it pre-
    learning and a reduced overall execution time. The serves the dynamics of the original model. Hence a solu-
    algorithms proposed are particularly effective in the tion in the reduced model will lead to a solution in the origi-
    case of structured automorphisms even when the    nal model. However, the reduced model can be signiﬁcantly
    reduced model does not have fewer features. We    smaller when compared to the original model depending on
    demonstrate the results empirically on several do- the amount of symmetry information supplied. Thus, solv-
    mains.                                            ing the reduced model can be a lot easier and faster. Fur-
                                                      ther, we identify that an explicit construction of a reduced
                                                      model is not essential for the use of symmetry information
1  Introduction                                       in solving the MDP. We use the G-reduced Image Algorithm
Markov Decision Processes (MDPs) are a popular way to as a basis to present the Reduced Real Time Dynamic Pro-
model stochastic sequential decision problems. But most gramming(RTDP) algorithm that integrates the symmetry in-
modeling and solution approaches to MDPs scale poorly with formation into the RTDP algorithm [Barto et al., 1995] used
the size of the problem. Real world problems often tend to be for solving MDPs. Though the algorithm works directly with
very large and hence do not yield readily to the current so- the original model it considers only a portion of the original
lution techniques. However, models of real world problems model that does not exhibit redundancy and also is relevant
exhibit redundancy that can be eliminated, reducing the size in achieving its goals. This focus on the relevance of states
of the problem.                                       results in signiﬁcantly faster learning leading to huge savings
One way of handling redundancy is to form abstractions, as in overall execution time. To make the algorithms more ef-
we humans do, by ignoring details not needed for performing fective, especially in terms of space, we advocate the use of
the immediate task at hand. Researchers in artiﬁcial intelli- certain structural assumptions about MDPs. We use several
gence and machine learning have long recognized the impor- domains to demonstrate the improvement obtained by using
tance of abstracting away redundancy for operating in com- the reduced RTDP algorithm.
plex and real-world domains [Amarel, 1968]. Given a model, After introducing some notation and background informa-

                                                IJCAI-07
                                                  2556tion in Sec. 2, we present the G-reduced Image Algorithm X to a set Y induces a partition (or equivalence relation) on X,
                                                          [ ] =  [ ]             ( ) = ( )     , 
in Sec. 3. We then present the reduced RTDP algorithm in with x f x f if and only if f x f x and x x are f -
                                                                       ≡                        ⊆   ×
Sec. 4. The experiments done and results achieved are pre- equivalent written x f x .LetB be a partition of Z X Y,
sented in Sec. 5. Finally we conclude the article by giving where X and Y are arbitrary sets. The projection of B onto X
                                                                                             
some directions for future work in Sec. 6.            is the partition B|X of X such that for any x, x ∈ X, [x] | =
                                                                                                   B X
                                                      [x ] | if and only if every block containing a pair in which x
                                                         B X                               
                                                      is a component also contains a pair in which x is a component
2  Notation and Background                                                                 
                                                      or every block containing a pair in which x is a component
2.1  Markov Decision Processes
                                                      also contains a pair in which x is a component.
A Markov Decision Process is a tuple S, A, Ψ, P, R,where
S = {1, 2,...,n} is a set of states, A is a ﬁnite set of actions, Deﬁnition 1. An MDP homomorphism h from an MDP
                                                      M   =  , , Ψ, ,            M  =  , , Ψ, , 
Ψ ⊆ S × A is the set of admissible state-action pairs, P : Ψ × S A  P R  to an MDP        S  A     P  R
                                                                      Ψ   Ψ
S → [0, 1] is the transition probability function with P(s, a, s ) is a surjection from to , deﬁned by a tuple of surjections
                                                      , { | ∈ }      (( , )) = ( ( ), ( ))    :  →   
being the probability of transition from state s to state s under f gs s S , with h s a f s gs a ,wheref S S
              : Ψ →  R                                and   :   →     for ∈  , such that: ∀ ,  ∈ , ∈
action a,andR          is the expected reward function,   gs As    A f (s) s S           s s   S a  As
with R(s, a) being the expected reward for performing action                         
                  =  { |( , ) ∈ Ψ}⊆                                                          
a in state s.LetAs   a s a          A denote the set            ( ( ), ( ), ( )) =        ( , , )
                                       ∀  ∈  ,                P  f s gs a f s            P s a s      (3)
actions admissible in state s. We assume that s S As is                             ∈[ ]
non-empty.                                                                          s s f
                                                                    ( ( ), ( )) =   ( , )
 A stochastic policy π is a mapping Ψ → [0, 1], s.t.,            R   f s gs a      R s a             (4)
  ∈  π( , ) = 1, ∀ ∈ .Thevalue  of a state under pol-
  a As s a       s  S                   s
icy π is the expected value of the discounted sum of future We use the shorthand h(s, a) for h((s, a)).
rewards starting from state s and following policy π there-
                       π                                                                     =   , { | ∈
after. The value function corresponding to a policy π is Deﬁnition 2. An MDP homomorphism h      f gs s
                      V                                }            M   =    , , Ψ, ,          M   =
the mapping from states to their values under π. It can be S from MDP         S A   P R   to MDP
           π                                           , , Ψ, ,                          M     M
shown that V satisﬁes the bellman equation:            S  A     P  R  is an MDP isomorphism from   to
                                                      if and only if f and g , are bijective. M is said to be isomor-
                                                                   s
    π                                   π           phic to M and vice versa. An MDP isomorphism from MDP
     ( ) =   π( , )  ( , ) + γ   ( , , )  ( )   (1)
  V   s        s a R s a        P s a s V s           M  to itself is call an automorphism of M.
          ∈                  ∈
          a As              s S
                                                                                                      M
where 0 ≤ γ<1 is a discount factor.                   Deﬁnition 3. The set of all automorphisms of an MDP ,
                                         π∗           denoted by AutM, forms a group under composition of ho-
  The solution of an MDP is an optimal policy that uni-                                           M
formly dominates all other policies for that MDP. In other momorphisms. This group is the symmetry group of .
       π∗ ( ) ≥ π( )     ∈            π
words V  s   V   s for all s S and for all .            Let G  be a subgroup of AutM.  The subgroup G in-
                                                      duces a partition G of Ψ: [( 1, 1)] = [( 2, 2)] if and
2.2  Factored Markov Decision Processes                              B         s  a  BG     s a  BG
                                                      only if there exists h ∈Gsuch that h(s1, a1) = (s2, a2) and
Factored MDPs are a popular way to model structure in (s1, a1), (s2, a2) are said to be G equivalent written (s1, a1) ≡G
MDPs. A factored MDP is deﬁned as a tuple  , , Ψ, , . ( , )            ≡
                                       S A  P  R      s2 a2 . Further if s1 BG|S s2 then we write as shorthand
                                         ⊆    M         ≡
The state set, given by M features or variable, S i=1 Si, s1 G|S s2. It can be proved that there exists a homomorphism
                                                       G                  
where Si is the set of permissible values for the feature i, A h from M to some M , such that the partition induced by
                                                       G                                        G
is a ﬁnite set of actions, Ψ ⊆ × is the set of admissible  G                           M
                         S   A                        h , Bh ,isthesameBG.Theimageof      under h is called
state-action pairs. The transition probabilities P are often de- the G-reduced image of M.
scribed by a two-slice Temporal Bayesian Network (2-TBN). Adding structure to the state space representation allows
The state transition probabilities can be factored as: us to consider morphisms that are structured, e.g., Projection
                       M                             homomorphisms (see sec. 5 of [Ravindran and Barto, 2003]).
                                   
         P(s, a, s ) =    Prob(s |Pre(s , a))   (2)   It can be shown that symmetry groups do not result in projec-
                               i     i                tion homomorphisms, except in a few degenerate cases.
                       i=1
                                                      Another simple class of structured morphisms that do lead to
where    ( , ) denotes the parents of node  in the 2-
      Pre si a                          si            useful symmetry groups are those generated by permutations
TBN corresponding to action a and each of the probabilities
                                                    of feature values. Let M be the set of all possible permuta-
Prob(s |Pre(s , a)) is given by a conditional probabilitiy table                                 M
     i     i                                         tions of {1,...,M}. Given a structured set X ⊆ =1 X and
associated with node . The reward function may be simi-                                          i   i
                  si                                  a permutation σ ∈  , we can deﬁne a permutation on X by
larly represented.                                    σ(  ,..., ) =  M ,...,    
                                                         x1    xM      xσ(1)   xσ(M) , and it is a valid permu-
                                                                      ∈                    ,...,  ∈
2.3  Homomorphisms and Symmetry Groups                tation on X if xσ(i) Xi for all i and for all x1 xM X.
This section has been adapted from [Ravindran and Barto, Deﬁnition 4. A permutation automorphism h on a structured
2002].                                                MDP  M  = S, A, Ψ, P, R is a bijection on Ψ ,deﬁnedbya
                                    ∈  , [ ]                           ( ), ( )     (( , )) = ( ( ), ( ))
  Let B be a partition of a set X.Foranyx X x B denotes tuple of bijections f s gs a , with h s a f s gs a ,
                                                              ∈      :  →
the block of B to which x belongs. Any function f from a set where f M S    S is a valid permutation on S and

                                                IJCAI-07
                                                  2557  :   →     for ∈  , such that: ∀ ,  ∈ , ∈          from it to compute the image MDP parameters. The algo-
gs As    A f (s) s S           s s   S a  As
                                                      rithm terminates when at least one representative from each
  ( ( ), ( ), ( )) = ( , , )                                         G
 P  f s gs a f s    P s a s                           equivalence class of has been examined. For a proof that
                    M                                the transition probabilities actually represent those for the re-
                                                    duced image, see App. A. The algorithm as presented as-
                  =    Prob(s ( )| f (Pre ( )(s ( ), a))) (5)
                             f i     f s f i          sumes that all states are reachable from the initial state. It is
                     i=1
       ( ( ), ( )) = ( , )                           easy, however, to modify the algorithm suitably. Assuming an
      R  f s gs a   R s a                       (6)   explicit representation for the symmetry group and that table
                                                    look-up takes constant time, the algorithm will run in time
  Here  (   ( )( , )) = { ( )| ∈   (   , )} with ( )
       f Pre f s s f (i) a s f j sj Pre s f (i) a s f j            |Ψ|.|G|
                   ( )                                proportional to     . However an explicit representation
assigned according to f s .                           of G demands exorbitant memory of the order of |G|.|Ψ|.
                                                        As discussed in Sec. 2.3, structured morphisms can be used
3  G-reduced Image Algorithm                          advantageously to reduce the state space. The advantage here
                                                      is that the morphisms forming the symmetry group need not
3.1  Motivation                                       be stored explicitly as they are deﬁned on the features instead
In a large family of tasks, the symmetry groups are known of states.
beforehand or can be speciﬁed by the designer through a su- For example, let us consider the case of permutation auto-
                                                                                            
perﬁcial examination of the problem. A straight forward ap- morphisms. To check whether (s, a) ≡G (s , a ), we need to
                                                                                            
proach to minimization using symmetry groups would require generate |G| states that are equivalent to (s , a ) by applying
us to enumerate all the state-action pairs of the MDP. Even each h ∈G. Each application of h incurs a time linear in the
when the symmetry group, G, is given, constructing the re- number of features. Thus in this case the time complexity of
                                                                                            
duced MDP by explicit enumeration takes time proportional the algorithm presented is of the order of |Ψ| .|G|.M,where
to |Ψ|.|G|.                                           M is the number of features whereas no space is needed for
  We present in Fig. 1, an efﬁcient incremental algorithm for storing the G explicitly.
building the reduced MDP given a symmetry group or sub- Thus by restricting the class of automorphisms to func-
group. This is an adaptation of an algorithm proposed by tions that are deﬁned on features instead of states, we only
[Emerson and Sistla, 1996] for constructing reduced models incur additional time which is a function of the number of
for concurrent systems.                               features (signiﬁcantly less than the number of states) along
                                                      with a drastic decrease in the space complexity. The use of
01 Given M = S, A, Ψ, P, R and G≤AutM,              factored representations leads to further reduction in space
                             
02 Construct M/BG = S , A , Ψ , P , R .             needed for storing the transition probabilities and the reward
                             
03 Set Q to some initial state {s0}, S ←{s0}          function, thereby making the algorithm presented more effec-
04 While Q is non-empty                               tive than its use in the generic case. Also as G is just a sub-
05   s = dequeue{Q}                                   group, the algorithm can work with whatever little symmetry
06   For all a ∈ A                                    information the designer might have.
                s                     
07      If (s, a) G (s , a ) for some (s , a ) ∈ Ψ , then
                  
08         Ψ  ←  Ψ  ∪ (s, a)
                                                    4   Reduced RTDP Algorithm
09         A  ←  A ∪ a
             
10         R (s, a) = R(s, a)                         4.1  Motivation
11         For all t ∈ S such that P(s, a, t) > 0
                                                   Given a real world problem modeled as an MDP, the state
12            If t ≡G| s , for some s ∈ S ,
                   S                              space invariably consists of vast regions which are not rele-
13               P (s, a, s ) ← P (s, a, s ) + P(s, a, t) vant in achieving the goals. The minimization approach leads
14            else
                                                    to a certain degree of abstraction which reduces the extent of
15               S ←  S ∪ t
                                                     such regions. Nonetheless the reduced image still contains
16               P (s, a, t) = P(s, a, t)             regions which are not relevant in achieving the goals even in
17               add t to Q.                          the reduced model. Since our goal here is only to ﬁnd a pol-
                                                      icy for acting in the original model, we can forgo the explicit
Figure 1: Incremental algorithm for constructing the G- construction of the reduced model by integrating the informa-
                        M           G≤     M.
reduced image given MDP    and some     Aut    Qis    tion in the symmetry group into the algorithm which solves
the queue of states to be examined. Terminates when at least the original model. Though there are a variety of ways to
                                         G
one representative from each equivalence class of has been solve an MDP, we choose to take up RTDP as it uses the ex-
examined.                                             perience of the agent to focus on the relevant sections of the
                                                      state space. This saves the time spent on explicit construction
                                                      of the reduced model.
3.2  Comments                                           Also the G-reduced Image algorithm as presented doesn’t
The algorithm does a breadth-ﬁrst enumeration of states skip- preserve any structure in the transition probabilities or the
ping states and state-action pairs that are equivalent to those reward function that might have existed because of the use
already visited. On encountering a state-action pair not equiv- of factored representations. Consequently the reduced image
alent to one already visited, it examines the states reachable might take considerably more space than the original model.

                                                IJCAI-07
                                                  255801 Given M = S, A, Ψ, P, R and G≤AutM,              the reduced image. As normal RTDP converges to an optimal
02 Hashtable Q ← Nil is the action value function.    action value function [Barto et al., 1995], the reduced RTDP
03 Repeat (for each episode)
                                                     also converges, as long as it continues to back up all states in
04   Initialize s and S ←{s}                          the reduced image.
05   Choose a from s using policy derived from Q (e.g.
                                                       The complete construction of the reduced image can take
      -greedy policy)                                 up a considerable amount of time mapping all the irrelevant
06   Repeat (for each step in the episode)
                                              states into the reduced model whereas with the use of this
07      if (s, a) ≡G (s , a ) for some (s , a ) ∈ Q
                                                  algorithm one can get near optimal policies even before the
        where (s , a )  (s, a)
             ←   ; ←                              construction of a reduced image is complete. It is also faster
08          s   s  a   a                              than the normal RTDP algorithm as its state space is reduced
09         continue.                                  by the use of the symmetry group information.
10      Take action a and observe reward r and
                 
        next state s
                     
11      Choose a from s using policy derived from Q   5   Experiments and Results
        (e.g.-greedy policy)
12      For all t such that P(s, a, t) > 0            Experiments were done on three domains that are explained
                               
13         If t ≡G| s , for some s ∈ S ,              below. To show the effect of the degree of symmetry consid-
                S            
14            P (s, a, s ) ← P (s, a, s ) + P(s, a, t) ered in the domain we consider a 2-fold symmetry for which
15         else                                       G is a strict subgroup of AutM,thatis,G < AutM and full
                                                             G  =    M
16            S  ← S ∪ t                              symmetry      Aut  . We compare the reduced RTDP al-
               
17            P (s, a, t) = P(s, a, t)                gorithm using these 2 degrees of symmetry with the normal
18      if (s, a)  Q                                 RTDP algorithm. We present learning curves representing the
19         add (s, a) to Q.                           decrease in the number of steps taken to ﬁnish each episode.
20         Q(s, a) ← 0                                To show the time efﬁciency of the reduced RTDP algorithm
21      Q(s, a) ← R(s, a)                             we present a bar chart of times taken by the reduced RTDP al-
                                                     gorithm using 2 degrees of symmetry and the normal RTDP
                                 
          +     γ.P (s, a, s ). max Q(s , a )         algorithm for completing 200 episodes of each domain. All
                            ∈ 
                        a A                                                         γ = 0.9
            s ∈S              s                       the algorithms used a discount factor,  . An epsilon
                                                      greedy policy with  = 0.1 was used to choose the actions
          ←  ; ←   
22      s   s  a   a                                  at each step. Due to lack of space we present one graph per
                                                      domain though experiments were done with different sizes of
Figure 2: RTDP algorithm with integrated symmetries which each domain. The results are similar in other cases. We note
computes the Action Value function for the reduced MDP exceptions if any as is relevant.
without explicitly constructing it.
                                                      5.1  Deterministic Grid-World(DGW)
The algorithm we present in Fig. 2 tries to achieve the best
of both worlds as it not only works with the original model Two Grid-Worlds of sizes 10x10 and 25x25 with four de-
but also includes the state space reduction by integrating the terministic actions of going UP, DOWN, RIGHT and LEFT
symmetry group information into the RTDP algorithm.   were implemented. The initial state was (0,0) and the goal
                                                      states were {(0,9),(9,0)} and {(0,24),(24,0)} respectively. For
4.2  Convergence of Reduced RTDP                      the 2-fold symmetry, states about NE-SW diagonal, i.e., (x,y)
                                                      and (y,x) were equivalent. If the grid is of size M × N then
The algorithm is a modiﬁcation of the RTDP algorithm with          − 1               − 1
steps from the previous algorithm integrated into lines 7 to let maxX = M and maxY = N  . For the full sym-
                                                     metry case, states (x,y), (y,x), (maxX-x,maxY-y) and (maxY-
17. If we assume that we have the reduced MDP M ,then
leaving out lines 7 to 9 and lines 12 to 17 leaves us with y,maxX-x) were equivalent. State action equivalence was de-
the normal RTDP algorithm being run on the reduced image ﬁned accordingly.
                                    
since as explained below, for all (s, a) ∈ Ψ , R (s, a) = R(s, a).
Due to the equivalence tests done at lines 7 and 13, the algo- 5.2 Probabilistic Grid-World(PGW)
rithm maintains a policy for and considers only the reduced Two Grid-Worlds of sizes 10x10 and 25x25 with four actions
state space. From App. A, lines 12 to 17 compute the transi-
                                              ( , )   of going UP, DOWN, RIGHT and LEFT were implemented.
tion probabilities for the reduced image. From Eqn. 6, R s a Unlike the deterministic domain, here actions led to the rele-
is the expected reward under the reduced image. So for all
( , ) ∈ Ψ, ( , ) = ( , )                            vant grid only with a probability of 0.9 and left the state un-
s a       R  s a   R s a . Thus the update equation in changed with a probability of 0.1. The initial state was (0,0)
line 21 can be rewritten as,                                               {        }    {           }
                                                     and the goal states were (0,9),(9,0) and (0,24),(24,0) re-
                                              spectively. For the 2-fold symmetry, states about NE-SW di-
  Q(s, a) = R (s, a) + γ.P (s, a, s ). max Q(s , a ) (7)
                                  ∈               agonal, i.e., (x,y) and (y,x) were equivalent. For the full sym-
                   ∈          a As
                  s S                                 metry case, states (x,y), (y,x), (maxX-x,maxY-y) and (maxY-
which is nothing but the update equation for the reduced im- y,maxX-x) were equivalent. State action equivalence was de-
age. Thus it is exactly similar to running normal RTDP on ﬁned accordingly.

                                                IJCAI-07
                                                  2559            2500

                                                                   5000
            2000
                                                                           Normal RTDP
                    Normal RTDP                                    4000
            1500

                                                                   3000
                      Reduced RTDP                                           Reduced RTDP
            1000      2-fold symmetry                                        2-fold symmetry


           #  Steps per Episode                                    2000
                         Reduced RTDP
             500                                                  #  Steps per Episode Reduced RTDP
                         full symmetry                                         full symmetry
                                                                   1000

              0
              0     50   100   150   200
                                                                     0
                        # Episodes                                   0     50   100   150  200
                                                                              # Episodes
Figure 3:  Learning curves for the Deterministic Grid
World(25x25 grid).                                    Figure 5: Learning curves for the Probabilistic Towers of
                                                      Hanoi(5 disks).

            3000
                                                      Hanoi with 5 disks.1
            2500

            2000
                    Normal RTDP                                   140

            1500                                                  120
                      Reduced RTDP
                      2-fold symmetry
            1000                                                  100
           #  Steps per Episode
                         Reduced RTDP                              80
             500         full symmetry
                                                                   60
              0
              0     50   100   150   200                           40
                        # Episodes


                                                                 Time(scaled)  taken for 200 episodes 20

                                                                   0
Figure 4:  Learning curves for the Probabilistic Grid                   1       2       3
World(25x25 grid).                                                         Different Domains
                                                            Figure 6: Comparison of running times(scaled).
5.3  Probabilistic Towers of Hanoi(PTOH)
The towers of Hanoi domain as implemented had 3 pegs. Two
domains, one with 3 and the other with 5 disks were imple- 5.5 Discussion
mented. Actions that allowed the transfer of a smaller disk
                                                      The results are as expected. The comparisons show that the
onto a larger disk or to an empty peg were permitted. The
                                                      reduced RTDP algorithm learns faster than the normal RTDP
actions did the transfer of disk with a probability of 0.9 and
                                                      both in the full symmetry case as well as in the 2-fold sym-
left the state unchanged with a probability of 0.1. The initial
                                                      metry case. Further among the reduced RTDP algorithms, the
state in the case of 3 disks was {(1,3), (2), ()} and {(4), (1,2),
                                                      one using more symmetry is better than the one with lesser
(3,5)} in the 5 disk case. The goal states were designed to
                                                      symmetry. The same is reﬂected in the running times of algo-
allow various degrees of symmetry. For a 2-fold symmetry,
                                                      rithms. The full symmetry case is at least 5 times faster than
the goal states considered were states where, all disks were
                                                      the normal RTDP. The 2-fold symmetry is also faster than the
either on peg 1 or 2. Equivalent states were those that have
                                                      normal RTDP.
disk positions of pegs 1 and 2 interchanged. For the full sym-
metry case, the goal states considered were states where, all One observation contrary to the graph shown in the bar
disks were on any one peg. Equivalent states were those that graph of Fig. 6 is that when reduced RTDP algorithms are
have disk positions interchanged by any possible permutation used for very small domains like 3-disk Towers of Hanoi,
of the pegs. State action equivalence was deﬁned accordingly. the overhead involved in checking equivalence of states out-
                                                      weighs the beneﬁt from the reduction due to symmetry.
5.4  Time efﬁciency                                   Though we have not been able to quantify the exact extent
                                                      of the trade-offs involved, we feel that when the expected
The bar-graph in Fig. 6 shows the running times (scaled to length of a trajectory to the goal state from the initial state
even the graph) of the normal RTDP, reduced RTDP with a is small in comparison with the state space, the beneﬁts ob-
2-fold symmetry and reduced RTDP with full symmetry. The tained by using the symmetry group information are masked
ﬁrst cluster is on the Deterministic Grid-World domain with a
25x25 grid, the second cluster is on Probabilistic Grid-World 1Running times for domains of lesser size do not follow the pat-
with a 25x25 grid and the third on Probabilistic Towers of tern indicated by the graphs. See Sec. 5.5

                                                IJCAI-07
                                                  2560