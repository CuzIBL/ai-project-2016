                            Algebraic Markov Decision Processes

           Patrice Perny                   Olivier Spanjaard                    Paul Weng
      LIP6 - Université Paris 6         LIP6 - Université Paris 6         LIP6 - Université Paris 6
           4 Place Jussieu                   4 Place Jussieu                  4 Place Jussieu
    75252 Paris Cedex 05, France     75252 Paris Cedex 05, France      75252 Paris Cedex 05, France
        patrice.perny@lip6.fr           olivier.spanjaard@lip6.fr            paul.weng@lip6.fr

                    Abstract                          • Non-probabilistic representation of uncertainty might be
                                                      of interest. In practice, it is sometimes difﬁcult to quantify
    In this paper, we provide an algebraic approach to precisely the plausibility of states and consequences of ac-
    Markov Decision Processes (MDPs), which allows    tions. Assessing probabilities in such situations seems difﬁ-
    a uniﬁed treatment of MDPs and includes many ex-  cult. For this reason, alternative approaches based on quali-
    isting models (quantitative or qualitative) as par- tative representations of uncertainty have been proposed (see
    ticular cases. In algebraic MDPs, rewards are ex- e.g. [Darwiche and Ginsberg, 1992; Dubois and Prade, 1995;
    pressed in a semiring structure, uncertainty is rep- Wilson, 1995]) and might be used in the context of MDPs.
    resented by a decomposable plausibility measure
    valued on a second semiring structure, and prefer- • Non-EU theories offer also interesting descriptive possibil-
    ences over policies are represented by Generalized ities. Despite the appeal of the expected utility model and
    Expected Utility. We recast the problem of ﬁnd-   its theoretical foundations [von Neumann and Morgenstern,
    ing an optimal policy at a ﬁnite horizon as an alge- 1947; Savage, 1954], recent developments of decision theory
    braic path problem in a decision rule graph where have shown the descriptive potential of alternative representa-
    arcs are valued by functions, which justiﬁes the  tions of preferences under uncertainty. For example, the rank
    use of the Jacobi algorithm to solve algebraic Bell- dependent expected utility theory (RDEU) is a sophistication
    man equations. In order to show the potential of  of EU theory using probability transforms to better account
    this general approach, we exhibit new variations  for actual decision making behaviors under risk [Quiggin,
    of MDPs, admitting complete or partial preference 1993]. Besides this extension, various alternatives to EU have
    structures, as well as probabilistic or possibilistic been proposed for decision making in a non-probabilistic set-
    representation of uncertainty.                    ting. Among them, let us mention the qualitative expected
                                                      utility (QEU) theories proposed in [Dubois and Prade, 1995]
                                                      and [Giang and Shenoy, 2001], and very recently, the gen-
1  Introduction                                       eralized expected utility theory (GEU) proposed in [Chu and
                                                      Halpern, 2003a; 2003b] which generalizes the notion of ex-
In the ﬁeld of planning under uncertainty, the theory of pectation for general plausibility measures.
Markov Decision Processes (MDPs) has received much at-
tention as a natural framework both for modeling and solving Despite the diversity of models proposed for decision mak-
complex structured decision problems, see e.g. [Dean et al., ing under uncertainty, very few of them are used in the con-
1993; Kaebling et al., 1999]. In the standard MDP approach, text of dynamic decision making. This gap can be explained
the utilities of actions are given by scalar rewards supposed by the inconsistencies entailed by the use of non-EU criteria
to be additive, uncertainty in the states of the world and in (typically RDEU) in the dynamic context [Machina, 1989;
the consequences of the actions are represented with proba- Sarin and Wakker, 1998]. Indeed, when using a given non-
bilities, and policies are evaluated using the expected utility linear utility criterion at each decision stage, the Bellman
(EU) model. Although these choices are natural in various principle is generally violated, so that backward induction is
practical situations, many other options are worth investigat- likely to generate a dominated policy; further, there is in gen-
ing for several reasons :                             eral no operational way to determine an optimal policy. This
• Rewards of actions are not necessarily scalar nor additive. simple statement largely explains the predominance of the EU
In multi-agent planning or in multicriteria MDPs, the utility model in dynamic decision making under risk.
of any action is given by a vector of rewards (one per agent or In the last decade however, some alternative models to EU
criterion) and actions are compared according to Pareto dom- have been proved to be dynamically consistent, thus provid-
inance [Wakuta, 1995]. In qualitative frameworks, rewards ing new possibilities for sequential decision making. This
are valued on an ordinal scale and therefore are not additive. is the case of qualitative expected utility theory [Dubois and
The sum is then replaced by the min, max or any reﬁnement Prade, 1995] that has led to a possibilistic counterpart of
of them, depending on the context.                    MDPs  [Sabbadin, 1999] with efﬁcient algorithms adaptedfrom backward induction and value iteration, substituting op-             δ1  δ2   δ3  δ4
erations (+, ×) by (max, min) in computations. In the same           s1   a1  a1   a2  a2
vein, [Littman and Szepesvári, 1996] propose a generalized           s2   a1  a2   a1  a2
version of MDPs where     and   are substituted by ab-
                     max     +                                        Table 1: Decision rules.
stract operators in Bellman equations, and [Bonet and Pearl,
2002] propose a qualitative version of MDPs. Besides these       t
                                                      R(γt) =    i=1 R(si, ai). We denote Γt(s) the set of t-step
positive results, very few alternatives to standard MDPs have histories starting from s. For an initial state s, a t-step policy
been investigated.                                            P                                     π
                                                      π = (δt, . . . , δ1) induces a probability distribution Prt (s, ·)
  Among the diversity of possible choices for deﬁning a re- over histories. The t-step value of being in state s and exe-
ward system, a plausibility measure over events and a pref- cuting policy π is given by (expected accumulated reward):
erence over lotteries on rewards, we need to know which
                                                                 vπ(s) =         Prπ(s, γ)R(γ)
combination of them can soundly be used in the context of         t        γ∈Γt(s) t
                                                                π        P       th             π  i
MDPs and which algorithm should be implemented to deter- Denoting vt the vector whose i component is vt (s ), any
mine an optimal policy. For this reason, we propose in this pa- two t-step policies π, π′ can be compared using the compo-
per an algebraic generalization of the standard setting, relying nentwise dominance relation ≥Rn deﬁned by:
                                                                      ′                        ′
on the deﬁnition of a semiring structure on rewards, a semi-  π      π                π       π
                                                             v  ≥Rn v   ⇐⇒    ∀s ∈ S, v (s) ≥ v (s)
ring structure on plausibilities of events, and a generalized t      t                t       t
                                  [                   The t-step value of a policy  of the form (δi, π) is deﬁned re-
expectation model as decision criterion Chu and Halpern,            i                     i
2003a]. The generalization power of semirings have been al-       (δ ,π)                 (δ ,π)   i  π
                                                      cursively by v0   =  (0, . . . , 0) and vt = f (vt−1),
ready demonstrated in AI by [Bistarelli et al., 1995] in the where f i : Rn → Rn is the update function which associates
context of constraint satisfaction problems. Within this gen-                              i        i
                                                      to any vector x = (x1, . . . , xn) the vector (f1(x), . . . , fn(x))
eral setting, our aim is to present a uniﬁed treatment of MDPs i       j i  j      n      j  i j   k
                                                      where f (x) = R(s , δ (s )) +    T (s , δ (s ), s )xk.
and to provide algorithmic solutions based on the general Ja- j                    k=1
                                                                                 P
cobi algorithm (initially introduced to solve systems of linear Example 1 Consider an MDP with S = {s1, s2}, A =
equations).                                           {a1, a2}, T (si, aj , sk) = 1 if i = j = k and 0.5 other-
  The paper is organized as follows: in Section 2, we show wise, R(s1, a1) = 8, R(s1, a2) = 7, R(s2, a1) = 12 and
by example how to recast an MDP as an optimal path prob- R(s2, a2) = 11. Thus, there are N = 4 available decision
lem in a decision graph. Then we introduce an algebraic rules at each step (see Table 1). For instance, decision rule
framework for MDPs, relying on the deﬁnition of algebraic δ2 consists in applying action a1 in states s1 and action a2
structures on rewards, plausibilities and expectations (Section in state s2. In this example, the functions f i are given by:
                                                        1
3). In Section 4 we justify the use of the Jacobi algorithm f (x1, x2) = (8 + x1, 12 + 0.5x1 + 0.5x2)
                                                        2
as a general procedure to determine an optimal policy in an f (x1, x2) = (8 + x1, 11 + x2)
                                                        3
Algebraic MDP (AMDP). Finally, we consider in Section 5 f (x1, x2) =  (7 + 0.5x1 + 0.5x2, 12 + 0.5x1 + 0.5x2)
                                                        4
some particular instances of AMDPs, including new proba- f (x1, x2) = (7 + 0.5x1 + 0.5x2, 11 + x2)
bilistic and possibilistic MDPs using partial preferences over for all (x , x ) ∈ R2.
rewards.                                                      1  2
                                                        Given a ﬁnite horizon H, the optimal policy π∗ can be
2  Decision Rule Graph in MDPs                        found thanks to the following Bellman equations:
                                                                ∗
We brieﬂy recall the main characteristics of a Markov Deci-   vπ  = (0, . . . , 0)
                                                               0 ∗                 ∗
                 [             ]                               π               i  π                   (1)
sion Process (MDP) Puterman, 1994 . It can be described as    vt  = maxi=1...N f (vt−1)  t = 1 . . . H
a tuple (S, A, T, R) where:
• S = {s1, . . . , sn} is a ﬁnite set of states,        The solution of these equations can be reduced to a vector-
        1     m                                       weighted optimal path problem in a particular graph, with up-
• A = {a , . . . , a } is a ﬁnite set of actions,                      i
• T : S ×A → Pr(S) is a transition function, giving for each date functions (the f ’s) on the arcs allowing the propagation
                                                      of policy values over nodes. Indeed, consider a graph where
state and action, a probability distribution over states (in the i                          i
sequel, we write T (s, a, s′) for T (s, a)(s′)),      each node δt corresponds to decision rule δ at step t, and
                                                                         i  j
• R: S × A → R  is a reward function giving the immediate each arc of the form (δt, δt−1) corresponds to a transition be-
reward for taking a given action in a given state.                                       j
                                                      tween decision rules. Moreover, nodes δ1, j = 1 . . . N are
A decision rule is a function from the set of states S to the connected to a sink denoted 0, and a source denoted H is
                             n                                          j
set of actions A. There are N = m available decision rules connected to nodes δH , j = 1 . . . N. Hence, any path from
                          S      1     N                    i
at each step. We write ∆ = A = {δ , . . . , δ } the set of node δt to node 0 corresponds to a t-step policy where deci-
decision rules. A policy at step t (i.e., the tth-to-last step) is sion δi is applied ﬁrst. We name that graph the decision rule
a sequence of t decision rules. For a policy π and a decision graph. Note that the Bellman update via f i’s is nicely sepa-
                                                             i
rule δ, we note (δ, π) the policy which consists in applying rable (fj ’s can be computed independently) and therefore the
ﬁrst decision rule δ and then policy π.               vector value of a path can be obtained componentwise (state
  A history is a realizable sequence of successive states and by state) as usual in classic MDP algorithms. This property
actions. The accumulated reward corresponding to a his- will be exploited later on for Algebraic MDPs. Coming back
tory γt = (st, at, st−1, . . . , a1, s0) (with initial state st) is to Example 1 and assuming that H = 2, there are 16 available                   δ1         - δ1                          ⊞                  ⊠
                   2         37 1                   where   : V × V →  V and   : P × V →  V are the coun-
                                                      terparts of + and × in probabilistic expectation, and the three
                                                      following requirements are satisﬁed: (x⊞y)⊞z = x⊞(y⊞z),
                   δ2         s- δ2                     ⊞      ⊞   1  ⊠
                 *  2         37 1                    x   y = y  x, P   x = x. For any plausibility distribution
                                                         on V having its support in X, the generalized expectation
         H                              j^ 0          Pl
                                        *                     ⊞        ⊠
                                                      writes:  x∈X Pl(x)  x.
                 j δ3         s-w δ3
                    2         3  1                      An AlgebraicP MDP (AMDP) is described as a tuple
                                                      (S, A, T, R), where T and R are redeﬁned as follows:
                                                                    Pl                             Pl
                  ^ δ4        s-wU δ4                 • T : S × A →   (S) is a transition function, where (S)
                    2            1                    is the set of plausibility measures over S valued in P ,
                                                      • R: S × A →  V is a reward function giving the immediate
            Figure 1: A decision rule graph.          reward in V .
policies (all possible combinations of two successive decision Consistently with the standard Markov hypothesis, the next
rules). The corresponding graph is pictured on Figure 1. state and the expected reward depend only on the current state
  For all i = 1 . . . n, function f i is associated to every arc and the action taken. In particular, plausibility distributions
           i                                          of type T (s, a) are (plausibilistically) independent of the past
issued from δt, t = 1 . . . H. Moreover, the identity function
is assigned to every arc issued from H. Thus, the optimal states and actions. This plausibilistic independence refers to
policy can be computed thanks to backward induction on the the notion introduced by [Friedman and Halpern, 1995] and
decision rule graph, by propagating the value (0, . . . , 0) ∈ leads to the following algebraic counterpart of the probabilis-
Rn from the sink 0 corresponding to the empty policy. In tic independence property: Pl(X ∩ Y ) = Pl(X) ⊗P Pl(Y )
Example 1, backward induction leads to the labels indicated for any pair X, Y of independent events.
in Table 2. The optimal policy value can be recovered on In this setting, rewards and plausibilities take values in two
node H. The optimal vector value is (17, 23) and the optimal semirings. Roughly speaking, a semiring is a set endowed
policy (recovered from bolded values in Table 2) is (δ4, δ1). with two operators allowing the combination of elements (re-
                                            2  1      wards or plausibilities) together. We now recall some deﬁni-
              1        2       3        4
        t    δt       δt      δt       δt             tions about semirings.
       1    (8,12)  (8,11)   (7,12)  (7,11)
       2   (16,22)  (16,23) (17,22)  (17,23)          Deﬁnition 1 A semiring (X, ⊕, ⊗, 0, 1) is a set X with two
                                                      binary operations ⊕ and ⊗, such that:
   Table 2: Labels obtained during backward induction.
                                                      (A1) (X, ⊕, 0) is a commutative semigroup with 0 as neutral
                                                      element i.e., a b b a, a  b  c   a  b  c , a 0  a .
  In the next section, we show how to generalize this ap-    (     ⊕  =  ⊕  ( ⊕  )⊕  =  ⊕( ⊕  )  ⊕  =  )
                                                                 1                  1
proach to a wide range of MDPs. In this concern, we intro- (A2) (X, ⊗, ) is a semigroup with as neutral element, and
duce the notion of algebraic Markov decision process. for which 0 is an absorbing element (i.e., (a ⊗ b) ⊗ c = a ⊗
                                                      (b ⊗ c), a ⊗ 1 = 1 ⊗ a = a, a ⊗ 0 = 0 ⊗ a = 0).
3  Algebraic Markov Decision Process                  (A3) ⊗ is distributive with respect to ⊕ (i.e., (a ⊕ b) ⊗ c =
                                                      (a ⊗ c) ⊕ (b ⊗ c), a ⊗ (b ⊕ c) = (a ⊗ b) ⊕ (a ⊗ c)).
We now deﬁne a more general setting to model rewards and
uncertainty in MDPs. Our approach relies on previous works A semiring is said to be idempotent when (X, ⊕) is
aiming at generalizing uncertainty measurement and expec- an idempotent commutative semigroup (i.e., a commutative
tation calculus. Our rewards take values in a set V and we semigroup such that a ⊕ a = a). The idempotence of ⊕ en-
                     1
use plausibility measures [Friedman and Halpern, 1995] to ables to deﬁne the following canonical order relation X :
model uncertainty. A plausibility measure Pl is here a func-
tion from 2W (the set of events) to P , where W is the set     a X  b ⇐⇒   a ⊕ b = a  ∀a, b ∈ X
of worlds, P is a set endowed with two internal operators ⊕
                                                 P      From now on, we will assume that the rewards are elements
and ⊗P (the analogs of + and × in probability theory), a (pos-                          0   1
                                                      of an idempotent semiring (V, ⊕V , ⊗V , V , V ). Operator
sibly partial) order relation P , and two special elements 0P
                                                      ⊕V is used to select the optimal values in V , whereas operator
and 1P such that 1P P p P 0P for all p ∈ P . Further-
                                                      ⊗V  is used to combine rewards. In classic MDPs with the
more, Pl veriﬁes Pl(∅) = 0P , Pl(W ) = 1P and Pl(X) P
                                                      total reward criterion, ⊕V = max and ⊗V = +.
Pl(Y ) for all X, Y such that Y ⊆ X ⊆ W . We assume here                               0   1
                                                        Moreover the structure (P, ⊕P , ⊗P , P , P ) is also sup-
that Pl is decomposable, i.e. Pl(X ∪ Y ) = Pl(X) ⊕P Pl(Y )
for any pair of disjoint events X and Y . To combine plausi- posed to be a semiring. Operator ⊕P allows to combine the
bilities and rewards, we use the generalized expectation pro- plausibilities of disjoint events and operator ⊗P allows to
posed by [Chu and Halpern, 2003a]. This generalized ex- combine the plausibilities of independent events. Note that
                                                      the assumption that P, ,   , 0 , 1 is a semiring is not
pectation is deﬁned on an expectation domain (V, P, ⊞, ⊠) 2            (  ⊕P  ⊗P   P  P )
                                                      very restrictive since [Darwiche and Ginsberg, 1992], who
  1This notion must not be confused with the Dempster-Shafer no- use similar properties to deﬁne symbolic probability, have
tion of plausibility function.                        shown that it subsumes many representations of uncertainty,
  2In [Chu and Halpern, 2003a], an expectation domain is written such as probability theory, possibility theory and other impor-
(U, P, V, ⊞, ⊠). This structure can be simpliﬁed here since V = U. tant calculi used in AI.  Now that the general framework has been deﬁned, we  3) to justify a dynamic programming approach.
can follow the usual approach in MDPs and deﬁne a value                                         i
                                                      Proposition 1 If (C1) and (C2) hold, then f is non-
function for policies. The accumulated reward for a history           i
                                    t                 decreasing for all δ ∈ ∆, i.e
γ = (st, at, st−1, . . . , a1, s0) is R(γ) = i=1 R(si, ai). For      n              i         i
                                                            ∀x, y ∈ V , x V n y ⇒ f (x) V n f (y)
an initial state s, a t-step policy π = (δNt, . . . , δ1) induces a
                   π                                                     n                        
plausibility measure Plt (s, ·) over histories. Such a policy Proof. Let x, y in V s.t. x V n y. For sj ∈ S, we have
will be evaluated with respect to the generalized expectation ⊞                     ⊞
                                                          T (sj , δi(sj ), sk) ⊠x    T (sj , δi(sj ), sk) ⊠y
of accumulated reward, which writes:                    k                   k  V    k                   k
                                                      by (C1) and (C2). Thanks to distributivity of ⊗V over ⊕V ,
          π        ⊞       π                          P                           P
                                 ⊠                             i        i              i      n  i
         vt (s) =  γ∈Γt(s) Plt (s, γ) R(γ)            we have fj (x) V fj (y). Therefore, f (x) V f (y).
The policies can be comparedP with respect to the componen-
                                           n            Moreover, the value of a policy π can be computed recur-
twise dominance relation V n between vectors in V :
                                                      sively thanks to the following result:
       x V n y ⇐⇒   (∀i = 1, . . . , n, xi V yi)
                                                                             i  ′
                                       n              Proposition 2 Let π = (δ , π ) a (t + 1)-step policy, and
for all x = (x1, . . . , xn), y = (y1, . . . , yn) ∈ V .          π
                                                      assume that v =  (1V , . . . , 1V ). If (C3), (C4) and (C5)
  Most of the MDPs introduced previously in the literature        0       ′
                                                                π      i π
are instances of our algebraic MDP:                   hold, then vt+1 = f (vt ) for all t ≥ 0.
- In standard MDPs, the underlying algebraic structure Proof.  Let s be a state and a denote δi(s). We note
                    0  1
on P  is (P, ⊕P , ⊗P , P , P ) = ([0, 1], +, ×, 0, 1), and γt = (st, at, . . . , a1, s0) and γt+1 = (s, a, st, at, . . . , a1, s0).
operators ⊞ =   +  and ⊠   =   × are used to deﬁne    We have:
                                                                   ⊞
the classic expectation operation. When rewards are de-           X
                                                       π                  π         ⊠
                  0   1      R                        vt+1(s) =          Plt+1(s, γt+1) R(γt+1)
ﬁned on (V, ⊕V , ⊗V , V , V ) = ( , max, +, −∞, 0) where
R     R                                                        γt+1∈Γt+1(s)


   =    ∪ {−∞}, we recognize the total reward crite-     ⊞     ⊞


                                                        X     X


                                                                                       ¡                ¡
                                                                                 ′
rion. With (R, max, + , −∞, 0) (where x + y = x + γy),                    ′     π   ′   ⊠
                   γ                  γ               =             T (s, a, s )⊗P Plt (s , γt) R(s, a)⊗V R(γt)
                                                         ′       ′
we recognize the weighted total reward criterion. With  s ∈S γt∈Γt(s )


 R,    ,   ,    ,  where a   b   1 a  b, we recognize             ⊞    ⊞
                                                                       X


(  max  +H  −∞  0)        +H  =     +                            X


                                                                                                 ¡      ¡
                                 H                                                        ′
                                                                                   ′     π   ′    ⊠
the average reward criterion assuming that there is an initial = R(s, a)⊗V   T (s, a, s )⊗P Plt (s , γt) R(γt)
                                                                  ′       ′
dummy step with zero reward.                                     s ∈S γt∈Γt(s )
- Qualitative MDPs, introduced in [Bonet and Pearl, 2002], by (C4)


                                                                   ⊞                ⊞


                                                                  X                X
                                                                                                        ¡
                                                                                           ′
are AMDPs where the rewards and the plausibility measures                   ′ ⊠           π  ′   ⊠
                                                      = R(s, a) ⊗V   T (s, a, s )       Plt (s , γt) R(γt)
are deﬁned on the semiring of two-sided inﬁnite formal se-        ′                   ′
ries, which is a subset of the extended reals [Wilson, 1995].     s ∈S           γt∈Γt(s )
                                                      by (C3) and (C5)
  In order to assign functions to the arcs in the decision rule    ⊞

                                                                  X              ′
graph, we ﬁrst deﬁne f i : V n → V (the update function after               ′ ⊠  π  ′
                   j                                  = R(s, a) ⊗V   T (s, a, s ) vt (s )
                    i       j             n
applying decision rule δ in state s ), for all x ∈ V , by:        s′∈S
                                                                                           ′
                         ⊞                                            j      π   j     i  π
 i         j i  j                 j  i j   i ⊠        Therefore, for all s ∈ S, vt+1(s ) = fj (vt ).
fj (x) = R(s , δ (s )) ⊗V i=1..n T (s , δ (s ), s ) xi .
                       i                        
Then, for any decision rule δP, we deﬁne the update function In order to establish the algebraic version of Bellman equa-
 i    n     n                                 n
f : V   → V   which associates to any vector x ∈ V the tions, we deﬁne the subset of maximal elements of a set with
       i         i
vector (f1(x), . . . , fn(x)).                        respect to an order relation  as:
  Dynamic consistency in AMDPs is guaranteed by speciﬁc  ∀Y ⊆ X, M(Y,  ) = {y ∈ Y : ∀z ∈ Y, not(z ≻ y)}
properties on functions f i. The fulﬁllment of these properties
strongly relies on the following conditions:          Furthermore, we denote P∗(X, ) the set {Y ⊆ X : Y =
      ⊠              ⊠         ⊠                      M(Y, )}. When there is no ambiguity, these sets will be
(C1) p  (x ⊕V y) = (p  x) ⊕V (p  y)                                                 ∗
      ⊞              ⊞         ⊞                      denoted respectively M(Y ) and P (X). Besides, for any
(C2) x  (y ⊕V z) = (x  y) ⊕V (x  z)                   function f : V n → V n, for all X ∈ P∗(V n), f(X) denote
      ⊠   ⊠              ⊠
(C3) p  (q  x) = (p ⊗P q)  x                          {f(x) : x ∈ X}.
      ⊞   ⊠                     ⊞   ⊠                   We deﬁne the semiring (P∗(V n), ⊕, ⊗, 0, 1) where 0 =
(C4)    pi   (x ⊗V yi) = x ⊗V (  pi   yi)
      i                         i                     {(0  , . . . , 0 )}, 1 = {(1 , . . . , 1 )}, and for all X, Y ∈
(C ) Pp ⊠ (x ⊞ y) = (p ⊠ x) ⊞ (p ⊠Py)                    V      V            V      V
  5                                                   P∗(V n):  X  ⊕ Y = M(X   ∪ Y )
for all p, q, pi ∈ P, x, y, z, yi ∈ V .
                                                                 X ⊗ Y = M({x  ⊗V  y : x ∈ X, y ∈ Y })
  Conditions (C ) and (C ) are two distributivity properties
              1       2                               with x ⊗ y = (x  ⊗  y , . . . , x ⊗ y ). Hence, the alge-
entailing a kind of additivity of  w.r.t. ⊠ and ⊞ (i.e.,     V      1  V  1      n  V  n
                              V                       braic generalization of Bellman equations (1) writes:
x    y ⇒  (z ∗ x   z ∗ y) for ∗ ∈ {⊠, ⊞}). Condition
   V              V                                                 ∗
                                                                   π
(C3) enables the reduction of lotteries. Condition (C4) en-      V   = 1
                                                                  0 ∗              ∗
ables to isolate a sure reward in a lottery and is similar to the  π      N   i   π
                                                                 Vt  =    i=1 fM (Vt−1) t = 1 . . . H
distributivity axiom used in [Luce, 2003]. Condition (C5) is
                                                             i     ∗  nL      ∗  n               i
a distributivity condition as in classic expectation. We now where fM : P (V ) → P (V ) is deﬁned by fM (X) =
establish a monotonocity result that will be used later (Prop. M(f i(X)) for all X ∈ P∗(V n).4  Generalized path algebra for AMDPs                 accumulated rewards at step t associated to optimal paths.
                                                      The above algorithm is not efﬁcient since lines 5 and 6
Following the construction proposed in Section 2, a decision                    n
rule graph can be associated to an AMDP. Clearly, solving al- require to consider N = m decision rules in the com-
gebraic Bellman equations amounts to searching for optimal putation of the Bellman update. Actually, the complex-
                                                      ity of the algorithm can be signiﬁcantly improved. In-
paths with respect to the canonical order associated to ⊕. We          N    i      m    k         k
                                                      deed, remark that ∪i=1{fj } = ∪k=1{gj }, where gj (x) =
now show how to solve this problem.                                 ⊞
                                   ∗   n     ∗  n        j  k                j  k  i ⊠                 n
  Consider the set F of functions from P (V ) to P (V ) R(s , a ) ⊗V i=1...n T (s , a , s ) xi, and that ∀v ∈ V ,
                          ∗   n        ∗  n             N    i       m    k           m    k
satisfying for all f ∈ F, X ∈ P (V ), Y ∈ P (V ):     ∪i=1{f (v)} =P∪k=1{g1 (v)} × .. × ∪k=1{gn(v)}. Since the
     f(X  ⊕ Y ) =   f(X) ⊕ f(Y )                      maxima over a Cartesian product equals the Cartesian prod-
        0           0                                                                      N   i
     f(  )      =                                     uct of maxima over components, we have i=1 fM (Vt−1) =
                  ∗   n                                           m  gk v   ..    m   gk v  . Hence, lines 5
The ⊕ operation on P (V ) induces a ⊕ operation on F de- v∈Vt−1   k=1 1 ( ) × ×   k=1  n(L)
ﬁned, for all h, g ∈ F and X ∈ P ∗(V n), by:          Land 6 can  beL replaced by: L       
      (h ⊕ g)(X) = h(X) ⊕ g(X)                          5.1.       Vt ← ∅
Let ◦ denote the usual composition operation between func- 5.2.    for v ∈ Vt−1 do
tions, id the identity function, and (for simplicity) 0 the con- 5.3. for j = 1 to n do
                                                                                         k     k
stant function everywhere 0. It has been shown by [Minoux, 5.4.         for k = 1 to m do qtj ← gj (v)
1977] that the algebraic structure (F, ⊕, ◦, 0, id) is a semi-                   m   k
                                                        6.1.            Vtj ←    k  qtj
ring. We now prove that update functions f i ’s belong to .                      =1
                                    M           F       6.2.         endfor   L
Proposition 3 (C ) and (C ) imply ∀δi ∈ ∆, f i ∈ F .    6.3.         Vt ←  Vt ⊕ (Vt1 × .. × Vtn)
               1       2                 M              6.4.       endfor
                                        ⊞      
              i         i  0   0            ⊠0
Proof. For any δ , we have fM ( ) = since i pi V =    This algebraic counterpart of backward induction runs in
  ⊞   ⊠  0      0     0         ⊞   ⊠ 0      0        polynomial time when the reward scale is completely ordered
  i pi  ( V ⊗V   V ) =  V ⊗V (  i pi  PV ) =  V by
                                      i               and algebraic operators can be computed in O(1).
P(C4) and absorption. Now, we showP that fM (X ⊕ Y ) =
 i        i                ∗   n
fM (X) ⊕ fM (Y ), ∀X, Y ∈ P (V  ). Consider X, Y in
P∗(V n). First, we have f i(M(X ∪ Y )) ⊆ f i(X ∪ Y ) (∗). 5 Examples
                        i             i
Second, we prove that M(f (X ∪ Y )) ⊆ f (M(X ∪ Y ))   To illustrate the potential of the algebraic approach for MDPs,
                 i
(∗∗). Let z ∈ M(f (X ∪ Y )). Then it exists w ∈ X ∪ Y we now consider decision models that have not yet been in-
           i
such that f (w) =   z.   If w  ∈   M(X   ∪ Y ) then   vestigated in a dynamic setting.
z ∈  f i(M(X ∪ Y )). If w 6∈ M(X  ∪ Y ) then it exists
 ∗                          ∗              i          • Qualitative MDPs. In decision under possibilistic uncer-
w   ∈ M(X   ∪ Y ) such that w ≻V  w. As  f  is non-
                   i  ∗        i                      tainty, [Giang and Shenoy, 2001] have recently studied a new
decreasing, we have f (w ) V f (w). By assumption,
f i(w) ∈ M(f i(X ∪Y )), therefore f i(w∗) = f i(w). Finally, qualitative decision model (binary possibilistic utility), allow-
z ∈ f i(M(X ∪ Y )) since z = f i(w∗). By (∗) and (∗∗), ing to handle weak information about uncertainty while im-
we have M(f i(X ∪ Y )) ⊆ f i(M(X ∪ Y )) ⊆ f i(X ∪ Y ). proving the discrimination power of qualitative utility mod-
                                                                     [                    ]
Therefore M(f i(X ∪ Y )) = M(f i(M(X  ∪ Y ))), which  els introduced by Dubois and Prade, 1995 . The latter have
                                                                                                    [
means, by deﬁnition, that f i (X ⊕Y ) = f i (X)⊕f i (Y ). been investigated in sequential decision problems by Sab-
                      M            M       M          badin, 1999]. We show here that the former can be exploited
  Propositions 2 and 3 prove that the algebraic generaliza- in sequential decision problems as well.
tion of Jacobi algorithm solves algebraic Bellman Equations Possibilistic uncertainty is measured on a ﬁnite qualita-
(3) and (4) [Minoux, 1977]. Thanks to the particular struc- tive totally ordered set P , endowed with two operators ∨
   3                                                  and ∧ (max  and min respectively). We denote 0P (resp.
ture of the decision rule graph, the Jacobi algorithm takes 1
the following simple form:                             P ) the least (resp. greatest) element in P . The structure
                                                      (P, ∨, ∧, 0P , 1P ) is a semiring.
ALGEBRAIC  JACOBI ALGORITHM
                                                        In Giang and Shenoy’s model, rewards are valued in an
  1.    V0 ←  1; t ← 0
          i  0                                        ordered scale (UP , ) where UP = {hλ, µi : λ ∈ P, µ ∈ P,
  2.    Qt =  , ∀t = 1 . . . H, i = 1 . . . N                                   ′ ′          ′         ′
                                                      λ ∨ µ = 1P } and hλ, µi  hλ , µ i ⇔ (λ ≥ λ and µ ≤ µ ).
  3.    repeat                                        The relevant semiring is here (V, ⊕ , ⊗ , 0 , 1 ) where:
  4.      t ← t + 1                                                                V   V   V  V
                            i    i                              V = {hλ, µi : λ ∈ P, µ ∈ P }
  5.      for i = 1 to N do Qt ← f (Vt−1)
                                M                               hα, βi ⊕V hλ, µi = hα ∨ λ, β ∧ µi
                  N    i
  6.      Vt ←    i=1 Qt                                        hα, βi ⊗V hλ, µi = hα ∧ λ, β ∨ µi
  7.    until t = LH                                            0V = h0P , 1P i, 1V = h1P , 0P i
We recognize a standard optimal path algorithm on the de- Note that ⊕V and ⊗V are operators max and min on UP .
cision rule graph valued with functions f i . When the it- The binary possibilistic utility model is a generalized
                                   M                  expectation with operators ⊞ and ⊠ taken as componentwise
eration has ﬁnished, Vt is the set of generalized expected
                                                      ∨ and ∧ respectively. Note that this criterion takes values in
  3
   The graph is layered; update functions labelling arcs issued UP . Thanks to distributivity of ∨ (resp. ∧) over ∧ (resp. ∨),
from a same node are identical and invariant from a layer to another. conditions (C1) to (C5) hold, which proves that algebraic