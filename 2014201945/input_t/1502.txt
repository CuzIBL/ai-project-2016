      Ranking Cases with Decision Trees: a Geometric Method that Preserves
                                            Intelligibility

                   Isabelle Alvarez(1,2)                        Stephan Bernard    (2)
               (1) LIP6, Paris VI University                     (2) Cemagref, LISC
           4 place Jussieu, F-75005 Paris, France          F-63172 Aubiere Cedex, France
                  isabelle.alvarez@lip6.fr                  stephan.bernard@cemagref.fr

                    Abstract                          conditional probability estimate at the leaf by some corrected
                                                      ratio that shifts the probability toward the prior probability
    This paper proposes a new method to rank the cases of the class. The raw conditional probability estimate at the
    classiﬁed by a decision tree. The method applies                   r        k
                                                      leaf is deﬁned by p (c x) = n , where k is the number of
    a posteriori without modiﬁcation of the tree and  training cases of the class| label classiﬁed by the leaf, and n
    doesn’t use additional training cases. It consists is the total number of training cases classiﬁed by the leaf.
    in computing the distance of the cases to the deci- It is the same for all the cases that are classiﬁed by a leaf.
    sion boundary induced by the decision tree, and to The most general type of correction generally used are the
    rank them according to this geometric score. When m-estimate pm (see equation (1)), which uses the prior prob-
    the data are numeric it is very easy to implement ability of the class and a parameter m, and the Laplace cor-
    and efﬁcient. The distance-based score is a global rection pL which is a particular case of m-correction when
    assess, contrary to other methods that evaluate the all the C classes have the same priors (see [Cestnik, 1990;
    score at the level of the leaf. The distance-based Zadrozny and Elkan, 2001]).
    score gives good results even with pruned tree, so
    if the tree is intelligible this property is preserved
    with an improved ranking ability. The main reason                k + p(c).m               k + 1
                                                            pm(c x) =               pL(c x) =         (1)
    for the efﬁcacity of the geometric method is that in        |       n + m           |     n + C
    most cases when the classiﬁer is sufﬁciently accu-
    rate, errors are located near the decision boundary. The main interest of smoothing methods is that they don’t
                                                      modify the structure of the tree. But in order to improve the
                                                      probability estimate, these methods are often applied to un-
1  Introduction                                       pruned trees (see [Provost and Domingos, 2003]), so the in-
Decision Trees (DT) are a very popular classiﬁcation tool be- telligibility of the model is very much reduced, although it is
cause they are easy to build and they provide an intelligible one of the main interest of decision trees compared to other
model of the data, contrary to other learning methods. The classiﬁers (like Naive Bayes, Neural Networks for instance).
need for intelligibility is very important in artiﬁcial intelli- Ensemble methods like bagging are also used successfully to
gence for applications that are not fully automatic, if there rank cases, although the margin is not a priori an estimate of
is an interaction with the end-user, expert or not. This is the class membership. Nevertheless, ensemble methods loose
the reason why DT algorithm are widely used for classiﬁca- also the intelligibility of the model.
tion purpose (see Murthy [1998] for examples of real world The method we propose here aims ﬁrstly at preserving the
applications). But for some applications knowing the class intelligibility of the model, so the objective is to improve the
of each case is not sufﬁcient to make a decision, one needs ranking without modifying the tree itself. This method is
to compare the cases to one another in order to select the based on the computation of the distance of the cases from
most promising examples. This is often the case in marketing the decision boundary (the boundary of the inverse image of
applications, allocation of resources or of grants, etc. (See the different classes in the input space), when it is possible
[Zadrozny and Elkan, 2001] for a description of the chari- to deﬁne a metric on the input space. The distance of a case
table donation problem). The traditional idea in this case is from the decision boundary deﬁnes a score that is speciﬁc to
to look for the probability of each case to belong to the pre- each case, unlike other methods for which the score is de-
dicted class, rather than just the class. The cases are then ﬁned at the level of the leaf and so it is shared by all cases
ranked according to the probability-based score. Unfortu- classiﬁed by the same leaf. In other geometric methods, like
nately, methods that are highly suitable for probability esti- Support Vector Machine (SVM) it has been proved that the
mate produces generally unintelligible models. This is the distance to the decision boundary can be used to estimate the
reason why some recent works aim at improving decision tree posterior probabilities (see Platt [2000] for the details in the
probability estimate. Smoothing methods are particularly in- two-class problem): an additional database is needed in order
teresting for that purpose. They consist in replacing the raw to calibrate the probabilities. But since in many applications                 Size of   ∆N           N             estimator at the leaf, without modiﬁcation of the tree struc-
     Database    dataset  (UT-PT)      (UT)           ture. This method improves signiﬁcantly the class probability
                                                      estimates. But the practical use of kernel density estimator is
     bupa          345  19.53±0.68  30.52±0.55
     glass         214   2.90±0.16   6.25±0.15        limited to very low dimension, and the setting of parameters
     ionosphere    351   5.22±0.25  11.00±0.2         is not easy. Kohavi [1996] builds Naive Bayes classiﬁers at
     iris          151   1.49±0.12   4.94±0.11        the level of the leaf, using its own induction algorithm. The
     letter      20000  31.71±0.76  61.10±0.58        objective of the tree partition is not to separate the classes but
     newThyroid    215   2.95±0.19   7.25±0.13        to segment the data so that the conditional independence as-
     optdigits    5620   9.14±0.35  19.35±0.3         sumption is better veriﬁed. The size of the tree is limited to
     pendigits   10992   8.91±0.35  23.33±0.3         cover each leaf with enough data. In our experiment the size
     pima          768   32.88±1.11  47.2±0.96        of the Naive Bayes Trees (NBT) is comparable to the size of
     sat          6435   13.67±0.40 24.73±0.31        the pruned trees (but the segmentation of the space is com-
                             ±          ±
     segmentati    210   1.48 0.13   4.66 0.09        pletely different). With different objectives and structures,
     sonar         208   6.30±0.24  11.36±0.15
     vehicle       846   8.04±0.29  18.10±0.25        the interpretation of DT and NBT cannot compare easily.
     vowel         990   2.50±0.19   8.56±0.14          Other methods try to correct the probability estimate at
     wdbc          569   5.10±0.23  10.03±0.18        each nodes by propagating a case through the different pos-
     wine          178   1.43±0.10   4.22±0.09        sible path from each node. These methods, like fuzzy trees
                                                      [Umano  et al., 1994], fuzzy split [Quinlan, 1993], or more
Table 1: Comparison of the size of pruned (PT) and uncollapsed recently [Ling and Yan, 2003] deal with a different issue:
unpruned (UT) trees: Mean and standard deviation of the difference Managing the uncertainty in the input case and in the train-
of the number of leaves N over 100 resamples.         ing database. Generally the computation of the probability
                                                      estimate is very complex and in some cases difﬁcult to under-
we don’t need the exact posterior probability, it is generally stand: a lot of nodes can be involved, although non-convex
possible to use directly the score induced by the distance to area of the input space corresponding to one class can be di-
rank and to select the most interesting cases.        vided arbitrarily into several leaves. So from the point of view
  The paper is organized as follow: Section 2 examines from of intelligibility these methods are not totally convincing.
the intelligibility viewpoint the methods applied to decision We propose here to keep the structure of the pruned tree
trees to rank cases or to estimate posterior probabilities. Sec- but to rank the cases accordingly to their distance from the
tion 3 presents our method for obtaining a distance-based decision boundary which is deﬁned by the tree.
score, and it explains why it is interesting from a theoreti-
cal point of view. Section 4 presents the experimental re- 3 Distance ranking methods for decision trees
sults which have been drawn from the numerical databases of We consider here axis-parallel DT (ADT) operating on nu-
the UCI repository, in comparison with the results obtained merical data: Each test of the tree involves a unique attribute.
from the smoothing methods applied on the same databases. We note Γ the decision boundary induced by the tree. Γ con-
We make further comments about geometric score and hybrid sists of several pieces of hyperplanes which are normal to
method in the concluding section.                     axes.
                                                        We consider a multi-class problem, with a class of interest
2  Decision Tree methods for ranking: the             c (the positive class). Let x be a case, c(x) the class label
   intelligibility viewpoint                          assigned to x by the tree, d = d(x, Γ) the distance of x from
                                                      the decision boundary Γ. We use the distance of an example
The success of Decision Trees as classiﬁcation method is for from the decision boundary to deﬁne its geometric score.
a good part due to the intelligibility of the model produced
by the algorithms. Pruning methods [Breiman et al., 1984; 3.1 Global and local geometric ranking
Bradley and Lovell, 1995; Esposito et al., 1997] produce Deﬁnition 1 Geometric score
shorter trees with at least the same performance than longer The geometric score g(x) of x is the distance of x from the
trees, since the generalization performance are enhanced. decision boundary if c(x) = c and its opposite otherwise.
They also produce shorter tree on purpose, seeking for a com-
promise between accuracy (or other performance criteria) and       d(x, Γ)   if c(x) is the positive class,
                                                           g(x) =                                     (2)
the size of the tree. Table 1 shows that unpruned trees can         d(x, Γ) otherwise.
be very large compared to pruned trees with similar accuracy       −
(the mean absolute difference over the databases is 0.38% and Theorem 1 Global geometric ranking
it is always less than 2.3%). Because of this size problem, it is The geometric score induce a quasi-order over the ex-
desirable to improve the probability estimate given by DT, in amples classiﬁed by the tree. 
order to allow a compromise between size and ranking ability.
                                                                      x   y   g(x)   g(y)             (3)
  With smoothing methods the probability estimate is the                   ⇔      ≥
same for all the examples classiﬁed by a leaf. In order to Cases are ranked in decreasing order relatively to the geomet-
produce more speciﬁc probability estimates, other methods ric score. The most promising cases have the highest geomet-
learn directly the probability class membership at the leaf. ric score, which means that their predicted class is the positive
For instance, [Smyth et al., 1995] use kernel-based density one and that they are far from the decision boundary.  With the geometric score, examples are ranked individu- positive examples is plotted against the ratio of all other (neg-
ally, not leaf by leaf.                               ative) examples as the score varies. With methods that give
  The geometric score is speciﬁc to each example, so it is constant probability estimates at the leaf, the points are plot-
also possible to ﬁrst rank the leaves with a smoothing method ted from one leaf to another. The afﬁne interpolation between
(or an equivalent method that ranks the leaves, not the cases) consecutive points assumes that examples are selected ran-
and then to rank the cases inside a leaf.             domly inside a leaf (or a set of leaves with the same score). If
                                                      we use a ROC curve to visualize the ranking, geometric rank-
Theorem 2 Local geometric ranking
                                                      ing will be very good at the beginning of the curve, as seen in
  The geometric score induce a quasi-order over the ex-
                                       L              Figure 1.
amples classiﬁed by a leaf.          

               p(c x) > p(c y)
  x  L y    or    |       |                     (4)
       ⇔     p(c x) = p(c y) and g(x) g(y).
                  |       |          ≥
  With the local geometric score, the leaves are ranked ac-

cording to the probability estimate, then inside each leaf (or                    ranking method
inside each group of leaves with the same output probability
estimate), examples are ranked according to their geometric                  m-estimate
score.                                                                       local geometric
                                                                             global geometric
  The distance of a case x to the decision boundary is com- positive  ratio
puted with the algorithm described in [Alvarez, 2004]. It con-
sists in projecting x onto all the leaves f which class label
differs from c(x). The nearest projection gives the distance.
Algorithm 1 distanceFrom(x,DT)
0. d =  ;
      ∞
1. Gather the set F of leaves f which class c(f) = c(x);   0.50.00 0.60.05 0.7 0.8 0.10 0.9 0.15 1.0 0.20 0.25
2. For each f F do:                     6                                     negative ratio
            ∈      {
3.   compute pf (x) = projectionOntoLeaf(x,f);
4.   compute d (x) = d(x, p (x));                     Figure 1: ROC curves of different ranking method (WD Breast-
              f          f                            Cancer sample). The local geometric ranking curve intersects the
5.   if (df (x) < d) then d = df (x)                  basic m-estimate curve at leaf points.
6. Return d = d(x, Γ)          }

Algorithm 2 projectionOntoLeaf(x,f = (Ti)i I )          Since DT algorithms are generally designed to maximize
1. y = x;                              ∈              accuracy, it is not unreasonable to hypothesize that errors lie
2. For i = 1 to size(I) do:                           near the decision boundary. In a very ideal case, it is even
                      {
3.   if y doesn’t verify the test Ti then yu = b      possible to demonstrate that this hypothesis is true.
                                       }
      where Ti involves attribute u with threshold value b A DT builds a partition in the input space. In a two class
4. Return y                                           problem, it is possible to associate to a DT a unique function
                                                      g : E    0, 1 such that g(x) is the predicted class of x. (But
The projection onto a leaf is straightforward in the case of several7→ decision { } trees are associated to the same function).
ADT since the area classiﬁed by a leaf f is a hyper-rectangle We consider an ideal case of statistical decision, where the
deﬁned by its tests. The complexity of the algorithm is in joint distribution P of observations would be uniform on the
O(Nn)  in the worst case where N is the number of tests of graph of the indicator function f of a set S in E = [0, 1]n. We
the tree and n the number of different attributes of the tree. also suppose that the size of all the maximal hypercubes in S
                                                      and E S  has a lower bound ν > 0 (to prevent pathological
3.2  Theoretical viewpoint                            situation\ for S and its boundary). In this case, decision trees
We expect geometric ranking to give interesting results when built on samples drawn from P can approach f as closely as
errors occur near the decision.                       wanted if the size of the sample can grow indeﬁnitely. For
  If this property is veriﬁed, positive cases (cases which class this particular case of function f, errors are located near the
is the class of interest) that are not recognized have negative decision boundary.
geometric score but with a small absolute value. False pos- Theorem 3 Proximity of errors. For a DT which associated
itive, that is negative cases classiﬁed as positive have also function g is close enough to f, errors are near the decision
small (but positive) geometric score. On the contrary, true boundary ∂g of g.
positive have higher geometric score and true negative have If we note A the area where f g = 0 (set of errors), A˙
negative geometric score with high absolute value. So inde- the interior of A, and if we consider− ǫ6 small enough so that
pendently from the score estimated at the leaf, the geometric ǫ < νn, we have:
score tends to bring side by side false negative and false pos-
itive, and to repel true positive and true negative. This can be
                                                                                                 n
seen on a Receiver Operating Characteristic (ROC) curve (in  f  g  < ǫ and x  A˙    d(x, ∂g) < αn √ǫ . (5)
the way described in [Adams and Hand, 1999]). The ratio of ZE | − |        ∈   ⇒                   ˙                 d
  Proof. Let x be in A, we consider B(x, 2 ) the maximal In this case the geometric score cannot be good. So we expect
hypercube centered at x in its connected component. The the geometric score to be better with more accurate trees.
                                               n
volume of B is included into E f g , so we have: d <
                            | −  |          n
ǫ < νn. So the size of B is smallerR than ν and d < √ǫ. The 4 Experimental Results
boundary of B, ∂B, encounters the decision boundary of f 4.1 Experimental Design
g on at least two meeting points of two different hyperplanes,−
                                                      We have studied the geometric ranking on the database of the
since B is maximal. If f is not constant on B, then both ∂f
                                                      UCI repository [Blake and Merz, 1998] that have numerical
and ∂g cross B, and so necessarily d(x, ∂g) < d √n. If f is
                                        2             attributes only and no missing values. We are not directly
constant on B, since the size of B is smaller than ν, at least
                                                      concerned in this study with the problem of the prevalence of
one of the meeting point lies on ∂g (otherwise the size of B
                                                      the positive class, since our method doesn’t build the decision
would be smaller than the lower bound of the maximal balls).
                                                      tree: it applies on the grown tree. So we didn’t pay any par-
So once again d(x, ∂g) d √n < √n √n ǫ.
                    ≤ 2       2     •                 ticular attention to the relative frequency of the classes in the
  Even if real conditions are very far from this ideal case datasets. We chose as positive class either the class with the
(in ﬁrst place, generally f doesn’t exist), we can test if the lowest frequency in the database, either a class which grouped
hypothesis of proximity of errors is generally veriﬁed. Table together several classes when it was more logical. When the
2 shows the mean of the difference of the mean distance of classes were equiprobable and with no particular meaning
correctly classiﬁed cases (hits) and errors from the decision we chose it randomly. Although there is a lot of work on
boundary. We also computed for each sample λ, the inverse the analysis of multi-class problem, for simplicity we have
of the coefﬁcient of variation for the difference of the means treated multi-class problem as a two class problem (class of
deﬁned by (6), where dh and σh are the mean and the standard the examples were modiﬁed before growing the trees).
deviation of the distance of correctly classiﬁed examples from For each database, we divided 100 bootstrap samples into
the decision boundary, and de and σe the same magnitude for separate training and test sets in the proportion 2/3 1/3, re-
error examples.                                       specting the prior of the classes (estimated by their frequency
                        dh  de                        in the total database). Even if it is not the best way to build
                  λ =     2−   2                (6)
                         σh + σe                      accurate trees for unbalanced dataset or different error costs,
                                                      here we are not interested in building the most accurate or
Table 2 shows the percentagep of the samples for which λ 2,
                                                      efﬁcient tree, we just want to study the effect of geometric
which is the 97.5% conﬁdence coefﬁcient under the normal≥
                                                      ranking on pruned trees. For the same reason we grow trees
assumption (the test is unilateral). We can see that errors are
                                                      with the default options of j48 (Weka’s [Witten and Frank,
closer from the decision boundary for a majority of databases.
                                                      2000] implementation of C4.5) although in many cases dif-
Datasets for which this property is not veriﬁed have generally
                                                      ferent options would build better trees. For unpruned trees
a low mean accuracy (62% for bupa and 69% for sonar). If
                                                      we disabled the collapsing function.
we consider only the samples for which the accuracy is better
                                                        We used Laplace correction and m-estimate smoothing
than 70%, the proportion shifts to 29% and 50% respectively.
                                                      methods to correct the raw probability estimate at the leaf
                                                      for reduced-error pruned tree and normal pruned tree. The
                ∆ of the              % of samples    value of m was chosen such that m p(c) = 10 where p(c)
                              λ            λ   2
  Database      means                  with  ≥        is the prior probability of the class of× interest (as suggested in
  bupa        0.009±0.002  0.86±0.12          17      [Zadrozny and Elkan, 2001]).
  glass       0.038±0.011  2.28±0.34          51        We used two different metrics in order to compute the dis-
  ionosphere  0.037±0.006  1.36±0.19          35      tance from the decision boundary, the Min-Max (MM) metric
  iris        0.092±0.003  5.34±0.28          95      and the standard (s) metric. Both metrics are deﬁned with the
  newThyroid  0.059±0.002  3.46±0.14          91      basic information available on the data: An estimate of the
  optdigits   0.542±0.008 24.87±1.45         100
                                                      range of each attribute i or an estimate of its mean Ei and
  pendigits   0.339±0.005  24.6±1.24         100      of its standard deviation s . The new coordinate system is
  pima        0.036±0.001  4.15±0.15          94                             i
  sat         0.135±0.005 12.59±0.56         100      deﬁned by (7).
  segment.    0.181±0.005  7.28±0.2           98           MM      xi   Mini          s   xi   Ei
  sonar       0.027±0.003  1.44±0.14          34          yi   =     −           or  yi =    −    .   (7)
                                                                 Maxi    Mini                si
  vehicle     0.026±0.004  4.75±0.49          55                       −
                                                      The parameters of the metric are estimated on each sample.
  vowel       0.215±0.003 16.88±0.74         100
  wdbc        0.113±0.002 10.24±0.29         100      The choice of the metric has a very limited effect on the geo-
  wine        0.152±0.004  6.73±0.39         97.7     metric score; If we measure the difference between the Area
                                                      Under the ROC curve (AUC) , for each database, it is always
                                                                   3       4
Table 2: Comparison of the mean distance of errors and hits to the less than 2 10− 9 10− , except for the thyroid and vehicle
                                                                    ±      3                           3
decision boundary, over the test bases of 100 samples per database. databases (less than 410− ) and the glass database (9.510− ).
The mean of the difference is estimated for each sample. (Bad re-
sults are bold)                                       4.2  Comparison between distance-based ranking
                                                           and smoothing methods
  A corollary of theorem 3 is that if a tree is not accurate, er- The geometric score is only used to rank the examples with-
rors may lie everywhere, not only near the decision boundary. out changing the tree structure. It is not used to estimate the          Red.-Error  Normal    No                                  Reduced-error  Normal
   Dataset  pruning  pruning   pruning  NBTree             Dataset     pruning    pruning   Unpruned
   bupa   0.46±0.48 -0.14±0.47 -0.82±0.50 0.08 ±0.79       bupa     1.75±0.23    0.88±0.09  0.69±0.09
   glass  1.78±0.72 -0.39±0.75 -1.87±0.73 -2.01±0.83       glass    4.21±0.44    3.39±0.34  2.61±0.31
   iono. -1.11±0.4  -2.30±0.4 -2.85±0.42 -5.14± 0.72       iono     0.04±0.29    0.51±0.17  0.49±0.18
   iris   4.69±0.40 3.85±0.35 3.67±0.37 1.56±0.43          iris     3.71±0.25    3.31±0.26  2.92±0.25
   letter 0.18±0.09 0.37±0.07 -0.26±0.05 0.40±0.12         letter   0.19±0.09    0.36±0.07  0.13±0.02
   thyroid 4.48±0.43 3.08±0.38 2.54±0.38 -2.13±0.62        thyroid  3.62±0.39    2.70±0.29  2.33±0.27
   optdig. 0.53±0.08 0.34±0.06 0.07±0.06 -0.12±0.06        optd.    0.67±0.06    0.43±0.04  0.27±0.03
   pendig. 0.46±0.04 0.40±0.03 0.28±0.03 0.58±0.05         pend.    0.34±0.03    0.28±0.02  0.20±0.02
   pima   1.34±0.43 -0.98±0.25 -1.07±0.30 2.25±0.55        pima     2.59±0.26    0.87±0.07  0.55±0.05
   sat    1.01±0.09 0.89±0.07 0.46±0.05 0.97±0.09          sat      1.09±0.08    0.89±0.06  0.45±0.04
   segment. 8.16±0.75 5.27±0.63 5.31±0.64 3.25±0.61        segment. 7.78±0.72    4.83±0.54  4.34±0.49
   sonar  2.55±0.47 1.99±0.51 1.80±0.49 -5.01±1.03         sonar    3.02±0.29    2.81±0.21  2.70±0.2
   vehicle 0.35±0.16 0.64±0.14 -0.12±0.16 0.78±0.30        vehicle  0.53±0.09    0.60±0.07  0.49±0.05
   vowel  4.18±0.34 3.09±0.29 2.83±0.26 0.78±0.44          vowel    3.83±0.3     2.85±0.26  2.56±0.23
   wdbc   3.75±0.24 2.24±0.18 2.15±0.18 2.09±0.22          wdbc     3.75±0.22    2.14±0.14  2.04±0.14
   wine   5.35±0.45 3.32±0.30 2.91±0.30 -0.06±0.25         wine     5.11±0.4     3.14±0.26  2.80±0.24

Table 3: Absolute difference of the AUC between global geometric Table 4: Absolute difference of the AUC between local geometric
ranking with standard metric and smoothing methods at the leaf. The ranking with standard metric and the best smoothing method. (All
last column shows the difference between global geometric ranking mean values and standard deviations are ×100. Insigniﬁcant values
on Red.-error pruning tree with NBTree. (All mean values and stan- are italic. There is no bad value.)
dard deviations are ×100. Insigniﬁcant values are italic. Bad results
are bold)
                                                      experiment partially conﬁrms the theoretical viewpoint con-
                                                      cerning the fact that geometric score gives interesting results
posterior probability of an example, so the appropriate mea- when misclassiﬁed examples are near the decision bound-
sure of performance in that case is the AUC. Table 3 shows ary. This is particularly true for the bupa (liver-disorder)
the difference between global geometric ranking and Laplace and ionosphere databases. Table 2 shows that these datasets
or m-estimate correction at leaf.                     doesn’t verify the hypothesis of proximity of errors on a ma-
  Apart from a few cases, global geometric ranking gives jority of samples, and actually the global geometric score give
better values than either Laplace or m-estimate correction bad results for these datasets.
(with a 95% conﬁdence coefﬁcent). The differences are rel- Concerning the improvement of the geometric ranking
atively small (from 0.004 to 0.08), but since they are ab- when the accuracy of the tree is better, the experiment is not
solute values the improvement can be important. We have conclusive. If we compute Table 3 and Table 4 for a subset
also shown the difference of the AUC between global geo- of the samples, the best quartile for tree accuracy, the global
metric ranking on reduced-error pruned tree and NBTree. geometric ranking is not improved (results are not signiﬁ-
  Table 4 shows the difference between local geometric rank- cant). But local geometric ranking gives always better results
ing and smoothing correction at leaf. Local geometric rank- than on the total sample, except on the glass and ionosphere
ing is always better (with a 95% conﬁdence coefﬁcent) than database (for which the hypothesis of proximity of errors is
smoothing method alone, except in one case which is not sig- not much improved on the subset of the samples).
niﬁcant. But like for global ranking, the improvement can
vary a lot (absolute value from 0.002 to 0.078).
  As we said in the theoretical viewpoint section, we expect 5 Conclusion
geometric ranking to outperform smoothing method at the be- We have presented in this article a geometric method to rank
ginning of the ROC curve. To measure the relative behavior cases that are classiﬁed by a decision tree. It applies to every
of ROC curves for increasing value of the negative ratio, we axis-parallel tree that classiﬁes examples with numerical at-
have computed AUC(x), 0    x    0.5, the integral func- tributes. We were not concerned here with the problem of
tion of the ROC curve, with a≤0.001≤step value, for the global growing the tree (problem with unbalanced datasets or dif-
geometric score (g) and the smoothing correction (s). Ta- ferent misclassiﬁcation costs which lead to pre-processing of
ble 5 shows for normal pruned trees theshows the maximum the data or new pruning methods). The geometric method
absisse value x such that AUCg(y) AUCs(y) with a con- doesn’t depend on the type of splitting or pruning criteria that
ﬁdence coefﬁcient of 0.95 (under the≥ normal assumption) for is used to build the tree. It only depends on the shape of de-
every y  x. For all smaller values of the negative ratio, the cision boundary induced by the tree. It consists in ranking
global geometric≤ ranking outperforms the other method (in the case according to their distance to the decision boundary,
term of AUC).                                         taking into account the class of interest and the class that is
  We can see in Table 5 that for most bases, the global predicted by the decision tree. Theoretical arguments suggest
geometric ranking methods is rather efﬁcient at the begin- that this method is interesting when the misclassiﬁed exam-
ning of the ROC curve, even when on the total range it per- ples lie near the decision boundary, and this was partially con-
forms badly (like for the Pima database, see Table 3). The ﬁrmed by the experimentation. The combination of geomet-