           Self-Adaptive Neural Networks Based on a Poisson Approach for              
                                   Knowledge Discovery 

                         Haiying Wang, Huiru Zheng, Francisco Azuaje 
              School of Computing and Mathematics, University of Ulster at Jordanstown 
                   Newtownabbey, Co. Antrim, Northern Ireland, UK, BT37 0QB 
                            {hy.wang, h.zheng, fj.azuaje}@ulster.ac.uk 

                   Abstract                      self-adaptive neural networks (SANNs)-based clustering 
                                                 models. This paper focuses on this clustering principle be-
    The ability to learn from data and to improve its cause SANNs have demonstrated several unique and inter-
    performance through incremental learning makes esting features in data mining and knowledge discovery. 
    self-adaptive neural networks (SANNs) a powerful 
    tool to support knowledge discovery. However, the 1.1  SANNs: Overview of Principles, Applications 
    development of SANNs has traditionally focused   and Limitations 
    on data domains that are assumed to be modeled by 
    a Gaussian distribution. The analysis of data gov- SANNs represent a family of unsupervised learning models, 
    erned by other statistical models, such as the Pois- which follow the basic principle of the self-organizing fea-
    son distribution, has received less attention from ture map (SOM) [Kohonen, 1995] with a focus on adaptive 
    the data mining community. Based on special con- architecture. A key advantage of these models is that they 
    siderations of the statistical nature of data follow- allow the shape, as well as the size, of the network to be 
    ing a Poisson distribution, this paper introduces a determined during the learning process rather than by a pre-
    SANN, Poisson-based Self-Organizing Tree Algo- determined grid of neurons. For example, the Growing Self-
    rithm (PSOTA), which implements novel similarity Organizing Map (GSOM) [Alahakoon et al., 2000] is ini-
    matching criteria and neuron weight adaptation tialized with a map of 2 x 2 neurons and new neurons are 
    schemes. It was tested on synthetic and real world incrementally grown from a boundary neuron where the 
    data (serial analysis of gene expression data). network exhibits a large cumulative representation error. 
    PSOTA-based data analysis supported the auto- After learning, GSOM can develop into different shapes 
    mated identification of more meaningful clusters. depending on the clusters present in the data. In the Grow-
    By visualizing the dendrograms generated by  ing Cell Structures (GCS) [Fritzke, 1994], the initial topol-
    PSOTA, complex inter- and intra-cluster relation- ogy consists of a two-dimensional output space where the 
    ships encoded in the data were also highlighted and neurons are arranged in triangles. A new neuron is inserted 
    readily understood. This study indicate that, in by the splitting of the longest edge emanating from the neu-
    comparison to the traditional Self-Organizing Tree ron with maximum accumulated error. GCS performs an 
    Algorithm (SOTA), PSOTA offers significant im- adaptation of the overall structure in those regions that rep-
    provements in pattern discovery and visualization resent large portions of the input data. Based on both the 
    in data modeled by the Poisson distribution, such SOM and the GCS principles, Dopazo and Carazo [1997] 
    as serial analysis of gene expression data.  proposed the Self-Organizing Tree Algorithm (SOTA). One 
                                                 of the main contributions of SOTA is that the output space 
 1 Introduction                                  is arranged following a binary tree topology, in which the 
                                                 number of output neurons is adapted to the intrinsic charac-
 Knowledge discovery has been defined as a nontrivial proc- teristics of the input data [Dopazo and Carazo, 1997; 
 ess of identifying valid, novel, potentially useful, and ulti- Herrero et al., 2001]. 
 mately understandable patterns in data [Fayyad et al., 1995]. Due to its dynamic, self-evolving nature, the resulting 
 Data mining is a particular step in this process, which in- maps of SANN can reveal relevant patterns from the under-
 volves the application of specific algorithms for extracting lying data in a more meaningful fashion. For example, due 
 patterns from data [Fayyad et al., 1996; Fayyad et al., to the ability to separate neurons into disconnected areas, 
1997]. There are a wide variety of techniques suitable for the GCS can produce explicit representations of cluster 
various data mining tasks. From the knowledge discovery boundaries. Thus, patterns hidden in the data become more 
perspective, unsupervised learning-based clustering analysis apparent [Fritzke, 1994]. The GSOM, on the other hand, can 
has become a fundamental approach, which has resulted in a indicate the patterns in the data by its shape and attract at-
large number of clustering techniques. Examples of power- tention to such areas by branching out. Such a flexible struc-
ful and meaningful techniques include the development of 


                                            IJCAI-07
                                             1101ture may provide a meaningful visualization of clusters in tion IV. This paper concludes with the discussion of results 
the data [Alahakoon et al., 2000].              and future research. 
 SANNs are well adapted to various application domains. 
For instance, they represent a promising way to improve 2  Algorithms and Implementation Protocols 
biomedical pattern discovery and visualization. The Grow-
ing cell structure visualization toolbox [Walker et al.,
1999], for example, is an implementation of GCS networks 2.1  Statistical Nature of a Poisson Distribution 
in the MatLab 5 computing environment. This tool has been The Poisson distribution describes a wide range of natural 
commonly used for the visualization of high-dimensional phenomena. This distribution may be used to model the 
biomedical data. SOTA has been shown to be capable of number of events occurring within a given time interval 
performing pattern discovery across various biomedical when such events are known to occur with an average rate. 
domains. Dopazo and Carazo [1997] used SOTA to cluster The formula for the Poisson probability mass function can 
aligned sequences. It has also been applied to the supervised be represented as: 
[Wang et al., 1998a] and unsupervised [Wang et al., 1998b] 
classification of protein sequences. More recently, Herrero p(m) exp( )   m m! (1) 
and colleagues [Herrero et al., 2001] extended its applica-
                                                     p m                         m
tion to the analysis of gene expression data derived from where ( ) is the probability of observing  occurrences, 
DNA array experiments.                          and  is the shape parameter that estimates the average 
 However, most of current SANNs are based on some heu- number of events in a given time interval. 
                                                  The Poisson distribution has several unique features. 
ristic criteria that take the accumulated quantization error 
                                                Most distinctively, the mean of any Poisson distribution is 
into account to guide the growth of neural networks.  For 
                                                equal to its variance. In other words, the larger the value of 
example, during the learning process GSOM [Alahakoon et
al., 2000] applies Euclidean distance to determine the win- the mean, the less significant the deviation between a count 
ning neuron for each input data and a cumulative error is value observed and its expected value. 
calculated for each winning neuron using the Euclidean dis- 2.2  Description of PSOTA 
tance-based metric. In the growing phase, the network keeps 
track of the highest error value and determines when and PSOTA is based on the same principle of the SOTA 
where to grow a new neuron. Such a criterion, however, is [Dopazo and Carazo, 1997]. Its structure is started by gener-
not suitable for problems in which the data are better ap- ating an initial network composed of two terminal neurons 
proximated by a Poisson distribution (i.e. a mixture of sepa- connected by an internal neuron, as shown in Figure 1(a).  
rate Poisson-distributed data sources), such as phenomena in The output topology is incrementally constructed by gener-
which events are observed a number of times over specific ating two new terminal neurons from the leaf neuron having 
intervals. Emerging problem domains in bioinformatics higher resources (measured as the mean distance between 
such as the study of Serial Analysis of Gene Expression the weight of each neuron and all the data samples assigned 
(SAGE) data [Velculescu et al., 1997] may also be ap- to this neuron) after each cycle (Figure 1(b) and (c)). For a 
proximated by a Poisson distribution. Euclidean distance- given training dataset, T, consisting of N samples, a learning 
based clustering analysis has demonstrated poor perform- cycle consists of a series of learning epochs, within which 
ance in these domains [Cai et al., 2004]. Without taking into the network is sequentially presented with each training 
account the statistical nature of the data during the learning sample. However, by taking into account the statistical na-
process, the full potential of SANNs may not be realized. ture of data closely following a Poisson distribution, 
                                                PSOTA adopts novel matching criteria (1) to determine the 
1.2  Objectives of This Study                   winning neuron for each input sample and (2) to update the 
                                                weight vectors of the winning neuron and its neighborhood. 
This paper aims to present a new SANN model, which takes 
into account the specific statistical nature of data approxi-
mated by a Poisson distribution, to improve data mining and 
knowledge discovery. The main objective of this study is, 
based on the incorporation of a Poisson statistics-based dis-
tance function, to develop a SANN model tailored to the 
data approximated by a Poisson distribution. This required 
the implementation of new strategies for weight adaptation 
and network growth. 
 The remainder of this paper is organized as follows. Sec-
tion II describes important statistical properties of the Pois- Figure 1: New neurons generation process for PSOTA. (a) The 
son distribution, followed by a detailed description of the PSOTA initial topology; (b) The accumulation of resources (the 
new SANN learning algorithm: Poisson-based Self- heterogeneity of each neuron) during learning process, the neuron 
Organizing Tree Algorithm (PSOTA). Two datasets, includ- marked with a filled circle (neuron B) has the highest cumulative 
ing synthetic and real world data, are described in Section resource after a learning cycle; (c) Neuron B gives rise to terminal 
III. Results and a comparative analysis are presented in Sec- neurons D and E (leaf neuron). Thus, D and E are sister neurons, 
                                                whose ancestor neuron is B. 


                                           IJCAI-07
                                            1102 Matching Criterion for Finding a Winning Neuron, wc, Weight Adaptation for a Winning Neuron and Its Top-
 for a Given Input Vector, xi                    logical Neighborhood 
 Traditional SANNs, e.g. SOTA, normally apply Euclidean Like other SANNs, once the winning neuron has been iden-
 or Pearson Correlation-based distance to determine the win- tified for each input sample, it is necessary to define a 
 ning neuron for each input data. These distance measures method to update the weight vectors of the winning neuron 
 have achieved a great success for data approximately fol- and its neighborhood in order to better match the input vec-
 lowing a normal distribution. For data associated with a tors and fulfill the overall clustering goals. In PSOTA, the 
 Poisson distribution, however, these measures have shown main goal is to assign an input data to a neuron with the 
 poor performance [Cai et al., 2004]. On the basis of the con- most similar relative vector. Thus, instead of performing 
 sideration of the statistical nature of a Poisson distribution, weight adaptation simply based on absolute values, like in 
 two new criteria based on Chi-square statistics and a joint other SANNs (e.g. traditional SOTA), we propose the fol-
 likelihood function are introduced here.        lowing weight adaptation strategy, which updates all rela-
                                    th
  Let xi be the input vector representing the i  input sam- tive weight values within the neighborhood,Nc (t ) , of a 
                                   th                                            th
ple, wj be the associated weight vector of the j  neuron, and winning neuron, c, according to the given i  input.   
 the index k indicate kth value of n-dimensional vector, the 
winning neuron represented by the subscript c can be deter-
                                                                             n
mined by the following minimum Chi-square statistics–                         w j, k (t)
                                                                             k 1
based distance matching criterion.                          w j,k (t) (t) (xi,k (t) n w j,k (t)),
                                                                              xi,k (t)
                 n  (x   xˆ )2                                               i 1
                      i,k i,k                     w j,k,i (t 1)                             (7)
         d (i, j)                          (2)                                  j  Nc (t)
                             xˆi,k
                 k 1

                                                            w j,k (t),          otherwise
          d (i,c) min d (i, j), j          (3)

                                                                                th
  Given that in the Poisson distribution, the probability of a where w j,k (t)  and w j,k,i (t 1)  are the k  weight values of 
number of events occurring within a given time interval is neuron j before and after the adaptation at iteration t.
considered to be independent of events that occurred in pre- N c (t)  and (t)  represent the neighborhood of the winning 
vious time intervals, the winning neuron can be also deter- neuron c and learning rate at iteration t respectively. The 
mined by using the maximum joint likelihood function- reader is referred to [Dopazo and Carazo, 1997; Herrero et
based matching criterion:                        al., 2001] for a more detailed description of the selection of 
              n                                  N c (t)  and (t) for SOTA-based algorithms. The learning 
                            xi,k                 algorithm of PSOTA is summarized in Table 1. 
        p(i)   (exp( xˆi,k ) xˆi,k xi,k !) (4)
             k 1
                                                  1: Initialization 
           p(i,c) max(p(i, j)), j (5) 2: Repeat cycle 
                                                  3:   Repeat epoch 
                                                  4:     For each input sample, 
 where xˆi,k  is the expected value of xi,k . After completing a 5:      Find the winning neuron for each input using (2) to (6)  
 learning process, each weight vector in the SOTA coincides 6:      Update the winner and its neighbors using  (7) 
 with the centroid of the respective cluster of the input data. 7:      Calculate the resource for each neuron. 
 Moreover, we are interested in grouping samples with simi-
                                                  8:   Until a cycle finishes: relative increase of the error between 
 lar relative values rather than the absolute values. Thus, the 
                                                    two consecutive epochs falls below a given threshold. 
 expected kth value of ith input given the weight vector of jth
 neuron, xˆ , is calculated as follows:           9:    Grow new neurons from the one having higher resource 
        i,k, j                                    10: Until The highest resource reaches a given threshold. 
                    n        n
        xˆi,k, j (w j,k (w j,k )) xi,k     (6)   Table 1: A summary of PSOTA learning algorithm 
                   k 1       k 1
  This equation is used, together with Equations (2) and (3) 2.3 Implementation Protocols 
or (4) and (5), to find a winning neuron. The matching crite- Both PSOTA and SOTA models were implemented within 
ria expressed in Equations (2) to (6) suggests that when the the software development framework provided by the open-
expected values are large, the deviation between actual and source platform, TIGR MeV [Saeed et al., 2003]. Unless 
expected count values become less significant. This is con- indicated otherwise, the learning parameters for PSOTA and 
sistent with an important property of the Poisson model, i.e. SOTA are: the maximum number of learning cycles = 5, the 
the variance of the dependent variable equals its mean, maximum number of learning epochs = 1000, and the learn-
which is totally ignored by using Euclidean (or other tradi- ing rates for the winning, ancestor and sister neurons are set 
tional) distance-based error calculation approaches. to 0.01, 0.005, and 0.001 respectively [Herrero et al., 2001].   


                                            IJCAI-07
                                             1103 3  The Datasets Under Study                     includes 21 tags that were found to be highly enriched in 
                                                 photoreceptor (PR)-enriched genes; (3) PerinatalCluster (11 
 Two datasets, including synthetic and real world data, were tags), whose expression peak appears around P0.5; (4) Cys-
 used to assess the PSOTA algorithm.             tallinCluster, which includes 12 cystallin proteins; (5) Em-
                                                 bryonicCluster (17 tags), which show strong expression 
 3.1 Synthetic Data                              levels during embryonic days, and (6) NeuroD4Cluster,
 The dataset was obtained from a study published by Cai et which includes 13 tags having similar expression patterns as 
 al. [2004]. It included 80 synthetic samples, each repre- gene NeuroD4. These “natural clusters” have been defined 
 sented by five simulated values at five time points: T1, T2, as key functional classes in previous studies [Blackshaw et
 T3, T4, and T5. All the simulated values are generated inde- al., 2004; Blackshaw et al., 2001] 
 pendently using Poisson distributions. Based on the models 
 they are generated from, the 80 samples are divided into 4 Results 
 four groups PA, PB, PC, and PD with 12, 16, 24 and 28 
 samples respectively. Samples within the same group have 
 similar profiles determined by the relative count numbers 4.1 Analysis of Synthetic Data 
 across different time points, as illustrated in Figure 2, which We first implemented a comparative analysis using the syn-
 shows the profiles of Groups PA and PB.         thetic data with SOTA (Figure 3). By incorporating Poisson 
                                                 statistics-based distance into the learning process, PSOTA 
                                                 correctly constructed a dendrogram that reflect significant 
                                                 inherent relationships between the data samples. For exam-
                                                 ple, PSOTA with joint likelihood function-based distance 
                                                 produced a hierarchical topology with 6 terminal neurons, 
                                                 each neuron uniquely representing one natural class (see the 
                                                 class distribution over terminal neurons given in the right 
                                                 panel in Figure 3(a)). Moreover, by visualizing the whole 
 Figure 2: An example of profiles for synthetic data. (a) Group A hierarchical clustering process, a more comprehensive pic-
 (12 samples). (b) Group B (16 samples). Five time points are ture that highlights the similarity between all the data sam-
 shown on the x-axis, while the y-axis represents the absolute simu- ples can be obtained. For instance, as can be seen from Fig-
 lated count numbers.  Different greys stand for different samples. ure 3(a), PSOTA first grouped 80 samples into 2 clusters 
                                                 (Branches A and B). All samples from Classes PA and PD 
 3.2 Mouse Retinal Gene Expression Data          are clustered together (Branch A), while all of samples from 
 To further evaluate the algorithm, a real world dataset gen- Classes PB and PC are grouped into Branch B. This is con-
 erated by SAGE in mature and developing mouse retina was sistent with the characteristics exhibited by this synthetic 
 analysed [Blackshaw et al., 2004]. SAGE is a global gene- data. Similar results were obtained when using Chi-square 
expression profiling technique designed to provide quantita- statistic-based distance as shown in Figure 3(b).  Clustering 
tive measures of gene expression in a particular cell or tis- analysis with traditional SOTA (based on Euclidean dis-
sue obtained from different developmental stages or patho- tance and Pearson correlation, Figure 3(c) and (d), however, 
logical processes [Velculescu et al., 1997]. The result of a fails to detect the underlying data structure. For example, 
SAGE experiment, known as a SAGE library, is a list of SOTA with Euclidean distance groups Classes PA and PD 
 tags and the number of times each tag is observed within a into the same cluster. 
 biological sample.  It has been suggested that the count val-
 ues of SAGE tags observed in a specific library can be ap-
 proximated by a Poisson distribution [Cai et  al., 2004]. 
  Such distributions tend to be independent across different 
 tags and libraries.  A detailed description of the SAGE tech-
 nique and relevant applications can be found in Velculescu 
 et al. [1997]. The dataset under study includes 10 murine 
 SAGE libraries from developing retina taken at 2-day inter-
 vals from embryonic day 12.5 to postnatal day 10.5 and 
 adult retina: E12.5, E14.5, E16.5, E18.5, P0.5, P2.5, P4.5, 
 P6.5, P10.5, and Adult. The reader is referred to Blackshaw 
 et al. [2004] for a full description of the generation and bio-
 logical meaning of these libraries. A subset of 92 tags with Figure 3: Data analysis for synthetic data by (a) PSOTA with joint 
 known biological functions and distinctive expression pat- likelihood function-based distance. (b) PSOTA with Chi-square 
 terns were analyzed. On the basis of their biological func- statistic-based distance; (c) Traditional SOTA with Euclidean dis-
 tions and temporal expression patterns during retinal devel- tance; (d) SOTA with Pearson correlation-based distance. The left 
 opment, these 92 tags may be divided into six distinctive panel on each figure shows the dendrogram obtained by each 
 clusters:  (1) P10Cluster (14 tags), which show high but method, while the right panel shows the class distribution over 
transient expression at P10.5; (2) PrenrichedCluster, which each neuron. 


                                            IJCAI-07
                                             11044.2 Analysis of Mouse Retinal SAGE Data         that both clusters have strong expression levels at the P10.5 
                                                time point. Significant relationships can also be obtained 
The outcomes of a comparative analysis of mouse retinal 
                                                when analyzing relationships between other clusters. 
gene expression data with PSOTA and SOTA are illustrated 
in Figure 4. Only the dendrograms generated by PSOTA, 
with either joint likelihood function or Chi-square statistics-
based distances, correctly depict significant relationships 
encoded in the SAGE data (Figure 4(a) and (b)). This can be 
further demonstrated by the analysis of class distributions 
over the terminal neurons shown in the right panel in Figure 
4(a) and (b).  For example, 14 P10Cluster tags and 17 Em-
bryonicCluster tags were grouped together and assigned to 
the neurons A and E respectively (Figure 4(a)). By contrast, 
the dendrograms produced by using SOTA with traditional 
distance measures are less meaningful, especially with 
Euclidean distance (see Figure 4(c)). This highlights the 
clear advantages of PSOTA when dealing with datasets that 
follow Poisson distribution. 


                                                Figure 5: Heat maps (generated by PSOTA) for SAGE tags that 
                                                fall into (a) Neuron A; (b) Neuron B; (c) Neuron C; (d) Neuron D; 
                                                (e) Neuron E; (f) Neuron F; and (g) Neuron G, as shown in Figure 
                                                4(a). Each row represents expression level of a SAGE tag across 
                                                SAGE libraries shown as columns in each image. The absolute 
                                                abundance of each SAGE tag correlates with color intensity, black 
                                                with the expression level equal to zero. The SAGE tags are dis-
                                                played on the right side. 

                                                5  Discussion and Conclusions 
                                                From the pattern discovery perspective, clustering-based 
Figure 4: Data analysis for Mouse SAGE data by (a) PSOTA with techniques have received great attention. However, cluster 
joint likelihood function-based distance. (b) PSOTA with Chi- analysis of data approximated by a Poisson distribution has 
square statistic-based distance; (c) SOTA with Euclidean distance; not been rigorously studied. By incorporating Poisson statis-
(d) SOTA with Pearson correlation-based distance. The left panel tics-based distance functions into the learning process, this 
on each figure shows the dendrogram obtained by each method, paper presented a new SANN model, PSOTA, specially 
while the right panel shows the class distribution over each neuron. designed to deal with problems modeled by Poisson statis-
                                                tics, such as SAGE data analysis. The results obtained indi-
 A closer examination of the dendrogram constructed by cate that PSOTA offers several advantages over traditional 
PSOTA (Figure 4 (a) and (b)) reveals that by monitoring the SANN techniques. Like SOTA [Dopazo and Carazo, 1997], 
learning process of PSOTA the potential relevance of inter- PSOTA not only incorporates some of the advantages dem-
and intra-cluster relationships hidden in the data can be onstrated by hierarchical clustering and SOM, but also it 
readily detected and understood. For example, as shown in implements unique features such as the generation of clus-
Figure 4(a), at the early learning stage, samples belonging to ters at different levels. Moreover, by using new matching 
P10Cluster and PRenrichedCluster were actually grouped criteria to determine the winning neurons and implement 
together, suggesting common patterns between these two weight adaptation, significant improvements in pattern dis-
classes. The heat maps shown in Figure 5(a) and (b) show covery and visualization are accomplished. By visualizing 


                                           IJCAI-07
                                            1105