                          A Decision-Theoretic Model of Assistance
            Alan Fern   and Sriraam Natarajan     and  Kshitij Judah   and  Prasad Tadepalli
                               School of EECS, Oregon State University
                                       Corvallis, OR 97331 USA
                         {afern,natarasr,judahk,tadepall}@eecs.oregonstate.edu

                    Abstract                          providing a formal basis for designing intelligent assistants.
                                                        The ﬁrst contribution of this work is to formulate the prob-
    There is a growing interest in intelligent assis- lem of selecting assistive actions as an assistant POMDP,
    tants for a variety of applications from organiz- which jointly models the application environment and the
    ing tasks for knowledge workers to helping peo-   agent’s hidden goals. A key feature of this approach is that
    ple with dementia. In this paper, we present and  it explicitly reasons about the environment and agent, which
    evaluate a decision-theoretic framework that cap- provides the potential ﬂexibility for assisting in ways unfore-
    tures the general notion of assistance. The objec- seen by the developer. However, solving for such policies is
    tive is to observe a goal-directed agent and to se- typically intractable and we must rely on approximations. A
    lect assistive actions in order to minimize the over- second contribution is to suggest an approximate solution ap-
    all cost. We model the problem as an assistant    proach that we argue is well suited to the assistant POMDP
    POMDP where the hidden state corresponds to the   in many application domains. The approach is based on ex-
    agent’s unobserved goals. This formulation allows plicit goal estimation and myopic heuristics for online action
    us to exploit domain models for both estimating   selection. For goal estimation, we propose a model-based
    the agent’s goals and selecting assistive action. In bootstrapping mechanism that is important for the usability
    addition, the formulation naturally handles uncer- of an assistant early in its lifetime. For action selection, we
    tainty, varying action costs, and customization to propose two myopic heuristics, one based on solving a set of
    speciﬁc agents via learning. We argue that in many derived assistant MDPs, and another based on the simulation
    domains myopic heuristics will be adequate for se- technique of policy rollout. A third contribution is to evalu-
    lecting actions in the assistant POMDP and present ate our framework in two novel game-like computer environ-
    two such heuristics. We evaluate our approach in  ments using 12 human subjects. The results indicate that the
    two domains where human subjects perform tasks    proposed assistant framework is able to signiﬁcantly decrease
    in game-like computer environments. The results   user effort and that myopic heuristics perform as well as the
    show that the assistant substantially reduces user more computationally intensive method of sparse sampling.
    effort with only a modest computational effort.     The remainder of this paper is organized as follows. In
                                                      the next section, we introduce our formal problem setup,
1  Introduction                                       followed by a deﬁnition of the assistant POMDP. Next, we
The development of intelligent computer assistants has present our approximate solution technique based on goal es-
tremendous impact potential in many domains. A variety of timation and online action selection. Finally we give an em-
AI techniques have been used for this purpose in domains pirical evaluation of the approach in two domains and con-
such as assistive technologies for the disabled [Boger et al., clude with a discussion of related and future work.
2005] and desktop work management [CALO, 2003].How-
ever, most of this work has been ﬁne-tuned to the particular 2 Problem Setup
application domains. In this paper, we describe and evaluate We refer to the entity that we are attempting to assist
a more comprehensive framework for intelligent assistants. as the agent. We model the agent’s environment as a
  We consider a model where the assistant observes a goal- Markov decision process (MDP) described by the tuple
oriented agent and must select assistive actions in order to W, A, A,T,C,I,whereW is a ﬁnite set of world states,
best help the agent achieve its goals. To perform well the as- A is a ﬁnite set of agent actions, A is a ﬁnite set of assistant
sistant must be able to accurately and quickly infer the goals actions, and T (w, a, w) is a transition distributions that rep-
of the agent and reason about the utility of various assis- resents the probability of transitioning to state w given that
tive actions toward achieving the goals. In real applications, action a ∈ A ∪ A is taken in state w. We will sometimes use
this requires that the assistant be able to handle uncertainty T (w, a) to denote a random variable distributed as T (w, a, ·).
about the environment and agent, to reason about varying ac- We assume that the assistant action set always contains the
tion costs, to handle unforeseen situations, and to adapt to action noop which leaves the state unchanged. The compo-
the agent over time. Here we consider a decision-theoretic nent C is an action-cost function that maps W × (A ∪ A) to
model, based on partially observable Markov decision pro- real-numbers, and I is an initial state distribution over W .
cesses (POMDPs), which naturally handles these features, We consider an episodic setting where at the beginning of

                                                 IJCAI07
                                                  1879each episode the agent begins in some state drawn from I lenges in selecting assistive actions. The ﬁrst challenge is to
and selects a goal from a ﬁnite set of possible goals G.The infer the agent’s goals, which is critical to provide good as-
goal set, for example, might contain all possible dishes that sistance. We will capture goal uncertainty by including the
the agent might prepare. If left unassisted the agent will exe- agent’s goal as a hidden component of the POMDP state. In
cute actions from A until it arrives at a goal state upon which effect, the belief state will correspond to a distribution over
the episode ends. When the assistant is present it is able possible agent goals. The second challenge is that even if we
to observe the changing state and the agent’s actions, but is know the agent’s goals, we must reason about the uncertain
unable to observe the agent’s goal. At any point along the environment, agent policy, and action costs in order to select
agent’s state trajectory the assistant is allowed to execute a the best course of action. Our POMDP will capture this infor-
sequence of one or more actions from A ending in noop,af- mation in the transition function and cost model, providing a
ter which the agent may again perform an action. The episode decision-theoretic basis for such reasoning.
ends when either an agent or assistant action leads to a goal Given an environment MDP W, A, A,T,C,I, a goal
state. The cost of an episode is equal to the sum of the costs distribution G0, and an agent policy π we now deﬁne the cor-
of the actions executed by the agent and assistant during the responding assistant POMDP.
episode. Note that the available actions for the agent and as- The state space is W × G so that each state is a pair (w, g)
sistant need not be the same and may have varying costs. Our of a world state and agent goal. The initial state distribu-
                                                           
objective is to minimize the expected total cost of an episode. tion I assigns the state (w, g) probability I(w)G0(g),which
  Formally, we will model the agent as an unknown stochas- models the process of selecting an initial state and goal for
tic policy π(a|w, g) that gives the probability of selecting ac- the agent at the beginning of each episode. The action set
tion a ∈ A given that the agent has goal g and is in state is equal to the assistant actions A, reﬂecting that assistant
w. The assistant is a history-dependent stochastic policy POMDP will be used to select actions.
 ( |  )                                      ∈                              
π a w, t that gives the probability of selecting action a A The transition function T assigns zero probability to any
given world state w and the state-action trajectory t observed transition that changes the goal component of the state, i.e.,
starting from the beginning of the trajectory. It is critical that the assistant cannot change the agent’s goal. Otherwise, for
the assistant policy depend on t, since the prior states and any action a except for noop, the state transitions from (w, g)
actions serve as a source of evidence about the agent’s goal, to (w,g) with probability T (w, a, w).Forthenoop action,
which is critical to selecting good assistive actions. Given an 
                                                     T  simulates the effect of executing an agent action selected
initial state w, an agent policy π, and assistant policy π we according to π.Thatis,T ((w, g), noop, (w,g)) is equal to
    (        )                                                                        
let C w, g, π, π denote the expected cost of episodes that the probability that T (w, π(w, g)) = w .
begin at state w with goal g and evolve according to the fol-           
                                                       The cost model C  reﬂects the costs of agent and assis-
lowing process: 1) execute assistant actions according to π tant actions in the MDP. For all actions a except for noop we
until noop is selected, 2) execute an agent action according have that C((w, g),a)=C(w, a). Otherwise we have that
to π,3)ifg is achieved then terminate, else go to step 1. C((w, g), noop)=E[C(w, a)],wherea is a random vari-
  In this work, we assume that we have at our disposal the able distributed according to π(·|w, g). That is, the cost of a
environment MDP and the set of possible goals G. Our ob-
                                                     noop action is the expected cost of the ensuing agent action.
jective is to select an assistant policy π that minimizes the                       
                                                       The observation distribution μ is deterministic and re-
expected cost given by E[C(I,G0,π,π )],whereG0 is an
                                                      ﬂects the fact that the assistant can only directly observe the
unknown distribution over agent goals and π is the unknown
                                                      world state and actions of the agent. For the noop action in
agent policy. For simplicity we have assumed a fully observ-                                        
                                                      state (w, g) leading to state (w ,g), the observation is (w ,a)
able environment and episodic setting, however, these choices
                                                      where a is the action executed by the agent immediately af-
are not fundamental to our framework.
                                                      ter the noop. For all other actions the observation is equal to
                                                                                               
3  The Assistant POMDP                                the W component of the state, i.e. μ (w ,g)=(w ,a).For
                                                      simplicity of notation, it is assumed that the W component of
POMDPs provide a decision-theoretic framework for deci- the state encodes the preceding agent or assistant action a and
sion making in partially observable stochastic environments. thus the observations reﬂect both states and actions.
A POMDP is deﬁned by a tuple S, A, T, C, I, O, μ,where
                                           (     )     We assume an episodic objective where episodes begin by
S is a ﬁnite set of states, A is a ﬁnite set of actions, T s, a, s drawing initial POMDP states and end when arriving in a
is a transition distribution, C is an action cost function, I is an                          
                                                      state (w, g) where w satisﬁes goal g. A policy π for the assis-
initial state distribution, O is a ﬁnite set of observations, and tant POMDP maps state-action sequences to assistant actions.
 ( | ) is a distribution over observations ∈ given the                                  
μ o s                               o   O             The expected cost of a trajectory under π is equal to our ob-
current state s. A POMDP policy assigns a distribution over                      
                                                      jective function E[C(I,G0,π,π )] from the previous section.
actions given the sequence of preceding observations. It is of- Thus, solving for the optimal assistant POMDP policy yields
tenusefultoviewaPOMDPasanMDPoveraninﬁnitesetof        an optimal assistant. However, in our problem setup the as-
belief states, where a belief state is simply a distribution over
                                                      sistant POMDP is not directly at our disposal as we are not
S. In this case, a POMDP policy can be viewed as a map-
                                                      given π or G0. Rather we are only given the environment
ping from belief states to actions. Actions can serve to both MDP and the set of possible goals. As described in the next
decrease the uncertainty about the state via the observations
                                                      section our approach will approximate the assistant POMDP
they produce and/or make direct progress toward goals.
                                                      by estimating π and G0 based on observations and select as-
  We will use a POMDP model to address the two main chal- sistive actions based on this model.

                                                 IJCAI07
                                                  18804  Selecting Assistive Actions                        POMDP may be solved quite effectively by just observing
                                                      how the agent’s actions relate to the various possible goals.
Approximating the Assistant POMDP. One approach to ap- This also suggests that in many domains there will be little
proximating the assistant POMDP is to observe the agent act- value in selecting assistive actions for the purpose of gath-
ing in the environment, possibly while being assisted, and to ering information about the agent’s goal. This suggests the
learn the goal distribution G0 and policy π. This can be done effectiveness of myopic action selection strategies that avoid
by storing the goal achieved at the end of each episode along explicit reasoning about information gathering, which is one
with the set of world state-action pairs observed for the agent of the key POMDP complexities compared to MDPs. We note
during the episode. The estimate of G0 can then be based on that in some cases, the assistant will have pure information-
observed frequency of each goal (perhaps with Laplace cor- gathering actions at its disposal, e.g. asking the agent a ques-
rection). Likewise, the estimate of π(a|w, g) is simply the tion. While we do not consider such actions in our experi-
frequency for which action a was taken by the agent when ments, as mentioned below, we believe that such actions can
in state w and having goal g. While in the limit these esti- be handled via shallow search in belief space in conjunction
mates will converge and yield the true assistant POMDP, in with myopic heuristics. With the above motivation, our action
practice convergence can be slow. This slow convergence can selection approach alternates between two operations.
lead to poor performance in the early stages of the assistant’s Goal Estimation. Given an assistant POMDP with agent
lifetime. To alleviate this problem we propose an approach to policy π and initial goal distribution GO, our objective is
bootstrap the learning of π.                          to maintain the posterior goal distribution P (g|Ot),which
  In particular, we assume that the agent is reasonably close gives the probability of the agent having goal g conditioned
to being optimal. This is not unrealistic in many applica- on observation sequence Ot. Note that since the assistant
tion domains that might beneﬁt from intelligent assistants. cannot affect the agent’s goal, only observations related to
There are many tasks, that are conceptually simple for hu- the agent’s actions are relevant to the posterior. Given the
mans, yet they require substantial effort to complete. Given agent policy π, it is straightforward to incrementally update
this assumption, we initialize the estimate of the agent’s pol- the posterior P (g|Ot) upon each of the agent’s actions. At
icy to a prior that is biased toward more optimal agent ac- the beginning of each episode we initialize the goal distribu-
tions. To do this we will consider the environment MDP with tion P (g|O0) to G0. On timestep t of the episode, if ot does
the assistant actions removed and solve for the Q-function not involve an agent action, then we leave the distribution un-
 (      )
Q a, w, g . The Q-function gives the expected cost of exe- changed. Otherwise, if ot indicates that the agent selected
cuting agent action a in world state w and then acting opti- action a in state w, then we update the distribution according
mally to achieve goal g using only agent actions. We then to P (g|Ot)=(1/Z) · P (g|Ot−1) · π(a|w, g),whereZ is a
deﬁne the prior over agent actions via the Boltzmann dis- normalizing constant. That is, the distribution is adjusted to
tribution. In our experiments, we found that this prior pro- place more weight on goals that are more likely to cause the
vides a good initial proxy for the actual agent policy, al- agent to execute action a in w.
lowing for the assistant to be immediately useful. We up- The accuracy of goal estimation relies on how well the pol-
date this prior based on observations to better reﬂect the pe- icy π learned by the assistant reﬂects the true agent. As de-
culiarities of a given agent. Computationally the main ob- scribed above, we use a model-based bootstrapping approach
stacle to this approach is computing the Q-function, which for estimating π and update this estimate at the end of each
need only be done once for a given application domain. episode. Provided that the agent is close to optimal, as in our
A number of algorithms exist to accomplish this includ- experimental domains, this approach can lead to rapid goal
ing the use of factored MDP algorithms [Boutilier et al., estimation, even early in the lifetime of the assistant.
1999], approximate solution methods [Boutilier et al., 1999; We have assumed for simplicity that the actions of the
Guestrin et al., 2003], or developing specialized solutions. agent are directly observable. In some domains, it is more
  Action Selection Overview. Let O = o1, ..., o be the ob- natural to assume that only the state of the world is observ-
                               t         t            able, rather than the actual action identities. In these cases,
servation sequence from the beginning of the current trajec-                                      
                                                      after observing the agent transitioning from w to w we can
tory until time t. Each observation provides the world state use the MDP transition function T to marginalize over possi-
and the previously selected action (by either the assistant or ble agent actions yielding the update,
agent). Given Ot and an assistant POMDP the goal is to se-                        
                                            (  )                                                   
lect the best assistive action according to the policy π Ot . P (g|Ot)=(1/Z) · P (g|Ot−1) · π(a|w, g)T (w,a,w ).
  Unfortunately, exactly solving the assistant POMDP will                         a∈A
be intractable for all but the simplest of domains. This has led Action Selection. Given the assistant POMDP M and a
us to take a heuristic action selection approach. To motivate distribution over goals P (g|Ot), we now address the problem
the approach, it is useful to consider some special characteris- of selecting an assistive action. For this we introduce the idea
tics of the assistant POMDP. Most importantly, the belief state of an assistant MDP relative to a goal g and M, which we will
corresponds to a distribution over the agent’s goal. Since the denote by M(g). Each episode in M(g) evolves by drawing
agent is assumed to be goal directed, the observed agent ac- an initial world state and then selecting assistant actions until
tions provide substantial evidence about what the goal might a noop, upon which the agent executes an action drawn from
and might not be. In fact, even if the assistant does nothing its policy for achieving goal g. An optimal policy for M(g)
the agent’s goals will often be rapidly revealed. This sug- gives the optimal assistive action assuming that the agent is
gests that the state/goal estimation problem for the assistant acting to achieve goal g. We will denote the Q-function of

                                                 IJCAI07
                                                  1881M(g) by Qg(w, a), which is the expected cost of executing 5.1 Doorman Domain
action a and then following the optimal policy.
  Our ﬁrst myopic heuristic is simply the expected Q-value In the doorman domain, there is an agent and a set of possible
of an action over assistant MDPs. The heuristic value for goals such as collect wood, food and gold. Some of the grid
assistant action a in state w given observations Ot is, cells are blocked. Each cell has four doors and the agent has
                                                     to open the door to move to the next cell (see Figure 1). The
           (       )=       (   ) · ( | )
          H w, a,Ot      Qg w, a  P g Ot              door closes after one time-step so that at any time only one
                       g                              door is open. The goal of the assistant is to help the user reach
  and we select actions greedily according to H. Intuitively his goal faster by opening the correct doors.
H(w, a, O ) measures the utility of taking an action under
         t                                              A state is a tuple s, d,wheres stands for the the agent’s
the assumption that all goal ambiguity is resolved in one step.
                                                      cell and d is the door that is open. The actions of the agent
Thus, this heuristic will not select actions for purposes of
                                                      are to open door and to move in each of the 4 directions or
information gathering. This heuristic will lead the assistant
                                                      to pickup whatever is in the cell, for a total of 9 actions. The
to select actions that make progress toward goals with high
                                                      assistant can open the doors or perform a noop (5 actions).
probability, while avoiding moving away from goals with
                                                      Since the assistant is not allowed to push the agent through
high probability. When the goal posterior is highly ambigu-
                                                      the door, the agent’s and the assistant’s actions strictly alter-
ous this will often lead the assistant to select noop.
                                                      nate in this domain. There is a cost of −1 if the user has to
  The primary computational complexity of computing H is
                                                      open the door and no cost to the assistant’s action. The trial
to solve the assistant MDPs for each goal. Technically, since
                                                      ends when the agent picks up the desired object.
the transition functions of the assistant MDPs depend on the
approximate agent policy π, we must re-solve each MDP af- In this experiment, we evaluated the heuristics Hd and Hr.
ter updating the π estimate at the end of each episode. How- In each trial, the system chooses a goal and one of the two
ever, using incremental dynamic programming methods such heuristics at random. The user is shown the goal and he tries
as prioritized sweeping [Moore and Atkeson, 1993] can al- to achieve it, always starting from the center square. After
leviate much of the computational cost. In particular, before every user’s action, the assistant opens a door or does nothing.
deploying the assistant we can solve each MDP ofﬂine based The user may pass through the door or open a different door.
on the default agent policy given by the Boltzmann bootstrap- After the user achieves the goal, the trial ends, and a new one
ping distribution described earlier. After deployment, prior- begins. The assistant then uses the user’s trajectory to update
itized sweeping can be used to incrementally update the Q- the agent’s policy.
values based on the learned reﬁnements we make to π.    The results of the user studies for the doorman domain are
  When it is not practical to solve the assistant MDPs, we presented in Figure 2. The ﬁrst two rows give cummulative
may resort to various approximations. We consider two ap- results for the user study when actions are selected according
proximations in our experiments. One is to replace the user’s to Hr and Hd respectively. The table presents the total op-
policy to be used in computing the assistant MDP with a ﬁxed timal costs (number of actions) for all trials across all users
default user’s policy, eliminating the need to compute the as- without the assistant N, and the costs with the assistant U,
                                                      and the average of percentage cost savings (1-(U/N)) over all
sistant MDP at every step. We denote this approximation by                   1
                                                      trials and over all the users . As can be seen, Hr appears to
Hd. Another approximation uses the simulation technique
of policy rollout [Bertsekas and Tsitsiklis, 1996] to approx- have a slight edge over Hd.
imate Qg(w, a) in the expression for H. This is done by
ﬁrst simulating the effect of taking action a in state w and
then using π to estimate the expected cost for the agent to
achieve g from the resulting state. That is, we approximate
Qg(w, a) by assuming that the assistant will only select a
single initial action followed by only agent actions. More
           ¯
formally, let Cn(π, w, g) be a function that simulates n tra-
jectories of π achieving the goal from state w andthenav-
eraging the trajectory costs. The heuristic Hr is identical to
  (      )                      (   )
H w, a, Ot except that we replace Qg w, a with the expec-
              (      ) · ¯(    )
tation w∈W  T w, a, w  C π, w ,g . We can also com-
bine both of these heuristics, which we denote by Hd,r.Fi-
nally, in cases where it is beneﬁcial to explicitly reason about    Figure 1: Doorman Domain.
information gathering actions, one can combine these myopic
heuristics with shallow search in belief space of the assistant To compare our results with a more sophisticated solution
MDP. One approach along these lines is to use sparse sam- technique, we selected actions using sparse sampling [Kearns
pling trees [Kearns et al., 1999] where myopic heuristics are et al., 1999] for depths d =2 and 3 while using b=1 or 2 sam-
used to evaluate the leaf nodes.                      ples at every step. The leaves of the sparse sampling tree are
5  Experimental Results
                                                         1This gives a pessimistic estimate of the usefulness of the assis-
We conducted user studies and simulations in two domains tant assuming an optimal user and is a measure of utility normalized
and present the results in this section.              by the optimal utility without the aid of the assistant.

                                                 IJCAI07
                                                  1882               Total   User     Average    Time       contents of the bowl, or replace an ingredient back to the
                               1 − (   )
   Heuristic  Actions Actions      U/N      per       shelf. The assistant can perform all user actions except for
                N       U                  action
                                  ±                   pouring the ingredients or replacing an ingredient back to the
      Hr       750     339    0.55  0.055  0.0562     shelf. The cost of all non-pour actions is -1. Experiments
      Hd       882     435     0.51 ± 0.05 0.0021
                                   ±                  were conducted on 12 human subjects. Unlike in the doorman
      Hr       1550    751    0.543  0.17  0.031      domain, here it is not necessary for the assistant to wait at ev-
  d=2,b=1      1337    570    0.588 ± 0.17 0.097
  d=2,b=2      1304    521    0.597 ± 0.17 0.35       ery alternative time step. The assistant continues to act until
  d=3,b=1      1167    467     0.6 ± 0.15  0.384      the noop becomes the best action according to the heuristic.
  d=3,b=2      1113    422    0.623 ± 0.15 2.61         Since this domain has much bigger state-space than the ﬁrst
                                                      domain, both heuristics use the default user’s policy. In other
Figure 2: Results of experiments in the Doorman Domain. words, we compare Hd and Hd,r. The results of the user stud-
The ﬁrst half of the table presents the results of the user stud- ies are shown in top half of the Figure 4. Hd,r performs better
ies while the lower half presents the results of the simulation. than Hd. It was observed from the experiments that the Hd,r
                                                      technique was more aggressive in choosing non-noop actions
evaluated using Hr. For these experiments, we did not con- than the Hd, which would wait until the goal distribution is
duct user studies, due to the high cost of such studies, but highly skewed toward a particular goal. We are currently try-
simulated the human users by choosing actions according to ing to understand the reason for this behavior.
policies learned from their observed actions. The results are
presented in the bottom half of Figure 2. We see that sparse         Total   User      Average    Time
sampling increased the average run time by an order of mag- Heuristic Actions Actions 1 − (U/N)    per
nitude, but is able to produce a reduction in average cost for        N       U                   action
                                                                                          ±
the user. This is not surprising, for in the simulated experi- Hd,r  3188    1175    0.6361 0.15  0.013
                                                                                          ±
ments, sparse sampling is able to sample from the exact user Hd      3175    1458    0.5371 0.10  0.013
policy (i.e. it is sampling from the learned policy, which is Hd,r   6498    2332   0.6379 ± 0.14 0.013
also being used for simulations).It remains to be seen whether d=2,b=1 6532  2427   0.6277 ± 0.14 0.054
these beneﬁts can be realized in real experiments with only d=2,b=2  6477    2293    0.646 ± 0.14 0.190
                                                                                          ±
approximate user policies.                               d=3,b=1     6536    2458    0.6263 0.15  0.170
                                                         d=3,b=2     6585    1408   0.6367 ± 0.14 0.995
5.2  Kitchen Domain
                                                      Figure 4: Results of experiments in the Kitchen Domain. The
In the kitchen domain, the goals of the agent are to cook vari- ﬁrst half of the table presents the results of the user studies
ous dishes. There are 2 shelves with 3 ingredients each. Each while the lower half presents the results of the simulation.
dish has a recipe, represented as a partially ordered plan. The
ingredients can be fetched in any order, but should be mixed We compared the use of sparse sampling and our heuristic
before they are heated. The shelves have doors that must be on simulated user trajectories for this domain as well(see bot-
opened before fetching ingredients and only one door can be tomhalfofFigure4). Thereisno signiﬁcant difference be-
open at a time.                                       tween the solution quality of rollouts and sparse sampling on
                                                      simulations, showing that our myopic heuristics are perform-
                                                      ing as well as sparse sampling with much less computation.
                                                      6   Related Work
                                                      Much of the prior work on intelligent assistants did not take
                                                      a sequential decision making or decision-theoretic approach.
                                                      For example, email ﬁltering is typically posed as a supervised
                                                      learning problem [Cohen et al., 2004], while travel planning
                                                      combines information gathering with search and constraint
                                                      propagation [Ambite et al., 2002].
                                                        There have been other personal assistant systems that are
                                                      explicitly based on decision-theoretic principles. Most of
                                                      these systems have been formulated as POMDPs that are ap-
Figure 3: The kitchen domain. The user is to prepare the
                                                      proximately solved ofﬂine. For instance, the COACH system
dishes described in the recipes on the right. The assistant’s
                                                      helped people suffering from Dementia by giving them ap-
actions are shown in the bottom frame.
                                                      propriate prompts as needed in their daily activities [Boger et
  There are 8 different recipes. The state consists of the lo- al., 2005]. They use a plan graph to keep track of the user’s
cation of each of the ingredient (bowl/shelf/table), the mixing progress and then estimate the user’s responsiveness to deter-
state and temperature state of the ingredient (if it is in the mine the best prompting strategy. A distinct difference from
bowl) and the door that is open. The state also includes the our approach is that there is only a single ﬁxed goal of wash-
action history to preserve the ordering of the plans for the ing hands, and the only hidden variable is the user respon-
recipes. The user’s actions are: open the doors, fetch the in- siveness. Rather, in our formulation there are many possible
gredients, pour them into the bowl, mix, heat and bake the goals and the current goal is hidden to the assistant. We note,

                                                 IJCAI07
                                                  1883