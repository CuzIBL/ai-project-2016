  Online Learning and Exploiting Relational Models in Reinforcement Learning

          Tom Croonenborghs, Jan Ramon, Hendrik Blockeel           and  Maurice Bruynooghe
                                K.U.Leuven, Dept. of Computer Science
                                 Celestijnenlaan 200A, B-3001 Leuven
     {Tom.Croonenborghs, Jan.Ramon, Hendrik.Blockeel, Maurice.Bruynooghe}@cs.kuleuven.be


                    Abstract                          by vectors. In contrast, in a relational setting concepts such
                                                      as “on” and “clear” are intuitive to work with.
    In recent years, there has been a growing inter-    Several studies have shown that learning a model of the
    est in using rich representations such as relational world (a transition function and the reward function) is of-
    languages for reinforcement learning. However,    ten beneﬁcial for the agent. Once a model has been learned,
    while expressive languages have many advantages   it can be used in several different ways. First, it can help
    in terms of generalization and reasoning, extending to speed up the learning process by generating more training
    existing approaches to such a relational setting is examples through simulation of actions in states, as happens
    a non-trivial problem. In this paper, we present a in the Dyna architecture [Sutton, 1991]. Second, it allows
    ﬁrst step towards the online learning and exploita- the agent to reason about actions in a way similar to planning
    tion of relational models. We propose a represen- [Tesauro, 1995], which may allow it to achieve better rewards
    tation for the transition and reward function that in exploitation mode and to make better estimates of Q-values
    can be learned online and present a method that ex- in exploration mode by using lookahead (the TD-leaf method
    ploits these models by augmenting Relational Rein- [Baxter et al., 1998] is an example of the latter). Note that
    forcement Learning algorithms with planning tech- both options are complementary and can be combined.
    niques. The beneﬁts and robustness of our ap-
                                                        Though in the relational setting most research so far has
    proach are evaluated experimentally.
                                                      focused on a model-free setting, recently there is growing in-
                                                      terest in extending methods for learning and exploiting mod-
                                                      els of the world to a relational setting (see [van Otterlo, 2005]
1  Introduction                                       for a recent overview). This is a non-trivial task as the use of
In reinforcement learning, an agent can observe its world and a more expressive relational language inevitably implies that
perform actions in it. Its goal is to maximize the obtained models of the world are more complex and harder to learn
reward. For small domains with a limited number of states, and apply. For instance, with the Dyna strategy, it is fairly
exact solution methods such as dynamic programming exist. easy to learn a model by keeping a probability distribution on
However, these methods are unable to handle real-world do- states. In the relational case however a probability distribu-
mains with large state spaces. For such problems, structur- tion on the large space of relational states is necessary, which
ing the world and generalization becomes essential. Recently, is a lot harder, as shown in the ﬁeld of statistical relational
there is a strong interest in Relational Reinforcement Learn- learning.
ing [Tadepalli et al., 2004], a framework not only providing Our work investigates the feasibility and beneﬁt of using
an expressive language for describing the world and for gen- relational learned models for lookahead. Moreover, we study
eralization, but also able to handle “relational” state spaces whether such a strategy is still beneﬁcial in complex worlds
which can not be easily represented using vector spaces. where it is not possible to learn a perfect model. We present
  For example, consider a blocks world where there is a set MARLIE (Model-Assisted Reinforcement Learning in Expres-
of blocks {b1,b2,...bn}. Every block bi stands either on the sive languages), the ﬁrst system to learn a relational transi-
ﬂoor (denoted on(bi,fl)) or on some other block bj (denoted tion and reward function on-line. Our contribution is three-
on(bi,bj)) and on every block bi there is either exactly one fold. (1) We propose a representation for the transition func-
other block or it is clear (denoted clear(bi)). Here, we de- tion that facilitates its efﬁcient and incremental learning. (2)
scribe a state by the set of facts that are true in that state, e.g. We propose a learning and exploitation method. In contrast
{clear(b1),on(b1,b2),on(b2,fl),clear(b3),on(b3,fl)}.  to earlier approaches learning relational models off-line (e.g.
The agent can take a clear block bi and put it on another clear [Zettlemoyer et al., 2005]), the partial model is exploited
block bj or on the ﬂoor (denoted move(bi,bj)). Move actions immediately (avoiding as much as possible an initial period
with other arguments (e.g., move(fl,b1)) are possible but where the agent gains poor rewards trying to learn a good
have no effect. As the number of blocks is not limited in this model) and in contrast to work such as [Kersting et al., 2004]
world, it is clear that such states can not be easily represented this model does not need to be complete. Also note that we

                                                IJCAI-07
                                                   726are considering the full RL problem in that our technique does learned in an on-line setting. This module learns these func-
not require resets or generative models as e.g. in [Fern et al., tions in the form of probability distributions T (s|s, a) and
                                                        
2006]. (3) We experimentally evaluate the efﬁciency and ben- R (r(s, a)|s, a),givenPS, PA and C of the RMDP.
eﬁts of our approach and examine the inﬂuence of the quality
of the learned model on these results.                3.1  Representation of the World Model
                                                      As said above, a state is a set of ground state atoms; hence,
Organization. Section 2 presents some background and  using a binary random variable for every possible ground
Section 3 shows how the transition function of a Relational state atom at each time point t,aDynamic Bayesian network
MDP can be represented and learned online. Section 4 de- (DBN) [Dean and Kanazawa, 1989] can represent the transi-
scribes how using the model to look some steps ahead im- tion function. The action taken at time point t is represented
proves over standard Q-learning. The experimental evalua- by a random variable at (one for every t) that ranges over all
tion is shown in Section 5. Section 6 discusses related work atoms from A that represent valid actions. The reward at time
and we conclude in Section 7.                         t is represented by a real-valued random variable rt.
                                                        The reward in the current state depends on the random vari-
2  Reinforcement Learning and MDPs                    ables representing the state and the action taken. The action
                                                      taken depends on the current knowledge of the agent and the
Reinforcement Learning (RL) [Sutton and Barto, 1998] is of- current state. Its conditional probability distribution (CPD) is
ten formulated in the formalism of Markov Decision Pro- not explicitly modeled as the chosen action is the result of the
cesses (MDPs). The need to model relational domains has agent’s reasoning process. The current state in turn depends
led to different formalizations of Relational MDPs (RMDPs), on the previous state and the action taken in that state. This
e.g. [Fern et al., 2006; Kersting and De Raedt, 2004; Kersting speciﬁes a layered network structure which is a partial order
et al., 2004]1. We use the following simple form:     over the random variables. There can still be dependencies
Deﬁnition 1 A Relational MDP (RMDP) is deﬁned as the  between variables of the same state. We assume an expert
                                                      provides an order on the random variables describing states
ﬁve-tuple M = PS,PA,C,T,R,wherePS   is a set of state
                                                      such that a random variable only depends on those preceding
predicates, PA is a set of action predicates and C is a set
of constants. A ground state (action) atom is of the form it in this order. Hence we avoid the problem of learning the
                                                      structure of the network, a problem that would be especially
p(c1,...,cn) with p/n ∈ PS (p/n ∈ PA) and ∀i : ci ∈ C}.
A state in the state space S is a set of ground state atoms; an hard in the case of online learning because a revision of the
action in the action state A is a ground action atom. structure would interfere with the learning of the conditional
  The transition function T : S × A × S →  [0, 1] de- probability tables in uninvestigated ways.
ﬁnes a probability distribution over the possible next states: The CPDs of state random variables can be compactly rep-
T (s, a, s) denotes the probability of landing in state s when resented by relational probability trees [Neville et al., 2003;
executing action a in state s. The reward function R : Fierens et al., 2005]. In our setting, the main idea is to have
S × A →  R deﬁnes the reward for executing a certain ac- a single relational probability tree for every predicate symbol
                                                      p ∈ PS. This tree is used to model the conditional proba-
tion in a certain state.                                              
                                                      bility distribution Tp(x|s, a) that gives for every ground atom
The task of reinforcement learning consists of ﬁnding an x with predicate symbol p the probability that it will be true
optimal policy for a certain MDP, which is (initially) un- in the next state given the current state s and action a.This
known to the RL-agent. As usual, we deﬁne it as a func- allows for maximal generalizations by using the same tree for
tion of the discounted, cumulative reward, i.e. ﬁnd a policy
                                            π         all atoms of the same predicate symbol, but avoids the prob-
π : S → A that maximizes the value function: V (s)=  lems arising when generalizing over predicates with a differ-
E    ∞  γiR  s ,π s  |s   s, s     π s           ≤
 π[  t=0    ( t  ( t)) 0 =   t+1 =  ( t)],where0      ent number of arguments with different types and different
γ<1  is the discount factor, which indicates the relative im- semantics. As an example, Figure 1 shows a probability tree
portance of future rewards with respect to immediate rewards. CPD for clear(X) in the blocks world. Another relational
  The RRL-system [Driessens, 2004] applies Q-Learning in probability tree is used to represent the CPD of the reward
relational domains, by using a relational regression algorithm random variable. Note that the learned relational probability
to approximate the Q-function deﬁned as               tree does not necessarily use all preceding random variables
                    
                                                   of the network as splits in internal nodes.
 Q(s, a) ≡ R(s, a)+γ    T (s, a, s )maxa Q(s ,a) (1)
                    s∈S                              3.2  Learning the Model
Knowing these Q-values, an optimal policy π∗ of the MDP All the uninstantiated parts of the model are CPD’s repre-
                   ∗
can be constructed as π (s) = argmaxa Q(s, a).        sented by relational probability trees. Therefore, learning the
                                                      model reduces to the online learning of a set of decision trees.
                                                                                          2
3  Online Learning of Relational Models               In our implementation, we use an improved version of the in-
                                                      cremental relational tree learning algorithm TG [Driessens et
In this section we propose a representation for the transition al., 2001].
function and reward function that can be easily and efﬁciently
                                                         2Which learns several trees in parallel and is much more space-
  1See [van Otterlo, 2005] for an overview.           efﬁcient while only slightly less time-efﬁcient.

                                                IJCAI-07
                                                   727                    State, move(X, Y ),clear(A)       tion only indirectly states the preconditions of an action, the
                                                      introduction and the learning of an extra binary random vari-
                       clear(State, A)?
                                                      able lt (intended to be true if the state of the world changes)
                    yes           no                  allows one to prune away actions predicted to be illegal.

              A == Y ?            on(State, X, A)?      Besides the sampling width, also other parameters can in-

            yes     no            yes     no          ﬂuence a lookahead tree. Currently, we are investigating the
                                                      use of beam-like and randomized searches.
       clear(State, X)? 1.0  clear(State, Y )? 0.0
      yes      no                    no               5   Empirical Evaluation
                       clear(State, X)?
     0.0        1.0                   0.0             In this section we will present an empirical evaluation of our
                       yes     no                     approach. First, we want to evaluate whether our incremental
                     1.0        0.0                   relational decision tree learner is able to build a model of the
                                                      world. Second, we want to investigate how much the perfor-
                                                      mance and speed of the agent improves by adding lookahead.
Figure 1: This probability tree shows the probability that Third, we want to evaluate the robustness of the approach,
block A will be clear when the action move(X, Y ) is exe-
           State                                      i.e., evaluate whether the lookahead is still beneﬁcial when
cuted in state  . The ﬁrst node in the tree checks if block the learner is not able to build a complete model.
A was already clear (essentially the frame assumption). If
this is the case, it can only become not clear when some other 5.1 Domains Experimental setup
block is moved on top of it. If the block was not clear in the
                                                                                              [
original state, it can only become clear in the afterstate if the In all the following experiments, the RRL-TG Driessens et
                                                              ]
block directly on top of it got moved to another block. al., 2001 system is used to estimate the Q-values. Since the
                                                      transition function for these domains are still learned rather
                                                      easily, a sampling width of two is used. The agent also learns
  Learning examples can easily be created from the agents the function modeling the preconditions to prune the looka-
experience. Note that the number of possible facts in the next head tree. The exploration policy consists of performing a
state may be very large, and therefore we do not generate all single step lookahead. To eliminate the inﬂuence of a speciﬁc
possible examples but apply a suitable sampling strategy. ordering on the random variables, independence is assumed
                                                      between random variables in the same state. The ﬁgures show
4  Online Exploitation of Relational Models           the average over a 5-fold run where each test run consists of
The (partially correct) transition function T (s|s, a) that is 100 episodes and the average reward over this 100 episodes
being learned enables the agent to predict future states. In this following a greedy policy, i.e., the percentage of episodes in
paper, we investigate the use of Q-learning with lookahead which a reward is received, is used as a convergence measure.
trees to give the agent more informed Q-values by looking
some steps into the future when selecting an action. These Blocks World In the following experiments, we use the
lookahead trees are similar to the sparse lookahead trees used blocks world as described in the introduction with the stack-
in [Kearns et al., 2002] to obtain near-optimal policies for goal and the on(A, B) goal (deﬁned in the same way as in
large MDPs.                                           [Driessens, 2004]3). In the on(A, B)-goal the agent only re-
  Since the transition function can be stochastic or there may ceives a reward iff block A is directly on top of block B.The
be uncertainty on the effect of a particular action (due to in- objective of the stack-goal is to put all blocks in one and the
complete learning), an action needs to be sampled several same stack, i.e., if there is only one block on the ﬂoor. The
times to obtain an accurate value. This sampling width SW is results for the unstack-goal where the agent is rewarded iff
a parameter of the algorithm. Starting from the current state all blocks are on the ﬂoor are not included as the behavior
in the root node, we generate for every possible action, SW was comparable to the stack-goal.
(directed) edges using the action as a label for that edge. The During exploration, episodes have a maximum length of
node at the tail of this edge represents the state obtained from 25 steps above the ones needed by the optimal policy, dur-
executing that action in the head node. This can be contin- ing testing only optimal episodes are allowed. In the stack-
ued until the tree reaches a certain depth. The Q-values of problem the blocks world has seven blocks, for the on(A, B)-
the deepest level can be estimated by the learned Q-function. goal the number of blocks was varied for each episode be-
By back-propagating these values to the top level, a policy tween four and seven. The same language bias is used as
can be constructed using these top level Q-values as in regu- in previous experiments with the RRL-TG algorithm in the
lar Q-learning. When back-propagating, the Q-values for the Blocks World [Driessens, 2004].
different samples of a certain action in a certain state are av-
             Q
eraged and the -value of a higher level is determined using Logistics The second domain is a logistics domain contain-
the Bellman equation (Eq. 1).                         ing boxes, trucks and cities. The goal is to transport cer-
  Several optimizations to this scheme are possible. Our im-
plementation uses preconditions. This is especially useful in 3The main difference is that here the agent can execute every
planning domains as, typically, the world remains unchanged possible action in a certain state instead of only the legal ones. This
if the agent tries an illegal action. While the transition func- makes the problem more difﬁcult.

                                                IJCAI-07
                                                   728tain boxes to certain cities4. The possible actions in this respectively and for the logistics domain the results are plot-
domain are load box on truck/2, which loads the speciﬁed ted in Figure 5.
box on the speciﬁed truck if they are both in the same city,
the unload box on truck/2 which takes the box of the truck
and moves it in the depot of the city where the truck is located.
The third possible action is the move/2-action, which moves
the truck to the speciﬁed city. The state space PS consists
of the following predicates: box on truck/2, box in city/2
and truck in city/2. These predicates also make up the lan-
guage bias used in these experiments, i.e., the tree learning
algorithm can test if a certain box is on a certain truck etc.
  In the ﬁrst setting there are two boxes, two cities and three
trucks and the goal is to bring the two boxes to two speciﬁc
cities. During exploration, 150 steps are allowed, but during
testing the maximum length of an episode is 20 steps. For
the second setting the domain is extended to four boxes, two
cities and two trucks and the goal is to bring three speciﬁc
boxes to certain locations within 50 time steps.

5.2  Experiments                                             Figure 3: Blocks world with on(A, B) goal


  Figure 2: Errors made by the learned transition function
                                                               Figure 4: Blocks world with stack goal
  First, we test the quality of the transition function. To
achieve this, we apply the learned model as a classiﬁer. We These experiments show that the number of episodes
randomly generate facts and predict if they will be true in the needed to obtain a good policy is much smaller when look-
resulting next state, given some state and action. Figure 2 ing ahead. However, learning and lookahead takes additional
shows the percentage of errors made per step, False Positives computation time. Therefore, for worlds where performing
(FP) and False Negatives (FN) indicate the number of atoms actions is not expensive, it is also useful to evaluate how the
that were incorrectly predicted as true and false respectively. total time needed to obtain a good policy compares with and
For the blocks world with seven blocks the transition func- without learning. Figure 6 shows the average reward in func-
tion is learned rather rapidly and is optimal after about 200 tion of the time needed to learn the policy for the on(a,b)
episodes. For the logistics domain with ﬁve boxes, trucks goal (the other settings produce similar results, omitted due
and cities, it appears that a reasonable transition function is to lack of space). Due to the computational costs of learn-
learned rapidly, but it never reaches optimal performance. ing the model, using no lookahead performs better than sin-
  Next, we evaluate the improvement obtained by lookahead gle step lookahead. Indeed, in our current implementation,
planning. Therefore, we ran both experiments with standard learning the model of the world is the computational bottle-
RRL-TG and experiments with different amounts of looka- neck for small levels of lookahead. This is mainly due to the
head using the learned model. Figure 3 and Figure 4 show the sampling techniques to create the learning examples, some-
results for the on(a, b) and stack goals of the blocks world thing we will improve in future work. However, performing
                                                      two steps of lookahead still outperforms the standard version
  4Speciﬁc instantiations vary from episode to episode. without lookahead.

                                                IJCAI-07
                                                   729                 Figure 5: Logistics                       Figure 7: Reward versus the quality of the model

                                                      and uses these approximations of the transition and reward
                                                      function to perform hypothetical actions to generate extra up-
                                                      dates for the Q- or value function. Algorithms such as prior-
                                                      itized sweeping [Moore and Atkeson, 1993] (and extensions
                                                      of these) focus the hypothetical actions on interesting parts of
                                                      the state space. As mentioned earlier, these approaches are
                                                      orthogonal to our approach and will be explored in our future
                                                      work.
                                                        Learning a model of the environment becomes a non-trivial
                                                      task in the relational setting. One method that focuses speciﬁ-
                                                      cally on learning transition functions with relational structure
                                                      is [Zettlemoyer et al., 2005]. They present a method for learn-
                                                      ing probabilistic relational rules when given a dataset about
                                                      state transitions that are applicable in large noisy stochastic
                                                      worlds. The main difference with (the learning part of) our
                                                      approach is that this method is not directly applicable to Re-
                                                      inforcement Learning, since it does not work incrementally
     Figure 6: Blocks world with the on(A, B) goal    and it is limited to domains where actions only have a small
                                                      number of effects.
  Finally, we evaluate the robustness of our approach. We set The emerging ﬁeld of Statistical Relational Learning has
up this experiment by generating less training examples for seen a lot of work on relational upgrades of Bayesian net-
the transition function per step, extending the time to learn works. More speciﬁcally, [Sanghai et al., 2005] deﬁnes Re-
a good transition function. Every 30 episodes, we test both lational Dynamic Bayesian Networks (RDBNs) as a way to
the quality of the transition function (the number of classiﬁ- model relational stochastic processes that vary over time.
cation errors) and the average reward obtained by the learned Combining search and RL has shown to be successful in
policy using single step lookahead. In Figure 7 we plotted the past, for instance in the context of game playing [Baxter et
these points for two different learning rates, showing the ob- al., 1998].In[Davies et al., 1998], online search is used with
tained reward in function of the quality of the model. The a (very) approximate value function to improve performance
horizontal line shows the performance of a learning agent af- in continuous-state domains. Our approach can be seen as an
ter 2000 episodes using no lookahead. The ﬁgure shows that instance of the Learning Local Search (LLS) algorithm de-
even when the learned model is less accurate, it is still bene- scribed there.
ﬁcial to use lookahead. Only when the model performs very Many new RRL algorithms have been proposed lately, but
inaccurately, performance can drop.                   to our knowledge this is the ﬁrst indirect RRL approach.
                                                      The most related is [Sanner, 2005], where a ground relational
                                                      naive Bayes Net is learned as an estimation of the Value func-
6  Related Work                                       tion. The major difference however is that this work does not
An important part of related work are the indirect or model- consider the aspects of time since they consider game playing
based approaches in the propositional setting. Most of these and hence restrict themselves to undiscounted, ﬁnite-horizon
ﬁt the Dyna architecture [Sutton, 1991] as mentioned in the domains that only have a single terminal reward for failure or
introduction. There the agent learns a model of the world success.

                                                IJCAI-07
                                                   730