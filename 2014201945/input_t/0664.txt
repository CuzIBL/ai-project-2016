     View   Learning     for Statistical   Relational   Learning:     With   an  Application     to
                                          Mammography
                       Jesse Davis, Elizabeth Burnside,  Inesˆ Dutra, David  Page,
                      Raghu   Ramakrishnan,    V´ıtor Santos Costa, Jude  Shavlik
                                   University of Wisconsin  - Madison
                                           1210  West Dayton
                                       Madison,  WI  53706, USA
                                       email: jdavis@cs.wisc.edu 

                                                                              ++ 
                    Abstract                                                Ca  Lucent Milk of  
                                                                 Mass Stability 
                                                                              Centered Calcium 
                                                                                          Ca ++ Dermal 
    Statistical relational learning (SRL) constructs       Mass Margins 
                                                                                             ++ 
    probabilistic models from relational databases. A     Mass Density                      Ca Round 
    key capability of SRL is the learning of arcs (in    Mass Shape                         Ca ++ Dystrophic 

    the Bayes net sense) connecting entries in differ-                                         ++
                                                         Mass Size                           Ca  Popcorn 
    ent rows of a relational table, or in different tables.                                   Ca ++ Fine/ 
                                                           Breast                 Disease 
    Nevertheless, SRL approaches currently are con-        Density  Mass P/A/O                 Linear 
                                                                                               ++ 
    strained to use the existing database schema. For    Skin Lesion                         Ca  Eggshell 
    many database applications, users ﬁnd it proﬁtable
                                                          Tubular                           Ca ++ Pleomorphic 
                                                                    LN           Age 
    to deﬁne alternative “views” of the database, in ef-  Density 
                                                                                              Ca ++ Punctate 
    fect deﬁning new ﬁelds or tables. Such new ﬁelds                      HRT 
                                                          Architectural 
                                                                                             ++ 
    or tables can also be highly useful in learning. We    Distortion                       Ca Amorphous 
                                                                 Asymmetric         
                                                                           Family 
    provide SRL with the capability of learning new                                      . Ca ++ Rod-like
                                                                   Density                         
    views.                                                                   hx 

                                                                    Figure 1: Expert Bayes Net             
1  Introduction
Statistical Relational Learning (SRL) focuses on algorithms to an expert developed system. This provides a base refer-
for learning statistical models from relational databases. SRL ence against which we can evaluate our work. Third, a large
advances beyond Bayesian network learning and related tech- proportion of examples are negative. This distribution skew
niques by handling domains with multiple tables, represent- is often found in multi-relational applications. Last, our data
ing relationships between different rows of the same table, consists of a single table. This allows us to compare our tech-
and integrating data from several distinct databases. SRL niques against standard propositional learning. In this case, it
techniques currently can learn joint probability distributions is sufﬁcient for view learning to extend an existing table with
over the ﬁelds of a relational database with multiple tables. new ﬁelds. It should be clear that for other applications the
Nevertheless, they are constrained to use only the tables and approach can yield additional tables.
ﬁelds already in the database, without modiﬁcation. Many
human users of relational databases ﬁnd it beneﬁcial to de-
ﬁne alternative views of a database—further ﬁelds or tables 2 View Learning for Mammography
that can be computed from existing ones. This paper shows Offering breast cancer screening to the ever-increasing num-
that SRL algorithms also can beneﬁt from the ability to deﬁne ber of women over age 40 represents a great challenge. Cost-
new views, which can be used for more accurate prediction of effective delivery of mammography screening depends on a
important ﬁelds in the original database.             consistent balance of high sensitivity and high speciﬁcity. Re-
  We will augment SRL algorithms by adding the ability to cent articles demonstrate that subspecialist, expert mammog-
learn new ﬁelds, intensionally deﬁned in terms of existing raphers achieve this balance and perform signiﬁcantly better
ﬁelds and intensional background knowledge. In database than general radiologists [24; 1]. General radiologists have
terminology, these new ﬁelds constitute a learned view of higher false positive rates and hence biopsy rates, dimin-
the database. We use Inductive Logic Programming (ILP) ishing the positive predictive value for mammography [24;
to learn rules which intensionally deﬁne the new ﬁelds. 1]. Despite the fact that specially trained mammographers
  We test the approach in the speciﬁc application of creating detect breast cancer more accurately, there is a longstanding
an expert system in mammography. We chose this applica- shortage of these individuals [6].
tion for a number of reasons. First, it is an important prac- An expert system in mammography has the potential to
tical application with sizable data. Second, we have access help the general radiologist approach the effectiveness of a


                                                      subspecialty expert, thereby minimizing both false negative that capture “hidden” concepts central to accurately predict-
and false positive results.                           ing malignancy, but that are not explicit in the given database
  Bayesian networks are probabilistic graphical models that tables. One learned rule states that a change in the shape of
have been applied to the task of breast cancer diagnosis from the abnormality at a location since an earlier mammogram
mammography  data [12; 2; 3]. Bayesian networks produce may be indicative of a malignancy. The other says that an in-
diagnoses with probabilities attached. Because of their graph- crease in the average of the sizes of the abnormalities may be
ical nature, they are comprehensible to humans and useful for indicative of malignancy. Note that both rules require refer-
training. As an example, Figure 1 shows the structure of a ence to other rows in the table for the given patient, as well as
Bayesian network developed by a subspecialist, expert mam- intensional background knowledge to deﬁne concepts such as
mographer. For each variable (node) in the graph, the Bayes “increases over time.” Neither rule can be captured by stan-
net has a conditional probability table giving the probability dard aggregation of existing ﬁelds.
distribution over the values that variable can take for each
possible setting of its parents. The Bayesian network in Fig- 3 View Learning Framework
ure 1 achieves accuracies higher than that of other systems One can imagine a variety of approaches to perform view
and of general radiologists who perform mammograms, and learning. Our closing section discusses a number of alterna-
commensurate with the performance of radiologists who spe- tives, including performing view learning and structure learn-
cialize in mammography [2].                           ing at the same time, in the same search. For the present work,
  Figure 2 shows the main table (with some ﬁelds omitted for we apply existing technology in a new fashion to obtain a
brevity) in a large relational database of mammography ab- view learning capability.
normalities. Data was collected using the National Mammog- Any relational database can be naturally and simply rep-
raphy Database (NMD) standard established by the American resented in a subset of ﬁrst-order logic [21]. Inductive logic
College of Radiology. The NMD was designed to standard- programming (ILP) provides algorithms to learn rules, also
ize data collection for mammography practices in the United expressed in logic, from such relational data [15], possibly to-
States and is widely used for quality assurance. Figure 2 also gether with background knowledge expressed as a logic pro-
presents a hierarchy of the types of learning that might be gram. ILP systems operate by searching a space of possible
used for this task. Level 1 and Level 2 are standard types of logical rules, looking for rules that score well according to
Bayesian network learning. Level 1 is simply learning the some measure of ﬁt to the data. We use ILP to learn rules to
parameters for an expert-supplied network structure. Level 2 predict whether an abnormality is malignant. We treat each
involves learning the actual structure of the network in addi- rule as an additional binary feature; true if the body, or condi-
tion to its parameters.                               tion, of the rule is satisﬁed, and otherwise false. We then run
  Notice that to predict the probability of malignancy of an the Bayesian network structure learning algorithm, allowing
abnormality, the Bayes net uses only the record for that ab- it to use these new features in addition to the original fea-
normality. Nevertheless, data in other rows of the table may tures. Below is a simple rule, covering 48 positive examples
also be relevant: radiologists may also consider other abnor- and 123 negative examples:
malities on the same mammogram or previous mammograms. Abnormality   A  in mammogram    M
For example, it may be useful to know that the same mam-         may  be  malignant   if:
mogram also contains another abnormality, with a particular A’s tissue  is not  asymmetric,
size and shape; or that the same person had a previous mam- M contains  another   abnormality   A2,
mogram with certain characteristics. Incorporating data from A2’s margins are  spiculated,    and
other rows in the table is not possible with existing Bayesian A2 has no architectural  distortion.
network learning algorithms and requires statistical relational
learning (SRL) techniques, such as probabilistic relational This rule can now be used as a ﬁeld in a new view of the
models [8]. Level 3 in Figure 2 shows the state-of-the-art database, and consequently as a new feature in the Bayesian
in SRL techniques, illustrating how relevant ﬁelds from other network. The last two lines of the rule refer to other rows of
rows (or other tables) can be incorporated in the network, us- the relational table for abnormalities in the database. Hence
ing aggregation if necessary. Rather than using only the size this rule encodes information not available to the current ver-
of the abnormality under consideration, the new aggregate sion of the Bayesian network.
ﬁeld allows the Bayes net to also consider the average size
of all abnormalities found in the mammogram.          4   Experiments
  Presently, SRL is limited to using the original view of the The purposes of the experiments we conducted are two-fold.
database, that is, the original tables and ﬁelds, possibly with First, we want to determine if using SRL yields an im-
aggregation. Despite the utility of aggregation, simply con- provement compared to propositional learning. Secondly, we
sidering only the existing ﬁelds may be insufﬁcient for accu- want to evaluate whether using Inductive Logic Programming
rate prediction of malignancies. Level 4 in Figure 2 shows the (ILP) to create features, which embody a new “view” of the
key capability that will be introduced and evaluated in this pa- database, adds a beneﬁt over current SRL algorithms. We
per: using techniques from rule learning to learn a new view. looked at adding two types of relational attributes, aggregate
The new view includes two new features utilized by the Bayes features and horn-clause (ILP) rules. Aggregate features rep-
net that cannot be deﬁned simply by aggregation of existing resent summaries of abnormalities found either in a partic-
features. The new features are deﬁned by two learned rules ular mammogram or for a particular patient. We performed                            Patient   Abnormality     Date  Mass Shape  …   Mass Size    Location   Benign/Malign 
                             P1             1            5/02       Spic                0.03       RU4            B 
                             P1             2            5/04       Var                 0.04       RU4            M 
                             P1             3            5/04       Spic                0.04       LL3             B 
                             …            …             …        …                      …          …             …                 


                                                        Parameter Learning: 
                 Level 1.       Be/Mal                  Given: Features (node labels, or fields 
                                                        in database), Data, Bayes net structure 
                                                        Learn: Probabilities.  Note: 
                          Shape          Size           probabilities needed are Pr(Be/Mal), 
                                                        Pr(Shape|Be/Mal), Pr (Size|Be/Mal) 

                                                        Structure Learning: 
                 Level 2.      Be/Mal                   Given: Features, Data                
                                                        Learn: Bayes net structure and 
                                                        probabilities.  Note: with this structure, 
                         Shape          Size            now will need Pr(Size|Shape,Be/Mal) 
                                                        instead of Pr(Size|Be/Mal).  

                                                        Aggregate Learning: 
                                                        Given: Features, Data, Background 
                 Level 3.      Be/Mal        Avg size 
                                             this date  knowledge – aggregation functions 
                                                        such as average, mode, max, etc.           
                                                        Learn: Useful aggregate features, 
                         Shape          Size            Bayes net structure that uses these 
                                                        features, and probabilities.  New 
                                                        features may use other rows/tables. 
                 Level 4. 

                      Shape change     Increase in      View Learning: 
                      in abnormality   average size of 
                      at this location abnormalities    Given: Features, Data, Background 
                                                        knowledge – aggregation functions 
                                                        and intensionally-defined relations 
                                                        such as “increase” or “same location” 
                               Be/Mal        Avg size   Learn: Useful new features defined 
                                             this date  by views (equivalent to rules or SQL 
                                                        queries), Bayes net structure, and 
                                                        probabilities. 
                         Shape          Size 
                                                                                             
            
Figure 2: Hierarchy of learning types. Levels 1 and 2 are available through ordinary Bayesian network learning algorithms,
Level 3 is available only through state-of-the-art SRL techniques, and Level 4 is described in this paper.

a series of experiments, aimed at discovering if moving up able. The TAN model can be constructed in polynomial
in the hierarchy outlined in Figure 2 would improve perfor- time with a guarantee that the model maximizes the Log
mance. First, we tried to learn a structure with just the orig- Likelihood of the network structure given the dataset [10;
inal attributes which performed better than the expert struc- 7].
ture. This corresponds to Level 2 learning. Next, we added
aggregate features to our network. Finally, we created new 4.1 Methodology
features using ILP. We investigated adding the features pro- The dataset contains 435 malignant abnormalities and 65365
posed by the ILP system as well as the aggregate to the net- benign abnormalities. To evaluate and compare these ap-
work.                                                 proaches, we used stratiﬁed 10-fold cross-validation. We ran-
                                                      domly divided the abnormalities into 10 roughly equal-sized
  We experimented with a number of structure learning al- sets, each with approximately one-tenth of the malignant ab-
gorithms for the Bayesian Networks, including Na¨ıve Bayes, normalities and one-tenth of the benign abnormalities. When
Tree Augmented Na¨ıve Bayes [7], and the sparse candidate evaluating just the structure learning and aggregation, 9 folds
algorithm [9]. However, we obtained best results with the were used for the training set. When performing aggregation,
TAN  algorithm in all experiments, so we will focus our we used binning to discretize the created features. We took
discussion on TAN. In a TAN network, each attribute can care to only use the examples in the train set to determine the
have at most one other parent in addition to the class vari- cut bin widths. When performing “view learning”, we hadtwo steps in the learning process. In the ﬁrst part, 4 folds             Level 1 vs Level 2
of data were used to learn the ILP rules. Afterwards, the re-
maining 5 folds were used to learn the Bayes net structure   1
and parameters.
  When using cross-validation on a relational database, there  0.8
exists one major methodological pitfall. Some of the cases
may be related. For example, we may have multiple abnor-   0.6
malities for a single patient. Because these abnormalities are
related (same patient), having some of these in the training set  0.4
and others in the test set may cause us to perform better on
those test cases than we would expect to perform on cases for  0.2
other patients. To avoid such “leakage” of information into a True  Positive Rate      Level 2
training set, we ensured that all abnormalities associated with                        Level 1
                                                             0
a particular patient are placed into the same fold for cross-  0      0.2    0.4     0.6     0.8      1
validation. Another potential pitfall is that we may learn a
                                                                         False Positive Rate
rule that predicts an abnormality to be malignant based on
properties of abnormalities in later mammograms. We never Figure 3: ROC Curves for Structure Learning. (Level 2)
predict the status of an abnormality at a given date based on
ﬁndings recorded with later dates.
  We present the results of our ﬁrst experiment using both               Level 1 vs Level 2
ROC  and precision recall curves. Because of our skewed      1
                                                                                       Level 2
class distribution, or large number of benign cases, we prefer                         Level 1
precision-recall curves over ROC curves because they better  0.8
show the number of “false alarms,” or unnecessary biopsies.
Therefore, we use precision-recall curves for the remainder  0.6
of the results. Here, precision is the percentage of abnormal-
ities that we classiﬁed as malignant that are truly cancerous.
Recall is the percentage of malignant abnormalities that were  0.4
                                                        Precision
correctly classiﬁed. To generate the curves, we pooled the re-
sults over all ten folds by treating each prediction as if it had  0.2
been generated from the same model. We sorted the estimates
and used all possible split points to create the graphs.     0
                                                               0      0.2    0.4     0.6     0.8      1
4.2  Results                                                                   Recall
The relational database containing the mammography data
                                                      Figure 4: Precision Recall Curves for Structure Learning.
contains one row for each abnormality in a mammogram.
                                                      (Level 2)
Fields in this relational table include all those shown in the
Bayesian network of Figure 1. Therefore it is straightforward
to use existing Bayesian network structure learning algo- ciﬁc patient. On the mammogram level, we only considered
rithms to learn a possibly improved structure for the Bayesian the abnormalities present on that speciﬁc mammogram. To
network. We compared the performance of the best learned discretize the averages, we divided each range into three bins.
networks against the expert deﬁned structure shown in Fig- For binary features we had predeﬁned bin sizes, while for
ure 1. We estimated the parameters of the expert structure the other features we attempted to get equal numbers of ab-
from the dataset using maximum likelihood estimates with normalities in each bin. For aggregation functions we used
Laplace correction. Figure 3 shows the ROC curve for these maximum and average. The aggregation introduced 108 new
experiments, and Figure 4 shows the Precision-Recall curves. features. For the interested reader, the following paragraph
Figure 7 shows the area under the precision-recall curve for presents further details of our aggregation process.
the expert network (L1) and with learned structure (L2). We We used a three step process to construct aggregate fea-
only consider recalls above 50%, as for this application ra- tures. First, we chose a ﬁeld to aggregate. Second, we se-
diologists would be required to perform at least at this level. lected an aggregation function. Third, we needed to decide
We further use the paired t-test to compare the areas under over which rows to aggregate the feature, that is, which keys
the curve for every fold. We found the difference to be statis- or links to follow. This is known as a slot chain in PRM termi-
tically signiﬁcant with a 99% level of conﬁdence.     nology. In our database, two such links exist. The patient id
  With the help of a radiologist, we selected the numeric and ﬁeld allows access to all the abnormalities for a given patient,
ordered features in the database and computed aggregates for providing aggregation on the patient level. The second key is
each of these features. We determined that 27 of the 36 at- the combination of patient id and mammogram date, which
tributes were suitable for aggregation. We computed aggre- returns all abnormalities for a patient on a speciﬁc mammo-
gates on both the patient and the mammogram level. On the gram, providing aggregation on the mammogram level. To
patient level, we looked at all of the abnormalities for a spe- demonstrate this process we will work though an example of Patient        Abnormality       Date           Mass Shape          ...           Mass Size       Location         Average           Average              Be/Mal
                                                                                                                                                            Patient             Mammogram
                                                                                                                                                            Mass Size        Mass Size

   P1                    1                    5/02              Spic                 ...            0.03                RU4                 0.0367               0.03                        B

   P1                    2                    5/04              Var                   ...            0.04                RU4                 0.0367               0.04                       M

   P1                    3                    5/04              Spic                 ...            0.04                LL4                  0.0367               0.04                        B

   ...       ...        ...       ...       ...    ...        ...         ...       ...           ...


                             Figure 5: Database after Aggregation on Mass Size Field


computing an aggregate feature for patient 1 in the database    Comparison of All Levels of Learning
given in Figure 2. We will aggregate on the Mass Size ﬁeld
                                                            1
and use average as the aggregation function. Patient 1 has                             Level 4
three abnormalities, one from a mammogram in May 2002                                  Level 3
                                                          0.8                          Level 2
and two from a mammogram in May 2004. To calculate the                                 Level 1
aggregate on the patient level, we would average the size for
all three abnormalities, which is .0367. To ﬁnd the aggregate 0.6
on the mammogram  level for patient 1, he have to perform
two separate computations. First, we follow the link P1 and 0.4

5/02, which yields abnormality 1. The average for this key Precision
mammogram   is simply .03. Second, we follow the link P1  0.2
and 5/04, which yields abnormalities 2 and 3. The average
for these abnormalities is .04. Figure 5 shows the database
                                                            0
following construction of these aggregate features.          0      0.2     0.4     0.6     0.8      1
  Next, we tested whether useful new ﬁelds could be com-                       Recall
puted by rule learning. Speciﬁcally, we used the ILP sys-
tem Aleph [26] to learn rules predictive of malignancy. Sev- Figure 6: Precision Recall Curves for Each Level of Learning
eral thousand distinct rules were learned for each fold, with
each rule covering many more malignant cases than (incor- Level 4 performs as well as aggregates for high recalls, and
rectly covering) benign cases. In order to obtain a varied set close to ILP for medium recalls. According to the paired t-test
of rules, we ran Aleph using every positive example in each the improvement of Level 4 over Level 2 is signiﬁcant, using
fold as a seed for the search. We avoid the rule overﬁtting the area under the curve metric, at the 99% level. Meanwhile,
                   [  ]
found by other authors 18 by doing breadth-ﬁrst search for Level 3 presents an improvement over Level 2, using the area
rules and by having a minimal limit on coverage. Each seed under the curve metric, at the 97% conﬁdence level.
generated anywhere from zero to tens of thousands of rules.
                                                        Levels 1 and 2 correspond to standard propositional learn-
We post processed the rules using a greedy algorithm, where
                                                      ing whereas levels 3 and 4 incorporate relational information.
we selected the best scoring rule that covers new examples
                                                      In this task, considering relational information is crucial for
ﬁrst. For each fold, the 50 best clauses were selected based
                                                      improving preformance. Furthermore, the process of gener-
on 3 criteria: (1) they needed to be multi-relational; (2) they
                                                      ating the views in Level 4 has been useful to the radiologist
needed to be distinct; (3) they needed to cover a signiﬁcant
                                                      as it has identiﬁed novel correlations between attributes.
number of malignant cases. The resulting views were added
as new features to the database. Figure 6 includes a compari-
son of all levels of learning.                        5   Related  Work
  We  can observe very signiﬁcant improvements when   Research in SRL has advanced along two main lines: meth-
adding multi-relational features. Both rules and aggregates ods that allow graphical models to represent relations, and
achieved better performance. Aggregates do better for higher frameworks that extend logic to handle probabilities. Along
recalls, while rules do better for medium recalls. We believe the ﬁrst line, probabilistic relational models, or PRMs, intro-
this is because ILP rules are more accurate than the other fea- duced by Friedman, Getoor, Koller and Pfeffer, represent one
tures, but have limited coverage.                     of the ﬁrst attempts to learn the structure of graphical mod-