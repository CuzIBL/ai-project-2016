     A Fast Normalized Maximum Likelihood Algorithm for Multinomial Data
                                  Petri Kontkanen, Petri Myllymaki¨
                            Complex Systems Computation Group (CoSCo)
                          Helsinki Institute for Information Technology (HIIT)
                      University of Helsinki & Helsinki University of Technology
                               P.O. Box 9800, FIN-02015 HUT, Finland.
                              {Firstname}.{Lastname}@hiit.fi

                    Abstract                          rectly believe that MDL and BIC are equivalent. The latest
                                                      instantiation of the MDL is not directly related to BIC, but
    Stochastic complexity of a data set is deﬁned as the to the formalization described in [Rissanen, 1996]. Unlike
    shortest possible code length for the data obtainable Bayesian and many other approaches, the modern MDL prin-
    by using some ﬁxed set of models. This measure is ciple does not assume that the chosen model class is correct.
    of great theoretical and practical importance as a It even says that there is no such thing as a true model or
    tool for tasks such as model selection or data clus- model class, as acknowledged by many practitioners. The
    tering. In the case of multinomial data, comput-  model class is only used as a technical device for constructing
    ing the modern version of stochastic complexity,  an efﬁcient code. For discussions on the theoretical motiva-
    deﬁned as the Normalized Maximum Likelihood       tions behind the modern deﬁnition of the MDL see, e.g., [Ris-
    (NML) criterion, requires computing a sum with    sanen, 1996; Merhav and Feder, 1998; Barron et al., 1998;
    an exponential number of terms. Furthermore, in   Grunwald,¨ 1998; Rissanen, 1999; Xie and Barron, 2000;
    order to apply NML in practice, one often needs to Rissanen, 2001].
    compute a whole table of these exponential sums.    The most important notion of the MDL principle is the
    In our previous work, we were able to compute this Stochastic Complexity (SC), which is deﬁned as the shortest
    table by a recursive algorithm. The purpose of this description length of a given data relative to a model class M.
    paper is to signiﬁcantly improve the time complex- The modern deﬁnition of SC is based on the Normalized
    ity of this algorithm. The techniques used here are Maximum Likelihood (NML) code [Shtarkov, 1987]. Unfor-
    based on the discrete Fourier transform and the con- tunately, with multinomial data this code involves a sum over
    volution theorem.                                 all the possible data matrices of certain length. Computing
                                                      this sum, usually called the regret, is obviously exponential.
1  Introduction                                       Therefore, practical applications of the NML have been quite
The Minimum Description Length (MDL) principle devel- rare,
                                                                           [                        ]
oped by Rissanen [Rissanen, 1978; 1987; 1996] offers a well- In our previous work Kontkanen et al., 2003; 2005 , we
founded theoretical formalization of statistical modeling. The presented a polynomial time (quadratic) method to compute
main idea of this principle is to represent a set of models the regret. In this paper we improve our previous results and
(model class) by a single model imitating the behaviour of show how mathematical techniques such as discrete Fourier
any model in the class. Such representative models are called transform and convolution can be used in regret computation.
universal. The universal model itself does not have to belong The idea of applying these techniques for computing a sin-
                                                                                     [            ]
to the model class as often is the case.              gle regret term was ﬁrst suggested in Koivisto, 2004 , but as
                                                                 [                  ]
  From a computer science viewpoint, the fundamental idea discussed in Kontkanen et al., 2005 , in order to apply NML
of the MDL principle is compression of data. That is, given to practical tasks such as clustering, a whole table of regret
some sample data, the task is to ﬁnd a description or code terms is needed. We will present here an efﬁcient algorithm
of the data such that this description uses less symbols than for this speciﬁc task. For a more detailed discussion of this
                                                               [                           ]
it takes to describe the data literally. Intuitively speaking, work, see Kontkanen and Myllymaki,¨ 2005 .
this approach can in principle be argued to produce the best
possible model of the problem domain, since in order to be 2 NML for Multinomial Data
able to produce the most efﬁcient coding of data, one must The most important notion of the MDL is the Stochastic Com-
capture all the regularities present in the domain.   plexity (SC). Intuitively, stochastic complexity is deﬁned as
  The MDL principle has gone through several evolutionary the shortest description length of a given data relative to a
steps during the last two decades. For example, the early re- model class. To formalize things, let us start with a deﬁnition
alization of the MDL principle, the two-part code MDL [Ris- of a model class. Consider a set Θ ∈ Rd, where d is a pos-
sanen, 1978], takes the same form as the Bayesian BIC cri- itive integer. A class of parametric distributions indexed by
terion [Schwarz, 1978], which has led some people to incor- the elements of Θ is called a model class. That is, a modelclass M is deﬁned as                                  that an efﬁcient way to compute the regret term is via the
                                                      following recursive formula:
              M  =  {P (· | θ) : θ ∈ Θ}.        (1)
                                                                                                  −
                                                                     N                r          N r
  Consider now a discrete data set (or matrix) xN =         N              N!       r    N  − r
                                                          RMT  ,K =
(x1, . . . , xN ) of N outcomes, where each outcome xj is            =0 r!(N − r)! N       N
                                                                    r             ³  ´  µ      ¶
an element of the set X consisting of all the vectors of the        X    r       N−r
                                                                      · RMT ,k1 · RMT ,k2 ,           (7)
form (a1, . . . , am), where each variable (or attribute) ai takes
on values v ∈ {1, . . . , ni}. Given a model class M, the Nor- where k1 + k2 = K.
malized Maximum Likelihood (NML) distribution [Shtarkov, As discussed in [Kontkanen et al., 2005], in order to ap-
1987] is deﬁned as                                    ply NML to the clustering problem, we need to compute a
                                                      whole table of regret terms. This table consists of the terms
                         P (xN | θˆ(xN ), M)
              xN                                        n    for            and            , where  is the
        PNML(     | M) =         N        ,     (2)   RMT  ,k   n = 0, . . . , N k = 1, . . . , K K
                               RM                     maximum number of clusters.
                                                        The procedure of computing the regret table starts by ﬁll-
where θˆ(xN ) denotes the maximum likelihood estimate of
                                                      ing the ﬁrst column, i.e., the case k = 1, which is trivial
data xN , and RN is given by
             M                                        (see [Kontkanen et al., 2005]). To compute the column k,
             N           N     N                      for k = 2, . . . , K, the recursive formula (7) can be used by
           RM   =    P (x  | θˆ(x ), M),        (3)
                                                      choosing k1 = k − 1, k2 = 1. The time complexity of ﬁlling
                  xN                                                          2
                  X                                   the whole table is O K · N . For more details, see [Kon-
and the sum goes over all the possible data matrices of size N. tkanen et al., 2005; Kontkanen and Myllymaki,¨ 2005].
          N                                                             ¡      ¢
The term RM is called the regret. The deﬁnition (2) is intu- In practice, the quadratic dependency on the size of data
itively very appealing: every data matrix is modeled using limits the applicability of NML to small or moderate size data
its own maximum likelihood (i.e., best ﬁt) model, and then a sets. In the next section, we will present a novel, signiﬁcantly
penalty for the complexity of the model class M is added to more efﬁcient method for computing the regret table.
normalize the distribution.
  The stochastic complexity of a data set xN with respect to a 3 The Fast NML Algorithm
model class  can now be deﬁned as the negative logarithm
          M                                           In this section we will derive a very efﬁcient algorithm for
of (2), i.e.,
                                                      the regret table computation. The new method is based on
                       xN  ˆ xN                       the Fast Fourier Transform algorithm. As mentioned in the
     xn             P (   | θ(  ), M)
 SC(    | M) = − log        N                   (4)   previous section, the calculation of the ﬁrst column of the
                           RM                         regret table is trivial. Therefore, we only need to consider the
                        N     N             N
             = − log P (x | θˆ(x ), M) + log RM. (5)  case of calculating the column k given the ﬁrst k−1 columns.
                                                      Let us deﬁne two sequences a and b by
       [                   ]
  As in Kontkanen et al., 2005 , in the sequel we focus on            n                 n
a multi-dimensional model class suitable for cluster analysis.      n    n            n    n
                                                               an =    RM    −1, bn =    RM   1,      (8)
The selected model class has also been successfully applied          n!    T ,k        n!    T ,
to mixture modeling [Kontkanen et al., 1996], case-based for n = 0, . . . , N. Evaluating the convolution of a and b
reasoning [Kontkanen et al., 1998], Naive Bayes classiﬁca- gives
tion [Grunwald¨ et al., 1998; Kontkanen et al., 2000b] and
               [                   ]                                n   h                n−h
data visualization Kontkanen et al., 2000a .                           h   h      (n − h)     n−h
                                                         (a ∗ b)n =      RM     −1          R     1   (9)
  Let us assume that we have m variables, (a1, . . . , am), and        h!    T ,k  (n − h)!   MT ,
we also assume the existence of a special variable c (which        h=0
                                                                   X   n                           −
can be chosen to be one of the variables in our data or it can     nn         n!     h  h   n − h n  h
be latent). Furthermore, given the value of c, the variables     =
                                                                    n! =0 h!(n − h)! n       n
(a1, . . . , am) are assumed to be independent. The resulting         h             µ  ¶  µ      ¶
                                                                      Xh        n−h
model class is denoted by MT . Suppose the special vari-
                                                                     · RMT ,k−1RMT ,1                (10)
able c has K values and each ai has ni values. The NML               n
                                                                   n    n
distribution for the model class MT is now                       =    RM    ,                        (11)
                                                                    n!    T ,k
                    K        hk m  K  ni        fikv
        N                hk               fikv        where the last equality follows from the recursion for-
PNML(x    | MT ) =
                         N                 hk         mula (7). This derivation shows that the column k can be
                  "k=1         i=1 k=1 v=1         #
                    Y  µ   ¶   Y  Y   Y  µ    ¶       computed by ﬁrst evaluating the convolution (11), and then
                        1                             multiplying each term by n!/nn.
                    ·  N     ,                  (6)
                     RMT  ,K                            The standard convolution theorem states that convolutions
                                                      can be evaluated via the (discrete) Fourier transform, which in
                                          N
where hk is the number of times c has value k in x , fikv is turn can be computed efﬁciently with the Fast Fourier Trans-
                                             N
the number of times ai has value v when c = k, and RMT ,K form algorithm (see [Kontkanen and Myllymaki,¨ 2005] for
is the regret term. In [Kontkanen et al., 2005] it was proven details). It follows that the time complexity of computing thewhole regret table drops to O (N log N · K). This is a ma- [Kontkanen et al., 1998] P. Kontkanen, P. Myllymaki,¨ T. Si-
jor improvement over O N 2 · K obtained by the recursion lander, and H. Tirri. On Bayesian case matching. In
method of Section 2.                                     B. Smyth and P. Cunningham, editors, Advances in Case-
                    ¡       ¢                            Based Reasoning, Proceedings of the 4th European Work-
4  Conclusion And Future Work                            shop (EWCBR-98), volume 1488 of Lecture Notes in Arti-
                                                         ﬁcial Intelligence, pages 13–24. Springer-Verlag, 1998.
The main result of this paper was a derivation of a novel algo- [Kontkanen et al., 2000a] P. Kontkanen, J. Lahtinen, P. Myl-
rithm for the regret table computation. The theoretical time lymaki,¨ T. Silander, and H. Tirri. Supervised model-based
complexity of this algorithm allows practical applications of visualization of high-dimensional data. Intelligent Data
NML in domains with very large datasets. With the earlier Analysis, 4:213–227, 2000.
quadratic-time algorithms, this was not possible.
  In the future, we plan to conduct an extensive set of em- [Kontkanen et al., 2000b] P. Kontkanen, P. Myllymaki,¨ T. Si-
pirical tests to see how well the theoretical advantage of the lander, H. Tirri, and P. Grunwald.¨ On predictive distribu-
new algorithm transfers to practice. On the theoretical side, tions and Bayesian networks. Statistics and Computing,
our goal is to extend the regret table computation to more 10:39–54, 2000.
complex cases like general graphical models. We will also [Kontkanen et al., 2003] P. Kontkanen, W. Buntine, P. Myl-
research supervised versions of the stochastic complexity, de- lymaki,¨ J. Rissanen, and H. Tirri. Efﬁcient computation of
signed for supervised prediction tasks such as classiﬁcation. stochastic complexity. In C. Bishop and B. Frey, editors,
                                                         Proceedings of the Ninth International Conference on Ar-
Acknowledgements                                         tiﬁcial Intelligence and Statistics, pages 233–238. Society
This work was supported in part by the Academy of Finland for Artiﬁcial Intelligence and Statistics, 2003.
under the projects Minos and Civi and by the National Tech-
                                                      [Kontkanen et al., 2005] P. Kontkanen, P. Myllymaki,¨
nology Agency under the PMMA project. In addition, this
                                                         W. Buntine, J. Rissanen, and H. Tirri. An MDL frame-
work was supported in part by the IST Programme of the Eu-
                                                         work for data clustering. In P. Grunwald,¨ I.J. Myung,
ropean Community, under the PASCAL Network of Excel-
                                                         and M. Pitt, editors, Advances in Minimum Description
lence, IST-2002-506778. This publication only reﬂects the
                                                         Length: Theory and Applications. The MIT Press, 2005.
authors’ views.
                                                      [Merhav and Feder, 1998] N. Merhav and M. Feder. Univer-
                                                         sal prediction. IEEE Transactions on Information Theory,
References                                               44(6):2124–2147, October 1998.
[Barron et al., 1998] A. Barron, J. Rissanen, and B. Yu. The [Rissanen, 1978] J. Rissanen. Modeling by shortest data de-
  minimum description principle in coding and modeling.  scription. Automatica, 14:445–471, 1978.
  IEEE Transactions on Information Theory, 44(6):2743–
  2760, October 1998.                                 [Rissanen, 1987] J. Rissanen. Stochastic complexity. Jour-
                                                         nal of the Royal Statistical Society, 49(3):223–239 and
[Grunwald¨ et al., 1998] P. Grunwald,¨ P. Kontkanen, P. Myl- 252–265, 1987.
  lymaki,¨ T. Silander, and H. Tirri. Minimum encod-
                                                      [            ]
  ing approaches for predictive modeling. In G. Cooper Rissanen, 1996 J. Rissanen.  Fisher information and
  and S. Moral, editors, Proceedings of the 14th Interna- stochastic complexity. IEEE Transactions on Information
  tional Conference on Uncertainty in Artiﬁcial Intelligence Theory, 42(1):40–47, January 1996.
  (UAI’98), pages 183–192, Madison, WI, July 1998. Mor- [Rissanen, 1999] J. Rissanen. Hypothesis selection and test-
  gan Kaufmann Publishers, San Francisco, CA.            ing by the MDL principle. Computer Journal, 42(4):260–
                                                         269, 1999.
[Grunwald,¨ 1998] P. Grunwald.¨ The Minimum Description
  Length Principle and Reasoning under Uncertainty. PhD [Rissanen, 2001] J. Rissanen. Strong optimality of the nor-
  thesis, CWI, ILLC Dissertation Series 1998-03, 1998.   malized ML models as universal codes and informa-
                                                         tion in data. IEEE Transactions on Information Theory,
[Koivisto, 2004] M. Koivisto. Sum-Product Algorithms for 47(5):1712–1717, July 2001.
  the Analysis of Genetic Risks. PhD thesis, Report A-
  2004-1, Department of Computer Science, University of [Schwarz, 1978] G. Schwarz. Estimating the dimension of a
  Helsinki, 2004.                                        model. Annals of Statistics, 6:461–464, 1978.
                                                      [            ]
[Kontkanen and Myllymaki,¨ 2005] P. Kontkanen and P. Myl- Shtarkov, 1987 Yu M. Shtarkov. Universal sequential cod-
  lymaki.¨ Computing the regret table for multinomial data. ing of single messages. Problems of Information Trans-
  Technical Report 2005-1, Helsinki Institute for Informa- mission, 23:3–17, 1987.
  tion Technology (HIIT), 2005.                       [Xie and Barron, 2000] Q. Xie and A.R. Barron. Asymp-
                                                         totic minimax regret for data compression, gambling, and
[Kontkanen et al., 1996] P. Kontkanen, P. Myllymaki,¨ and
                                                         prediction. IEEE Transactions on Information Theory,
  H. Tirri. Constructing Bayesian ﬁnite mixture models
                                                         46(2):431–445, March 2000.
  by the EM algorithm. Technical Report NC-TR-97-003,
  ESPRIT Working Group on Neural and Computational
  Learning (NeuroCOLT), 1996.