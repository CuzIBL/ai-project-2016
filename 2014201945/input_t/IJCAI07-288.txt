                    Ambiguous Part-of-Speech Tagging for Improving
                  Accuracy and Domain Portability of Syntactic Parsers

          Kazuhiro Yoshida∗      Yoshimasa Tsuruoka†      Yusuke Miyao∗      Jun’ichi Tsujii∗†‡
                        ∗Department of Computer Science, University of Tokyo
                            †School of Informatics, University of Manchester
                                    ‡National Center for Text Mining
                          {kyoshida,tsuruoka,yusuke,tsujii}@is.s.u-tokyo.ac.jp

                    Abstract                          clude their ability to adapt to domains. POS taggers for a new
                                                      domain are much easier to develop than full parsers, because
    We aim to improve the performance of a syntac-    training corpora for POS taggers are easier to construct com-
    tic parser that uses a part-of-speech (POS) tagger pared to those for full parsers, which require the annotation
    as a preprocessor. Pipelined parsers consisting of of nested phrase structures.
    POS taggers and syntactic parsers have several ad-
    vantages, such as the capability of domain adapta-  However, independence assumption of taggers and parsers
    tion. However the performance of such systems on  may degrade the overall accuracy of the parsers. Wat-
                                                          [    ]
    raw texts tends to be disappointing as they are af- son 2006 reported that using an automatic POS tagger
    fected by the errors of automatic POS tagging. We caused the F1 score of grammatical relations output by a
    attempt to compensate for the decrease in accu-   parser to drop by 2.02 points. She attempted to weaken the
    racy caused by automatic taggers by allowing the  independence assumption by letting the taggers output multi-
    taggers to output multiple answers when the tags  ple tags for each word, weighted by probability values. Her
    cannot be determined reliably enough. We empir-   approach improved the F1 score by 0.66 points.
    ically verify the effectiveness of the method us-   In this paper, we verify Watson’s results on ambiguous
    ing an HPSG parser trained on the Penn Treebank.  POS tagging using an HPSG [Pollard and Sag, 1994] parser
    Our results show that ambiguous POS tagging im-   developed and trained on the Penn Treebank [Marcus et al.,
    proves parsing if outputs of taggers are weighted 1994]. At the same time, we investigate the effectiveness of
    by probability values, and the results support pre- ambiguous POS tagging for domain adaptation of parsers us-
    vious studies with similar intentions. We also ex- ing the GENIA corpus [Ohta et al., 2002] as the test set. Ex-
    amine the effectiveness of our method for adapting perimental results show that the multiple output without prob-
    the parser to the GENIA corpus and show that the  ability values cannot improve the parser much and suggest
    use of ambiguous POS taggers can help develop-    the importance of probability distribution of multiple tags ob-
    ment of portable parsers while keeping accuracy   tained by POS taggers. Additionally, we show that the pos-
    high.                                             itive effect of ambiguous POS tagging is maintained for do-
                                                      mains unfamiliar to the parser.
1  Introduction
                                                      2   Background
Some parsers use POS taggers as their preprocessors, and
some use integrated models that achieve tagging and parsing In this section, we describe the POS tagger and syntactic
simultaneously. Because the latter approach is more general, parser used in our experiments. These taggers and parsers
it is successfully used by some of the state-of-the-art parsers, were combined to make a pipelined syntactic parser for raw
such as Charniak’s [Charniak and Johnson, 2005],asanatural texts using the strategy described in the next section.
consequence of the pursuit of accuracy. However, integrated As both of our tagging and parsing models are based
models of tagging and parsing tend to be complex and com- on log-linear classiﬁers, we ﬁrst brieﬂy introduce log-linear
putationally expensive, both in terms of training and run-time models and then describe our tagger and parser.
costs.
  On the other hand, pipelined systems of POS taggers and 2.1 Log-linear models
parsers can be built with independently developed taggers and
parsers. In such models, we can easily make use of taggers Log-linear models are among the most widely used machine
that use state-of-the-art sequence labeling techniques, most learning techniques in NLP literature, and we use the models
of which are difﬁcult to be incorporated into syntactic disam- both for POS taggers and syntactic parsers. A conditional log-
biguation models. Advantages of pipelined parsers also in- linear model calculates the probability of an event E given the

                                                IJCAI-07
                                                  1783Input: Sentence s
                   t ,...t                            the HPSG grammar is a combination of two log-linear mod-
Output: Tag sequence 1   n                            els.
Algorithm:                                              The ﬁrst log-linear model is for selecting lexical entries for
 1.  foreach ti do ti := NULL
             t   NULL                                 words of a POS-tagged sentence, which estimates the proba-
 2.  foreach i =                                      bility
       Compute probability distribution
       P (ti|ti−2,ti−1,ti+1,ti+2,s) (1)                                 P (li|wi,ti),                 (2)
       by log-linear models
 3.  Let i be easiest place to tag in                 where wi, ti,andli represent an ith word, POS tag, and lex-
                                                                                                       w
     ti := (Most probable tag for ti)                 ical entry in a sentence, respectively. The information of i
 4.  repeat 2 and 3 until                             and ti are used in combination as features of the log-linear
     ti = NULL for each i                            model.
                                                        The second model is for selecting the most probable parse
          Figure 1: Algorithm for POS tagging         from an HPSG parse forest F which is constructed from lex-
                                                      ical entries that are weighted by the ﬁrst model:

condition C as follows:                                          P (T |l1,...,n,w1,...,n,t1,...,n),
                       
                   exp(   λifi(E,C))                  where T ∈ F . The overall disambiguation model is
         P (E|C)=        i           ,
                          ZC
                                                                        argmax P (T |w1,...,n,t1,...,n)=
                                                                          T ∈F
where the real-valued functions fi are used to indicate use-                          
                       E            λ
ful features for predicting , parameters i are optimized argmax P (T |l1,...,n,w1,...,n,t1,...,n) P (li|wi,ti).
to ﬁt the model to the training data, and ZC is a normal- T ∈F                         i
ization constant. Several criteria for estimating the param-
eters of log-linear models exist, and we used MAP estimation The parsing algorithm for the disambiguation model is a
with Gaussian prior distribution [Chen and Rosenfeld, 1999], CYK-like chart parsing with iterative beam search [Ninomiya
which is most commonly and successfully applied in various et al., 2005].
NLP tasks.
                                                      3   Combining POS taggers and parsers
2.2  POS tagger
                                                      In pipelined parsers, POS taggers and syntactic parsers are
We employ the bidirectional inference model proposed by developed separately, and we can freely change taggers for
Tsuruoka et al. [2005] for our POS taggers. The model con- parsers, without any special care for the algorithms. To make
sists of 16 log-linear models, each of which provides the pipelined systems of ambiguous POS taggers and parsers
probability                                           work the same way, we restrict the interface of ambiguous
                                                      taggers and formalize the condition of the syntactic disam-
           P (ti|ti−2,ti−1,ti+1,ti+2,s),        (1)
                                                      biguation models that can be applied to the output of ambigu-
where s is a given sentence, and ti is the POS tag for the ith ous taggers without modiﬁcation.
word. An algorithm used by the POS taggers is shown in Fig- We concentrate on the following situation: for each word
ure 1. The key idea of the algorithm is that it does not work in a sentence, POS taggers output a set of candidate POS tags,
in the usual left-to-right manner, instead it iteratively tries to and the obtained tags are used to restrict parses in the parsing
tag words that can be tagged most easily. “Easiness” of tag- model.
ging is measured by the highest probability value among the In our experiments, we compared the following three types
probability distribution of the tags. When we calculated Eq. of taggers.
1 in step 2 of the algorithm, each from ti−2,ti−1,ti+1,ti+2
could be NULL, so we prepared 24 =16classiﬁers to cover
                               NULL                   single The single tagger outputs the most probable single
every pattern of the appearances of . Each token in a tag for each word.
training corpus was used as training data for all 16 classiﬁers.
The features used by the classiﬁers were the surface strings of
words, base forms of words obtained by a dictionary, preﬁxes multi The multi tagger outputs a set of candidate POS tags
and sufﬁxes, and POSs of already tagged words.        for each word.
  Though the algorithm described above is totally determin-
istic, a slight modiﬁcation with beam search strategy could prob The prob tagger is similar to multi, but each POS tag is
make it output an approximation of k-best tag sequences with weighted by its probability. That is, it provides the probability
probability values.                                   distribution P (tij |S),whereS is an input sentence and tij
                                                      represents the jth candidate POS tag of the ith word.
2.3  Grammar and parser                                 We assume that the parsing model consists of two indepen-
The parser we used is based on Head-Driven Phrase Struc- dent models: terminal and non-terminal.AparseT of a given
ture Grammar (HPSG), which was developed using the Penn sentence S consists of the terminal structure T t and non-
Treebank [Miyao et al., 2004]. The disambiguation model for terminal structure T nt, and terminal and non-terminal models

                                                IJCAI-07
                                                  1784are used to estimate the probability distributions P (T t|S) and         LP/LR       F1
P (T nt|T t,S). The best parse is given by                             87.12/86.71 86.91
                          nt  t
               argmax P (T  ,T |S)=
                T nt,T t                                 Table 1: Accuracy on section 22 with correct POS tags
                     nt t       t
          argmax P (T |T ,S)P (T |S).
          T nt,T t
                                                        To control the ambiguity of the tagger, we introduce a pa-
Then, we assume that the terminal structure T t can be fur- rameter θ which is used to threshold the candidate tags. If the
ther decomposed into a sequence of terminal labels li,...,ln, marginalized probability of a candidate tag is smaller than the
where the label li corresponds to the ith word of the sentence, probability of the best tag of the same word multiplied by θ,
and terminal labels are independent of one another in the ter- the candidate is discarded.
minal model:                                            Figure 2 illustrates example outputs of each tagger from
                       
             P T t|S      P l |S .                    the 3 best analysis of the sentence, “Mr. Meador had been
              (    )=       ( i )                     executive vice president of Balcor,” without thresholding.
                        i                               Marginalization of probability distribution output by POS
The overall disambiguation model becomes              taggers loses information about the sequential dependency of
                              
                 nt                                   tags, which can harm the performance of the parser. For ex-
      argmax P (T |l0,...,ln,S)  P (li|S).      (3)   ample, let us consider the sentence, “Time ﬂies like an arrow.”
        nt t
      T  ,T                    i                      When there are two candidate POS tag sequences “NN VBZ
                                                      IN DT NN” and “VB NNS IN DT NN,” the output of multi
We assume that a tractable estimation method and disam-            {       }{          }
biguation algorithm applicable to Eq. 3 exist.        tagger will be “ NN, VB VBZ, NNS   IN DT NN,” which
                                                      can induce a tag sequence “VB VBZ IN DT NN” that is not
  Let us then rewrite the distribution P (li|S) to make it de-
pendent on outputs of POS taggers, so that we can incorporate included in the original candidates.
the information of POS tags into the parsing model.
                                                     4   Experiments
        P (li|S)=   P (tij ,S)P (li|tij ,S),    (4)
                  j                                   The ﬁrst experiment veriﬁed the effect of ambiguous POS
                                                      tagging using the Penn Treebank for both training and test-
where tij is the jth element of a set of POS tags assigned to ing. The second experiment examined the parser’s capability
the ith word of the sentence, and P (tij ,S) is the probability of domain adaptation, using the Penn Treebank for training
of that tag calculated by the prob tagger (multi and single tag- the parser, biomedical texts for training the tagger, and the
gers can be integrated into the model similarly by assuming GENIA corpus for testing.
the tags assigned to the same word by the taggers are equally Both experiments used the same HPSG grammar and
probable). By replacing P (li|S) that appears in Eq. 3 with parser. The grammar was developed using the Penn Treebank
Eq. 4, we can apply the same disambiguation algorithm as sections 02-21, and the disambiguation model was trained
Eq. 3 to the combined system of an ambiguous POS tagger on the same data. The accuracy of the parser evaluated on
and the parsing model.                                the Penn Treebank section 221 using the correct POS tags is
  This strategy is applicable to a wide range of grammars and shown in Table 1. Labeled precision (LP) and labeled re-
disambiguation models including PCFG, where the terminal call (LR) are the precision and recall of dependencies with
model is used to assign POS tags to words and lexicalized predicate-argument labels, and F1 is the harmonic mean of
grammars such as HPSG, where the terminal model assigns LP and LR. Each dependency between a pair of words is
lexical entries to words. The HPSG parser described in Sec- counted as one event. These measures were also used in the
tion 2.3 is straightforwardly adapted to this model, by taking following experiments. The ﬁgures in Table 1 can be consid-
Eq. 2 as a terminal model.                            ered the practical upper bounds, because they are not affected
  One problem with this strategy is the increased ambiguity by incorrect predictions of POS taggers. (We will refer to the
introduced by multiple tags. As reported by Watson [2006], results using the correct POS tags as gold.) In the follow-
the increase in computational costs can be suppressed by ap- ing, comparisons of the systems were performed using the F1
plying appropriate parsing algorithms. The parsing algorithm scores.
we used [Ninomiya et al., 2005] is suitable for such purposes,
and we will show experimentally that the disambiguation of 4.1 Parsing Penn Treebank
the above model can be performed with efﬁciency similar to
                                                      The POS taggers described in this section were trained on the
models with single taggers.
                                                      Penn Treebank sections 00-18, and the performance of the
3.1  Implementation of prob tagger                    tagger on the development set was 96.79%. As described
                                                      in Section 3.1, our taggers multi and prob have a hyper-
There can be various methods for implementing the prob tag- parameter θ for controlling the number of alternative answers
ger described above. Our implementation outputs approxi-
mation of tag probabilities by marginalizing the probability 1Following the convention of literature on Penn Treebank pars-
of k-best tag sequences obtained by the algorithm described ing, we used section 22 for development, and section 23 for the ﬁnal
in Section 2.2.                                       test.

                                                IJCAI-07
                                                  1785        Input         Mr        Meador     had   been  executive  vice president of  Balcor .
    3 best sequences NNP         NNP      VBD   VBN       JJ      NN     NN     IN    NNP   .  (0.879)
     (probability)    NN          NN      VBD   VBN       JJ      NN     NN     IN    NNP   .  (0.075)
                     NNP         NNP      VBD   VBN       NN      NN     NN     IN    NNP   .  (0.046)
        single       NNP         NNP      VBD   VBN       JJ      NN     NN     IN    NNP   .
        multi      {NNP, NN}{NNP, NN}     VBD   VBN    {JJ, NN}   NN     NN     IN    NNP   .
        prob       NNP(0.925) NNP(0.925)  VBD   VBN     JJ(0.955) NN     NN     IN    NNP   .
     (probability) NN(0.075)   NN(0.075)               NN(0.045)

                                     Figure 2: Example of tagging results

                           LP/LR       F1                   #oftrain-  Tagging
                                                                                    LP/LR      F1
        multi θ =0.1     85.24/84.68 84.96                  ing data   strategy
        multi θ =0.01    84.13/83.64 83.88                               gold     63.63/36.57 46.45
        multi θ =0.001   79.29/78.68 78.98                       300     single   62.51/35.43 45.23
        multi θ =0.0001  71.01/70.36 70.68                               prob     61.46/39.62 48.18
        prob θ =0.1      85.26/84.73 84.99                               gold     75.21/68.41 71.65
        prob θ =0.01     85.28/84.88 85.08                      1000     single   73.84/66.33 69.88
        prob θ =0.001    85.19/84.80 84.99                               prob     73.56/68.10 70.72
        prob θ =0.0001   85.08/84.60 84.84                               gold     81.72/78.94 80.31
                                                                3000     single   79.97/77.42 78.67
                                                                         prob     79.90/78.34 79.11
           Table 2: Accuracy with different θ
                                                                         gold     85.35/84.32 84.83
                                                               10000     single   83.50/82.09 82.79
included in the output, where a smaller θ means a larger num-            prob     83.47/82.59 83.03
ber of alternative answers. Before comparing ambiguous tag-              gold     87.12/86.71 86.91
gers (i.e., multi and prob) with single, we ﬁrst determined    39832     single   85.26/84.72 84.99
which of multi and prob and which value of θ perform best.       (all)   prob     85.28/84.88 85.08
  The performance of the parser for various θ is shown in
Table 2. The parsers with the multi tagger was outperformed Table 3: Accuracy with different sizes of training data
by prob in all cases2, so we only used the prob tagger in the
following experiments. The performance of the parser with
the prob tagger has a peak around θ =0.01. The accuracy       87
                        θ
slowly decreases for smaller s, the reason for which might    86
be the problem noted in the last paragraph of Section 3.1. We
used the best performing θ (=0.01) in the following experi-   85
ments.                                                        84
  It was interesting to see whether the contribution of am-   83
                                                          F1
biguous tagging changed according to the performance of       82
the syntactic disambiguation model, because if ambiguous
                                                              81
POS tagging did not work well with a poorer disambiguation
model, ambiguous tagging could be considered not to help      80                        gold
                                                                                       single
the performance in domains unfamiliar for the parser, where   79                        prob
the performance of the disambiguation model was likely to     78
be lower. We observed a change in performance of the parser       5000  10000  15000  20000  25000  30000  35000  40000
when the number of sentences used for developing the gram-                  # of sentences
mar and for training the syntactic disambiguation model was
changed. Table 3 shows the performance of the parser trained
with various numbers of sentences selected from the Penn    Figure 3: F1 of different sizes of training data
Treebank sections 02-21. Figure 3 shows F1 scores of the
systems trained on 3000, 10000, and 39832 sentences.                                     mean
                                                                                         parsing
  Ambiguous POS tagging with the prob tagger could be                  LP/LR       F1
                                                                                         time
contended to contribute to the performance of the parser, be-
                                                              gold   86.91/86.28  86.59  739 msec
cause the performance of the parser with the prob tagger was
                                                              single 84.41/83.80  84.10  785 msec
consistently better than that of the single tagger in all of the
                                                              prob   85.24/84.89  85.06  936 msec
  2Actually, multi has a negative effect, compared with the results
of single shown in the ’all’ row of Table 3, which has an F1 score of Table 4: Accuracy on test set
84.99.

                                                IJCAI-07
                                                  1786                                     mean
                                                      so signiﬁcant.
     POS tagger    LP/LR       F1    parsing
                                     time               In summary, we showed that the adaptation of POS taggers
     gold        83.21/81.57  82.38  845 msec         can signiﬁcantly improve the performance of parsers in new
     single-Penn 76.84/76.48  76.66  799 msec         domains, and the use of ambiguous POS tagging can extend
     prob-Penn   78.48/78.11  78.29  950 msec         the improvement further.
     single-BIO  81.75/80.65  81.20  803 msec
     prob-BIO    82.60/81.44  82.02  873 msec         5   Related Work
                                                      Charniak et al. [1996] investigated the use of POS taggers that
             Table 5: Accuracy on GENIA               output multiple tags for parsing and concluded that single tag-
                                                      gers are preferable, because the accuracy of the tag sequences
                                                      selected by parsers did not improve signiﬁcantly, while the in-
above experiments if evaluated in the F1 measure. With the crease in computational cost is considerable. Watson [2006]
entire training data used for the development of the parser, revisited the same task and observed that in terms of the ac-
the contribution of ambiguous POS tagging was about 0.093. curacy of parsing, multiple taggers improve the performance
Table 4 summarizes the performance of some of our systems signiﬁcantly. Our results show that the taggers should pass to
evaluated using section 23 of the Penn Treebank. The overall the parser not only multiple answers, but also probability val-
tendency observed on the development set remained, but the ues for each candidate. Watson’s results also imply that ap-
contribution of ambiguous tagging was considerably bigger propriate parsing strategies can make the increase of compu-
than in the development set.4 As can be seen from the table, tational cost not as problematic as Charniak et al. suggested.
prob was the slowest of all, but the difference is not so big. Our results again agree with Watson’s at this point.
This may be because Ninomiya’s method of HPSG parsing,  Clark et al. [2004] introduced supertaggers that output
which outputs the ﬁrst answer found, worked tolerantly to in- multiple supertag candidates, and the taggers were used as
crease of ambiguity.                                  a front end of their CCG parser. Because the training of su-
                                                      pertaggers require corpora more deeply annotated than POS
4.2  Parsing GENIA corpus                             tagged ones, their method should have difﬁculties if we ex-
One of the most crucial advantages of separating POS tagging ploit taggers for domain adaptation.
from syntactic parsing is that the domain adaptation of such Adaptation of general parsers to a biomedical domain has
systems can be partly achieved by just changing POS tag- already attracted several researchers. Because the biggest dif-
gers. In this section, we verify that the effect of ambiguous ference between general English and biomedical literature
POS tagging is also remains for domains with which parsing is their vocabularies, use of lexical resources and tools is a
models are not familiar.                              natural approach. Lease et al. [2005] modiﬁed Charniak’s
  The test data we used was the GENIA treebank [Tateisi et parser [1999] to make it conform to GENIA’s tagging con-
al., 2005], which annotates Penn Treebank-style tree struc- vention and made use of biomedical dictionaries to restrict
tures to sentences of the GENIA corpus. The GENIA corpus the parser output. They obtained an impressive error reduc-
consists of the abstracts of biomedical papers adopted from tion of 14.2% and expected that further improvement is pos-
MEDLINE   [Campbell et al., 1997]. We trained our POS tag- sible using named-entity recognizers.
ger with the mixture of the WSJ part of the Penn Treebank, We can see that the performance shown in Section 4.2
the Penn BioIE corpus [Kulick et al., 2004],andtheGENIA is signiﬁcantly lower than those in 4.1. This suggests that
corpus, which consist of 38219, 29422, and 18508 sentences, changing POS taggers can only partially achieve domain
respectively. Following Hara et. al. [2005], we adopted 50 adaptation, so we should consider training syntactic disam-
abstracts (467 sentences) for the evaluation. Performance of biguation models on the target domain if we desire further
the single tagger on these sentences was 98.27%.      improvement. This is what Hara et al. [2005] explored, and
  The result is shown in Table 5. single-Penn and prob-Penn their results suggested that a small number of in-domain data
are single and prob taggers trained on the Penn Treebank, and could bring considerable improvements of the performance if
single-BIO and prob-BIO are those trained on the biomedical out-of-domain data was as large as the Penn Treebank. Be-
domain (i.e., the Penn BioIE corpus and the GENIA corpus). cause their evaluation was done with POS tagged sentences,
Compared to the results with gold POS tags, the use of auto- whether this is still the case for the task of parsing raw texts
matic taggers trained on a different domain degraded the per- is an open question
formance by about 4 to 5 points in the F1 measure. This drop
was recovered using well-tuned taggers, and remarkably, the 6Conclusion
results of prob-BIO were only 0.36 points lower than the up-
per bound.5 The difference in processing time was, again, not In this paper, we demonstrated that the accuracy of parsers
                                                      could be improved by allowing POS taggers to output mul-
  3Though this ﬁgure alone seems not to be signiﬁcant, we will tiple answers for some words. When the performance of the
show that the results of the same experiment using the test set have parser was evaluated in terms of precision/recall of predicate
signiﬁcant improvements.                              argument relations, the performance of the parser with the
  4The improvement of prob over single is signiﬁcant with p<
  −4
10  according to stratiﬁed shufﬂing tests [Cohen, 1995]. niﬁcant with p<10−4 according to stratiﬁed shufﬂing tests. The
  5The improvement, in terms of recall, of prob over single is sig- improvement of precision is not signiﬁcant.

                                                IJCAI-07
                                                  1787