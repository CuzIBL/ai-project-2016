                                Multiple Agents Moving Target Search 

              Mark Goldenberg, Alexander Kovarsky, Xiaomeng Wu, Jonathan Schaeffer 
                            Department of Computing Science, University of Alberta, 
                                       Edmonton, Alberta, Canada T6G 2E8 
                            {goldenbe,kovarsky,xiaomeng,jonathan}@cs.ualberta.ca 


                        Abstract                               are allowed to communicate with any agent that they can see. 
                                                               As the target flees, it may become obscured from sight. 
     Traditional single-agent search algorithms usually          The agent's knowledge of the locations of the target and 
     make simplifying assumptions (single search agent,        other agents may be fuzzy (since some of them may be hid•
     stationary target, complete knowledge of the state,       den from sight). Given whatever knowledge of the target and 
     and sufficient time). There are algorithms for re•        other agents is available, an agent must decide how to pursue 
     laxing one or two of these constraints; in this pa•       the target. The target's possible locations provide informa•
     per we want to relax all four. The application do•        tion of where to search; the other agents' possible locations 
     main is to have multiple search agents cooperate          provide information that can be used to coordinate the search 
     to pursue and capture a moving target. Agents are         effort. The challenge is to have the agents act autonomously 
     allowed to communicate with each other. For solv•         to catch the target as quickly as possible. 
     ing Multiple Agents Moving Target (MAMT) ap•                This paper makes the following contributions: MAMT -
     plications, we present a framework for specifying a       a challenging application domain for exploring issues related 
     family of suitable search algorithms. This paper in•      to Multiple Agents pursuing a Moving Target, a framework 
     vestigates several effective approaches for solving       for expressing real-time search algorithms for the MAMT 
     problem instances in this domain.                         domain, and several solutions that allow an agent to act au•
                                                               tonomously (using information about the possible positions 
1 Introduction                                                 of the target and other agents in the decision-making process). 
                                                                 More details are available at www. cs . ualberta. ca/ 
In the 2002 Steven Spielberg movie Minority Report, John       ~jonathan/Papers/ai.2003.html. 
Anderton (played by Tom Cruise) is on the run. In one se•
quence, Anderton is hiding in a building, and his pursuers     2 Literature 
unleash a team of mini-robots to flush him out. The robot 
                                                               There are a family of A* algorithms that relax solution opti•
team separates, each covering a different part of the building. 
                                                               mally by requiring the agent to make the best decision pos•
Anderton, realizing the danger, stops fleeing and comes up 
                                                               sible given limited search resources (e.g., time) [Korf, 1990]. 
with a unique solution - he submerges himself in a bathtub 
                                                               The Minimin Lookahead Search algorithm uses a fixed-depth 
of water so as to avoid the robotic detectors. Sadly, he can 
                                                               search (d moves), keeping track of the moves that lead to the 
only hold his breath for so long, before he has to emerge and 
                                                               (heuristically) best d-move outcome. Real-Time A* (RTA*) 
is found by the robots. 
                                                               is an A* variant that uses the results produced by the min•
  The classic algorithms (such as A*) are effective for solv•  imin lookahead search as heuristic values in order to guide the 
ing search problems that satisfy the properties: one search    search towards achieving the goal [Korf, 1990]. Moving Tar•
agent, the agent has perfect information about the environ•    get Search (MTS) is an LRTA* variant that allows a moving 
ment, the environment and goal state do not change, and        target [Ishida and Korf, 1995]. An assumption in most MTS 
enough time is given to make an optimal decision. Relax•       papers is that the target moves slower than the agent. With•
ing even one of the assumptions gives rise to new algorithms   out this requirement, the target can stay ahead of the agent 
(e.g., moving target search [Ishida and Korf, 1995]), real-time and possibly elude capture. There are many real-time search 
search [Korf, 1990], and D* [Stentz, 1995]). Many real-        variants (e.g., LPA* [Koenig and Likhachev, 2003]). For the 
world problems do not satisfy all of these properties. In this most part, these are orthogonal to our work. The difficult part 
paper we use an application domain that breaks all of them.    for an agent is deciding on the search goal; once achieved, 
  Consider the task of multiple agents having to pursue and    then any of several different search algorithms can be used. 
capture a moving target; for example a squad of policemen 
                                                                 Having multiple agents participating in a search is a topic 
chasing a villain. We want to design a test environment that 
                                                               of recent interest. RoboCup is an example, but that work 
is as realistic as possible. We assume a grid with obstacles. 
                                                               has more limited scope, and agents (players) generally have 
Agents can only "see" what is directly visible to them. Agents 
                                                               global knowledge. 


1536                                                                                                   POSTER PAPERS                                                                Region belief set. This set has the same update as the all-
                                                               scenarios belief set. After the search goal is chosen, the agent 
                                                               commits to only consider beliefs that arc connected to the goal 
                                                               location. Single-location: The agent maintains a single be•
                                                               lief (one grid square). The belief is updated by choosing a 
                                                               random direction and moving the belief in that direction until 
                                                               an obstacle is encountered or new target information is avail•
                                                               able. 
                                                                 We use a simple greedy algorithm to approximate the four 
                                                               "corners" of the variable shaped belief set. This subset of the 
                                                               belief set is called the filtered belief set. 
                                                               4.2 Goal Selection 
         Figure 1: Framework for MAMT solutions 
                                                               Each agent selects a goal from their filtered belief set. Ran•
                                                               domly selecting a location from this set is an obvious control 
3 Problem Description                                          strategy to implement. However, a more intelligent strategy is 
The MAMT domain has the following properties. Agents           needed one that considers information about other agents. 
and target: multiple agents pursuing a single moving target.     The difference metric is used to identify a goal that ideally 
Grid: m x n in size, with randomly-placed obstacles. All       is (a) closest to the agent and yet (b) farthest from the clos•
agents and the target have complete knowledge of the grid      est other agent. For each location in the filtered belief set, 
topology. Moving: all moves are horizontal or vertical and     we compute two metrics: the distance from the agent to the 
arc made simultaneously. Starting position: The target al•     belief, and the minimum distance from the belief to the last 
ways starts in the middle of the grid. The agents are all placed known agent positions. These values can be determined by 
 in the lower left corner of the grid. The target is visible to search (expensive) or by heuristics (inaccurate). 
at least one agent. Vision: can "see" anything that is in an     For each location in the filtered belief set, the agent com•
unobstructed direct line. Communication: between moves,        putes the difference of the above two values, and chooses the 
agents communicate with any agent that is visible to them.     one with the minimum difference. The idea is that the agent 
The agents exchange information as to where they believe       should assist the other agents by covering the possible es•
the target and other agents are located. Objective: catch the  capes of the target that are hard for the other agents to reach. 
target in the fewest number of moves. 
                                                               4.3 Search 
4 Multiple Agent Moving Target Search                          Given a goal, each agent performs a search to find a "best" 
The intent of this work is not to build a new search algorithm. move that progresses towards that goal. Since this has to be a 
Rather, we want to plug standard search algorithms into a      real-time algorithm, the search algorithm is allocated a fixed 
framework that, given a goal selection, will find the "best"   number of search nodes (approximating a fixed amount of 
way to reach the objective.                                    time per decision). The manhattan distance is used as the 
   When an agent does not know the exact position of the tar•  search evaluation function. 
get (from vision or communication), it must maintain a belief    We experimented with two search algorithms. Single-
set of where the target might be. The belief set can take into agent minimin search [Korf, 1990]: Here the agent uses 
account the topology of the grid, knowledge of the opponent,   single-agent search to find the shortest path to the goal lo•
and the time since the last known target location. As the time cation. The search has the effect of selecting the move that 
increases since the last sighting, the knowledge of where the  tries to chase the target. Adversarial search: The search can 
target is gets fuzzier. An agent should choose its search area alternate between moves for the agent (move towards the tar•
based on its beliefs about the target and other agents.        get) and target (move away from the agent). This becomes 
   Figure 1 shows the four-step method that is the frame-      an alpha-beta search, where the agent tries to minimize the 
work used for specifying our solution algorithms. There is     minimax value of the search (the distance from the target). 
no "right" way of solving any of these steps. In the following 4.4 Comments 
we detail several algorithm alternatives. 
                                                               For non-trivial belief sets, as long as information is known 
4.1 Belief Set                                                 about other agent's positions the agents will separate to avoid 
Whenever an agent knows the exact location of the target,      redundancy in the search. Each agent wanders about the grid 
that agent's belief set contains only one location, otherwise  trying to maximize their coverage, as a function of what they 
it can grow. Since this is a real-time search application, and know (about the target and other agents). In many cases (es•
real agents have limited memory, the belief set size is limited. pecially for large mazes) an agent may go a long time without 
Before a move, each agent sends its belief set information     getting an update on the target's position, effectively negating 
about the target and other agents to any agent it can see (who the effectiveness of the belief set. 
may, in turn forward it to agents that they see).              5 Experiments 
   We implemented three strategies for maintaining the belief 
set. All-scenarios. Expand the current belief set to include   There are a variety of target strategies that can be investi•
all possible locations that can be reached in one more move.   gated. The strategy used is a weighted combination of four 


POSTER PAPERS                                                                                                       1537      Figure 2: Comparing solutions Figure 3: Varying size and # of agents Figure 4: Varying the obstacle density 

 sub-strategies: distance, mobility, visibility, and random. The hence, usually yields a lesser-quality solution. 
 maximum scoring move is selected. The weights were hand-        In cases where the target eluded capture, a familiar pattern 
 tuned based on the perceived realism of the target's behavior. emerged. The target would stay hidden in a small area, and 
   Solutions to MAMT instances were tested using grids of      the agent's knowledge of where the target was became obso•
 sizes from 10 x 10 to 70 x 70. Grids had obstacles randomly   lete and effectively useless. The agents would independently 
 placed, occupying 10% to 30%, in increments of 5%, of the     wander about, hoping to find the target. In the real world, 
 space. The experiments used 1 to 10 agents. Each pursuer      if such a scenario arises, the agents should wait until more 
 was allocated 50,000 search nodes to make its decision. If    help arrives and then begin the search anew, going through 
 search was used to compute the difference metric, then half   the entire grid systematically. 
 the search nodes were allocated to this task, and the other half Figure 3 shows that more agents are better than fewer (us•
 to move selection. Agents were allowed a maximum belief set   ing All_D(MD)_SAS). Note that as the number of agents in•
 size of 100. An experiment ended when one of the pursuers     creases, the number of nodes per agent per search in the goal 
 caught the target, or when a maximum number of moves was      selection gets smaller (recall the total search size is limited). 
 reached. For an n x n grid, the maximum was set to 15 x n.    Even so, adding more agents is beneficial despite less re•
 With few exceptions, the target was caught in less than 8 x n sources available. 
 moves, or not at all.                                           Figure 4 shows that as the mazes become more congested 
   Figure 2 compares several different solutions. The belief   with obstacles, it gets harder for the agents to find the tar•
 set was maintained using one of: all-scenarios (ALL), region, get. Essentially, a higher percent of obstacles gives the target 
 and single-location (Single). Except for a single-location be• more opportunities to hide. However, at 30% of obstacles, 
 lief set, the choice of goal was done using the difference met• our target starts having problems with avoiding the dead ends 
 ric based on either manhattan distance (no search) - D(MD),   and is sometimes caught more easily. 
 single-agent search - D(SAS), or alpha-beta - D(AB). Hav•       Several control experiments were also done, and they gave 
 ing chosen a goal, single-agent search (SAS) or alpha-beta    predictable results. Simplistic targets (e.g., random, or avoid) 
 (AB) was used to select the best move. The graph shows the    were much easier to catch. 
 percentage of problems where the target was caught (100 tri•  6 Future Work and Conclusions 
 als with different random seeds per data point) as a function 
                                                               This problem domain is rich in possibilities, and can be ex•
 of the maze size. 
                                                               tended to increase the "realism" of the simulations. Some ex•
   The control experiment is randomly selecting a goal 
                                                               amples include: multiple moving targets, more realistic com•
 (single-location), and using single-agent search (SAS) to de•
                                                               munication, creating "human-like" target behavior, and op•
 cide on the move choice. Not surprisingly, this gets poor per•
                                                               ponent modeling. The framework used of this research has 
 formance. Region was expected to perform quite well, but 
                                                               many opportunities for interesting extensions. 
 instead its results were mixed (not shown). 
   A11-D(MD)-SAS and A11_D(SAS)_SAS performed best in          7 Acknowledgments 
 all our experiments, with a preference for the manhattan dif• This research was supported by NSERC, iCORE, and IRIS. 
 ference metric. That the difference heuristic gets the best per• References 
 formance is gratifying, since it is better informed by using be•
 liefs about the other agents. Using a simple heuristic appears [Ishida and Korf, 1995] T. Ishida and R. Korf. Moving tar•
to be as good as or better than using search for determining     get search: A real-time search for changing goals. IEEE 
the "best" search goal. The simplest way of computing the        PAMI, 17(6):609-619, 1995. 
difference metric - using static manhattan distance instead of [Koenig and Likhachev, 2003] S. Koenig and M. Likhachev. 
more accurate search - also leads to shorter solution lengths    D* lite. AAAl, pages 476-483, 2003. 
(up to 10% on average). This shows that it is more beneficial  [Korf, 1990] R. Korf. Real-time heuristic search. Artificial 
to invest search effort in move selection than in goal selection. Intelligence, 42(2-3):189-211, 1990. 
   Alpha-beta is out-performed by single-agent search. Since 
                                                               [Stentz, 1995] A. Stentz. The focussed D* algorithm for 
alpha-beta takes into account the target's moves, it cannot 
                                                                 real-time replanning. In IJCAI, pages 1652-1659, 1995. 
reach the search depths that single-agent search can and, 


 1538                                                                                                  POSTER PAPERS 