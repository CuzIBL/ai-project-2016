             Reconstructing an Agent’s Epistemic State from Observations

                      Richard Booth                               Alexander Nittka
                   Macquarie University                         University of Leipzig
                    Dept. of Computing                       Dept. of Computer Science
               Sydney NSW 2109 Australia                       Leipzig 04109 Germany
                  rbooth@ics.mq.edu.au                    nittka@informatik.uni-leipzig.de

                    Abstract                          One area in which such questions might arise is in human-
                                                      machine dialogues [del Cerro et al., 1998], where the φi cor-
    We look at the problem in belief revision of trying respond to inputs given to A – the user – by a machine, and
    to make inferences about what an agent believed
                                                      the θi are the user’s responses. Here, it can be useful for the
    – or will believe – at a given moment, based on   machine to keep a model of the evolution of a user’s beliefs
    an observation of how the agent has responded to  during a dialogue. Another setting where reconstruction of
    some sequence of previous belief revision inputs  a prior belief set is needed is in law cases, where guilt and
    over time. We adopt a “reverse engineering” ap-   innocence often depend on who knew what, when. Inquiry is
    proach to this problem. Assuming a framework      a possible method of approaching this type of problem. The
    for iterated belief revision which is based on se- above mentioned sequences can be seen as the information
    quences, we construct a model of the agent that   the inquiry yields and which is now used for drawing conclu-
    “best explains” the observation. Further consider- sions. Our aim in this paper is to answer these questions.
    ations on this best-explaining model then allow in-
    ferences about the agent’s epistemic behaviour to   Our strategy for dealing with these questions will be to
    be made. We also provide an algorithm which com-  adopt a “reverse engineering” approach – constructing a
    putes this best explanation.                      model of the agent. (Similar approaches have already been
                                                      tried in the context of trying to infer an agent’s goals from
                                                      observable actions, e.g., [Brafman and Tennenholtz, 1997;
1  Introduction                                       Lang, 2004].) Having no access to the agent’s internals, we
The problem of belief revision, i.e., of how an agent should assume a belief revision framework A uses for determining
modify its beliefs about the world given some new informa- its beliefs and for incorporating new information, and con-
tion which possibly contradicts its current beliefs, is by now a struct a model of A that explains the observation about it.
well-established research area in AI [Gardenfors,¨ 1988]. Tra- By considering this model, we will then be able to make ex-
ditionally, the work in this area is done from the agent’s per- tra inferences or predictions about A’s epistemic behaviour.
spective, being usually pre-occupied with constructing actual Of course this raises the problem of which belief revision
revision operators which the agent might use and with ratio- framework to choose. Such a framework will obviously
nality postulates which constrain how these operators should need to support iterated revision [Darwiche and Pearl, 1997;
behave. In this paper we change viewpoint and instead cast Lehmann, 1995b; Nayak et al., 2003], and preferably also
ourselves in the role of an observer of the agent. Imagine non-prioritised revision [Hansson et al., 2001; Makinson,
the following scenario. Suppose we are given some sequence 1997], i.e., revision in which new inputs are allowed to be
(φ1, . . . , φn) of revision inputs which a particular agent, here- rejected. In this paper, we restrict the investigation to one
after A, has received over a certain length of time and suppose such framework that has been studied in [Booth, 2005]. The
we are also given a sequence (θ1, . . . , θn) with the interpre- idea behind it is that an agent’s epistemic state is made up of
                      th
tation that following the i input φi, A believed (at least) two components: (i) a sequence ρ of sentences representing
θ . Throughout the paper, we make the assumptions that A the sequence of revision inputs the agent has received thus
 i                                                                            N
received no input between φ1 and φn other than those listed, far, and (ii) a single sentence standing for the agent’s set of
and that the θi are correct (but possibly partial) descriptions core beliefs, which intuitively are those beliefs of the agent it
of A’s beliefs after each input. A couple of questions now considers “untouchable”. The agent’s full set of beliefs in the
suggest themselves:                                   state [ρ, N] is then determined by a particular calculation on
                                                      ρ and N, while new revision inputs are incorporated by sim-
  • What will A believe following a further revision input ply appending them to the end of ρ. Note that our choice of
    φn+1?                                             this framework does not imply that others are less worthy of
  • What did A believe immediately before it received the investigation. The challenge now becomes to ﬁnd that partic-
    ﬁrst input φ1? What, apart from θi did A believe after ular model of this form which best explains the observation
        th
    the i input φi?                                   o = h(φ1, . . . , φn), (θ1, . . . , θn)i we have made of A.  The plan of the paper is as follows. In Sect. 2 we describe that N is always believed, and that Bel([ρ, N]) is inconsistent
in more detail the model of epistemic state we will be assum- if and only if N is inconsistent.
ing. This will enable us to pose more precisely the problem Example 2.1. Consider N = ¬p and ρ = (q, q → p).
we want to solve. We will see that the problem essentially re- Bel([ρ, N]) = f (q, q → p, ¬p). In order to determine
duces to trying to guess what A’s initial epistemic state [ρ, N] f (q, q → p, ¬p) we need to know if q is consistent with
(i.e., before it received φ1) was. In Sect. 3, inspired by work f (q → p, ¬p). As f (¬p) = ¬p and q → p is con-
done on reasoning with conditional beliefs, we propose a way sistent with ¬p, f (q → p, ¬p) = (q → p) ∧ ¬p ≡ ¬q ∧ ¬p.
of ﬁnding the best initial sequence – or preﬁx – ρ(N) for any So q is inconsistent with f (q → p, ¬p). Consequently we
given ﬁxed N. Then, in Sect. 4 we focus on ﬁnding the best get f (q, q → p, ¬p) = f (q → p, ¬p) and Bel([ρ, N]) =
N. This will amount to equating best with logically weakest. f (q → p, ¬p) ≡ ¬q ∧ ¬p.
The epistemic state [ρ(N), N] obtained by combining our an- An agent incorporates a new revision input λ into its epis-
swers will be our proposed best explanation for o, which we temic state [ρ, N] by simply appending λ to ρ, i.e., the agent’s
will call the rational explanation. In Sect. 5 we present an revision function ∗ is speciﬁed by setting, for every λ ∈ L,
algorithm which constructs the rational explanation for any           [ρ, N] ∗ λ = [ρ · λ, N].
given o, before giving some examples to show the type of in- Given this, we see that a new input λ will not always be be-
ferences this explanation leads to in Sect. 6. In Sect. 7 we lieved in the new state. Indeed (when N is consistent) it will
brieﬂy mention a related piece of work by Dupin de Saint- be so only if it is consistent with N. If it contradicts N then
Cyr and Lang, before concluding and giving some pointers it will not be accepted, and in fact in this case the agent’s be-
for future research.                                  lief set will remain unchanged (c.f. screened revision [Makin-
                                                                              N
2  Modelling the Agent                                son, 1997]). Note also that remains unaffected by a revi-
                                                      sion input, i.e., ∗ is a core-invariant revision operator [Booth,
                        N
We assume sentences φi, θi, , etc. are elements of some 2005].1 Core beliefs are needed to ensure that revision inputs
ﬁnitely-generated propositional language L. In our examples, can be rejected. If they were not allowed, which corresponds
p, q, r denote distinct propositional variables. The classical to demanding N = > in the above deﬁnitions, any consistent
logical entailment relation between sentences is denoted by revision input would belong to the agent’s beliefs.
`, while ≡ denotes classical logical equivalence. Wherever As is shown in [Booth, 2005], the above revision method
we use a sentence to describe a belief set the intention is that satisﬁes several natural properties. In particular, it stays
it represents all its logical consequences. The set of all pos- largely faithful to the AGM postulates [Gardenfors,¨ 1988]
sible observations o = hι, τi which can be made of A, where (leaving aside the “success” postulate, which forces all new
ι = (φ1, . . . , φn) and τ = (θ1, . . . , θn) are two ﬁnite se- inputs to be accepted), and satisﬁes slight,“non-prioritised”
quences of sentences of the same length, is denoted by O. The variants of several postulates for iterated revision which
operation · on sequences denotes sequence concatenation. have been proposed, including those of [Darwiche and Pearl,
  As indicated in the introduction, we follow [Booth, 2005] 1997]. One characteristic property of this method is the fol-
by assuming that, at any given moment in time, an agent’s lowing variant of the rule “Recalcitrance” from [Nayak et al.,
epistemic state is represented by a pair [ρ, N]. ([Konieczny 2003]:
and Perez,´ 2000; Lehmann, 1995b] also use sequences to rep-
                                                          If N 6` (λ2 → ¬λ1) then Bel([ρ, N] ∗ λ1 ∗ λ2) ` λ1
resent epistemic states, but without core beliefs). In order to
                                                      This entails if the agent accepts an input λ1, then it does so
fully specify the agent’s epistemic processes, we also need to wholeheartedly, in that the only way it can be dislodged from
formally specify (i) how the agent determines its set of beliefs
      N                     N                         the belief set by a succeeding input λ2 is if that input contra-
Bel([ρ, ]) in any given state [ρ, ], and (ii) how it incorpo- dicts it given the core beliefs N.
rates new revision inputs into its epistemic state. Turning ﬁrst
                          N                             Returning to our agent A from the introduction, from
to (i), we can describe Bel([ρ, ]) neatly with the help of a now on we assume A’s epistemic state is always of the
function f, which takes as argument a non-empty sequence form [ρ, N], and that A determines its belief set and
σ = (αm, . . . , α1) of sentences, and returns a sentence. f is incorporates new inputs into its epistemic state as de-
deﬁned by induction on the length m of σ: if m = 1 then scribed above. Then, suppose we make the observation
f (σ) = α1. If m > 1 then
                                                      o  =  h(φ1, . . . , φn), (θ1, . . . , θn)i about A. Then after
                                                                  th
            ϕ = α   ∧ f (α   , . . . , α ) if ϕ 6` ⊥  receiving the i input φi, A’s epistemic state must be
   f (σ) =       m       m−1      1                                  N                                N
            f (αm−1, . . . , α1)      otherwise       [ρ · (φ1, . . . , φi), ] and its belief set f (ρ · (φ1, . . . , φi) · ),
                                                      where [ρ, N] is A’s unknown initial (i.e., before φ1) epistemic
In other words f (σ) is determined by ﬁrst taking α1 and state. Observation o now amounts to the following:
then going backwards through σ, adding each sentence as
                                                             f (ρ · (φ1, . . . , φi) · N) ` θi i = 1, . . . , n (1)
we go, provided that sentence is consistent with what has We make the following deﬁnitions:
been collected so far (cf. the “linear base-revision opera-
                                                      Deﬁnition 2.2. Let o = h(φ , . . . , φ ), (θ , . . . , θ )i ∈ O.
tion” of [Nebel, 1994] and the “basic memory operator” of                      1     n    1      n
                                                      Then [ρ, N] explains o (or is an explanation for o) iff (1) above
[Konieczny and Perez,´ 2000].) The belief set associated to the
                                                      holds. We say N is an o-acceptable core iff [ρ, N] explains o
state [ρ, N] is then given by Bel([ρ, N]) = f (ρ · N). Hence
                                                      for some ρ.
when calculating its beliefs from the sentences appearing in
its epistemic state, an agent gives highest priority to N. Af- 1In fact the model of [Booth, 2005] allows the core itself to be
ter that, it prioritises more recent information received. Note revisable. We do not explore this possibility here.Example 2.3.  (i) [ρ, N] =  [(p  →    q), r] explains C of such beliefs has been well-studied and several solutions
h(p, q), (q, r)i because f (p → q, p, r) ≡ p ∧ q ∧ r ` q and have been proposed, e.g., [Geffner and Pearl, 1992; Lehmann,
f (p → q, p, q, r) ≡ p ∧ q ∧ r ` r.                   1995a]. One particularly elegant and well-motivated solution
(ii) [(p → q), >] does not explain h(p, q), (q, r)i because is to take the rational closure of C [Lehmann and Magidor,
f (p → q, p, q, >) ≡ p ∧ q 6` r.                      1992]. Furthermore, as is shown in, e.g., [Freund, 2004], this
  If we had some explanation [ρ, N] for o then we would be construction is amenable to a relatively simple representation
able to answer the questions in the introduction: following a as a sequence of sentences! Our idea is essentially to take
                                                          N
new input φn+1 A will believe f (ρ · (φ1, . . . , φn, φn+1) · N), ρ(o, ) to be this sequence corresponding to the rational clo-
before receiving the ﬁrst input A believes f (ρ · N), and the sure of CN(o). First let us describe the general construction.
beliefs after the ith input are f (ρ · (φ , . . . , φ ) · N).
                               1     i                3.1  The rational closure of a set of conditionals
  Note for any o ∈ O there always exists some explanation
[ρ, N] for o, since the contradiction ⊥ is an o-acceptable core Given a set of conditionals C = {λi ⇒ χi | i = 1, . . . , l}
using any ρ. But this would be a most unsatisfactory expla- we denote by C˜ the set of material counterparts of all the
nation, since it means we just infer A believes everything at conditionals in C, i.e., C˜ = {λi → χi | i = 1, . . . , l}. Then a
every step.                                           sentence ν is exceptional for C iff C˜ ` ¬ν, and a conditional
  Our job now is to choose, from the space of possible ex- ν ⇒ µ is exceptional for C iff its antecedent ν is. To ﬁnd the
planations for o, the best one. As a guideline, we consider (sequence corresponding to the) rational closure ρR(C) of C,
an explanation good if it only makes necessary (or minimal) we ﬁrst deﬁne a decreasing sequence of sets of conditionals
assumptions about what A believes. But how do we ﬁnd  C0 ⊇ C1 ⊇ · · · ⊇ Cm by setting (i) C0 = C, (ii) Ci+1 equals
this best one? Our strategy is to split the problem into two the set of conditionals in C which are exceptional for C , and
                   N                                                        i                       i
parts, handling ρ and separately. First, (i) given a ﬁxed (iii) m is minimal such that Cm = Cm+1. Then we set
o-acceptable core N, ﬁnd a best sequence ρ(o, N) such that
                                                                        ^  ˜  ^  ˜        ^  ˜
[ρ, N] explains o, then, (ii) ﬁnd a best o-acceptable core N(o). ρR(C) = ( Cm,   Cm−1, . . . , C0).
Our best explanation for o will then be [ρ(o, N(o)), N(o)].        V
                                                      Writing αi for C˜i, the rational closure of C is then the rela-
                                                                                               
3  Finding  ρ                                         tion ⇒R given by λ ⇒R χ iff either αm ` ¬λ or αj ∧λ ` χ
                                                            j                 α  6` ¬λ     α   a · · · a α
Given o = h(φ1, . . . , φn), (θ1, . . . , θn)i, let us assume a ﬁxed where is minimal such that j . Since m 0
core N. To ﬁnd that sequence ρ(o, N) such that [ρ(o, N), N] it easy to check that in fact this second disjunct is equivalent
is the best explanation for o, given N, we will take inspiration to f (αm, . . . , α0, λ) ` χ.
from work done in the area of non-monotonic reasoning on We now make the following deﬁnition:
reasoning with conditional information.               Deﬁnition 3.1. Let o ∈ O and N ∈ L. We call ρR(CN(o))
  Let’s say a pair (λ, χ) of sentences is a conditional be- the rational preﬁx of o with respect to N, and will denote it by
lief in the state [ρ, N] iff χ would be believed after revising ρR(o, N).
[ρ, N] by λ, i.e., Bel([ρ, N] ∗ λ) ` χ. In this case we will                               N
                2                                     Example 3.2. Let o = h(p, q), (r, ¬p)i and = ¬p. Then
write λ ⇒[ρ,N] χ. This relation plays an important role,
because it turns out A’s beliefs following any sequence of CN(o)  =   {f (p, ¬p) ⇒ r, f (p, q, ¬p) ⇒ ¬p}
revision inputs starting from [ρ, N] is determined entirely by    =   {¬p ⇒  r, (q ∧ ¬p) ⇒ ¬p}.
                                  N
the set ⇒[ρ,N] of conditional beliefs in [ρ, ]. This is because, Since neither of the individual conditionals are exceptional
for any sequence of revision inputs φ , . . . , φ , our revision
                               1      m               for CN(o) we get C =  CN(o) and C =  ∅. Clearly then
method satisﬁes                                                        0              1
                                                      also C = ∅ = C  so we obtain ρ (o, N) = (V ∅, V C˜N(o)).
      N                        N               N           2        1             R
Bel([ρ, ]∗φ1 ∗· · ·∗φm) = Bel([ρ, ]∗f(φ1, . . . , φm, )). Rewriting the sequence using logically equivalent sentences
Thus, as far as their effects on the belief set go, a sequence of we get ρR(o, N) = (>, ¬p → r).
revision inputs starting from [ρ, N] can always be reduced to a Now, an interesting thing to note about the rational pre-
single input. (But note the set of conditional beliefs ⇒[ρ,N]∗λ ﬁx construction is that it actually goes through indepen-
in the state [ρ, N] ∗ λ following revision by λ will generally dently of whether N is o-acceptable. In fact a useful side-
not be the same as ⇒[ρ,N].)                           effect of the construction is that it actually reveals whether
  All this means observation o may be translated into a par- N is o-acceptable. Given we have constructed ρR(o, N) =
tial description of the set of conditional beliefs that A has in (αm, . . . , α0), all we have to do is to look at sentence αm
its initial epistemic state:                          and check if it is a tautology:
                                                      Proposition 3.3. Let o ∈ O and N ∈ L, and let ρ (o, N) =
    CN(o) = {f (φ1, . . . , φi, N) ⇒ θi | i = 1, . . . , n}.                                    R
                                                      (α  , . . . , α ) be the rational preﬁx of o w.r.t. N. Then
Clearly, if we had access to the complete set of A’s condi- m   0
                                                      (i) if αm ≡ > then [ρR(o, N), N] is an explanation for o.
tional beliefs in its initial state, this would give another way to (ii) if α 6≡ > then N is not an o-acceptable core.
answer the questions of the introduction. Now, the problem of m
determining which conditional beliefs follow from a given set Thus this proposition gives us a necessary and sufﬁcient
                                                      condition for N to be an o-acceptable core. This will be used
  2
   The relation ⇒[ρ,N] almost satisﬁes all the rules of a rational in- in the algorithm of Sect. 5.
ference relation [Lehmann and Magidor, 1992]. More precisely the In Example 3.2 ρR(o, N) = (>, ¬p → r) was cal-
                        0       N
modiﬁed version does, viz., λ ⇒[ρ,N] χ iff [ ` ¬λ or λ ⇒[ρ,N] χ]. culated. The above proposition implies [(>, ¬p → r), ¬p]is an explanation for o = h ( p , q) , ( r , ¬p ) i. This is 4 Minimising N
veriﬁed by f (>, ¬p → r, p, ¬p) ≡ ¬p ∧ r  `  r and    As argued earlier, core beliefs are needed, but at the same
f (>, ¬p → r, p, q, ¬p) ≡ ¬p ∧ q ∧ r ` ¬p.            time we try to minimise the assumptions about the agent’s
                                                      beliefs. This includes minimising N. The ﬁrst idea would
3.2  Justiﬁcation for using the rational preﬁx        be to simply take the disjunction of all possible o-acceptable
                                N
In the rest of this section we assume to be some ﬁxed o- cores, i.e., to take N∨(o), deﬁned by
                                 N   N                               _
acceptable core. As we just saw, [ρR(o, ), ] then provides   N  (o) ≡  {N | N is an o-acceptable core}.
an explanation for o given this N. In this section we want to ∨
                                                              N
show in precisely what sense it could be regarded as a best But is ∨(o) itself o-acceptable? Thankfully the answer is
explanation given N. Let Σ = {σ | [σ, N] explains o}. yes, a result which follows (in our ﬁnite setting) from the fol-
  One way to compare sequences in Σ is by focusing on lowing proposition which says that the family of o-acceptable
the trace of belief sets they (in combination with N) induce cores is closed under disjunctions.
through o, i.e., for each σ ∈ Σ we can consider the sequence Proposition 4.1. If N1 and N2 are o-acceptable then so is
    σ    σ        σ           σ                       N    N
(Bel0 , Bel1 , . . . , Beln), where Beli is deﬁned to be the be- 1 ∨ 2.
            th
liefs after the i input in o (under the explanation [σ, N]). In So as a corollary N∨(o) does indeed satisfy:
             σ                    N         σ
other words Beli = f (σ · (φ1, . . . , φi) · ). (So Bel0 gives (Acceptability) N(o) is an o-acceptable core
the initial belief set.)
                                                      What other properties does N (o) satisfy? Clearly, N (o)
                  N          N                                                  ∨                    ∨
Example 3.4. Let o, and ρR(o,  ) be as in Example 3.2. will always be consistent provided at least one consistent o-
Then the belief trace is (¬p ∧ r, ¬p ∧ r, ¬p ∧ q ∧ r). acceptable core exists:
  The idea would then be to deﬁne a preference relation 1 (Consistency) If N(o) ≡ ⊥ then N0 ≡ ⊥ for every
over the sequences in Σ (with more preferred sequences cor-          o-acceptable core N0.
responding to those “lower” in the ordering) via some prefer-
ence relation over their set of associated belief traces. Given Acceptability and Consistency would appear to be abso-
                                                      lute rock-bottom properties which we would expect of any
any two possible belief traces (β0, . . . , βn) and (γ0, . . . , γn),
let us write (β , . . . , β ) ≤ (γ , . . . , γ ) iff, for all i = method for ﬁnding a good o-acceptable core. However for
            0      n   lex   0     n                  N  we can say more. Given two observations o = hι, τi and
0, . . . , n, β ≡ γ for all j < i implies γ ` β . Then we ∨
          j    j                   i    i             o0 = hι0, τ 0i, let us denote by o · o0 the concatenation of o and
deﬁne, for any ρ, σ ∈ Σ:                               0        0      0     0                     0
                                                      o , i.e., o · o = hι · ι , τ · τ i. We shall use o vright o to de-
                ρ        ρ          σ        σ                 0                 0      00
   ρ 1 σ iﬀ (Bel0, . . . , Beln) ≤lex (Bel0 , . . . , Beln). note that o right extends o, i.e., o = o·o for some (possibly
                                                              00               0         0
(  is a pre-order (i.e., reﬂexive and transitive) on Σ.) Thus, empty) o ∈ O, and o vleft o to denote o left extends o, i.e.,
  1                                                    0   00                          00
given two sequences in Σ, we prefer that one which leads to A o = o · o for some (possibly empty) o ∈ O.
                                                                                      0           0
having fewer (i.e., weaker) beliefs before any of the inputs φi Proposition 4.2. Suppose o vright o or o vleft o . Then
were received. If the two sequences lead to equivalent beliefs every o0-acceptable core is an o-acceptable core.
at this initial stage, then we prefer that which leads to A hav- As a result of this we see N∨ satisﬁes the following 2 prop-
ing fewer beliefs after φ1 was received. If they lead to equiv- erties, which say extending the observation into the future or
alent beliefs also after this stage, then we prefer that which past leads only to a logically stronger core being returned.
leads to A having fewer beliefs after φ2 was received, and                         0        0
                                                       (Right Monotony)  If o vright o then N(o ) ` N(o)
so on. Thus, under this ordering, we prefer sequences which (Left Monotony) If o v o0 then N(o0) ` N(o).
induce A to have fewer beliefs, earlier in o. The next result                 left
                                                      Right- and Left Monotony provide ways of expressing that
shows ρR(o, N) is a best element in Σ under this ordering.
                                                      N(o) leads only to safe conclusions that something is a core
                    N
Proposition 3.5. ρR(o, ) 1 σ for all σ ∈ Σ.          belief of A – conclusions that cannot be “defeated” by addi-
  Another way to compare sequences is to look at their con- tional information about A that might come along in the form
sequences for predicting what will happen at the next step of observations prior to, or after o.
after o.                                                We should point out, though, that it is not the case that by
                                                                                        N
ρ 2 σ  iff Bel([σ, N] ∗ φ1 ∗ · · · ∗ φn ∗ λ) `       inserting any observation anywhere in o, ∨ will always lead
                Bel([ρ, N] ∗ φ1 ∗ · · · ∗ φn ∗ λ) for all λ to a logically stronger core. Consider o1 = h(p, q), (p, ¬p)i
                                                      and o2 = h(p, ¬p, q), (p, ¬p, ¬p)i, i.e., h(¬p), (¬p)i was in-
Thus, according to this preference criterion we prefer ρ to σ serted in the middle of o . N (o ) ≡ q → ¬p whereas
if it always leads to fewer beliefs being predicted after the               1    ∨  1
                                                      N∨(o2) ≡ >. So although o2 extends o1 in a sense, the cor-
next revision input. It turns out ρR(o, N) is a most preferred
                                                      responding N∨ is actually weaker. Looking at o1, assuming
element under 2 amongst all minimal elements under 1. as we do that A received no inputs between p and q, the only
Proposition 3.6. For all σ ∈ Σ, if σ 1 ρR(o, N) then way to explain the end belief in ¬p is to ascribe core belief
ρR(o, N) 2 σ.                                        q →  ¬p to A (cf. the “Recalcitrance” rule in Sect. 2). How-
                                                      ever, looking at o2, the information that A received (and ac-
  Thus if we take a lexicographic combination of 1 and
                                                      cepted) intermediate input ¬p is enough to “explain away”
2 (with 1 being considered as more important), ρR(o, N)
emerges overall as a best, most preferred, member of Σ. Hav- this end belief without recourse to core beliefs. Our assump-
ing provided a method for ﬁnding the best explanation [ρ, N] tion that A received no other inputs between φ1 and φn dur-
given N, we now turn our attention to ﬁnding the best N itself. ing an observation o = h(φ1, . . . , φn), (θ1, . . . , θn)i is ratherstrong. It amounts to saying that, during o, we kept our eye In our setting, in which we assumed a ﬁnite propositional lan-
on A the whole time. The above example shows that relax- guage, this means, in the worst case, the process will continue
ing this assumption gives us an extra degree of freedom with until N ≡ ⊥. However in this case it can be shown the ratio-
which to explain o, via the inference of intermediate inputs. nal preﬁx of o w.r.t. ⊥ is just (>), and so the termination
This will be a topic for future work.                 condition will be satisﬁed at the very next step.
  It turns out the above four properties are enough to actually Now, to show the output matches the rational explana-
characterise N∨. In fact, given the ﬁrst two, just one of Right- tion, consider the sequence [ρ0, N0], . . . , [ρk, Nk] of epis-
and Left Monotony is sufﬁcient for this task:         temic states generated by the algorithm. We need to show
Proposition 4.3. Let N : O → L be any function which  Nk  ≡ N∨(o). The direction Nk ` N∨(o) follows from the
returns a sentence given any o ∈ O. Then the following are fact that [ρk, Nk] is an explanation for o and so Nk is an o-
equivalent:                                           acceptable core. The converse N∨(o) ` Nk is proved by
(i) N satisﬁes Acceptability, Consistency and Right Monotony. showing inductively that N∨(o) ` Ni for each i = 0, . . . , k:
(ii) N satisﬁes Acceptability, Consistency and Left Monotony. the case i = 0 clearly holds since N0 ≡ >. The inductive
(iii) N(o) ≡ N∨(o) for all o ∈ O.                     step uses the following property:
  Note that as a corollary to this proposition we get the sur- Lemma 5.2. Let 0 < i ≤ k and suppose ρi−1 =
                                                                                                      N0
prising result that, in the presence of Acceptability and Con- (αm, . . . , α0). Then, for any o-acceptable core ,
                                                        N0  N        N0
sistency, Right- and Left Monotony are in fact equivalent. if ` i−1 then ` αm.
  Combining the ﬁndings of the last two sections, we are now This enables us to prove that, given N∨(o) ` Ni−1, we
ready to announce our candidate for the best explanation for must also have N∨(o) ` Ni. Thus N∨(o) ` Nk as required.
o. By analogy with “rational closure”, we make the following Since obviously ρk is the rational preﬁx of o w.r.t. Nk by
deﬁnition:                                            construction, we have:
Deﬁnition 4.4. Let o ∈ O be an observation. Then we call Proposition 5.3. Given input observation o, the algorithm
[ρR(o, N∨(o)), N∨(o)] the rational explanation for o. outputs the rational explanation for o.
  In Sect. 6 we will give some examples of what we can infer Example 5.4. Let o = h(p, q), (r, ¬p)i. Starting with N=>,
     A
about  under the rational explanation. But how might we in the ﬁrst run C˜0 = {f (p, >) → r, f (p, q, >) → ¬p} =
ﬁnd it in practice? The next section gives an algorithm for {p → r, p ∧ q → ¬p}. Only the second conditional is ex-
just that.
                                                      ceptional, so C˜1 = {p ∧ q → ¬p}. Now the remaining con-
5  Constructing the Rational Explanation              ditional is exceptional for itself, so C˜2 = C˜1. N is updated to
The idea behind the algorithm is as follows. Given an obser- N ≡ p → ¬q because ρ = (p → ¬q, p → (r ∧ ¬q)).
                                                                                 ˜
vation o, we start with the weakest possible core N0 = > and The next calculation yields C0 = {f (p, p → ¬q) → r,
construct the rational preﬁx (αm, . . . , α0)=ρ0 of o w.r.t. N0. f (p, q, p → ¬q) → ¬p} = {p ∧ ¬q → r, q ∧ ¬p → ¬p}.
We then check whether αm is a tautology. If it is then we This time none of the conditionals are exceptional, so C˜1 = ∅.
                       N
know by Prop. 3.3 that [ρ0, 0] is an explanation for o and so As this means α1 = >, no further run is necessary and
we stop and return this as output. If it isn’t then Prop. 3.3 tells the result is ρ = (>, (p ∧ ¬q) → r), N = p → ¬q.
us N0 cannot be o-acceptable. In this case, we modify N0 by [(>, (p∧¬q) → r), p → ¬q] is the rational explanation for o.
conjoining αm to it, i.e., by setting N1 = N0 ∧ αm. Con-
structing the rational preﬁx of o w.r.t. the new core then leads 6 Some Examples
to a different preﬁx, which can be dealt with the same way. In this section we want to give a few simple examples to il-
                                                      lustrate the rational explanation.
Algorithm 1 Calculation of the rational explanation     For o = h(p), (q)i, the rational explanation is [(>, p → q),
Input: observation o                                  >]. So we infer A’s initial belief set is p → q. Indeed to
Output: the rational explanation for o                explain A’s belief in q following receipt of p it is clear A must
  N ⇐  >                                              initially believe at least p → q since p itself does not entail q.
  repeat                                              It seems fair to say we are not justiﬁed in ascribing to A any
    ρ ⇐ ρR(o, N)      {ρ = (αm, . . . , α0)}          initial beliefs beyond this. After A receives p we assume A
    N ⇐  N ∧ αm                                       accepts this input – we have no reason to expect otherwise –
  until αm ≡ >                                        and so has belief set p∧q. If A is given a further input ¬(p∧q)
Return [ρ, N]                                         we predict A will also accept this input, but will hold on to its
                                                      belief in p. The reason being we assume A, having only just
  Before showing that the output of this algorithm matches been told p, now has stronger reasons to believe p than q. If,
the rational explanation, we need to be sure it always termi- instead, A is given further input ¬p we predict its belief set
nates. This is a consequence of the following:        will be just ¬p, i.e., we do not assume A’s belief in q persists.
Lemma 5.1.  Let N and αm be as after the calculation of Essentially the rational explanation assumes the prior input p
ρR(o, N). If αm 6≡ > then N 6≡ N ∧ αm.                must have been responsible for A’s prior belief in q. And with
  This result assures us that if the termination condition of this input now being “overruled” by the succeeding input, A
the algorithm does not hold, the new core will be strictly log- can no longer draw any conclusions about the truth of q.
ically stronger than the previous one. Thus the cores gener- Another illustrative example is o = h(p), (¬p)i, for which
ated by the algorithm become progressively strictly stronger. the rational explanation is [(>), ¬p]. Indeed ¬p must be a