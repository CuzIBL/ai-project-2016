   Manifold-Ranking Based Topic-Focused Multi-Document Summarization

                      Xiaojun Wan, Jianwu Yang and Jianguo Xiao
                       Institute of Computer Science and Technology
                         Peking University, Beijing 100871, China
                  {wanxiaojun, yangjianwu, xiaojianguo}@icst.pku.edu.cn

                 Abstract                    such as Google News1, NewsBlaster2, have been devel-
                                             oped to group news articles into news topics, and then
   Topic-focused multi-document summarization
                                             produce a short summary for each news topic. The users
   aims to produce a summary biased to a given topic
                                             can easily understand the topic they have interest in by
   or user profile. This paper presents a novel extrac-
                                             taking a look at the short summary. Topic-focused
   tive approach based on manifold-ranking of sen-
                                             summary can be used to provide personalized services
   tences to this summarization task. The mani-
                                             for users after the user profiles are created manually or
   fold-ranking process can naturally make full use of automatically. The above news services can be person-
   both the relationships among all the sentences in
                                             alized by collecting users’ interests, and both the re-
   the documents and the relationships between the
                                             trieved related news articles and the news summary bi-
   given topic and the sentences. The ranking score is
                                             ased to the user profile are delivered to the specified
   obtained for each sentence in the manifold-ranking
                                             user. Other examples include Question/Answer systems,
   process to denote the biased information richness
                                             where a question-focused summary is usually required
   of the sentence. Then the greedy algorithm is em- to answer the information need in the issued question.
   ployed to impose diversity penalty on each sen-
                                               The  challenges for topic-focused multi-document
   tence. The summary is produced by choosing the
                                             summarization are as follows: the first one is a common
   sentences with both high biased information rich-
                                             problem for general multi-document summarization, that
   ness and high information novelty. Experiments on
                                             the information stored in different documents inevitably
   DUC2003 and DUC2005 are performed and the
                                             overlaps with each other, and hence we need effective
   ROUGE evaluation results show that the proposed summarization methods to merge information stored in
   approach can significantly outperform existing ap-
                                             different documents, and if possible, contrast their dif-
   proaches of the top performing systems in DUC
                                             ferences; the second one is a particular challenge for
   tasks and baseline approaches.
                                             topic-focused multi-document summarization that the in-
                                             formation in the summary must be biased to the given
1  Introduction                              topic, so we need effective summarization methods to
Multi-document summarization aims to produce a take into account this topic-biased characteristic during
summary delivering the majority of information content the summarization process. In brief, a good topic-focused
from a set of documents about an explicit or implicit summary is expected to preserve the information con-
main topic. Topic-focused multi-document summariza- tained in the documents as much as possible, and at the
tion is a particular kind of multi-document summariza- same time keep the information as novel as possible, and
tion. Given a specified topic description (i.e. user profile, moreover, the information must be biased to the given
user query), topic-focused multi-document summariza- topic. In recent years, a series of workshops and confer-
                                                                                       3
tion (i.e. query-based multi-document summarization) is ences on automatic text summarization (e.g. NTCIR ,
to create from the documents a summary which either DUC4), special topic sessions in ACL, COLING, and
answers the need for information expressed in the topic SIGIR have advanced the technology and produced a
or explains the topic.                       couple of experimental online systems.
  Automatic multi-document summarization has drawn In this study, we propose a novel extractive approach
much attention in recent years and it exhibits the practi- based on manifold-ranking [Zhou et al., 2003a; Zhou et
cability in document management and search systems. al., 2003b] of sentences to topic-focused
Multi-document summary can be used to concisely de- multi-document summarization. The proposed approach
scribe the information contained in a cluster of docu-
ments and facilitate the users to understand the docu- 1 http://news.google.com
ment cluster. For example, a number of news services, 2 http://www1.cs.columbia.edu/nlp/newsblaster/
                                               3 http://research.nii.ac.jp/ntcir/index-en.html
                                               4 http://duc.nist.gov


                                         IJCAI-07
                                          2903first employs the manifold-ranking process to compute and named entities in the topic description are investi-
the manifold-raking score for each sentence that denotes gated in [Ge et al., 2003] and CLASSY [Conroy and
the biased information richness of the sentence, and then Schlesinger, 2005] for event-focused/query-based
uses the greedy algorithm to penalize the sentences multi-document summarization. In [Hovy et al., 2005],
highly overlapping with other informative sentences. The the important sentences are selected based on the scores
summary is produced by choosing the sentences with of basic elements (BE). CATS [Farzindar et al., 2005]
highest overall scores, which are deemed both informa- is a topic-oriented multi-document summarizer which
tive and novel, and highly biased to the given topic. In first performs a thematic analysis of the documents, and
the manifold-ranking algorithm, the intra-document and then matches these themes with the ones identified in
inter-document links between sentences are differentiated the topic. More related work can be found on DUC
with different weights. Experimental results on two DUC 2003 and DUC 2005 publications.
tasks show that the proposed approach significantly out- To the best of our knowledge, the above systems are
performs the top performing approaches in DUC tasks usually simple extensions of generic summarizers and do
and baseline approaches.                     not uniformly fuse the information in the topic and the
  In the rest of this paper: Section 2 discusses previous documents. While our approach can naturally and simul-
work. The proposed summarization approach is proposed in taneously take into account that information in the mani-
Section 3. Section 4 describes the evaluation results. Section fold-ranking process and select the sentences with both
5 presents our conclusion and future work.   high biased information richness and information novelty.

2  Previous Work                             3  The Manifold-Ranking Based Approach
A variety of multi-document summarization methods 3.1 Overview
have been developed recently. Generally speaking, the The manifold-ranking based summarization approach
methods can be either extractive summarization or ab- consists of two steps: (1) the manifold-ranking score is
stractive summarization. Extractive summarization in- computed for each sentence in the manifold-ranking
volves assigning salience scores to some units (e.g. sen- process where the score denotes the biased information
tences, paragraphs) of the documents and extracting the richness of a sentence; (2) based on the mani-
sentences with highest scores, while abstraction sum- fold-ranking scores, the diversity penalty is imposed on
marization (e.g. NewsBlaster) usually needs information each sentence and the overall ranking score of each
fusion, sentence compression and reformulation. In this sentence is obtained to reflect both the biased informa-
study, we focus on extractive summarization. tion richness and the information novelty of the sen-
  The centroid-based method [Radev et al., 2004] is one tence. The sentences with high overall ranking scores
of the most popular extractive summarization methods. are chosen for the summary. The definitions of biased
MEAD   is an implementation of the centroid-based information richness and information novelty are given
method that scores sentences based on such features, as as below:
cluster centroids, position, TF*IDF. NeATS [Lin and Biased Information Richness: Given a sentence collection
Hovy, 2002] uses sentence position, term frequency, χ ={xi | 1in} and a topic T, the biased information richness
topic signature and term clustering to select important of sentence xi is used to denote the information degree of the
content, and use MMR [Goldstein et al., 1999] to remove sentence xi with respect to both the sentence collection and T,
redundancy. XDoX [Hardy et al., 2002] first identifies the i.e. the richness of information contained in the sentence xi
most salient themes within the document set by passage biased towards T.
clustering and then composes an extraction summary, Information Novelty: Given a set of sentences in the sum-
which reflects these main themes. Harabagiu and Laca- mary R={xi | 1im}, the information novelty of sentence xi is
tusu [2005] investigate five different topic representations used to measure the novelty degree of information contained in
and introduce a novel representation of topics based on the sentence xi, with respect to all other sentences in the set R..
topic themes. Recently, graph-based methods have been The underlying idea of the proposed approach is that
proposed to rank sentences or passages. Websumm [Mani a good summary is expected to include the sentences
and Bloedorn, 2000], LexPageRank [Erkan and Radev, with both high biased information richness and high in-
2004] and Mihalcea and Tarau [2005] are three such sys- formation novelty.
tems using algorithms similar to PageRank and HITS to
compute sentence importance.                 3.2  Manifold-Ranking Process
  Most topic-focused document summarization meth- The manifold-ranking method [Zhou et al., 2003a; Zhou
ods incorporate the information of the given topic or et al., 2003b] is a universal ranking algorithm and it is
query into generic summarizers and extracts sentences initially used to rank data points along their underlying
suiting the user’s declared information need. In [Sag- manifold structure. The prior assumption of mani-
gion et al., 2003], a simple query-based scorer by com- fold-ranking is: (1) nearby points are likely to have the
puting the similarity value between each sentence and same ranking scores; (2) points on the same structure
the query is incorporated into a generic summarizer to (typically referred to as a cluster or a manifold) are
produce the query-based summary. The query words likely to have the same ranking scores. An intuitive de-

                                         IJCAI-07
                                          2904scription of manifold-ranking is as follows: A weighted In the above iterative algorithm, the normalization in
network is formed on the data, and a positive rank score the third step is necessary to prove the algorithm’s con-
is assigned to each known relevant point and zero to the vergence. The fourth step is the key step of the algo-
remaining points which are to be ranked. All points then rithm, where all points spread their ranking score to
spread their ranking score to their nearby neighbors via their neighbors via the weighted network. The parame-
the weighted network. The spread process is repeated ter of manifold-ranking weight  specifies the relative
until a global stable state is achieved, and all points ob- contributions to the ranking scores from neighbors and
tain their final ranking scores.              the initial ranking scores. Note that self-reinforcement is
  In our context, the data points are denoted by the avoided since the diagonal elements of the affinity ma-
topic description and all the sentences in the documents. trix are set to zero.
The manifold-ranking process in our context can be The theorem in [Zhou et al., 2003b] guarantees that
formalized as follows:                        the sequence {f(t)} converges to
                      χ =          ⊂  m                    *         −1
  Given a set of data points {x0 , x1,..., xn } R ,the   f  = β (I −αS) y           (1)
first point x0 is the topic description and the rest n points where =1-. Although f* can be expressed in a closed
are the sentences in the documents. Note that because form, for large scale problems, the iteration algorithm is
the topic description is usually short in our experiments
                            5                 preferable due to computational efficiency. Usually the
and we treat it as a pseudo-sentence ,andthenitcanbe convergence of the iteration algorithm is achieved when
processed in the same way as other sentences. Let
   χ →                                        the difference between the scores computed at two suc-
 f :    R denote a ranking function which assigns to cessive iterations for any point falls below a given
each point xi (0in) a ranking value fi. We can view f
                   T                          threshold (0.0001 in this study).
as a vector f=[f0,…,fn] . We also define a vector
         T                                     Note that in our context, the links (edges) between
y=[y0,…,yn] ,inwhichy0=1 because x0 is the topic sen- sentences in the documents can be categorized into two
tence and yi=0 (1in) for all the sentences in the classes: intra-document link and inter-document link.
documents. The manifold ranking algorithm goes as Given a link between a sentence pair of x and x ,ifx and
follows:                                                                    i    j   i
                                              xj come from the same document, the link is an in-
                                              tra-document link; and if xi and xj come from different
 1.  Compute the pair-wise similarity values be- documents, the link is an inter-document link. The links
     tween sentences (points) using the standard Co- between the topic sentence and any other sentences are all
     sine measure. The weight associated with term t inter-document links. We believe that intra-document
     is calculated with the tft*isft formula, where tft is links and inter-document links have unequal contribu-
     the frequency of term t in the sentence and isft is tions in the above iterative algorithm. In order to investi-
     the inverse sentence frequency of term t,i.e. gate this intuition, distinct weights are assigned to the in-
     1+log(N/nt), where N is the total number of tra-document links and the inter-document links respec-
     sentences and nt is the number of the sentences tively. In the second step of the above algorithm, the af-
     containing term t. Given two sentences (data finity matrix W can be decomposed as
     points) xi and xj, the Cosine similarity is denoted    =    +                 (2)
                                                         W   Wintra Winter
     as sim(xi,xj), computed as the normalized inner
     product of the corresponding term vectors. where Wintra is the affinity matrix containing only the in-
 2.  Connect any two points with an edge if their tra-document links (the entries of inter-document links
                                              are set to 0) and W is the affinity matrix containing
     similarity value exceeds 0. We define the affin-        inter
                                              only the inter-document links (the entries of in-
     ity matrix W by W =sim(x ,x ) if there is an edge
                   ij    i j                  tra-document links are set to 0).
     linking x and x Note that we let W =0 to avoid
            i    j.             ii             We differentiate the intra-document links and in-
     loops in the graph built in next step.
 3.  Symmetrically normalize W by S=D-1/2WD-1/2 in ter-document links as follows:
                                                     ~  = λ      + λ               (3)
     which  D  is  the diagonal matrix with          W     1Wintra   2Winter
     (i,i)-element equal to the sum of the i-th row of We let 1, 2 ∈[0,1] in the experiments. If 1<2,the
     W.                                       inter-document links are more important than the in-
 4.  Iterate f(t+1)= Sf(t)+(1-)y. until convergence, tra-document links in the algorithm and vice versa. Note
     where  is a parameter in (0,1).         that if 1=2=1, Equation (3) reduces to Equation (2). In
         *                                                              ~
 5.  Let fi denote the limit of the sequence {fi(t)}. the manifold-ranking algorithm, W is normalized into
                                              ~
     Each sentences xi (1in) gets its ranking score S in the third step and the fourth step uses the follow-
      *                                                            ~
     fi .                                     ing iteration form: f(t+1)=  S f(t)+(1-)y.
       Figure 1: The manifold-ranking algorithm.
                                              3.3 Diversity Penalty Imposition

5                                             The original affinity matrix W is normalized
 The topic can also be represented by more than one sentence, -1
                                              by S =D W to make the sum of each row equal to 1.
and in this case only the vector y needs to be modified to rep- Based on , the greedy algorithm similar to [Zhang et
resent all the topic sentences in the manifold-ranking algo- S
rithm.                                        al., 2005] is applied to impose the diversity penalty and

                                         IJCAI-07
                                          2905compute the final overall ranking scores, reflecting both remaining words were stemmed using the Porter’s
the biased information richness and the information stemmer6.
novelty of the sentences. The algorithm goes as follows:
                                                             DUC 2003  DUC 2003  DUC 2005

1.  Initialize two sets A=Ø, B={xi | i=1,2,…,n}, and Task     Task 2    Task 3  the only task
    each sentence’s overall ranking score is initial- Number of clusters 30 30     50
    ized to  its manifold-ranking score, i.e. Data source      TDT      TREC      TREC
                 *                           Summary length  100 words 100 words 250 words
    RankScore(xi)=fi , i=1,2,…n.              Table 1: Summary of data sets used in the experiments.
2.  Sort the sentences in B by their current overall
    ranking scores in descending order.
3.  Suppose xi is the highest ranked sentence, i.e. the 4.2 Evaluation Metric
    first sentence in the ranked list. Move sentence x
                                         i   We used the ROUGE [Lin and Hovy, 2003] toolkit7 for
    from B to A, and then the diversity penalty is im-
                                             evaluation, which was adopted by DUC for automatically
    posed to the overall ranking score of each sen-
                                             summarization evaluation. It measures summary quality
    tence linked with xi in B as follows:
    For each sentence x ∈B,                  by counting overlapping units such as the n-gram, word
                  j                          sequences and word pairs between the candidate sum-
                =           −  ⋅  ⋅ *
    RankScore(x j ) RankScore(x j )  S ji fi mary and the reference summary. ROUGE-N is an
    where >0 is the penalty degree factor. The n-gram recall measure computed as follows:
                                                                             −
    larger  is, the greater penalty is imposed to the      ∑∑Countmatch(n    gram)
                                                           ∈∈
    overall ranking score. If =0, no diversity pen- ROUGE − N = SS{ Re f Sum} n-gram (4)
    alty is imposed at all.                                   ∑∑Count(n     − gram)
                                                            SS∈∈{ Re f Sum} n-gram
4.  Go to step 2 and iterate until B= Ø or the iteration
                                             where n stands for the length of the n-gram, and Count-
    count reaches a predefined maximum number.
                                             match(n-gram) is the maximum number of n-grams
  Figure 2: The algorithm of diversity penalty imposition. co-occurring in a candidate summary and a set of refer-
                                             ence summaries. Count(n-gram) is the number of
  In the above algorithm, the third step is crucial and its n-grams in the reference summaries.
basic idea is to decrease the overall ranking score of less The ROUGE toolkit reports separate scores for 1, 2, 3
informative sentences by the part conveyed from the and 4-gram, and also for longest common subsequence
most informative one. After the overall ranking scores co-occurrences. Among these different scores, uni-
are obtained for all sentences, several sentences with gram-based ROUGE score (ROUGE-1) has been shown
highest ranking scores are chosen to produce the sum- to agree with human judgment most [Lin and Hovy,
mary according to the summary length limit.  2003]. We show three of the ROUGE metrics in the ex-
                                             perimental results, at a confidence level of 95%:
4  Experiments                               ROUGE-1 (unigram-based), ROUGE-2 (bigram-based),
4.1 Data Set                                 and ROUGE-W   (based on weighted longest common
                                             subsequence, weight=1.2).
Topic-focused multi-document summarization has been In order to truncate summaries longer than length
evaluated on tasks 2 and 3 of DUC 2003 and the only limit, we used the “-l” option in the ROUGE toolkit and
task of DUC 2005, each task having a gold standard we also used the “-m” option for word stemming.
data set consisting of document clusters and reference
summaries. In our experiments, task 2 of DUC 2003 4.3 Experimental Results
was used for training and parameter tuning and the other
                                             4.3.1 System Comparison
two tasks were used for testing. Note that the topic rep-
                                             In the experiments, the proposed approach was compared
resentations of the three topic-focused summarization
                                             with top three systems and four baseline systems on task
tasks are different: task 2 of DUC 2003 is to produce
                                             3 of DUC 2003 and the only task of DUC 2005 respec-
summaries focused by events; task 3 of DUC 2003 is to
                                             tively. The top three systems are the systems with highest
produce summaries focused by viewpoints;thetaskof
                                             ROUGE scores, chosen from the performing systems on
DUC 2005 is to produce summaries focused by DUC
                                             each task respectively. The lead baseline and coverage
Topics. In the experiments, the above topic representa-
                                             baseline are two baselines employed in the topic-focused
tions were treated uniformly because they were deemed
                                             multi-document summarization tasks of DUC 2003 and
to have no substantial differences from each other. Ta-
                                             2005. The lead baseline takes the first sentences one by
ble 1 gives a short summary of the three data sets.
                                             one in the last document in the collection, where docu-
  As a preprocessing step, dialog sentences (sentences
                                             ments are assumed to be ordered chronologically. And
in quotation marks) were removed from each document.
The stop words in each sentence were removed and the
                                               6 http://www.tartarus.org/martin/PorterStemmer/
                                               7 We use ROUGEeval-1.4.2 downloaded from
                                             http://haydn.isi.edu/ ROUGE/

                                         IJCAI-07
                                          2906the coverage baseline takes the first sentence one by one ROUGE scores8. The high performance achieved by the
from the first document to the last document. Manifold-Ranking benefits from the following factors:
  In addition to the two standard baseline systems, we 1) Manifold-ranking process: The manifold-ranking
have implemented two other baseline systems, i.e. process in the proposed approach makes full use of the
Similarity-Ranking1andSimilarity-Ranking2. The inter-relationships between sentences by spreading the
Similarity-Ranking1 first computes the similarity be- rank scores. In comparison with the Similarity-Ranking1,
tween the topic description and each sentence in the the ROUGE-1 scores of the proposed approach increase
documents, and then the greedy algorithm proposed in by 0.01244 and 0.01038 on the two tasks, respectively.
Section 3.3 is employed to impose the diversity penalty 2) Diversity penalty imposition: If the proposed ap-
on each sentence, with the normalized similarity value proach does not impose diversity penalty on sentences
as the initial overall ranking score. The sentences with (i.e. =0), the ROUGE-1 scores will decrease by
highest overall ranking scores are chosen to produce the 0.02778 and 0.01952 on the two tasks, respectively. We
summary. In essence, the Similarity-Ranking1canbe can also see for the tables that the Similarity-Ranking1
considered as a simplified version of the proposed much outperforms the Similaity-Ranking2 because of
manifold-ranking based system by ignoring the rela- imposing diversity penalty on sentences.
tionships between the sentences in the documents. And 3) Intra-document/Inter-document link differen-
the Similarity-Ranking2 does not employ the diversity tiation: If the proposed approach does not differentiate
penalty imposition process and simply ranks the sen- the intra-document and inter-document links between
tences by their similarity value with the topic descrip- sentences (i.e. 1=2=1), the ROUGE-1 scores will
tion, which can be considered as a simplified version of slightly decrease by 0.00139 and 0.0007 on the two
Similarity-Ranking1 without the step of imposing diver- tasks, respectively.
sity penalty.                                   In next sections we will mainly show ROUGE-1 per-
  Tables 2 and 3 show the system comparison results formance due to page limit.
on the two tasks respectively. In the tables, S4-S17 are
                                              4.3.2 Parameter Tuning
the system IDs of the top performing systems, whose
                                              Figure 3 demonstrates the influence of the penalty factor
details are described in DUC publications. The Mani-
                                               in the proposed approach (i.e. Manifold-Ranking)and
fold-Ranking adopts the proposed approach described in
                                              the baseline approach (i.e. Similarity-Ranking1) when  :
Section 3. The parameters of the Manifold-Ranking are                                   1
set as follows: =8,  =0.3 and  =1, =0.6. And the 2=0.3:1 and =0.6. Wecanseethatwhen varies
                  1        2                  from 0 to 20, the performances of the Manifold-Ranking
only parameter of the Similarity-Ranking1issetas=8.
                                              are always better than the corresponding performances
 System         ROUGE-1  ROUGE-2   ROUGE-W    of the Similarity-Ranking1 on the two tasks, respec-
 Manifold-Ranking 0.37332 0.07677   0.11869   tively. This verifies that the use of the relationships be-
 Similarity-Ranking1 0.36088 0.07229 0.11540  tween the sentences of the documents in the proposed
 S16             0.35001  0.07305   0.10969   approach can benefit the summarization task. It is also
 Similarity-Ranking2 0.34542 0.07283 0.11155  clear that no diversity penalty and too much diversity
 S13             0.31986  0.05831   0.10016   penalty will deteriorate the performances.
 S17             0.31809  0.04981   0.09887
 Coverage Baseline 0.30290 0.05968  0.09678     Figure 4 demonstrates the influence of the in-
 Lead Baseline   0.28200  0.04468   0.09077   tra-document/inter-document link differentiating weight
   Table 2: System comparison on Task 3 of DUC 2003. 1:2 in the proposed approach when =8 and =0.6. 1
                                              and 2 range from 0 to 1 and 1: 2 denotes the real val-
                                              ues  and  are set to. Different  :  gives different
 System         ROUGE-1  ROUGE-2   ROUGE-W        1    2                 1  2
 Manifold-Ranking 0.38434 0.07317   0.10226   contribution weights to the intra-document links and the
 S4             0.37396   0.06842   0.09867   inter-document links. It is observed that when more
 S15            0.37383   0.07244   0.09842   importance is attached to the intra-document links (i.e.
 Similarity-Ranking1 0.37356 0.06838 0.09949  1 =1 and 2<0.9), the performances decrease evidently.
 S17            0.36901   0.07165   0.09751   It is the worst case when inter-document links are not
 Similarity-Ranking2 0.35752 0.06893 0.09596  taken into account (i.e.  :  =1:0), however, when in-
 Coverage Baseline 0.34568 0.05915  0.09103                      1  2
 Lead Baseline  0.30470   0.04764   0.08084   tra-document links are not taken into account (i.e. 1:
   Table 3: System comparison on the task of DUC 2005. 2=0:1), the performances are still very well, which
                                              demonstrates that inter-document links are more impor-
  Seen from Tables 2 and 3, the proposed system (i.e. tant than intra-document links for the summarization task.
Manifold-Ranking) outperforms the top performing sys- Figure 5 demonstrates the influence of the manifold
tems and all baseline systems on all three tasks over all weight  in the manifold-ranking algorithm of the pro-
                                              posed approach when =8, 1:2=0.3:1.

                                                8 The improvement is significant over the ROUGE-1 score
                                              by comparing the 95% confidence intervals provided by the
                                              ROUGE package.

                                         IJCAI-07
                                          2907