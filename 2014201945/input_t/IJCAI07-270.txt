             Efﬁcient HPSG Parsing with Supertagging and CFG-ﬁltering
          Takuya Matsuzaki     1             Yusuke Miyao   1             Jun’ichi Tsujii  1,2,3
                        1. Department of Computer Science, University of Tokyo
                           Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
                       2. School of Computer Science, University of Manchester
                             3. NaCTeM (National Center for Text Mining)
                              {matuzaki, yusuke, tsujii}@is.s.u-tokyo.ac.jp

                    Abstract                          lexical entry for each word, because the number of supertags
                                                      is generally large (more than 1000 in our case) and only one
    An efﬁcient parsing technique for HPSG is pre-    or two tagging errors in a sentence will cause a parse failure
    sented. Recent research has shown that supertag-  in many cases. In previous research [Clark and Curran, 2004;
    ging is a key technology to improve both the speed Ninomiya et al., 2006], the problem is overcome by assign-
    and accuracy of lexicalized grammar parsing. We   ing several supertags to each word in a sentence, i.e., multi-
    show that further speed-up is possible by elimi-  supertagging; the supertagger initially assigns only a small
    nating non-parsable lexical entry sequences from  number of lexical entries to each word and the number of
    the output of the supertagger. The parsability of lexical entries is gradually increased until the parser ﬁnds a
    the lexical entry sequences is tested by a technique successful parse. In short, the parser ‘takes over’ a part of the
    called CFG-ﬁltering, where a CFG that approxi-    supertagger’s task and resolves a certain amount of the lexical
    mates the HPSG is used to test it. Those lexical  ambiguity by itself to avoid parse failures.
    entry sequences that passed through the CFG-ﬁlter
    are combined into parse trees by using a simple     In this paper, we show that, by making the supertagger
    shift-reduce parsing algorithm, in which structural more powerful, the workload of the parser can be further re-
    ambiguities are resolved using a classiﬁer and all duced and the overall system becomes more efﬁcient. Specif-
    the syntactic constraints represented in the original ically, we combine the supertagger with a CFG that approxi-
    grammar are checked. Experimental results show    mates the original lexicalized grammar, to enumerate maybe-
    that our system gives comparable accuracy with    parsable supertag assignments in the order of their scores
    a speed-up by a factor of six (30 msec/sentence)  given by the supertagger. We say a supertag sequence is
    compared with the best published result using the maybe-parsable if the sequence is parsable by the approxi-
    same grammar.                                     mating CFG. The most time-consuming part of the enumer-
                                                      ation algorithm is parsing of the input sentence with the ap-
                                                      proximating CFG. However, we can do this CFG parsing ef-
1  Introduction                                       ﬁciently because the CFG is generally sparse, i.e., the combi-
Deep syntactic analysis by lexicalized grammar parsing of- nation of symbols that appear in a CF-rule is highly restricted.
fers a solid basis for intelligent text processing, such as The enumerated supertag sequences are parsed by an
question-answering, text mining, and machine translation. In HPSG parser one by one, until a successful parse is obtained.
those tasks, however, a large amount of text has to be pro- Though the enumerated supertag sequences are not necessar-
cessed to build a useful system. One of the main difﬁculties ily parsable by the original HPSG, we observed in the exper-
of using lexicalized grammars for such large-scale applica- iments that the parser ﬁnds a successful parse within only a
tions is the inefﬁciency of parsing caused by complicated data few maybe-parsable supertag sequences for most sentences.
structures used in such grammars.                       The biggest advantage of our approach is that the HPSG
  Recent research showed the importance of supertagging parser used in the last stage of the system, which is the most
for the speed, as well as the accuracy, of lexicalized gram- computationally demanding part in the previous approach,
mar parsing [Clark and Curran, 2004; Ninomiya et al., 2006]. can now be replaced by a simpler and more efﬁcient parsing
Supertagging is a tagging process where lexical entries are algorithm. Our HPSG parser is implemented as a classiﬁer-
assigned to the words in the input sentence [Bangalore and based, deterministic shift-reduce parser that is guided by a
Joshi, 1999]. In lexicalized grammars, a lexical entry encodes CFG-forest. The CFG-forest is created by the approximating
many constraints on how a word is combined with surround- CFG, and it approximates the HPSG-forest that would be ob-
ing words or phrases. The parsing efﬁciency is therefore in- tained if the input supertag sequence were fully parsed with
creased by supertagging because it makes the search space the HPSG. We do not need to keep many hypothetical sub-
explored by the parser much narrower.                 analyses represented by complex feature structures, as in the
  The current accuracy of the supertagger is, in general, not chart parsers used in the previous approach, since the input to
sufﬁcient to use it as a single-tagger, which assigns only one the HPSG parser is single-supertagged and also we can know

                                                IJCAI-07
                                                  1671almost surely which sub-analysis grows into a well-formed                  n1
parse tree by referring to the CFG-forest.
                                                                       t1     n2
2  Background                                                           I   t2   t3
2.1  Head-driven Phrase Structure Grammar                                   like it
HPSG  [Pollard and Sag, 1994] is a linguistic theory based  Figure 2: Analysis by the approximating CFG
on the lexicalized grammar formalism. An instance of HPSG
grammar mainly consists of two parts: a small number of
rule schemata and a large number of lexical entries. The rule 2.3 CFG-ﬁltering
schemata represent general grammatical constraints, while CFG-ﬁltering [Kiefer and Krieger, 2000; Torisawa et al.,
the lexical entries in the lexicon express word-speciﬁc char- 2000] is a parsing technique for uniﬁcation-based grammars.
acteristics. In HPSG, both lexical entries and phrasal con- In HPSG parsing based on CFG-ﬁltering, an input sentence
stituents are represented by typed feature structures called is ﬁrst parsed by a CFG that approximates the original HPSG
signs and applications of the rule schemata are implemented and then the CFG-parsing is ‘replayed’ using the HPSG. The
by uniﬁcation of signs and schemata.                  CFG parsing in the ﬁrst stage is much faster than normal
  Figure 1 presents an example of HPSG parsing for the sen- HPSG parsing since the uniﬁcation operations are replaced
tence “I like it.”1 First, three lexical entries are selected from by identity checking of atomic symbols of CFG, which is a
ones associated with each word in the lexicon. Then, the much more faster operation than uniﬁcation.
lexical entries of “like”and“it” are combined by applying The non-terminal symbols N, the terminal symbols T ,and
the Head-Complement schema to them and then the resultant the set of CF-rules R in the approximating CFG respectively
phrasal sign of the verb phrase “like it” is combined with the represent (abstracted) phrasal signs, (abstracted) lexical en-
lexical entry of “I” by the Subject-Head schema.      tries, and instantiations of the rule schemata. For example, the
  Variations of syntactic constructions allowed for a word are CFG tree in Figure 2, which is an analysis of the sentence “I
represented by different lexical entries associated to the word like it.” by an approximating CFG, corresponds to the HPSG
in the lexicon. These variations include not only ones with parse tree in Figure 1. In Figure 2, n1, n2, t1, t2,andt3
different subcategorization frames (e.g., transitive and in- are non-terminal and terminal symbols of the approximating
transitive) but also ‘transformational’ variations such as pas- CFG which represent feature structures at the corresponding
sivization and wh-extraction. The form of a parse tree con- positions in the HPSG parse tree.
structed upon a word sequence is hence highly constrained The number of possible phrasal signs in an HPSG is in-
once a lexical entry is selected for each word.       ﬁnite in general. To approximate the HPSG by a CFG, we
                                                      thus need to abstract away several parts of the phrasal signs
2.2  Supertagging                                     to keep the number of abstracted signs ﬁnite. The abstraction
                                                      of the feature structures is speciﬁed by means of a restrictor
Supertagging is a process where the words in the input sen-
                                                      [Shieber, 1985] on the feature structures; a restrictor takes a
tence are tagged with ‘supertags.’ In our case, a supertag is a
                                                      feature structure and overwrites several feature values in it to
lexical template, which is a common structure shared among
                                                      the most general values allowed for the features.
lexical entries for different words.
                                                        We use one of the algorithms proposed by Kiefer and
  Supertagging can be formulated as a sequence labeling task
                                                      Krieger [2000] to approximate the HPSG. The algorithm
and several types of techniques have been applied to it. We
                                                      ﬁnds all possible abstracted phrasal signs by iteratively ap-
selected a simple approach proposed by Clark [2002],which
                                                      plying the schemata to abstracted signs that have been found
uses a maximum entropy classiﬁer. The conditional probabil-
                                                      so far. A sketch of the algorithm is as follows:
ity of a supertag sequence t = t1 ...tn given a (POS-tagged)
sentence s is calculated as:                           1: Restrict all the lexical entries and obtain the set of termi-
                                                          nal symbols T
           n          n
                                                       2: i ← 0; Ni ← T
  P t|s       P t |s      1      W  ·  s,t ,i
    (  )=      ( i )=     Z exp (    Φ(   i ))  (1)    3: while Ni = Ni+1 do
           i=1         i=1 i
                                                       4:   Ni+1 ← Ni
                                                       5:   for all n1,n2 ∈ Ni do
where W is a vector of feature weights, Φ(s,ti,i) is the fea-
                                                       6:     for all schema s do
ture vector, and Zi is a normalization constant.
                                                       7:       Apply s to n1 and n2 as daughters and obtain the
  As is clear from the equation above, the supertagger ne-
                                                                mother phrasal sign m
glects the parsability of the supertag sequence. We empiri-                  
                                                       8:       Restrict m to m
cally show that, by combining a CFG-ﬁlter with the supertag-       
                                                       9:       if m ∈/ Ni+1 then
ger, it is possible to enumerate supertag sequences which are                      
                                                      10:         Ni+1 ←  Ni+1 ∪{m  }
most of the case parsable, in the order of the probability de-                
                                                      11:         R ←  R ∪{m   → n1n2}
ﬁned in Eq.(1), with only a small additional processing time.
                                                      12: N ←  Ni
  1Feature structures used in the example are very simpliﬁed ones; The restrictor deﬁnes a many-to-one mapping from the set
a much more complicated grammar is used in the experiments. of supertags (i.e., lexical entries) to the set of terminal sym-

                                                IJCAI-07
                                                  1672                                          Figure 1: HPSG parsing

bols T . We however pretend there is a one-to-one mapping tion describes the algorithm of enumerating maybe-parsable
between these two sets by allowing multiple occurrences of supertag sequences (line 6) and the disambiguator (line 8).
the same terminal symbols in T , if necessary, and remember-
ing which supertag was mapped to which terminal symbol. 3.1 Enumeration of maybe-parsable supertag
In the rest of the paper, we assume such a one-to-one relation sequences
and treat terminal symbols and supertags interchangeably.
                                                      The algorithm is based on Jim´enez and Marzal’s algorithm
  An important property of the CFG approximation is that
                                                      [Jim´enez and Marzal, 2000], which enumerates, given a
the language of the obtained CFG is a superset of the set of
                                                      weighted CFG and an sentence, the N-best parse trees for
parsable supertag sequences. In other words, if a supertag
                                                      the sentence in order of their weights. In their algorithm, the
sequence is not parsable by the CFG, it is also not parsable
                                                      input sentence is ﬁrst parsed with the weighted CFG using the
by the HPSG. We use this property to eliminate non-parsable
                                                      CKY algorithm and then the N-best parse trees are obtained
supertag sequences from the output of the supertagger.
                                                      by enumerating the top N derivations of the root edge, i.e.,
                                                      the edge which spans the whole input and whose label is the
3  An efﬁcient multi-stage parsing algorithm          start symbol. The N-best derivations of an edge e are recur-
   for HPSG                                           sively obtained by combining the N-best sub-derivations of
                                                      the sub-edges of e and selecting the top N derivations from
Our parsing system takes as input a POS-tagged sentence, the combinations. In reality, it is not necessary to calculate all
 w ,p  ,...,w ,p       w             p
(  1  1        n  n ) where i is a word and i is a POS the N-best derivations for every edge in the chart because the
tag, and outputs an HPSG parse tree. The algorithm is as N-best sub-derivations can be enumerated in a ‘lazy’ manner.
follows:                                              See their paper for further details on their algorithm.
 1: for all wi,pi in the input sentence do            To obtain the N-best parsable supertag sequences, instead
 2:  Assign scores by the supertagger to supertags associ- of the N-best parse trees, we modify Jim´enez and Marzal’s
     ated to wi in the lexicon                        algorithm as follows. Our algorithm takes as input a sentence
   j ←
 3:    0                                              w  =(w1,w2,...)   and a list of scored supertags for each
 4: repeat                                            word, {tij ,sij },wheretij is the j-th scored supertag for
     j ←  j
 5:        +1                                         wi and sij is the log-probability of tij . Given the input and
     t ←  j
 6:       -th best maybe-parsable supertag sequence   the approximating CFG, Ga, we make a weighted CFG such
          t
 7:  Parse  with the approximating CFG                that each CF-rule in Ga has a weight zero and each ‘leaf rule’
 8:  Select an HPSG parse tree by the disambiguator, using tij → wi has a weight sij . Note that the score of a deriva-
     the CFG forest made in the above step            tion deﬁned by the weighted CFG equals the log-probability
 9: until a well-formed HPSG parse tree is obtained   of the supertag sequence on the fringe of the derivation. The
10: return the selected HPSG parse tree               fringe of a derivation is the sequence of pre-terminals (i.e.,
  The approximating CFG is generally not equivalent to the tij s) of the tree of the derivation. In the ﬁrst phase of the al-
HPSG, and therefore, a maybe-parsable supertag sequence gorithm, we parse the input sentence with the weighted CFG
might not be parsable by the HPSG. In such cases, the dis- by using the CKY algorithm. In the second phase of the al-
ambiguator at line 8 fails to ﬁnd a well-formed HPSG parse gorithm, we enumerate the N-best fringes of the derivations
tree and another supertag sequence is tried in the next iter- of the root edge. Just as in Jim´enez and Marzal’s algorithm,
ation. The disambiguator might fail even when the maybe- we can recursively obtain the N-best fringes for an edge e by
parsable sequence is truly parsable by the HPSG because the concatenating the N-best sub-fringes for the sub-edges of e
CFG-forest created at line 7 may contain a tree not licensed and selecting the top N fringes from them in a lazy manner.
by the HPSG and the disambiguator might mistakenly try to We made several modiﬁcations to the basic algorithm de-
reproduce such a tree with the HPSG. However, we found scribed above in order to make it more efﬁcient. The ﬁrst
such cases are empirically rare and thus decided to simply try is that we replace the CKY algorithm used in the ﬁrst step
the next supertag sequence in such cases. The rest of this sec- of the algorithm with an agenda-based best-ﬁrst parsing al-

                                                IJCAI-07
                                                  1673gorithm. We further split the agenda-based parsing into two approximating CFG, using the CKY algorithm. The disam-
stages. The ﬁrst stage of the best-ﬁrst parsing stops when a biguation algorithm is as follows:
root edge is found. In most cases, the ﬁrst maybe-parsable su- 1: S ← empty stack; Q ← (t1,t2,...)
pertag sequence is truly parsable with the original HPSG. In 2: while Q is not empty or length(S) > 1 do
such cases, we need only the best-scored parse tree since the 3: if Γ(S, f)=φ then
ﬁrst maybe-parsable supertag sequence is its fringe. When 4:  return fail
the second supertag sequence is requested, we start the sec- 5: a ← argmax W · Φ(S, Q, a)
ond stage, in which the best-ﬁrst parsing is continued until all a∈Γ(S,f)
the edges scored greater than (α − θ) are added to the chart, 6: S, Q←apply(a, S, Q)
where α is the score of the best-scored parse tree and θ is a 7: r ← pop(S)
user-deﬁned threshold. At the end of the second stage, any 8: return r
edge used in a parse tree with a score greater than (α − θ) is In the algorithm, a denotes a parser action, Γ(S, f) is the set
stored in the chart and therefore we can ﬁnd all the maybe- of possible parser actions given the stack S and the forest
parsable sequences scored greater than (α − θ) without fully f, W is the vector of feature weights, and Φ(S, Q, a) is the
parsing the input sentence.                           feature vector. There are two types of parser action:
  Another modiﬁcation is that we set the weight of the leaf
                                                      SHIFT:  it pops a lexical sign from Q and pushes it into S.
rule tij → wi to (sij −si1) instead of sij . Note that this mod-
iﬁcation does not alter the supertag sequences enumerated by APPLY SCHEMA X: it applies an n-ary schema X to the
the algorithm or their orders. By this modiﬁcation, generation top n elements of S and replaces them with the resulting
of edges with small scores is suppressed for the same reason mother phrasal sign.
      ∗       [                     ]
as in A -parsing Klein and Manning, 2003 .            For example, the HPSG parse tree in Figure 1 is constructed
  The last modiﬁcation is that, before the agenda-based pars- by the following sequence of actions: SHIFT, SHIFT, SHIFT,
                                          w
ing starts, we discard all the supertags for word i whose APPLY SCHEMA Head Complement,
                s  − β       β
scores are less than i1 ,where is a user-deﬁned thresh- APPLY SCHEMA   Subject Head. The weight vector W is
old. By this modiﬁcation, we might lose some maybe-   obtained with the averaged-perceptron algorithm [Collins and
parsable sequences in which those discarded supertags ap- Duffy, 2002] with the polynomial kernel of degree 2.
pear. In the experiments, however, we found that the accu- The set of possible actions Γ(S, f) is determined so that
racy of parse trees created upon such supertag sequences with the parser never ‘goes out of the forest.’; Γ(S, f) is cre-
low scores is generally low. We therefore decided to use this ated by ﬁrst mapping each element in S, i.e., signs of sub-
               β
thresholding with for the efﬁciency of the enumeration. trees, to its corresponding node in f, then selecting actions
                                                      which can lead to at least one complete CFG parse tree in
3.2  Disambiguation by shift-reduce parsing with a    f that includes all the nodes to which some stack elements
     guiding forest                                   are mapped, and ﬁnally eliminating from them any such AP-
                                                      PLY  SCHEMA   Xs whose schema X is not applicable to the
We use an algorithm based on deterministic shift-reduce pars-
                                                      stack elements on the top of S.
ing to select an HPSG parse tree from those that can be
constructed upon a given supertag sequence. Although we
could use other disambiguation algorithms, the deterministic- 4 Experiments
parser-like algorithm is well suited to our framework because In this section, we ﬁrst give a brief summary of the speciﬁc
there is almost no need to keep multiple hypothetical sub- HPSG grammar used in the experiments and also describe the
analyses (as in bottom-up chart parsing algorithms) to avoid test/training data we used. We then give implementation de-
search failure, since the supertag sequence given to the parser tails of the supertagger, the CFG-ﬁlter, and the disambiguator.
is maybe-parsable and we also have a CFG-forest on the su- Finally the experiments and their results are discussed.
pertag sequence, which guides the shift-reduce parser just like
an LR-table with inﬁnite look-ahead.                  4.1  Enju grammar
  Deterministic, classiﬁer-based approaches to disambigua- We used Enju version 2.12, an HPSG grammar for English
                                           [
tion have been applied to dependency parsing (e.g., Yamada [Miyao et al., 2005]. The design of the grammar basically
                                        ]
and Matsumoto, 2003; Nivre and Scholz, 2004 )andCFG   follows the deﬁnition of [Pollard and Sag, 1994].Twelve
            [                   ]
parsing (e.g., Sagae and Lavie, 2005 ). Note that we can- schemata are deﬁned in the grammar (9 binary schemata and
not apply these algorithms to HPSG parsing in a straightfor- 3 unary schemata). The lexicon of the grammar was extracted
ward manner since they cause many parse failures for highly from Sections 02-21 of the Penn Treebank [Marcus et al.,
constrained grammars such as HPSG; to avoid parse failures, 1994] (39,832 sentences). The grammar consists of 2,253
we need the approximating CFG for ﬁltering the supertag se- lexical entries for 9,567 words.
quences and for making the guiding CFG forest.          A program that converts the Penn Treebank into an HPSG
  The disambiguator has two components, a stack of phrasal treebank is also distributed with the grammar. We used it to
and lexical signs, S, and a queue of lexical signs (i.e., su-
        Q                                             make the training and test data for the experiment. A standard
pertags), .  The input to the disambiguator is a POS- training/development/test split of the data is used; i.e., we
tagged sentence, a maybe-parsable supertag assignment t =
(t1,t2,...), and a forest, f, created by parsing t with the 2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html

                                                IJCAI-07
                                                  1674      description           feature templates             1. surface form of the head word of x
   surrounding words          w−1, w0, w1                 2. POS tag of the head word of x
  surrounding POS tags    p−2,p−1,p0,p1,p2,p3             3. lexical template of the head word of x
                     w−1w0, w0w1, p−1w0, p0w0, p1w0,      4. phrasal category8 of x (e.g., S, NP, NX) 9
     combinations     p0p1p2p3, p−2p−1p0, p−1p0p1,                    <  S(0),...,S(3),Q(0),...,Q(3), =
                     p p p p  p   p  p  p p  p p                  x ∈     S(0).     ,S(0).      ,
                      0 1 2, −2 −1, −1 0, 0 1, 1 2        1-4 are for :       left dep   right dep ;
                                                                          S(1).left dep,S(1).right dep
        Table 1: Features used in the supertagger         5. distance between head words of S(0) and S(1)
                                                          6. whether a comma exists between S(0) and S(1)
                                                          7. whether a comma exists inside S(0) or S(1)
                                                          8. pair of S(1).rmost pos and S(0).lmost pos
used the HPSG treebanks created from Section 02-21/22/23                            S(0)   S(1)
of the Penn Treebank as training/development/test data.   9. number of words dominated by and
                                                          10. valence features of S(0),S(1),Q(0),andQ(1)
4.2  Implementation details
                                                                 Table 2: Features used in the parser
HPSG supertagger
Table 1 lists the features used in our supertagger. In the ta-
ble, px and wx respectively denote a POS tag and a word at is the harmonic mean of LP and LR. All the timing informa-
relative position x from the target word. We used the same tion was collected on an AMD Opteron server with a 2.4-GHz
feature set as one used in Ninomiya et al.’s HPSG supertag- CPU. The two parameters, β and θ, were manually tuned us-
ger so that the comparison of our system and theirs becomes ing the development set; β = log(1000) and θ = log(100).
more meaningful. The number of supertags is 2,253 (i.e., the Table 3 lists the results of the parsing experiments on the
number of lexical entries). 3                         test set. The table also lists several reported results on the
CFG-ﬁlter                                             same test set by other HPSG parsers with the same grammar:
                                                      Ninomiya et al.’s parser is the model III in [Ninomiya et al.,
We created an approximating CFG from the Enju grammar by
                                                      2006], which is the supertagger-based HPSG parser brieﬂy
Kiefer and Krieger’s algorithm. Examples of the features we
                                                      explained in Section 1. Miyao and Tsujii’s parser is a CKY-
restricted in the approximation are PHON (phonology fea-
                                                      style HPSG parser [Miyao and Tsujii, 2005]5.Bothparsers
ture), SYNSEM:LOCAL:CONT    (semantic structures of the
                                                      use maximum entropy models for the disambiguation, and the
phrase), and SYNSEM:LOCAL:CAT:HEAD:AGR       (agree-
                                                      main difference between them is the inclusion of a supertag-
ment feature). The CFG contains 1,241 terminal symbols,
                                                      ger by Ninomiya et al.’s parser. The higher efﬁciency of our
20,647 non-terminal symbols, and 458,988 rules.
                                                      approach is clear from the table: out system runs around six
Disambiguator                                         times faster than Ninomiya et al.’s parser with comparable
Table 2 lists features used in the disambiguator.4 Features 1- accuracy.
4 are adaptations of the features used in Sagae and Lavie’s On the test set of the sentences ≤ 100 words, our method
                                                                                                     6
CFG shift-reduce parser [Sagae and Lavie, 2005], and fea- found a well-formed parse for 97.1% of the sentences. In
tures 5-9 are adaptations of ones used in Miyao and Tsujii’s preliminary experiments on the development set, we found
CKY-style HPSG parser [Miyao and Tsujii, 2005]. Feature the rate of successfully parsed sentences reached nearly 100%
10 includes several types of valency constraint read off from when we chose larger values for β. However, for such set-
the phrasal/lexical signs. For example, from a lexical sign for tings, average parse time signiﬁcantly increased and the F1
a ditransitive usage of “give”, we extract two features, “sub- score did not improve because while the recall slightly im-
ject=NP” and “complement=NP NP”.                      proved, the precision slightly deteriorated. For example, with
                                                      β =  log(105), 99.6% of the sentences in the development
4.3  Experimental results                             set got a parse with an average parse time of 52.9 ms. This
We evaluated the speed and the accuracy of the proposed fact means that when the ﬁrst maybe-parsable supertag se-
method on sentences in the test data of ≤ 40 words (2,162 quence is assigned very low probability by the supertagger,
sentences) and ≤ 100 words (2,299 sentences). We measured the enumeration algorithm needs to generate many edges un-
the accuracy of the parse results by the precision (LP) and re- til it ﬁnds the sequence, but the HPSG parse created on such
call (LR) of the (labeled) predicate-argument relations output a sequence is not so accurate.
by the parser. See [Miyao and Tsujii, 2005] for details of the Table 4 shows the cumulative percentages of the sentences
                                                      on which the parser found a well-formed parse within a cer-
deﬁnition of the predicate-argument relations. The F1 score
                                                      tain number of maybe-parsable supertag sequences. This ex-
  3Many of them do not, however, appear in the training treebank. periment was done on the development set and with the same
The ‘effective’ size of the tag set is thus around 1,300 since the rest parameter values as the above experiment. For about 95%
of supertags, which do not appear in the training data, are assigned of the sentences, the ﬁrst maybe-parsable supertag sequence
very low probabilities by the supertagger and hence rarely used.
  4S(i) and Q(i) denote i-th elements from the top of the stack and 5Enju parser (ver2.1). http://www-tsujii.is.s.u-tokyo.ac.jp/enju
the queue. S(n).left dep (resp. S(n).right dep) denotes the most re- 6For those sentences on which the parser failed, we collected par-
cently found lexical dependent of the head word of S(n) that is to the tial parse results by applying the disambiguator (without a guiding
left (resp. right) of S(n)’s head; S(n).lmost pos (S(n).rmost pos) forest) on the 1-best supertag sequence and evaluated the predicate-
denotes the POS tag of the left-most (right-most) word of S(n). argument relations identiﬁed in those partial parse results.

                                                IJCAI-07
                                                  1675