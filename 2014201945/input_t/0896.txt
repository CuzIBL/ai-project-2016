    A Novel Approach to Model Generation for Heterogeneous Data Classification 

                                     Rong Jin*, Huan Liu† 
   *Dept. of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824  
                                     rongjin@cse.msu.edu 
 † Department of Computer Science and Engineering, Arizona State University, Tempe, AZ85287-8809
                                         hliu@asu.edu 

                   Abstract                        A widely used approach for constructing an ensemble of 
                                                 models is to sample different subsets from the training data 
    Ensemble methods such as bagging and boosting 
                                                 and create a classification model for each subset. Bagging 
    have been successfully applied to classification [Briemann, 1996] and AdaBoost [Schapire and Singer, 
    problems. Two important issues associated with an 1999] are two representative methods in this category. Bag-
    ensemble approach are: how to generate models to 
                                                 ging randomly draws samples from the training data with 
    construct an ensemble, and how to combine them replacement and AdaBoost samples training data according 
    for classification. In this paper, we focus on the to a dynamically changed distribution, which is updated by 
    problem of model generation for heterogeneous 
                                                 putting more weight on the misclassified examples and 
    data classification. If we could partition heteroge- smaller weights on the correctly classified examples. 
    neous data into a number of homogeneous parti- Clearly, both methods do not treat homogeneous data and 
    tions, we will likely generate reliable and accurate 
                                                 heterogeneous data differently.  
    classification models over the homogeneous parti- For ensemble methods to work effectively on heteroge-
    tions. We examine different ways of forming ho- neous data, one intuitive solution is to first divide the het-
    mogeneous subsets and propose a novel method 
                                                 erogeneous data into a set of homogeneous partitions and 
    that allows a data point to be assigned multiple then to create a model for each partition of data. Member 
    times in order to generate homogeneous partitions classifiers built with different homogeneous partitions will 
    for ensemble learning. We present the details of the 
                                                 likely result in good diversity of an ensemble. One way to 
    new algorithm and empirical studies over the UCI realize this homogeneity-based partition is to employ stan-
    benchmark datasets and datasets of image classifi- dard clustering algorithms, such as K-means [Hartigan and 
    cation, and show that the proposed approach is ef-
                                                 Wong, 1979] and the EM clustering algorithm [Celeux and 
    fective for heterogeneous data classification. Govaert, 1992]. An example is the Gaussian Mixture Model 
                                                 (GMM). But, in general, there are two problems with this 
 1 Introduction                                  simple clustering approach:  
  Ensemble approaches such as bagging and boosting have • Single cluster membership. Most clustering algo-
been successfully applied to many classification problems rithms assume that cluster membership is mutually exclu-
[Dietterich, 2000; Bauer and Kohavi, 1999]. The basic idea sive and each data point can only belong to a single cluster. 
of ensemble methods is to construct a number of classifiers Even though the EM clustering algorithm allows soft mem-
over training data and then classify new data points by tak- bership for a data point, in the resulting clusters, each data 
ing a (weighted) vote of their predictions. Thus, two impor- point still only belongs to a single cluster [Witten and 
tant issues associated with an ensemble approach are: 1) Frank, 2000]. Therefore, when we use these clustering algo-
how to generate accurate yet diverse classification models, rithms to partition data, if the number of clusters is large and 
and 2) how to combine the models for ensemble classifica- the subsets of training data formed by a clustering algorithm 
tion. Diverse classifiers ensure good ensembles [Quinlan, are mutually disjoint, some clusters may have a very small 
1996]. In this paper, we focus on the first issue with an em- number of data points, which can lead to unreliable classifi-
phasis on heterogeneous data classification. Heterogeneous cation models. This is similar to the data fragmentation 
data classification refers to the problem when input data of a problem occurred in decision tree induction [Quinlan, 
single class are widely distributed into multiple modes. It 1993]. In contrast, the subsets of training data produced by 
arises when training data are collected under different envi- Bagging and AdaBoost are not mutually disjoint. For exam-
ronments or through different sources. An example of het- ple, in bootstrap sampling, each subset contains around 
erogeneous data classification is image classification, in 63.2% of the original training data.  
which labeled images are acquired from multiple resources • Unbalanced cluster sizes. Since most clustering al-
and exhibit disparate characteristics. For instance, some gorithms do not have control over cluster sizes, unbalanced 
images are black and white, and others are colorful.  cluster sizes resulting from clustering cannot be easily cor- rected. When the resulting clusters have very different sizes, pling training examples. Important methods in this group 
 a classifier built over a small cluster can be unreliable and include Bagging [Brieman, 1996] and AdaBoost [Schapire 
 thus degrade the performance of the ensemble in forming and Singer, 1999]. Although these methods have been 
 final ensemble classification. On the contrary, both Bagging shown to be effective for classification, they are not de-
 and AdaBoost have data samples of similar sizes when signed to take into account characteristics of heterogeneous 
 learning different models. Note that there have been previ- data. In this paper, we propose HISS –an algorithm that 
 ous efforts on balancing the sizes of different clusters, par- constructs homogeneous strata from heterogeneous data 
 ticularly for spectral clustering algorithms (e.g., the normal- while maintains the nice property of boostrap sampling pro-
 ized cut algorithm [Melta & Shi, 2001]). But, since the con- cedure - each stratum contains a similar number of data 
 trol of cluster size comes indirectly from the objective func- points.  
tion, the resulting clusters can still have unbalanced sizes.  Another line of research closely related to this work is the 
  In sum, a clustering approach may produce homogeneous study of clustering algorithms. In general, clustering algo-
data partitions, but cannot ensure similar sizes of different rithms can be categorized into parametric approaches and 
partitions; methods like Bagging can produce equally sized non-parametric approaches. The parametric approach is to 
partitions, but partitions are not homogeneous. Therefore, find a parametric model that minimizes a cost function asso-
we need a novel approach to partitioning data into homoge- ciated with instance-cluster assignments. Such methods in-
neous subsets of similar sizes in ensemble learning for het- clude the Mixture Model [Celeux and Govaert, 1992] and 
erogeneous data classification.                  K-means algorithm. For the non-parametric approaches, a 
  The goal of this work is to divide heterogeneous data into cost function is minimized by either merging two separate 
homogeneous subsets of similar sizes in order to generate clusters into a larger one or dividing a cluster into two 
reliable and accurate classification models. By focusing on smaller ones. The representative examples of this category 
homogeneous subsets, we do not require that each data point are the agglomerative approach and the divisive approach.  
belong to one subset; by ensuring similar sizes of data sub- Most clustering approaches assume that each data point 
sets, each classification model can be built with a similar only belongs to a single cluster. This assumption may not be 
number of data points. In this paper, we propose a HISS appropriate since the ultimate goal of clustering is to group 
(Homogeneous data In Similar Size) algorithm specially similar data points together. When it is uncertain to assign a 
designed for the above purposes for heterogeneous data data point to a single cluster, it is better off assigning it to 
classification. Specifically, HISS allows the user to specify multiple clusters. Although the traditional probabilistic 
the size of a subset. For example, the user can ask the algo- model and the fuzzy clustering algorithm allow for multi- or 
rithm to create 20 subsets with each containing 40% of the soft-memberships, the uncertainty of cluster membership is 
original data. This algorithm is similar to the bootstrap sam- only exploited during the process of estimation. In the re-
pling procedure in that both the number of subsets and the sulting clusters, each data point is assigned to only a single 
percentage of training data covered by each cluster can be cluster. Furthermore, most clustering algorithms do not have 
specified and varied. However, it differs from the simple any control over the size of clusters. Hence, the resulting 
bootstrap sampling procedure in that it puts the similar data clusters can be very unbalanced in size and the clusters of 
points into a single subset while bootstrap sampling ran- too small sizes could be useless in learning.  
domly selects data to form a subset. This property is impor-
tant in ensemble learning for classifying heterogeneous data. 3  The HISS Algorithm for Model Generation  
We will use strata for the homogeneous data partitions, and 
subsets for data partitions resulting from random sampling. 3.1  From Probabilistic Clustering to HISS 
 2 Related Work                                    We first describe the traditional probabilistic clustering 
                                                 algorithm, and then introduce algorithm HISS. 
 There have been many previous studies on how to create an The general idea of probabilistic clustering is to describe 
 ensemble of models. The methods for constructing an en- data with a mixture of generative models. Optimal parame-
 semble of models can be categorized into five groups [Diet- ters are usually obtained by maximizing the likelihood of 
 terich, 2000]: 1) Bayesian methods, which creates an en- data using the mixture model. Let n be the number of input 
 semble of model by sampling them from a estimated poste-
                                                 data points, K be the number clusters, {,x xx ,..., } be the 
 rior model distribution; 2) Sampling training examples,                        12     n
 which creates multiple subsets of training examples and input data, and {mm12 , ,..., mK } be the underlying models 
 trains a classifier for each of the subsets; 3) Sampling input that generate the data. By assuming that each data point is 
 features, which creates a number of subsets of the input generated from a mixture of models{mm , ,..., m }, we 
 features and a classifier is built for each subset of input fea-                12     K
                                                 have the likelihood of the data written as: 
 tures;  4) Error correct output code (ECOC), which convert 
a multiple class problem into a set of binary class problems;    nK
                                                           Kj
5) Injecting randomness, that generates ensembles of classi- lm({},)ij=1 τ = ∑∑ logτ i pxm (| i j ) (1) 
fiers by injecting randomness into the learning algorithm.       ij==11
  Among the five categories, our work is closely related to 
the second one, which creates multiple classifiers by sam- where px(|ij m ) is the likelihood of generating xi from the ing a reliable and accurate ensemble for heterogeneous data. 
              j                                  By setting γ  to be a reasonably large value (0.4 in this 
 model m j , and τ i  is the likelihood for data point xi  to be work), we ensure that each stratum has a sufficiently large 
 in the j-th cluster. Based on the assumption that each data number of examples for building a statistical learning 
 point can only belong to a single cluster, we have constraint model. For later reference, we refer this new clustering ap-
   K  j                                          proach as “HISS”, which stands for Homogeneous data In 
     τ = 1. An example of probabilistic clustering is the 
 ∑ j=1 i                                         Similar Size.  
 Gaussian Mixture Model (GMM), in which both τ j and 
                                          i      3.2  Optimization for HISS 
 px(| m ) are parameterized as: 
    ij                                           Putting Equations (3) and (4) together, we have: 
                 j
               τθij= , and                                         nK
                                                             Kj
                             T −1                    maxlm ({ij }=1 ,τ )= logτ i pxm ( i | j )
                                                     m ,τ j        ∑∑
             1       ()()xxijjij−Σ−µµ(2)            ii           ij==11
 px(|ij m )=−      exp
          2||π d /2 Σ 1/ 2 2                       subject to
         ()     j                                                                       (5) 
                                                     1   n j
 where θ  denotes the prior for the j-th cluster, and µ  and τγi =≤≤, 0 γ 1, for jK = 1,..., 
       j                                 j           n ∑ i=1
                                                         j
 Σ j  are the mean and variance matrix for the j-th cluster, 
                                                     0≤≤τ i 1  for injK = 1,..., , and,  = 1,...,
 respectively. Expectation and Maximization algorithm (EM) 
 (Dempster et al, 1977) can be used to search for the optimal Let us assume the Gaussian distribution for px(|ij m ), i.e., 
 parameters.  
                                                 px(| mj )~ N (µ jj ,σ ). Following the idea of the EM algo-
                           K
  By removing the constraint τ j = 1, we allow each 
                         ∑ j=1 i                 rithm, the difference in the likelihood of data between two 
 data point to belong to multiple homogeneous clusters, or in consecutive iterations is bound by: 
 short, strata. Hence, the optimization problem becomes       KK
                                                     lmt({ii (++− 1)}==11 ,ττ ( t 1)) lmt ({ ii ( )} , ( t ))
                   nK                                               j
             Kj                                                     τ (1)t +
     maxlm ({ij }=1 ,τ )= logτ i pxm ( i | j )        nKj           i
      m ,τ j       ∑∑                               ≥+∑∑υ      ()logt               
      ii           ij==11                             ij==11i        j
                                                                    τi ()t            (6) 
     subject to                           (3) 
                                                                   
         j                                             nKj          px(|ij m (1)) t+
     0≤≤τ i 1  for injK = 1,..., , and,  = 1,...,    ∑∑ij==11υi ()logt 
                                                                   px(|ij m ()) t
         j
 where all τ i  are constrained to between 0 and 1 to maintain 
                                                 where υ j  is defined as 
 the probability interpretation. It is easy to see that the opti- i
 mal solution is to set all τ j  to be 1, which means that each  τ j ()tpx ( | m ()) t
                    i                                      j      iij 
                                                         υi ()t = K                       (7) 
data point is included in every stratum.                           τ k ()tpx ( | m ()) t
                            j                                  ∑ j=1 iik
  To avoid the trivial solution for τ i , we choose to enforce 
 the percentage of training data that are covered by each Thus, the optimal solutions for the mean and variance of 
 cluster to be a predefined constantγ , i.e.,    Gaussian distribution can be obtained as follows: 
                                                            nnjj2
                                                             υυii()tx           ii () tx
      1   n j                                             ∑∑ii==1122
           τγ=≤≤, 0 γ 1, for jK = 1,...,  (4)     µσjj(1)tt+ =+=−+nn , (1)            µ j (1) t
       n ∑ i=1 i                                              υυjj()tt            ()
                                                           ∑∑ii==11ii
With the above constraint, we guarantee that the number of                   j
                                                 However, the optimal solution for τ i  is rather difficult to 
data points that support each stratum is around γ n .                                    j
  Compared to the single membership constraint, this new obtain because of the inequality constraints 01≤ τ i ≤  . 
 constraint has the following two advantages: 1) It does not Directly optimizing the Equation (6) with only the equality 
 assume that each data point has to belong to one stratum.                             j
                                                 constraint will result in the following solution for τ i (1)t + : 
 For this new stratifying method, on average each data point 
 can belong to γ K  number of strata. Therefore, when γ K  is          j
                                                              j      υi ()tnγ
larger than one, each data point is allowed to be in more    τ i (1)t += n                (8) 
                                                                        υ j ()t
than one stratum simultaneously. 2) It ensures that differ-         ∑  i=1 i
ent strata have balanced numbers of data points. In con-
trast to most clustering algorithms, the new algorithm en- Apparently, the above solution will always be nonnegative 
                                                    j
sures almost the same size for each stratum. This is particu- if  τ i ()t  is nonnegative. However, it does not guarantee that  
larly important to the research goal of this paper - generat- j
                                                 τ i (1)t +  is not greater than 1.                              j
             Finding Optimal τ i (1)t +              Data Set  # Examples  #Class  # Features
                                                       Ecoli 327 5                   7 
  Inputs: υ j ()t  for in= 1,...,  and jK= 1,...,  
          i                                           Pendigit 2000 10              16 
           j
  Outputs: τ i (1)t +  that maximizes Equation (7).    Glass 204 5 10 
  Initialization:                                      Yeast 1479 10                 8 
        j                                             Vehicle 946           4       17 
       τ i (1)0t += for in= 1,...,  and jK= 1,...,  
                                                    Image/Indoor  3500 2            190 
  for each cluster j                               Image/Outdoor  3500 2            126 
     do                                             Table 1: Description of datasets for the experi-
          For all examples i,                       ment for heterogeneous data classification. 
           set τ j (1)1t += if τ j (1)1t +> 
               i          i                      convert the classification problem of multiple classes into a 
          Compute the probability mass           set of binary class problems. The representative examples 
                        j                        include the one-against-all approach and error correct output 
              sni=−γτ{|i ( t += 1) 1} 
                                                 coding (ECOC) method [Dietterich, 1995]. During this 
         Re-compute                              process, multiple classes are grouped into two subsets of 
                   j
      jjυi          ()t                          classes. Data points from one subset of classes are used as 
     ττii(1)ts+=              ∀ j s.t.  (1)1 t +< positive examples and the remaining are used as negative 
                     υ j ()t
                 ∑    i                          examples. Because both the positive and negative pools can 
              {|itτ j (+< 1) 1}
                i                                be comprised of examples from multiple classes, it will cre-
                j
    while ( ∃+>jt s.t. τi ( 1) 1 )               ate data heterogeneity for each of the binary classes. 
  end                                              As discussed, an intuitive solution to classifying hetero-
                                                 geneous data is to create a set of classification models with 
    Figure 1: Algorithm for finding optimal τ j (1)t +  each classifier built on a homogeneous partition (stratum) of 
                                    i            the data, and then combine classifiers for the final predic-
                                         j
  In order to satisfy the inequality constraints 01≤ τ i ≤  , tion.  The traditional clustering algorithms are not designed 
 we use the KKT conditions [Fletcher, 1987] to efficiently for this task because of the potential unbalanced cluster-
                                           j     sizes and the data fragmentation problem. With the pro-
 adjust the value of τ j (1)t + . The basic idea is to reset τ  to 
                i                         i      posed algorithm HISS, we can avoid these two problems by 
 be 1 whenever the output from Equation (10) violates the setting the parameter to be large (0.4 in the experiment).  
             j
 constraint 01≤≤τ i  . After the adjustment, we will re- In sum, to classify heterogeneous data, we first apply 
                                                 HISS to obtain homogeneous strata and then create a classi-
compute τ j (1)t +  that are less than 1 using Equation (8). 
         i                                       fication model for each stratum to form an ensemble. We 
                                     j
 The procedure of adjusting and recomputing τ i (1)t +  will will refer to this model generation method as ‘HISS-based 
               j                                 Model Generation’ in our empirical study next. Finally, a 
continue until no τ (1)t +  violates the constraint. Figure 1 
               i                                 stacking approach [Wolpert, 1992] is used to combine mod-
shows the detailed steps for finding the optimal solution for els that are generated by the HISS-based model generation 
  j
 τ i (1)t + . Due to the space limit, the proof for the optimality method for the final prediction of the ensemble. 
 of the algorithm in Figure 1 is not provided here.  
                                                 4. Experimental Study 
 3.3 Classifying Heterogeneous Data 
                                                 The experimental study is designed to answer the following 
  For classification problems, heterogeneous data can be questions: 
found in many applications and in experiments:   1) Is the proposed model generation method effective for 
1)  Data acquired from multiple sources. In many cases, classifying heterogeneous data? To this end, we compare 
training data are acquired from multiple sources. Because the proposed model generation method to Bagging and 
each source has its own data distribution that may be differ- AdaBoost in classifying heterogeneous datasets.  
ent from others, the data merged from multiple sources are 2) Is the proposed HISS algorithm effective for generating 
therefore heterogeneous. For example, consider building a reliable models? To address this question, we will apply 
classification model for outdoor scenes. The training images both the proposed HISS algorithm and the probabilistic 
are collected from several different types of videos. Some of clustering algorithm to partition the training data and build a 
the videos are news stories and some of them are of adver- classification model for each partition.  
tisement. Some of them are of high quality and some of 
them are not. Thus, the widely disparate characteristics in 4.1 Experimental Design 
videos cause the merged data to be heterogeneous. Seven different datasets are used in the experiments: five 
2)  Data by converting a multiple class problem into a set multiple class datasets from the UCI Machine Learning re-
of binary class problems. In order to apply the binary class pository [Blake and Merz, 1998] and two binary class data-
classification algorithm to multiple class case, we need to                               AdaBoost     Bagging     HISS-based   Bagging     AdaBoost  
      Data Set     Baseline 
                             (Standard)  (Standard)    Ensemble    (Stacking)   (Stacking) 
   Ecoli        0.047 (0.012)  0.046 (0.006)  0.057 (0.014)  0.037 (0.006)  0.046 (0.006)  0.059 (0.006) 
   Pendigit     0.010 (0.003)  0.013 (0.003)  0.012 (0.002)  0.008 (0.002)  0.012 (0.001)  0.012 (0.003) 
   Glass        0.382 (0.027)  0.385 (0.081)  0.379 (0.046)  0.161 (0.044)  0.379 (0.027)  0.379 (0.027) 
   Yeast        0.314 (0.012)  0.320 (0.023)  0.313 (0.013)  0.313 (0.013)  0.315 (0.012)  0.315 (0.008) 
   Vehicle      0.103 (0.020)  0.163 (0.048)  0.131 (0.024)  0.048 (0.012)  0.100 (0.017)  0.085 (0.033) 
   Image/Indoor  0.153 (0.008)  0.140 (0.007)  0.156 (0.014)  0.140 (0.013)  0.157 (0.011)  0.144 (0.007) 
   Image/Outdoor  0.116 (0.008)  0.111 (0.017)  0.120 (0.011)  0.088 (0.005)  0.114 (0.006)  0.112 (0.007) 
   Table 2: Classification errors for the baseline model (SVM), AdaBoost, Bagging and the propose model generation 
   method (‘HISS-based Ensemble’). The column ‘Bagging (Stacking)’ refers to the case when the ensemble of mod-
   els is created by the Bagging algorithm but combined through the stacking approach using an SVM. The same is 
   for the column ‘AdaBoost (Stacking)’.  The variance of classification error is listed in parenthesis. 
 sets for image classification. The characteristics of these vector machine, the proposed HISS-based ensemble learn-
 seven datasets are listed in Table 1.           ing approach, standard Bagging and standard AdaBoost. 
    For the multiple class datasets, we introduce the hetero- First, we can see that the baseline model performs well 
 geneity into the data by converting the original multiple- comparing with both standard Bagging and AdaBoost. This 
 class problem into a binary one. Similar to the one-against- observation indicates that these seven heterogeneous data-
 all approach, examples from the most popular class are used sets are rather difficult for the standard ensemble ap-
 as the positive instances and examples from the remaining proaches to learn. In contrast, the proposed HISS-based en-
 classes are assigned to the negative class. Because data of semble method performs better than the baseline model and 
 the negative class are from multiple classes, we would ex- the two standard ensemble methods. For the datasets 
 pect some degree of heterogeneity inside the negative class. ‘Glass’, ‘Vehicle’, and ‘Image/Outdoor’, the improvement 
 For the two datasets of image classification, they both are is substantial, from 38.2% to 16.1% for ‘Galss’, 10.3% to 
 binary classification problems. The heterogeneity of data is 4.8% for ‘Vehicle’, and from 11.6% to 8.8% for ‘Im-
 due to the fact that images are from seven different video age/Outdoor’.  
 clips and each video clip provides 500 images. Since each Since the HISS-based ensemble method uses the stacking 
 video clip is of different type (e.g., varied quality in im- approach for combining different models, it is different from 
 ages), we would expect certain amount of heterogeneity the combination method that is used by AdaBoost and Bag-
 within the data.                                ging. To address this difference, we conduct the experi-
  The baseline algorithm used in this experiment is support ments that apply a stacking method to combine the models 
 vector machine [Burger, 1998]. In all the experiments, each generated by both Bagging and AdaBoost. The results are 
ensemble method generates 20 different SVMs; a stacking listed in Table 2 on the right side of the HISS-based ap-
approach [Wolpert, 1992] that also uses a SVM is employed proach, titled as ‘Bagging (Stacking)’ and ‘AdaBoost 
 to combine the outputs from all 20 models to form the final (Stacking)’, respectively. Compared these results to the re-
 prediction of the ensemble. For each experiment, we ran- sults of ‘Bagging (Standard)’ and ‘AdaBoost (Standard)’, 
 domly select 70% of the data as training and the remaining we see that there is no substantial change in classification 
 30% as testing. The experiment is repeated 10 times and the errors when using a stacking approach to combine models in 
 average classification error of the ten runs is used as the ensemble learning. For all the seven datasets, the ensemble 
 final result with the variance of classification errors.  of models generated by HISS performs the best. The reason 
                                                 why a stacking approach is useful for the HISS-based model 
 4.2  Heterogeneous Data Classification          generation method but not to the other two is that models 
 Table 2 shows classification errors for the baseline support generated by the HISS-based algorithm are much more di-
                                                 verse than the ones generated by both Bagging and 
  Data Set     HISS       EM         EM          AdaBoost. As a result, applying another layer of classifica-
                       (3 Clusters) (10 Clusters) tion model to combine the outputs from the distinguishable 
Ecoli       0.037(0.006)  0.448 (0.021)  0.448 (0.021) models (or stacking) will be able to take full advantage of 
Pendigit    0.008(0.002)  0.081 (0.043)  0.110 (0.023) all the models and obtain the best performance. 
Glass       0.161(0.044)  0.292 (0.101)  0.353 (0.017) Based on the above discussion, we conclude that the 
Yeast       0.313 (0.013)  0.314 (0.013)  0.314 (0.019) HISS-based ensemble model is more effective for classify-
Vehicle     0.048 (0.012)  0.219 (0.068)  0.052 (0.026) ing heterogeneous data than existing ensemble approaches. 
Image/Indoor  0.140(0.013)  0.184 (0.022)  0.203 (0.014)
Image/Outdoor  0.088(0.005)  0.156 (0.031)  0.182 (0.036) 4.3  Comparison with Other Clustering-based En-
Table 3: Classification error for using different cluster- semble Methods 
ing algorithms for model generation. ‘EM’ refers to us- The advantage of HISS versus the traditional clustering al-
ing Expectation-Maximization algorithm to cluster data.  gorithms is that HISS allows each data point to be in multi-
                                                 ple different strata. Thus it can ensure that the number of 