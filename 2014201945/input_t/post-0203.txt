           Learning Global Models Based on Distributed Data Abstractions

                               Xiaofeng Zhang and William K. Cheung
                                    Department of Computer Science
                                     Hong Kong Baptist University
                                      Kowloon Tong, Hong Kong
                                 {xfzhang,william}@comp.hkbu.edu.hk

                    Abstract                          as a rough ensemble illustration. Thus, by sharing only lo-
                                                      cal model parameters for global analysis, the degree of data
    Due to the increasing demand of massive and dis-  privacy preservation and the bandwidth requirement in a dis-
    tributed data analysis, achieving highly accurate tributed environment can readily be controlled. In this paper,
    global data analysis results with local data privacy an EM-like algorithm is proposed for learning a global GMM
    preserved becomes an increasingly important re-   directly from the aggregated version of the local abstractions.
    search issue. In this paper, we propose to adopt a
    model-based method (Gaussian mixture model) for
    local data abstraction and aggregate the local model 2 Problem Formulation
    parameters for learning global models. To support
    global model learning based on solely local GMM   2.1  Local Data Abstraction
                                                               d
    parameters instead of virtual data generated from Let ti ∈ < denote an observed data instance, θl denote the
    the aggregated local model, a novel EM-like algo- parameters of the lth local model (GMM) as the abstraction
    rithm is derived. Experiments have been performed of the lth source. The probability density function of the lth
    using synthetic datasets and the proposed method  local model plocal(ti|θl) with Kl components is given as,
    was demonstrated to be able to achieve the global
                                                                          Kl                Kl
    model accuracy comparable to that of using the data     plocal(ti|θl) = Pj=1 αjlpj (ti|θlj ) Pj=1 αjl = 1
                                                                      d     1
    regeneration approach at a much lower computa-                   − 2   − 2      1       T  −1
                                                       pj (ti|θlj ) = (2π) |Σlj | exp{− (ti − µlj ) Σlj (ti − µlj )}.
    tional cost.                                                                    2

                                                        L  sets of local GMMs’ parameters {θ1, θ2, ..., θL} ob-
1  Introduction                                       tained by performing local model-based density estimation
Most machine learning and data mining algorithms work with (e.g., the EM algorithm) are sent to a global server to learn a
a rather basic assumption that all the data can ﬁrst be pooled global data model.
together in a centralized data repository. Recently, there exist
a growing number of cases that the data have to be physically 2.2 Learning Global Model Parameters
distributed due to some practical constraints, such as privacy We start the derivation of our learning algorithm by consid-
concerns. The need to tackle them calls for the recent re- ering ﬁrst the conventional setting of the Expectation Maxi-
search interest in distributed data mining.           mization (EM) algorithm. Let Rik denote the estimated indi-
  A common methodology for distributed data mining is to cator for the kth component of the global model generating
ﬁrst perform local data analysis and then combine the results the ith data instance.
                 [                  ]                                                         th
to form a global one Kargupta et al., 2000 . One major lim- Assume Rlk is now a new indicator for the l local com-
itation is that local analysis may result in loss of informa- ponent1 with its underlying data to be generated by the kth
                                [                ]
tion for global pattern discovery. In Zhang et al., 2004 , global component and Rik denotes the average estimate of
a model parameter exchange approach was proposed to im- the probability that data instances from the lth local compo-
prove the global model accuracy, which however is not a nent are generated by the kth global component. By approx-
generic methodology which can easily be generalized to dif- imating Rik as
ferent types of global models. An alternative is to adopt a
ﬂexible enough model for local data abstraction and have
                                                                              Pi∈lthsource Rik
the subsequent analysis based on the local model parameters,       Rik ≈ Rik =                         (1)
                                                                                    Nl
which is what we adopt in this paper.
                                                                                              th
  As inspired by [Merugu and Ghosh, 2003], Gaussian mix- where Nl denotes the number of data from the l source, the
ture model (GMM) [Bishop, 1995] is chosen for the local data
abstraction. Via different parameter settings, GMM can in- 1Note that we abuse the index “l” to refer to one local component
terpolate a set of data with more detailed information as well of the aggregated local model.new M-steps become

               L
                 Rlkµl
             Pl=1            1  L
        µk =    L       αk = L Pl=1 Rlk
                  Rlk
             Pl=1
                L           T                   (2)
                   Rlk(Σl+µlµ )
              Pl=1          l      T
         Σk =       L         − µkµk .
                       Rlk
                  Pl=1
KL-divergence is adopted to compute Rlk (E-step), resulting
in
               exp{−D(pglobal(t|φk)||plocal(t|θl))}
       Rlk =   M                                (3)
                  exp{−D(pglobal(t|φi)||plocal(t|θl))}
             Pi=1
where D(P ||Q) denotes the KL-divergence between the two
probabilistic models P and Q, which can be derived as       (a) The KL-divergence of the model learned us-
      1                                                     ing the proposed method with different numbers of
      2
   |Σl|   1       −1             T  −1                      components of the global model.
 ln   1 +  (trace(Σ Σk) + (µk − µl) Σ (µl − µk) − 1).
      2   2       l                 l
   |Σk|

3  Experiments
Experiments have been performed using synthetic data sets
for performance evaluation. Four data sets (as ground truth)
were generated using known mixture models with 4, 5, 6 and
7 components, respectively. The data instances were then
randomly partitioned as four sources and local GMMs were
learned via the conventional EM algorithm for each source.
Two global models were trained, one using the proposed
method and one using the conventional data regeneration ap-
proach. As shown in the Table 1, the proposed method can
achieve a speedup factor ranging from 1.5 to 257 with highly   (b) GMM  with 10 components learned
comparable model accuracy.                                     based on 100 local Gaussian components
                                                               for the aggregated local model.
Table 1: Comparing the global models learned using the pro-
posed method and the data regeneration method (gx denotes Figure 1: Experimental results with a sizeable dataset.
the true data model with x components).
          time/speedup log-like./diff. mean/diff. KL
  g4  2:   0.09s/1.5   -1470.2/+2   3.7/-0.03 0.43    the global mediator and the local data sources for compro-
      3:   0.22s/1.8  -1169.4/-70.9 1.0/-0.03 0.12    mising individual local data sources’ privacy levels can be
  g5  2:    1.33s/33   -1850.6/-17  3.2/-0.02 0.21    adopted to improve the global model accuracy.
      3:   8.11s/162   -1898.5/-67 4.1/+0.04 1.31
      4:   10.3s/257   -1788.4/+1  3.3/+0.02 3.66     Acknowledgement
                                                      This work has been partially supported by RGC Central Al-
  The proposed approach has also been applied to a more location Group Research Grant (HKBU 2/03/C).
sizeable dataset generated using a GMM with 100 compo-
nents. Global models with different number of components References
were learned and their corresponding KL-divergence values
                                                      [Bishop, 1995] C. M. Bishop. Neural Networks for Pattern
when compared with the true model are shown in Figure 1 (a).
                                                         Recognition. Oxford University Press, 1995.
The divergence value decreased rapidly when the number of
components increased from 1 to 10 and then started to satu- [Kargupta et al., 2000] H. Kargupta, B. Park, D. Hersh-
rate. One could determine the optimal number of global com- berger, and E. Johnson. Collective Data Mining: A New
ponents by setting a threshold on the KL-divergence value as Perspective Towards Distributed Data Mining. In Ad-
cut-off. Figure 1 (b) shows the result with 10 global compo- vances in Distributed and Parallel Knowledge Discovery.
nents identiﬁed.                                         MIT/AAAI Press, 2000.
                                                      [Merugu and Ghosh, 2003] S. Merugu  and  J.  Ghosh.
4  Conclusion and Future Plan                            Privacy-preserving Distributed Clustering using Gen-
In this paper, we proposed a novel EM-like algorithm for erative Models. In Proceedings of IEEE International
learning a global model directly from the aggregated local Conference on Data Mining, 2003.
model parameters with promising experimental results on [Zhang et al., 2004] Xiaofeng Zhang, C.M. Lam, and
synthetic datasets. Currently, we are extending the obtained William K. Cheung. Mining local data sources for learn-
results to learn more sophisticated global models. Further- ing global cluster models via local model exchange. The
more, we are investigating how active negotiation between IEEE Intelligent Informatics Bulletin, 4(2), 2004.