                   Learning against opponents with bounded memory                    ∗

                        Rob Powers                                 Yoav Shoham
              Computer Science Department                  Computer Science Department
                    Stanford University                          Stanford University
                    Stanford, CA 94305                           Stanford, CA 94305
             powers@cs.stanford.edu                       shoham@cs.stanford.edu

                    Abstract                          that their own algorithm achieves these criteria in arbitrary re-
                                                      peated games. Note however that neither of these algorithms
    Recently, a number of authors have proposed cri-  makes any guarantee about the payoffs achieved by their al-
    teria for evaluating learning algorithms in multi- gorithm against non-stationary opponents and can potentially
    agent systems. While well-justiﬁed, each of these be exploited arbitrarily by adaptive opponents, as shown in
    has generally given little attention to one of the [Chang and Kaelbling, 2002].
    main challenges of a multi-agent setting: the ca-   In recent work, [Bowling, 2005] addresses this vulnerabil-
    pability of the other agents to adapt and learn as ity by adding a requirement that the agent experience zero av-
    well. We propose extending existing criteria to ap- erage regret. In this context, regret is traditionally deﬁned as
    ply to a class of adaptive opponents with bounded the maximum payoff that could have been achieved by play-
    memory. We then show an algorithm that prov-      ing any stationary policy against the opponent’s entire history
                  ǫ
    ably achieves an -best response against this richer of actual moves minus the actual payoff the agent received.
    class of opponents while simultaneously guarantee- Several algorithms have been proven to achieve at most zero
    ing a minimum payoff against any opponent and     regret in the limit (see [Hart and Mas-Colell, 2000] and [Ja-
    performing well in self-play. This new algorithm  fari et al., 2001] for examples in both game theory and AI).
    also demonstrates strong performance in empirical
                                                                   [                        ]
    tests against a variety of opponents in a wide range The work of Fudenberg and Levine, 1995 on ‘universal-
    of environments.                                  consistency’ is representative of this literature and also points
                                                      out two limitations of the regret minimization approach as a
                                                      whole. The ﬁrst is the inability of no-regret strategies to capi-
1  Introduction                                       talize on simple patterns in the opponent’s play. They address
                                                      this limitation with a proposal for the stronger concept of
Recent work in multi-agent learning has put forth a number of
                                                      ‘conditional consistency’ and a new algorithm that achieves
proposals for judging algorithms [Bowling and Veloso, 2002;
                                                      it in [Fudenberg and Levine, 1999]. The second limitation is
Conitzer and Sandholm, 2003; Powers and Shoham, 2005].
                                                      that while a no-regret algorithm guarantees a minimum pay-
In addition to arguing the merits of their proposal, each re-
                                                      off against any possible opponent, it ignores the possibility
searcher also demonstrated an algorithm meeting their crite-
                                                      that the sequence of moves played by the opponent is de-
ria. Unfortunately, the algorithms and even the criteria them-
                                                      pendent on the agent’s own moves. While this assumption
selves are in general applicable only within a very limited
                                                      is quite justiﬁed in games with a large number of players, it
setting. In particular, there has been a focus on designing al-
                                                      becomes a serious liability in repeated interactions with only
gorithms that behave well in the presence of stationary oppo-
                                                      a few players. While we are not aware of much work dealing
nents, dodging the complexities that arise when the opponent
                                                      explicitly with this limitation, [de Farias and Megiddo, 2004]
may be adapting to the agent’s past play.
                                                      address it in the design of their experts algorithm and the ra-
                           [                      ]
  The two criteria proposed in Bowling and Veloso, 2002 tional learning approach of [Kalai and Lehrer, 1993] can in
require that the agent both converge to a stationary policy principle handle adaptive algorithms of arbitrary complexity
against some class of opponents and that the agent play a as long as they are assigned positive probability in the prior.
best response if the opponent converges to a stationary pol-
icy. If these criteria are satisﬁed by all the players, this re- To see how the failure to consider adaptive opponents
sults in a guarantee of ultimately repeatedly playing a Nash could hurt an algorithm’s performance, let us consider a re-
equilibria of the stage game. They then propose an algorithm peated version of the Prisoner’s Dilemma game shown in
that provably meets these criteria in two player normal form Figure 1. Prisoner’s Dilemma has been extensively studied
                                                      [            ]
games with two actions per player. [Conitzer and Sandholm, Axelrod, 1984 and numerous algorithms proposed that al-
2003] adopt a restatement of these same criteria and prove low two agents to cooperate on the advantageous cooperation
                                                      outcome without being exploited. The simplest but perhaps
  ∗This work was supported in part by Air Force Grant F30602- most effective of these is the Tit-for-Tat algorithm. Tit-for-Tat
00-2-0598 and by NSF Grant IIS-0205633                starts by cooperating and thereafter repeats whatever actionthe opponent played last. Note that any approach that con- full game structure and payoffs are known to both agents. Fi-
siders only stationary opponents must always play Defect, nally, although we shall generally refer to the other player as
since this is the unique best response to any stationary op- the opponent, we do not mean to imply an adversarial setting,
ponent and the only strategy that can ever result in no-regret but instead consider the full space of general-sum games.
performance. Against Tit-for-Tat this results in a payoff of 1,
but the strategy of always playing Cooperate would yield a      Cooperate Defect           Left Right
payoff of 3. Clearly, a no-regret policy is not the best response
in this richer strategy space.                        Cooperate    3, 3    0, 4       Up    1, 0  3, 2
  As another example of the advantages of considering adap-
                                                        Defect     4, 0    1, 1      Down   2, 1  4, 0
tive opponents, consider playing the Stackelberg game of Fig-
ure 1 repeatedly. Notice that Up is a strictly dominated strat- (a) Prisoner’s Dilemma (b) Stackelberg Game
egy, regardless of what the opponent chooses the row agent
would prefer to play Down. However, if the opponent is
                                                      Figure 1: Example stage games. The row player’s payoff is
learning, this would presumably prompt them to play Left,
                                                      given ﬁrst, with the column player’s payoff following.
resulting in a payoff of 2 for the row agent. If it instead played
the seemingly suboptimal action of Up, the opponent may 3 Adaptive Opponents
learn to play Right, giving the row agent a higher payoff of While the goal of this work is to expand the set of possible
3. We can see that in this instance, teaching can play as much opponents against which we can achieve a best response, we
of a role in achieving a desirable outcome as learning. In will need to limit their capabilities in some way. If we con-
both of these games some of the most successful strategies sider opponents whose future behavior can depend arbitrar-
are those that have the ability to either cooperate with their ily on the entire history of play, we lose the ability to learn
opponents or manipulate their opponents as appropriate. anything about them in a single repeated game, since we will
  The weaknesses of this reliance on an assumption of only ever see a given history once. We will therefore assume a
stationarity were previously acknowledged in [Powers and limit on the opponent’s ability to condition on the history. We
                                                  1
Shoham, 2005] and we proposed the following three criteria: propose directly limiting the amount of history available, by
  Targeted Optimality: Against any member of the target requiring that the opponents play a conditional strategy where
set of opponents, the algorithm achieves within ǫ of the ex- their actions can only depend on the most recent k periods of
pected value of the best response to the actual opponent. past history, Fi : O−1 × ... × O−k → ∆Ai, where O−t is
  Compatibility: During self-play, the algorithm achieves the outcome of the game t periods ago. We will assume that
at least within ǫ of the payoff of some Nash equilibrium that the opponents have a default past history they assume at the
is not Pareto dominated by another Nash equilibrium.  start of the game. Note that even this simple model allows
                                                      us to capture many methods such as Tit-for-Tat that current
  Safety: Against any opponent, the algorithm always re- approaches are unable to properly handle.
ceives at least within ǫ of the security value for the game. Let’s now consider how we could apply the criteria given
  One of the key aspects of the proposal is the use of a param- above to this set of opponents. While the value of the best
eterized target class of opponents against which to achieve response to a given conditional strategy is well-deﬁned, it
optimal performance. While this could also address adap- would prove an unreasonable requirement for many possi-
tive agents, the previous work only provides an algorithm for ble strategies. Let’s again consider the Prisoner’s Dilemma
stationary opponents. In this work, we adopt the same crite- game with an opponent that is either playing the grim strategy
ria and analyze how to develop algorithms that behave well or always plays Cooperate. In the grim strategy the oppo-
against opponents that can adapt to their past experience. nent initially starts playing Cooperate but switches to play-
                                                      ing Defect indeﬁnitely if its opponent ever plays Defect (a
2  Environment                                        conditional strategy with history 1). Note that no possible
                                                      learning strategy can achieve the value of the best response
Within this paper, we will focus on the class of two-player re-
                                                      against both opponents, since it must play Defect at least
peated games with average reward. In this setting the two
                                                      once to distinguish them, at which point the option of always
players repeatedly play a simultaneous move normal form
                                                      cooperating with the grim strategy will no longer exist. There
game, represented as a tuple, G = (n, A, R1 ), where n
                                       ...n           are two possible approaches to remedying this problem. One
is the number of players, A = A1 × ... × A , where A is
                                      n         i     way is to constrain the class of opponents we consider. Suf-
the set of actions for player i, and R : A → ℜ is the reward
                              i                       ﬁcient requirements for conditional strategies are that either
function for agent i. After each round, the agents accumu-
                                                      the opponent only condition on the actions of the agent, not
late their reward from the joint outcome and get to observe
                                                      its own past actions, or that the policies the opponent plays as-
the prior actions of the other agent. Each agent is assumed
                                                      sign non-zero probability to each action for every past history.
to be trying to maximize its average reward. Two example
                                                      An alternative approach would be to relax our best response
normal form games with the rewards organized into a table
                                                      target. Instead of requiring the agent achieve the best value
are shown in Figure 1. For our purposes we assume that the
                                                      possible by any strategy played from the start of the game,
  1These three criteria were additionally required to hold for any we can set the target to be the highest average value that can
choice of ǫ with probability at least 1 − δ after a polynomial period be achieved after any arbitrary initial sequence of moves to
of learning at the start of the game.                 account for the need for exploration.  Even with these restrictions on our target, we can see that Set strategy = StochGodfather
in order to guarantee an ǫ-best response with high probabil- for τ1 time steps,  Play  strategy
ity we will require an exponential exploration period, since for τ2 time steps
to ﬁnd a good outcome an agent needs to sample from the     if  (AvgV alue < VGodfather − ǫ1)
exponential number of histories the opponent considers. Fur-   With  probability    p,
thermore, if we allow the opponent to condition on its own       set  strategy   =  MemBR
actions, the number of observations required can become un- Play  strategy
bounded unless we add a requirement of a minimum proba-   if opponentInTargetSet()
                            2
bility of playing any given action.                         for  τ1 time  steps,   Play  MemBR
                                                            if  opponentInTargetSet()
4  A Manipulative Algorithm                                    Set bestStrategy     = MemBR
Although the MetaStrategy algorithm we introduced in [Pow-  else  set  bestStrategy     = strategy
ers and Shoham, 2005] is only explicitly designed for station- else if (strategy == StochGodfather
ary opponents, we can use much of the intuition behind the     AND  AvgV alue > VGodfather − ǫ1)
approach to design a new algorithm for our class of adaptive Set bestStrategy    =  StochGodfather
opponents. The idea behind MetaStrategy is to start with a else set  bestStrategy    = MemBR
teaching strategy for an initial coordination/exploration phase while not end of game
and use its payoff and the opponent’s play to determine which if AvgV alue < Vsecurity − ǫ0
of three possible strategies to play. If its opponent is con-  Play  maximin   strategy
sistent with the target class it adopts a best response. If it else
achieves its target value by getting its opponent to adopt a   Play  bestStrategy
best response to its teaching strategy it continues playing it,
                                                                Figure 2: The Manipulator algorithm
otherwise it selects a default strategy.3 The algorithm then
plays according to this strategy as long as it exceeds its secu-
rity level, reverting to a maximin policy if its payoff drops. values and plays its portion of the target outcome. If the op-
  In ﬁgure 2, we show the implementation of this general ponent ever plays an action other than the matching action
approach for our target class. The MemBR strategy calcu- for the target outcome, the agent plays a strategy that forces
lates a best response strategy against conditional strategies. the opponent to achieve no more than its security value until
This approach maintains counts of the opponent’s actions af- the opponent again plays its target action. For our purposes
ter each history of length k, which it uses to calculate the cy- we’ve created a stochastic version of Godfather that selects a
cle of agent actions with the highest expected reward out of mixed strategy for the agent and a target action for the oppo-
all possible unique agent action sequences (those that don’t nent such that the joint strategy gives the opponent a higher
contain a length k repeated subsequence). Given sufﬁcient expected value than its security value and also denies it any
observations, this lets us guarantee that we achieve an ǫ-best advantageous deviations. This is necessary since we want
response against any member of our target opponent set.4 We our godfather algorithm to be implementable by a conditional
can calculate the probability the opponent’s play is consistent strategy with history 1. Because of this constraint, we need to
with our target set by comparing the observed distribution of make sure the opponent can’t achieve a net proﬁt by deviating
play for each history at separate times and measuring the de- one turn and then playing the target action the next, incurring
viation in action proﬁles. We continue to use the minimax only one period of punishment. The parameters speciﬁed in
                                                                                                    ǫ
strategy to achieve the security value guarantee. And for the the algorithm are a function of the desired values for and
                                                      δ
self-play guarantee we can replace the Bully-Mixed strategy in the theoretical guarantees. (Our empirical results used
                                                      τ1        , τ2       , p  .     , ǫ0 ǫ1    .
with a generous stochastic version of Godfather [Littman and = 20000 = 60000 = 0 005%    =    = 0 025)
Stone, 2001]. Godfather was motivated by the folk theorem An additional advantage of reusing our existing framework
for repeated games and selects some outcome in the game is that we can apply the proof used for the MetaStrategy algo-
matrix with greater payoff for each agent than their security rithm with only minor modiﬁcations to show Theorem 1. The
                                                      main change is that we must require that the agent play fully
  2To see this, consider a conditional strategy that always plays mixed strategies during the beginning of the game in order
Defect in Prisoner’s Dilemma if its opponent played Defect the to get sufﬁcient observations to test whether the opponent’s
previous move, but plays Cooperate with δ probability if its op- play was consistent with the target set of strategies. Given
ponent played Cooperate and it played Defect, and always plays this constraint, the proof consists mainly of a long string of
Cooperate if both it and the agent played Cooperate. The agent applications of the Hoeffding inequality [Hoeffding, 1956].
will require a number of observations proportional to 1/δ in order to
distinguish this opponent from one that always plays Defect.
  3                                                   Theorem 1  Our algorithm, Manipulator, satisﬁes the three
   In MetaStrategy this was Fictitious Play [Brown, 1951], but properties stated in the introduction for the class of condi-
note that by instead using a no-regret policy, we could have achieved
                                                      tional strategies with bounded memory k, after a training pe-
a stronger payoff guarantee against many opponents.                      k
  4                                                                   |A|
   This implementation is suitable for conditional strategies that riod depending on λ , where λ is the minimum probability
only depend on the agent’s actions. For general conditional strate- the opponent assigns to any action, or λ = 1 for opponents
gies we need to consider the full space of deterministic conditional that condition only on the agent’s actions.
strategies to ﬁnd a best response.  0.6         Manipulator         MetaStrategy         Godfather          Hyper-Q         SmoothFP

  0.5

  0.4

  0.3

  0.2

  0.1

   -

                  y                                           P        C       Q                  r
  (0.1)                    m       her
                          e                         alQ     hF                                  ato
                Bull             fat               c
                                d                 Lo                                          pul
     Random                     o                                          Hyper-
                              G        MemBR1            moot                               ani
                     StochM                             S       WoLF-PH                    M
                                                                                 MetaStrategy
 Figure 3: Average value for last 20K rounds (of 200K) across selected games in GAMUT. Game payoffs range from -1 to 1.

5  Experimental Results                               And Dove, Godfather is able to perform better by manipulat-
In order to test the performance of our approach we’ve used ing more of the opponents into yielding, both by sending a
the comprehensive testing environment detailed in [Powers clearer message, since it doesn’t have to do any exploration,
and Shoham, 2005; Nudelman et al., 2004]. Besides MetaS- and by waiting longer for its opponent to adapt. This stub-
trategy, our set of opponents includes Bully and Godfather bornness, however, proves its undoing in games where it is
from [Littman and Stone, 2001], Hyper-Q [Tesauro, 2004], critical to adapt to the other opponent, such as the Disper-
Local Q-learning [Watkins and Dayan, 1992], smooth ﬁcti- sion Game. MetaStrategy’s strong performance in Shapley’s
tious play [Fudenberg and Levine, 1995], and WoLF-PHC Game seems to stem from its default Fictitious Play strategy
[Bowling and Veloso, 2002]. We also include random sta- exploiting LocalQ and WoLF-PHC. However, we can see the
tionary strategies (Random), random conditional strategies advantage of Manipulator over MetaStrategy in games like
(StochMem), and MemBR1 which learns a best response to Prisoner’s Dilemma and Travelers Dilemma which have equi-
conditional strategies with history 1.                libria in the space of repeated game strategies that Pareto-
  In Figure 3 we show the performance of the most suc- dominate any equilibria of the stage game.
cessful of the distinct algorithms across a variety of nor-
mal form games. All of the adaptive algorithms fare well 6 Discussion
against the stationary opponents, Random and Bully, while Although Manipulator demonstrates consistent performance
Manipulator and to a lesser degree, Hyper-Q, fare the best across a wide variety of games, we are by no means claim-
against the bounded memory adaptive strategies, Godfather ing that it would be the best approach for all settings. In
and StochMem. Manipulator and Godfather also have an ad- particular, it doesn’t fare nearly as well in the most adver-
vantage against opponents that learn a best response to con- sarial games, like MatchingPennies, Rochambeau, and Shap-
ditional strategies, such as MemBR1 and Manipulator itself. leysGame. This is not surprising since it will be unable to
For other approaches that fall outside the target sets of either ﬁnd any deals to offer with its Godfather component and its
MetaStrategy or Manipulator we can see that MetaStrategy model-based assumption that its opponent is a conditional
has a slight advantage, mainly because of its ability to play strategy offers no particular advantage against other adaptive
pure strategies, while Manipulator is constrained to explore opponents. An alternate approach we considered was adapt-
the opponent’s strategy space during the initial coordination ing a model-free algorithm such as Q-learning [Watkins and
period. However, an additional advantage of Manipulator in Dayan, 1992] to the multi-agent setting, following in the foot-
this setting is its ability to perform well in self-play, achieving steps of numerous previous researchers attempting to ﬁnd ef-
the highest payoff of all algorithms tested, while MetaStrat- fective multi-agent learning strategies (e.g. [Littman, 1994;
egy is limited to the space of stationary policies and misses Claus and Boutilier, 1998; Tesauro, 2004]). In traditional Q-
more complex opportunities for cooperation in some games. learning the algorithm learns values for each action at each
  Let us now turn to the performance of our new algorithm in possible state of the world and then chooses the action that
different types of games. Figure 4 shows the relative reward maximizes its expected reward. We propose two possible al-
achieved by the most successful algorithms for a selection of ternatives for dealing explicitly with adaptive opponents. One
games in GAMUT averaged across the set of opponents. We method is to incorporate the recent history into the state of the
can see that Manipulator has the best performance in nearly world and learn action values for each possible recent history.
every game. In games like Prisoner’s Dilemma and Hawk A second approach is to instead learn values over sequences                   Manipulator        MetaStrategy        Godfather        Hyper-Q         SmoothFP
     100%

      95%

      90%

      85%

      80%

      75%

      70%

                ly       e           e  ar                                         m       e
               o            me  me  m   P   llar         ies fort             ical u      m
              p        am  a   a       n                n       mma          h                    me
          exes o      G               o        Dove    n                    p   oS                a
         S   g       n        tG      ti      d                ile
        e   li  Chicken o    n                        Pe      D    mpound      er       sGa
           O        ti  tionG a                                   o                    y
       Th  d       a        ri      Ac       An              rs                       le       woG
      f   n                a       W        k               e                 mZ      p       T
     O   a            ina v        e                    inimumEfn mC         o
         r                o                   MajorityVotingtching M o o RandomGame Rochambeau velersDilemmaBy
        rt           rd      ispersionGam GrabTheDoHaw a  s    d                    Sha     o
                     o   C  D                                 n        andomGraand
                                Ga               M       Pri  a       R   R           Tra  Tw
  Battle Be    CollaborCo                                    R
Figure 4: Percent of maximum value for last 20K rounds (of 200K) averaged across all opponents for selected games in
GAMUT. The rewards were divided by the maximum reward achieved by any agent to make visual comparisons easier.

of actions. When we conducted tests for these two algorithms In particular, we need to consider how to handle ﬁnite au-
we found that the approach that conditioned on previous his- tomata with multiple ergodic sets of states and automata with
tory demonstrated the ability to exploit some of the other arbitrarily small transition probabilities.
adaptive approaches including: Local Q-learning, SmoothFP,
and Hyper-Q, resulting in a higher average value in zero-sum
games than any of the previous approaches. However, nei- 7 Conclusions and Future Work
ther of these model-free methods performed as well in general We feel that explicitly addressing the issue of adaptive oppo-
sum games and both were dominated by Manipulator against nents is a critical element of learning in multi-agent systems.
each opponent when tested on the full set of games. Although It is this very quality that seems to deﬁne the difference be-
these approaches also lack the theoretical guarantees of Ma- tween the multi-agent setting and the single-agent one. Our
nipulator, they proﬁt from not requiring any advance knowl- algorithm approaches this setting by combining a teaching
edge about the game payoffs. Additionally, the ability to iden- approach which manipulates adaptive opponents into playing
tify settings where individual approaches are particularly ef- to our agent’s advantage with a cooperative/learning approach
fective may lead to more powerful methods using portfolios that adapts itself to its best estimate of its opponent’s strat-
of algorithms as suggested in [Leyton-Brown et al., 2003]. egy. Our algorithm can be shown to achieve ǫ-optimal aver-
  Finally, it should be pointed out that the model we’ve age reward against a non-trivial class of adaptive opponents
put forth for modelling opponents with bounded capabilities while simultaneously guaranteeing a minimum payoff against
is only one of many possible ones. A common approach  any opponent and performing well in self-play, all with high
used in the literature on bounded rationality [Neyman, 1985; probability. These results translate into good empirical per-
Papadimitriou and Yannakakis, 1994] is to assume the agents formance in a wide variety of environments. There is clearly
can be modelled by ﬁnite automata with k states. Note that more work that can be done, however. As we discussed in
the automata model is more comprehensive than the set of the previous section, we’ve so far only analyzed one possi-
conditional strategies since any conditional strategy opponent ble model for adaptive opponents with bounded memory and
with bounded memory can be modelled by an automata with are still considering how best to incorporate other approaches
|A|k states if we allow stochastic outputs, but there exist au- that achieve better empirical performance against existing al-
tomata that cannot be modelled by any function on a ﬁnite gorithms. Additionally, our approach still has signiﬁcant re-
ﬁxed history. In the case of automata with deterministic tran- strictions on the set of environments it considers. We can
sitions, we can modify our Manipulator algorithm to handle immediately identify ﬁve limitations of our current approach:
this new class by implementing our version of Godfather as
a DFA and replacing the best response function. Note that 1. Single Opponent: The criteria are only clearly deﬁned
learning a best response to an opponent modelled by an un- for games with two players.
known ﬁnite automata is equivalent to ﬁnding the best policy 2. Single State: The criteria are only clearly deﬁned for
                                    [
for an unknown POMDP, investigated in Chrisman, 1992;     repeated games (rather than general stochastic games).
Nikovski and Nourbakhsh, 2000]. While a difﬁcult computa-
tion problem, we should be able to achieve the same theoreti- 3. Average Reward: The criteria are deﬁned for games in
cal properties for this alternate set of opponents given similar which the agent only cares about the average of its ag-
constraints to those we placed on the conditional strategies. gregated rewards (rather than a discounted sum).