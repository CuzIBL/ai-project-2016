 Boosting Kernel Discriminant Analysis and Its Application to Tissue Classiﬁcation
                                     of Gene Expression Data
                                      Guang Dai & Dit-Yan Yeung
                            Department of Computer Science and Engineering
                            Hong Kong University of Science and Technology
                                 Clear Water Bay, Kowloon, Hong Kong
                                     {daiguang,dyyeung}@cse.ust.hk

                     Abstract                          Masip and Vitri`a, 2006] which effectively integrate the boost-
                                                       ing and LDA techniques have been developed recently to fur-
    Kernel discriminant analysis (KDA) is one of the   ther enhance the classiﬁcation performance.
    most effective nonlinear techniques for dimension-   On the other hand, a major problem of linear subspace
    ality reduction and feature extraction. It can be ap- methods such as PCA and LDA is that they fail to extract
    plied to a wide range of applications involving high- nonlinear features representing higher-order statistics. In or-
    dimensional data, including images, gene expres-   der to overcome this limitation, kernel dimensionality reduc-
    sions, and text data. This paper develops a new al- tion techniques such as kernel principal component analy-
    gorithm to further improve the overall performance sis (KPCA) [Sch¨olkopf et al., 1999] and kernel discriminant
    of KDA by effectively integrating the boosting and analysis (KDA) [Baudat and Anouar, 2000; Lu et al., 2003b;
    KDA   techniques. The proposed method, called      Dai and Qian, 2004; Xiong et al., 2005; Dai and Yeung, 2005;
    boosting kernel discriminant analysis (BKDA), pos- Zheng et al., 2005] have been proposed recently to extend lin-
    sesses several appealing properties. First, like all ear dimensionality reduction techniques to nonlinear versions
    kernel methods, it handles nonlinearity in a disci- by using the kernel trick, demonstrating better performance in
    plined manner that is also computationally attrac- many applications than that obtained using their linear coun-
    tive; second, by introducing pairwise class discrimi- terparts. The basic idea is to ﬁrst map each input data point
    nant information into the discriminant criterion and x ∈ Rn into the feature space F via a nonlinear mapping φ
    simultaneously employing boosting to robustly ad-  and then apply the corresponding linear dimensionality reduc-
    just the information, it further improves the classi- tion algorithm in F. Moreover, similar to their linear counter-
    ﬁcation accuracy; third, by calculating the signiﬁ- parts, KDA-based methods are generally better than KPCA-
    cant discriminant information in the null space of the based methods for classiﬁcation tasks. However, KDA-based
    within-class scatter operator, it also effectively deals algorithms usually suffer from the small sample size problem,
    with the small sample size problem which is widely because the number of training examples available is usually
    encountered in real-world applications for KDA;    smaller than the dimensionality of the feature space F espe-
    fourth, by taking advantage of the boosting and    cially for high-dimensional data. In order to overcome this
    KDA techniques, it constitutes a strong ensemble-  problem, many approaches have been developed based on dif-
    based KDA framework. Experimental results on       ferent criteria. Recently, more effective solutions [Dai and
    gene expression data demonstrate the promising per- Qian, 2004; Zheng et al., 2005] have been proposed to cal-
    formance of the proposed methodology.              culate the optimal discriminant vectors in the null space of
                                                       the within-class scatter operator where the signiﬁcant discrim-
1   Introduction                                       inant information exists. On the other hand, similar to con-
                                                       ventional LDA-based techniques [Lotlikar and Kothari, 2000;
Principal component analysis (PCA) and linear discriminant Tang et al., 2005; Dai and Yeung, 2005], the performance of
analysis (LDA) are two classical feature extraction and dimen- KDA degrades further due to the following deﬁciencies which
sionality reduction techniques broadly used in many tasks in- are referred to as non-balanced problems in this paper:
volving high-dimensional data. It is generally believed that
for pattern classiﬁcation problems such as face recognition 1. For multi-class pattern classiﬁcation problems, the op-
and tissue classiﬁcation of gene expression data, LDA-based timal criterion based on the conventional between-class
algorithms usually outperform PCA-based ones. The reason   scatter is not directly related to classiﬁcation accuracy.
is that the former optimizes the low-dimensional representa- In particular, the corresponding dimensionality reduc-
tion of the objects for classiﬁcation by maximizing the ratio tion procedure tends to overemphasize the inter-class dis-
of the between-class scatter to the within-class scatter, while tances of well-separated outlier classes in the input space
the latter simply optimizes object reconstruction without tak- at the expense of classes that are close to each other, lead-
ing class information into consideration. Many LDA-based   ing to signiﬁcant overlap between them.
algorithms have been proposed. Among them, as an attrac- 2. The expression of the average within-class scatter has an
tive approach, some new LDA algorithms [Lu et al., 2003a;  implicit assumption that all classes have the same weight

                                                IJCAI-07
                                                   744                                                       xio ∈X                             ∈{         }
    for the covariances. In fact, if the class with dominant i  has the corresponding label i 1,...,C .By
    covariance is simultaneously an outlier class in the input the implicit nonlinear mapping φ : Rn →F,theN images
                                                         F                            {  xio }
    space, the within-class scatter will fail to estimate the cor- in can be represented by the set φ( i ) . The conven-
    rect value for improved classiﬁcation, due to minimizing                           Sφ
                                                       tional between-class scatter operator b , within-class scatter
    the spread of the outlier classes while neglecting the min- Sφ                            Sφ
    imization of other covariances.                    operator w, and population scatter operator t can be ex-
                                                       pressed as: Sφ = 1 C  N  (mφ −mφ)(mφ   −mφ)T  , Sφ =
  In this paper, we further improve the overall performance     b   N   i=1  i   i        i          w
                                                        1   C    Ni (φ(xio ) − mφ)(φ(xio ) − mφ)T , Sφ = Sφ +
of KDA by proposing a novel KDA algorithm called boosting N i=1 io=1   i     i     i      i     t    b
KDA  (BKDA). The proposed approach effectively integrates Sφ = 1 C    Ni  (φ(xio ) − mφ)(φ(xio ) − mφ)T ,where
                                                        w    N  i=1  io=1    i          i   
the boosting technique with the most recently developed KDA mφ = 1 Ni φ(xio ) and mφ = 1   C    Ni  φ(xio ).
algorithms based on the pairwise class discriminant informa- i Ni io=1    i            N   i=1  io=1    i
                                                       We maximize the Fisher criterion below to obtain the optimal
tion. The BKDA approach employs the boosting technique to
                                                       projection directions w in F:
robustly calculate the pairwise class discriminant information
that is integrated into the scatter operators in order to solve the              wT Sφw
                                                                         φ
non-balanced problems of KDA. Although boosting has been                J  (w)=      b   .              (1)
                                                                                 wT Sφ w
applied to LDA, so far it has not been studied much for KDA.                         w
It is worthwhile to mention here several appealing properties However, in many practical applications, one of the major
of the proposed BKDA method:                           problems with KDA is the so-called small sample size problem
 1. It handles nonlinearity in a disciplined manner that is also with respect to (1), because of the degeneracy of the within-
                                                       class scatter operator in F. In general, this problem is solved
    computationally attractive like all kernel methods.
                                                       by applying techniques such as pseudoinverse, kernel PCA,
 2. By introducing the pairwise class discriminant informa- and QP decomposition. Recently, some more effective meth-
    tion into the discriminant criterion and simultaneously ods [Dai and Qian, 2004; Zheng et al., 2005] have been de-
    employing boosting to robustly adjust the information, veloped to explore the signiﬁcant discriminant information in
    it effectively overcomes the non-balanced problems in the null space of the within-class scatter operator. In general,
    KDA and further increases the classiﬁcation accuracy. a simple and efﬁcient approach of ﬁnding this signiﬁcant dis-
                                                       criminant information is to calculate the optimal discriminant
 3. It effectively boosts the signiﬁcant discriminant informa-                              
    tion contained in the null space of the within-class scatter                      Sφ      Sφ
                                                       vectors in the intersection subspace t (0) w(0) with re-
    operator and simultaneously deals with the small sam- spect to the following modiﬁed criterion:
    ple size problem. However, the algorithm and analysis
                                                                     φ w    wT Sφw   w
    presented here can also be applied easily to boost other        Jf ( )=      b  (    =1).           (2)
    discriminant information such as that included in the or- Furthermore, in many situations, nonlinear feature extraction
    thogonal complement of the null space of the within-class based on the above criterion (2) indeed further enhances the
    scatter operator.                                  overall performance in terms of classiﬁcation accuracy and nu-
 4. It constitutes a strong ensemble-based kernel KDA  merical stability. In addition, it should be pointed out that, ac-
    framework, taking advantage of both the boosting and cording to [Chen et al., 2000], for conventional LDA, when the
    KDA techniques.                                    dimensionality n of the training examples is less than the total
  To demonstrate the effectiveness of the BKDA method, we number of examples N, some useful discriminant information
apply the proposed method to effectively extract discriminant in the null space of the within-class scatter operator will be
                                                                                                       −
features for the tissue classiﬁcation of gene expression data. lost. This is especially the case when n is less than N C
Experimental results conﬁrm that our BKDA method is supe- because all this discriminant information will be fully dis-
rior to several existing dimensionality reduction methods in carded. In KDA described above, this situation can be avoided
terms of classiﬁcation accuracy.                       by choosing the kernel function appropriately, with which the
                                                       dimensionality of F nonlinearly mapped from the input space
2   Nonlinear Feature Extraction via Kernel            can become larger than the number of training examples.
    Discriminant Analysis                              3   Boosting Kernel Discriminant Analysis
As a nonlinear extension of LDA, KDA essentially performs  Learner
LDA in the feature space F. Note that F may be inﬁnite-
dimensional and it should be regarded as a Hilbert space. As 3.1 Boosting Kernel Discriminant Analysis
such, the scatter matrices in the input space for LDA corre- Similar to LDA, KDA also suffers from the non-balanced
spond to operators in F for KDA. For any operator A in F,we problems described in Section 1. Here, we will present the
let A(0) denote the null space of A, i.e., A(0) = {x | Ax = BKDA algorithm that combines the strengths of the boosting
0},andA(0)   the orthogonal complement of A(0). Thus,  and KDA techniques and effectively solves the non-balanced
A(0) ⊕ A(0) = F.                                       problems of KDA at the same time.
                                                         Boosting is a general machine learning meta-algorithm for
  Suppose X denotes a training set of N examples belonging
                                            Rn         improving the accuracy of any given learning algorithm. One
to C classes, with each example being a vector in .Let of the most effective boosting algorithms, referred to as Ad-
X  ⊂X                                            xio
 i      be the ith class containing Ni examples, with i aBoost, can be used in conjunction with many learning al-
denoting the ioth example in it. In addition, each example gorithms to improve their performance. Boosting algorithms

                                                IJCAI-07
                                                   745                                                         −                          t
are adaptive in the sense that the classiﬁers built are tweaked Generally, a larger value of qiio indicates a greater difﬁ-
in favor of those instances misclassiﬁed by previous classi-                       xio
                                                           culty of classifying example i w.r.t. the previous boost-
ﬁers [Freund and Schapire, 1997]. More speciﬁcally, the un-                   t
                                                           ing results. As such, qiio emphasizes the difﬁcult exam-
derlying idea of AdaBoost is based on the sample distribu-                                     X
tion, which, in essence, is a measure of how hard to classify ples in the within-class covariance of class i.
an example. Moreover, it should be notable that for multi- Based on the AdaBoost.M2 algorithm and the deﬁnitions
class problems, a version of AdaBoost called AdaBoost.M2 Sφ     Sφ
                                                       of  B and  W , we propose the BKDA algorithm as detailed
outperforms AdaBoost.M1 for many real-world applications.     1
Thus we prefer using AdaBoost.M2 in this paper. In order in Fig.1. It should be pointed out that the KDA involved in
to effectively overcome the non-balanced problems of KDA the BKDA algorithm is to calculate the optimal discriminant
and simultaneously form a strong connection between KDA                               Sφ
                                                       vectors in the null space of the new W in order to take ad-
and AdaBoost.M2, following [Freund and Schapire, 1997;                                               φ
                                                       vantage of the signiﬁcant discriminant information in S ,
Lu et al., 2003a], the pairwise class discriminant distribution                                      W (0)
is introduced on the basis of the mislabel distribution from Ad- and the detailed calculation procedure can be found in sub-
aBoost.M2. Further, at the tth iteration in AdaBoost.M2, any section 3.2 below. In addition, discriminant analysis is quite
pairwise class discriminant distribution dt between classes a strong feature extraction technique for classiﬁcation. As a
                                   i,j                 result, the boosting process cannot go forward due to the very
Xi and Xj can be calculated as follows:
     j                                                 small pseudo-loss t. In general, some sampling procedures
         `P               PN           ´
        1   Ni  Γt(xio ,j)+  j  Γt(xjo ,i) ,  i = j;
 t      2   i =1    i       j =1   j        if         are employed to artiﬁcially weaken the corresponding discrim-
di,j =       o              o
        0,                                  otherwise. inant technique, and, in BKDA, we choose some examples in
                                                                         t
                                                 (3)   each class based on qiio to focus on the hardest examples in
 Here, the mislabel distribution Γt(, ∗) measures the extent each class. For the features extracted by discriminant analy-
of the difﬁculty of discriminating the example  from the im- sis, a simple nearest neighbor classiﬁer is generally employed
proper label ∗ on the basis of the previous boosting results. for classiﬁcation. In order to be consistent with the AdaBoost
                          t                                                                ∗
Obviously, a larger value of di,j intuitively indicates worse algorithm, in BKDA, the hypothesis ht(, ) between exam-
separability between two classes Xi and Xj, further embody- ple  and class ∗ can be built easily based on the normalized
ing also that both are closer together in F.           nearest neighbor classiﬁer, while it gives results identical to
  To address the ﬁrst non-balanced problem in KDA, we re- the classical nearest neighbor classiﬁer.
                                            Sφ
place the ordinary between-class scatter operator b by a
weighted between-class scatter operator Sφ :           3.2  How to Calculate the Optimal Discriminant
                                   B                        Vectors in the Feature Space F
       Xc−1 Xc N  N
  Sφ =           i j w(dt )(mφ − mφ)(mφ − mφ)T ,
   B            N 2    i,j  i     j   i    j     (4)   In what follows, we will describe how to efﬁciently calculate
       i=1 j=i+1                                       the signiﬁcant discriminant information in the null space of
                                                                 Sφ
where the weighting function w() is generally chosen to be a the variant W in (5). Furthermore, on the basis of the vari-
                                                           Sφ     Sφ
monotonically increasing function of . For simplicity, we let ants B and W , we also represent the corresponding popu-
w()=  in this paper. Obviously, based on the deﬁnitions of                 Sφ    Sφ   Sφ
                                                       lation scatter operator as T = B + W . As described in
the pairwise class discriminant distribution and the weighting Section 2, the optimal discriminant vectors can be obtained in
function w(), it can be seen that classes that are not well sep- 
                                                       Sφ (0)  Sφ (0) with respect to the following criterion:
arated in F and thus can potentially impair the classiﬁcation T W
                                         F
performance should be more heavily weighted in .                     φ w    wT Sφ w  w
  In addition, a weighting scheme should also be employed to       JF ( )=      B   (    =1).           (6)
alleviate the second non-balanced problem, i.e., the negative
effect of outlier classes when estimating the within-class scat- Furthermore, based on the above criterion, we have to cal-
                                                             Sφ     Sφ                             Sφ
ter operator. Further, at the tth iteration, we replace the ordi- culate T (0) W (0), which requires calculating T (0) or
                            Sφ
nary within-class scatter operator w by the weighted within- Sφ (0) ﬁrst. However, the computation of Sφ (0) or Sφ (0) is
                  Sφ                                    W                                   T       W
class scatter operator W as follows:                   quite intractable to some extent, due to the following reasons:
                                                                φ                                φ
          XC XNi                                       1) unlike S , it is intractable to directly compute S (0) by the
   φ    1         t t    i      φ    i      φ T                 t                                T
  S  =           r q (φ(x o ) − m )(φ(x o ) − m ) ,                   φ        φ
   W    N         i iio  i      i    i      i    (5)   eigenanalysis of S ,sinceS cannot be explicitly expressed
          i=1 i =1                                                    T        T
              o                                          Sφ    AAT         A
                                                      as  T =      ,where   is an operator with the explicit for-
       t            t                                                                    φ
 where ri =  j=i w(di,j ) is the relevance-based weight for                            S
                                                      mulation; 2) the direct computation of W (0) is very infeasi-
            t            t  io             io
class Xi and q = w(   = Γ (x ,j)) is that for x .Letus         Sφ                           F
            iio      j i    i              i           ble, since W (0) is in general very large in ; 3) according
highlight some characteristics of Sφ here:                                   Sφ
                             W                         to [Cevikalp et al., 2005], W (0) can be indirectly calculated
 −                  t                                          φ                                φ
    By incorporating ri in (5), it ensures that the estimated by F−S (0) on the basis of ﬁrst calculating S (0), while,
    Sφ                                X                  F     W                                W
     W  is only inﬂuenced slightly if class i is an outlier in , high computational complexity is involved at the same
    class. This is reasonable since if one class is well sep- time. In order to efﬁciently solve this problem, we provide the
    arated from the other classes in F, then whether the following theorem.
    within-class covariance operator of this class in the new
    space is compact or not will not have much effect on clas- 1For simplicity, KDA in subsection 3.2 is described on X but it is
    siﬁcation [Tang et al., 2005].                     essentially very similar to that based on St.

                                                IJCAI-07
                                                   746                                 X     {xio | xio ∈    √1                                     Sφ   F
 Input: A set of training examples  =     i   i           [Φ1,...,ΦC ]. Since the dimensionality of t in is usu-
 Rn                             }                        N
    ,i =1,...,C,io   =1,...,Ni   ; a set of all mis-   ally very high or even inﬁnite-dimensional, KPCA can be car-
       M        {        |     ∈{          }     ∈
 labels     =    [(i, io),j] i, j   1,...,C ,io                                  ΞT Ξ                 ×
 {        }     }                             M       ried out by the eigenanalysis of instead, with size N N.
  1,...,Ni ,i=  j ; the initial mislabel distribution on :               {  xio }     ×         K
     io      1       1                                 From the training set φ( i ) ,anN N matrix can be de-
  t x                                                                                               =1
 Γ ( i ,j)= |M| =  N(C−1) ; a small constant ε.                                                   jo  ,...,Nj
                                                       ﬁned as K =(K   )  =1    where K   =(k  o o )
 for t =1,...,T    do                                                 ij i,j ,...,C     ij    i j io=1,...,Ni
               max                                                    io     jo                     T
                                                       and k o o = 
φ(x ),φ(x ). By the kernel trick, Ξ Ξ can
   − Calculate the terms dt by (3), and rt and qt in (5).   i j       i      j
                       i,j          i     iio          be expressed as
                                                                                                
   − Select s hardest examples per class based on qt to
                                              iio         ΞT Ξ    1   K −  1 K1    1K     1  1K1
     form a training subset S ⊂X.                               =           (    +    )+   2      ,     (8)
                         t                                        N       N              N
   − Apply KDA in subsection 3.2 on St, and constitute
                                                       where 1 is an N × N matrix with all terms being one. Let λl
     the KDA-based feature extraction technique, denoted and e (l =1,...,m) be the ith positive eigenvalue and the
                             X         {yio,t ∈ Rr}         l
     by KDA-t; apply KDA-t on  to obtain i        ;                             ΞT Ξ                  θ
                           ∗  ∈                 Yt     corresponding eigenvector of  , respectively. Then l =
     build the hypothesis ht(, ) [0, 1] on the subset Ξe   −1/2
          io,t   r                                        lλl   (l =1,...,m) constitute the orthonormal basis of
     of {y   ∈ R }, corresponding to St.
          i                                            Sφ       Sφ
   −                                                    t (0) (or T (0)) [Dai and Qian, 2004; Zheng et al., 2005].
     Calculate the pseudo-loss based on ht:  t =                     x  ∈ Rn                       r
                Γt(xio )(1+ (yio,t )− (yio,t ))       Hence, any input        can be transformed into in the
                     i ,j  ht i  ,j ht i  ,i                                m
       [(  ) ]∈M              2             .          low-dimensional space R via KPCA as follows:
         i,io ,j                                                            
                                                                    T               T         T  T
   − Set βt = t/(1 − t).Ifβt ≤ ε,thenTmax = t − 1           r = P  φ(x)=    1/N E  (I − 1/N ) Υ ,     (9)
     and break.                                                                          −1/2        −1/2
                                                       where P  =[θ1,...,θ   ], E =[e1λ      ,...,e λ    ],
   −                               t  t+1 xio                               m            1         m m
     Update the mislabel distribution Γ : Γ ( i ,j)=   I                              Υ          1      N1
               (1+ (yio,t )− (yio,t )) 2                  is the identity matrix, and      =(k1,...,k1    ,
         io       ht i ,i  ht i ,j /
     Γt(x  ,j)β                      .                  1      N2      1      NC        io    
 xio    x 
         i     t                                       k2,...,k2 ,...,kC ,...,kC ) with ki =  φ(  i ),φ( ) .
   −            t+1  t+1 xio                           Then, by (9), the training examples {xio } in Rn (or { xio }
     Normalize Γ   : Γ  ( i ,j)=                                                       i            φ( i )
                                                                                                    io
       +1  io                  +1  lo                     F                                        {r  }
       t  x                   t   x                    in  ) can be mapped to the corresponding points i in
     Γ   ( i ,j)/(  [(l,lo),g]∈M Γ ( l ,g)).                                   m
                                                       the low-dimensional space R . The scatter matrices SB and
 end for                                                                              m                  φ
                                                       SW  in the low-dimensional space R corresponding to S
 Output: The ﬁnal hypothesis: h (x)=                                                                     B
                           f                          and Sφ  can be computed by either of the following two ap-
            {−   Tmax          yt  }                        W
 supi∈{1,...,C}  t=1  log(βt)ht( ,i) , where, for any  proaches (for simplicity, this paper adopts the ﬁrst approach):
            t    r
 example x, y ∈ R  is the corresponding nonlinear feature                                 φ      φ
                                                         − By the corresponding deﬁnitions of S and S , SB and
 vector extracted by KDA (KDA-t) in subsection 3.2 over    S                              B      W
 S .                                                         W can be reconstructed based on the training examples
  t                                                        {rio } Rm
                                                             i  in   ;
         Figure 1: Summary of BKDA algorithm.            − By the kernel trick, the scatter matrices can be directly
                                                                         S      PT Sφ P    S      PT Sφ P
                                                           computed using  B =      B   and  W  =     W   ,
Theorem 1. The subspace Sφ (0) = {x |
Sφ , x =0, x ∈     similar to KPCA.
                         T            T                                                                Sφ
F}                            Sφ       {x |
Sφ x     Furthermore, we can directly calculate the null space of W
    is equivalent to the subspace t (0) =   t ,   =
                φ     φ    φ      φ                      Sφ                         S         S
0, x ∈F},whereS   = S  + S   and S  is the conventional in T (0) by the eigenanalysis of W ,since W is only an
                T     B    W      t                       ×                               V     γ      γ
population scatter operator.                           m    m  matrix. More speciﬁcally, let =[1,...,    u]
                                                       be the eigenvectors corresponding to the zero eigenvalues
  Based on Theorem  1, we thus propose to use the fol-                      
                                                          S            Sφ     Sφ                      PV
lowing steps to calculate the optimal discriminant vectors of W ,andthen T (0)  W (0) can be spanned by   .
                                                       As a result, the discriminant criterion (6) can be further
for (6): 1) Calculate the orthonormal basis of Sφ(0);2)                                         
                                            t          transformed into the projection space of Sφ (0) Sφ (0) by
                              Sφ       Sφ                                                  T       W
Calculate the orthonormal basis of W (0) in t (0) to con- z    zT VT PT Sφ PVz    zT VT S Vz  z
                                                      JF ( )=           B      =        B   (    =1).Let
      Sφ     Sφ                                                                             T
struct T (0)   W (0); 3) Calculate the optimal discrimi- zl (l =1,...,r) be the eigenvectors of V SBV,sortedin
nant vectors with respect to the discriminant criterion (6) in
                                                      descending order of the corresponding eigenvalues. Accord-
Sφ      Sφ                                             ing to [Dai and Qian, 2004; Zheng et al., 2005], it is clear
 T (0)   W (0).
                                                       that wl = PVzl  (l =1,...,r) constitute the optimal dis-
  In what follows, we will present the detailed computation criminant vectors with respect to the corresponding criterion
procedure. From the discussions above, we ﬁrst need to calcu-     
                                                            Sφ      Sφ                      x
                        Sφ                      Sφ     (6) in T (0)  W (0). For an input pattern , its correspond-
late the orthonormal basis of t (0) by applying KPCA on t . ing nonlinear feature vector extracted by the KDA procedure
         Sφ                                                                                              r
Moreover, t can be rewritten as:                       described above can be computed as y = 
W,φ(x)∈R  ,
                                                       where W  =(w1,...,wr). This expression can be rewritten
                     C
               φ   1                T                  via the kernel trickp as follows:
              S  =       Φ ΦT =  ΞΞ  ,           (7)    y = W,φ(x) =   1/N (z ,...,z )T VT ET (I − 1/N )T k .
               t   N      i i                                                 1     r                   x
                      i=1
                                                        Here, 1 is an N × N matrix with all terms being one, I is the
               x1  − mφ        xNi  − mφ       Ξ                        k       x x1       x  xNC  T
where Φi =[φ(   i )     ,...,φ( i )      ] and    =    identity matrix, and x =(k( , 1),...,k( , C )) .
                                                IJCAI-07
                                                   7474   Experimental Results                               is capable of improving the overall performance for both ker-
Gene expression data are usually high-dimensional involving nel functions on all three data sets due to its advantages as
a large number of genes but a small sample size. Thus, how to discussed above; specially, BKDA can effectively improve the
effectively extract discriminant features plays a very important performance of the corresponding KDA-r on the same num-
role in gene expression data classiﬁcation [Ye et al., 2004].To ber of features. In addition, we ﬁnd that when s is ﬁxed at
evaluate the performance of the proposed BKDA algorithm, very small values, BKDA fails to show its effectiveness when
we conduct some gene expression classiﬁcation experiments compared with KDA* since the ensemble-based learner is too
to compare BKDA with some other dimensionality reduction weak.
methods.

                                                        100                       100
  Since the non-balanced problems mentioned above only ex-                BKDA: Training            BKDA: Training
                                                                          BKDA: Test                BKDA: Test
                                                                          KDA−r                     KDA−r
ist in multi-class classiﬁcation problems, our experiments are            KDA*                      KDA*
                                                         95                        95
performed on three different data sets involving more than two
classes each in order to demonstrate the behavior of BKDA:2 90                     90

 1. 11 Tumors: 174 human tumor examples corresponding    85                        85
                                                        Classification  accuracy rate (%)
    to 11 different cancer types;                                                 Classification  accuracy rate (%)
                                                         80                        80
 2. A subset A of 14 Tumors: contains all 218 human tumor
                                                         75                        75
                                                         0    20  40  60   80  100  0   20  40   60  80  100
                                                                 the T  values in BKDA    the T  values in BKDA
    examples corresponding to 14 different cancer types;          max                       max
                                                                   (a)                       (b)
 3. A subset B of 14 Tumors: contains all examples cor-
                                                        100                       100
                                                                         BKDA: Training             BKDA: Training
    responding to various human tumor and normal tissue                  BKDA: Test                 BKDA: Test
                                                        95               KDA−r     95               KDA−r
    types, each of which contains at least eight examples.               KDA*                       KDA*
                                                        90                         90

  Each data set is randomly partitioned into disjoint training 85                  85

                                                        80                         80
and test sets. For the training set X , each class Xi contains
L examples. Since the ﬁrst data set is less challenging than 75                    75

                                                        70                        Classification  accuracy rate (%) 70
the second and third data sets, we set L =5for the ﬁrst Classification  accuracy rate (%)
data set and L =7for the other two. Moreover, in our ex- 65                        65
                                                        60                         60
                                                         0    20  40  60   80  100  0   20  40   60  80  100
                                                                the T  values in BKDA      the T  values in BKDA
periments, no preprocessing such as gene selection is applied     max                       max
on the data sets. For each feature extraction method, we use       (c)                       (d)

a simple and efﬁcient minimum mean distance rule with Eu- 100                     100
                                                                        BKDA: Training              BKDA: Training
                                                                        BKDA: Test                  BKDA: Test
                                                        95                         95               KDA−r
clidean distance measure to assess the classiﬁcation accuracy,          KDA−r                       KDA*
                                                                        KDA*
and simultaneously build the corresponding normalized ver- 90                      90
sion for the hypothesis in BKDA based on [Lu et al., 2003a]. 85                    85
Each experiment is repeated 10 times and the average classiﬁ- 80                   80
cation rate is reported. For the kernel methods, we use the RBF 75                 75
                            2                           70                        Classification  accuracy rate (%) 70
kernel k(z1, z2)=exp(z1 − z2 /σ) and polynomial kernel Classification  accuracy rate (%)
                       2             12                 65                         65
  z1 z2     zT z2       where         . In addition, it
k(  ,  )=(   1  /σ +1)        σ =10                     60                         60
                                                         0   20   40  60   80  100  0   20  40   60  80  100
                                                                                           the T  values in BKDA
                                                               the T  values in BKDA        max
should be noted that BKDA can effectively boost the discrim-      max
ination ability of the different features extracted by KDA in      (e)                       (f)
BKDA. To reduce the computational cost and simultaneously Figure 2: Comparative performance of BKDA and KDA under dif-
consider the space limitation in this paper, we simply ﬁx the ferent Tmax values for BKDA. BKDA:Training and BKDA:Test de-
number of features r in BKDA to C − 3 for each data set, note the results of BKDA on the training set X and test set, re-
where C is the number of classes.                      spectively. (a) Polynomial kernel on 11 Tumors; (b) RBF kernel
  For comparison, the ﬁrst set of experiments implements on 11 Tumors; (c) Polynomial kernel on subset A of 14 Tumors;
               [               ]                       (d) RBF kernel on subset A of 14 Tumors; (e) Polynomial kernel
BKDA and KDA    Zheng et al., 2005 using both the polyno- on subset B of 14 Tumors; (f) RBF kernel on subset B of 14 Tumors.
mial and RBF kernels above, since KDA [Zheng et al., 2005]
can effectively calculate the signiﬁcant discriminant informa-
tion in the null space of the within-class scatter operator and The second experiment compares BKDA with several ef-
the proposed method [Ye et al., 2004] for gene expression fective linear dimensionality reduction methods, including
data classiﬁcation is in essence a special case of KDA in high- BDLDA [Lu et al., 2003a],LPP[He and Niyogi, 2004],and
dimensional spaces. Furthermore, to explicitly show the effec- NPE [He et al., 2005], and several other effective kernel-
tiveness of BKDA, in each comparison, KDA offers two base- based nonlinear dimensionality reduction methods, including
lines: KDA* denotes the maximum classiﬁcation rate over the KPCA [Sch¨olkopf et al., 1999],GDA[Baudat and Anouar,
variant features; KDA-r denotes the classiﬁcation rate based 2000], KDDA [Lu et al., 2003b],WKDA[Dai and Yeung,
on r ﬁxed features. In addition, in BKDA, the number of cho- 2005], and KDA/QR [Xiong et al., 2005]. Table 1 reports the
sen examples per class is set to s = L−1,whereL is the num- maximum classiﬁcation rates of different methods on the three
ber of examples for each class in the training set. The experi- data sets. We can see that BKDA is generally more effective
mental results shown in Fig. 2 reveal that: as expected, BKDA than all other methods compared here.
                                                         Notice that there has been extensive research based on clas-
   2All data sets are available at http://discover1.mc.vanderbilt.edu siﬁers for tissue classiﬁcation of gene expression data [Stat-
/discover/public/mc-svm/.                              nikov et al., 2005], including k-nearest neighbor classiﬁer

                                                IJCAI-07
                                                   748