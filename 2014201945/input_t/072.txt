            Distributed Clustering Based on Sampling Local Density Estimates 

            Matthias Klusch Stefano Lodi Gianluca Moro 
  Deduction and Multiagent Systems Department of Electronics, Department of Electronics, 
        German Research Centre Computer Science and Systems Computer Science and Systems 
        for Artificial Intelligence University of Bologna University of Bologna 
         Stuhlsatzenhausweg 3 Viale Risorgimento 2 Via Rasi e Spinelli, 176 
    66123 Saarbruecken, Germany 1-40136 Bologna BO, Italy 1-47023 Cesena FC, Italy 
          klusch@dfki.de slodi@deis.unibo.it gmoro@deis.unibo.it 

                        Abstract                               tributed data into a data warehouse on which to apply the 
                                                               usual data mining techniques. Data warehousing is a popular 
     Huge amounts of data are stored in autonomous, 
                                                               technology which integrates data from multiple data sources 
     geographically distributed sources. The discov•
                                                               into a single repository in order to efficiently execute complex 
     ery of previously unknown, implicit and valuable 
                                                               analysis queries [Moro and Sartori, 2001]. However, despite 
     knowledge is a key aspect of the exploitation of 
                                                               its commercial success, this approach may be impractical or 
     such sources. In recent years several approaches 
                                                               even impossible for some business settings, for instance: 
     to knowledge discovery and data mining, and in 
     particular to clustering, have been developed, but          • when huge amounts of data are (frequently) produced at 
     only a few of them are designed for distributed data           different sites and the cost for their centralization cannot 
     sources. We propose a novel distributed clustering             scale in terms of communication, storage and computa•
     algorithm based on non-parametric kernel density               tion; 
     estimation, which takes into account the issues of          • whenever data owners cannot or do not want to release 
     privacy and communication costs that arise in a dis•           information, for instance to protect privacy or because 
     tributed environment.                                          disclosing such information may result in a competitive 
                                                                    advantage or a considerable commercial added value. 
1 Introduction                                                   One of the most studied data mining techniques in central•
                                                               ized environments is data clustering. The goal of this tech•
Knowledge discovery is a process aiming at the extraction 
                                                               nique is to decompose or partition a data set into groups such 
of previously unknown and implicit knowledge out of large 
                                                               that both intra-group similarity and inter-group dissimilarity 
databases, which may potentially be of added value for some 
                                                               are maximized. Despite the success of data clustering in cen•
given application [Fayyad et al., 1996]. 
                                                               tralized environments, only a few approaches to the problem 
   Data mining, which is devoted to the automated extrac•
                                                               in a distributed environment are available to date. 
tion of unknown patterns from given data, is a central element 
                                                                 In this work we present KDEC, a novel approach to dis•
among the steps of the overall knowledge discovery process; 
                                                               tributed data clustering based on sampling density estimates. 
the steps include preparation of the data to be analyzed as well 
                                                               In KDEC each data source transmits an estimate of the prob•
as evaluation and visualization of the discovered knowledge. 
                                                               ability density function of its local data to a helper site, and 
The large variety of data mining techniques which have been 
                                                               then executes a density based clustering algorithm that is 
developed over the past decade include: methods for pattern-
                                                               driven by the overall density estimate, which is built by the 
based similarity search, cluster analysis, decision-tree based 
                                                               helper from the samples of the local densities. 
classification, generalization taking the data cube or attribute-
                                                                 The paper is organized as follows. In Section 2 we de•
oriented induction approach, and mining of association rules 
                                                               scribe related work and highlight differences with respect to 
[Chen et al. , 1996] 
                                                               our approach. Section 3 and 4 present the KDEC scheme to 
   The increasing demand to scale up to massive data sets 
                                                               distributed data clustering. Finally, Section 5 concludes the 
which are inherently distributed over networks with limited 
                                                               paper and outlines ongoing and future research work. 
bandwidth and computational resources has led to methods 
for parallel and distributed knowledge discovery [Kargupta 
et al., 2000], The related pattern extraction problem in dis•  2 Related work 
tributed knowledge discovery is referred to as distributed data In [Johnson and Kargupta, 1999] a tree clustering approach is 
mining. Distributed data mining is expected to perform par•    taken to build a global dendrogram from individual dendro•
tial analysis of data at individual sites and then to send the grams that are computed at local data sites subject to a given 
outcome as partial result to other sites where it is sometimes set of requirements. In contrast to the approach presented in 
aggregated to the global result.                               this paper, the distributed data sets are assumed to be hetero•
   One of the most common approaches of business applica•      geneous, therefore every site has access only to a subset of 
tions to perform distributed data mining is to centralize dis• the features of an object. The proposed solution implements 


LEARNING                                                                                                             485  a distributed version of the single-link clustering algorithm of density estimation based clustering. 
 which generates clusters that are substantially different from 
 the ones generated by density-based methods. In particular,   3.2 Density estimation based clustering 
 it suffers from the so-called chaining effect, by which any   In density estimation (DE) based clustering the search for 
 of two well separated and internally homogeneous groups of    densely populated regions is accomplished by estimating a 
 objects connected only by a dense sequence of objects are re• so-called probability density function from which the given 
 garded as a single cluster. [Kargupta et al, 2001] proposes a data set is assumed to have arisen. Many techniques for 
 technique for distributed principal component analysis, Col•  DE-based clustering are available from the vast KDD liter•
 lective PCA. It is shown that the technique satisfies efficiency ature [Ankerst et al, 1999; Ester et al, 1996; Schikuta, 1996; 
and data security requirements and can be integrated with ex•  Hinneburg and Keim, 1998] and statistics [Silverman, 1986]. 
isting clustering methods in order to cluster distributed, high- In both areas, the proposed clustering methods require the 
dimensional heterogeneous data. Since the dimensionality       computation of a non-parametric estimation of the density 
of the data is reduced prior to clustering by applying PCA,    function from the data. One important family of non-
the approach is orthogonal to ours. Another related research   parametric estimates is known as kernel estimators. The idea 
direction deals with incremental clustering algorithms. The    is to estimate a density function by defining the density at 
BIRCH iZhang et a/., 1996] and related BUBBLE method           any data object as being proportional to a weighted sum of 
[Ganti et al, 1999], compute the most accurate clustering,     all objects in the data set, where the weights are defined by 
given the amount of memory available, while minimizing the     an appropriately chosen kernel function. In the following 
number of I/O operations. It uses a dynamic index struc•       we introduce kernel-based density estimation [Parzen, 1962; 
ture of nodes that store synthetic, constant-time maintainable Silverman, 1986] and our approach to density estimation 
summaries of sets of data objects. The method is sufficiently  based clustering. 
scalable requiring 0(N\ogN) time and linear I/O. However,        Let us assume a set of data 
since it uses the centroid to incrementally aggregate objects, points or objects. Kernel estimators originate from the in•
the method exhibits a strong bias towards globular clusters.   tuition that the higher the number of neighbouring data ob•
IncrementalDBSCAN LEster et a/., 1998] is a dynamic clus•      jects x; of some given object , the higher the density 
tering method supporting both insertions and deletions, which  at this object x. However, there can be many ways of cap•
is shown to be equivalent to the well-known static DBSCAN      turing and weighting the influence of data objects. When 
algorithm. Since in turn DBSCAN can be shown to be equiv•      given the distance between one data object x and another Jc, 
alent to a method based on density estimation when the kernel  as an argument, the influence of jf; may be quantified by us•
function is the square pulse and the clusters are density-based, ing a so called kernel function. A kernel function K (x) is 
IncrementalDBSCAN is less general than methods based on        a real-valued, non-negative function on R which has finite 
kernel density estimates. Its time complexity is 0(N\ogN).     integral over R. When computing a kernel-based density es•
                                                               timation of the data set 5, any element jf,- in S is regarded 
3 Data Clustering                                              as to exert more influence on some than elements 
                                                               which are farther from than the element. Accordingly, 
3.1 The cluster analysis problem                               kernel functions are often non-increasing with . Promi•
Cluster analysis is a a descriptive data mining task which     nent examples of kernel functions are the square pulse func•
aims at decomposing or partitioning a usually multivariate     tion and the Gaussian function 
data set into groups such that the data objects in one group 
are similar to each other and are different as possible from 
                                                                 A kernel-based density estimate is 
those in other groups. Therefore, a clustering algorithm J?(-) 
                                                               defined, modulo a normalization factor, as the sum over all 
is a mapping from any data set S of objects to a clustering of 
                                                               data objects x  in S of the distances between scaled by 
5, that is, a collection of pairwise disjoint subsets of 5. Clus•           i
                                                               a factor A, called window width, and weighted by the kernel 
tering techniques inherently hinge on the notion of distance 
                                                               function K: 
between data objects to be grouped, and all we need to know 
is the set of interobject distances but not the values of any of 
the data object variables. Several techniques for data cluster•                                                       (1) 
ing are available but must be matched by the developer to the 
objectives of the considered clustering task [Grabmeier and      The influence of data objects and the smoothness of the 
Rudolph, 2002]. In partition-based clustering, for example,    estimate is controlled by both the window width h and the 
the task is to partition a given data set into multiple disjoint shape of kernel K\ h controls the smoothness of the estimate, 
sets of data objects such that the objects within each set are whereas K determines the decay of the influence of a data 
as homogeneous as possible. Homogeneity here is captured       object according to the distance. Even if the number N of data 
by an appropriate cluster scoring function. Another option     objects is very large, in practice it is not necessary to compute 
is based on the intuition that homogeneity is expected to be   N distances for calculating the kernel density estimate at a 
high in densely populated regions of the given data set. Con•  given object x. In fact, the value of commonly used kernel 
sequently, searching for clusters may be reduced to searching  functions is negligible for distances larger than a few h units; 
for dense regions of the data space which are more likely to 
be populated by data objects. That leads us to the approach 


486                                                                                                           LEARNING it may even be zero if the kernel has bounded support, as it 
is the case, for example, for the square pulse. Using kernel-
based density estimation, it is straightforward to decompose 
the clustering problem into three phases as follows. 
  1. Choose a window width h and a kernel function K. 
  2. Compute the kernel-based density estimate 
     from the given data set. 
  3. Detect regions of the data space where the value of the 
     estimate is high and group all data objects of space re•
     gions into corresponding clusters. 
   In the literature, many different definitions of cluster have 
been proposed formalizing the clusters referred to in step 3 
above. A density-based cluster [Ester el al, 1996] collects 
all data objects included in a region where density exceeds 
a threshold. Center-defined clusters [Hinneburg and Kcim, 
 1998] are based on the idea that every local maximum of 
corresponds to a cluster including all data objects which can 
be connected to the maximum by a continuous, uphill path in 
the graph of (p. Finally, an arbitrary-shape cluster LHinneb-
urg and Keim, 1998] is the union of center-defined clusters 
having their maxima connected by a continuous path whose 
density exceeds a threshold. 
   Algorithm 1 (DE-cluster) implements the computation of 
center-defined clusters by a climbing procedure driven by the 
density estimate. The main procedure is DECluster, taking 
as inputs an instance S of the class of data objects, the kernel 
function K, the window width h, and returning a clustering 
represented by C, which stores a mapping from each x, to       equipping the class of S with a spatial access method like the 
                                                               KD-, or MVP-, or M-tree. Therefore, the time complexity of 
the unique integer label ofxif,'s cluster. It is assumed that S 
is an instance of a class which provides the following meth•   DEClusler is 0(Nq(N)), where q(N) is the cost of a k near•
ods: get(i) to access object x, given index i, NQ(k,x) and     est neighbour query in any such access method. Note that in 
Radius to retrieve, given the indexes and maxi•                many practical cases, q(N) is very close to log TV. 
mum distance of. nearest neighbours. Uphill computes 
the steepest direction on the graph of the estimated density as 4 Distributed Data Clustering 
the versor of its gradient, computed by function DEGradient    The body of work on applications of data clustering in dis•
(cf. [Hinneburg and Keim, 1998]). Uphill then moves in that    tributed environments, the problem of so called distributed 
direction a fractionS . _ of the distance S.Radius of the      data clustering (DDC), is comparatively small. In this sec•
k th nearest neighbour off, and finally returns the index of the tion we adopt the kernel density estimation based clustering 
nearest neighbour in S of the reached position. Every nested   approach presented above for the distributed case assuming 
call to FixedPoint marks the current object x, as visited and  homogeneous data, which means that a data object cannot be 
calls Uphill to get the index of the next data object Xj. If   split across two sites. 
such object has already been visited, the proximity of a local 
maximum has been reached and j is taken as new cluster la•     4.1 The DDC Problem 
bel. Otherwise is inductively assumed to lie at the bottom 
                                                               We define the problem of homogeneous distributed data clus•
end of a path leading to the proximity of a local maximum, 
                                                               tering for a clustering algorithm A as follows. Let 
and to be already labeled accordingly. (If is not marked as 
clustered, a recursive call ensures that the assumption holds.)                    be a data set of objects. Let 
   The complexity of the DE-cluster algorithm is that of call• 1,..., M, be a finite set of sites. Each site stores one data 
ing N — S.count times FixedPoint. At the beginning of every    set Dj, and it will be assumed that . The DDC 
iteration in DECluster, the sets of clustered and visited ob•  problem is to find a site clustering Cj residing in the data 
jects are equal. FixedPoint is never called with a clustered   space of L j, for such that 
object as argument, and visits unclustered objects at most      (i). (correctness requirement) 
once. Therefore, even if the number of visited data objects 
                                                               (ii). Time and communications costs are minimized (effi•
in one call of FixedPoint is bounded only by N, the number 
                                                                    ciency requirement) 
of visited data objects in all calls is still only N. For each 
visited object a single K-nearest neighbour query suffices to (iii). At the end of the computation, the size of the subset of S 
compute the gradient and the next uphill object. The methods        which has been transferred out of the data space of any 
                          can be efficiently implemented by         site is minimized (privacy requirement). 


LEARNING                                                                                                              487  The traditional solution to the homogeneous distributed data  is immediate to see by (2) that addivity holds for the sampled 
 clustering problem is to simply collect all the distributed data forms: 
 sets Dj into one centralized repository where their union S 
 is computed, and the clustering C of the union S is com•                                                              (4) 
 puted and transmitted to the sites. Such approach, however, 
 does not satisfy our problem's requirements both in terms of 
 privacy and efficiency. We therefore propose a different ap•  Therefore, after receiving the sampled forms of the 
 proach yielding a kernel density estimation based clustering  M density estimates, the helper site can compute by (4) the 
 scheme, called KDEC.                                          sampled form of the overall estimate and transmit it to the 
                                                               sites Lj. Sites Lj can then cluster local data with respect to the 
 4.2 The KDEC Scheme for DDC                                   overall density estimate, using the gradient of the sampling 
 The key idea of the KDEC scheme is based on the follow•       series 
 ing observation: Although the density estimate computed on 
 each local data set gives information on the distribution of 
 the objects in the data set, it conceals the objects themselves.                                                      (5) 
 Moreover, the local density estimate can be coded to provide 
 a more compact representation of the data set for the purpose 
of transmission. In the sequel, we tacitly assume that all sites 
Lj agree on using a global kernel function K and a global 
 window width h. We will therefore omit K and h from our 
notation, and write . Density estimates                        as needed in the hill-climbing function. 
 in the form of Equation (1) are additive, i.e. the global den•  We briefly discuss the extent to which the series (5) can 
                                                               be used to represent It is well known that, under 
sity estimate can be decomposed info the sum of the 
                                                               mild conditions, sampling a function is an invertible 
site density estimates, one estimate for every data set Dj. 
                                                               transformation if, for every coordinate i = 1,... n, there is 
                                                        (2)    a frequency such that the Fourier transform of g differs 
                                                               from zero only in the interval , and the samples 
Thus, the local density estimates can be transmitted to and    are computed with a period not greater than 
summed up at a distinguished helper site yielding the global   (cf. [Higgins, 1996]). Under these assumptions, the value of 
estimate which can be returned to all sites. Each site Lj may  the sampling series computed at x equals g(x). Unfortunately, 
then apply, in its local data space, the hill-climbing technique most popular kernel functions (hence summations of kernel 
of Algorithm 1 (DE-cIuster) to assign clusters to the local data functions) do not satisfy these hypotheses since the support 
objects. There is nevertheless a weakness in such a plan: the  of their Fourier transform is unbounded. Consequently sam•
definition of a density estimate explicitly refers to all the data pling density estimates yields an information loss. However, 
objects J?;. Hence, knowing how to manipulate the estimate     it can be shown that the Fourier transform of a kernel density 
entails knowing the data objects, which contradicts the pri•   estimate is negligible everywhere except for not greater 
vacy requirement. However, only an intensional, algebraic      than Therefore, the global density estimate can be re•
definition of the estimate includes knowledge of the data ob•  constructed from its samples by (5) introducing only a small 
jects. Multidimensional sampling theory provides the basis     error if 
for an alternative extensional representation of the estimate    It is worth noting that the infinite series (5) need not be ap•
which makes no explicit reference to the data objects.         proximated, if it has finitely many nonzero terms. The latter 
   The theoretical idea of sampling is to represent a function case holds if the used kernel function has a bounded support, 
/ by a sampling series, that is, a summation of suitable ex•   since the density estimate will also have bounded support. If, 
pansion functions weighted by the values of / at a discrete    however, the kernel function has unbounded support, like the 
subset of its domain [Higgins, 1996]. In the following, let the Gaussian kernel, then the density estimate can be approxi•
/th coordinate of be denoted by and let Diag                   mated by regarding its value to be zero everywhere except 
  = , denote th< diagonal matrix                               inside an appropriately chosen bounded region. 
having diagonal w, i.e., defined by if According to this approach, we propose the following al•
Diag Further let' a vector                                     gorithmic KDEC scheme for computing the kernel density es•
of sampling periods. The sampled form of at intervals          timation based clusters for local data spaces at M distributed 
  is the sequencedefined by                                    data sites Lj (see Algorithm 2). Every local site runs the pro•
                                                               cedure SiteDECluster whereas the helper site runs Helper. 
                                                        (3) 
                                                               SiteDECluster is passed a reference H to the helper and the 
where • is the inner product between vectors. Therefore,       local data set D, and returns a clustering in a class instance 
is the sequence of the values of at all the real, n-           C. Helper is passed a list of references L to the local sites. 
dimensional vectors whose i th coordinates are spaced by a     The procedure SiteNegotiate carries out a negotiation with 
multiple of the i th sampling period Ti ,1=1,...,/!. The sam•  the other local sites through HelperNegotiate at the helper 
pled forms of the local density estimates are defined in a sim• site to determine the sampling periods x, the bounding cor•
ilar way by ...,M. It                                          ners of the sampling rectangle , the kernel K, and 


488                                                                                                           LEARNING                                                                vances a fraction 5 of the gradient in its direction, if the gradi•
                                                               ent's norm exceeds a threshold e. If a of the 
                                                               object returmed by Uphill contains an already clustered data 
                                                               object, the current cluster label Id is set from that object's la•
                                                               bel. Otherwise, checking whether Uphill returned the same 
                                                               space object signals to FixedPoint that the proximity of the 
                                                               local maximum has been reached. The maximum is marked 
                                                               by adding the current space object x as a dummy object to the 
                                                               local data set; this ensures that subsequent paths converging 
                                                               to the same local maximum will use the same cluster label 
                                                               as the current path. Method D.Add{) returns the identifier 
                                                               of the added object, which is used as current cluster label. If 
                                                               neither case holds, the label Id is obtained by a recursive call. 
                                                               Finally, all objects in a small neighbourhood of the current 
                                                               object are labeled by Id. Note that adding dummy objects 
                                                               has eflect only on the range queries, and does not modify the 
                                                               density estimate. 

                                                               4.3 Complexity of the KDEC scheme 
                                                               In terms of the complexity in computation and communica•
                                                               tion one crucial point of the KDEC scheme is how many sam•
                                                               ples have to be computed and transferred among the sites. In 
                                                               most cases, to obtain good density estimates, h must not be 
                                                               less than a small multiple of the smallest object distance. As 

                                                               Ti ~ h/2, the number of samples should rarely exceed the 
                                                               number of objects, if only space regions where the density 
                                                               estimate is not negligible are sampled. Since the size of a 
                                                               sample is usually much smaller than the size of a data ob•
                                                               ject, the overall communication costs of our DDC approach 
                                                               will be in most cases significantly lower than in a centralized 
                                                               approach. Of course, the precise number of samples depends 
                                                               on the bounding region that is being sampled by every site. In 

                                                               Algorithm 2 the site Lj determines autonomosly the rectangle 
                                                               that contains the computed samples. 
                                                                 The computational costs of the KDEC scheme in terms of 
                                                               used CPU cycles and I/O do not exceed the one in the cen•
                                                               tralized approach where clustering is performed on data col•
                                                               lected in a single repository. The computational complexity is 
                                                               linear in the number of samples. The precise cost of compu•
                                                               tation of any KDEC-based DDC algorithm as an instance of 
                                                               the proposed scheme largely depends also on the used kernel 
the window width h. The negotiation procedures contain ap•     function and local clustering algorithm. The DE-cluster algo•
propriate handshaking primitives to ensure that all sites par• rithm we developed for the KDEC scheme in Section 3.2 is 
ticipate and exit negotiations only if an agreement has been   of complexity 0(Nq(N)), where q(N) is the cost of a nearest 
reached. Each local site computes the sampled form of the      neighbour query (which in practical cases is close to logjV). 
estimate of D, by calling function Sample, and sends it to     Algorithm 2 implements a slightly different approach in the 
the helper. (Function DE computes the density estimate and     hill-climbing function than Algorithm 1, since the function 
is omitted for brevity.) The helper receives the sampled es•   does not use data objects to direct the uphill path. How•
timates, sums them by sampling indexes into a global sam•      ever, preliminary results of experiments conducted on a pro•
ple, and returns it to all sites. Procedures Send and Receive  totype implementation show good scalability of the approach, 
implement appropriate blocking and handshaking to ensure       in terms of number of executed range queries. 
the transmission takes place. Each local site uses the global 
sample in functions FixedPoint and Uphill to compute the       5 Conclusion 
values of the gradient of the global density estimate. Func•
tion SeriesGradient can be easily derived from (5). Local      Due to the explosion in the number of autonomous data 
sites perform a DE-cluster algorithm to compute the corre•     sources there is a growing need for effective approaches to 
sponding local data clusters. The details of the hill-climbing distributed knowledge discovery and data mining. In this pa•
strategy are however different from Algorithm 1 because the    per we have presented KDEC, a novel scheme for distributed 
sites are allowed access to local data objects only. Uphill ad• data clustering which computes the density estimation, to per-


LEARNING                                                                                                             489 