                         Simultaneous Adversarial Multi-Robot Learning 

                                     Michael Bowling and Manuela Veloso 
                                           Computer Science Department 
                                             Carnegie Mellon University 
                                             Pittsburgh PA, 15213-3891 


                        Abstract                               equilibria. An equilibrium is simply a policy for all of the 
                                                               players where each is playing optimally with respect to the 
     Multi-robot learning faces all of the challenges of 
                                                               others. This concept is a powerful solution for these games 
     robot learning with all of the challenges of mul•
                                                               even in a learning context, since no agent could learn a better 
     tiagent learning. There has been a great deal of 
                                                               policy when all the agents are playing an equilibrium. 
     recent research on multiagent reinforcement learn•
                                                                 Multiagent learning in stochastic games, thus far, has only 
     ing in stochastic games, which is the intuitive ex•
                                                               been applied to small games with enumerable state and action 
     tension of MDPs to multiple agents. This recent 
                                                               spaces. Robot learning tasks though have continuous state 
     work, although general, has only been applied to 
                                                               and action spaces, and typically with more than just a couple 
     small games with at most hundreds of states. On 
                                                               dimensions. Discretizations of this space into an enumerable 
     the other hand robot tasks have continuous, and of•
                                                               state set also do not typically perform well. In addition, data 
     ten complex, state and action spaces. Robot learn•
                                                               is considerably more costly to gather, and millions of training 
     ing tasks demand approximation and generaliza•
                                                               runs to learn policies is not feasible. Typical solutions to this 
     tion techniques, which have only received exten•
                                                               robot learning problem is to use approximation to make the 
     sive attention in single-agent learning. In this paper 
                                                               learning tractable and generalization for more efficient use of 
     we introduce GraWoLF, a general-purpose, scal•
                                                               training experience. 
     able, multiagent learning algorithm. It combines 
                                                                 In this paper we introduce a new algorithm, GraWoLF     , 
     gradient-based policy learning techniques with the                                                                 1
                                                               that combines approximation and generalization techniques 
     WoLF ("Win or Learn Fast") variable learning rate. 
                                                               with the WoLF multiagent learning technique. We show em•
     We apply this algorithm to an adversarial multi-
                                                               pirical results of applying this algorithm to a problem of si•
     robot task with simultaneous learning. We show re•
                                                               multaneous learning in an adversarial robot task. In Section 2, 
     sults of learning both in simulation and on the real 
                                                               we give a brief overview of key concepts from multiagent 
     robots. These results demonstrate that GraWoLF 
                                                               learning along with the the formal model of stochastic games. 
     can learn successful policies, overcoming the many 
                                                               In Section 3, we describe a particular adversarial robot task 
     challenges in multi-robot learning. 
                                                               and its challenges for learning. In Section 4, we present the 
                                                               main components of GraWoLF: policy gradient ascent and 
1 Introduction                                                 the WoLF variable learning rate. In Section 5, we present 
                                                               experimental results of applying this algorithm to our adver•
Multi-robot learning is the challenge of learning to act in 
                                                               sarial robot task, and then conclude. 
an environment containing other robots. These other robots, 
though, have their own goals and may be learning as well. 
Other adapting robots make the environment no longer sta•      2 Multiagent Learning 
tionary, violating the Markov property that traditional single-
                                                               Multiagent learning has focused on the game theoretic frame-
agent behavior learning relies upon. Multi-robot learning 
                                                               work of stochastic games. A stochastic game is a tuple 
combines all of these multiagent learning challenges with the 
                                                               (n, S, A1...ni T, R1...n), where n is the number of agents, S is 
problems of learning in robots, such as continuous state and 
                                                               a set of states, A1 is the set of actions available to agent i (and 
action spaces and minimal training data. 
                                                               A is the joint action space A1 x ... x ,4n), T is a transition 
   A great deal of recent work on multiagent learn•
                                                               function S x AxS -+ [0,1], and R1 is a reward function for 
ing has looked at the problem of learning in stochastic        the ith agent S x A —R. This looks very similar to the MDP 
games [Littman, 1994; Singh et a/., 2000; Bowling and          framework except we have multiple agents selecting actions 
Veloso, 2002a; Greenwald and Hall, 2002]. Stochastic games     and the next state and rewards depend on the joint action of 
are a natural extension of Markov decision processes (MDPs)    the agents. Another important difference is that each agent 
to multiple agents and have been well studied in the field of 

game theory. The traditional solution concept for the prob•       1 GraWoLF is short for "Gradient-based Win or Learn Fast", and 
lem of simultaneously finding optimal policies is that of Nash the V has the same sound as in "gradient". 


MULTIAGENT SYSTEMS                                                                                                    699  has its own separate reward function. The goal for each agent 
 is to select actions in order to maximize its discounted future 
 rewards with discount factor  
   Stochastic games are a very natural extension of MDPs to 
 multiple agents. They are also an extension of matrix games 
 to multiple states. Each state in a stochastic game can be 
 viewed as a matrix game with the payoffs for each joint action 

 determined by the matrices Rt{s, a). After playing the matrix 
 game and receiving their payoffs the players are transitioned 
 to another state (or matrix game) determined by their joint 
 action. We can see that stochastic games then contain both 
 MDPs and matrix games as subsets of the framework. 

 Stochastic Policies. Unlike in single-agent settings, deter•
ministic policies in multiagent settings can often be exploited 
by the other agents. Consider the children's game rock-paper-
                                                               Figure 1: An adversarial robot task. The top robot is trying to 
scissors. If one player were to play any action determinis-
                                                               get inside the circle while the bottom robot is trying to stop 
tically, the other player could win every time by selecting 
                                                               it. The lines show the state and action representation, which 
the action that defeats it. This fact requires the considera•
                                                               is described in Section 4.3. 
tion of mixed strategies and stochastic policies. A stochastic 
policy, is a function that maps states to 
mixed strategies, which are probability distributions over the other players' possibly changing policies. This approach, too, 
player's actions. We later show that stochastic policies are   has a strong connection to equilibria. If these algorithms con•
also useful for gradient-based learning techniques.            verge when playing each other, then they must do so to an 
                                                               equilibrium [Bowling and Veloso, 2002a]. 
                                                                 Neither of these approaches, though, have been scaled be•
Nash Equilibria. Even with the concept of stochastic poli•     yond games with a few hundred states. Games with a very 
cies there are still no optimal policies that are independent of large number of states, or games with continuous state spaces, 
the other players' policies. We can, though, define a notion of make state enumeration intractable. Since previous algo•
best-response. A policy is a best-response to the other play•  rithms in their stated form require the enumeration of states 
ers' policies if it is optimal given their policies. The major ad• either for policies or value functions, this is a major limita•
vancement that has driven much of the development of matrix    tion. In this paper we examine learning in an adversarial robot 
games, game theory, and even stochastic games is the notion    task, which can be thought of as a continuous state stochas•
of a best-response equilibrium, or Nash equilibrium [Nash,     tic game. Specifically, we build on the idea of best-response 
Jr., 1950].                                                    learners using gradient techniques [Singh et ai, 2000; Bowl•
   A Nash equilibrium is a collection of strategies for each of ing and Veloso, 2002a]. We first describe our robot task and 
the players such that each player's strategy is a best-response then describe our algorithm and results. 
to the other players' strategies. So, no player can do bet•
ter by changing strategies given that the other players also 
                                                               3 An Adversarial Robot Task 
don't change strategies. What makes the notion of equilib•
rium compelling is that all matrix games and stochastic games  Consider the adversarial robot task shown in Figure 1. The 
have such an equilibrium, possibly having multiple equilib•    robot at the top of the figure, the attacker, is trying to reach 
ria. Zero-sum, two-player games, like the adversarial task     the circle in the center of the field, while the robot closer to 
explored in this paper, have a single Nash equilibrium.2       the circle, the defender, is trying to prevent this from hap•
                                                               pening. If the attacker reaches the circle, it receives a re•
                                                               ward of one and the defender receives a reward of negative 
Learning in Stochastic Games. Stochastic games have 
                                                               one. These are the only rewards in the task. When the at•
been the focus of recent research in the area of reinforcement 
                                                               tacker reaches the circle or ten seconds elapses, the trial is 
learning. There are two different approaches being explored. 
                                                               over and the robots reset to their initial positions, where the 
The first is that of algorithms that explicitly learn equilibria 
                                                               attacker is a meter from the circle and the defender half-way 
through experience, independent of the other players' policy, 
                                                               between. The robots simultaneously learn in this environment 
e.g., [Littman, 1994; Greenwald and Hall, 2002]. These al•
                                                               each seeking to maximize its own discounted future reward. 
gorithms iteratively estimate value functions, and use them 
                                                               For all of our experiments the discount factor used was 0.8 
to compute an equilibrium for the game. A second approach 
                                                               for each full second of delay. 
is that of best-response learners, e.g., [Claus and Boutilier, 
1998; Singh et ai, 2000; Bowling and Veloso, 2002a]. These       The robots themselves are part of the CMDragons robot 
learners explicitly optimize their reward with respect to the  soccer team, which competes in the RoboCup small-size 
                                                               league. There are two details about the robot platform that 
                                                               are relevant to this learning task. First, the learning algorithm 
   2There can actually be multiple equilibria, but they all have equal 
payoffs and are interchangeable.                               itself is situated within a large and complex architecture of 


700                                                                                             MULTIAGENT SYSTEMS existing capability. The team employs a global vision system   stochastic policy according to, 
mounted over the field. This input is processed by an elab•
orate tracking module that provides accurate positions and 
velocities of the robots. These positions comprise the input 
state for the learning algorithm. The team also consists of ro• Sutton and colleagues' main result was a convergence proof 
bust modules for obstacle avoidance and motion control. The    for the following policy iteration rule that updates a policy's 
actions for the learning algorithm then involve providing tar• parameters in a Gibbs distribution according to, 
get points for the obstacle avoidance module. Situating the 
learning within the context of this larger architecture focuses 
the learning. Rather than having the robot learn to solve well 
understood problems like path planning or object tracking,               is an independent approximation of Qn°*(s,a) 
the learning is directed at the heart of the problem, the multi- with parameters w, which is the expected value of taking ac•
robot interaction.                                             tion a from state a and then following the policy 7r^fc. For 
   Second, the system control loop that is partially described the Gibbs distribution, Sutton and colleagues showed that for 
above has inherent, though small, latency. Specifically, after convergence this approximation should have the following 
an observed change in the world 100ms will elapse before the   form, 
robot's response is executed. This latency is overcome for 
each robot's own position and velocity by predicting through                                                          (2) 
this latency using knowledge of the past, but not yet executed, 

actions. Since the actions of the opponent robots are not      As they point out, this amounts to /w being an approximation 
known, this prediction is not possible for other robots. La•   of the advantage function,  
tency effectively adds an element of partial observability to  where is the value of following policy -K from state 
the problem, since the agents do not have the complete state   .s. It is this advantage function that we estimate and use for 
of the world, and in fact have separate views of this state. No• gradient ascent. 
tice, that this also adds a tactical element to successful poli• Using this basic formulation we derive an on-line version 
cies. A robot can "fake" the opponent robot by changing its    of the learning rule, where the policy's weights are updated 
direction suddenly, knowing the other robot will not be able   with each state visited. The summation over states can be re•
to respond to this change for a full latency period.           moved by updating proportionately to that state's contribution 
   This task involves numerous challenges for existing multi-  to the policy's overall value. Since we are visiting states on-
agent learning techniques. These challenges include continu•   policy then we only need to weight later states by the discount 
ous state and action spaces, elements of partial observability factor to account for their smaller contribution. Iff time has 
due to system latency, and violation of the Markov assump•     passed since the trial start, this turns Equation 1 into, 
tion since many of the system components have memory, e.g., 
the tracking and the obstacle avoidance modules. In fact, 
these limitations may even make equilibria cease to exist alto•                           a 
gether [Bowling and Vcloso, 2002b]. This is a further reason     Since the whole algorithm is on-line, we do the policy im•
for exploring best-response learning techniques, which don't   provement step (updating , simultaneously with the value 
directly seek to learn an equilibrium. We now present Gra-     estimation step (updating w). We do value estimation using 
WoLF, a best-response learning algorithm that can handle the   gradient-descent Sarsa i LSutton and Barto, 1998] over the 
challenges of multi-robot learning.                            same feature space as the policy. This requires maintaining 
                                                               an eligibility trace vector, e. The update rule is then, if at 
                                                               time k the system is in state s and takes action a transitioning 
4 GraWoLF                                                      to state s' in time t and then taking action a', we update the 
GraWoLF, or Gradient-based WoLF, combines two key ideas        trace and the weight vector using, 
from reinforcement learning: policy gradient learning and the                                                         (4) 
WoLF variable learning rate. Policy gradient learning is a 
technique to handle intractable or continuous state spaces.                                                           (5) 
WoLF is a multiagent learning technique that encourages 
best-response learning algorithms to converge in situations of where * is the Sarsa parameter and is an appropriately de•
simultaneous learning. We briefly describe these techniques    cayed learning rate. The addition of raising to the power 
and how they are combined.                                     t allows for actions to take differing amounts of time to 
                                                               execute, as in semi-Markov decision processes [Sutton and 
4.1 Policy G radicnt Ascent                                    Barto, 1998]. 
We use the policy gradient technique presented by Sutton and     The policy improvement step then uses Equation 3 where 
colleagues [2000]. Specifically, we define a policy as a Gibbs s is the state of the system at time k and the action-value es•
distribution over a linear combination of features of a candi• timates from Sarsa, , are used to compute the advantage 
date state and action pair. Let 6 be a vector of the policy's  term. We then get, 

parameters and let <j6.sa be an identically sized feature vector 
for state 6' and action a, then the Gibbs distribution defines a 


MULTIAGENT SYSTEMS                                                                                                   701  This forms the crux of GraWoLF. What remains is the selec•    dimensional input space. These seven dimensions are shown 
 tion of the learning rate, This is where the WoLF variable    by the white overlaid lines in Figure 1. 
 learning rate is used.                                           We chose to use tile coding [Sutton and Barto, 1998], also 
                                                               known as CMACS, to construct our feature vector. Tile cod•
 4.2 Win or Learn Fast                                         ing uses a number of large offset tilings to allow for both a 
 WoLF ("Win or Learn Fast") is a method by Bowling and         fine discretization and large amount of generalization. We 
 Veloso [2002a] for changing the learning rate to encourage    use 16 tiles whose size was 800mm in distance dimensions 
 convergence in a multiagent reinforcement learning scenario.  and 1600mm/s in velocity dimensions. We simplify the ac•
 Notice that the policy gradient ascent algorithm above does   tion space by requiring the attacker to select its navigation 
 not account for a non-stationary environment that arises with point along a perpendicular line through the circle's center. 
 simultaneous learning in stochastic games. All of the other   This is shown by the dark overlaid line in Figure 1. This line, 
 agents actions are simply assumed to be part of the envi•     whose length is three times the distance of the attacker to the 
 ronment and unchanging. WoLF provides a simple way to         circle, is then discretized into seven points evenly distributed. 
 account for other agents through adjusting how quickly or     The defender uses the same points for its actions but then nav•
 slowly the agent changes its policy.                          igates to the point that bisects the line from the attacker to the 
   Since only the rate of learning is changed, algorithms      selected action point. The robot's action is also included in 
 that are guaranteed to find (locally) optimal policies in non- the tiling as an eighth dimension with a tile size for that di•
 stationary environments retain this property even when us•    mension of the entire line, so actions are distinguishable but 
 ing WoLF. In stochastic games with simultaneous learning,     there is also generalization. Agents select actions every tenth 
 WoLF has both theoretical evidence (limited to two-player,    of a second, or after every three frames, unless the feature 
 two-action matrix games), and empirical evidence (experi•     vector has not changed over that time. This keeps the robots 
 ments in games with enumerated state spaces) that it en•      from oscillating too much during the initial parts of learning. 
 courages convergence in algorithms that don't otherwise con•
 verge. The intuition for this technique is that a learner should 5 Results 
 adapt quickly when it is doing more poorly than expected. 
 When it is doing better than expected, it should be cautious, Before presenting results on applying GraWoLF to this prob•
 since the other players are likely to change their policy. This lem we first consider some issues related to evaluation. 
 implicitly accounts for other players that are learning, rather 
 than other techniques that try to explicitly reason about the 5.1 Evaluation 
 others' action choices.                                       Evaluation of multi-robot learning algorithms present a num•
   The WoLF principle naturally lends itself to policy gradi•  ber of challenges. The first is the problem of data gathering 
 ent techniques where there is a well-defined learning rate,   on a real robot platform. Learning often requires far more 
 With WoLF we replace the original learning rate with two      trials than is practical to execute in the physical world. We 
 learning rates ;o be used when winning or los•                believe and demonstrate that the GraWoLF technique is prac•
 ing, respectively. One determination of winning and losing    tical for the time constraints of real robots. Yet, from a re•
that has been successful is to compare the value of the cur•   search standpoint, we want thorough and statistically signifi•
rent policv V"[s) to the value of the average policy over time cant evaluation, which requires far more trials than just those 
                               then the algorithm is consid•   used for learning. We solve this problem by using both a sim•
ered winning, otherwise it is losing. With the policy gradient ulator of our robot team as well as the robots themselves to 
technique above, we cannot easily compute the average pol•     show that the approach is both practical for robots while still 
icy. Instead we examine the approximate value, using Qw,       providing an extensive analysis of the results. 
of the current weight vector 6 with the average weight vector    The second challenge is the problem of evaluating the suc•
over time 0. Specifically, wc are "winning" if and only if,    cess of simultaneous learning. The traditional single-agent 
                                                               evaluation that shows performance over time converging to 
                                                               some optimal value is not applicable. Multiagent domains 
                                                               have no single optimal value independent of the other agents' 
When winning in a particular state, we update the parameters   behavior. The optimal value is changing over time as the 
for that state using otherwise                                 other agents also learn. This is especially true of self-play 
                                                               experiments where both agents are using an identical learn•
4.3 Our Task                                                   ing algorithm, and any performance success by one agent is 
Returning to the robot task shown in Figure 1, in order to     necessarily a performance failure for the other. 
apply GraWoLF we need to select a policy parameterization.       On the other hand we would still want learning robots, even 
Specifically we need to find a mapping from the continuous     in self-play, to improve their policy over time. In this paper, 
space of states and actions to a useful feature vector, i.e., to our main evaluation compares the performance of the learned 
define for a given s and a. The filtered positions and ve•     policy with the the performance of the initial policy before 
locities of the two robots form the available state information, learning. The initial policy is a random selection of the avail•
and the available actions are points on the field for navigation. able actions, and by design of the available actions is actually 
By observing that the radial angle of the attacker with respect a fairly capable policy for both agents. We also use the eval•
to the circle is not relevant to the task we arrive at a seven uation technique of challengers, which was first examined by 


702                                                                                             MULTIAGENT SYSTEMS Figure 2: The value of learned policies compared to a random  Figure 3: The value of learned policies playing their chal•
opponent in simulation. Lines to the right of the bars show   lengers in simulation. Lines to the right of the bars show 
standard deviations.                                          standard deviations. 

Littman [1994], This technique trains a worst-case opponent,  to specifically measure the policy's worst-case performance. 
or challenger, for a particular policy to see the generality of The better the worst-case performance, the less exploitable 
the learned policy. In this paper we present challenger results and more robust the policy is to unknown opponents. We 
demonstrating that the learned policies are indeed robust, and trained a challenger against the policies learned after 3000, 
that the WoLF variable learning rate plays a critical role in 3500, and 4000 trials, averaging together the results. We used 
keeping the learning away from easily exploited policies.     this experiment to investigate the robustness of the learned 
                                                              policy and the affect of the WoLF variable learning rate on 
5.2 Experiments                                               the GraWoLF algorithm. The left side of the graph shows the 
In all of the performance graphs in this section, the y-      worst-case performance of learned defender policies, while 
axis shows the attacker's expected discounted reward, which   the right side shows the worst-case performance for attacker 
roughly corresponds to the expected time it takes for the at• policies. "WoLF" corresponds to the described GraWoLF 
tacker to reach the circle. On the right of the graph the range algorithm, "Slow" does not use a variable learning rate but 
is shown in seconds. All measurements of expected dis•        rather always uses the WoLF's winning rate, while "Fast" al•
counted reward are gathered empirically over the course of    ways uses its losing rate. 
1000 trials. All training occurred over 4000 trails, and takes  First, notice that the distance between the worst-case per•
approximately 6-7 hours of training time on the robot plat•   formance of the defender and the worst-case performance of 
form or in simulation. Unless otherwise noted, the training   the attacker (the third and fourth column of Table 3, respec•
was always done with a simultaneously learning opponent,      tively) is quite small. This demonstrates that the learned poli•
both using Gra WoLF. The experiments in simulation were re•   cies are quite close to the equilibrium policy, if one exists. 
peated nine times and the averages are shown in the graphs.   This also means that the learned policies are robust and diffi•
We first examine the performance of the policies learned in   cult to exploit. 
simulation, and then examine the performance of learning on      Second, notice that the WoLF learned defender policy per•
the robot.                                                    forms better against its challenger, i.e., keeps its challenger 
  Figure 2 shows the results of various learned policies when to a lower reward, than either of the two learning rates it 
playing against an opponent following the random policy,      switches between. For the attacker, the learned policy per•
which was the starting point for learning. The middle bar,    forms better against its challenger than the fast learning rate, 
"R v. R", corresponds to the expected value of both players   but is not significantly different than the slow learning rate. 
following the random policy. "L v. R" corresponds to the      There are a couple of possible reasons for this. One expla•
value of the attacker following the policy learned in self-play nation is due to the initialization of values. Since all values 
against a random defender. "R v. L" corresponds to the value  were intialized to zero for both sides, this amounts to an opti•
of a random attacker against the defender policy learned in   mistic initialization for the defender, and a pessimistic one for 
self-play.                                                    the attacker (as all rewards are non-negative for the attacker.) 
  Notice that, as was desired, learning does improve perfor•  This may mean the attacker considers itself winning far more 
mance over the starting policy. The learned attacker policy   often than the defender, causing the slower learning rate to be 
against random does far better than the random attacker pol•  employed most of the time. There is evidence to this effect 
icy against the learned defender. These experiments demon•    when examining the percentage of updates that use 6W versus 
strate that GraWoLF improves considerably on its starting     Si. During training, the attacker used the slower, winning rate 
policy. The next experiment explores how robust the learned   for 92.3% of its updates, while the defender used the winning 
                                                              rate for only 84.2%. The effect of value initialization on Gra•
policy is and the effect of the WoLF component. 
                                                              WoLF is an interesting top to be explored further. Overall, 
  Figure 3 shows results of challenger experiments. Poli-
                                                              although the results are certainly not as dramatic as the unap-
cies are trained using simultaneous learning. The policy is 
                                                              proximated results [Bowling and Veloso, 2002a], WoLF still 
then fixed and a challenger policy is trained for 4000 trails, 


MULTIAGENT SYSTEMS                                                                                                   703 