                            Understanding the Power of Clause Learning 

                       Paul Beanie* Henry Kautz* Ashish Sabharwal* 

                                         Computer Science and Engineering 
                                              University of Washington 
                                               Seattle, WA 98195-2350 
                                     {beame,kautz,ashish}@cs.Washington.edu 

                        Abstract                               mance of backtrack search algorithms by generating expla•
                                                               nations for failure (backtrack) points, and then adding the 
     Efficient implementations of DPLL with the addi•          explanations as new constraints on the original problem [de 
     tion of clause learning are the fastest complete sat•     Kleer and Williams, 1987; Stallman and Sussman, 1977; 
     isfiability solvers and can handle many significant       Genesereth, 1984; Davis, 1984]. For general constraint sat•
     real-world problems, such as verification, planning,      isfaction problems the explanations are called "conflicts" 
     and design. Despite its importance, little is known       or "no goods"; in the case of Boolean CNF satisfiability, 
     of the ultimate strengths and limitations of the tech•    the technique becomes clause learning. A series of re-
     nique. This paper presents the first precise charac•      searchers [Bayardo Jr. and Schrag, 1997; Marques-Silva 
     terization of clause learning as a proof system, and      and Sakallah, 1996; Zhang, 1997; Moskewicz et al, 2001; 
     begins the task of understanding its power. In par•       Zhang et al, 2001] showed that clause learning can be ef•
     ticular, we show that clause learning using any non-      ficiently implemented and used to solve hard problems that 
     redundant scheme and unlimited restarts is equiva•        cannot be approached by any other technique. 
     lent to general resolution. We also show that with•
                                                                 Despite its importance there has been little work on formal 
     out restarts but with a new learning scheme, clause 
                                                               properties of clause learning, with the goal of understanding 
     learning can provide exponentially smaller proofs 
                                                               its fundamental strengths and limitations. A likely reason for 
     than regular resolution, which itself is known to be 
                                                               such inattention is that clause learning is a rather complex 
     much stronger than ordinary DPLL. 
                                                               rule of inference - in fact, as we describe below, a complex 
                                                               family of rules of inference. A contribution of this paper is 
1 Introduction                                                 that we provide a precise specification of clause learning. 
                                                                 Another problem in characterizing clause learning is defin•
In recent years the task of deciding whether a CNF propo-
                                                               ing a formal notion of the strength or power of a reason•
sitional logic formula is satisfiable has gone from a problem 
                                                               ing method. This paper uses the notion of proof complex•
of theoretical interest to a practical approach for solving real-
                                                               ity [Cook and Reckhow, 1977], which compares inference 
world problems. SAT procedures are now a standard tool for 
                                                               systems in terms of the sizes of the shortest proofs they sanc•
hardware verification, including verification of super-scalar 
                                                               tion. A family of formulas C provides an exponential sep•
processors [Velev and Bryant, 2001; Biere et al, 1999]. Open 
                                                               aration between systems S1 and 52 if the shortest proofs of 
problems in group theory have been encoded and solved using 
                                                               formulas in C in system S1 are exponentially smaller than the 
satisfiability [Zhang and Hsiang, 1994]. Other applications of 
                                                               corresponding shortest proofs in S2. From this basic proposi-
SAT include circuit diagnosis and experiment design [Konuk 
                                                               tional proof complexity point of view, only families of unsat-
and Larrabee, 1993; Gomes et al, 1998b]. 
                                                               isfiable formulas are of interest, because only proofs of unsat-
  The most surprising aspect of such relatively recent practi•
                                                               isfiability can be large; minimum proofs of satisfiability are 
cal progress is that the best complete satisfiability testing al•
                                                               linear in the number of variables of the formula. Neverthe•
gorithms remain variants of the DPLL procedure [Davis and 
                                                               less, Achlioptas et al [2001] have shown how negative proof 
Putnam, 1960; Davis et al., 1962] for backtrack search in the 
                                                               complexity results for unsatisfiable formulas can be used to 
space of partial truth assignments. The main improvements 
                                                               derive time lower bounds for specific inference algorithms 
to DPLL have been better branch selection heuristics (e.g., 
                                                               running on satisfiable formulas as well. 
[Li and Anbulagan, 1997]), and extensions such as random•
ized restarts [Gomes et al, 1998a] and clause learning. One      Proof complexity does not capture everything we intu•
can argue that clause learning has been the most significant   itively mean by the power of a reasoning system, because it 
of these in scaling DPLL to realistic problems.                says nothing about how difficult it is to find shortest proofs. 
  Clause learning grew out of work in AI on explanation-       However, it is a good notion with which to begin our anal•
based learning (EBL), which sought to improve the perfor-      ysis, because the size of proofs provides a lower-bound on 
                                                               the running time of any implementation of the system. In the 
   * Research supported by NSF Grant ITR-0219468               systems we consider, a branching function, which determines 


1194                                                                                                     SATISFIABILITY which variable to split upon or which pair of clauses to re•   2.1 The DPLL Procedure 
solve, guides the search. A negative proof complexity result   The basic idea of the Davis-Putnam-Logemann-Loveland 
for a system tells us that a family of formulas is intractable (DPLL) procedure [Davis and Putnam, 1960; Davis et al., 
even with a perfect branching function; likewise, a positive   1962] for testing satisfiability of CNF formulas is to branch 
result gives us hope of finding a branching function.          on variables, setting them to TRUE or FALSE, until either an 
   A basic result in proof complexity is that general resolution initial clause is violated (i.e. has all literals set to FALSE) or all 
is exponentially stronger than the DPLL procedure [Bonet et    variables have been set. In the former case, we backtrack to 
al., 2000; Ben-Sasson et al, 2000]. This is because the trace  the last branching decision whose other branch has not been 
of DPLL running on an unsatisfiable formula can be con•        tried yet, reverse the decision, and proceed recursively. In the 
verted to a tree-like resolution proof of the same size, and   latter, we terminate with a satisfying assignment. If all pos•
tree-like proofs must sometimes be exponentially larger than   sible branches have been unsuccessfully tried, the formula 
the DAG-like proofs generated by general resolution. Al•       is declared unsatisfiable. To increase efficiency, pure liter•
though resolution can yield shorter proofs, in practice DPLL   als (those whose negation does not appear) and unit clauses 
is better because it provides a more efficient way to search for (those with only one unset literal) are immediately set to true. 
proofs. The weakness of the tree-resolution proofs that DPLL 
                                                               Definition 1. A branching sequence for a CNF formula F is 
finds is that they do not reuse derived clauses. The conflict 
                                                               a sequence of literals of F, possibly with 
clauses found when DPLL is augmented by clause learning 
                                                               repetitions. DPLL on F branches according to a if it always 
correspond to reuse of derived clauses in the associated res•
                                                               picks the next variable v to branch on in the literal order given 
olution proofs and thus to more general forms of resolution 
                                                               by o, skips it if v is currently assigned a value, and branches 
proofs. An intuition behind the results in this paper is that 
                                                               further by setting the chosen literal to FALSE otherwise. 
the addition of clause learning moves DPLL closer to general 
resolution while retaining its practical efficiency.             In this paper, we will use the term DPLL to denote the ba•
                                                               sic branching and backtracking procedure described above. It 
   It has been previously observed that clause learning can be 
                                                               will, for instance, not include extensions such as learning con•
viewed as adding resolvents to a tree-like proof [Marques-
                                                               flict clauses or restarting, but will allow intelligent branching 
Silva, 1998]. However, this paper provides the first mathe•
                                                               heuristics. Note that this is in contrast with the occasional use 
matical proof that clause learning is exponentially stronger 
                                                               of the term DPLL to encompass practically all branching and 
than tree-like resolution. Further, we provide a family of for•
                                                               backtracking approaches, including those involving learning. 
mulas that exponentially separates clause learning from reg•
ular resolution, a system that is known to be intermediate in  2.2 Resolution 
strength between general and tree resolution. The proof uses 
a new clause learning scheme called FirstNewCut that we in•    Resolution is a simple proof system that can be used to prove 
troduce. We also show that combining clause learning with      unsatisfiability of CNF formulas. The resolution rule states 
restarts is as strong as general resolution.                   that given clauses andwe can derive clause 
                                                               (A B) by resolving on A resolution derivation of C 
   Although this paper focuses on basic proof complexity, we 
                                                               from a CNF formula F is a sequence 
briefly indicate how the understanding we gain through this 
                                                               C) where each clause CI is either a clause of F (an initial 
kind of analysis may lead to practical applications. For exam•
                                                               clause) or derived by applying the resolution rule to and 
ple, our proofs describe an improvement to the clause learn•
                                                                           (a derived clause). The size of is s, the number 
ing rules previously suggested in the literature, and suggest 
                                                               of clauses occurring in it. We will assume that each 
an approach to leveraging the structure of a problem encoded 
                                                               in r is used to derive at least one other clause 
as a CNF formula in order to create a branching heuristic that 
                                                               Any derivation of the empty clause A from F, also called a 
takes the greatest advantage of clause learning. As an ex•
                                                               refutation or proof of F, shows that F is unsatisfiable. 
ample, we apply these ideas to certain natural satisfiable and 
                                                                 Despite its simplicity, resolution is not efficiently imple-
unsatisfiable formulas where we obtain significant speed-ups 
                                                               mentable due to the difficulty of finding good choices of 
over existing methods. 
                                                               clauses to resolve; natural choices typically yield huge stor•
                                                               age requirements. Various restrictions on the structure of res•
2 Preliminaries                                                olution proofs lead to less powerful but easier to implement 
                                                               variants such as tree-like, regular, linear and positive resolu•
A CNF formula F is an AND of clauses, where each               tion. Tree-like resolution uses non-empty derived clauses ex•
clause is an OR of literals, and a literal is a variable or    actly once in the proof and is equivalent to an optimal DPLL 
its negation It is natural to think of F as a set of clauses   procedure. Regular resolution allows any variable to be re•
and each clause as a set of literals. A clause that is a subset solved upon at most once along any "path" in the proof from 
of another is called its subclause.                            an initial clause to A. Both these variants are sound and com•
   Let p be a partial assignment to the variables of F. The    plete but differ in efficiency - regular resolution is known to 
restricted formula is obtained from F by replacing vari•       be exponentially stronger than tree-like [Bonet et al., 2000; 
ables in with their assigned values. F is said to be simplified Ben-Sasson et al., 2000], and general resolution is exponen•
if all clauses with at least one TRUE literal are deleted, all oc• tially stronger than regular [Alekhnovich et al., 2002]. 
currences of FALSE literals are removed from clauses, and the  Definition 2. A resolution derivation is 
resulting formula, if different, is simplified recursively.    trivial iff all variables resolved upon are distinct and each 


SATISFIABILITY                                                                                                       1195            is either an initial clause or is derived by resolving are called implied variables. Decision and implied literals 
           an initial clause.                                  are analogously defined. Upon backtracking, the last deci•
                                                               sion variable no longer remains a decision variable and might 
   A trivial derivation is tree-like as well as regular. More•
                                                               instead become an implied variable depending on the clauses 
 over, the condition that each derived clause use in its 
                                                               learned so far. The decision level of a decision variable x 
 derivation makes it linear. As we will see, trivial derivations 
                                                               is one more than the number of current decision variables at 
 correspond to conflicts in clause learning algorithms. 
                                                               the time of branching on x. The decision level of an implied 
                                                               variable is the maximum of the decision levels of decision 
 3 Clause Learning                                             variables used to imply it. The decision level at any step of 
 Clause learning proceeds by following the normal branching    the underlying DPLL procedure is the maximum of the deci•
 process of DPLL until there is a "conflict" after unit propaga• sion levels of all current decision variables. 
 tion. If this conflict occurs without any branches, the formula 
                                                               3.1 Implication Graph and Conflicts 
 is declared unsatisfiable. Otherwise, the "conflict graph" is 
 analyzed and the "cause" of the conflict is learned in the form Unit propagation can be naturally associated with an impli•
 of a "conflict clause." We now backtrack and continue as      cation graph that captures all possible ways of deriving all 
 in ordinary DPLL, treating the learned clause just like initial implied literals from decision literals. 
ones. A clause is said to be known at a stage if it is either  Definition 4. The implication graph G at a given stage of 
an initial clause or has already been learned. The learning    DPLL is a directed acyclic graph with edges labeled with sets 
process is expected to save us from redoing the same compu•    of clauses. It is constructed as follows: 
tation when we later have an assignment that causes conflict     1. Create a node for each decision literal, labeled with that 
due in part to the same reason.                                     literal. These will be the indegree zero root nodes of G. 
   If a given CNF formula F is unsatisfiable, clause learn•
ing terminates with a conflict without any branches. Since 
all clauses used in this conflict themselves follow directly or 
indirectly from F, this failure of clause learning in finding a 
satisfying assignment constitutes a logical proof of unsatisfi-
ability of F. Our bounds compare the size of such a proof 
with the size of a (possibly restricted) resolution proof of un-
satisfiability of F. 
   Variations of such conflict driven learning [Bayardo Jr. 
                                                                3. Add to G a special node A. For any variable x which 
and Schrag, 1997; Marques-Silva and Sakallah, 1996; Zhang, 
                                                                   occurs both positively and negatively in G\ add directed 
 1997; Moskewicz et al., 2001; Zhang et al., 2001] include 
                                                                   edges from and to  
different ways of choosing the clause to learn and possibly 
allowing multiple clauses to be learned from a single con•       Since all node labels in G are distinct, we identify nodes 
flict. Although many such algorithms have been proposed        with the literals labeling them. Any variable x occurring both 
and demonstrated to be empirically successful, a theoretical   positively and negatively in G is a conflict variable, and x as 
discussion of the underlying concepts and structures needed    well as arc conflict literals. G contains a conflict if it has 
for our bounds is lacking. The rest of this section focuses on at least one conflict variable. DPLL at a given stage has a 
this formal framework.                                         conflict if the implication graph at that stage contains a con•
                                                               flict. A conflict can equivalently be thought of as occurring 
Definition 3. A clause learning proof-K of an unsatisfiable    when the residual formula contains the empty clause A. 
CNF formula F under scheme S and induced by branching            By definition, an implication graph may contain many con•
sequence a is the result of applying DPLL with unit propa•     flict variables and several ways of deriving any single literal. 
gation on F, branching according to a, and using scheme S      To better understand and analyze a conflict, we work with a 
to learn conflict clauses such that at the end of this process, subgraph, called the conflict graph (see Figure 1), that cap•
there is a conflict without any branches. The size of the proof, tures only one among possibly many ways of reaching a con•
size(Tt)% is |er|.                                             flict from the decision variables. The choice of the conflict 
  All clause learning algorithms discussed in this paper are   graph is part of the strategy of the solver. It can also be 
based on unit propagation, which is the process of repeat•     thought of as giving power to clause learning by adding non-
edly applying the unit clause rule followed by formula sim•   determinism. 
plification until no clause with exactly one unassigned literal Definition 5. A conflict graph H is any subgraph of an im•
remains. In this context, it is convenient to work with resid• plication graph with the following properties: 
ual formulas at different stages of DPLL. Let p be the partial 
                                                                1. H contains A and exactly one conflict variable. 
assignment at some stage of DPLL on formula F. The resid•
ual formula at this stage is obtained by simplifying F\p and    2. All nodes in H have a path to A. 
applying unit propagation.                                      3. Every node I in H other than A either corre•
  When using unit propagation, variables assigned values           sponds to a decision literal or has precisely the nodes 
through the actual branching process are called decision vari•                      as predecessors where  
ables and those assigned values as a result of unit propagation           is a known clause. 


1196                                                                                                     SATISFIABILITY    Consider the implication graph at a stage where there is a 
conflict and fix a conflict graph contained in that implication 
graph. Pick any cut in the conflict graph that has all decision 
variables on one side, called the reason side, and A as well as 
at least one conflict literal on the other side, called the con•
flict side. All nodes on the reason side that have at least one 
edge going to the conflict side form a cause of the conflict. 
The negations of the corresponding literals forms the conflict 
clause associated with this cut. 
Proposition 1. Every conflict clause corresponds to a cut in 
a conflict graph that separates decision variables from A and 
a conflict literal. 

Proof Let S denote the set containing the negations of the 
literals of a given conflict clause C and pred(S) be the set of 
all predecessors of these literals in the underlying implication 
graph. Let T denote the set containing all literals obtained by 
unit propagation after setting literals in S to TRUE. Since C 
is a conflict clause, T must contain a conflict literal. Consider 
the subgraph GS,T of the implication graph induced by A and 
the literals in but having no edges going 
from preid(S) to T. Fix any conflict graph that is a subgraph 
of G S,T The cut in this conflict graph with T as the conflict 
side has C as the conflict clause.  

Proposition 2. If there is a trivial resolution derivation of a 
clause C from a set of clauses F, then setting all literals ofC 
to FALSE leads to a conflict. 

                                                               3.2 Different Learning Schemes 
                                                               Different cuts separating decision variables from A and a con•
                                                               flict literal correspond to different learning schemes (see Fig•
                                                               ure 1. One can also create learning schemes based on cuts not 
                                                               involving conflict literals at all [Zhang et al, 2001], but their 
                                                               effectiveness is not clear. These will not be considered here. 
                                                                 It is insightful to think of the non-deterministic scheme as 
                                                               the most general learning scheme. Here we pick the cut non-
                                                               deterministically, choosing, whenever possible, one whose 
                                                               associated clause is not already known. Since we can re•
                                                               peatedly branch on the same last variable, non-deterministic 
                                                               learning subsumes learning multiple clauses from a single 
                                                               conflict as long as the sets of nodes on the reason side of the 
                                                               corresponding cuts form a (set-wise) decreasing sequence. 
                                                               For simplicity, we will assume that only one clause is learned 
                                                               from any conflict. 
                                                                 In practice, however, we employ deterministic schemes. 
                                                               The decision scheme [Zhang et al, 2001], for example, uses 
                                                               the cut whose reason side comprises all decision variables, 
                                                               re 1 - sat [Bayardo Jr. and Schrag, 1997] uses the cut whose 
                                                               conflict side consists of all implied variables at the current de•
Proposition 3. Any conflict clause can be derived from         cision level. This scheme allows the conflict clause to have 
known clauses using a trivial resolution derivation.           exactly one variable from the current decision level, causing 
                                                               an automatic flip in its assignment upon backtracking. 
Proof. In the light of Proposition 1, assume that we have        This nice flipping property holds in general for all unique 
a conflict clause associated with a cut in a fixed conflict    implication points (UIPs) [Marques-Silva and Sakallah, 
graph. Let denote the set of variables on the                  1996]. A U1P of an implication graph is a node at the current 
conflict side of o, but including the conflict variable only if decision level d such that any path from the decision vari•
it occurs both positively and negatively on the conflict side. able at level d to the conflict variable as well as its negation 


SATISFIABILITY                                                                                                      1197 must go through it. Intuitively, it is a single reason at level  This scheme starts with the cut that is closest to the con•
d that causes the conflict. Whereas rel-sat uses the de•       flict literals and iteratively moves it back toward the decision 
cision variable as the obvious U1P, GRASP [Marques-Silva       variables until a new associated conflict clause is found. This 
and Sakallah, 1996] and zChaf f [Moskewicz et al, 2001]        backward search always halts because the cut with all deci•
use FirstUIP, the one that is "closest" to the conflict variable. sion variables on the reason side is certainly a new cut. Note 
GRASP also learns multiple clauses when faced with a con•      that there are potentially several ways of choosing a literal to 
flict. This makes it typically require fewer branching steps   move the cut back, leading to different conflict clauses. The 
but possibly slower because of the time lost in learning and   FirstNewCut scheme, by definition, always learns a clause 
unit propagation.                                              not already known. This motivates the following: 
   The concept of UIP can be generalized to decision levels    Definition 8. A clause learning scheme is non-redundant if 
other than the current one. The IUIP scheme corresponds        on a conflict, it always learns a clause not already known. 
to learning the FirstUIP clause of the current decision level, 
the 2UIP scheme to learning the FirstUIP clauses of both the   3.3 Fast Backtracking and Restarts 
current level and the one before, and so on. Zhang et al [2001 ] Most clause learning algorithms use fast backtracking where 
present a comparison of all these and other learning schemes   one uses the conflict graph to backtrack not only the last 
and conclude that 1UIP is quite robust and outperforms all     branching decision but also all other recent decisions that did 
other schemes they consider on most of the benchmarks.         not contribute to the conflict [Stallman and Sussman, 1977]. 
                                                               This adds power to clause learning because the current con•
                                                               flict might use clauses learned earlier as a result of branching 
                                                               on the apparently redundant variables. Hence, fast backtrack•
                                                               ing in general cannot be replaced by a "good'* branching se•
                                                               quence that does not produce redundant branches. For the 
                                                               same reason, fast backtracking cannot either be replaced by 
                                                               simply learning the decision scheme clause. However, the re•
                                                               sults we present in this paper are independent of whether or 
                                                               not fast backtracking is used. 
                                                                 Restarts allow clause learning algorithms to arbitrarily 
                                                               restart their branching process. All clauses learned so far 
                                                               are however retained and now treated as additional initial 
                                                               clauses [Baptista and Silva, 2000]. As we will show, un•
Figure 1: A conflict graph depicting various learning schemes  limited restarts can make clause learning very powerful at 
                                                               the cost of adding non-determinism. Unless otherwise stated, 
                                                               clause learning proofs will be assumed to allow no restarts. 
The FirstNewCut Scheme 
We propose a new learning scheme called FirstNewCut            4 Clause Learning and General Resolution 
whose ease of analysis helps us demonstrate the power of 
clause learning. We would like to point out that we use this   Lemma 1. Let F be a CNF formula over n variables. If 
scheme here only to prove our theoretical bounds. Its effec•   F has a general resolution proof of size s, then it also has 
tiveness on other formulas has not been studied yet.           a clause learning proof of size at most ns using any non-
   The key idea behind FirstNewCut is to make the con•         redundant learning scheme and at most s restarts. 
flict clause as relevant to the current conflict as possible by 
                                                               Proof A) be a general resolu•
choosing a cut close to the conflict literals. This is what the 
                                                               tion proof of F where each is either an initial clause or 
FirstUIP scheme also tries to achieve in a slightly different 
                                                               derived by resolving two clauses and occur•
manner. For the following definitions, fix a cut in a conflict 
                                                               ring earlier in If contains a derived clause whose strict 
graph and let S be the set of nodes on the reason side that 
                                                               subclause C[ can be derived by resolving two previously oc•
have an edge to some node on the conflict side (5 is the rea•
                                                               curring clauses, then we can replace with do trivial 
son side frontier of the cut). Let be the conflict clause 
                                                               simplifications on further derivations that used and obtain 
associated with this cut. 
                                                               another proof of F of size at most s. Doing this repeat•
Definition 6. Minimization of conflict clause Cs is the pro•   edly will remove all such redundant clauses and leave us with 
cess of repeatedly identifying, if one exists, a node S, all   a simplified proof no larger in size. Hence we will assume 
of whose predecessors are also in S, moving it to the conflict without loss of generality that has no such clause. 
side, and removing it from S.                                    A clause learning proof of F can be constructed by choos•
Definition 7. FirstNewCut scheme: Start with a cut whose       ing derived clauses of in order, learning each of them, and 
conflict side consists of A and a conflict literal. If necessary, restarting. Suppose every clause is already known 
repeat the following until the associated conflict clause, af• and we are at decision level zero. This is trivially true when 
ter minimization, is not already known: pick a node on the                   are initial clauses. If there are two 
conflict side, pull all its predecessors except those that cor• known clauses and whose resolution generates 
respond to decision variables into the conflict side. Finally, In this case we have a conflict from the known clauses at de•
learn the resulting new minimized conflict clause.             cision level zero and our clause learning proof is complete. 


1198                                                                                                     SATISFIABILITY 