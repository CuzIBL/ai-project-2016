   Dynamic Weighting A* Search-based MAP Algorithm for Bayesian Networks
            Xiaoxun Sun                   Marek J. Druzdzel                   Changhe Yuan
   Computer Science Department        Decision Systems Laboratory    Department of Computer Science
  University of Southern California  School of Information Sciences           and Engineering
      Los Angeles, CA 90089          & Intelligent Systems Program      Mississippi State University
       xiaoxuns@usc.edu                 University of Pittsburgh        Mississippi State, MS 39762
                                          Pittsburgh, PA 15260        cyuan@cse.msstate.edu
                                       marek@sis.pitt.edu
                    Abstract                          2MAP
                                                      The MAP problem is deﬁned as follows: Let M be the set
    In this paper we propose the Dynamic Weight-
                                                      of MAP variables, the conﬁguration of which is what we are
    ing A* (DWA*) search algorithm for solving MAP
                                                      interested in; E is the set of evidence, namely the variables
    problems in Bayesian networks. By exploiting
                                                      whose states we have observed; The remainder of the vari-
    asymmetries in the distribution of MAP variables,
                                                      ables, denoted by S, are variables whose states we neither
    the algorithm is able to greatly reduce the search
                                                      know nor care about. Given an assignment e of variables E,
    space and offer excellent performance both in terms
                                                      the MAP problem is that of ﬁnding the assignment m of vari-
    of accuracy and efﬁciency.
                                                      ables M which maximizes the probability P (m | e), while
                                                      the MPE problem is the special case of MAP with S being
                                                      empty, i.e.,
1  Introduction                                                               
                                                                  MAP   =max     p(M,S  | E) .        (1)
The Maximum a  Posteriori assignment (MAP) is the prob-                    M
lem of ﬁnding the most probable instantiation of a set of                      S
variables given partial evidence on the remaining variables In general, in Bayesian networks, we use the Conditional
in a Bayesian network. One specialization of MAP that has Probability Table (CPT) φ as the potential over a variable and
received much attention is the Most Probable Explanation its parent nodes. The notation φe stands for the potential in
(MPE) problem. MPE is the problem of ﬁnding the most  whichwehaveﬁxedthevalueofe   ∈ E. Then the probability
probable assignment of a set of variables given full evidence     Φ
                                                      of MAP with   as its CPTs turns out to be a real number:
on the remaining variables. MAP turns out to be a much more
                                                                    MAP   =max          φe .          (2)
difﬁcult problem compared to MPE and computing the prob-                     M
ability of evidence. Particularly, the decision problem for                      S  φ∈Φ
MPE is NP-complete while the corresponding MAP prob-    In Equation 2, summation commutes with summation, and
         PP
lem is NP   -complete [Park, 2002]. MAP is more useful maximization commutes with maximization. However, sum-
than MPE for providing explanations. For instance, in diag- mation does not commute with maximization and vice versa.
nosis, we are generally only interested in the conﬁguration of Therefore, it is obligatory to do summation before any max-
fault variables given some observations. There may be many imization. The order is called the elimination order.The
other variables that have not been observed that are outside size of the largest clique minus 1 in a jointree constructed
the scope of our interest.                            based on an elimination order is called the induced width.
  In this paper, we introduce the Dynamic Weighting A* The induced width of the best elimination order is called the
(DWA*) search algorithm for solving MAP that is generally treewidth. However, for the MAP problems in which the set
more efﬁcient than existing algorithms. The algorithm ex- S and the M are both non-empty, the order is constrained.
plores asymmetries among all possible assignments in the Then the constrained elimination order is known as the con-
joint probability distribution of the MAP variables. Typi- strained treewidth. Generally, the constrained treewidth is
cally, a small fraction of the assignments can be expected much larger than treewidth, leading the problem beyond the
to cover a large portion of the total probability space with limit of feasibility for complex models. Speciﬁcally, for some
the remaining assignments having practically negligible prob- MAP problems, variable elimination on polytrees is subject to
ability [Druzdzel, 1994]. Also, the DWA* uses dynamic the constrained treewidth, which requires exponential time,
weighting based on greedy guess [Park and Darwiche, 2001; while MPE problems can be computed in linear time [Park
Yuan et al., 2004] as the heuristic function. Although it is and Darwiche, 2003].
theoretically not admissible (admissible heuristic should of- A very efﬁcient approximate search-based algorithm based
fer an upper bound on the MAP), the heuristic signiﬁcantly on local search, proposed by Park and Darwiche [2001],is
reduces the size of the search tree, while rarely pruning away capable of solving MAP efﬁciently. An exact method, based
the optimal solutions.                                on branch-and-bound depth-ﬁrst search, proposed by Park

                                                IJCAI-07
                                                  2385and Darwiche [2003], performs quite well when the search layer to the leaf nodes of the search tree. In order to guaran-
space is not too large. Another approximate algorithm pro- tee the optimality of the solution, h(n) should be admissible,
posed more recently by Yuan et al. [2004] is a Reheated An- which in this case means that it should be an upper-bound
nealing MAP algorithm. It is somewhat slower on simple net- on the value of any assignment with the currently instantiated
works but it is capable of handling difﬁcult cases that exact MAP variables as its elements.
methods cannot tackle.
                                                      3.2  Heuristic Function with Dynamic Weighting
3  Solving MAP using Dynamic Weighting A*             The A* Search is known for its completeness and optimality.
We propose in this section an algorithm for solving MAP us- For each search step, we only expand the node in the frontier
                                                                            ( )
ing Dynamic Weighting A* search, which incorporates the with the largest value of f n .
dynamic weighting [Pohl, 1973] in the heuristic function, rel- Deﬁnition 1 A heuristic function h2 is said to be more in-
evance reasoning [Druzdzel and Suermondt, 1994] and dy- formed than h1 if both are admissible and h2 is closer to the
namic ordering in the search tree.                    optimal cost. For the MAP problem, the probability of the
                                                      optimal assignment Popt <h2 <h1.
3.1  A* search                                                                                    ∗
MAP can be solved by A* search in the probability tree that Theorem 1 If h2 is more informed than h1 then A2 domi-
                                                            ∗          [         ]
is composed of all the variables in the MAP set. The nodes nates A1 (Nilsson). Pearl, 1985
in the search tree represent partial assignments of the MAP The power of the heuristic function is measured by the
variables M. The root node represents an empty assignment. amount of pruning induced by h(n) and depends on the ac-
The MAP variables will be instantiated in a certain order. If curacy of this estimate. If h(n) estimates the completion cost
avariablex in the set of MAP variables M is instantiated at precisely (h(n)=Popt), then A* will only expand nodes on
the ith place using its jth state, it will be denoted as Mij . the optimal path. On the other hand, if no heuristic at all is
Leaves of the search tree correspond to the last MAP variable used (for the MAP problem this amounts to h(n)=1), then
that has been instantiated. The vector of instantiated states of a uniform-cost search ensues, which is far less efﬁcient. So it
each MAP variable is called an assignment or a scenario. is critical for us to ﬁnd an admissble and tight h(n) to get
  We compute the probability of assignments while search- both accurate and efﬁcient solutions.
ing the whole probability tree using chain rule. For each inner
node, the newly instantiated node will be added to the evi- Greedy Guess
dence set, i.e., the evidence set will be extended to Mij ∪ E. If each variable in the MAP set M is conditionally indepen-
Then the probability of the MAP problem which consists of dent of all the rest of MAP variables (this is called exhaustive
n MAP variables can be presented as follows:          independence), then the MAP problem amounts to a simple
                                                      computation based on the greedy chain rule. We instantiate
   P (M | E)=P    (Mni  | M1j,M2k,...M(n−1)t,E)       the MAP variable in the current search layer to the state with
                 ...P(M2k | M1j,E)P (M1j | E) .       the largest probability and repeat this for each of the remain-
                                                      ing MAP variables one by one. The probability of MAP is
Suppose that we are in the xth layer of the search tree and then
preparing for instantiating the xth MAP variables. Then the         n
function above can be rewritten as follows:                        
                                                         P (M|E)=     max P (Mij |M(i−1)k ...M1m,E) . (4)
                                                                        j
                    P (M | E)=                                     i=1
                         b
                                                    The requirement of exhaustive independence is too strict
  P (M  | M  ...M      ,E) ...P(M     | M  ...E)
      ni   1j     (n−1)t         (x+1)z  xy           for most of the MAP problems to be calculated by using
    · P (M | M  ,M  ...M      ,E) ...P(M  | E) .
        xy   1j  2k    (x−1)q        1j     (3)   the function above. Simulation results show that in prac-
                         a                            tice, when this requirement is violated, the product is still
                                                      extremely close to the MAP probability [Yuan et al., 2004].
  The general idea of the Dynamic Weighting A* search is This suggests that it can be potentially used as a heuristic
that during the search, in each inner node of the probabil- function for MAP.
ity tree, we can compute the exact value of item (a) in the The curve Greedy Guess Estimate in Figure 1 shows that
function above. We can estimate the heuristic value of the with the increase of the number of MAP variables, the ratio
item (b) for the MAP variables that have not been instanti- between the greedy guess and the accurate estimate of the
ated given the initial evidence set and the MAP variables that optimal probability diverges from the ideal ratio one although
have been instantiated as the new evidence. In order to ﬁt the not always monotonically.
typical format of the cost function of A* Search, we can take
the logarithm of the equation above, which will not change its Dynamic Weighting
monotonicity. Then we get f(n)=g(n)+h(n),whereg(n)    Since the greedy guess is a tight lower bound on the optimal
and h(n) are obtained from the logarithmic transformation of probability of MAP, it is possible to compensate for the error
items (a) and (b) respectively. g(n) gives the exact cost from between the greedy guess and the optimal probability. We can
the start node to node in the nth layer of the search tree, and achieve this by adding a weight to the greedy guess such that
h(n) is the estimated cost of the best search path from the nth their product is equal or larger than the optimal probability

                                                IJCAI-07
                                                  2386for each inner node in the search tree under the following set α to be a safe parameter which is supposed to be larger
assumption:                                           than  (In our experiments, we set α to be 1.0). However,
                                                      what if is accidentally smaller then leading the weighted
         ∃{∀PGreedyGuess ∗ (1 + ) ≥ Popt∧                  α                        
                                                      heuristic to be inadmissible? Suppose there are two candi-
     ∀ (           ∗ (1 +  ) ≥   ) ⇒  ≤   }
       PGreedyGuess          Popt        ,        date assignments: s1 and s2 with probability p1 and p2 re-
  where  is the minimum weight that can guarantee the spectively, among which s2 is the optimal assignment that
heuristic function to be admissible. Figure 1 shows that if we the algorithm fails to ﬁnd. And s1 is now in the last step of
just keep  constant, neglecting the changes of the estimate search, which will lead to a suboptimal solution. We skip the
accuracy with the increase of the MAP variables, the estimate logarithm in the function for the sake of clarity here (then the
function and the optimal probability can be represented by the cost function f is a product of transformed g and h instead of
curve Constant Weighting Heuristic. Obviously, the problem their sum).
with this idea is that it is less informed when the search pro-    f1 = g1 · h1 and f2 = g2 · h2.
gresses, as there are fewer MAP variables to estimate.
                                                      The error introduced by an inadmissible h2 is f1 >f2.The
  Dynamic Weighting [Pohl, 1973] is an efﬁcient tool for im-
                                                      algorithm will then ﬁnd s1 instead of s2, i.e.,
proving the efﬁciency of A* Search. If applied properly, it
will keep the heuristic function admissible while remaining          f1 >f2 ⇒  g1 · h1 >g2 · h2.
tight on the optimal probability. For MAP, in shallow layers Since s1 is now in the last step of search, f1 = p1 (Section
of the search tree we get more MAP variables than in deeper 3.2). Now suppose that we have an ideal heuristic function
layers. Hence, the greedy estimate will be more likely to di-                 
                                                      h2, which leads to p2 = g2 · h2.Thenwehave:
verge from the optimal probability. We propose the following
Dynamic Weighting Heuristic Function for the xth layer of g1 · h1 g2 · h2  p1   g2 · h2   p1   h2
                                                               >        ⇒    >       ⇒    >    .
the Search tree of n MAP variables:                        p2     g2 · h2  p2   g2 · h2   p2   h2
                           n − (x +1)
 h(x)=GreedyGuess   · (1 + α         ), (α ≥ ) . (5)   It is clear that only when the ratio between the probability
                                n                     of suboptimal assignment and the optimal one is larger than
Rather than keeping the weight constant throughout the the ratio between the inadmissible heuristic function and the
search, we dynamically change it, so as to make it less heavy ideal one, may the algorithm ﬁnd a suboptimal solution.
as the search goes deeper. In the last step of the search Because of large asymmetries among probabilities that
(x = n − 1), the weight will be zero, since the Greedy Guess are further ampliﬁed by their multiplicative combination
for only one MAP variable is exact and then the cost func- [Druzdzel, 1994], we can expect that for most of the cases,
tion f(n-1) is equal to the probability of the assignment. Fig- the ratios between p1 and p2 are far less than 1. Even though
ure 1 shows an empirical comparison of greedy guess, con- the heuristic function will sometimes break the rule of admis-
stant, and dynamic weighting heuristics against accurate esti- sibility, if only the greedy guess is not too divergent from the
mates of the probability. We see that the dynamic weighting ideal estimate, the algorithm will still not diverge from the
heuristic is more informed than constant weighting.   optimal probability. Our simulation results also conﬁrm the
                                                      robustness of the algorithm in ﬁnding optimal solutions.
     1.6
                                                      3.4  Improvements to the Algorithm
     1.4

     )                                                There are several techniques that can improve the efﬁciency
     1.2                                              of the basic A* algorithm.
      1                                               Relevance Reasoning
     0.8                                              The main problem faced by our approach is the complexity

     0.6                                              of probabilistic inference. The critical factor in exact infer-
                                                      ence schemes for Bayesian networks is the topology of the
     0.4


     Quality of H(X) (h(x)/optimal  H(X)  of Quality  underlying graph and, more speciﬁcally, its connectivity. Rel-
     0.2                                              evance reasoning [Druzdzel and Suermondt, 1994] is a tech-
                                                      nique based on d-separation and other simple and compu-
      0
       0    5    10  15   20   25   30  35   40       tational efﬁcient techniques for pruning irrelevant parts of a
                    The number of MAP variables       Bayesian network and can yield sub-networks that are smaller
                                                      and less densely connected than the original network. For
Figure 1:  Constant Weighting Heuristic and Dynamic   MAP, our focus is the set of variables M and the evidence set
Weighting Heuristic based on Greedy Guess.            E. Parts of the model that are probabilistically independent
                                                      from the nodes in M given the observed evidence E are com-
                                                      putationally irrelevant to reasoning about the MAP problem.
3.3  Searching with Inadmissible Heuristics           Dynamic Ordering
Since the minimum weight  that can guarantee the heuristic As the search tree is constructed dynamically, we have the
function to be admissible is unknown before the MAP prob- freedom to order the variables in a way that will improve
lem is solved, and it may vary in different cases, we normally the efﬁciency of the DWA* search. Expanding nodes with

                                                IJCAI-07
                                                  2387the largest asymmetries in their marginal probability distribu- second group, and all four algorithms generated results within
tions lead to early cut-off of less promising branches of the the time limit. The P-SYS is an exact algorithm. So Table 3
search tree. We use the entropy of the marginal probability only reports the number of MAP problems that were solved
distributions as a measure of asymmetry.              optimally by the P-LOC,ANNEALEDMAP and DWA*. The
                                                      DWA* found all optimal solutions. The P-LOC missed only
4  Experimental Results                               one case on Andes and the ANNEALEDMAP missed one on
To test DWA*, we compared its performance on MAP prob- Hailﬁnder and two cases on Andes.
lems in real Bayesian networks against those of current state            P-LOC   A-MAP     DWA*
of the art MAP algorithms: the P-LOC and P-SYS algo-          Alarm       20        20       20
rithms [Park and Darwiche, 2001; 2003] in SamIam, and         CPCS179     20        20       20
ANNEALEDMAP    [Yuan et al., 2004] in SMILE. We imple-        CPCS360     20        20       20
mented DWA* in C++ and performed our tests on a 3.0 GHz       Hailﬁnder   20        19       20
Pentium D Windows XP computer with 2GB RAM. We used           Pathﬁnder   20        20       20
the default parameters and settings for all the three algorithms Andes    19        18       20
above during comparison, unless otherwise stated.             Win95pts    20        20       20
4.1  Experimental Design                                      Munin       20        20       20
                                                              HRL1        20        20       20
The Bayesian networks that we used in our experiments in-     HRL2        20        20       20
cluded Alarm [Beinlich et al., 1989],Barley[Kristensen
                  ]                      [
and Rasmussen, 2002 , CPCS179 and CPCS360 Pradhan et  Table 2: The number of cases that were solved optimally out
       ]          [                   ]
al., 1994 , Hailﬁnder Abramson et al., 1996 , Munin, Dia- of 20 random cases for the ﬁrst and second groups of net-
     [                   ]       [               ]
betes Andreassen et al., 1991 , Andes Conati et al., 1997 , works.
Pathﬁnder, and Win95pts [Heckerman et al., 1995],someof
which were constructed for diagnosis. We also tested the al- Since both ANNEALEDMAP and P-LOC failedtoﬁndall
gorithms on two very large proprietary diagnostic networks optimal solutions in Andes, we studied the performance of the
built at the HRL Laboratories (HRL1 and HRL2). The statis- four algorithms as a function of the number of MAP variables
tics for all networks are summarized in Table 1. We divide (we randomly generated 20 cases for each number of MAP
the networks into three groups: (1) small and middle-sized, variables).
(2) large but tractable, and (3) hard networks.
                                                               #MAP     P-SYS    P-LOC   A-MAP
         Group   Network    #Nodes  #Arcs                      10         0        0        0
                  Alarm       37      46                       20         0        1        2
                 CPCS179     179     239                       30         0        1        0
                 CPCS360     360     729                       40      TimeOut     4        4
         1       Hailﬁnder    56      66                       50      TimeOut     6        2
                 Pathﬁnder   135     195                       60      TimeOut     5        2
                  Andes      223     338                       70      TimeOut     6        5
                 Win95pts     76     112                       80      TimeOut     6        1
         2        Munin     1,041    1,397
                  HRL1      1,999    3,112            Table 3: The number of cases for which the existing algo-
                  HRL2      1,528    2,492            rithms found smaller probabilities than A* Search in network
         3        Barley      48      84              Andes.
                 Diabetes    413     602
                                                        Because the search time of P-SYS increased quickly with
 Table 1: Statistics for the Bayesian networks that we use. the number of MAP variables, and it failed to generate any
                                                      result when the number of MAP variables reached 40, while
  For each network, we randomly generated 20 cases. For DWA* found all largest probabilities, we compared all the
each case, we randomly chose 20 MAP variables from the other three algorithms against DWA*. With the increase
root nodes or all of them if there were fewer than 20 root of the number of MAP variables, both P-LOC and AN-
nodes. We chose the same number of evidence nodes from NEALEDMAP turned out to be less accurate than DWA*
the leaf nodes. To set evidence, we sampled from the prior on Andes. When the number of MAP variables was above
probability distribution of a Bayesian network in its topolog- 40, there were about 25% cases of P-LOC and 15% cases
ical order and cast the states of the sample to the evidence in which ANNEALEDMAP found smaller probabilities than
nodes. Following previous tests of MAP algorithms, we set DWA*. We notice from Table 5 that P-LOC spent less time
the search time limit to be 3, 000 seconds (50 minutes). than DWA* when using its default settings for Andes, so we
                                                      increased the search steps of P-LOC such that it spent the
4.2  Results for the First and Second Group           same amount of time as DWA* in order to make a fair com-
In the ﬁrst experiment, we ran the P-LOC,P-SYS,AN-    parison. However, in practice the search time is not continu-
NEALEDMAP and DWA* on all the networks in the ﬁrst and ous in the number of search steps, so we just chose parameters

                                                IJCAI-07
                                                  2388for P-LOC such that it spent slightly more time than DWA*. and P∗ stand for the probability of MAP solutions found by
Table 4 shows the comparison results. We can see that after P-LOC,ANNEALEDMAP and DWA* respectively.
increasing the search steps of P-LOC, DWA* still maintains
better accuracy.                                                     P∗>PL/P∗<PL     P∗>PA/P∗<PA
                                                            Barley         3/2             5/3
       #MAP    P-LOC<DWA*     P-LOC>DWA*                    Diabetes       5/0             4/0
       10            0              0
       20            0              0                 Table 6: The number of cases that are solved differently from
       30            0              0                 P-LOC,ANNEALEDMAP and DWA*.
       40            1              0
       50            2              0                   For Barley, the accuracy of the three algorithms was quite
       60            2              1                 similar. However, for Diabetes DWA* was more accurate:
       70            3              2                 it found solutions with largest probabilities for all 20 cases,
       80            5              0                 while P-LOC failedtoﬁnd5andANNEALEDMAP failed to
                                                      ﬁnd4ofthem.
Table 4: The number of cases that the P-LOC   found
larger/smaller probabilities than DWA* in network Andes              P-SYS    P-LOC   A-MAP      A*
when spending slightly more time than DWA*.               Barley    TimeOut   68.63     31.95   122.1
                                                          Diabetes  TimeOut   338.4     163.4    81.8
  In addition to the precision of the results, we also com-
pared the efﬁciency of the algorithms. Table 5 reports the Table 7: Average running time in seconds of the P-SYS,P-
average running time of the four algorithms on the ﬁrst and LOC,ANNEALEDMAP and DWA* on the third groups.
the second groups of networks. For the ﬁrst group, the AN-
                                                        DWA* turned out to be slower than P-LOC  and AN-
               P-SYS   P-LOC    A-MAP     A*          NEALEDMAP on Barley but more efﬁcient on Diabetes (see
    Alarm       0.017   0.020    0.042   0.005        Table 7).
    CPCS179     0.031   0.117    0.257   0.024
    CPCS360     0.045   75.20    0.427   0.072        4.4  Results for Incremental MAP Test
    Hailﬁnder   2.281   0.109    0.219   0.266        Our last experiment focused on the robustness of the four al-
    Pathﬁnder   0.052   0.056    0.098   0.005        gorithms to the number of MAP variables. In this experiment,
    Andes       14.49   1.250    4.283   2.406        we set the number of evidence variables to be 100 and gen-
    Win95pts    0.035   0.041    0.328   0.032        erated MAP problems with an increasing number of MAP
    Munin       3.064   4.101    19.24   1.763        nodes and ran four algorithms on these cases. We chose
    HRL1        0.493   51.18    2.831   0.193        the Munin network, because it seemed the hardest network
    HRL2        0.092   3.011    2.041   0.169        among the group 1 & 2 and had sufﬁciently large sets of root
                                                      and leaf nodes. The running time for each of the cases are
Table 5: Average running time in seconds of the P-SYS,P- shown in Figure 2. Typically, P-SYS and P-LOC need more
LOC,ANNEALEDMAP and DWA* algorithms on the ﬁrst       running time in face of more complex problems, while AN-
and second group of networks.                         NEALEDMAP and DWA* seem more robust in comparison.

NEALEDMAP, P-LOC   and P-SYS algorithms showed similar
efﬁciency on all except the CPCS360 and Andes networks.
DWA* generated solutions within the shortest time on the av-        P-Sys
                                                                    P-Loc
erage. Its smaller variance of the search time indicates that       AnnealingMAP
                                                                    DWA*
DWA* is more stable across different networks.
  For the second group, which consists of large Bayesian net-
works, P-SYS,ANNEALEDMAP and DWA* were all efﬁ-
cient. DWA* search still spent shortest time on the average,


while the P-LOC was much slower on the HRL1 network.       Running  Time (m)
4.3  Results for the Third Group
The third group consisted of two complex Bayesian networks:
Barley and Diabetes, many nodes of which have more than
                                                                   Number of MAP Variables(100 Evidences)
10 different states. Because the P-SYS algorithm did not pro-
duce results within the time limit, the only available mea-
                                                      Figure 2: Plot of the running time of the P-SYS,P-LOC,
sure of accuracy was a relative one: which of the algorithms
                                                      ANNEALEDMAP and DWA* algorithms when increasing the
found an assignment with a higher probability. Table 6 lists
                                                      number of MAP nodes on the Munin network.
the number of cases that were solved differently between the
P-LOC,ANNEALEDMAP, and DWA* algorithms.     PL, PA

                                                IJCAI-07
                                                  2389