                             Inverse Resolution as Belief Change
                    Maurice Pagnucco                             David Rajaratnam
        ARC Centre of Excel. for Autonomous Sys.     School of Comp. Science and Engineering
        School of Comp. Science and Engineering          The University of New South Wales
            The University of New South Wales              Sydney, NSW, 2052, Australia.
              Sydney, NSW, 2052, Australia.                Email: daver@cse.unsw.edu.au
              Email: morri@cse.unsw.edu.au
                    Abstract                            In this paper we investigate inductive logic programming
                                                      as a form of belief change. In particular, we consider
    Belief change is concerned with modelling the way how the inverse resolution operators introduced by Mug-
    in which a rational reasoner maintains its beliefs as gleton [Muggleton, 1987; Muggleton and Buntine, 1988;
    it acquires new information. Of particular interest Muggleton, 1992] can be re-constructed as belief change op-
    is the way in which new beliefs are acquired and de- erators. Our strategy is to view the machine learning process
    termined and old beliefs are retained or discarded. as one in which the agent’s belief corpus represents its cur-
    A parallel can be drawn to symbolic machine learn- rent theory. Examples are considered consecutively as new
    ing approaches where examples to be categorised   information that causes the agent to revise its belief corpus
    are presented to the learning system and a theory according to one of the operations. This is essentially the ap-
    is subsequently derived, usually over a number of proach adopted in theory revision.
    iterations. It is therefore not surprising that the Since the inductive process that inverse resolution is trying
    term ‘theory revision’ is used to describe this pro- to capture is an unsound form of logical inference (i.e., the
    cess [Ourston and Mooney, 1994]. Viewing a ma-    conclusions do not necessarily follow from the premises) we
    chine learning system as a rational reasoner allows require a form of belief change that allows for this type of in-
    us to begin seeing these seemingly disparate mech- ference. Unfortunately, the AGM operators for belief change
    anisms in a similar light.                        do not cater for this inferential process. The problem stems
    In this paper we are concerned with characterising from the fact that the inductive process requires the reasoner
    the well known inverse resolution operations [Mug- to formulate and accept hypotheses; hypotheses that go be-
    gleton, 1987; 1992] (and more recently, inverse en- yond the content carried by the new information (examples)
    tailment [Muggleton, 1995]) as AGM-style belief   and current belief corpus. As a result we use the notion of ab-
    change operations. In particular, our account is  ductive expansion introduced by Pagnucco [Pagnucco et al.,
    based on the abductive expansion operation [Pag-  1994; Pagnucco, 1996] and formulate the required conditions
    nucco et al., 1994; Pagnucco, 1996] and charac-   to be satisﬁed by the epistemic entrenchment relation in order
    terised by using the notion of epistemic entrench- for the inverse resolution operators to be carried out.
    ment [G¨ardenfors and Makinson, 1988] extended      The aim  of this paper is to take a step towards rec-
    for this operation. This work provides a basis for onciling the areas of symbolic machine learning and be-
    reconciling work in symbolic machine learning and lief change. To that end, we show how the various in-
    belief revision. Moreover, it allows machine learn- verse resolution operations—absorption, identiﬁcation, inter-
    ing techniques to be understood as forms of non-  construction, intra-construction and truncation—can be
    monotonic reasoning.                              viewed as AGM-style belief change operations. In particu-
                                                      lar, we show how the inverse resolution operations can be
  The study of belief change is concerned with how a rational captured as forms of abductive expansion by providing the
reasoner maintains its beliefs in the face of new information. required restrictions on the epistemic entrenchment relation.
As this new information is acquired it must be assimilated In the next section we brieﬂy survey AGM belief change,
into the reasoner’s stock of beliefs. This requires that de- abductive expansion and the inverse resolution operators. In
cisions be made as to which beliefs are retained, which are Section 2 we provide the necessary conditions for each in-
abandoned and which are incorporated by the reasoner. In verse resolution operation. Our approach is discussed in Sec-
this paper we shall consider belief change that adheres to the tion 3 and suggestions for future work provided in Section 4.
popular AGM [G¨ardenfors, 1988] paradigm.
  Symbolic machine learning deals with the generalisation of 1 Background
data for the purpose of classiﬁcation and categorisation. Here We consider an underlying classical propositional logic with
we ﬁnd the closely related notion of theory revision [Ourston a propositional language L, over a set of atoms, or proposi-
and Mooney, 1994]. It too can be viewed as a form of belief tional letters, P = {a, b, c, . . .}, and truth-functional connec-
change; one in which the aim is to assimilate the new infor- tives ¬, ∧, ∨, →, and ↔. L also includes the truth-functional
mation by generalising the reasoner’s belief corpus.  constants ⊤ and ⊥. The closure operator Cn denotes classicalpropositional consequence. We use the term belief set to refer It can be shown that the operation of abductive expansion sat-
to a set of formulas closed under Cn (i.e., K = Cn(K)). K isﬁes the following rationality postulates:
represents the set of all belief sets. The distinguished element (K⊕1) For any sentence φ and any belief set K,
K⊥  ∈ K denotes the inconsistent belief set.                K  ⊕ φ is a belief set
  We shall also adopt the following linguistic conventions (K⊕2) If ¬φ 6∈ K, then φ ∈ K ⊕ φ
to simplify the presentation. Lower case Greek characters (K⊕3) K ⊆ K ⊕ φ
φ, ψ, χ, . . . will represent arbitrary logical formulas. Lower (K⊕4) If ¬φ ∈ K, then K ⊕ φ = K
case Roman characters k, l, . . . will denote propositional lit- (K⊕5) If ¬φ 6∈ K, then ¬φ 6∈ K ⊕ φ
erals. Upper case Roman characters A, B, C, . . . denote (K⊕6) If K ⊢ φ ↔ ψ, then K ⊕ φ = K ⊕ ψ
propositional Horn clauses (disjunctions of literals contain- (K⊕7) K ⊕ φ ⊆ Cn(K ⊕ (φ ∨ ψ) ∪ {φ})
ing at most one positive literal).                    (K⊕8) If ¬φ 6∈ K ⊕ (φ ∨ ψ), then K ⊕ (φ ∨ ψ) ⊆ K ⊕ φ
1.1  Belief Change
                                                      Postulate (K⊕1) maintains the expanded belief corpus as a
The mostly widely accepted approach to belief change is the belief set. (K⊕2) states that the new information should be
one proposed by Alchourr´on, G¨ardenfors and Makinson [Al- included in the expanded belief state provided that it is con-
chourr´on et al., 1985; G¨ardenfors, 1988; Hansson, 1999]. sistent with the original corpus while (K⊕3) requires that the
This approach is motivated by the concern to characterise ra- expanded corpus be at least as large as the original. When the
tional belief change operators. That is, operations of belief new information is inconsistent with the corpus, (K⊕4) pre-
change that are guided by principles of rationality. The AGM vents change. Postulate (K⊕5) ensures that the corpus does
approach introduces three belief change operations: expan- not expand into inconsistency. (K⊕6) states that the expan-
sion (+), contraction (−), and revision (∗). Expansion deals sion process is syntax insensitive. These postulates sufﬁce
with the addition of new information without retraction of ex- to characterise abductive expansion as deﬁned above. Often
isting beliefs; contraction takes care of the removal of belief; postulates (K⊕7) and (K⊕8) are added and restrict the mech-
and, revision deals with the addition of beliefs with the possi- anism that selects hypotheses to one that satisﬁes transitivity.
bility of retracting current beliefs in order to maintain consis- An elegant way of constructing an abductive expansion op-
tency. Each of these operations takes a belief set representing eration is by providing an ordering of epistemic entrenchment
the reasoner’s original belief corpus and a sentence represent- over the sentences in the language that indicates their plausi-
ing the new information, returning the modiﬁed belief corpus: bility or importance. For AGM operations, entrenchment or-
(+, −, ∗ : K × L → K). The belief change operations are ders the beliefs while the non-beliefs are relegated together as
characterised by rationality postulates and various construc- the least important sentences. For abductive expansion how-
tions. The interested reader is referred to [G¨ardenfors, 1988; ever, we need to be able to discriminate between potential
Hansson, 1999] for further details.                   hypotheses and so the traditional epistemic entrenchment or-
  In this paper we are concerned with the process of belief dering is modiﬁed so that beliefs are lumped together as the
expansion as this is the typical setting under which inverse most important while the non-beliefs are ordered by impor-
resolution is applied. This should not be seen as a signif- tance in what we term an abductive entrenchment ordering.
icant restriction because belief revision proper can be ob-
tained by combining contraction and expansion. However, Deﬁnition 1.2 An ordering ≤ over L is an abductive en-
AGM  expansion is very limited in scope. It can be shown trenchment ordering iff it satisﬁes (SEE1)–(SEE3) and con-
that AGM expansion corresponds to closure under logical de- dition (AE4):
duction: K+φ = Cn(K∪{φ}). What we require is a form of (SEE1) For any φ, ψ, χ ∈ L, if φ ≤ ψ and ψ ≤ χ then φ ≤ χ
belief change that elaborates upon the newly acquired infor- (SEE2) For any φ, ψ ∈ L, if {φ} ⊢ ψ then φ ≤ ψ
mation. Such an operation has been introduced by Pagnucco (SEE3) For any φ, ψ ∈ L, φ ≤ φ ∧ ψ or ψ ≤ φ ∧ ψ
[Pagnucco, 1996] in the form of an operation called abductive (AE4) When K 6= K⊥, φ ∈ K iff ψ ≤ φ for all ψ ∈ L
expansion (⊕ : K×L → K). This operation looks to amplify
the newly acquired information by trying to ﬁnd an explana- This ordering is a total preorder (SEE1)–(SEE3) in which the
tion for it (via abduction) and is reminiscent of the way that beliefs are maximally entrenched (AE4); it effectively ranks
inverse entailment is deﬁned [Muggleton, 1992]. Formally, the reasoner’s non-beliefs.
abductive expansion may be deﬁned as follows:           The following properties of abductive epistemic entrench-
Deﬁnition 1.1 K ⊕ φ is an abductive expansion of K with ment will be useful in proving the results in this paper.
respect to φ iff                                      Lemma 1.1  Suppose ≤ satisﬁes (SEE1)–(SEE3), then1
            Cn(K  ∪ {ψ})  for some ψ ∈ L such that:     1. For all φ ∈ L either φ ≤ ψ or ¬φ ≤ ψ for all ψ ∈ L.
                            (i) K ∪ {ψ} ⊢ φ
 K ⊕ φ =                                                2. For all φ ∈ L, {ψ : φ ≤ ψ} = Cn({ψ : φ ≤ ψ})
                           (ii) K ∪ {ψ} 6⊢ ⊥
                                                        3.
            K             if no such ψ exists             φ ∧ ψ = min(φ, ψ)
                                                      4. φ ∨ ψ ≥ max(φ, ψ)
That is, when confronted with new information φ the reasoner
seeks an hypothesis ψ that explains φ with respect to its cur- 1Note that φ < ψ iff φ ≤ ψ and ψ 6≤ ψ and φ = ψ iff φ ≤ ψ
rent beliefs and incorporates this explanation into its corpus. and ψ ≤ ψ.The ﬁrst property states that for every sentence φ, either φ V-Operators
or ¬φ (possibly both) is minimally entrenched. The second Previously, a single resolution step was presented in terms of
property notes that taking all sentences more entrenched than a “V”-shaped diagram. The two V-operators can derive one of
a certain rank gives a deductively closed set of sentences. The the clauses at the top of the V given the other clause at the top
third property indicates that a conjunction is entrenched at the of the V and the clause at the bottom of the V. The absorption
same level as the minimum of the conjuncts while the fourth operator derives C2 given C1 and C while the identiﬁcation
property states that a disjunction is at least as entrenched as operator derives C1 given C2 and C.
the maximally entrenched disjunct. These are well known Since the new clause is constructed by ﬁnding the inverse
properties of orderings satisfying conditions (SEE1)–(SEE3) of a resolved product, the notion of a resolved quotient3 of C
which are termed expectation orderings by G¨ardenfors and and C1 is deﬁned [Muggleton and Buntine, 1988] as C2 =
Makinson [G¨ardenfors and Makinson, 1994].            C/C1. Rearranging equation (1) for resolution can obtain
  Pagnucco [Pagnucco, 1996] gives a condition that allows C2 = (C \(C1 \{l}))∪{¬l} under the following assumption
us to determine an abductive expansion function ⊕≤ for a [Muggleton and Buntine, 1988]:
particular epistemic state K given an abductive entrenchment
                                                          Separability Assumption — Clauses 1    and  2
ordering ≤.                                             •                                C  \ {l}    C  \
                                                          {¬l} contain no common literals.
    (C⊕)    ψ ∈ K ⊕≤  φ iﬀ either ψ ∈ K or
              both ¬φ 6∈ K and φ → ¬ψ < φ →  ψ        This assumption also simpliﬁes the calculation of resolved
We omit the subscript ≤ unless necessary. This condition quotients (i.e., absorptions or identiﬁcations).
states that we can accept sentence ψ in the abductively ex- W-Operators
panded belief state whenever it is believed in the original cor- Joining two resolution “V”s we obtain a form analogous to
pus or when ψ is more plausible given φ than ¬ψ given φ. that for the V-operators [Muggleton and Buntine, 1988]:
1.2  Inverse Resolution                                              C1        A         C2
Resolution is a valid inference procedure which deduces a             @        @         
                                                                         @        @
clause C from two clauses C1 and C2. Given a clause C1                   @         @ 
containing a literal l and a clause C2 containing the literal ¬l,          B1       B2
the resolvent of C1 and C2, denoted C = C1.C2, is
                                                      In this situation a common literal l, contained in A, resolves
                   1          2
            C  = (C \ {l}) ∪ (C \ {¬l})         (1)   with clauses C1 and C2 to produce B1 and B2. Clauses B1
This process may be visualised in the following diagram and B2 represent the new information and clauses A, C1 and
                           2
[Muggleton and Buntine, 1988]:                        C2 the constructed clauses. Interestingly, since l is resolved
                    C1        C2                      away, the constructed clauses A, C1 and C2 will contain a
                    +         −                       literal whose propositional symbol does not appear in either
                     @                                B1 or B2. If l occurs negative in A then the operator is re-
                       @@                             ferred to as intra-construction and if it occurs positive in A
                         C                            the operator is called inter-construction.
  Inverse resolution (and more recently, inverse entailment),
on the other hand, is a machine learning technique based upon Truncation
the following characterisation of inductive inference [Mug- The truncation operator results from the special case where
gleton, 1987; 1989]. Given a partial domain theory Γ and a the empty clause occurs at the base of a V or W schemata.
positive example E that is not a consequence of the domain In a propositional system, this corresponds to dropping neg-
theory (Γ 6⊢ E) we attempt to determine a new domain theory ative literals from a clause. In the ﬁrst-order case Muggle-
Γ′, using Γ and E, that will account for the example (Γ′ ⊢ E) ton and Buntine [Muggleton and Buntine, 1988] show that
and the original domain theory (Γ′ ⊢ Γ). If we think of Γ′ as two literals may be truncated by taking their least-general-
Γ ∪ I where I represents the result of inverse resolution, then generalisation. Rouveriol and Puget [Rouveirol and Puget,
the relationship with abduction should become much clearer. 1990] generalise this to a truncation operator which replaces
In practice, the domain theory and example are usually repre- terms by variables and drops literals from clauses.
sented as Horn clauses. This technique is based on inverting In the case of a propositional language, a number of
the resolution process and consists of ﬁve operators: two V- schema may be used to compute the required inverse resolu-
operators, two W-operators and the truncation operator. tion. These are displayed in Table 1 (see [Muggleton, 1987;
                                                            4
  So as not to countenance invalid inference, the notion of an 1989]). In each instance, the top line represents the ini-
oracle is adopted. An oracle is an entity that accepts a clause, tial clauses and the bottom line represents the constructed
constructed using one of the inverse resolution operators, if clause(s). We shall use these directly in our approach to char-
it is valid in the intended model. In our framework abduc- acterise the inverse resolution operators as belief change op-
tive entrenchment is used to discriminate between potential erators (abductive belief expansion operators in fact).
hypotheses and takes the place of the oracle.
                                                         3Absorption is considered here. Identiﬁcation is similar.
  2The plus (+) (respectively minus (−)) sign in the diagram de- 4We adopt a slight renaming of the terms to those presented in
notes that the literal resolved upon appears positive (respectively [Muggleton, 1989], having found them more amenable to study. In
negative) in that clause.                             the case of absorption and identiﬁcation, the ﬁrst clause on the top             Name               Rule                    What we require here is a restriction on abductive entrench-
        Absorption        A→k, A∧B→j                  ment that, given A → k ∈  K, guarantees this condition.
                            B∧k→j                     Consider the following restriction:
                          B∧k→j, A∧B→j
        Identiﬁcation         A→k                      (Abs)               ⊥  < B ∧ k → j
        Inter-Construction A∧B→j, A∧C→k               It states that the sentence B ∧ k → j is not minimally en-
                          B∧l→j, C∧l→k, A→l           trenched (by Lemma 1.1(1), ⊥ is minimally entrenched). In
                          A∧B→j, A∧C→j
        Intra-Construction A∧l→j, B→l, C→l            other words, when an example A ∧ B →  j is presented,
                                                      any hypothesis of the form B ∧ k → j that is not mini-
        Truncation        A∧B→j
                           A→j                        mally entrenched and where A → k is believed, will be ac-
                                                      cepted. Now, by Lemma 1.1, condition (Abs) is equivalent to
   Table 1: Propositional inverse resolution operators.5 ¬(B ∧ k → j) < B ∧ k → j which says that this hypothesis
                                                      is preferred to its negation. The following theorem indicates
                                                      the appropriateness of this condition.

2  Approach                                           Theorem 2.1 Given a belief set K 6= K⊥, a sentence A →
Our aim here is to render the individual inverse resolution k ∈ K and an abductive epistemic entrenchment relation ≤
operations as belief change operators. In particular, we shall satisfying (Abs), the abductive expansion operator ⊕≤ ob-
give conditions on the abductive epistemic entrenchment that tained from ≤ via condition (C⊕) gives the same result as
will guarantee that the results of an inverse resolution oper- Absorption (i.e., B ∧ k → j ∈ K ⊕ (A ∧ B → j)).
ation are included in the revised (or, more precisely, abduc- Proof: Now (A → k) ∧ (B ∧ k → j) ⊢ A ∧ B → j so
tively expanded) belief corpus.                       (A →  k) ∧ (B ∧ k →  j) ≤ A ∧ B  →  j by (SEE2). But
  To make this more precise, we take the reasoner’s belief B ∧ k → j ≤ A → k by (AE4). It follows by Lemma 1.1(3)
corpus K and an example φ as the newly acquired informa- that B ∧ k → j = A ∧ B → j. Furthermore (B ∧ k →
tion. We want to construct abductive expansion operations j)∧(A∧B → j) = B∧k → j. But by (Abs) ⊥ < (B∧k →
⊕ for each of the ﬁve inverse resolution operations such that j) ∧ (A ∧ B → j). Therefore ¬((B ∧ k → j) ∧ (A ∧ B →
K ⊕ φ represents the result of applying that particular inverse j)) = ⊥ by Lemma 1.1(1) and so ¬((B ∧k → j)∧(A∧B →
resolution operation on the given example and elements of the j)) < B ∧ k → j. It follows by Lemma 1.1(4) ¬((B ∧ k →
current belief corpus. This construction is achieved by spec- j) ∧ (A ∧ B → j)) < ¬(A ∧ B → j) ∨ (B ∧ K → j). Hence
ifying a condition(s) on abductive entrenchment that, when by logical equivalence (A ∧ B → j) → ¬(B ∧ k → j) <
added to conditions (SEE1)–(SEE3) and (AE4), guarantees (A ∧ B → j) → (B ∧ k → j) as required.         2
the result of the inverse resolution operation.         Now it may seem strange that condition (Abs) does not
  As noted above, abductive entrenchment encodes the  mention the new example A ∧ B → j. By Lemma 1.1(2) it
‘choices’ that are made by the oracle. In other words, en- can be shown that whenever A → k ∈ K, it is always the
trenchment will be used to determine which potential hy- case that B ∧ k → j ≤ A ∧ B → j. That is, the new example
potheses are accepted and which are rejected. Furthermore, it cannot be less entrenched than any potential hypothesis.
may be the case that in expanding the reasoner’s belief corpus However, we might re-consider the typical Absorption case
in this way, more than one potential hypothesis is accepted for and look at what happens when we assume that A∧B → j ∈
a given example φ. This could be easily restricted to one hy- K and A → k is the new example. In this case we require
pothesis by imposing additional restrictions on the entrench- condition (Abs) together with the following condition:
ment ordering however, intuitively, this seems procrustean. (Abs′)        B ∧ k → j ≤ A → k
Multiple generalisations allow for a more ﬂexible character- The following result indicates this formally:
isation of the inverse resolution process. Finally note that
we will only be concerned with examples (new information) Theorem 2.2 Given a belief set K 6= K⊥, a sentence A ∧
                                                      B  → j ∈ K  and an abductive epistemic entrenchment rela-
that are presented as Horn clauses in keeping with the way in                     ′
which inverse resolution is traditionally realised.   tion ≤ satisfying (Abs) and (Abs ), the abductive expansion
                                                      operator ⊕≤ obtained from ≤ via condition (C⊕) gives the
2.1  Absorption                                       same result as Absorption (i.e., B ∧ k → j ∈ K ⊕ (A → k)).
The typical case for Absorption occurs when A → k is be- 2.2 Identiﬁcation
lieved and A ∧ B → j is presented as an example (newly The typical case for Identiﬁcation occurs when B ∧ k → j
acquired information). The Absorption operator looks for a is believed and A ∧ B → j is presented as the new example.
sentence B ∧ k → j to add to the current theory in this case. Identiﬁcation returns A → k to be added to the belief corpus.
By (C⊕), the condition we need to guarantee is therefore: The condition that we need to guarantee is, by (C⊕):
(A∧B  → j) → ¬(B∧k  →  j) < (A∧B  → j) → (B∧k  → j)    (A ∧ B →  j) → ¬(A →  k) < (A ∧ B → j) → (A →  k)
                                                        Analogously to Absorption we require the following re-
line of the schemata is taken from the domain theory while the sec- striction on entrenchment to guarantee this condition.
ond represents the new data.
  5Here A, B, C represent conjunctions of atoms while j, k, l (Ident)         ⊥ < A →  k
represent atoms.                                      It states that any potential hypothesis A → k is not minimallyentrenched and by Lemma 1.1 is equivalent to saying that any 2.4 Intra-Construction
potential hypothesis be preferred to its negation. This result In Intra-construction, again without loss of generality, we
is stated formally in the following theorem:          may assume that A ∧ B →  j ∈ K and A ∧ C  → j is the
Theorem 2.3 Given a belief set K 6= K⊥, a sentence    new example. In this case, by (C⊕), we need to guarantee
B ∧ k →  j ∈ K and an abductive epistemic entrenchment the following three conditions:
relation ≤ satisfying (Ident), the abductive expansion oper-
                                                      (A∧C  →  j) → ¬(A∧l →  j) < (A∧C  → j) → (A∧l →  j)
ator ⊕≤ obtained from ≤ via condition (C⊕) gives the same
result as Identiﬁcation (i.e., A → k ∈ K ⊕ (A ∧ B → j)). (A ∧ C → j) → ¬(B → l) < (A ∧ C → j) → (B →  l)
The proof follows a similar idea to that for Absorption and (A ∧ C → j) → ¬(C → l) < (A ∧ C → j) → (C → l)
so, due to lack of space, we omit it here.
  Again, if we re-consider Identiﬁcation and suppose that Again, the desired result is achieved by ensuring that none of
A ∧ B →  j ∈ K while B ∧ k → j is the new example, then the potential hypotheses is minimally entrenched.
we also require the following condition on entrenchment: (Intra)  ⊥  < (A ∧ l → j) ∧ (B → l) ∧ (C → l)
(Ident′)            A →  k ≤ B ∧ k → j                The following theorem shows that this condition is correct.

                                                      Theorem 2.5 Given a belief set K 6=  K⊥, a sentence
2.3  Inter-Construction                               A ∧ B  → j ∈ K  and an abductive epistemic entrenchment
In the case of Inter-Construction there is only ever one case to relation ≤ satisfying (Intra), the abductive expansion oper-
consider because, without loss of generality, we may assume ator ⊕≤ obtained from ≤ via condition (C⊕) gives the same
that sentence A ∧ B → j is believed and A ∧ C → k is  result as Intra-construction (i.e., A ∧ l → j, B → l, C →
presented as the new example. In this case three new clauses l ∈ K ⊕ (A ∧ C → j)).
are derived (B ∧ l → j, C ∧ l → k, and A → l) and we need
to guarantee three conditions which are, by (C⊕):     2.5  Truncation
(A∧C  → k) →  ¬(B∧l →  j) < (A∧C →  k) → (B∧l →  j)   For truncation, there are no requirements on the original be-
                                                      lief corpus and all we assume is that A ∧ B → j is the new
(A∧C  → k) →  ¬(C∧l →  k) < (A∧C →  k) → (C∧l →  k)   information. Truncation generalises this by adding A → j to
 (A ∧ C →  k) → ¬(A →  l) < (A ∧ C → k) → (A → l)     the belief corpus. The condition we need guarantee is:
  Given that A ∧ B → j ∈ K, the condition we require is: (A ∧ B → j) → ¬(A → j) < (A ∧ B → j) → (A →  j)
(Inter)   ⊥ <  (B ∧ l → j) ∧ (C ∧ l → k) ∧ (A → l)    This can be achieved by the following condition stating that
This condition can be read in a number of ways. Literally it A → j is not minimally entrenched.
says, in analogy to  and       , that the conjunction of
               (Abs)    (Ident)                        (T runc)               ⊥  < A →  j
the potential hypotheses is not minimally entrenched. How-
ever, by Lemma 1.1(3), this also means that each of the in- The following theorem indicates that this is correct.
dividual hypotheses themselves is not minimally entrenched Theorem 2.6 Given a belief set K 6= K⊥ and an abduc-
and so by Lemma 1.1(1) that they are each preferred to their tive epistemic entrenchment relation ≤ satisfying (T runc),
negations. We now obtain the following result.        the abductive expansion operator ⊕≤ obtained from ≤ via
Theorem 2.4 Given a belief set K 6= K⊥, a sentence    condition (C⊕) gives the same result as Truncation (i.e.,
A ∧ B →  j ∈ K  and an abductive epistemic entrenchment A → j ∈ K ⊕ (A ∧ B → j)).
relation ≤ satisfying (Inter), the abductive expansion oper-
ator ⊕≤ obtained from ≤ via condition (C⊕) gives the same 3 Discussion
result as Inter-construction (i.e., B ∧l → j, C ∧l → k, A → The idea of using a form of belief expansion capable of in-
l ∈ K ⊕ (A ∧ C → k)).                                 troducing new hypotheses has also been suggested by Levi
  In inverse resolution, the literal l in the sentences above is [Levi, 1991] who introduces an operation known as deliber-
newly introduced. It represents a concept that was not present ate expansion. While his method of determining and select-
in the original theory. The terms ‘predicate invention’ or ing hypotheses differs from ours, the notion that belief ex-
‘constructive induction’ are often used to describe what the pansion should allow for the addition of sentences that do not
W-operators are achieving by introducing a new element into necessarily follow from the new information and the belief
the object language. Current theories of belief change do not corpus is the same. The level to which the reasoner wishes
allow for the expansion of the object language so we assume to accept such hypotheses will be determined by their aver-
that the language is suitably equipped with a supply of propo- sion to courting erroneous beliefs and this is reﬂected in the
sitions that can be used to create literals like l here. Of course, abductive entrenchment ordering in our framework. Such er-
l will be present in any belief set; tautologies like l → l are rors, when realised later, may of course be ﬁxed by subse-
present in every belief set. However, we can require addi- quent revision. We follow Levi’s commensurability thesis in
tional conditions like l 6∈ K if we want to ensure that l is maintaining that any form of belief revision can be obtained
newly generated. This is not an ideal solution as one is dif- via a sequence of (abductive) expansions and contractions.
ﬁcult to derive using belief sets but should go some way to Forms of symbolic machine learning such as inverse res-
achieving the desired results.                        olution are often equated with inductive inference (or what