           Simple Training of Dependency Parsers via Structured Boosting

           Qin Iris Wang                      Dekang Lin                     Dale Schuurmans
 Department of Computing Science              Google Inc.            Department of Computing Science
        University of Alberta           lindek@google.com                  University of Alberta
    wqin@cs.ualberta.ca                                                 dale@cs.ualberta.ca


                    Abstract                          It has been shown in many application areas that structured
                                                      prediction models that directly capture the relationships be-
    Recently, signiﬁcant progress has been made on
                                                      tween output components perform better than models that do
    learning structured predictors via coordinated train-
                                                      not directly enforce these relationships [Lafferty et al., 2001;
    ing algorithms such as conditional random ﬁelds
                                                      Tsochantaridis et al., 2004; Altun et al., 2003; Taskar et al.,
    and maximum margin Markov networks. Unfor-
                                                      2003; 2004]. In particular, these ideas have started to have
    tunately, these techniques are based on specialized
                                                      a signiﬁcant impact in the ﬁeld of natural language pars-
    training algorithms, are complex to implement, and
                                                      ing [McDonald et al., 2005; McDonald and Pereira, 2006;
    expensive to run. We present a much simpler ap-
                                                      Corston-Oliver et al., 2006], where state of the art results
    proach to training structured predictors by apply-
                                                      have recently been achieved through the use of structured
    ing a boosting-like procedure to standard super-
                                                      training algorithms. Parsing is a large-scale structured pre-
    vised training methods. The idea is to learn a lo-
                                                      diction problem where multiple predictions must be coordi-
    cal predictor using standard methods, such as lo-
                                                      nated to achieve an accurate parse for a given input sentence.
    gistic regression or support vector machines, but
                                                      Parsing is a particularly challenging and important task for
    then achieve improved structured classiﬁcation by
                                                      machine learning, since it involves complex outputs (parse
    ”boosting” the inﬂuence of misclassiﬁed compo-
                                                      trees) and limited training data (manually parsed treebanks),
    nents after structured prediction, re-training the lo-
                                                      and yet machine learning methods have proved to provide the
    cal predictor, and repeating. Further improvement
                                                      best approach to obtaining robust parsers for real data.
    in structured prediction accuracy can be achieved
    by incorporating ”dynamic” features—i.e. an ex-     One drawback with current structured prediction training
    tension whereby the features for one predicted    algorithms, however, is that they involve new, specialized
    component can depend on the predictions already   parameter optimization algorithms, that are complex, non-
    made for some other components.                   trivial to implement, and usually require far more computa-
                                                      tion than standard classiﬁcation learning methods [Lafferty et
    We apply our techniques to the problem of learn-
                                                      al., 2001; Taskar et al., 2003]. The main reason for increased
    ing dependency parsers from annotated natural lan-
                                                      complexity is the fact that some form of structured prediction
    guage corpora. By using logistic regression as an
                                                      algorithm, such as a parser or a Viterbi decoder, must be con-
    efﬁcient base classiﬁer (for predicting dependency
                                                      sidered in the underlying training principle, which causes the
    links between word pairs), we are able to efﬁciently
                                                      structured inference of output predictions to be tightly cou-
    train a dependency parsing model, via structured
                                                      pled with the parameter optimization process during training.
    boosting, that achieves state of the art results in En-
    glish, and surpasses state of the art in Chinese.   In this paper, we demonstrate the somewhat surprising re-
                                                      sult that state of the art performance on natural language pars-
                                                      ing can be achieved through the use of conventional, local
1  Introduction                                       classiﬁcation methods. In particular, we show how a simple
Recently, a signiﬁcant amount of progress has been made on form of structured boosting can be used to improve the train-
developing training algorithms for learning structured predic- ing of standard local classiﬁcation methods, in the structured
tors from data [Tsochantaridis et al., 2004; Altun et al., 2003; case, without modifying the underlying training method. The
Taskar et al., 2003]. Structured prediction learning extends advantage of this approach is that one can use off-the-shelf
the standard supervised learning framework beyond the uni- classiﬁcation techniques, such as support vector machines
variate setting, where a single output variable is considered, or logistic regression, to achieve competitive structured pre-
to the multivariate setting, where complex, non-scalar pre- diction results with little additional effort. We achieve this
dictions yˆ must be produced for inputs x. The challenge through the use of two simple ideas. First, we introduce a
is that each component yˆi of yˆ should not depend only on very simple form of “structured boosting”, where a structured
the input x, but instead should take into account correla- predictor, such as a parser, is used to modify the predictions
tions between yˆi and its neighboring components yˆj ∈ yˆ. of the local, weak learning algorithm, which then inﬂuences

                                                IJCAI-07
                                                  1756                                                                 S
                                                                     VP
                                                                        S
    Investors continue to pour cash into money funds
                                                                         VP
             Figure 1: A dependency tree                                     VP
                                                            NP
the example weightings and subsequent hypotheses, implic-                        NP    PP
itly encouraging more accurate structured predictions. Sec-                                   NP
ond, we exploit an old idea from natural language parsing,
but highlight it here more generally, that “dynamic” features NNS   VBP  TO  VB  NNIN     NN    NNS
can be used in the local classiﬁers, which take into account Investors continue to pour cash into money funds
the previous classiﬁcations of a restricted set of neighboring     Figure 2: A constituency tree
examples.
  We demonstrate these ideas concretely on the problem of Cherry and Lin, 2003; Ding and Palmer, 2005], coreference
learning natural language dependency parsers from labeled resolution [Bergsma and Lin, 2006], and information extrac-
(treebank) data. Although dependency parsing is a very com- tion [Culotta and Sorensen, 2004].
plex problem, we are able to achieve state of the art results We consider the problem of automatically learning a de-
by training a local “link predictor” that merely attempts to pendency parser for a given language from a treebank: a col-
predict the existence and orientation of a link between two lection of manually parsed sentences. Treebanks usually con-
words given input features encoding context—without wor- sist of constituency parses, but a dependency parse can be
rying about coordinating the predictions in a coherent global automatically extracted from a constituency parse by a few
parse. Instead, a wrapper approach, based on structured simple rules [Bikel, 2004]. Thus, the training data consists
boosting, is used to successively modify the training data so of a set of sentences, each annotated with a directed spanning
that the training algorithm is implicitly encouraged to facili- tree over words (Figure 1). For most languages, like English
tate improved global parsing accuracy.                and Chinese, this tree is planar (often called “projective”).
  The remainder of the paper is organized as follows. First, Although a dependency tree is a complex object, parsing
in Section 2 we brieﬂy describe the dependency parsing prob- can conceptually be reduced to a set of local classiﬁcations:
lem and introduce some of our notation and terminology. We each word pair in a sentence can be classiﬁed into one of three
then explain the relationship between structured prediction categories; no link, left link, or right link. One aspect of the
learning and traditional classiﬁcation learning in Section 3, problem is local: for a given pair of words in a given sentence
and point out opportunities for connecting these problems. context, what should their link label be? The other aspect
Next, in Section 4 we brieﬂy explain, in general terms, the of the problem is global: how should the local link predic-
idea of “dynamic” features in classiﬁcation learning and how tions be coordinated globally to produce a consistent and ac-
these can be used to improve structured prediction. Then in curate dependency tree? Both aspects of the problem—local
Section 5 we introduce the main technique proposed in this link prediction versus global link coordination—are crucial
paper, structured boosting, and explain its relation to standard to achieving a state of the art dependency parser. Locally, the
boosting approaches. Finally, in Sections 6 and 7, we de- problem is amenable to standard classiﬁcation learning ap-
scribe our experiments in learning dependency parsers from proaches. Globally, the problem requires coordination—that
treebank data, and show how competitive results can be ob- is, interaction with a parsing algorithm.
tained through the use of standard learning methods. In fact,
our results surpass state of the art accuracy in Chinese pars- 3 From Local to Coordinated Training
ing, and are competitive with state of the art in English. Sec-
tion 8 concludes the paper with a discussion of future work. The problem of learning a structured predictor can generally
                                                      be formulated as follows. We are given a set of annotated
                                                      objects (s1, t1), ..., (sT , tT ), where each object si consists
2  Dependency Parsing                                 of a composite observation (i.e. a sentence consisting of a
Since our main application involves learning dependency word string) and each annotation ti is a complete labeling of
parsers from labeled data, we brieﬂy introduce the problem the relevant subcomponents in si (i.e. a link label for every
and some of the issues it creates. In natural language pars- pair of words in the sentence). Typically, a single composite
ing, a dependency tree speciﬁes which words in a sentence example (si, ti) can be broken down into a (coordinated)
                                                                         (s  ,t  )|    , ...(s ,t )|
are directly related. That is, the dependency structure of a set of local examples i,1 i,1 (si,ti) i,N i,N (si,ti),
natural language sentence is a directed tree where the nodes where each label tij ∈ ti is an atomic classiﬁcation of a sub-
are the words in the sentence and links represent the direct component si,j ∈ si, taken in context (si, ti). For example,
dependency relationships between the words; see Figure 1. asentencesi = wi,1...wi,n and a dependency tree labeling
Generally speaking, a dependency tree is much easier to un- ti = ti,1,2...ti,n−1,n can be decomposed into local examples
                                                      (w  ,w   ; t  )|    , ...(w  ,w   ; t    )|
derstand and annotate than constituency trees that involv- i,1 i,2 i,1,2 (si,ti) i,n−1 i,n i,n−1,n (si,ti),
ing part of speech and phrase labels (e.g., Figure 2). Con- consisting of arbitrary (not necessarily adjacent) word pairs
sequently, there has been a growing interest in dependency and their link label (none, left, right) in context (si, ti).
parsing in recent years. Dependency relations have been The context is important because accurately predicting a
playing important roles in machine translation [Fox, 2002; component label, even locally, requires the consideration of

                                                IJCAI-07
                                                  1757more than just the subcomponent (word pair) itself, but also hind these training algorithms has been to explicitly incor-
the surrounding components, and possibly even the labels of porate the effects of the structured predictor directly into the
some of the surrounding components. (We will discuss the training algorithm. That is, parameter optimization of a lo-
latter point in more detail below.)                   cal predictor is performed by directly considering the im-
  This decomposition facilitates a purely local approach plied effects on the structured (global rather than local) pre-
to the learning problem. Given the original compos-   diction error. The extension to structured training loss has
ite data (s1, t1), ..., (sT , tT ) (e.g. sentences and their been developed for both the large margin training princi-
parses) one can ﬁrst break the data up into local exam- ple of support vector machines [Tsochantaridis et al., 2004;
    (w   ,w  ; t  )|     , ...(w ,w ; t  )|    , ...
ples  1,1  1,2 1,1,2 (s1,t1)  i,j i,k i,j,k (si,ti) , Altun et al., 2003; Taskar et al., 2003] and the maximum con-
ignore the relationships between examples, and use a stan- ditional likelihood principle of logistic regression [Lafferty et
dard supervised learning algorithm to learn a local predictor al., 2001]. Subsequently, training algorithms based on these
wi,j ,wi,k → ti,j,k in context (si, ti). For example, if we re- principles have been applied to parsing [Taskar et al., 2004;
strict attention to linear predictors (support vector machines Wang et al., 2006], and recently resulted in state of the art
or logistic regression models), we only need to learn a weight accuracy for English dependency parsing [McDonald et al.,
vector θ over a set of features deﬁned on the local examples 2005; McDonald and Pereira, 2006]. Unfortunately, the main
f(wi,j ,wi,k,ti,j,k; si, ti). Here, each feature fm computes its drawback with these structured training techniques is that
value based on the component (wi,j ,wi,k), the label ti,j,k,in they are specialized, non-trivial to implement, and require a
their context (si, ti). In this case, a multiclass support vector great deal of reﬁnement and computational resources to ap-
machine [Crammer and Singer, 2001] or logistic regression ply to a signiﬁcant task like parsing [McDonald et al., 2005;
model [Hastie et al., 2001] could be trained in a conventional Corston-Oliver et al., 2006].
manner to achieve an accurate local prediction model.   In this paper, we pursue a simpler, more general approach
  The only question that remains is how to perform valid that can be applied to any local prediction learning strategy,
structured prediction on composite test objects using a lo- without requiring that the underlying training algorithm be
cal prediction model. The problem is that structured clas- modiﬁed, while still ensuring that the training outcome is di-
siﬁcation requires that constraints be respected between the rectly inﬂuenced by the resulting accuracy of the global struc-
classiﬁcations of different local components. For example, tured predictor (the parser).
in dependency parsing, the predicted word pair labels (no
link, left link, right link) must form a valid directed span- 4 Dynamic Features
ning tree, which in the case of some languages like English
and Chinese, should also be a planar tree. This form of Before describing our general structured boosting method, we
global consistency is achieved in practice simply by combin- ﬁrst brieﬂy describe a simple but useful idea for improving
ing a local link classiﬁer with a parsing algorithm. A de- the structured classiﬁcation accuracy of local predictors. The
pendency parsing algorithm is, in effect, a dynamic program- idea is to use so-called “dynamic” features; that is, features
ming algorithm that has the goal of producing a maximum that take into account the labels of (some) of the surrounding
weight spanning tree subject to the constraints [Eisner, 1996]. components when predicting the label of a target component.
The output of the local predictor can be used as a numerical In particular, when predicting the label of a target component
weight to score a potential link, in context, which the parser si,j ∈ si from a composite object si, one can assume that
can then use to make decisions about which link labels to the labels for some other components, say si,j−1 ∈ si,have
choose. In this sense, the role of a local predictor is just to already been computed. The only constraint is that the neigh-
supply a learned scoring function to a pre-existing parsing boring labels used must always be available before attempting
algorithm. Exactly this approach to combining local link pre- to label si,j . For supervised training, this is never a problem,
dictors with dependency parsing algorithms has been tried, because the labels are always available for every component.
with some success, by many researchers—using support vec- However, the real issue arises during testing—that is, during
tor machines [Yamada and Matsumoto, 2003], logistic regres- the structured classiﬁcation of a test object. Here the labels
sion (aka. maximum entropy models) [Ratnaparkhi, 1999; for each component have to be determined in a systematic
Charniak, 2000], and generative probability models [Collins, order that ensures the required features are always available
1997; Wang et al., 2005]—to learn local scoring functions. when the next component needs to be labeled.
  Unfortunately, these simple local learning strategies have The easiest way to illustrate the concept is in a sequential
an obvious shortcoming. The problem is that the training labeling task, like part of speech tagging: Given a sentence
loss being minimized during local parameter optimization has s = w1...wn, the goal is to predict the corresponding tag se-
nothing directly to do with the parser. Although it is true quence t = t1...tn.Herethepreceding tags can be used as
that an accurate local predictor is a prerequisite for an ac- features for the current word under consideration—e.g. in
curate parse prediction, the parameters of the local model a maximum entropy Markov model (MEMM) [McCallum et
are not being trained to directly optimize the global accu- al., 2000]—while still permitting an efﬁcient Viterbi decod-
racy of the parser. That is, a far better choice of parame- ing algorithm to be used for structured prediction. Alterna-
ters might exist within the given space deﬁned by the fea- tively, one could use the following tags as features or use both
tures that leads to better global parsing accuracy. This is the preceding and following tags [Toutanova et al., 2003].
where the advent of recent training algorithms for learning The idea of dynamic features, however, is more general than
structured predictors has been helpful. The main idea be- sequence labeling and maximum conditional likelihood train-

                                                IJCAI-07
                                                  1758ing.                                                    • Then the global structured predictor is used to classify
  For dependency parsing, dynamic features can also be eas- the data using the current weak local predictor. For ex-
ily employed. For example, when considering a possible link ample, a parsing algorithm is used to re-predict the train-
label that connects a head word to a subordinate word, one ing labels, in a coordinated global fashion, using the
will always have access (in any standard parsing algorithm) learned link predictor as an internal scoring function.
to the existing children of the head that occur between the • Based on the resulting misclassiﬁcations of the struc-
two words under consideration. In this case, the number and tured predictor (i.e. the parser output), the ensemble
types of pre-existing subordinate children are valid features weight for the current weak local predictor is calculated,
that can be used to predict whether the new head-subordinate and the local example weights are updated, according to
link should occur, which turns out to be a very informa-  any standard boosting method; for example, either ex-
tive feature for link prediction in parsing [Collins, 1997; ponential loss Adaboost [Freund and Schapire, 1996;
Wang et al., 2005].                                       Schapire and Singer, 1999] or logistic regression loss
  Although the idea of using dynamic features is not new in boosting [Collins et al., 2002].
                    [                          ]
some ﬁelds like parsing Collins, 1997; Magerman, 1995 ,it •
is still not as widely appreciated as perhaps it should be. Dy- The above steps are then repeated for some number of
namic features are often a trivial way to improve structured boosting rounds.
predictors without requiring any modiﬁcation of the under- The resulting ensemble of weak local predictors then provides
lying training methods. In fact, even still the possibility is a combined local predictor that can be used for subsequent
not always used in parsing [McDonald et al., 2005], only to global structured prediction on composite test examples.
yield immediate improvements when subsequently reintro- The advantage of this approach is its simplicity and gener-
duced [McDonald and Pereira, 2006].Belowweﬁndthat     ality. It can be applied to any standard local training method
simple dynamic features easily improve structured prediction without requiring any modiﬁcation of the underlying algo-
performance.                                          rithm, yet via structured boosting, the local learning algo-
                                                      rithm is forced to respond to the behavior of the global pre-
5  Structured Boosting                                dictor. In effect, it is a simple training wrapper, where local
                                                      examples are reweighted, not based on the predictions of a
Even though dynamic features can signiﬁcantly improve current hypothesis, but instead on the predictions that the lo-
the structured prediction performance of local training algo- cal hypothesis forces the global structured predictor to make.
rithms, local training is still myopic, and the parameter opti- Below we ﬁnd that a structured boosting method of this form
mization process remains uninﬂuenced by the global behav- can improve the quality of dependency parsers learned from
ior of the structured predictor. In fact, even though dynamic treebank data. Note that only a few boosting rounds are ever
features capture some weak aspects of global prediction, we feasible in our application, because each round requires the
still expect proper structured training algorithms to yield sig- entire corpus to be re-parsed and the local prediction model
niﬁcant global accuracy improvements in structured prob- re-trained. Nevertheless, we still witness some useful im-
lems like parsing. For example, in simpler sequence labeling provements and achieve state of the art results.
tasks, it has already been well observed that structured train-
ing algorithms, like conditional random ﬁelds [Lafferty et 6 Experimental Design: Dependency Parsing
al., 2001], outperform their myopic counterparts, maximum
entropy models [McCallum et al., 2000], because MEMM  We applied the above ideas for learning structured predictors
training still fails to consider global prediction accuracy [Laf- to the challenging problem of learning a dependency parser
ferty et al., 2001]. However, by attempting to incorporate from treebank data. In particular, we considered two lan-
the global predictor directly into local parameter optimiza- guages, English and Chinese.
tion, one is inevitably led to design new, complex training Data sets We used the English Penn Treebank 3.0 [Mar-
algorithms that require standard local training methods to be cus et al., 1993] and the Chinese Treebanks 4.0 [Palmer et al.,
replaced. The problem we seek to avoid is to complicate the 2004] and 5.0 [Palmer et al., 2005] for our experiments. For
training process in this manner.                      English, we converted the constituency structures to depen-
                                                                                   [
  We now introduce our main proposal, structured boost- dency trees using the same rules as Yamada and Matsumoto,
                                                      2003] and adopted the standard training/development/test
ing, that provides a straightforward way to combine global
                                                      split used throughout the literature. The development and
structured prediction (parsing) with local parameter opti-
                                                                                                   [
mization, without modifying the underlying local training test sets were tagged with the part of speech tagger of Ratna-
                                                      parkhi, 1996]. For Chinese, we used the rules in [Bikel, 2004]
algorithm. In fact, the procedure is a trivial variant of
                                                      for conversion and created the same data split as [Wang et al.,
standard boosting algorithms [Freund and Schapire, 1996;
                                                          ]
Schapire and Singer, 1999; Collins et al., 2002], altered to 2005 on the Chinese Treebank 4.0 data set, and the same data
                                                      split as [Corston-Oliver et al., 2006] on the Chinese Treebank
incorporate a structured prediction algorithm during the clas-
                                                      5.0 data set. Chinese Treebank 5.0 contains Chinese Tree-
siﬁcation phase. The procedure is as follows.
                                                      bank 4.0 as a subset, but adds approximately 3000 sentences
  • First, a standard predictor is trained on the local labeled (100,000 words) of Taiwanese Chinese text.
    components, as discussed in Sec. 3, to produce a “weak” Static features For both English and Chinese we used a
    local predictor (e.g. a local link predictor for parsing). common set of feature templates. In particular, for the static

                                                IJCAI-07
                                                  1759features, we used the same set of features described in [Mc-    Table 1: Boosting with static features
Donald et al., 2005], except the “In Between POS Features”.
                                                                    English        Chinese (Treebank 4.0)
Given a target word pair and their context, these static fea- Iter
tures consisted of indicators of the individual words, their part DA  RA     CM     DA     RA      CM
of speech tags, and also the part of speech tags of words in the 1 87.77 89.61 27.44 82.20 88.58  19.38
surrounding context. In addition to the indicator features used 2 88.08 89.61 28.97 82.33  89.62  19.72
in [McDonald et al., 2005] we also added a distance feature 3 88.10  89.74  28.81  82.22   89.97  18.69
that simply measures how far apart the two words are in the 4 88.49  90.62  29.04  82.79   89.97  19.38
sentence, which is highly predictive of link existence, since
most links in a dependency parse are short range.
  Dynamic features  For dynamic features, we used the each mis-parsed local example were simply increased by an
number of previous children of a candidate head word, and additive constant, with other weights kept the same, and only
an indicator of the part of speech, if any, of the previous child the last hypothesis is kept. In fact, in our experiments below
word on the same side of the candidate head word. For En- we obtain state of the art results just using this simpliﬁed pro-
glish, we used one special dynamic feature to try to capture cedure, and so we focus on these initial results here. Compar-
prepositional phrase attachment preference: if a candidate isons to standard boosting algorithms, such as Adaboost M1,
child is tagged PP, then we use a feature that indicates the M2 [Freund and Schapire, 1997] and the logistic regression
tag and word of the ﬁrst grandchild (ﬁrst child of the child). form of boosting described in [Collins et al., 2002] are still in
  Local training For the local training algorithm we used progress.
a standard logistic regression model (aka maximum entropy
model) [Hastie et al., 2001] to attempt to learn a predictor for
one of three word pair labels (no link, left link, right link), 7 Results
given the features described above. To regularize the parame- To determine the effectiveness and generality of our approach
ters in the model, we used the linear, non-negative regularizer we conducted a number of experiments on each of the data
described in [Goodman, 2004]. The regularization param- sets (English and Chinese). These results were achieved us-
eter, λ, was set to 0.5 in our experiments. This parameter ing only the simpliﬁed boosting procedure mentioned above
was selected with some tuning on the English development (additive weight updates, keeping only the last hypothesis).
set, and then used without modiﬁcation on the other data sets. First, to determine the effectiveness of the basic structured
Unfortunately, the number of features and number of local boosting idea, we started with a simple local prediction model
examples were both so large that training the logistic regres- (static features only) and measured parsing accuracy on the
sion model, even once, took more than a day. So to accelerate held out test set as a function of the number of boosting
the training process, we employed a further trick: We parti- rounds. Table 1 shows that parsing accuracy is improved in
tioned the set of local examples (determined by word pairs each round of boosting with static features, on both English
in each sentence) according to the part of speech tags of the and Chinese (using Chinese Treebank 4.0). To explain the
pair. Within each equivalence class, the number of features improvements more carefully, note that DA (Dependency Ac-
could then be further reduced by dropping those features that curacy) indicates how many word pairs are correctly linked;
became constant within the class. This partitioning dropped RA (Root Accuracy) measures how many sentence roots are
the overall training cost to a few hours on a few computers, correctly identiﬁed; and CM (Complete Match) is the number
since the separate partitions could then be trained in parallel. of sentences where the entire tree is correct. Our focus in this
Interestingly, the quality of the learned model was not signif- work is on improving the dependency accuracy scores (DA),
icantly affected by this training procedure. This suggests that rather than the root accuracy and complete match scores.
the part of speech tags of the word pair, which are used to This fact is reﬂected in the boosting procedure, since instance
create the partitions, are the most essential piece of informa- reweighting is based only on whether each candidate link is
tion in deciding the existence and orientation of a dependency predicted correctly, not whether the root is labeled correctly,
link.                                                 nor whether the complete sentence is matched correctly.
  Parser There are many dependency parsing algorithms   Not surprisingly, Table 1 shows that the dependency accu-
available with differing computational cost. They range from racy (DA) improves on each round of boosting for English,
Eisner’s O(n3) time parser, where n is the length of the sen- and improves on most rounds (and improves overall) for Chi-
tence, to an O(n5) time CKY parser [McDonald et al., 2005]. nese; while the RA and CM results ﬂuctuate somewhat. Note
The difference between these algorithms has to do with the that although the improvements appear small, the observed
global constraints they can enforce and the types of features DA differences are all statistically signiﬁcant. For English,
they can use during dynamic programming. Faster algorithms the test corpus consists of 564,848 instances (word pairs oc-
enforce fewer global constraints and need to use a more re- curring in a sentence), and differences of 0.02 in the per-
stricted class of features. In our experiments, we used a stan- centages shown in the tables are statistically signiﬁcant with
dard CKY parser [Jurafsky and Martin, 2000]. which allowed greater than 99% conﬁdence. For Chinese, the test corpus
us to use all of the features described above, while also en- consists of 99,922 instances, and differences of 0.05 in the
forcing the planarity constraint.                     percentages shown in the tables are statistically signiﬁcant
  Boosting method We experimented with a few boosting with greater than 99% conﬁdence.
methods, including a simpliﬁed variant where the weights of Second, to determine the effectiveness of dynamic fea-

                                                IJCAI-07
                                                  1760