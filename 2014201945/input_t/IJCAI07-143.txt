     Avoidance of Model Re-Induction in            SVM-based Feature Selection for Text
                                           Categorization

                    Aleksander Kołcz                             Abdur Chowdhury
             Microsoft Research and Live Labs              Illinois Institute of Technology
                    ark@microsoft.com                              abdur@ir.iit.edu


                    Abstract                          features beyond a certain count is very small. It is therefore
                                                      important to be able to estimate at which point the perfor-
    Searching the feature space for a subset yielding mance curve of the classiﬁer measured against the number
    optimum performance tends to be expensive, es-    of most informative features either achieves a maximum or
    pecially in applications where the cardinality of “levels off”. Unfortunately, the search for optimum feature
    the feature space is high (e.g., text categorization). settings can be time-consuming due to repetitive model re-
    This is particularly true for massive datasets and training.
    learning algorithms with worse than linear scaling  In this work we investigate alternatives to SVM model re-
    factors. Linear Support Vector Machines (SVMs)    induction during feature selection. We are able to demon-
    are among the top performers in the text classiﬁ- strate that the proposed techniques not only result in substan-
    cation domain and often work best with very rich  tial gains in terms computational efﬁciency but may actually
    feature representations. Even they however beneﬁt lead to more accurate solutions and typically lead to equiva-
    from reducing the number of features, sometimes   lent results. As for the feature selection criterion we focus on
    to a large extent. In this work we propose alterna- the feature ranking induced by the SVM itself, since it was
    tives to exact re-induction of SVM models during  found that in the text categorization domain it compares fa-
    the search for the optimum feature subset. The ap- vorably [Mladenic et al., 2004] with the mainstream feature
    proximations offer substantial beneﬁts in terms of selection criteria such as Information Gain [Rogati and Yang,
    computational efﬁciency. We are able to demon-    2002].
    strate that no signiﬁcant compromises in terms of
    model quality are made and, moreover, in some
    cases gains in accuracy can be achieved.          2   Model-driven feature selection for SVMs
                                                      A linear SVM creates a classiﬁcation model by attempting
                                                      to separate elements of two different classes by a maximum
1  Introduction                                       margin [Vapnik, 1998]. For problems that are linearly separa-
Linear Support Vector Machines (SVMs) [Vapnik, 1998]  ble this results in identifying the subsets of both positive and
have been found to be among the best performers in tasks negative examples that lie exactly at the margin — these are
involving text categorization [Joachims, 1998][Lewis et al., called support vectors (SVs). It is quite common, however,
2004]. Due to the richness of natural language, text cat- that the problem is not linearly separable, in which case the
egorization problems are characterized by very large num- support vector set is enriched by those training examples that
bers of features. Even with infrequent features removed, the cannot be classiﬁed by the model correctly (the soft-margin
dimensionality of the attribute space tends to be very high SVM [Cortes and Vapnik, 1995] balances the margin with
(e.g., ≈100,000), which for some learners poses computa- the total loss over the training data). In either case, the weight
tional challenges and/or leads to overﬁtting the training data. vector of the SVM solution (derived via convex optimization)
In the case of linear SVMs good performance is in many is given by a linear combination of the SVs, and the output of
cases is achieved with little or no feature selection [Joachims, a the SVM to an input vector x is expressed as
1998][Lewis et al., 2004][Mladenic et al., 2004], although                       ⎛          ⎞
best performance using all features is by no means guaran-                       
                                                                                 ⎝     j   j⎠
teed. It has been shown [Gabrilovich and Markovitch, 2004] f (x)=  wixi + b =         y αjdi  xi + b  (1)
that it is relatively easy to identify text classiﬁcation prob-  i             i    j
lems where best accuracy is achieved with aggressive feature j
selection.                                            where di is the value corresponding to feature i for the train-
  Even if optimum performance is achieved with no selec- ing example j, wi is the weight assigned to the i-th feature by
tion, the dependence between the classiﬁcation accuracy and the SVM and αj is the Lagrange multiplier associated with
                                                       j                         j
the number of features used often exhibits saturation whereby d (the value of αj is 0 unless d is a support vector - other-
the improvement in accuracy due to increasing the number of wise αj > 0; for soft-margin SVMs the Lagrange multipliers

                                                IJCAI-07
                                                   889                        j
must satisfy 0 <αj ≤ C); y ∈{−1, +1} is the class label datasets and rich feature representations, both of which are
              j
associated with di .                                  common in text categorization applications, identifying the
  SVMs have proven to be quite robust in dealing with large optimum can be thus quite expensive.
numbers of features and overﬁtting tends to be less of an is- Published results on SVM-based feature selection indicate
sue compared to other learners applied in the text classiﬁ- that SVM is good in feature ranking in text applications. One
cation domain. When feature selection needs to be applied interesting question however is: “how stable is the original
SVMs have been reported to perform well with established solution subject to subsequent feature selection?". Put dif-
feature selection approaches such as IG, χ2 and BNS [For- ferently, we are interested if the original solution can be sig-
man, 2003].                                           niﬁcantly reoptimized in the restricted domain of using the
  In recent work [Mladenic et al., 2004] investigated the efﬁ- top-N features.
cacy of using the feature ranking imposed by a trained SVM For linear SVMs, the quality of the solution is primarily de-
itself. It was found that ranking according to absolute fea- termined by the orientation of the hyperplane normal. Given
ture weight values outperforms other established alternatives. the direction of this vector, the ﬁnal position of the hyper-
Its use is justiﬁed [Mladenic et al., 2004] by the fact that the plane (controlled by the bias term b in (1)) can be adjusted so
sensitivity of the margin separating the two classes (as deter- as to satisfy a particular utility function (e.g., misclassiﬁca-
mined by a linear SVM) to changes in the j-th feature is di- tion cost, precision or recall).
rectly dependent on the sensitivity of the norm of the weight For any feature subset one can obtain a projection of the
vector to such changes. This can be expressed as [Mladenic original hyperplane normal onto the reduced feature space,
et al., 2004]                                         which is obtained by ignoring or “masking" the coordinates
                                                    corresponding to the removed features. Our contribution is
                             
                     δ                             the address the following questions:
                        w2 ∝ |w |
                       j         i                    •
             i∈trn set δxi                                Is the direction of the projected hyperplane normal sub-
                                                          stantially different from the direction of the normal in-
Therefore, features receiving low absolute weights can be ex- duced over the reduced feature representation?
pected to have little effect on the orientation of the optimum • How does the classiﬁcation performance of the solution
hyperplane.                                               associated with the projected hyperplane normal com-
  [                  ]
   Mladenic et al., 2004 proposed to apply this criterion in pare with the solution induced over the reduced feature
              [              ]
the ﬁlter fashion John et al., 1994 , whereby the ranking is representation?
obtained just once and subsequent attempts to identify the op-
                                                        •
timum subset use the top-N features according to the initial How is the membership of the support vector set affected
ordering. [Guyon et al., 2002] followed an alternative wrap- by reducing the dimensionality of the training data.
           [              ]
per approach John et al., 1994 where, after each subset of 3.1 Normal vector feature masking
the least relevant features is removed, the SVM model trained
with the remaining ones provides the new ranking, which is Let us consider a simple procedure which uses the original
then used to identify the next subset of features to remove. SVM model to derive one that operates within the reduced
                                                                 N
As discussed in [Hardin et al., 2004] an SVM may assign space of top- features. In this approximation the original
low weights to features that are redundant given the presence model is unchanged, except that only those weight vector
                                                                                        N
of other relevant features. The recursive approach of [Guyon components which intersect with the top- feature subset are
et al., 2002] is thus likely to be more successful in compen- retained. A corresponding feature masking transformation is
sating for this effect, especially if it is desired to identify a applied to the documents. The output of the masked model
                                                      can be expressed as:
fairly small set of the most relevant features.                                     ⎛              ⎞
  In the text domain, where the optimum number of features                          
tends to be large, the differences in quality between mod-                          ⎝        j    j ⎠
                                                      fmask (x)=     miwixi+b =           αjy midi   xi+b
els utilizing the recursive and non-recursive approaches are      i               i   j∈SV
rather small [Li and Yang, 2005][Kalousis et al., 2005].In
this work we focus exclusively on the ﬁlter variant of SVM where            
                                                                              1   i ∈M
based feature selection proposed in [Mladenic et al., 2004].          m  =      if
                                                                        i     0 if i/∈M
3  Avoiding model re-induction in SVM-based           and M  is the set of features to be masked.
   feature selection                                    To gain some insight into the properties of masking let us
                                                      start with the original model and assume that the feature to be
Traditionally, investigation of the efﬁcacy of different feature masked has the index of N. Given that the original solution
subsets relies on inducing a model with each particular choice is optimal (for a linearly separable problem), it minimizes the
of the subset and comparing the results achieved. Depending Lagrangian
on the type of the learner, repetitive model re-induction may       1                                

carry a more or less signiﬁcant cost in terms of the compu- L (w, α, b)= w · w −    α yj  w · dj + b − 1
                                                                    2                j
tational time required. In the case of linear SVMs the cost                  j∈trn set
is quadratic in terms of the number of training instances and                    

                                                                  yj w · dj + b − 1 ≥ 0     j
linear in terms of the number of features used. For massive subject to:               for all         (2)

                                                IJCAI-07
                                                   890and thus satisﬁes the ﬁrst-order local minimum conditions: Note that such renormalization would typically be applied by
            δL                                       default when re-inducing the SVM model using top-N fea-
               = w −         α yjdj =0
            δw                j                 (3)   tures.
                     j∈trn set
            δL     
               =         α yj =0                      4   Experimental Setup
            δb            j
                 j∈trn set                            We used linear SVM induced with all features as the baseline
Let us now mask out the N-th feature of vector w and each and as the source of feature ranking. Given the original fea-
               dj                                     ture set, the methodology was to examine the quality of mod-
of training vector , while keeping the Lagrange multipliers          N
unchanged, and consider the same optimization problem but els using only top features. To asses the extent and impor-
in the N − 1 dimensional space. Notice that the derivatives tance of these differences we compared the effectiveness of
of the new Lagrangian in (3) with respect to w and b remain exact model re-induction and the proposed alternatives over
0 and thus the original solution when projected on the lower- the following document collections:
dimensional space meets the necessary optimality conditions TREC-AP1: This dataset represents a collection of AP
(with respect to w and b) there as well. To be a true solution in newswire articles for which the training/test split de-
the space in which feature N is ignored it also needs to maxi- scribed in [Lewis et al., 1996] was used. The collection
mize L with respect to α and meet to constraints (2) however. consisted of 142,791 training and 66,992 test documents
The constraints in (2) were met originally with equality for divided into 20 categories. Each document belonged to
the SV set and with strong inequality by all other training one category only.
vectors. For the sake of an argument let us assume that (2) Reuters-RCV12
will hold for points outside the SV set. For the true SVs the          : This collection represents of the most
                                                          recent and largest corpus used in research involving text
constraints will be violated only for those in which feature                     23, 149
N was actually present (given the sparsity of text this may categorization. There are   training documents
                                                              781, 265                           [
be only a small fraction of the SV set). The amount of the and        test documents As described in Lewis et
                                                                  ]
violation for each such SV will be                        al., 2004 there are several ways of grouping the docu-
                
                     
                   ments and we restricted ourselves to the Topic ontology
 yj w · dj + b − 1 − yj w−N · dj + b − 1 = yjdj w
                                             N  N         using the101 topic categories represented in the train-
                                                (4)       ing portion of the data (out of the 103 categories total).
where it can be seen that a small value of |wN | leads to a small
                          −N                              A document may belong to more than one topic and in
departure from the optimum (w represents the vector with  certain cases there is a hierarchical relationship where a
the N-th feature masked out).                             topic may be considered a subtopic of another.
  Based on the above we can expect that by keeping the
                                                       TechTC-1003: 100 two-class problems (based on the
values of Lagrange multipliers ﬁxed and masking a low                        4)
weight feature we can achieve a solution that lies close to Open Directory Project generated to purposefully rep-
the optimum in the reduced-dimensionality space. The va-  resent different challenges for SVMs as far as feature
                                                                               [
lidity of such an assumption will increase for features that selection is concerned Gabrilovich and Markovitch,
                                                              ]
are infrequent (i.e., inequality constraints for fewer training 2004 .
points will be affected) and for features assigned low ab- 4.1 Document representation
solute weights (i.e., the departure from optimality will likely
be small).                                            Documents were preprocessed by removing markup and
                                                      punctuation, converting all characters to lower case and ex-
3.2  Feature masking and document normalization       tracting feature tokens as sequences of alphanumerics de-
In the text domain one often transforms the document feature limited by whitespace. In the case of the RCV1 and
vectors to unit norm (e.g., according to L2) to compensate for TechTC-100 collections, we used the pre-tokenized feature
the document length variability [Joachims, 1998][Dumais et ﬁles provided on the corpus websites. In-document term fre-
al., 1998][Leopold and Kindermann, 2002]. Such transfor- quency was ignored and each feature received the weight of
mation introduces feature weighting that is uniform for fea- one if it appeared in a document at least once — otherwise its
tures within a document but varied across documents. The weight was zero. This binary representation in our experience
normal-based feature masking preserves the original feature performs as well as TF-IDF when coupled with document-
weighting as the less relevant features are removed, which length normalization. More importantly, in the context of this
counters to some extent the original length normalization. An work, with the binary representation the magnitude of pertur-
alternative that we consider here is to retain the set of SVs and bations (4) to the conditions (2) depended primarily on the
the associated Lagrange multipliers but renormalize the train- SVM weights and not on independently derived factors, e.g.,
ing vectors after each set of features is removed. The output TF-IDF. L2 length normalization was applied since it has
of such a model is thus given by (assuming L2 length normal- been found to be beneﬁcial in text classiﬁcation [Joachims,
ization)
              ⎛                       ⎞                  1http://www.daviddlewis.com/resources/testcollections/trecap
                           dj                          2http://www.daviddlewis.com/resources/testcollections/rcv1/
              ⎝               i       ⎠
  fsv (x)=         αj                 mixi + b         3http://techtc.cs.technion.ac.il/techtc100/techtc100.html
                                 j   j                   4
            i    j        k mk · dk · dk                 http://www.dmoz.org

                                                IJCAI-07
                                                   8911998][Dumais et al., 1998][Leopold and Kindermann, 2002].

Words which occurred just once in the training corpus were      0.925
eliminated.
                                                                0.92
4.2  Experimental procedure
                                                                0.915
The multi-class categorization problems were broken into a
series of one-against-the-rest tasks, where each class was      0.91

treated in turn as the target with the remaining ones playing  avg  F1 0.905
the role of the anti-target. The TechTC-100 dataset natu-
                                      TREC-AP                    0.9
rally consisted of 100 two-class tasks. Unlike and                                         exact
RCV1 however the two-class problems here were fairly well                                  sv set
                                                                0.895                      mask
balanced, with comparable numbers of training and test ex-
                                                                0.89
amples available for the target and the anti-target.              0     20     40    60    80    100
  Classiﬁcation performance was measured in terms of the                   percentage of features used
Area under the Receiver Operating Characteristic (ROC) Figure 1: Average best-F1 as a function of top-N features for
curve (AUC) and the best attainable value of F1, as tuned via TREC-AP.
decision-threshold adjustment. Both metrics were estimated
over the test data and are reported in terms of the macro-
average (for best F1) and micro-average (for AUC) across the      0.90
categories.
                                                                  0.88
  For each two-class task an SVM model was induced using
all features, which were then ranked according to their ab-       0.86
solute weight values. Features not represented in the SV set      0.84
were removed and we then considered using just a fraction of
                                                                 avg  best  AUC
the top ranking features (with respect to the non-zero weight     0.82
ones) with values in {0.01, 0.05, 0.1, 0.2, ..., 0.9, 0.99}.      0.80
  In addition to computing the average performance ﬁgures                all   mask  sv_set exact
for each fraction we also provided the average of the best re-
sults on a per category basis, which acknowledges that the op- Figure 2: Average-best AUC across the 100 two-class prob-
timum number of features according to a given performance lems for TechTC-100. According to the one-sided paired t-
metric may change from one two-class problem to another. test the difference between the exact and masking approaches
  In labeling the results the exact reinduction approach is de- is signiﬁcant (P-value < 10−9), while the difference between
noted as exact, normal-based feature masking is denoted as the exact and SV set approaches is not (P-value= 0.0852).
mask and the weight re-computation using the masked and
renormalized SV set is labeled as sv set.
                                                      is chosen. For lower feature counts the differences become
5  Results                                            more pronounced.
                                                                  TechTC-100
5.1  Accuracy effects                                   Since the              collection consisted of prob-
                                                      lems with balanced numbers of positive and negative ex-
Table 1 one compares the average AUC and best F1 results for amples, in reporting classiﬁcation accuracy we limited our-
the TREC-AP and Reuters-RCV1    collections, where the selves to the average AUC metric. Following [Gabrilovich
best feature selection results were determined on a per cate- and Markovitch, 2004], 4-fold cross-validation was used to
gory basis. It can be seen that the best results according these estimate accuracy and one-sided paired t-test was applied to
two metrics are numerically very close to each other within estimate signiﬁcance of the differences in classiﬁcation per-
each dataset when using the three model feature selection ap- formance.
proaches. According to the P-values the differences between The average best AUC results for using all features and the
the exact and approximate approaches can also be considered three feature selection methods are shown in Figure 2. Note
statistically insigniﬁcant (we used the macro t-test outlined in that all approaches to optimize the feature set are substantially
[Yang and Liu, 1999]), except for the best-F1 measure and better than using all features, with the differences being sta-
the normal-based masking method for RCV1.Overthetwo   tistically signiﬁcant (P-values ≈ 0). The P-values show that
collections, best AUC performance was achieved with all or the SV set approach was statistically equivalent to the exact
almost all features, but the best F1 performance was more one, although normal-based masking underperformed in this
varied as illustrated in Figure 1 for the case of TREC-AP, case. Given that TechTC-100 was speciﬁcally designed to
where it is apparent that reducing the number of features can illustrate the beneﬁts of feature selection for SVM it is not
have a beneﬁcial effect. This exempliﬁes the fact that opti- surprising to see the big gains in AUC shown in Figure 2.
mality of feature selection is dependent on the performance
criterion. Note that when a high fraction of top ranking fea- Correlation effects
tures is retained (> 50%) there is essentially no difference Aside from measuring the impact of feature selection on the
whether an exact or an approximate feature selection method classiﬁcation performance it is interesting to investigate the

                                                IJCAI-07
                                                   892Table 1: Average best AUC and F1 results for the exact and approximate methods for feature selection over the TREC-AP and
Reuters-RCV1   collections. P-values for one-sided pairwise t-test (macro) are also given for determining at which point the
differences between the two approximate variants and the exact approach can be considered signiﬁcant.
                             best AUC                               best F1
                   exact     mask/P-val     sv set/P-val  exact     mask/P-val      sv set/P-val
        TREC-AP    0.989499  0.989537/0.21  0.989525/0.23 0.920365  0.92107/0.36    0.919142/0.22
        RCV1       0.974551  0.97452/0.117  0.97451/0.126 0.595599  0.594883/0.003  0.595328/0.086


           1                                                     1

         0.98                                                   0.9

                                                                0.8
         0.96
                                                                0.7
         0.94
                                                                0.6
         0.92
                                                                0.5
          0.9
                                                                0.4
                                                               support set overlap/containmentsupport  set  


        weight  vector  correlation/cosine  sim Pearson coeff                       sv overlap
         0.88                                                   0.3
                             cosine similarity                                      sv containment

         0.86                                                   0.2
           0     20    40    60    80    100                     0     20    40     60    80    100
                    percentage of features used                           percentage of features used

           1                                                     1

                                                                0.9
         0.95
                                                                0.8

                                                                0.7
          0.9

                                                                0.6

         0.85
                                                                0.5

                                                                0.4
          0.8
                            Pearson coeff                      overlap/containmentsupport  set  


        weight  vector  correlation/cosine  sim                                     sv overlap
                            cosine similarity                   0.3                 sv containment

         0.75                                                   0.2
           0     20    40    60    80    100                     0     20    40     60    80    100
                    percentage of features used                           percentage of features used

Figure 3: Pearson correlation coefﬁcient and cosine similarity Figure 4: Overlap between the original set of support vec-
between the masked original weight vector and the weight tors and the set obtained when training with reduced feature
vector obtained after SVM retraining using the top-N features representation for TREC-AP (top) and RCV1 (bottom). The
for TREC-AP (top) and RCV1 (bottom).                  fraction (containment) of SVs corresponding to the original
                                                      SV set is also shown.
similarity between the weight vectors assigned by the origi-
nal SVM and the SVM obtained using a reduced feature rep- SVs tends to decrease when fewer features are used, they also
resentation.                                          show the fraction of SVs corresponding to the original SV set
  Figure 3 shows the dependence of the weight-vector simi- that are part of the SV set obtained using the reduced feature
larity according to the Pearson correlation coefﬁcient and the representation. As can be seen, SV set overlap is quite high
cosine similarity on the number of top-ranking features used (exceeds 80%) as long as a sizable fraction of the original
for TREC-AP.andRCV1. For both datasets the weight vec- features is used (at least 50%). For smaller values of N,the
tors are very close to each other, even when the fraction of overlap goes down, but this is mainly due to the decrease in
top-N features used is small. The direction of the hyperplane the number of SVs in such cases, since the fraction of orig-
normal vector is thus only very weakly dependent on the less inal SVs used remains consistently very high. The overlap
relevant features and by projecting the original weight vector and containment of SV sets do not approach 1 as the frac-
onto the sub-space of the top ranking features one obtains a tion of features used approaches 100%. This is because the
direction that is oriented very close the optimum.    fraction is deﬁned with respect to the set of features that re-
  Figure 4 shows set the average overlap between the origi- ceived non-zero weights in the original SVM run (i.e., after
nal set of SVs and the one obtained when using top-N fea- discarding the zero-weight ones). It is thus apparent that the
tures for TREC-AP and RCV1. Since the overall number of use of features that were deemed irrelevant did in fact have an

                                                IJCAI-07
                                                   893