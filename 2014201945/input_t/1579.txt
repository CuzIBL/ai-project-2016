                            2D Shape Classiﬁcation and Retrieval

                                Graham McNeill, Sethu Vijayakumar
                              Institute of Perception, Action and Behavior
               School of Informatics, University of Edinburgh, Edinburgh, UK. EH9 3JZ
                   Email: G.J.McNeill-2@sms.ed.ac.uk, sethu.vijayakumar@ed.ac.uk


                    Abstract                          shape matching is required. These transformations represent
                                                      the chosen deﬁnition of shape, i.e. they are the operations that
    We present a novel correspondence-based tech-     leave shape unchanged.
    nique for efﬁcient shape classiﬁcation and retrieval. In this paper, we introduce a simple, generic technique for
    Shape boundaries are described by a set of (ad hoc) shape classiﬁcation and retrieval. Shapes are represented by
    equally spaced points – avoiding the need to ex-  a potentially large number of points from their boundaries.
    tract “landmark points”. By formulating the corre- The points lie at ﬁxed intervals in terms of distance along
    spondence problem in terms of a simple generative the boundary or radial angle. This gives an accurate and ro-
    model, we are able to efﬁciently compute matches  bust description of shape that avoids the somewhat arbitrary
    that incorporate scale, translation, rotation and re- decision of what constitutes a good point. The high compu-
    ﬂection invariance. A hierarchical scheme with    tational cost associated with using many points is reduced in
    likelihood cut-off provides additional speed-up. In three ways. Firstly, a hierarchical approach avoids the need to
    contrast to many shape descriptors, the concept of consider all possible label assignments. Secondly, potential
    a mean (prototype) shape follows naturally in this correspondences are considered simultaneously and a clear
    setting. This enables model based classiﬁcation,  winner often emerges early in the computation. Finally, for
    greatly reducing the cost of the testing phase. Equal classiﬁcation problems, the algorithm produces class means
    spacing of points can be deﬁned in terms of ei-   with associated variances at each point. This allows model
    ther perimeter distance or radial angle. It is shown based classiﬁcation, requiring fewer shape comparisons at the
    that combining the two leads to improved classiﬁ- testing stage. By considering both the distance and angle
    cation/retrieval performance.                     based descriptors, we can accurately describe a wider range
                                                      of shapes and increase the classiﬁcation/retrieval accuracy.
1  Introduction                                       The effectiveness of the method is demonstrated on a bench-
                                                      mark data set and compared to other state-of-the-art methods
In many object recognition problems, the examples are most in the ﬁeld.
easily disambiguated on the basis of shape - as opposed to
features such as color or texture. Often the classiﬁcation of
objects extracted from an image database are most intuitively 2 Method
formulated as a shape classiﬁcation task. Here we present a We wish to develop generic techniques for classifying or re-
fast, general algorithm for classiﬁcation and retrieval of 2D trieving shapes. There are two important issues to consider:
objects.                                              Firstly, the ultimate goal is to work with large data sets and
  Much of the recent work in this area uses a ﬁnite set of hence, computational expense is an important factor. Sec-
points taken from the object’s boundary as the shape repre- ondly, the shapes involved may be very diverse, with large
sentation. Points can be selected on the basis of maximal differences between some classes. For example, one class
curvature [Super, 2004], distance from the centroid [Zhang might consist of very jagged shapes and the other smoothly
et al., 2003] or any criteria deemed suitable to the class of curved shapes. This implies that overly speciﬁc models of
shapes involved. More sophisticated approaches parameter- shape variation should be avoided.
ize the boundary as a closed curve and slide points along the
outline to minimize an objective function (e.g. [Wang et al., 2.1 Shape Correspondence
2004]). These methods produce good results but generally Given two shapes, we wish to optimally align them in order
use expensive optimization algorithms. An alternative to ﬁnd- to observe the genuine differences in shape. When shapes
ing the ‘correct points’ is to simply place points at roughly are represented by a set of points, it is necessary to ﬁnd the
equal intervals along the boundary. Belongie et al. [Belongie best match over all possible label assignments (Fig.1). In this
et al., 2002] used this approach effectively in their work on paper, we use a deﬁnition of shape from statistical shape anal-
shape contexts.                                       ysis (e.g. [Dryden and Mardia, 1998]): The shape of a set of
  Having chosen how to represent the objects, one must solve labelled points is all the information that is invariant to ro-
the correspondence problem in order to compare shapes. This tation, scaling and translation. In addition to this, we also
is illustrated in Fig.1. Also, a valid set of transformations for consider reﬂection to be a valid transformation. AllowingFigure 1: Two identical shapes (left). Large symbols indicate Figure 2: Two shapes from the same class, each described
corresponding points under a random assignment of labels. by 100 points (left) - linear interpolation is used to aid visu-
To align the shapes, we must ﬁnd the correct labels as well as alization. The correspondence algorithm correctly cycles the
the correct transformations (right).                  labels and the shapes can be aligned (right).

                                                                                                      m
                                                      [Dryden and Mardia, 1998 ] we have, for a ﬁxed cycling ,
only basic transformations reﬂects our lack of prior knowl-                k  |z |2    k   |w |2
edge about the shape classes involved. We now formulate and unit sized shapes ( j=1 j = j=1  j  =1)
the correspondence problem in a probabilistic fashion. This        min d2(X, Y)=1−|w∗z|2
will enable correspondences to be chosen at minimal compu-         Γ,s,t
tational cost.                                        where w∗ denotes the conjugate transpose of w. Absorbing
                                                      the size normalization into the distance expression gives the
Correspondence Algorithm
                                                      expression for the full Procrustes distance:
                                      k
Consider two shapes, each represented by points from                                  ∗ 2
                         X     x  ...x  T     Y                                    |w  z|
their respective boundaries: =(1      k) and    =                  dF (z, w)=1−          .           (3)
         T    k×2                          k                                      w∗wz∗z
(y1 ...yk) ∈ R   . Both shapes are centered: j=1 xj =
                                                     This is a standard metric in shape analysis. Substituting
  k  y                       y
  j=1  j = 0. We assume that the j are generated indepen- 2                                         ˆ
                                                      dF (z, w) into eq.(2) allows us to evaluate L(m, s,ˆ ˆt, Γ) for
dently from                                           all m ∈{0, ..., k − 1} and hence, ﬁnd mˆ . Fig.1 demonstrates
             y ∼    s x        t,σ2I                  the correspondence algorithm applied to two identical shapes
              j  N(  Γ j+m|k +      2)          (1)   - alignment is achieved using the full Procrustes ﬁt (Section
for j =1,...,k, where s is a scaling parameter, t a trans- 2.2). As expected, the shapes can be perfectly matched. In
lation vector and Γ a 2D rotation matrix - reﬂection is dealt Fig.2, the shapes are different examples from the same class.
with later. m ∈{0,...,k− 1} cycles the labels of the points A good match is possible after ﬁnding the best correspon-
in X. Finding the maximum likelihood estimates (MLEs) of dence.
the parameters is a restricted version of the matching prob- Invariance with respect to reﬂection is not yet incorporated
lem for two unlabelled sets investigated in [Kent et al., 2004]. into the distance computation. It is worth mentioning that
For the general case, the large number of potential correspon- Procrustes distances can directly incorporate reﬂection invari-
                                                           [                ]
dences (k!) can make ﬁnding the MLEs prohibitively expen- ance Mardia et al., 1979 . Unfortunately, this idea is not
sive. However, here we are dealing with boundaries and need compatible with our formulation of the correspondence prob-
                                                      lem. A simple example demonstrates this: Imagine z is the re-
only consider cycling of the labels. This means that there are w
only k potential label assignments and the computational cost ﬂection of . When faced with the task of corresponding the
                    ˆ, s, ˆt m                        two shapes, we are obviously unaware of the reﬂection and
of computing the MLEs Γ ˆ and ˆ is more acceptable.   proceed to label both shapes in an anticlockwise direction. w
  The likelihood of the parameters can be written as  will only be the same as its reﬂection (in the Procrustes sense)
                      1         −1                    if the labels are reﬂected along with the points. Doing this
        m, s, t,              {    d2 Y, X }          would ﬂip the direction of the labels to clockwise. z cannot
      L(      Γ) =   πσ2  k exp  σ2  (     )    (2)
                   (2    )      2                     be matched to the original w (due to reﬂection) nor to the re-
                                                      ﬂected w (because the labels run in opposite directions). The
where                                                                        ∗
                                                      reﬂection of z is given by z ≡ (¯z1,...,z¯k). This operation
                  k                                  changes the direction of the labels to clockwise. Reversing
        2                                 2                                                 T
       d (Y, X)=     yj − (sΓxj+m|k + t) .          the order of the entries: zR ≡ (¯zk,...,z¯1) , switches the
                  j=1                                 direction back to anticlockwise. The optimal correspondence
                                                      between zR and w can now be found as normal. This appar-
For a ﬁxed m ∈{0, ..., k − 1}, s,ˆ ˆt and Γˆ take values that ent doubling in computational expense can be signiﬁcantly
minimize d2(Y, X) - a consequence of assuming isotropic reduced by tracking the correspondence calculations for the
variance and independence between points. We now exploit non-reﬂected shapes.
                                      2
a closed form solution for the minimum of d (Y, X) which Reducing the Computational Cost
can be written compactly using complex notation. Letting
              T                                       Any classiﬁcation or retrieval problem will require a large
xj ≡  (xj1,xj2)  →  xj1 + ixj2 ≡ zj ∈  C, the shapes  number of correspondences to be found. The cost of com-
become complex vectors: X → z, Y → w ∈ Ck. Following  puting correspondences is dominated by k - the number of       Reduce dimensions using                                               Reduce number of columns
       a coarse shape representation               zw*                       using likelihood cut-off


Figure 3: Example of how the speed-up techniques can affect the dimensions of the outer product matrix - see text for details.

boundary points used. Firstly, the number of cycles to con- fewer iterations when ‘new’ points are selected at random
sider increase linearly with k. Secondly, the distance com- (e.g. w137¯ z, w¯49z,...) so that the full boundary is explored
putation used to evaluate a correspondence is essentially a quickly.1 Clearly if the threshold is never reached and all
dot product of vectors in Ck. From an accuracy perspective, the points are needed, this incremental method will be more
it may be necessary to have a large k. Fortunately, for any costly than the basic algorithm. The additional cost comes
given k, there are options for reducing both the number of from updating the k different z∗z associated with each la-
cycles that are considered and the cost of each distance com- belling of z and from evaluating the likelihood at each stage.2
putation.                                             It can be partly avoided by evaluating zw∗ in blocks, rather
             2
  To evaluate dF (z, w) it is necessary to compute the dot than single columns. Often very few points are required to
products w∗w, z∗z and w∗z. The values of w∗w and z∗z  ﬁnd the best correspondence (Section 3.3). It is worth not-
do not change with the cycle value and need only be com- ing that our algorithm could assign correspondences on the
puted once. The cost of corresponding z and w is therefore basis of minimum Procrustes distance (dF ) - so why bother
dominated by calculating the dot product between w∗ and all with likelihoods? Fig.4 shows the evolution of the likeli-
cycled versions of z. This is equivalent to summing along hoods compared to the squared full Procrustes distances for a
the diagonals of the outer product matrix zw∗ (for example, typical problem. The likelihood approach magniﬁes the best
summing the leading diagonal gives the value of w∗z when correspondences and kills-off the worst, leading to a quicker
                                                      choice of cycle parameter m. Recall from eq.(1) that there is
the cycle value is zero - see Fig.3). The following procedures         2               2
reduce the dimensions of zw∗ and hence, the computational isotropic variance, σ , at each point. σ is a free parameter
cost of the algorithm.                                that affects how quickly the likelihoods peak to a single cy-
  The number of points used, k, is assumed to be large (say cle value. A small variance encourages fast correspondence
k = 100). To avoid considering all k possible values of the selection (Fig.4 (right)) but assumes little within class vari-
cycling parameter m, we initially consider a coarse represen- ability. Note that this thresholding technique is applicable
tation of the shapes using k <kpoints (say k =20and whenever the correspondence algorithm is used, i.e. it is in-
the points are found by selecting every ﬁfth point of the 100). dependent of the above method for reducing the number of
The likelihoods for all 20 values of the cycling parameter m cycle values considered.
are then evaluated as normal. It seems reasonable to assume Shape Descriptors
that the most likely m will correspond the shapes in a simi-
                                                      The correspondence algorithm is independent of how the k
lar way to that which would be achieved using all 100 points.
                                                      boundary points are selected in the ﬁrst place. To keep our
Since the 20 points come from the original 100, it is easy
                                                      approach as generic as possible, very simple point selection
to relate these approximate correspondences back to the true
                                                      techniques are used. We either choose points at roughly equal
shapes (those described by 100 points). The ﬁnal correspon-
                                                      intervals along the boundary or points at equal increments
dences are found in the standard way, but only cycle values
                                                      of the radial angle (the centroid radii model [Chang et al.,
close to the approximated value are investigated. The outer
                  20×20           100×100             1991]). We refer to the former method as the perimeter shape
product matrix is in C rather than C     for the ﬁrst
                                                      descriptor and the latter as the radial descriptor. In some
round of correspondences (Fig.3). At the ﬁne tuning stage,
                                                      cases, a line from the origin can intercept the boundary at
only a small number of the potential 100 correspondences are
                                                      more than one point. To make the radial descriptor consis-
evaluated. Thus, the overall cost is greatly reduced.
                                                      tent, all boundary points close to these intercepts are consid-
  Regardless of the number of points used to describe a
                                 ∗                    ered and the point most distant from the origin is selected.
shape, it seems that the full matrix zw must be evaluated This can lead to parts of the shape being ignored. However,
and used to compute the Procrustes distances. However, the results in Section 3 demonstrate the advantage of using
by computing the columns of zw∗ incrementally, the like-
lihoods over all k possible cycles can be monitored and a 1Discontinuing the likelihood computation for any m whose like-
correspondence chosen when one of them exceeds a given lihood drops below a given threshold would further increase the
threshold (c.f. Fig.3). This approach captures the intu- speed of the algorithm.
itive idea that some correspondences will be much easier to 2A random subset of a centered shape will be approximately cen-
ﬁnd than others. The likelihood threshold is reached after tered so there is no need to update the center incrementally.Figure 4: Squared full Procrustes distances (left) and normalized likelihoods (middle) between two shapes from the mouse
class. The different lines indicate the distance/likelihood after using a given number of points - or equivalently, computing a
given number of columns of zw∗. Decreasing σ2 exaggerates the peaked proﬁle of the likelihoods (right).

this sub-optimal descriptor.
                                                      Table 1: Algorithm for corresponding all shapes from a given
2.2  Shape Model                                      class to a running class mean.
The correspondence technique discussed above can be incor-
porated into a nearest neighbor classiﬁer: given a test shape, Initialize: µ = ﬁnal shape in class
ﬁnd its optimal correspondence with every training shape and for i = 1:cycles for j =1:ﬁnal shape in class
classify on the basis of minimum Procrustes distance. This   1. get corresponding labels of shape j and µ
requires ﬁnding a large number of correspondences, a prob-   2. ﬁt shape j to µ
lem that can be avoided using prototypes. Ideally, the full  3. recompute µ using arithmetic mean at each point
Procrustes mean of each class would form the set of proto- end
types. The deﬁnition of this mean follows naturally from the                      µ
                                 k                       end Fit all shapes to the ﬁnal
deﬁnition of dF . It is deﬁned as µ¯ ∈ C such that ||µ¯|| =1
and
                         n
                             2
              µ¯ = arg min  dF (zl, µ)          (4)
                      µ
                         l=1
where z1,...,zn are all the shapes in the class. Bookstein
[Bookstein, 1996] has proposed an iterative method for ﬁnd-
ing µ¯. By incorporating the correspondence algorithm into
this method, we can correspond shapes directly onto a run-
ning class mean.3 This procedure relies on the full Procrustes
ﬁt for aligning two shapes. Fitting z to w transforms z so that
the sum of squared distances between corresponding points is
equal to dF (z, w). In Fig.1, dF (z, w) is zero and a perfect ﬁt
is achieved. In Fig.2, dF (z, w) is small because the shapes
are similar and a reasonable ﬁt is possible. The ﬁt of z onto Figure 5: Left: class mean (bold) and the twenty class ex-
w is given by                                         amples ﬁtted to the mean using 100 points (interpolated for
                        ∗     ∗
                zfit = z wz/(z z).              (5)   visualization); Right: class mean and scatter of correspond-
                                                      ing points at four positions.
The algorithm for ﬁnding the correspondences and the mean
within a given class is shown in Table 1. Shapes are succes-
sively corresponded and ﬁtted to a running mean.      isotropic covariance is replaced by the sample covariance ma-
  Given a mean, the sample covariance matrices can be cal- trix4 at each point. Thinking of shapes as sets of points in R2,
culated at each point. The ﬁnal class model consists of a mean   X    x  ,...,x T                   c X
shape - k independent 2D points, each with an associated co- a test shape =( 1 k) is assigned to a class ( )
variance matrix. Fig.5 shows the algorithm applied to twenty using maximum likelihood (ML)
objects from the same class.                                                    k
                                                              c(X) = arg  max  (   f(xj; µcj, Σcj))   (6)
                                                                        c=1,...,C
2.3  Shape classiﬁcation                                                        j=1
The most likely correspondence and ﬁt between a test shape                                      T     k×2
                                                      where C is the number of classes, (µc1,...µck) ∈ R
and each class mean is found as described in Section 2.1. For                    2×2
                                                      is the mean of class c, Σcj ∈ R is the sample covariance
classiﬁcation, the model for ﬁnding correspondences (eq.(1)) µ  f · µ  ,
is modiﬁed. Points are still assumed independent, but the at cj and ( ; cj Σcj) is the pdf of the bivariate normal
                                                      distribution N(µcj, Σcj).
  3µ¯ can be calculated analytically when the correct labels are
known(see e.g. [Dryden and Mardia, 1998]).               4Computed during the training phase.                                                      Table 2: Classiﬁcation performance(%) for the thirty class
                                                      data set (k = number of points, perimeter descriptor used).
                                                         k            12      24     48     72     96
                                                         N. neighbor 95.67  97.83  97.83   97.50  98.00
                                                         ML          93.17  96.00  96.83   96.67  87.17

                                                      Table 3: Classiﬁcation performance(%) for the seventy class
                                                      data set with k =48.
                                                                        Perimeter  Radial  Combined
                                                            N. neighbor   95.71    91.00     96.29
                                                            ML            90.36    83.79     92.64
 Figure 6: Example boundaries from the MPEG-7 data set.
                                                      reason for the slightly poorer results on the full data set is
3  Evaluation and Results                             that there are now classes for which the perimeter descriptor
                                                      is deﬁnitely not suitable. Despite the generally inferior per-
The proposed technique is tested on the benchmark MPEG- formance of the radial descriptor (Table 3, middle column),
7 shape database [Latecki et al., 2000]. This consists of 70 it performs signiﬁcantly better for some classes. This sug-
classes with 20 observations in each class. Pixels lying on a gests that using both descriptors may improve classiﬁcation
closed boundary are extracted in a clockwise direction using accuracy. Here, we demonstrate how a naive combination
Matlab’s image processing toolbox. Some of these bound- of the descriptors can enhance performance. The ML clas-
aries are shown in Fig.6 (the same data was used for Figs. 1, siﬁer is modiﬁed as follows: Each descriptor (perimeter and
2 and 5). The ﬁrst principle component of these (2D) points radial) gives a vector of class-conditional likelihoods for the
is aligned with the x-axis. This helps reduce the problem of test shape. The two vectors are normalized so that they each
‘sub-interval shifts’ producing an artiﬁcially high Procrustes sum to one. For a given class, the maximum of the two can-
distance between similar shapes – particulary for small k.
                      k                               didate likelihoods is used for classiﬁcation. This reﬂects an
The shape is described by points using either the perimeter assumption that the descriptor with the most conﬁdent predic-
or radial descriptor as described in Section 2.1. Initially we tion is correct. It is less clear how to normalize the distances
present results without using any speed-up techniques. The for the nearest neighbor classiﬁer. We assume that the major-
impact of these are explored in Section 3.3.          ity of shapes bear no resemblance to the test shape and hence,
                                                      give rise to meaningless correspondences and distances. Nor-
3.1  Leave-one-out Shape Classiﬁcation                malization is therefore carried over the minimum q entries in
A thirty class subset of the MPEG-7 database was recently each distance vector (q  total number of shapes). The dis-
used by [Kunttu et al., 2003] in their work on multiscale tance used for classiﬁcation is the minimum of the two can-
Fourier descriptors. Their approach outperformed the stan- didate distances. Here, and in Section 3.2, we ﬁx q = 200.
dard contour Fourier descriptor in various classiﬁcation tasks. The results in Table 3 show a small rise in classiﬁcation ac-
Using leave-one-out classiﬁcation with a nearest neighbor curacy for the combined case over the perimeter only case.
classiﬁer, they achieved 94.2-96.3% accuracy (depending on Evidently, the increase in performance is limited by the al-
the length of shape descriptor). We use the perimeter descrip- ready high performance of the perimeter descriptor and the
tor with the same data set and testing procedure (i.e. leave- simplicity of the normalization technique.
one-out and nearest neighbor) to enable a direct comparison
with their results. Test shapes are ﬁtted to training shapes us- 3.2 Bullseye Test for Shape Retrieval
ing the correspondence method described in Section 2.1. The
nearest neighbor is the training shape closest to the test shape This is a frequently used test in shape retrieval and enables us
in terms of full Procrustes distance. Results are shown in the to compare our algorithm against many of the best perform-
ﬁrst row of Table 2. Classiﬁcation accuracy is slightly better ing shape retrieval techniques. A single shape is presented
than that of [Kunttu et al., 2003] and the results are consistent as a query and the top forty matches are retrieved (from the
over a wide range of k. The same tests were carried out using entire data set - the test shape is not removed). The task is
the ML method described in Section 2.3. Results are given repeated for each shape and the number of correct matches
                                                      (out of a maximum possible 20) are noted. A perfect per-
in the bottom row of Table 2. This prototype based technique                ×
is almost as accurate as the nearest neighbor classiﬁer and re- formance results in 1400 20 = 28000 matches. Results
quires that many fewer correspondences are found - 30 versus are given as a percentage of this perfect score. The ﬁrst two
599 per test shape.                                   rows of Table 4 show the result of the proposed technique us-
  The ﬁrst column of Table 3 shows the corresponding re- ing the perimeter and radial descriptors when no speed-ups
sults for the full seventy class data set with k =48.5 One were used. The bullseye score was also computed using the
                                                      combined approach as described for nearest neighbor clas-
  5Classiﬁcation using distance to the nearest prototype is less ac- siﬁcation in the previous section. Results are shown in the
curate than the ML classiﬁer. This indicates that the (point-wise) bottom row of Table 4. [Super, 2004] notes that the best per-
scatter about the mean is sufﬁciently ‘tight’ so as to produce covari- forming algorithms in this test have scored between 75.44%
ance matrices that accurately reﬂect the within-class variability. and 78.17%. A naive combination of two simple descriptors