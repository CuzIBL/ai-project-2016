               Average-Reward Decentralized Markov Decision Processes

                       Marek Petrik                              Shlomo Zilberstein
             Department of Computer Science               Department of Computer Science
                University of Massachusetts                  University of Massachusetts
                   Amherst, MA 01003                            Amherst, MA 01003
                   petrik@cs.umass.edu                         shlomo@cs.umass.edu

                    Abstract                          problem, which has not been previously studied in a decen-
                                                      tralized settings.
    Formal analysis of decentralized decision making
                                                        The need to optimize average reward has been demon-
    has become a thriving research area in recent years,
                                                      strated in many applications, including ones in reinforce-
    producing a number of multi-agent extensions of
                                                      ment learning [Mahadevan, 1996], decentralized robot con-
    Markov decision processes. While much of the
                                                      trol [Tangamchit et al., 2002], decentralized monitoring,
    work has focused on optimizing discounted cumu-
                                                      sensor networks, and computer networks [Rosberg, 1983;
    lative reward, optimizing average reward is some-
                                                      Hajek, 1984]. In these problems, the system operates over
    times a more suitable criterion. We formalize a
                                                      an extended period of time and the main objective is to per-
    class of such problems and analyze its character-
                                                      form consistently well over the long run. The more common
    istics, showing that it is NP complete and that opti-
                                                      discounted reward criterion usually leads to poor long-term
    mal policies are deterministic. Our analysis lays the
                                                      performance in such domains.
    foundation for designing two optimal algorithms.
    Experimental results with a standard problem from   One motivation for using discounted reward models–even
    the literature illustrate the applicability of these so- when the average reward criterion seems more natural–is that
    lution techniques.                                the problem is easier to analyze and solve [Puterman, 2005].
                                                      But it is not clear if the same argument prevails in decentral-
                                                      ized settings. In fact, in some problems the average reward
1  Introduction                                       analysis may be simpler because the system may exhibit com-
Decentralized decision making under uncertainty is a grow- plex behavior initially, but quickly settle into simple one. One
ing area of artiﬁcial intelligence addressing the interaction such example is the automaton used to prove that an optimal
of multiple decision makers with different goals, capabili- policy for a POMDP may not be regular [Madani, 2000]. The
ties, or information sets. Markov decision processes (MDPs) optimal discounted policy in this simple example is inﬁnite,
have been proved useful for centralized decision making in while the optimal average reward policy is very simple and
stochastic environments, leading to the development of many contains just a single action.
effective planning and learning algorithms. More recently, In the case of POMDPs, the analysis of the average re-
extensions of MDPs to decentralized settings have been de- ward case is known to be much more involved than the dis-
veloped. Examples include stochastic games (SGs) or com- counted reward criterion. A thorough account of these is-
petitive Markov decision processes [Filar and Vrieze, 1996], sues and solutions may be found in [Arapostathis et al., 1993;
and decentralized partially observable Markov decision pro- Puterman, 2005]. Since a DEC-POMDP is a generalization
cesses (DEC-POMDPs) [Bernstein et al., 2000]. SGs concen- of a POMDP, one would expect to face the same difﬁcul-
trate mainly on observable domains, capturing the competi- ties. However, we demonstrate a way to circumvent these
tiveness among the players. DEC-POMDPs generalize par- difﬁculties by focusing on a subclass of the problem–the
tially observable MDPs to multi-agent settings, modeling co- case of observation and transition independent decentralized
operative players who may have different partial knowledge MDPs [Becker et al., 2004]. The results we obtain for this
of the overall situation.                             class of problems are encouraging and they lay the foundation
  This paper addresses the problem of average-reward de- for studying more complex models and competitive settings.
centralized MDPs. We analyze a class of DEC-POMDP prob- The main contribution of this paper is the formulation and
lems, where the dependency and interaction among the agents analysis of the DEC2-MDP problem with the average reward
is deﬁned by a common reward function. The agents are oth- criterion. Section 2 deﬁnes the problem and provides a mo-
erwise independent and they each have full observation of tivating example. In Section 3, we show that calculating the
their local, mutually-independent states. A similar model has gain of a ﬁxed policy is easy, but ﬁnding an optimal policy
been previously studied by [Becker et al., 2004] with the ob- is NP complete. In Section 4, we propose two practical algo-
jective of optimizing cumulative reward over a ﬁnite-horizon. rithms to solve the problem. In addition, these algorithms
In contrast, we analyze the inﬁnite-horizon average reward show a connection with centralized average reward MDPs

                                                IJCAI-07
                                                  1997and thus help to analyze the properties of the optimal policies. previous states. If the action selection depends only on the
Both algorithms are based on mathematical programming for- last state of both agents, the policy is stationary,orMarkov.
mulations. Finally, in Section 5 we demonstrate the algo- We extend the standard terminology for the structure of av-
rithms on a standard problem, in which the average-reward erage reward MDPs to the decentralized case. A DEC2-MDP
objective is the most natural one. Some of the results we is said to be recurrent or unichain if the local decision pro-
provide are easily extensible to the competitive case. In the cesses of all the agents are recurrent or unichain respectively.
interest of clarity, we only mention these extensions in this Otherwise, it is said to be multichain [Puterman, 2005].
paper.                                                  Given a ﬁxed policy, the history of the process can be repre-
                                                                                                  ∞
                                                      sented by a sequence of random variables {(Xt,Yt)}t=0. The
2  Average Reward DEC-MDPs                            random variables Xt and Yt represent the state-action pairs of
                                                      the two agents at stage t. The corresponding sequence of re-
This section provides a formal deﬁnition of the problem and                       ∞
                                                      wards is deﬁned as {R(Xt,Yt)}  . We seek to maximize
shows that under certain conditions, the average reward may                       t=0
                                                      the expected average reward, or gain, deﬁned as follows.
be expressed compactly, leading to a compact formulation of
the problem. The model we propose is similar to the one Deﬁnition 2.2. The gain of a Markov policy is
proposed in [Becker et al., 2004]. We introduce a different                                      
                                                                           1          N−1
deﬁnition, however, because the original is not easily extensi-
                                                           g(s1,s2) = lim   Es  ,s ,u     R(Xt,Yt)  .
                                                                     N→∞       1 2 0
ble to inﬁnite-horizon problems. To simplify the discussion,              N           t=0
the problem is deﬁned for two agents. However, the deﬁni-
tion as well as some of the results can be easily extended to The expectation subscript denotes the initial value of the ran-
an arbitrary number of decision makers.               dom variables, thus X0 =(s1,π(s1)) and Y0 =(s2,π(s2)).
                                                      Furthermore, the gain matrix G is deﬁned as G(s1,s2)=
Deﬁnition 2.1. A DEC2-MDP is an n-tuple (S, A, P, R) such
                                                      g(s1,s2). The actual gain depends on the agents’ initial dis-
that:                                                                                          T
                                                      tributions α1,α2 and may be calculated as g = α1 Gα2.
• A = A1 × A2 represents the actions of the players;
• S = S1 × S2 is the set of states;                     One problem  that ﬁts the average reward criterion is
• P  =(P1,P2)  is the transition probability. Each Pi is a the Multiple Access Broadcast Channel (MABC) [Rosberg,
function: Si × Si × Ai → R. The transition probabilities 1983; Ooi and Wornell, 1996]. In this problem, which has
             
(s1,s2) → (s1,s2) given a joint action (a1,a2) is:    been used widely in recent studies of decentralized control,
                                      
P (s1,s2) (s1,s2), (a1,a2)=P1(s1 s1,a1)P2(s2 s2,a2)   two communication devices share a single channel, and they
• R(s1,a1,s2,a2) ∈ R is the joint reward function; and need to periodically transmit some data. However, the chan-
• (s1,s2) is a given initial state for both agents.   nel can transmit only a single message at the same time. Thus,
                                                      when several agents send messages at the same time inter-
  The process, as we deﬁned it, is both transition and obser-
                                                      val, this leads to a collision, and the transmission fails. The
vation independent as deﬁned in [Becker et al., 2004]. Tran-
                                                      memory of the devices is limited, thus they need to send the
sition independence means that the transition to the next state
                                                      messages sooner rather than later. We adapt the model from
of one agent is independent of the states of all other agents.
                                                      [Rosberg, 1983], which is particularly suitable because it as-
Observation independence means that the observations that
                                                      sumes no sharing of local information among the devices.
an agent receives are independent of the current states and
                                                        The MABC problem may be represented as a DEC2-MDP
observations of other agents.
                                                      where each device is represented by an agent. The state space
  While transition independence is a plausible assumption
                                                      of each agent represents the number of messages in the buffer.
in many domains, observation independence is somewhat re-
                                                      There are two possible actions, send and do not send. The
strictive. It prevents any kind of communication among the
                                                      arriving data is modeled by random transitions among the
agents. This assumption is suitable when communication
                                                      buffer states. We provide more details on the formulation and
is prohibitively expensive, risky, or when it does not add
                                                      the optimal policy in Section 5.
much value to the process. When communication is use-
ful, the problem with no observations provides as an easily-
computable lower bound that could help to decide when to 3 Basic Properties
communicate.                                          In this section, we derive some basic properties of DEC2-
  We refer to (S1,A1,P1) and (S2,A2,P2) as the local pro- MDPs, showing how to efﬁciently evaluate ﬁxed policies and
cesses. In fact, they represent MDPs with no reward. To sim- establishing the complexity of ﬁnding optimal policies. The
plify the analysis, we assume that the state set S and action analysis is restricted to stationary policies. An extension to
set A are ﬁnite, and that the Markov chains for both agents, non-stationary policies is mentioned in Section 6.
induced by any policy, are aperiodic. A Markov chain is ape- For average reward MDPs, the gain of a policy can be cal-
riodic when it converges to its stationary distribution in the culated directly by solving a set of linear equations, which re-
limit [Puterman, 2005]. We discuss how to relax these as- semble value function equations in the discounted case [Put-
sumptions in Section 6.                               erman, 2005]. These value optimality functions cannot be
  A policy π is a function that maps the history of the lo- used in DEC2-MDPs, but as the following theorem states, the
cal states into an action for each agent. To prevent the need gain can be expressed using the invariant or limiting distribu-
for introducing measure-theoretic concepts, we assume that tion. The limiting distribution p represents the average proba-
the policy may only depend on a ﬁnitely-bounded number of bility of being in each state over the whole inﬁnite execution.

                                                IJCAI-07
                                                  1998It is deﬁned as:
                                                                    l        l      l
                       1 N−1                                        11      12      13
          p(si) = lim        P [Xt = si] .                 s                                     f
                 N→∞
                       N  t=0
                                                                    l        l      l
The limiting distribution can be calculated from a the initial       21      22      23
distribution using a limiting matrix. Limiting matrix of tran-
sition matrix P is deﬁned as
                            N−1                              v1     v2      v3     v4      v5
                 ∗        1       t
               P  =lim          P  .
                    N→∞
                          N  t=0
Then, we have that p = αP ∗, where α is the initial distribu-
tion.
Theorem 3.1. Suppose 1 and 2 are state-action transition
                    P     P                           Figure 1: A sketch of the DEC2-MDP created from a 3-SAT
matrices of the two agents for ﬁxed policies, and R is the problem with 2 clauses and 5 variables.
reward matrix. Then, the gain matrix can be calculated as
follows:
                    =   ∗ (  T )∗
                  G   P1 R P2   .                     values to variable instances to satisfy the problem, and the
  The theorem follows immediately from the fact that both second one can assign values to variables. The agents are pe-
processes are aperiodic and from the deﬁnition of gain. nalized for assigning a different value to the actual variable
              ∗
  Calculating P is not practical, because it typically re- and to the variable instance.
quires calculating all eigenvectors of the matrix. However, Let S =(V,C) be a 3-SAT problem in conjunctive nor-
the same idea used for calculating the gain in the single agent mal form, where V is the set of variables and C is the set of
case can be used here as follows.                     clauses. If the literal l is an instantiation of variable v, then
Theorem 3.2. For any initial distribution α and transition we write l ∼ v. We construct a corresponding DEC2-MDP
matrix P , the limiting distribution p fulﬁlls:       in the following way. The state and action sets for the agents
                        T                             are:
                  (I − P  )p  =0              (3.1)
                +(   −  T )   =
               p   I   P   q     α.           (3.2)           S1  =   {s, f}∪{lij i ∈ C, j =1, 2, 3}
Moreover, if p fulﬁlls (3.1) and (3.2) then:                  S2  =   V
                          T ∗                                     =   {   }
                   p =(P   ) α.               (3.3)           A1       t, f
                                                              A2  =   {t, f}
  Note that I denotes the identity matrix of the appropriate
                               n
size and that (3.2) also implies that p(i)=1. Due
                                 i=1                  Each state lij ∈ S1 represents the j-th literal in the i-th
to space constraints, we do not include the proof, which is a clause, which is an instantiation of a single variable. States
similar derivation to the one in [Puterman, 2005].
                                                      S2 represent the variables of the problem. The actions rep-
  Next, we examine the complexity of ﬁnding an optimal resent whether the variable value is true or false. The main
policy in this model. The complexity of ﬁnite-horizon DEC- idea is to make the ﬁrst agent assign arbitrary values to vari-
MDPs has been shown to be NEXP complete, while inﬁnite- able instances to satisfy the formula. The second agent then
                              [                  ]
horizon problems are undecidable Bernstein et al., 2000 . determines whether the value assigned to multiple variable
While the model we study here is not tractable, the follow- instances is the same.
ing theorem shows that it is easier than the general problem.
However, we focus only on stationary policies, while the truly For lack of space, we omit the complete deﬁnition of transi-
optimal policy may depend on a longer history. The complex- tion matrices. A sketch of the reduction is shown in Figure 1.
ity further justiﬁes the use of the algorithms we present later, The ﬁrst agent follows the transitions depicted when the ac-
since a polynomial algorithm is unlikely to exist.    tion is chosen such that the literal is not true. Otherwise, the
                                                      agent transitions to state s. For both agents, for each state,
Theorem  3.3. The problem of ﬁnding an optimal Markov there is a small probability (< 1) of remaining the in same
policy for the aperiodic DEC2-MDP is NP-complete.     state. For the second agent, the transition probabilities are
Proof. To show that ﬁnding an optimal policy is in NP, we independent of the actions taken. Notice that this process is
note that Theorem 4.2 states the optimal Markov policies are aperiodic.
deterministic. Since calculating the gain, given ﬁxed policies The rewards are deﬁned such that a negative reward is re-
is possible in polynomial time, and the policy size is polyno- ceived when the value assigned by the ﬁrst agent to an in-
mial, all policies can be checked in nondeterministic polyno- stance of a variable is different from the value assigned to the
mial time.                                            variable by the second agent. In addition, a negative reward is
  The NP hardness can be shown by reduction from satisﬁa- received when the ﬁrst agent transitions to state f. Notice that
bility (SAT). The main idea is that the ﬁrst agent may assign this happens only when the assigned variable instance values

                                                IJCAI-07
                                                  1999do not satisfy at least one clause.                     The solution of (4.1) may also be used to determine an opti-
    ((    ) (   )) = −1   ∀  ∈       ∈      ∈         mal policy, given an optimal value of the program’s variables.
   R  f,a1 , s, a2         a1  A1,a2   A2,s   S2      The procedure is the same as determining an optimal policy
  R((lij,a1), (s, a2)) = −1 li,j ∼ v and a1 = a2.    from a solution to the dual of an MDP. That is, the optimal
                                                      policy is randomized such that for each recurrent state:
In all other cases the reward is zero.                                               
                                                                                i(   )
                                          2                      ∗            p  s, a
  It is easy to show that the gain of the above DEC -MDP is  P [πi (s)=a ]=             ,i=1,  2.
0 if and only if the SAT problem is satisﬁable.                                a∈A pi(s, a)
                                                      This is well-deﬁned because the recurrent states are those for
4  Optimal DEC2-MDP Algorithms                        which   a∈A pi(s, a) > 0.Fortransient states the policy can
                                                      be determined as
In this section we present two methods for ﬁnding optimal                            
                                                                 ∗            qi(s, a )
policies for DEC2-MDPs. Both methods are based on for-       P [  ( )=   ]=                =1  2
                                                                πi s   a            (   ),i    , .
mulating the problem as a mathematical program. We also                        a∈A qi s, a
discuss additional approaches to solving the problem. Proposition 4.1. The policy constructed according to the
  In previous work, a similar problem was solved using an above description is optimal.
iterative method that ﬁxes the policy of one agent, while The proof is only a small variation of the proof of opti-
computing the optimal policy of the other agent [Nair et al., mal policy construction for average reward MDPs. See, for
2003]. Such methods are unlikely to produce good results in example, [Puterman, 2005].
the average-reward case, because of the high number of local It has also been shown that for any average-reward MDP
minima. In contrast, the following MILP formulation facili- there exists an optimal deterministic policy. Notice, that when
tates an efﬁcient search through the set of all local minima. p1 is ﬁxed, the formulation for p2 is equivalent to that of the
                                                      dual of average-reward linear program. Thus, for any optimal
4.1  Quadratic Programming Solution                    ∗     ∗
                                                      p1 and p2, we can construct a deterministic policy as follows.
                                                             ∗
In this section, we give a mathematical program that deﬁnes Fixing p1, we can ﬁnd p˜2 with the same gain, by solving the
the optimal solutions. The reward deﬁnition is based on The- problem as an MDP. Following the same procedure, we can
orem 3.1. We also discuss how to obtain the optimal policy obtain also a deterministic p˜1. Hence the following theorem.
from the solution of the quadratic program.           Theorem 4.2. There is always a deterministic Markov policy
                T                                     for DEC2-MDP with optimal gain.
 maximize  F = p1 Rp2
 subject to p1 ≥ 0                                    4.2  Mixed Integer Linear Program
                                                      This formulation offers a method that can be used to calcu-
           p2 ≥ 0
                                                   late the optimal policies quite efﬁciently. The approach gen-
 ∀j ∈ S1      p1(j, a) −        p1(s, a)P1 (j s, a)=0 eralizes the approach of [Sandholm et al., 2005], which is
          a∈A1        s∈S1 a∈A1                     based on game-theoretic principles and works only for ma-
 ∀j ∈ S1      p1(j, a)+     q1(j, a)−                 trix games. Our approach is based on Lagrange multipliers
                                                      analysis of the problem and thus may be extended to include
          a∈A1       a∈A1
                                                      any additional linear constraints.
                    q1(s, a)P1 (j s, a)=α1(j)
                                                        For clarity, we derive the algorithm only for the unichain
           s∈S1 a∈A1
                                                   DEC2-MDP. The algorithm for the multi-chain case is sim-
 ∀ ∈            (   ) −           (   )  (     )=0
  j  S2       p2 j, a           p2 s, a P2 j s, a     ilar and is derived in the same way. To streamline the pre-
          a∈A2        s∈S2 a∈A2                     sentation, we reformulate the problem in terms of matrices as
 ∀j ∈ S2      p2(j, a)+     q2(j, a)−                 follows:
          a∈A          a∈A                                                      T
           2             2                                    maximize  F  = p1 Rp2
                    q2(s, a)P2 (j s, a)=α2(j)
                                              (4.1)             subject to T1p1 =0     T2p2 =0
           s∈S a∈A
              2   2                                                        T            T            (4.2)
                                                                          e1 p1 =1     e2 p2 =1
  The beneﬁt of this formulation is not so much the compu-
                                                                          p1 ≥ 0       p2 ≥ 0
tational efﬁciency of the solution technique, but that it helps
identify some key properties of optimal policies. Later, we where e1 and e2 are vectors of ones of length n1 and n2
use these properties to develop a more efﬁcient formulation, respectively, and T1 and T2 are matrix representations of the
which is also shown to be optimal.                    3rd and the 5th constraint in (4.1).
                                                        The Mixed Integer Linear Program (MILP) formulation is
Theorem 4.1. An optimal solution of (4.1) has the optimal based on application of the Karush-Kuhn-Tucker (KKT) the-
gain.                                                 orem [Bertsekas, 2003]. Thus, the Lagrangian for the formu-
  We omit the proof due to lack of space. The correctness lation (4.2) is
of the constraints follows from the dual formulation of opti- L(p1,p2,λ,ρ,μ)=
                                   [             ]
mal average reward, shown for example in Puterman, 2005 ,   =    T    +    ( T  − 1) +   ( T  − 1) +
and from Theorem 3.2. The choice of the objective function      p1 Rp2  λ1 e1 p1       λ2 e2 p2
                                                                 T        T        T      T
follows from Theorem 3.1.                                   +   ρ1 T1p1 + ρ2 T2p2 + μ1 p1 + μ2 p2.

                                                IJCAI-07
                                                  2000  Let A1 and A2 denote the sets of active constraints for in-    s1   s2  s3   s4  s5   s6  s7  s8
equalities p1 ≥ 0 and p2 ≥ 0 respectively. Following from    a1  N    N   N    S   S    S   S    S
the KKT theorem, every local extremum must fulﬁll the fol-   a2  N    N   S    S   S
lowing constraints:
                                                      Figure 2: Sample policy for agents a1 and a2. In each state,
     = −     −   T  −       T   = −      −  T   −
 Rp2     λ1e1  T1 ρ1   μ1  R  p1    λ2e2   T2 ρ2  μ2  action S represents sending and N, not sending. The states
p1(i)=0   ∀i ∈ A1          p2(i)=0    ∀i ∈ A2         are s1 – s8.
μ1(i)=0   ∀i/∈ A1          μ2(i)=0    ∀i/∈ A2
                                                      practical application introduced in Section 2. The experimen-
μ1(i) ≥ 0 ∀i ∈ A1          μ2(i) ≥ 0  ∀i ∈ A2
                                                      tal component of the paper is designed to illustrate the scope
                                                      of problems that can be solved quickly using our new algo-
The gain in a local extremum simpliﬁes to:            rithm.
                                                        As mentioned earlier, the problem we use is an adaptation
      T    = −    T   −  T  T   −  T   = −
     p1 Rp2    λ1p1 e1  p1 T1 ρ1  p1 μ1    λ1,        of the Multiple Access Broadcast Channel problem solved in
        T  T                                          [            ]
because P1 T1 =0and for all i,wehavep1(i)=0∨μi =0.     Rosberg, 1983 . In order to make the problem more interest-
Though the above constraints are linear, the problem cannot ing, we assume that the following events may occur: a single
be directly solved since A1 and A2 are not known. They or two messages arrive, a single message in the buffer expires,
can however be represented by binary variables b1(i),b2(i) ∈ or nothing happens. These events occur independently with
                                                                           (        )
{0, 1}, which determine for each constraint whether it is ac- some ﬁxed probabilities: α, β, γ, δ .
tive or not. Therefore, we can get the following MILP:  The communication devices may have buffers of ﬁxed but
           −                                          arbitrary length, unlike the original problem. In addition, the
 maximize    λ1                                       possible actions are send and not send. For the action not
 subject to T1p1 =0                   T2p2 =0         send, the probabilities are updated according to the list stated
            T   =1                     T   =1         above. For the action send, the buffer level is decreased by
           e1 p1                      e2 p2           1, and then the transition probabilities are the same as for the
                           T
           Rp2 = −λ1e1 −  T1 ρ1 − μ1                  action not send. Though [Rosberg, 1983] derives a simple
             T   = −     −  T   −                     analytical solution of the problem, it is not applicable to our
           R  p1    λ2e2   T2 ρ2  μ2                  extended problem.
           μ1 ≥ 0                     μ2 ≥ 0            The reward associated with overﬂowing either of the
           μ1 ≤ m1b1                  μ2 ≤ m2b2       buffers of the agents are r1 and r2 respectively. If both agents
              ≥ 0                        ≥ 0          decide to broadcast the message at the same time, a collision
           p1                         p2              occurs and the corresponding reward is r. The above rewards
           p1 ≤ e1 − b1               p2 ≤ e2 − b2    are negative to model the cost of losing messages. Otherwise,
                                                      the reward is zero.
           b1(i) ∈{0, 1}              b2(i) ∈{0, 1}
                                              (4.3)     We ran the experiments with the following message arrival
                                                      probabilities (0.1, 0.3, 0.5, 0.1). An optimal policy for the
                                                                            = −1     = −1  2  =  −4 5
  In this program, m1 and m2 are sufﬁciently large constants. problem with rewards r1 ,r2 . ,r     .  and
It can be show that they may be bounded by the maximal buffer sizes of 8 and 5 is depicted in Figure 2. While the op-
bias [Puterman, 2005] of any two policies. However, we timal policy has a simple structure in this case, the algorithm
are not aware of a simple method to calculate these bounds. does not rely on that. Note that choosing different thresholds
In practice, it is sufﬁcient to choose a number that is close for sending messages leads to signiﬁcantly lower gains.
the largest number representable by the computer. This only To asses the computational complexity of the approach and
eliminates solutions that are not representable using the lim- its practicality, we ran experiments using a basic commer-
ited precision of the computer.                       cially available MILP solver running on a PC. The solver
  The optimal policy can be in this case obtained in the same was a standard branch-and-bound algorithm, using linear pro-
way as in Subsection 4.1. Notice that λ1 = λ2, so unlike a gramming bounds. The linear programs to calculate the
competitive situation, one of them may be eliminated. The bound were solved by the simplex algorithm. For all our runs,
preceding analysis leads to the following proposition. we ﬁxed the transition probabilities and experimented with
                                                      two sets of rewards. The ﬁrst was r1 = −1,r2 = −1.2,r =
Proposition 4.2. The gain and optimal policy obtained from −4 5               = −1    = −2   =  −1 5
(4.2), and (4.3) are equivalent.                         . and the second was r1   ,r2     ,r     . .
                                                        The results are summarized in Figure 3. They show that
  Though we present this formulation only for cooperative
                                                      the time required to calculate an optimal solution is below 10
cases, it may be easily extended to the competitive variant of
                                                      seconds for problems of this size. It is clear from the results
the problem.
                                                      that the different reward sets have signiﬁcant impact on how
                                                      quickly the problem is solved. This is quite common among
5  Application                                        branch-and-bound algorithms. For larger problems, it may be
This section presents an application of our average-reward beneﬁcial to use one of the branch-and-cut algorithms, which
decentralized MDP framework. It is a simple instance of the often offer good performance even on large problems.

                                                IJCAI-07
                                                  2001