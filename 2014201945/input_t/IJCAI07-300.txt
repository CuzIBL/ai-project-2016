        Topological Value Iteration Algorithm for Markov Decision Processes

                                    Peng Dai   and  Judy Goldsmith
                                        Computer Science Dept.
                                         University of Kentucky
                                          773 Anderson Tower
                                      Lexington, KY 40506-0046

                    Abstract                          Recently several types of algorithms have been proposed
                                                      to efﬁciently solve MDPs. The ﬁrst type uses reachabil-
    Value Iteration is an inefﬁcient algorithm for    ity information and heuristic functions to omit some unnec-
    Markov decision processes (MDPs) because it puts  essary backups, such as RTDP [Barto, Bradke, & Singh,
    the majority of its effort into backing up the en- 1995],LAO*[Hansen & Zilberstein, 2001],LRTDP[Bonet
    tire state space, which turns out to be unnecessary & Geffner, 2003b] and HDP [Bonet & Geffner, 2003a].The
    in many cases. In order to overcome this prob-    second uses some approximation methods to simplify the
    lem, many approaches have been proposed. Among    problems, such as [Guestrin et al., 2003; Poupart et al., 2002;
    them, LAO*, LRTDP and HDP are state-of-the-       Patrascu et al., 2002]. The third aggregates groups of states of
    art ones. All of these use reachability analysis  an MDP by features, represents them as factored MDPs and
    and heuristics to avoid some unnecessary backups. solves the factored MDPs. Often the factored MDPs are ex-
    However, none of these approaches fully exploit the ponentially simpler, but the strategies to solve them are tricky.
    graphical features of the MDPs or use these fea-  SPUDD  [Hoey et al., 1999],sLAO*[Feng & Hansen, 2002],
    tures to yield the best backup sequence of the state sRTDP [Feng, Hansen, & Zilberstein, 2003] are examples.
    space. We introduce an algorithm named Topolog-   One can use prioritization to decrease the number of inefﬁ-
    ical Value Iteration (TVI) that can circumvent the cient backups. Focused dynamic programming [Ferguson &
    problem of unnecessary backups by detecting the   Stentz, 2004] and prioritized policy iteration [McMahan &
    structure of MDPs and backing up states based on  Gordon, 2005] are two recent examples.
    topological sequences. We prove that the backup     We propose an improvement of the value iteration algo-
    sequence TVI applies is optimal. Our experimen-   rithm named Topological Value Iteration. It combines the
    tal results show that TVI outperforms VI, LAO*,   ﬁrst and last technique. This algorithm makes use of graph-
    LRTDP and HDP on our benchmark MDPs.              ical features of MDPs. It does backups in the best order and
                                                      only when necessary. In addition to its soundness and op-
1  Introduction                                       timality, our algorithm is ﬂexible, because it is independent
State-space search is a very common problem in AI planning of any assumptions on the start state and can ﬁnd the opti-
and is similar to graph search. Given a set of states, a set of mal value functions for the entire state space. It can easily
actions, a start state and a set of goal states, the problem is be tuned to perform reachability analysis to avoid backups
to ﬁnd a policy (a mapping from states to actions) that starts of irrelevant states. Topological value iteration is itself not a
from the start state and ﬁnally arrives at some goal state. De- heuristic algorithm, but it can efﬁciently make use of extant
cision theoretic planning [Boutilier, Dean, & Hanks, 1999] is heuristic functions to initialize value functions.
an attractive extension of the classical AI planning paradigm,
because it allows one to model problems in which actions 2 Background
have uncertain and cyclic effects. Uncertainty is embodied In this section, we go over the basics of Markov decision pro-
in that one event can leads to different outcomes, and the oc- cesses and some of the extant solvers.
currence of these outcomes are unpredictable, although they
are guided by some form of predeﬁned statistics. The sys- 2.1 MDPs and Dynamic Programming Solvers
tems are cyclic because an event might leave the state of the An MDP is a four-tuple (S, A, T, R). S is the set of states
system unchanged or return to a visited state.        that describe how a system is at a given time. We consider
  Markov decision process (MDP) is a model for represent- the system developing over a sequence of discrete time slots,
ing decision theoretic planning problems. Value iteration or stages. In each time slot, only one event is allowed to take
and policy iteration [Howard, 1960] are two fundamental dy- effect. At any stage t, each state s has an associated set of
                                                                        t
namic programming algorithms for solving MDPs. How-   applicable actions As. The effect of applying any action is
ever, these two algorithms are sometimes inefﬁcient. They to make the system change from the current state to the next
spend too much time backing up states, often redundantly. state at stage t+1. The transition function for each action, Ta:

                                                 IJCAI07
                                                  1860S × S → [0, 1], speciﬁes the probability of changing to state 2.2 Other solvers
s            a       s R : S → R
  after applying in state .       is the instant reward           [                          ]
                       CC                             Barto et al. Barto, Bradke, & Singh, 1995 proposed an
(in our formulation, we use , the instant cost, instead of online MDP solver named real time dynamic programming.
R). A value function V , V : S → R, gives the maximum
                                                 s    This algorithm assumes that initially the algorithm knows
value of the total expected reward from being in a state . nothing about the system except the information on the start
The horizon of a MDP is the total number of stages the sys- state and the goal states. It simulates the evolution of the
tem evolves. In problems where the horizon is a ﬁnite number system by a number of trials. Each trial starts from the start
H                              V (s)=   H  C(si)
  , our aim is to minimize the value    i=0      in   state and ends at a goal state. In each step of the trial, one
H steps. For inﬁnite-horizon problems, the reward is accu- greedy action is selected based on the current knowledge and
mulated over an inﬁnitely long path. To deﬁne the values of the state is changed stochastically. During the trial, all the
an inﬁnite-horizon problem, we introduce a discount factor visited states are backed up once. The algorithm succeeds
γ ∈ [0, 1] for each accumulated reward. In this case, our goal when a certain number of trials are ﬁnished.
            V (s)=   ∞  γiC(si)
is to minimize       i=0       .                        LAO*  [Hansen & Zilberstein, 2001] is another solver that
  Given an MDP, we deﬁne a policy π : S → A to be a   uses heuristic functions. Its basic idea is to expand an ex-
mapping from states to actions. An optimal policy tells how plicit graph G iteratively based on some type of best-ﬁrst
we choose actions at different states in order to maximize the strategy. Heuristic functions are used to guide which state is
expected reward. Bellman [Bellman, 1957] showed that the expanded next. Every time a new state is expanded, all its an-
expected value of a policy π can be computed using the set cestor states are backed up iteratively, using value iteration.
                V π                        V π(s)
of value functions . For ﬁnite-horizon MDPs, 0   is   LAO* is a heuristic algorithm which uses the mean ﬁrst pas-
deﬁned to be C(s), and we deﬁne V π according to V π:
                             t+1            t        sage heuristic. LAO* converges faster than RTDP since it
        π                            π              expands states instead of actions.
      Vt+1(s)=C(s)+       {Tπ(s)(s |s)Vt (s )}. (1)
                      s∈S                              The advantage of RTDP is that it can ﬁnd a good sub-
                                                      optimal policy pretty fast, but the convergence for RTDP is
For inﬁnite-horizon MDPs, the (optimal) value function is de- slow. Bonet and Geffner extended RTDP to labeled RTDP
ﬁned as:                                                      [                    ]
                                                    (LRTDP)  Bonet & Geffner, 2003b , and the convergence of
V (s)=mina∈A(s)[C(s)+γ       Ta(s |s)V (s )],γ ∈ [0, 1]. LRTDP is much faster. In their approach, they mark a state
                         s∈S                         s as solved if the Bellman residuals of s and all the states
                                                (2)   that are reachable through the optimal policy from s are small
The above two equations are named Bellman equations.  enough. Once a state is solved, we regard its value function as
Based on Bellman equations, we can use dynamic program- converged, so it is treated as a “tip state” in the graph. LRTDP
ming techniques to compute the exact value of value func- converges when the start state is solved.
tions. An optimal policy is easily extracted by choosing an HDP is another state-of-the-art algorithm, by Bonet and
action for each state that contributes its value function. Geffner [Bonet & Geffner, 2003a]. HDP not only uses a
  Value iteration is a dynamic programming algorithm that similar labeling technique to LRTDP, but also discovers the
solves MDPs. Its basic idea is to iteratively update the value connected components in the solution graph of a MDP. HDP
functions of every state until they converge. In each iteration, labels a component as solved when all the states in that com-
the value function is updated according to Equation 2. We ponent have been labeled. HDP expands and updates states
call one such update a Bellman backup.TheBellman resid- in a depth-ﬁrst fashion rooted at the start states. All the
ual of a state s is deﬁned to be the difference between the states belonging to the solved components are regarded as tip
value functions of s in two consecutive iterations. The Bell- states. Their experiments show that HDP dominated LAO*
man error is deﬁned to be the maximum Bellman residual of and LRTDP on most of the racetrack MDP benchmarks when
the state space. When this Bellman error is less than some the heuristic function hmin [Bonet & Geffner, 2003b] is used.
threshold value, we conclude that the value functions have The above algorithms all make use of start state informa-
converged sufﬁciently. Policy iteration [Howard, 1960] is an- tion by constraining the number of backups. The states that
other approach to solve inﬁnite-horizon MDPs, consisting of are unreachable from the start state are never backed up. They
two interleaved steps: policy evaluation and policy improve- also make use of heuristic functions to guide the search to the
ment. The algorithm stops when in some policy improvement promising branches.
phase, no changes are made. Both algorithms suffer from ef-
ﬁciency problems. Although each iteration of each algorithm
is bound polynomially in the number of states, the number of 3 A limitation of current solvers
iterations is not [Puterman, 1994].                   None the algorithms listed above make use of inherent fea-
  The main drawback of the two algorithm is that, in each it- tures of MDPs. They do not study the sequence of state back-
eration, the value functions of every state are updated, which ups according to an MDP’s graphical structure, which is the
is highly unnecessary. Firstly, some states are backed up be- intrinsic property of an MDP and potentially decides the com-
fore their successor states, and often this type of backup is plexity of solving it [Littman, Dean, & Kaelbling, 1995].For
fruitless. We will show an example in Section 3. Secondly, example, Figure 1 shows a simpliﬁed version of an MDP. For
different states converge with different rates. When only a simplicity, we omit explicit action nodes, transition probabil-
few states are not converged, we may only need to back up a ities, and reward functions. The goal state is marked in the
subset of the state space in the next iteration.      ﬁgure. A directed edge between two states means the second

                                                 IJCAI07
                                                  1861state is a potential successor state when applying some action G by removing all the action nodes, and changing paths like
in the ﬁrst state.                                    s → a →  s into directed edges from s to s, we get a causal
                                                      relation graph Gcr of the original MDP M. A path from state
                                                      s1 to s2 in Gcr means s1 is causally dependent on s2.Sothe
                                                      problem of ﬁnding mutually causally related groups of states
  s1        s2        s3        s4        Goal        is reduced to the problem of ﬁnding the strongly connected
                                                      components in Gcr.
                                                        We use Kosaraju’s [Cormen et al., 2001] algorithm of de-
                                                      tecting the topological order of strongly connected compo-
             Figure 1: A simpliﬁed MDP                nents in a directed graph. Note that Bonet and Geffner [Bonet
                                                      & Geffner, 2003a] used Tarjan’s algorithm in detection of
  Observing the MDP in Figure 1, we know the best se- strongly connected components in a directed graph in their
quence to back up states is s4,s3,s2,s1, and if we apply solver, but they do not use the topological order of these
this sequence, all the states except s1 and s2 only require components to systematically back up each component of an
one backup. However, not enough efforts of the algorithms MDP. Kosaraju’s algorithm is simple to implement and its
mentioned above have been put to detect this optimal backup time complexity is only linear in the number of states, so
sequence. At the moment when they start on this MDP, all when the state space is large, the overhead in ordering the
of them look at solving it as a common graph search problem state backup sequence is acceptable. Our experimental re-
with 5 vertices and apply essentially the same strategies as sults also demonstrate that the overhead is well compensated
solving an MDP whose graphical structure is equivalent to a by the computational gain.
5-clique, although this MDP is much simpler to solve than a The pseudocode of TVI is shown in Figure 2. We ﬁrst
5-clique MDP. So the basic strategies of those solvers do not use Kosaraju’s algorithm to ﬁnd the set of strongly connected
have an “intelligent” subroutine to distinguish various MDPs components C in graph Gcr, and their topological order. Note
and to use different strategies to solve them. With this intu- that each c ∈ C maps to a set of states in M.Wethenuse
ition, we want to design an algorithm that is able to discover value iteration to solve each c. Since there are no cycles in
the intrinsic complexity of various MDPs by studying their those components, we only need to solve them once. Notice
graphical structure and to use different backup strategies for that, when the entire state space is causally related, TVI is
MDPs with different graphical properties.             equivalent to VI.
4  Topological Value Iteration                        Theorem 1  Topological Value Iteration is guaranteed to
                                                      converge to the optimal value function.
Our ﬁrst observation is that states and their value functions
are causally related. If in an MDP M, one state s is a succes- Proof We ﬁrst prove TVI is guaranteed to terminate in ﬁnite
sor state of s after applying action a,thenV (s) is dependent time. Since each MDP contains a ﬁnite number of states, it
on V (s). For this reason, we want to back up s ahead of s. contains a ﬁnite number of connected components. In solving
The causal relation is transitive. However, MDPs are cyclic each of these components, TVI uses value iteration. Because
and causal relations are very common among states. How value iteration is guaranteed to converge in ﬁnite time, TVI,
do we ﬁnd an optimal backup sequence for states? Our idea which is actually a ﬁnite number of value iterations, termi-
is the following: We group states that are mutually causally nates in ﬁnite time. We then prove TVI is guaranteed to con-
related together and make them a metastate, and let these verge to the optimal value function. According to the update
metastates form a new MDP M .ThenM    is no longer  sequence of TVI, at any point of the algorithm, the value func-
cyclic. In this case, we can back up states in M  in their re- tions of the states (of one component) that are being backed
verse topological order. In other words, we can back up these up only depend on the value functions of the components that
big states in only one virtual iteration. How do we back up have been backed up, but not on those of the components that
the big states that are originally sets of states? We can apply have not been backed up. For this reason, TVI lets the value
any strategy, such as value iteration, policy iteration, linear functions of the state space converge sequentially. When a
programming, and so on. How do we ﬁnd those mutually  component is converged, the value functions of the states can
causally related states?                              be safely used as tip states, since they can never be inﬂuenced
  To answer the above question, let us look at the graphical by components backed up later.
                               M
structure of an MDP ﬁrst. An MDP  can be regarded as  A straightforward corollary to the above theorem is:
a directed graph G(V,E).ThesetV has state nodes, where
each node represents a state in the system, and action nodes, Corollary 2 Topological Value Iteration only updates the
where each action in the MDP is mapped to a vertex in G. value functions of a component when it is necessary. And
The edges, E,inG represent transitions, so they indicate the the update sequence of the update is optimal.
causal relations in M. If there is an edge e from state node
s to node a, this means a is a candidate action for state s. 4.1 Optimization
Conversely, an edge e pointing from a to s means, applying In our implementation, we added two optimizations to our al-
action a, the system has a positive probability of changing to gorithm. One is reachability analysis. TVI does not assume
state s. If we can ﬁnd a path s → a → s in G, we know any initial state information. However, given that informa-
that state s is causally dependent on s. So if we simplify tion, TVI is able to detect the unreachable components and

                                                 IJCAI07
                                                  1862              Topological Value Iteration             5   Experiment
 TVI(MDP   M, δ)
 1.scc(M)                                             We tested the topological value iteration and compared its
 2.fori ← 1 to cpntnum;                               running time against value iteration (VI), LAO*, LRTDP and
 3.  S ← the set of states s where s.id = cpntnum     HDP. All the algorithms are coded in C and properly opti-
 4.  vi(S, δ)                                         mized, and run on the same Intel Pentium 4 1.50GHz proces-
 vi(S: a set of states, δ)                            sor with 1G main memory and a cache size of 256kB. The
 5 . while (true)                                     operating system is Linux version 2.6.15 and the compiler is
 6 . for each state s ∈ S                            gcc version 3.3.4.
                                              
 7.   V (s)=mina∈A(s)[C(s)+γ     s∈S Ta(s |s)V (s )]
 8 . if (Bellman error is less than δ)                5.1  Domains
 9 .  return                                          We use two MDP domains for our experiments. The ﬁrst
 scc(MDP  M) (Kosaraju’s algorithm)                   domain is a model simulating PhD qualifying exams. We
 10. construct Gcr from M by removing action nodes    consider the following scenario from a ﬁctional department:
                           G    G
 11. construct the reverse graph cr of cr             To be qualiﬁed for a PhD in Computer Science, one has to
 12. size ← number of states in Gcr                   pass exams in each CS area. Every two months, the depart-
 13. for s ← 1 to size                                ment offers exams in each area. Each student takes each
 14. s.id ←−1                                         exam as often as he wants until he passes it. Each time,
 15. // postR and postI are two arrays of length size he can take at most two exams. We consider two types of
 16. cnt ← 1,cpntnum←   1                             grading criteria. For the ﬁrst criterion, we only have pass
 17. for s ← 1 to size                                and fail (and of course, untaken) for each exam. Students
 18. if (s.id = −1)
                                                     who have not taken and who have failed certain exam be-
 19.  df s (G ,s)                                     fore have the same chance of passing that exam. The sec-
 20. for s ← 1 to size                                ond criterion is a little trickier. We assign pass, conditional
 21. postR[s] ← postI[s]                              pass, and fail to each exam, and the probabilities of passing
 22. cnt ← 1,cpntnum←   1                             certain exams vary, depending on the student’s past grade on
 23. for s ← 1 to size                                that exam. A state in this domain is a value assignment of the
 24. s.id ←−1                                         grades of all the exams. For example, if there are ﬁve exams,
 25. for s ← 1 to size                                fail,pass,pass,condpass,untaken is one state. We refer to the
       (s.id = −1)
 26. if                                               ﬁrst criterion MDPs as QEs(e) and second as QEt(e),where
 27.  df s (Gcr,postR[s])                             e refers to the number of exams.
      cpntnum  ←  cpntnum +1
 28.                                                    For the second domain, we use artiﬁcially-generated “lay-
          (cpntnum, G  )
 29. return          cr                               ered” MDPs. For each MDP, we deﬁne the number of states,
           G
 dfs(Graph  ,s)                                       and partition them evenly into a number nl of layers.We
    s.id ← cpntnum
 30.                                                  number these layers by numerical values. We allow states in
                    s  s
 31. for each successor of                            higher numbered layers to be the successor states of states in
       (s.id = −1)
 32. if                                               lower numbered layers, but not vice versa, so each state has
      df s (G, s)
 33.                                                  only a limited set of allowable successor states succ(s).The
    postI[cnt] ← s
 34.                                                  other parameters of these MDPs are: the maximum number
    cnt ← cnt +1
 35.                                                  of actions each state can have is ma, the maximum number of
                                                                                m              s
   Figure 2: Pseudocode of Topological Value Iteration successor states of each action, s. Given a state ,weletthe
                                                      pseudorandom number generator of C pick the number of ac-
ignore them in the dynamic programming step. Reachability tions from [1,ma], and for each action, we let that action have
is computed by a depth ﬁrst search. The overhead of this anal- a number of successor states in [1,ms]. The states are chosen
ysis is linear, and it helps us avoid considering the unreach- uniformly from succ(s) together with normalized transition
able components, so the gains can well compensate for the probabilities. The advantage of generating MDPs this way is
trouble introduced. It is extremely useful when only a small that these layered MDPs contain at least nl connected com-
portion of the state space is reachable. Since the reachabil- ponents.
ity analysis is straightforward, we do not provide any pseu- There are actual applications that lead to multi-layered
docode for it. The other optimization is the use of heuristic MDPs. A simple example is the game Bejeweled: each level
functions. Heuristic values can serve as a good starting point is at least one layer. Or consider a chess variant without
for value functions in TVI. In our program, we use the hmin pawns, played against a stochastic opponent. Each set of
heuristic from [Bonet & Geffner, 2003b]. Reachability anal- pieces that could appear on the board together leads to (at
ysis and the use of heuristics help strengthen the competitive- least one strongly-connected component. There are other,
ness of TVI. hmin replaces the expected future reward part of more serious examples, but we know of no multi-layered
the Bellman equation by the minimum of such value. It is an standard MDP benchmarks. Examples such as race-track
admissible heuristic.                                 MDPs tend to have a single scc, rendering TVI no better than
                                                     VI. (Since checking the topological structure of an MDP takes
  h   (s)=min   [C(s)+γ  · min        V (s )].
   min         a              s :Ta(s |s)>0     (3)   negligible time compared to running any of the solvers, it is

                                                 IJCAI07
                                                  1863  domain         QEs(7)     QEs(8)      QEs(9)     QEs(10)    QEt(5)    QEt(6)     QEt(7)     QEt(8)
  |S|            2187       6561        19683      59049      1024      4096       16384      65536
  |a|            28         36          45         55         15        21         28         36
  #ofsccs       2187       6561        19683      59049      243       729        2187       6561
   ∗
  v (s0)         11.129919  12.640260   14.098950  15.596161  7.626064  9.094187   10.565908  12.036075
  hmin           4.0        4.0         5.0        5.0        3.0       4.0        4.0        5.0
  VI(h =0)       1.08       4.28        15.82      61.42      0.31      1.89       10.44      59.76
  LAO*(h =0)     0.73       4.83        26.72      189.15     0.27      2.18       16.57      181.44
  LRTDP(h  =0)   0.44       1.91        7.73       32.65      0.28      2.05       16.68      126.75
  HDP(h =0)      5.44       75.13       1095.11    1648.11    0.75      29.37      1654.00    2130.87
  TVI(h =0)      0.42       1.36        4.50       15.89      0.20      1.04       5.49       35.10
  VI(hmin)       1.05       4.38        15.76      61.06      0.31      1.87       10.41      59.73
  LAO*(hmin)     0.53       3.75        19.16      126.84     0.25      1.94       14.96      123.26
  LRTDP(hmin)    0.28       1.22        4.90       20.15      0.28      1.95       16.22      124.69
  HDP(hmin)      4.42       59.71       768.59     1583.77    0.95      30.14      1842.62    2915.05
  TVI(hmin)      0.16       0.56        1.86       6.49       0.19      0.98       5.29       30.79

Table 1: Problem statistics and convergence time in CPU seconds for different algorithms with different heuristics for the qual.
exams examples ( =10−6)

easy to decide whether to use TVI.) Thus, we use our artiﬁ- layer number increases, the MDPs become more complex,
cially generated MDPs for now.                        since the states in large numbered layers have relatively small
                                                      succ(s) against ms, therefore cycles in those layers are more
5.2  Results                                          common, so it takes greater effort to solve large numbered
We consider several variants of our ﬁrst domain, and the re- layers than small numbered ones. Not surprisingly, from Ta-
sults are shown in Table 1. The statistics have shown that: ble 2 we see that when the number of layers increases, the
  • TVI outperforms the rest of the algorithms in all the in- running time of each algorithm also increases. However, the
    stances. Generally, this fast convergence is due to both increase rate of TVI is the smallest (the rate of greatest against
    the appropriate update sequence of the state space and smallest running time of TVI is 2 versus 4 of VI, 3.5 of LAO*,
    avoidance of unnecessary updates.                 and 2.3 of LRTDP). This is due to the fact that TVI applies the
                                                      best update sequence. As the layer number becomes large, al-
  • The hmin helps TVI more than it helps VI, LAO* and
                         QE                           though the update of the large numbered layers requires more
    LRTDP, especially in the s domains.               effort, the time TVI spends on the small numbered ones re-
  • TVI outperforms HDP, because our way of dealing with mains stable. But other algorithms do not have this property.
    components is different. HDP updates states of all the For the second experiment, we ﬁx the number of layers and
    unsolved components together in a depth-ﬁrst fashion vary the state space size. Again, TVI is better than other al-
    until they all converge. We pick the optimal sequence of gorithms, as seen in Table 3. When the state space is 80,000,
    backing up components, and only back up one of them TVI can solve the problems in around 12 seconds. This shows
    at a time. Our algorithm does not spend time checking that TVI can solve large problems in a reasonable amount of
    whether all the components are solved, and we only up- time. Note that the statistics we include here represent the
    date a component when it is necessary.            common cases, but were not chosen in favor of TVI. Our best
We notice that HDP shows pretty slow convergence in the QE result shows, TVI runs in around 9 seconds for MDPs with
domain. That is not due to our implementation. HDP is not |S|=20,000, nl=200, ma=20, ms=40, while VI needs more
suitable for solving problems with large numbers of actions. than 100 seconds, LAO* takes 61 seconds and LRTDP re-
Readers interested in the performance of HDP on MDPs with quires 64 seconds.
smaller action sets can refer to [Bonet & Geffner, 2003a].
  The statistics of the performance on artiﬁcially generated 6Conclusion
layered MDPs are shown in Table 2 and 3. We do not include
the HDP statistics here, since HDP is too slow in these cases. We have introduced and analyzed an MDP solver, topolog-
We also ignore the results on applying the hmin heuristic, ical value iteration, that studies the dependence relation of
since they display the same scale as not using the heuristic. the value functions of the state space and use the dependence
For each element of the table, we take the average of running relation to decide the sequence to back up states. The algo-
20 instances of MDPs with the same conﬁguration. Note that rithm is based on the idea that different MDPs have different
varying |S|, nl, ma,andms yields many MDP conﬁgura-   graphical structures, and the graphical structure of an MDP
tions. We present a few whose results are representative. intrinsically determines the complexity of solving that MDP.
  For the ﬁrst group of data, we ﬁx the state space to have We notice that no current solvers detect this information and
size 20,000 and change the number of layers. Statistics in Ta- use it to guide state backups. Thus, they solve MDPs of the
ble 2 show our TVI dominates others. We note that, as the same problem sizes but with different graphical structure with

                                                 IJCAI07
                                                  1864