             The Necessity of Syntactic Parsing for Semantic Role Labeling

                         Vasin Punyakanok         Dan Roth        Wen-tau Yih
                                    Department of Computer Science
                               University of Illinois at Urbana-Champaign
                                        Urbana, IL 61801, USA
                                {punyakan,danr,yih}@uiuc.edu

                    Abstract                          it is possible to use only shallow syntactic information to
                                                      build an outstanding SRL system.
    We provide an experimental study of the role of     Although PropBank is built by adding semantic annota-
    syntactic parsing in semantic role labeling. Our  tion to the constituents on syntactic parse trees in Penn Tree-
    conclusions demonstrate that syntactic parse infor- bank, it is not clear how important syntactic parsing is for
    mation is clearly most relevant in the very ﬁrst  building an SRL system. To the best of our knowledge, this
    stage – the pruning stage. In addition, the quality problem was ﬁrst addressed by Gildea and Palmer [2002].
    of the pruning stage cannot be determined solely  In their attempt of using limited syntactic information, the
    based on its recall and precision. Instead it depends parser was very shallow – clauses were not available and only
    on the characteristics of the output candidates that chunks were used. Moreover, the pruning stage in [Gildea
    make downstream problems easier or harder. Mo-    and Palmer, 2002] was too strict since only chunks are con-
    tivated by this observation, we suggest an effective sidered as argument candidates, meaning that over 60% of
    and simple approach of combining different seman- the arguments were not treated as candidates. As a result,
    tic role labeling systems through joint inference, the overall recall in their approach was very low. As we will
    which signiﬁcantly improves the performance.      demonstrate later, high recall of the pruning stage is in fact
                                                      essential to a quality SRL system.
                                                        Using only the shallow parse information in an SRL system
1  Introduction                                       has largely been ignored until the recent CoNLL-04 shared
Semantic parsing of sentences is believed to be an important task competition [Carreras and Marquez,` 2004]. In this com-
task toward natural language understanding, and has imme- petition, participants were restricted to only shallow parse
diate applications in tasks such information extraction and information for their SRL systems. As a result, it became
question answering. We study semantic role labeling (SRL) clear that the performance of the best shallow parse based sys-
in which for each verb in a sentence, the goal is to identify all tem [Hacioglu et al., 2004] is only 10% in F1 below the best
constituents that ﬁll a semantic role, and to determine their system that uses full parse information [Pradhan et al., 2004].
roles, such as Agent, Patient or Instrument, and their adjuncts, In addition, there has not been a true quantitative compari-
such as Locative, Temporal or Manner.                 son with shallow parsing. First, the CoNLL-04 shared task
  The PropBank project [Kingsbury and Palmer, 2002],  used only a subset of the data for training. Furthermore, its
which provides a large human-annotated corpus of seman- evaluation treats the continued and referential tags differently,
tic verb-argument relations, has enabled researchers to apply which makes the performance metric stricter and the results
machine learning techniques to improve SRL systems [Gildea worse. Second, an SRL system is usually complicated and
and Palmer, 2002; Chen and Rambow, 2003; Gildea and   consists of several stages. It is still unknown how much and
Hockenmaier, 2003; Pradhan et al., 2003; Surdeanu et al., where precisely the syntactic information helps the most.
2003; Pradhan et al., 2004; Xue and Palmer, 2004]. However, The goal of this study is twofold. First, we make a fair
most systems rely heavily on the full syntactic parse trees. comparison between SRL systems which use full parse trees
Therefore, the overall performance of the system is largely and those exclusively using shallow syntactic information.
determined by the quality of the automatic syntactic parsers This brings forward a better analysis on the necessity of full
of which state of the art [Collins, 1999; Charniak, 2001] is parsing in the SRL task. Second, to relieve the dependency of
still far from perfect.                               the SRL system on the quality of automatic parsers, we im-
  Alternatively shallow syntactic parsers (i.e., chunkers and prove semantic role labeling signiﬁcantly by combining sev-
clausers), although not providing as much information as a eral SRL systems based on different state-of-art full parsers.
full syntactic parser, have been shown to be more robust in To make our conclusions applicable to general SRL sys-
their speciﬁc task [Li and Roth, 2001]. This raises the very tems, we adhere to a widely used two step system architec-
natural and interesting question of quantifying the necessity ture. In the ﬁrst step, the system is trained to identify argu-
of the full parse information to semantic parsing and whether ment candidates for a given verb predicate. In the second step,the system classiﬁes the argument candidate into their types. [A1 The pearls] [R-A1 which] [A0 I] [V left] , [A2 to my
In addition, it is also common to use a simple procedure to          daughter-in-law] are fake.
prune obvious non-candidates before the ﬁrst step, and to use The distribution of these argument labels is fairly unbal-
post-processing inference to ﬁx inconsistent predictions after anced. In the ofﬁcial release of PropBank I, core arguments
the second step. We also employ these two additional steps. (A0–A5 and AA) occupy 71.26%, where the largest parts are
  In our comparison between the systems using shallow and A0 (25.39%) and A1 (35.19%). The rest portion is mostly
full syntactic information, we found the most interesting re- the adjunct arguments (24.90%). The continued (C-arg) and
sult is that while each step of the system using shallow in- referential (R-arg) arguments are relatively fewer, occupying
formation exhibits very good performance, the overall per- 1.22% and 2.63% respectively. For more deﬁnitions of Prop-
formance is signiﬁcantly inferior to the system that uses full Bank and the semantic role labeling task, readers can refer
information. This necessity of full parse information is es- to [Kingsbury and Palmer, 2002] and [Carreras and Marquez,`
pecially noticeable at the pruning stage. In addition, we pro- 2004].
duce a state-of-the-art SRL system by combining of different
SRL systems based on two (potentially noisy) automatic full 3 SRL System Architecture
parsers [Collins, 1999; Charniak, 2001].
  The rest of the paper is organized as follows. Section 2 Our SRL system consists of four stages: pruning, argument
gives a brief description of the semantic role labeling task identiﬁcation, argument classiﬁcation, and inference. In par-
and the PropBank corpus. Section 3 introduces the general ticular, the goal of pruning and argument identiﬁcation is to
architecture of an SRL system, including the features used in identify argument candidates for a given verb predicate. The
different stages. The detailed experimental comparison be- system only classiﬁes the argument candidate into their types
tween using full parsing and shallow parsing is provided in in the stage of argument classiﬁcation. Linguistic and struc-
Section 4, where we try to explain why and where the full tural constraints are incorporated in the inference stage to re-
parse information contributes to SRL. Inspired by the result, solve inconsistent global predictions. This section describes
we suggests an approach that combines different SRL sys- how we build these four stages, including the features used in
tems based on joint inference in Section 5. Finally, Section 6 training the classiﬁers.
concludes this paper.
                                                      3.1  Pruning
                                                      When the full parse tree of a sentence is available, only the
2  Semantic Role Labeling (SRL) Task                  constituents in the parse tree are considered as argument can-
The goal of the semantic-role labeling task is to discover the didates. In addition, our system exploits the heuristic rules
verb-argument structure for a given input sentence. For exam- introduced by Xue and Palmer [2004] to ﬁlter out simple con-
ple, given a sentence “ I left my pearls to my daughter-in-law stituents that are very unlikely to be arguments. The heuristic
in my will”, the goal is to identify different arguments of the is a recursive process starting from the verb of which argu-
verb left which yields the output:                    ments to be identiﬁed. It ﬁrst returns the siblings of the verb
                                                      as candidates; then it moves to the parent of the verb, and col-
  [A0 I] [V left ] [A1 my pearls] [A2 to my daughter-in-law] lects the siblings again. The process goes on until it reaches
                 [AM-LOC in my will].                 the root. In addition, if a constituent is a PP (propositional
Here A0 represents the leaver, A1 represents the thing left, phrase), its children are also collected.
A2 represents the benefactor, AM-LOC is an adjunct indicat-
ing the location of the action, and V determines the verb. In 3.2 Argument Identiﬁcation
addition, each argument can be mapped to a constituent in its The argument identiﬁcation stage utilizes binary classiﬁca-
corresponding syntactic full parse tree.              tion to identify whether a candidate is an argument or not.
  Following the deﬁnition of the PropBank and CoNLL-  When full parsing is available, we train and apply the binary
2004 shared task, there are six different types of arguments classiﬁers on the constituents supplied by the pruning stage.
labeled as A0-A5 and AA. These labels have different seman- When only shallow parsing is available, the system does not
tics for each verb as speciﬁed in the PropBank Frame ﬁles. In have the pruning stage, and also does not have constituents to
addition, there are also 13 types of adjuncts labeled as AM- begin with. Therefore, conceptually the system has to con-
adj where adj speciﬁes the adjunct type. In some cases, an sider all possible subsequences (i.e., consecutive words) in a
argument may span over different parts of a sentence, the la- sentence as potential argument candidates. We avoid this by
bel C-arg is used to specify the continuity of the arguments, using a learning scheme by ﬁrst training two classiﬁers, one
as shown in the example below.                        to predict the beginnings of possible arguments, and the other
                                                      the ends. The predictions are combined to form argument
   [A1 The pearls] , [A0 I] [V said] , [C-A1 were left to my candidates that do not violate the following constraints.
                  daughter-in-law].
                                                        1. Arguments cannot cover the predicate.
Moreover in some cases, an argument might be a relative pro-
noun that in fact refers to the actual agent outside the clause. 2. Arguments cannot overlap with the clauses (they can be
In this case, the actual agent is labeled as the appropriate argu- embedded in one another).
ment type, arg, while the relative pronoun is instead labeled 3. If a predicate is outside a clause, its arguments cannot be
as R-arg. For example,                                    embedded in that clause.  The features used in the full parsing and shallow parsing • Phrase type uses a simple heuristics to identify only VP,
settings are described as follows.                        PP, and NP.
Features when full parsing is available                 • Head word and POS tag of the head word   are the
Most of the features used in our system are standard features rightmost word for NP, and leftmost word for VP and
which include                                             PP.
                                                        • Shallow-Path records the traversal path in the pseudo-
  • Predicate and POS tag of predicate features indicate  parse tree constructed only from the clause structure and
    the lemma of the predicate verb and its POS tag.      chunks.
  • Voice feature indicates passive/active voice of the predi- • Shallow-Subcategorization feature describes the chunk
    cate.                                                 and clause structure around the predicate’s parent in the
  • Phrase type feature provides the phrase type of the con- pseudo-parse tree.
    stituent.                                           • Syntactic frame features are discarded.
  • Head word and POS tag of the head word feature pro-
    vides the head word and its POS tag of the constituent. 3.3 Argument Classiﬁcation
    We use rules introduced by Collins [1999] to extract this This stage assigns the ﬁnal argument labels to the argument
    feature.                                          candidates supplied from the previous stage. A multi-class
  • Position feature describes if the constituent is before or classiﬁer is trained to classify the types of the arguments sup-
    after the predicate relative to the position in the sentence. plied by the argument identiﬁcation stage. In addition, to re-
  • Path records the traversal path in the parse tree from the duce the excessive candidates mistakenly output by the pre-
    predicate to the constituent.                     vious stage, the classiﬁer can also classify the argument as
  • Subcategorization feature describes the phrase struc- NULL (meaning “not an argument”) to discard the argument.
    ture around the predicate’s parent. It records the imme- The features used here are the same as those used in the
    diate structure in the parse tree that expands to its parent. argument identiﬁcation stage. However, when full parsing
                                                      are available, an additional feature introduced by Xue and
  We also use the following additional features.      Palmer [2004] is used.
  • Verb class feature is the class of the active predicate de- • Syntactic frame describes the sequential pattern of the
    scribed in PropBank Frames.                           noun phrases and the predicate in the sentence.
  • Lengths of the target constituent, in the numbers of
    words and chunks separately.                      3.4  Inference
  • Chunk  tells if the target argument is, embeds, overlaps, The purpose of this stage is to incorporate some prior lin-
    or is embedded in a chunk with its type.          guistic and structural knowledge, such as “arguments do not
  • Chunk pattern encodes the sequence of chunks from overlap” or “each verb takes at most one argument of each
    the current words to the predicate.               type.” This knowledge is used to resolve any inconsistencies
  • Chunk pattern length feature counts the number of of argument classiﬁcation in order to generate ﬁnal legiti-
    chunks in the argument.                           mate predictions. We use the inference process introduced
                                                      by Punyakanok et al. [2004]. The process is formulated as
  • Clause relative position feature is the position of the an integer linear programming (ILP) problem that takes as
    target word relative to the predicate in the pseudo-parse inputs the conﬁdences over each type of the arguments sup-
    tree constructed only from clause constituent. There plied by the argument classiﬁer. The output is the optimal so-
    are four conﬁgurations—target constituent and predicate lution that maximizes the linear sum of the conﬁdence scores
    share the same parent, target constituent parent is an an- (e.g., the conditional probabilities estimated by the argument
    cestor of predicate, predicate parent is an ancestor of tar- classiﬁer), subject to the constraints that encode the domain
    get word, or otherwise.                           knowledge.
  • Clause coverage describes how much of the local clause
    (from the predicate) is covered by the target argument. 4 The Necessity of Syntactic Parsing
  • NEG  feature is active if the target verb chunk has not
    or n’t.                                           We study the necessity of syntactic parsing experimentally by
                                                      observing the effects of using full parsing and shallow pars-
  • MOD  feature is active when there is a modal verb in the
                                                      ing at each stage of an SRL system. In Section 4.1, we ﬁrst
    verb chunk. The rules of the NEG and MOD features
                                                      describe how we prepare the data, as well as the basic system
    are used in a baseline SRL system developed by Erik
                                                      including features and the learning algorithm. The compar-
    Tjong Kim Sang [Carreras and Marquez,` 2004].
                                                      ison of full parsing and shallow parsing on the three stages
Features when only shallow parsing is available       (excluding the inference stage) is presented in the reversed
Features used are similar to those used by the system with full order (Sections 4.2, 4.3, 4.4).
parsing except those that need full parse trees to generate. For
these types of features, we either try to mimic the features 4.1 Experimental Setting
with some heuristics rules or discard them. The details of We use PropBank sections 02 through 21 as training data, and
these features are as follows.                        section 23 as testing. In order to apply the standard CoNLL-04 evaluation script, our system conforms to both the input and clauses. However, it is unclear how to mimic the syn-
and output format deﬁned in the shared task.          tactic frame feature since it relies on the internal structure of
  The CoNLL-04 evaluation metric is slightly more re- a full parse tree. Therefore, it does not have a corresponding
stricted since an argument prediction is only considered cor- feature in the shallow parsing case.
rect when all its continued arguments (C-arg) are correct and Table 1 reports the experimental results of argument clas-
referential arguments (R-arg) are included – these require- siﬁcation when argument boundaries are known. Although
ments are often absent in previous SRL systems, given that full parsing features seem to help when using the gold stan-
they only occupy a very small percentage of the data. To pro- dard data, the difference in F1 is only 0.32% and 0.13% for
vide a fair comparison, we also report the performance when the CoNLL-2004 and ArgM+ evaluation respectively. When
discarding continued and referential arguments. Following the automatic (full and shallow) parsers are used, the gap is
the notation used in [Xue and Palmer, 2004], this evaluation smaller.
metric is referred as “argM+”, which considers all the core ar-
guments and adjunct arguments. We note here that all the per-             Full Parsing Shallow Parsing
formance reported excludes V label which usually improves      Gold         91.32          91.00
the overall performance if included.                           Auto         90.93          90.69
  The goal of the experiments in this section is to under- Gold (ArgM+)     90.67          91.54
stand the effective contribution of full parsing versus shal- Auto (ArgM+)  90.87          90.93
low parsing using only the part-of-speech tags, chunks, and
clauses. In addition, we also compare performance when us- Table 1: The accuracy of argument classiﬁcation when argu-
ing the correct (gold standard) versus using automatic parse ment boundaries are known
data. The automatic full parse trees are derived using Char-
niak’s parser [Charniak, 2001] (version 0.4). In automatic
shallow parsing, the information is generated by a state- Lesson When the argument boundaries are known, the per-
of-the-art POS tagger [Even-Zohar and Roth, 2001], chun- formance of the full paring systems is about the same as the
ker [Punyakanok and Roth, 2001], and clauser [Carreras and shallow parsing system.
Marquez,` 2003].
  The learning algorithm used is a variation of the Winnow 4.3 Argument Identiﬁcation
update rule incorporated in SNoW [Roth, 1998; Roth and
Yih, 2002], a multi-class classiﬁer that is tailored for large Argument identiﬁcation is an important stage that effectively
scale learning tasks. SNoW learns a sparse network of linear reduces the number of argument candidates after pruning.
functions, in which the targets (argument border predictions Given an argument candidate, an argument identiﬁer is a
or argument type predictions, in this case) are represented as binary classiﬁer that decides whether or not the candidate
linear functions over a common feature space. It improves should be considered as an argument. To evaluate the inﬂu-
the basic Winnow multiplicative update rule in several ways. ence of full parsing in this stage, the candidate list used here
For example, a regularization term is added, which has the ef- is the pruning results on the gold standard parse trees.
fect of trying to separate the data with a large margin separa- Similar to the argument classiﬁcation stage, the only differ-
tor [Grove and Roth, 2001; Hang et al., 2002] and voted (av- ence between full-parse and shallow-parse is the use of path
eraged) weight vector is used [Freund and Schapire, 1999]. and subcategorization features. Again, we replace them with
  Experimental evidences have shown that SNoW activa- shallow-path and shallow-subcategorization when the binary
tions correlate with the conﬁdence of the prediction and can classiﬁer is trained using the shallow parsing information.
provide an estimate of probability to be used for both argu- Table 2 reports the performance of the argument identiﬁer
ment identiﬁcation and inference. We use the softmax func- on the test set using the direct predictions of the trained binary
tion [Bishop, 1995] to convert raw activation to conditional classiﬁer. The recall and precision of the full parsing system
probabilities. Speciﬁcally, if there are n classes and the raw are around 2 to 3 percents higher than the shallow parsing
activation of class i is acti, the posterior estimation for class system on the gold standard data. As a result, the F1 score
i is                                                  is 2.5% higher. The performance on automatic parse data is
                              eacti                   unsurprisingly lower, but the difference between full parsing
           score(i) = pi =            .
                          P       eactj               and shallow parsing is relatively the same. In terms of ﬁlter-
                            1≤j≤n                     ing efﬁciency, around 25% of the examples are predicted as
4.2  Argument Classiﬁcation                           positive. In other words, both argument identiﬁers ﬁlter out
To evaluate the performance gap between full parsing and around 75% of the argument candidates after pruning.
shallow parsing in argument classiﬁcation, we assume the
argument boundaries are known, and only train classiﬁers           Full Parsing        Shallow Parsing
to classify the labels of these arguments. In this stage, the  Prec   Rec     F1     Prec   Rec     F1
only difference between full parsing and shallow parsing is Gold 96.53 93.57 95.03  93.66  91.72   92.68
the construction of three full parsing features: path, sub- Auto 94.68 90.60 92.59  92.31  88.36   90.29
categorization and syntactic frame. As described in Sec-
tion 3, path and subcategorization can be approximated by Table 2: The performance of argument identiﬁcation after
shallow-path and shallow-subcategorization using chunks pruning (based on the gold standard full parse trees)            Full Parsing         Shallow Parsing      argument; the other classiﬁer is to predict the end (E) of an
        Prec    Rec     F1    Prec    Rec    F1       argument. If the product of probabilities of a pair of S and E
 Gold   92.13  95.62  93.84   88.54  94.81  91.57     predictions is larger than a predeﬁned threshold, then this pair
 Auto   89.48  94.14  91.75   86.14  93.21  89.54     is considered as an argument candidate. The pruning compar-
                                                      ison of using the classiﬁers and heuristics is shown in Table 6.
Table 3: The performance of argument identiﬁcation after
pruning (based on the gold standard full parse trees) and with     Full Parsing       Classiﬁer th=0.04
threshold=0.1
                                                               Prec   Rec     F1     Prec   Rec     F1
                                                        Gold  25.94  97.27   40.96  29.58  97.18   45.35
  Since the recall in argument identiﬁcation sets the upper Auto 22.79 86.08 36.04  24.68  94.80   39.17
bound of the recall in argument classiﬁcation, in practice, the
threshold that predicts examples as positive is usually low-    Table 6: The performance of pruning
ered to allow more positive predictions. That is, a candidate is
predicted as positive when its probability estimation is larger Amazingly, the classiﬁer pruning strategy seems better
than the threshold. Table 3 shows the performance of the ar- than the heuristics. With about the same recall, the classiﬁers
gument identiﬁers when the threshold is 0.1.          achieve higher precision. However, to really compare sys-
  Since argument identiﬁcation is just an intermediate step tems using full parsing and shallow parsing, we still need to
of a complete system, a more realistic evaluation method is to see the overall performance. We build two semantic role sys-
see how each ﬁnal system performs. Table 4 and Table 5 re- tems based on full parsing and shallow parsing. The full pars-
port the ﬁnal results in recall, precision, and F1 in CoNLL and ing system follows the pruning, argument identiﬁcation, ar-
ArgM+ metrics. The F1 difference is about 4.5% when using gument classiﬁcation, and inference stages, as described ear-
the gold standard data. However, when automatic parsers are lier. For the shallow parsing system, pruning is replaced by
used, shallow-parse is in fact slightly better. This may be due the word-based pruning classiﬁers, and the rest stages are de-
to the fact that shallow parsers are more accurate in chunk or signed only to use shallow parsing information as described
clause predictions compared to a regular full parser [Li and in previous sections. Table 7 and Table 8 show the overall
Roth, 2001].                                          performance in the two evaluation methods.
            Full Parsing        Shallow Parsing
                                                                   Full Parsing        Shallow Parsing
        Prec    Rec    F      Prec    Rec    F
                         1                    1                Prec   Rec     F      Prec   Rec     F
 Gold   88.81  89.35  89.08   84.19  85.03  84.61                              1                     1
                                                        Gold  88.81  89.35   89.08  75.34  75.28  75.31
 Auto   84.21  85.04  84.63   86.17  84.02  85.08
                                                        Auto  77.09  75.51   76.29  75.48  67.13  71.06
Table 4: The CoNLL-04 evaluation of the overall system per-
                                                      Table 7: The CoNLL-04 evaluation of the overall system per-
formance when pruning (using the gold standard full parse
                                                      formance
trees) is available
                                                                   Full Parsing        Shallow Parsing
            Full Parsing        Shallow Parsing
                                                               Prec   Rec     F1     Prec   Rec     F1
        Prec    Rec    F1     Prec   Rec     F1         Gold  89.02  89.57   89.29  75.35  75.20  75.27
 Gold   89.02  89.57  89.29   84.46  85.31  84.88       Auto  77.09  75.57   76.32  75.54  67.14  71.09
 Auto   84.38  85.38  84.87   86.37  84.36  85.35
                                                          Table 8: ArgM+ performance of the overall system
Table 5: ArgM+ performance of the overall system when
pruning (using the gold standard full parse trees) is available
                                                        As indicated in the tables, the gap in F1 between the full
                                                      parsing and shallow parsing systems enlarges to more than
Lesson  Full parsing helps in argument identiﬁcation. How- 13% on the gold standard data. At ﬁrst glance, this result
ever, when the automatic shallow parser is more accurate than seems to contradict our conclusion in Section 4.3. After all,
the full parser, using the full parsing information may not if the pruning stage of the shallow parsing SRL system per-
have advantages over shallow parsing.                 forms equally well or even better, the overall performance gap
                                                      in F1 should be small.
4.4  Pruning                                            After we carefully examine the output of the word-based
As shown in the previous two subsections, the performance classiﬁer pruning, we realize that it in fact ﬁlters out “easy”
difference of full parsing and shallow parsing is not large candidates, and leaves examples that are difﬁcult to the later
when the pruning information is given. We conclude that the stages. To be speciﬁc, these argument candidates often over-
main contribution of the full parse is in the pruning stage. lap and differ only with one or two words. On the other
Since the shallow parsing system does not have enough in- hand, the pruning heuristics based on full parsing never out-
formation for the pruning heuristics, we train two word based puts overlapping candidates. The following argument iden-
classiﬁers to replace the pruning stage. One classiﬁer is tiﬁcation stage can be thought of as good in discriminating
trained to predict whether a given word is the start (S) of an different types of candidates.