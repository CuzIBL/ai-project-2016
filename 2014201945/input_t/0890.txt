 Scale-Based Monotonicity Analysis in Qualitative Modelling with Flat Segments
                       Martin Brooks1    and Yuhong Yan1     and  Daniel Lemire2
                1National Research Council                      2University of Quebec
                200 Montreal Road, M-50                       4750 avenue Henri-Julien
                  Ottawa, ON K1A 0R6                           Montréal, QC H2T 3E4
                 martin.brooks@nrc.gc.ca                        lemire@ondelette.com
                  yuhong.yan@nrc.gc.ca

                    Abstract                          be fast (O(n)) and not require excessive amounts of memory
                                                      (O(n)).
    Qualitative models are often more suitable than     In this paper, we address the “scale-based monotonicity”
    classical quantitative models in tasks such as    problem: monotonicity can be deﬁned relative to a scale. We
    Model-based Diagnosis (MBD), explaining system    deﬁne a metric for monotonic approximation. We solve the
    behavior, and designing novel devices from ﬁrst   following problem: given a number of segments K, ﬁnd K
    principles. Monotonicity is an important feature  segments having the smallest monotonic approximation error.
    to leverage when constructing qualitative models.
                                                        In short, the main novel results of this paper are:
    Detecting monotonic pieces robustly and efﬁciently
    from sensor or simulation data remains an open      1. A novel optimal segmentation algorithm;
    problem. This paper presents scale-based mono-      2. A novel deﬁnition of scale-based ﬂatness.
    tonicity: the notion that monotonicity can be de-
    ﬁned relative to a scale. Real-valued functions de- 2 Monotonicity in Qualitative Modelling
    ﬁned on a ﬁnite set of reals e.g. sensor data or
    simulation results, can be partitioned into quasi- We just list a few examples to show that the monotonic
    monotonic segments, i.e. segments monotonic with  features are important to leverage in qualitative modeling.
    respect to a scale, in linear time. A novel segmen- In QSIM [Kuipers, 1986], the qualitative value QV( f ,t), is
    tation algorithm is introduced along with a scale- the pair <qmag, qdir>, where qmag is a landmark value
    based deﬁnition of “ﬂatness”.                     li or an interval made up of landmark values (li,li+1), and
                                                      qdir takes a value from the sign domain (+,0,-) according
                                                      the f 0(t) is increasing, steady, or decreasing respectively.
1  Introduction                                       In Model-based Diagnosis (MBD), qualitative models are
Qualitative models are used in applications such as model- used to describe system behavior. Two kinds of qualitative
based diagnosis [Yan, 2003; Struss, 2002], explaining system models, based respectively on absolute or relative quanti-
behavior [Šuc, 2003; Forbus, 1984; Kuipers, 1986], and de- ties, rely on the monotonicity of the function. The ﬁrst one
signing novel devices from ﬁrst principles [Williams, 1992]. is Finite Relation Qualitative Model (FRQ) [Struss, 2002;
It is a challenge to build qualitative models for complex real Yan, 2003], where the qualitative relation is represented by
world engineering systems. Current research efforts are on tuples of real valued intervals. For a monotonic segment
automatic generation of qualitative models from numerical [xa,xb] of f , the tuple is determined by the bounding rect-
data obtained by numerical simulation or sensors. Many pro- angle  = [xa,xb] × [min( f (xa), f (xb)),max( f (xa), f (xb))],
                                                                                               V
posed methods, such as in [Struss, 2002] and [Console et al., that is the smallest rectangle such that y = f (x) xa ≤ x ≤
2003], work only when the functions are piecewise mono- xb ⇒ (x,y) ∈ . The second one is Qualitative Deviation
tonic. Hence, partitioning data arrays into quasi-monotonic Model [Console et al., 2003] where the qualitative relation
segments is an important problem [Yan et al., 2004b; 2004a]. is represented by the sign of deviation [∆y] from a refer-
  One could segment a data array in monotonic segments ence point xre f deﬁned as +,-, or 0, according to whether
using a naïve algorithm: simply segment wherever there is f is increasing, decreasing or ﬂat. For example, if f is
an extremum. For example, it is easy to segment the array monotonic increasing, we have [∆y] = sign( f (x)− f (xre f )) =
{0, 1, 2, 3, 2, 1, 0} in two monotonic segments. How- sign(x − xre f ) = [∆x].
ever, monotonic segments must be signiﬁcant for the appli- In this paper, we focus on abstracting qualitative models
cation at hand and algorithms must be robust because the from scattered data obtained from numerical simulation or
data will unavoidably be subject to noise or insigniﬁcant sensors. This work is motivated by at least two kinds of appli-
features. Hence, we might want to consider that the array cations. First, when applying model-based diagnosis to real
{0, 1.2, 1.1, 3, 2, 1, 0} has only two “signiﬁcant” mono- world engineering systems, we need to build symbolic qual-
tonic segments since the drop from 1.2 to 1.1 is not large itative models by a numerical simulation of the engineering
enough to indicate a downward trend. Also algorithms must models. Second, we need to explain system behaviors fromsensor data. Qualitative model abstraction in this paper is min f ∈Ω maxx∈D | f − F| where Ω is either Ω↑ or Ω↓. Sign +
deﬁned as transforming numerical values into qualitative val- or - is associated to Ω↑ or Ω↓.
ues and functions into qualitative constraints [Šuc and Bratko, The segmentation of a set D is a sequence S =
2001]. Monotonicity Analysis is deﬁned as partitioning a ﬁ- X1,...,Xm of closed intervals (called “segments”) in D with
                                                                     S
nite series of real values x1,x2,...,xn over an interval D into [minD,maxD] = i Xi such that maxXi = minXi+1 and Xi ∩
“monotonic” segments.                                 Xj = 0/ for j 6= i,i + 1,i − 1. Alternatively, we can deﬁne a
  Monotonicity analysis is a crucial step to abstract qualita- segmentation from the set of points Xi ∩ Xi+1 = {yi}. Given
tive models from scattered data but it rises a problem. The F : {x1,...,xn} → R and a segmentation {Xi}, the Optimal
difﬁculty is that when the scattered data contains noise, the Piecewise Monotonic Approximation Function Error (OP-
monotonicity is not absolute in a neighborhood of any point. MAFE) of the segmentation is given by maxi OMAFE(F|Xi )
The monotonic segments need to be signiﬁcant in the context where the directions of the segments Xi are alternating and
of the problem. The small ﬂuctuations caused by noise must such that the direction of the ﬁrst segment is chosen so as to
be ignored. This requires that noise be removed by compu- minimize the OPMAFE.
tationally efﬁcient methods such that the number of remain- Solving for a best monotonic function can be done as fol-
ing segments is dependent only on the characterization of the lows. If we seek the best monotonic increasing function, we
noise.                                                ﬁrst deﬁne f ↑(x) = max{F(y) : y ≤ x} (the maximum of all
  Linear splines is an obvious approach to solve this prob- previous values) and f (x) = min{F(y) : y ≥ x} (the mini-
                                                                          ↑
lem. We can use top-down, bottom-up and sliding window mum of all values to come). If we seek the best monotonic
algorithms to approximate the data with a set of straight decreasing function, we deﬁne f (x) = max{F(y) : y ≥ x}
    [               ]                                                             ↓
lines Keogh et al., 2001 . Then the same-sign slopes can be (the maximum of all values to come) and f (x) = min{F(y) :
aggregated as monotonic segments. Various algorithms are                                  ↓
derived from classic algorithms [Key et al., 2000] [Hunter y ≤ x} (the minimum of all previous values). These func-
and McIntosh, 1999]. The downside of these methods is that tions which can be computed in linear time are all we need
there is no link between linear spline approximation error and to solve for the best approximation function as shown by the
the actual monotonicity of the data (consider y = ex). Hence, next theorem which is a well-known result [Brooks, 1994;
using linear splines, it is difﬁcult to specify either the desired Ubhaya, 1974].
number of monotonic segments or some “monotonicity error” Theorem 1. Given F : D = {x1,...,xn} → R, a best mono-
threshold. In addition, linear ﬁtting algorithms are relatively tonic increasing approximation function to F is given by
expensive.                                                 f + f
                                                         =  ↑  ↑
  Inductive learning is used in [Šuc and Bratko, 2001] to au- f↑ 2 and a best monotonic decreasing approxima-
                                                                                f + f
tomatically construct qualitative models from quantitative ex-                   ↓  ↓
                                                      tion function is given by f↓ = 2 . The corresponding er-
amples. The induced qualitative model is a binary tree, called                     | ( )− ( )|
                                                                                   f ↑ x f ↑ x
a qualitative tree, which contains internal nodes (called splits) ror (OMAFE) is given by maxx∈D 2 (monotonic in-
                                                                        | ( )− ( )|
and qualitatively constrained functions at the leaves. A quali-         f ↓ x f ↓ x
tative constrained function takes the form Ms1,...,sm : Rm 7→ R, creasing) or maxx∈D 2 (monotonic decreasing).
si ∈ {+,−} and represents a function with m real-valued at- The implementation of the algorithm suggested by the the-
tributes strictly monotone increasing with respect to the i-th orem is straight-forward. Given a segmentation, we can com-
attribute if si = +, or strictly monotone decreasing if si = −. pute the OPMAFE in O(n) time using at most two passes.
                 +,−
For example, f = M  (x,y) means f is increasing when x The functions f↑ and f↓ are sometimes called the standard
is increasing, and decreasing when y is increasing. A split optimal monotone functions as the solution is not unique in
is a partition of a variable. The unsupervised learning algo- general.
rithm eq-QUIN determines the landmarks for the splits. The
training data set is all possible pairs of data points. eq-QUIN 4 Scale-Based Monotonicity
checks the best split against all possible hypotheses. Its com-
plexity is O(n22m). QUIN is a more efﬁcient algorithm that We present the notion of scale-based monotonicity. The in-
uses greedy search. Its complexity is O(n2m2). We do not tuition is that the ﬂuctuations within a certain scale can be
address multidimensional data in this paper.          ignored.
                                                        Given an ordered set of real values D = {x1,x2,...,xn},
                                                                                     R
3  Monotonicity Error                                 consider F : D = {x1,x2,...,xn} 7→ . Given some toler-
                                                      ance value δ > 0, we could say that the data points are not
In this section, we deﬁne a measure of monotonicity. Sup- going down or are upward monotone, if consecutive mea-
pose we are given a set of n ordered samples noted F : sures do not go down by more than δ, that is, are such that
                R    R
D = {x1,...,xn} ⊂ →    with real values F(x1),...,F(xn) F(xi)−F(xi+1) < δ. However, this deﬁnition is not very use-
and x1 < x2 < ... < xn. We deﬁne, F|[a,b] as the restric- ful because measures can repeatedly go down and thus the
tion of F over D ∩ [a,b]. We seek the best monotonic (in- end value can be substantially lower than the start value. A
creasing or decreasing) function f : R → R approximating more useful deﬁnition of upward monotonicity would be to
F. Let Ω↑ (resp. Ω↓) be the set of all monotonic increas- require that we cannot ﬁnd two values xi and x j (xi < x j) such
ing (resp. decreasing) functions. The Optimal Monotonic that F(x j) is lower than F(xi) by δ (i.e. F(xi) − F(x j) < δ).
Approximation Function Error (OMAFE) of F is given by This deﬁnition is more useful because in the worst case, the                                                      5   A Scale-Based Algorithm for
                                                          Quasi-Monotonic Segmentation
                                                      Suppose that D is a ﬁnite set of reals having at least two ele-
                                                      ments. F is a real-valued function on D, i.e., F : D → R.
                                                        We begin by deﬁning a segmentation at scale δ or a δ-
                                                      segmentation.
           δ
                                                      Deﬁnition 3. Let S = X1,...,Xn be a segmentation of D, and
                                                      let δ > 0, then S is a δ-segmentation of F when all the fol-
                                                      lowing conditions hold.

         δ                                              • Each Xi is δ-monotone.
                    δ−pair
                                                        • Each Xi for i 6= 1,n is strictly δ-monotone.
                                                        • At least one Xi is strictly δ-monotone.
                 Figure 1: A δ-pair.                    • Adjacent strictly δ-monotone segments have opposite di-
                                                          rections.
last measure will be only δ smaller than the ﬁrst measure. • For each strictly δ-monotone Xi, and for all x ∈ Xi, F(x)
However, we are still not guaranteed that the data does in fact lies in the closed interval bounded by F(minXi) and
increase at any point. Hence, we add the constraint that we F(maxXi).
can ﬁnd at least two data points xk < xl such that F(xl) is • When X is not strictly δ-monotone, then for all x ∈
                        δ                δ                       1
greater than F(xk) by at least (F(xl) − F(xk) ≥ ).        X1 − maxX1, F(x) lies in the open interval bounded
                              δ
  To summarize, given some value > 0, we say that a se-   by F(minX2)  and F(maxX2); and when Xn    is not
                          δ
quence of measures is upward -monotone if no two succes-  strictly δ-monotone, then for all x ∈ Xn − minXn, F(x)
                                δ
sive measures decrease by as much as , and at least one pair lies in the open interval bounded by F(minXn−1) and
                                     δ
of successive measures increases by at least . Similarly, we F(maxXn−1).
say that a set of measures is downward δ-monotone if no two           δ                       δ
                                      δ               Each Xi is called a -segment of S. When strictly -monotone,
successive measures increase by as much as , and at least X is a proper δ-segment; when not strictly δ-monotone, X
two measures decrease by at least δ.                   i                                                1
                                                      or Xn is an improper δ-segment. For strictly δ-monotone Xi,
  This generalized deﬁnition of monotonicity was introduced minX and maxX are δ-extrema.
in [Brooks, 1994] using δ-pairs (see Fig. 1):              i         i
                                                        As the following theorems show, all δ-segmentations are
Deﬁnition 1.                                          “equivalent” and the monotonic approximation error (OP-
  • x < y ∈ D is a δ-pair (or a pair of scale δ) for F if |F(y)− MAFE) is known precisely.
    F(x)| ≥ δ and for all z ∈ D, x < z < y implies |F(z) − Theorem 2. Let δ > 0. Let S1 and S2 be δ-segmentations of
    F(x)| < δ and |F(y) − F(z)| < δ.                  F; then |S1| = |S2|. Furthermore, the ﬁrst δ-segments of S1
      δ                                               and S2 are both either proper or improper, and similarly for
  • A  -pair’s direction is increasing or decreasing accord- the last δ-segments.
    ing to whether F(y) > F(x) or F(y) < F(x).
                                                      Theorem 3.  Let δ > 0. Let S be any δ-segmentation of F;
  Notice that pairs of scale δ having opposite directions can- then the monotonic approximation error (OPMAFE) ≤ δ/2.
                                                 δ
not overlap but they may share an endpoint. Pairs of scale                     δ
of the same direction may overlap, but may not be nested for Now we want to compute a -segmentation given F. There
a certain δ.                                          are two approaches depending on the application one has in
                                                      mind. The ﬁrst one is to choose δ and then solve for the
  We can deﬁne δ-monotonicity as follows:
                                                      segments [Brooks, 1994], when the magnitude of noise is
                                                      known. When one doesn’t know how to choose δ, the second
Deﬁnition 2. Let X be an interval, F is δ-monotone on X approach is to set the maximal number of segments K, espe-
if all δ-pairs in X have the same direction; F is strictly δ- cially when one knows the shape of the function. We focus
monotonic when there exists at least one such δ-pair. In this on this second approach. We begin by labelling the extrema
case:                                                 with a corresponding scale ∆(x).
  • F is δ-increasing on X if X contains an increasing δ- Deﬁnition 4. Let x ∈ D be a local extremum of F. x’s delta-
    pair.                                             scale ∆(x) is the largest δ > 0 such that there exists a δ-
                                                      segmentation having x as a δ-extremum.
  • F is δ-decreasing on X if X contains a decreasing δ-pair.
                                                        It is immediate from the deﬁnition that if δ > ∆(x), then
                                                  0
We say that a pair is signiﬁcant at scale δ if it is of scale δ x can’t be a δ-extremum, but also that if x is a δ-extremum,
    0
for δ ≥ δ.                                            then ∆(x) ≥ δ. This is stated in the following proposition.

                                                      Proposition 1. Let δ > 0, and let S = X1,...,Xn be a δ-
  In the next section, we discuss how to partition the data set segmentation of F. Then ∆(x) ≥ δ for every δ-extremum, x, of
into monotonic segments.                              S.  We observe that there must be a smallest δ such that the Algorithm 1 computes ∆(x) for every extremum x ∈ D. The
cardinality of Xδ = {x|∆(x) ≥ δ} is at most K. However the input to Algorithm 1 is a list of extrema - i.e. the extrema data
set Xδ might contain repeated maxima or repeated minima points in the data array.
and those might be removed before the set Xδ can deﬁne a
δ-segmentation. This is stated in the next theorem.   Algorithm 1 This algorithm labels the extrema in linear time
Theorem 4. Let δ > 0 and consider the set of extrema Xδ = (as in Deﬁnition 4).
{x|∆(x) ≥ δ} of F as an ordered set. Each extremum in Xδ INPUT: data - a list of the successive extremal values of F, al-
is either a maximum or a minimum, and a sequence of two ternating between maximum and minimum. Each element is an
                                             0          "extremum record", having three ﬁelds: value, sense, and index.
minima or two maxima is possible. Consider a subset Xδ ⊂ Xδ
such that                                               Index identiﬁes the data point, value gives F’s value at that data
                                                        point, and sense indicates whether the extremum is a maximum
  • it has no repeated maxima or minima;                or minimum.
                      00   0                            OUTPUT: a list of "scale records". Each scale record has two
  • there is no superset Xδ of Xδ in Xδ without repeated ex- ﬁelds: scale and extrema_list. Extrema_list comprises either one
    trema;                                              or two extremum records. The semantics of a scale record is that
                                             0          the indicated extrema have the indicated scale as delta-scale.
then there exists a δ-segmentation S of F such that Xδ is ex-
            δ                                           Notes about the algorithm: 1) Lists are accessed by element
actly the set of -extrema of S.                         numbers; e.g. data(2) is the second element of data. 2) Lists
  In order to compute ∆(x) for all extrema x, it is useful to are manipulated with functions Push and Pop. Push( thing, list )
introduce the following deﬁnitions.                     adds thing to list, resulting in thing being the ﬁrst element of list.
                                                        Pop(list) removes the ﬁrst element of list, and returns this ﬁrst
Deﬁnition 5. Opposite-sense extrema x < z ∈ X ⊂ D are an element as the value of the function call. 3) MakeList(item1..∗)
extremal pair for X if for all y ∈ X, x < y < z implies F(y) pushes the items into a list generated, starting from the ﬁrst item.
lies in the closed interval deﬁned by F(x) and F(z). The ex- 4) MakeScaleRecord creates a scale record.
tremal pair has an extent [F(x), F(z)] or [F(z), F(x)] and
increasing or decreasing direction according to F(x) < F(z) LET extrema = empty_list
or F(x) > F(z). An extremal pair is maximal for X when  LET scales = empty_list
no other extremal pair in X has larger extent. Similarly, an Push( Pop(data),extrema)
extremal pair is a maximal increasing (decreasing) when no Push( Pop(data),extrema)
increasing (decreasing) extremal pair has larger extent. for next_extremum IN data do
                                                          while Length(extrema) > 1 AND {{sense.next_extremum =
  Intuitively, a maximal extremal pair forms the largest de- "maximum" AND value.next_extremum > value.extrema(2)}
creasing (increasing) segment inside a larger increasing (de- OR {sense.next_extremum = "minimum" AND
creasing) segment X.                                      value.next_extremum < value.extrema(2)}} do
                                                            Push(MakeScaleRecord( | value.extrema(1) -
Deﬁnition 6. We recursively deﬁne ordinary and special in-  value.extrema(2) |, MakeList( extrema(2), extrema(1) )),
tervals: D is an ordinary interval. When I is an ordinary   scales)
interval, then an interval J ⊂ I is special when J’s endpoints Pop(extrema)
constitute a maximal extremal pair in I; in this case J has Pop(extrema)
direction inherited directly from its endpoints. Recursively, if if Length( extrema ) = 0 then
J is special and interval J0 ( J has endpoints constituting a Push( Pop( extrema_list.scales(1) ), extrema )
maximal extremal pair in J of direction opposite to that of J, Push( next_extremum, extrema )
then J0 is special. Let J ,i = 1...n be all special intervals in while Length(extrema) > 1 do
                   i                                      Push( MakeScaleRecord (| value.extrema(1) -
an ordinary or special interval X. Choose any nonempty sub- value.extrema(2) |, MakeList( extrema(1) )), scales)
set of the collection {Ji}, and let S be the segmentation of X Pop(extrema)
deﬁned by the endpoints of the special intervals in the subset. Push ( extrema(1), extrema_list.scales(1) )
Then any segment I ∈ S that does not contain any of the Ji is RETURN scales
an ordinary interval.
  One can see that the special intervals are nested. The The principle of Algorithm 1 is as follows: the while loop
endpoints x,z of the special intervals have ∆(x) = ∆(z) = inside the for loop detects the special interval and labels the
|F(x)−F(z)|. We can call them “twins” due to the same ∆(x) both endpoints with the difference of their values. But if the
value. The ordinary intervals are not nested. The endpoints extrema list is empty after this labelling, which means that
of the ordinary intervals have different ∆(x). We call each of the top item in the list needs to be checked against the com-
them “singleton”.                                     ing data, this top item is popped back to extrema in the if fol-
  If there are several extrema having an equal value, the way lowing the above while. Then the next extremum is pushed
to choose the endpoints to constitute a maximal extremal pair into the extrema list for the next for loops. The while outside
is not unique. Therefore, there are different sets of special the for loop determines the δ for the ordinary intervals. The
intervals. They are equivalent. For simplicity, we do not con- last push states that the last extremum in the extrema list has
sider the case of equal valued extrema in Algorithm 1 and the same scale as the one just popped into scale. Actually the
Algorithm 2. Instead, we explain how to deal with it verbally endpoints of the biggest ordinary interval are always the last
after introducing the algorithms.                     two pushed into scale record list and their scales are identical.  If one desires no more than K segments, it is a simple mat- 2                 0.06
                                                                                0.05
ter of picking δ as small as possible so that you have no more 1.5
                                                                                0.04

than K + 1 signiﬁcant extrema (∆(x) ≥ δ) together with the 1
                                                                                0.03


ﬁrst and last extrema (indexes 0 and n − 1). Algorithm 2                        0.02
                                                           0.5

shows how once the labelling is complete, one can compute                       0.01

                                                           0
the segmentation in time O(nK logK) which we consider to                        0

                                                           −0.5                −0.01
be linear time when K is small compared to n. It also uses  0     5     10    15 0     5     10    15
a ﬁxed amount of memory (O(K)). The principle of Algo-
rithm 2 is to select the K + 1 data points to be the endpoints Figure 2: Input ﬂow of tank A (left) and level of tank B (right)
of K segments. Since the endpoints of D are by default, the with added white noise.
remaining endpoints come from the largest δ-extrema. The
                      δ
for is to select the largest -extrema. As in Algorithm 1, we 6 Scale-Based Flatness
know that the labels are either "twins" or "singleton". Thus
in the for loop, a maximum of K + 2 data points are chosen. For applications, it is important to be able to ﬁnd “ﬂat” seg-
Then, after the ﬁrst if outside for, the smallest δ-extrema are ments robustly in a data set. We propose the following deﬁ-
removed to reduce the total endpoints to at most K + 1. The nition:
rest of the code checks whether the ﬁrst and the last endpoints Deﬁnition 7. Given δ > 0, consider a δ-monotonic segment
                                 δ
of D are included. If not, the smallest -extrema will be re- in a δ-segmentation, an interval I in this segment is δ-ﬂat if
moved in favor of the ﬁrst and the last endpoints. The number the standard optimal monotonic approximating function (as
of segments can be less than K since several extrema can take deﬁned in Theorem 1) is constant over I.
the same δ value and can be removed together.
                                                        Because we can compute the standard optimal monotonic
                                                      approximating function in O(n) time, we can ﬁnd ﬂat seg-
Algorithm 2 Given a labelling algorithm (see Algorithm 1), ments in O(n) time.
this algorithm returns an optimal segmentation using at most The next proposition addresses the ﬂat segments in differ-
K segments. It is assumed that there are at least K +1 extrema ent δ-segmentations. The ﬁrst point is derived from the fact
to begin with.                                        that when the δ increases, the new segments are always the
  INPUT: an array d containing the values indexed from 0 to n − 1 result of mergers of the previous segments. Hence, the seg-
  INPUT: K a bound on the number of segments desired  ments at a small δ are always subsets of the segments at a
  OUTPUT:  segmentation points as per Theorem 4 and Proposi- larger δ. The second point says that the old ﬂat segments will
  tion 1                                              still be ﬂat in the merged segment.
   ←                    +
  L   empty array (capacity K 3)                                             δ                   0     δ0
  for e is index of an extremum in d having scale δ, e are visited in Proposition 2. Let S be a -segmentation and let S be a -
                                                                      0
  increasing order do                                 segmentation for δ > δ, then 1) any δ-segment X ∈ S is a
    insert (e,δ) in L so that L is sorted by scale in decreasing order subset of some δ0-segment X0 ∈ S0, 2) for any δ-ﬂat interval I
    (sort on δ) using binary search                   in X, I is also δ0-ﬂat in X0.
    if length(L) = K + 3 then
      pop last(L)
  if length(L) > K+1 then                             7   Experimental Results
    remove all elements of L having the scale of last(L). 7.1 Cascade Tank Sample Data
  if indexes {0,n − 1} 6⊂ L then
    if (index 0 ∈ L OR index n − 1 ∈ L) AND length(L) = K+1 As a source of synthetic data, we consider a system which
    then                                              consists of two cascade tanks A and B. Each tank has an in-
      remove all elements of L having the scale of last(L). put pipe (incoming water) and an output pipe (outgoing wa-
    if (index 0 6∈ L AND index n−1 6∈ L) AND length(L) ≥ K then ter). Tank A’s output pipe is the input pipe of tank B. In the
      remove all elements of L having the scale of last(L). equations below, let A and B be the level of water for the two
  RETURN:  the indexes in L adding 0 and/or n − 1 when not al- tanks respectively. The change of water level is proportional
  ready present                                       to the difference of the input ﬂow and the output ﬂow. As-
                                                      sume in is the input ﬂow of tank A. f (A) is the out ﬂow of
                                                      tank A. g(B) is the output ﬂow of tank B [Kuipers, 1994].
  Algorithms 1 and 2 assume that no two extrema can have Thus, we have
the same value. In case of equal valued extrema, we can mod-
ify these algorithms without increasing their complexity. In           A0 =   in − f (A)
Algorithm 1, we can generalize ScaleRecord so that entries in          B0 =   f (A) − g(B)
the extrema_list ﬁeld can also be lists of extrema of the same
value. Then in the while loop, we can put the extrema of the In principle, f (A) and g(B) are increasing functions. We
same value into the corresponding list. For Algorithm 2, we assume f (A) = k1A and g(B) = k2B. If we variate the input
just need to remember that whenever removing a middle point ﬂow of tank A, in, we can control the level of tank B. In this
between two equal valued extrema, the two equal valued ex- way, we generated some sample data and added noise to it.
trema have to be treated as one extrema to avoid having two See Fig. 2 for the the input ﬂow, in, of tank A at the left and
adjacent minima or maxima.                            the level of tank B at the right.