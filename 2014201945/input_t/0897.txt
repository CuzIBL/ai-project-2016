                              Learning with Labeled Sessions 

                                     Rong Jin*, Huan Liu† 
   *Dept. of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824  
                                     rongjin@cse.msu.edu 
 † Department of Computer Science and Engineering, Arizona State University, Tempe, AZ85287-8809
                                         hliu@asu.edu 

                   Abstract                      user may fall into sleep for a short period of time. More 
                                                 information can be found from the website 
    Traditional supervised learning deals with labeled 
                                                 http://www.cs.utexas.edu/users/sherstov/pdmc/. 
    instances. In many applications such as physiologi- Another application is speaker identification. To deter-
    cal data modeling and speaker identification, how- mine the speaker for a speech sample that is a minute long, a 
    ever, training examples are often labeled objects 
                                                 typical strategy is to first divide the long speech sample into 
    and each of the labeled objects consists of multiple a number of short ones. Then, a standard classification 
    unlabeled instances. When classifying a new ob- model, such as Gaussian Mixture Model (GMM), is applied 
    ject, its class is determined by the majority of its 
                                                 to determine the speaker identity for each short sample. Fi-
    instance classes. As a consequence of this decision nally, the dominant speaker that is classified for short sam-
    rule, one challenge to learning with labeled objects ples will be used as the predicted speaker for the long sam-
    (or sessions) is to determine during training which 
                                                 ple. Compared with the strategy that extracts a single set of 
    subset of the instances inside an object should be- features for the whole long speech sample, this majority 
    long to the class of the object. We call this type of vote approach is usually more robust and accurate [Rey-
    learning ‘session-based learning’ to distinguish it 
                                                 nolds, 1995]. This is because features extracted from a long 
    from the traditional supervised learning. In this pa- speech sample may include significant amounts of back-
    per, we introduce session-based learning problems, ground noise, while the noise can be reduced substantially 
    give a formal description of session-based learning 
                                                 when a long speech sample is divided into short ones.  
    in the context of related work, and propose an ap- The common characteristics of the above two applica-
    proach that is particularly designed for session- tions are that (1) training examples are labeled objects (e.g., 
    based learning. Empirical studies with UCI datasets 
                                                 sessions of physiological records in physiological data mod-
    and real-world data show that the proposed ap- eling, and a long speech sample in speaker identification); 
    proach is effective for session-based learning. (2) every object consists of multiple instances that are not 
                                                 labeled; and (3) in predicting an object’s class, it is deter-
 1 Introduction                                  mined by the class that is assigned to the majority of its in-
 A typical supervised learning problem is to learn a model stances. To distinguish this new type of learning from the 
 from training examples that are usually labeled instances. traditional supervised learning, we call it ‘session-based 
 But for many applications, training examples are labeled learning’. 
objects and each labeled object consists of multiple unla- The challenge of session-based learning arises from the 
beled instances. During classification, an object is labeled as majority vote strategy that is used to determine the label of 
class ‘a’ if the majority of its instances are classified as ‘a’.  an object. This decision rule makes it ambiguous, during 
    One application is physiological data modeling, whose training, as to which subset of the instances inside an object 
goal is to predict context activities of individual users based should belong to the class of the object – we name it the 
on their physiological data. Typically, physiological signals label ambiguity problem. A straightforward strategy to-
are measured and recorded continuously for every other ward this problem is to treat every unlabeled instance within 
second. Continuous physiological signals are then divided a labeled object as a positive example for the class of the 
into a number of sessions. Each session is of minutes long object. In physiological data modeling, every physiological 
and consists of hundreds of records of physiological data. record within a labeled session is treated as a training in-
To predict the user’s activities for a session, prediction is stance for the activity assigned to the session. In speaker 
first made for each record, and the dominant activity pre- identification, every short sample within a long speech is 
dicted for records in the session is then used as the activity used as a positive training instance for the speaker of the 
for the session. Although each session is labeled as a single long speech. We call this simple strategy the ‘naive ap-
activity, a user may perform activities other than the labeled proach’. 
one. For example, during a session of ‘watching TV’, the                                                  lems where K (K > 2) is the number of classes. Let co() be 
                    A                            an object-based classification function, which takes an ob-
                                                 ject o as input and outputs its class label. In general, any 
                                                 supervised learning problem can be formulated as an opti-
                                                 mization problem: 
                     D 
                                                          *          n
                                                         cLcoy= arg min∑ () (ii ),       (1) 
                                                               c     i=1
           B                  C                  where L is a loss function that determines the amount of 

                                                 punishment when prediction co()i  is different from yi . 
   Figure 1: A toy example of session-based learning   For a traditional supervised learning problem, each object 
   problem with four classes ‘A’, ‘B’, ‘C’, and ‘D’. only contains a single instance. As a result, training exam-
  One obvious problem with the naïve approach is that in- ples can be simplified as Dyy= {}(xx11 , ),( 2 , 2 ),...,( xnn , y ) , 
stances within a single object often belong to multiple dif- where each instance x ∈ℜd  is a vector in a d dimension 
ferent classes, not just to the class of the object. Thus, by     i
treating every instance within an object as a positive exam- space. Furthermore, the object-based classification function 
ple for the class of the object, we likely introduce training co() becomes an instance-based classification function 
examples with noisy labels, which, as a result, degrades the f ():xYℜ→d . Thus, a traditional supervised learning 
quality of classification models. To further illustrate the 
problem with the naïve approach, let us consider a toy learn- problem is usually formulated as follows: 
ing problem in Figure 1. It has four classes, with class ‘A’, *      n
                                                         fLfy= arg min∑ () (xii ),       (1a)
‘B’, and ‘C’ centering on the vertices of a triangle, and the  f     i=1
fourth class ‘D’ sitting on the center of the triangle. In tradi- In session-based learning, each object consists of multiple 
tional supervised learning, training examples are labeled unlabeled instances. Let the training data be denoted by 
instances. Since these four classes are well separated, we Dyy= (XX , ),( , ),...,( X , y ) , where each object 
would expect that a simple Naïve Bayes model should work { 11 2 2      nn}
fine for this problem. But, for a session-based learning prob- mi
                                                 Xx=         contains mi different instances. Given an 
lem, training examples are labeled objects that consist of iij{ , } j=1
instances from different classes. In particular, consider the instance-based classification function f ():xYℜ→d , the 
case when training objects for classes ‘A’, ‘B’, and ‘C’ are 
mixtures of instances from these three classes, while train- decision rule for session-based learning is that, an object Xi  
ing objects for class ‘D’ only contain instances from ‘D’. is labeled as class ‘a’ if the majority of its instances are 
With appropriate mixtures, the input means of instances classified as ‘a’. Thus, in session-based learning, the object-
from labeled objects for the four classes can stay close to based classification function cs can be written into the fol-
each other, which makes it hard for the Naïve Bayes method lowing form of the instance-based classification f ()x : 
to learn if assigning each instance with its object class.             m
  In the following, we first give a formal description of ses- cf(;)argmaxXx= i δ f ( )= y 
                                                      si∑              j 1 () ij,         (2) 
sion-based learning, and elaborate on the differences be-       y∈Y     =
tween this new type of learning problem and other related where δ (⋅)  is a delta function that outputs 1 when the input 
learning problems, such as multiple-instance learning. Then, 
we present a novel approach that is particularly designed for is positive and zero otherwise.  
session-based learning. The key idea is to develop an inno-   Compared to traditional supervised learning, the chal-
vative way that handles the label ambiguity problem. Fi- lenge of session-based learning is due to the label ambiguity 
nally, we demonstrate the effectiveness of the proposed ap- problem. Although each training object is provided with a 
proach for session-based learning using both UCI datasets class label, the label information of instances within objects 
and data from physiological data modeling.       is not given. Hence, it is difficult to learn an instance-based 
                                                 classification function from labeled objects. In the naïve 
 2  Formal Description of Session-based          approach, each instance within an object is treated as a posi-
                                                 tive example for the class of the object. As a result, a ses-
    Learning                                     sion-based learning problem is simplified as a traditional 
 Let training examples be denoted by supervised learning problem: 
 Doyoy= ( , ),( , ),...,( oy , ) , where each (,oy ) is a          n mi
    {}11    2  2    nn                ii                 *
                                                        fLfy= arg min     (x  ),         (1b)
labeled object and y ∈ Y  is the class label assigned to ob-      ∑∑    ()ij,   i
                i                                             f   ij==11
ject oi . Domain Y is {1,1}−  for binary-class classification   The most related work to session-based learning is multi-
problems, and [1..K ]  for multiple-class classification prob- ple-instance learning[Dietterich, et al.,1997]. Similar to ses-
                                                 sion-based learning, in multiple-instance learning, class la- bels are assigned to objects that consist of multiple in-
 stances, which are called “bags” in multiple-instance learn- Trad. Super- Multiple-instance Session-based 
 ing. In the past, there have been many studies on multiple- vised Learning Learning Learning 
 instance learning, including the approach of learning axis- 0.1 0.2 0.3 -1  0.1 0.2 0.3  0.1 0.2 0.3
 parallel rectangles [Dietterich, et al.,1997], the diverse den- 0.2 0.5 0.4 -1  0.2 0.5 0.4 -1 0.2 0.5 0.4 -1 
 sity algorithm [Maron and Lozano-Pérez,1998], the ap- 0.6 0.2 0.1 -1  0.6 0.2 0.1 0.6 0.2 0.1
proaches based on support vector machines [Andrews, et …… ……                       …… 
al.,2002, Tao, et al.,2004], the nearest neighbor approach 0.3 0.7 0.6 -1  0.3 0.7 0.6  0.3 0.7 0.6
[Amar et.al. 2001; Wang and Zucker, 2000], and the boost- 0.7 0.2 0.2 -1  0.7 0.2 0.2 +1 0.7 0.2 0.2 -1 
ing approach [Andrews and Hofmann,2003].           0.8 0.1 0.0 +1  0.8 0.1 0.0 0.8 0.1 0.0
  Multiple-instance learning differs from session-based …… ……                      …… 
learning in its decision rule. In multiple-instance learning, 0.5 0.7 0.1 +1  0.5 0.7 0.1  0.5 0.7 0.1

an object Xi  is labeled as positive class when at least one of 0.1 0.6 0.9 -1  0.1 0.6 0.9 +1 0.1 0.6 0.9 +1 
 its instances is classified as positive. A negative class is 0.7 0.1 0.2 +1  0.7 0.1 0.2 0.7 0.1 0.2
 assigned to an object when all of its instances are classified Figure 2: Training examples for traditional super-
 as negative. Thus, given the instance-based classification vised learning, multiple-instance learning, and ses-
                                                   sion-based leaning. 
 function f ():xYℜ→d , the object-based classification 
 function c for multiple-instance learning is written as: rors, which has been demonstrated to be effective in the 
        m                                        AdaBoost algorithm [Freund and Schapire, 1997]. 
             +∃∈11...,()1jmf[ iij] x , =+        In particular, in designing objective functions, two types 
    cfmi(;)X =                          (2’)    of errors are considered: instance-based errors and session-
              −∀∈11...,()1imf[]iijx , =−
                                               based errors. An instance-based error is a prediction mistake 
Examples for the three different types of learning are shown made for an instance, and a session-based error is a predic-
in Figure 2 for comparison. The first column shows tradi- tion mistake made for a session. One key difference be-
tional supervised learning with instances and their labels. tween session-based learning and traditional supervised 
The columns for Multiple Instance Learning and Session- learning is that the former is concerned with both session-
based Learning show how these two different strategies ag- based errors and instance-based errors while the latter con-
gregate instances and assign labels to the aggregates. cerns with only instance-based errors. To include both types 
  Session-based learning is more challenging than multiple- of errors, given an instance-based classification function 
                                                 H ()x , we define the loss function as: 
instance learning in the following sense: in multiple-
instance learning, when an object is labeled as ‘negative’, all LcX((ii ; H ), y )=
                                                             mm
of its instances will belong to the negative class. Thus, for γ y ii             
                                                       i                              (3) 
multiple-instance learning, there is no label ambiguity for exp−−∑∑H (xHxyij,, ) exp() ( ij ) i 
                                                       mi                        
negatively labeled objects. In contrast, the label ambiguity  jj==11
 
 
problem exists for session-based learning regardless of the Session-based error Instance-based error
sign of labels. For instance, in Figure 2, for session-based In the above expression, both session-based and instance-
learning, both the first and second objects are labeled as based errors are approximated by an exponential function. 
‘negative’. However, the labels of instances in these two The loss function is defined as the product of these two er-
objects are different. The two learning schemes also differ rors. In other words, a misclassified instance is important 
in degrees of difficulty when deciding positive classes for only when its related session is also misclassified. Constant 
objects: multiple-instance learning adopts the “at-least one” γ  in (3) determines the relative importance between the 
strategy and session based learning uses the majority one.  session-based error and the instance-based error. In experi-
                                                 ment, a cross validation with 20/80 split of training data is 
 3  SBoost – An Algorithm for Session-based      used to determine appropriate values for γ . In the follow-
                                                 ing, we will discuss how to efficiently find H ()x  that 
    Learning                                     minimizes the loss function in (3). 
  In this session, we will present a boosting-based algo-
rithm for session-based learning problems of binary classes.  3.1  Boosting-based Optimization Algorithm 
The key for designing a learning algorithm for session- With the loss function defined in (3), our goal is then to 
based learning is to define a simple yet effective loss func- search for optimal H(x) that minimizes the overall cost for 
                                                 the training data, i.e., 
tion   Lc()(;),Xii f y . A simple choice is 
                                                                       n
                                                  Herr* ==argmin argmin  LcHy (X ; ),
 Lc(;),XX f y=≠δ c (;) f y . However, this choice                    ∑i=1 ()ii
  ()(ii            i     i )                            HH
will lead to a non-smooth objective function, which is usu- n      mm                       (4)
                                                              γ y ii                 
ally difficult for optimization. Hence, we choose to use the =−arg min expi H (xHxy ) exp − ( )
                                                         ∑∑∑m         ij,,()        ij i
exponential loss function to approximate classification er- H ijj===111i                                                  mi       d
  Given: (X1, y1), …, (X m, ym) where Xx=∈ℜ∈−,  x , y 1,1 ; Weighting constant γ  
                               iij{},,j=1 ij     i{}

  Initialize the weight distribution Dij(, )= 1 n m , Hx()= 0 
                            0       ()∑i=1 i  0
  For t = 1,…,T 
     1. Sample training instances xx,..., ,..., x ,..., x according to D  
                              {}1,1 1,mn1  ,1   nm , n          t−1
                                  d
     2.  Train a weak classifier ht ():x ℜ→− {1,1} on sampled examples 

                       y γ   mi                m
     3. Compute gH=−expi            (x  ) , aHy=−i exp     (x )  , and 
                  itij∑      j=1 −1,      itiji∑ j=1 ( −1,    )
                       mi
             mmiiyaγ
         byh=−+()expxx    H  ()  yii        h () x for each session. 
          i∑∑   i  ij,1,() t−  ij im           ij ,
             jj==11i
                        n
                        ∑ giii[](1++γ )ab
                  1     
     4. Let α =       ln i=1              
             t          n
                2(1+ γ ) 
                         ∑ giii[](1+−γ )ab
                        i=1

                                         gitijiii(exp(−+Hyam−1, (x ) ) γ )
     5.  Update the weight distribution Dijt (), =                , where Zt  is a normaliza-
                                                     Zt

         tion factor (chosen so that Dt +1  is sum to 1). 

     6.  Update the classifier HHtt()xxx=+−1 ()α tt h () 

                                        T
  Output the final hypothesis: FTtt(xx )= arg max∑t=1α hy ( )  
                                 y∈−{}1,1

                           Figure 3: Description of the SBoost algorithm.
  An efficient approach for optimizing Eq. (4) is to divide it 
                                                             mi                  
into a series of simple learning problems that do not have     γ yi                
                                                           exp−+Hh   (xxij,, )α ( ij )
the label ambiguity problem and thus can be resolved by  n   m  ∑               
                                                             i  j=1              
traditional supervised learning techniques. We solve the err =                          (5) 
                                                        ∑    m
label ambiguity problem by maintaining weights for differ- i=1  i                  
                                                                   
ent instances such that only instances with large weights are ×−∑exp()Hx (ij,, ) +α hx ( ij ) y i 
                                                                                   
assigned to the class of their objects and used for training. j=1
Since boosting [Freund and Schapire, 1997] is a learning In the above expression, for the convenience of presenta-
algorithm that uses weighted instances to efficiently update tion, we drop the index for H ()x , h ()x , and α . 
classification functions, we design a boosting-based learning            T      T +1       T +1
algorithm for learning with labeled sessions below.   Using the convexity of an exponential function, we have  

  Let ht ()x  be the ‘weak’ classifier of the t-th iteration that m
                                                      n g  i
is learned using traditional supervised learning techniques. i    
                                                 err≤−+−∑∑exp()αγ h (xxij,, ) h ( ik ) y i H ( x ij , ) y i (6)
                                                        mi
The combined classifier HT ()x  for the first T iterations is ikj==1,1
          T
 Hh()xx=   α   (), where α  is the combination con-          y γ  mi
  Ttt∑    t=1            t                       where gH≡−expi         (x ) . Then, using inequal-
                                                       iij∑        j=1  ,
 stant for the t-th iteration. Our goal is to find another ‘weak’ mi
 classifier h ()x  and a constant α  such that the new   11+ xx−
         T +1               T +1                 ity e,αααx ≤+eex−       ∀∈−[1,1], we can further 
 combined classifier HHTTTT+++111()xx=+ ()α h () x will   22
 effectively minimize the function in (4). Given classi- upper bound (6) by the following expression: 

 fier HT +1()x , the objective function in (4) is rewritten as:                                                      Data Set     # Examples    # Features 
err≤ errupper
                                                       spam 4600                   58 
  eeαγ(1+−+ )+ αγ (1 ) n
=                ag                                    cmc 1470                    10 
              ∑   ii                                  german 680                   24 
       2       i=1
                                           (7)    Table 1: Statistics of UCI datasets used for synthesized data
                          
               n m         giijiexp()−Hy (x , )
  eeαγ(1+−+ )− αγ (1 ) i   
−                  yh()x
              ∑∑    iij,  γ  ai                                Positive Class  Negative Class 
     2(1+γ )  ij==11      +g i
                          mi                       # Instances     4343 3,6495 
                                                     # Objects       55 186 
              mi                                  # Avg of instances 
 where  aHy≡−exp       (x )  . Define weight                       69.3455 199.0538 
         iiji∑ j=1 (),                               per object 
                                                        Table 2: Statistics for the physiological data 
 Dij(),exp()≡− giijiii()() Hx , y +γ a m  and rewrite the 
second term in (7) as:                           sion-based learning with both a Decision Tree and 

      αγ(1+−+ ) αγ (1 ) n mi                     AdaBoost using a Decision Tree as its base classifier. 
      ee−                                          Two types of data are used in experiments to evaluate the 
                  ∑∑yhiij()(,)x , Di j    (8) 
         2(1+γ )   ij==11                        effectiveness of SBoost:  
Clearly, to minimize the objective err in (7), we need to 1) Synthesized data that are generated from binary UCI 
 maximize the above expression (8). The best case is that the datasets [Blake and Merz,1998] by combining multiple in-
 output of weak classifier h()x  is consistent with yi, for all stances into objects. Each object consists of ten different 
instances in an object. When this is not the case, the label instances. To create an object for the positive class, a ran-
ambiguity problem is then resolved based on weight dom number between 1 and 5 is first generated, and the cor-
 Di(, j ). In particular, instances with large weights are as- responding number of instances from the negative class are 
                                                 randomly chosen and added to the object. The rest of the 
 signed with the class of their objects, while the labels for object is filled out with instances randomly selected from 
 instances with small weights remain unlabeled. This is be- the positive class. A similar procedure is applied to generate 
 cause the contribution of an instance to (8) is mainly deter- objects for the negative class. By doing so, we guarantee 
 mined by its weight Di(, j ). When an instance has a small that the class of an object is consistent with the dominant 
weight, its contribution to (8) will be ignorable. Further- class assigned to its instances. The details of UCI datasets 
more, notice that Di(, j ) is proportional to g , which is used in this experiment are listed in Table 1. 
                                     i           2) Physiological data that come from the workshop of 
 related to session-based error. Thus, a misclassified instance physiological data modeling at the ICML 2004. We use the 
 will not be assigned with a large weight if the related ses- dataset for ‘watching TV’ with code 3004. In the original 
 sion error is small. Finally, the combination constant α  can problem, the number of instances for the negative class is 
 be obtained by setting the derivative of the upper bound in overwhelmingly larger than that for the positive class. Since 
 (7) w.r.t. to α  to be zero. That is,           we focus on the study of session-based learning, we inten-
                 n                             tionally reduce the effect of rare class by randomly selecting 
                                                 parts of negative instances. The resulting data set has 241 
                 ∑ giii[](1++γ )ab
            1    sessions with 40,838 instances. The details of this dataset 
      α =      ln  i=1                   (9) 
                 n                             are listed in Table 2.  
         2(1+γ ) 
                 ∑ giii[](1+−γ )ab               A decision tree [Quinlan,1993] is used as the baseline 
                 i=1                           classifier throughout the experiments. The session-based 
where                                            classification error, i.e., the percentage of sessions that are 
                                                 misclassified, is used for evaluation. 50% of data are ran-
       mmyaγ
 byhxHy≡−+i ()exp     ()xxii          i h ().    domly selected for training and the rest is used for testing. 
  iiij∑∑jj==11,,()     iji               ij ,
                               mi                The same experiment is repeated 10 times, and the average 
For later reference, we name this algorithm SBoost for ‘ses- session-based classification errors are reported. Finally, for 
sion-based boosting’. The details are given in Figure 3.  both SBoost and AdaBoost, the maximum number of itera-
                                                 tions is set to be 30. 
 4   Experiments                                 4.1 Results 
 The goal of this section is to examine the effectiveness of The results of three methods (SBoost, the naïve approach 
SBoost for session-based learning. We compare SBoost using a single decision tree, and with AdaBoost) are shown 
with the simple naïve approach that treats each instance in Table 3. First, we compare the performance of AdaBoost 
within an object as a positive example for the class of the with that of a decision tree as both adopting the naïve ap-
object. In particular, the naïve approach is applied to ses- proach to session based learning. We observe that AdaBoost 