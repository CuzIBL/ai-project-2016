         Choosing between heuristics and strategies: an enhanced model for
                                         decision-making       ∗

                              Shavit Talman, Rotem Toister, Sarit Kraus
                         Department of Computer Science, Bar-Ilan University
                                        Ramat-Gan,52900, Israel
                                  {toistet, talmans, sarit}@cs.biu.ac.il

                    Abstract                          an optimal amount of information units, through which the
                                                      best alternative could be determined.
    Often an agent that has to solve a problem must
    choose which heuristic or strategy will help it the
    most in achieving its objectives. Sometimes the     In this paper, we generalize their model to suit domains that
    agent wishes to obtain additional units of informa- involve choosing between heuristics or strategies. Azoulay-
    tion on the possible heuristics and strategies in or- Schwartz and Kraus’ assumptions hold in this research as
    der to choose between them, but it may be costly. well: the agent chooses the best alternative in advance and
    As a result, the agent’s goal is to acquire enough is able to hold onto its decision for a long period of time.
    units of information in order to make a decision  This approach is valid in many situations, especially when ac-
    while incurring minimal cost. We focus on situa-  quiring information is costly. In these cases, the agent would
    tions where the agent must decide in advance how  like to execute its decision-making process once and apply
    many units it would like to obtain. We present an the process’s output thereafter. Moreover, once the agent re-
    algorithm for choosing between two options, and   alizes the decision made does not produce satisfactory results
    then formulate three methods for the general case at any given time, it may re-execute the decision-making pro-
    where there are k > 2 options to choose from. We  cess, and proceed with its new output. Nevertheless, we are
    investigate the 2-option algorithm and the general more concerned with the applicative rather than the theoreti-
    k-option methods effectiveness in two domains: the cal aspect of the model. Indeed, the model suggests many im-
    3-SAT domain, and the CT computer game. In both   plementation challenges. We present two examples, from the
    domains we present the experimental performance   heuristics domain and from the computer games domain. In
    of our models. Results will show that applying the both domains we compare the utility of the automated agent
    2-option algorithm is beneﬁcial and provides the  with and without using our algorithm. Results will show that
    agent a substantial gain. In addition, applying the an agent that applies our model is more likely to choose the
    k-option method in the domains investigated results best option among the group of possible options. Further-
    in a moderate gain.                               more, it does not waste too many resources during the deci-
                                                      sion process, and thus, its overall utility increases.
1  Introduction
                                                        The paper ﬁrst brieﬂy presents the model constructed by
When making decisions, automated agents need to decide Azoulay-Schwartz and Kraus. It then introduces the algo-
which heuristic function or strategy would best assist them rithm for choosing between two options, which was formu-
in achieving their objectives. In particular, when an agent lated by Azoulay-Schwartz and Kraus, and thereafter the
possesses several alternatives to tackle a problem, and the three generalized methods for cases where there are more
best alternative is unknown in advance, its success depends than two options. The ﬁrst of these three methods is a the-
on choosing the most beneﬁcial alternative. In [Azoulay- oretical one which was formulated by Azoulay-Schwartz and
Schwartz and Kraus, 2002] a formal model for solving a prob- Kraus. We generalized their ideas to form the other two meth-
lem of choosing between alternatives in e-commerce have ods, which are much more feasible in real-time environments.
been presented. They formulated the problem in terms of We proceed by describing the experimental design and anal-
units of information and agent’s utility, and assumed that the ysis. In section 4 we describe the ﬁrst domain we considered
agent should decide in advance on how many information - the 3-SAT domain, and in the following section we describe
units to obtain about each alternative. Moreover, the agent the other domain, the CT game domain. In both sections we
could not change its decision during the information obtain- present the experiments we conducted and the outcomes of
ing process. Then they suggested an algorithm for acquiring applying the 2-option algorithm and the k-option generalized
  ∗This work was supported in part by NSF grant no. IIS-0208608 models. In section 6 we compare our work with previous re-
and by ISF grant no. 1211-04. Sarit Kraus is also afﬁliated with searches. The concluding section discusses the experimental
UMIACS.                                               results and suggests several avenues for future research.2  Model construction                                 is F change(µA, µB, σA, σB, xA, xB, nA, nB, mA, mB) =
A risk neutral agent has to choose a heuristic from k heuristics P r(Z > Zα), where Z is a random variable, having the
to solve M problems. After choosing alternative i, the agent standard normal distribution and Zα represents the ﬁrst value
                                                      where B outperforms A (see [Azoulay-Schwartz and Kraus,
will obtain a value of xi which is unknown in advance. We as-
                                                2     2002] for more information). Then, the expected bene-
sume that xi is normally distributed, that is, xi ∼ N(µi, σ ).
                                                i     ﬁts from obtaining additional m and m is a function of
In addition, the agent does not know µi, but it has some prior                    A      B
                                                      Fchange and is denoted by benefits(m , m ). The agent’s
beliefs about its distribution. We further assume that µi also                  R  R   A    B
                                      2               utility is utility(mA, mB) =     benefits(mA, mB) ·
follows the normal distribution - µi ∼ N(ζi, τi ). We assume                     µA µB
                                                       mA+mB
that σi, ζi and τi are known and reﬂect the agent’s believes. δ −cost(mA, mB)dµBdµA. The speciﬁcs of the ben-
Otherwise, if σi is unknown, the agent can use The student eﬁts function depend on the cost model. Later we will show
distribution. In addition, if ζi and τi are unknown, we assume the speciﬁcations for three different cost models. The agent’s
that the agent can estimate their values based on its beliefs as goal is to ﬁnd the pair mA and mB, that yields the highest
to the possible values of µi. Furthermore, in future work we utility(mA, mB) value. By considering all possible combi-
will investigate the effect of eliminating normal distribution nations of information units about A and B, the optimal com-
assumptions.                                          bination is achieved. Algorithm 1 presents the Heuristics-
  The agent has ni units of information about each alter- Strategies-Choosing (HSC) Algorithm for the two heuristics
native i, with an average value of xi. For instance, in the (or strategies) case.
e-commerce example presented by Azoulay-Schwartz and
                                                      Proposition 1. The HSC algorithm stops and returns mA and
Kraus, a customer would like to buy an item available from
                                                      mB  that maximize the utility function.
two suppliers. The customer collected n1 and n2 customer-
impressions from friends or from the web about these suppli- Proof. The intuition for the correctness of the algorithm lies
ers, in order to be able to decide between them. The average in the form of the utility function. Since it consists of the mul-
impression was calculated to form x1 and x2.          tiplication of similar normal distribution functions, its form is
  The agent is able to obtain a combination of comb = Gaussian as well, with only one maximum point. As soon
(m1, . . . , mk) additional units about the different alterna- as the algorithm ﬁnds that point it stops and returns mA and
tives. However, this operation is costly, either in time or in mB.
direct costs. Given 0 < δ ≤ 1, the discount time factor,
the cost model, the list of alternatives and the parameters for 2.2 Choosing Between Multiple Heuristics
each alternative, the agent decides whether to proceed with There are k > 2 alternatives, and the agent can obtain up
the information it has so far, or to accumulate additional in- to M-1 units of information about each of them. We suggest
formation units. One could simply use a greedy algorithm in three models:
order to ﬁnd the optimal allocation of additional experiments. 1. The Statistical model in which we considered the agent
In each step the option which proved to be better so far is cho- utility function given a combination of information units
sen, executed and its result is added to the data accumulated. as suggested by [Azoulay-Schwartz and Kraus, 2002].
The greedy algorithm stops when there is no additional sam- Nevertheless, in order to ﬁnd a combination of ad-
ple that increases the expected utility. However, [Azoulay- ditional units for each alternative, one must solve a
Schwartz and Kraus, 2002] showed that the greedy algorithm quadruple integral. As a result, the model proved to be
is not optimal. The reason being that there may be situations inapplicable for our purposes;
where obtaining one unit of information about a particular al-
ternative is not worthwhile since it will not lead to a change 2. The Binary Tree model where the alternatives construct
in the decision, but obtaining two and more may be worth- the leaves’ level of the tree. We apply the HSC algo-
while. In conclusion, ﬁrst we will present an algorithm for rithm for each pair. The winning alternative goes up a
k=2 heuristics (or strategies) and then generalize it for k > 2. level in the tree. We repeat the procedure until the best
                                                          alternative reaches the root. This model deviates from
                                                          our initial assumption that all experiments are conducted
Algorithm 1 The HSC Algorithm
                                                          prior to decision-making. The binary tree model is an in-
for mA from 0 to ∞ do
   for mB from 0 to ∞ do                                  termediate approach between the greedy algorithm and
     calculate Fchange.                                   the HSC algorithm. However, since the comparisons are
     calculate Utility.                                   done in pairs, a large number of experiments may be ex-
     if Utility(mA − 1, mB − 1) > Utility(mA, mB ) and
     Utility(mA − 1, mB − 1) > Utility(mA − 2, mB − 2) then ecuted for alternatives that would not have been consid-
        return (mA − 1, mB − 1).                          ered at all when regarding all the alternatives together;
Add mi experiments to alternative i.
Choose the best alternative according to the new results 3. The Fixed number of experiments model - in which we
                                                          distribute a ﬁxed number of experiments denoted by (N)
                                                          between the different alternatives using the information
2.1  Choosing Between Two Heuristics                      we have so far. For example, suppose we have ﬁve alter-
Suppose the agent has to decide between alternatives A and natives to choose from. Alternative 1 produces the best
B. Currently, xA > xB. Since the agent is risk neutral, al- results and alternative 5 produces the worst ones. Thus,
ternative A will be chosen if no additional information is ob- it is worthwhile to execute more experiments of alterna-
tained. Firstly, the probability of changing the winning option tive 1 than of alternative 5, as less time will be wasted    on fewer alternatives while accumulating information on two possible heuristics, we assumed that the better heuris-
    all possible alternatives. Following preliminary experi- tic after nA experiments is heuristic A. As a result, the agent
    ments, we set N to be four times the number of alterna- would have chosen heuristic A for the M formulas. In that
    tives. Thus, it is large enough to accommodate exper- case, it would have taken the agent the average number of
    iments of all alternatives and nonetheless, economical ﬂips multiplied by the number of formulas, M · µA, to solve
    in the number of additional experiments. Consequently, M formulas. After the HSC algorithm is executed, the total
    for the example of ﬁve alternatives N=20, and we sug- number of ﬂips will be assembled from,
    gest that the best alternative be assigned one third of N.
                                                        1. The number of ﬂips in the mA + mB extra experiments
    The rest will be assigned two thirds of the remaining yielded by the HSC algorithm;
    experiments: m = [20 · 1 ] = 7; m = [13 · 2 ] = 5;
                 1        3        2        3           2. The number of ﬂips to solve the remaining M − m −
    m  = [8· 2 ] = 3; m = [3· 2 ] = 2 and m = [1· 2 ] = 1.                                           A
      3     3       4      3          5     3             m   formulas in case heuristic B prevails;
    This ad-hoc approach will be reﬁned in future work.     B
                                                        3. The number of ﬂips to solve the remaining formulas in
3  Experimental design and analysis                       case heuristic A still leads.
                                                      δ  in this case is 1, since the agent solves the re-
Our investigation using the HSC algorithm was conducted
                                                      quired formulas in the additional experiments.  Ac-
in two domains. The ﬁrst, a classic NP-complete problem,
                                                      cordingly, the  utility function in this domain  is
i.e. the 3-SAT problem. To demonstrate the HSC algorithm’s                    R   R
                                                      utility(mA, mB)    =            benefits(mA, mB)  ·
vast usage possibilities, it was employed in two different cost                µA  µB   R   R
                                                       mA+mB
                                                      δ        − cost(mA, mB)dµBdµA   =         M  · µA −
models:                                                                                  µA  µB
 1. Minimal Time (MT) scenario - the agent had to solve M F change(M −mA −mB)·µB −(1−F changeR R )(M −mA −
                                                      mB)·µA−(mAµA+mBµB)dµBdµA        =        (M−mA−
    formulas as quickly as possible;                                                     µA  µB
                                                      m  )·F change·(µ  −µ   )−m   ·(µ  −µ  )dµ  dµ . The
 2. Maximal Formulas (MF) scenario - the agent had T units B           A   B     B    B    A   B   A
                                                      agent searches for m and m that maximize this equation.
    of time to solve as many formulas as possible.                     A       B
                                                        The MF scenario in the domain is a maximum problem: the
  We chose three best-known search algorithms as the  agent must solve as many formulas as possible within T ﬂips.
agent’s possible heuristics: Greedy-SAT (GSAT), Simulated Considering heuristic A is the better heuristic after nA exper-
Annealing (SA) and GSAT with Random Walk. The agent   iments, the agent would have solved T/µA 3-SAT formulas.
had to perform its task by using one of these search algo- However, if it would have used the HSC algorithm, the total
rithms. The second domain was a computer game, the CT number of 3-SAT formulas would have been the summation
game (for game speciﬁcations see [Grosz et al., 2004]). Here, of:
the agent’s task was to maximize its game score. To this end, 1. The number of formulas solved during the extra experi-
our agent had seven different strategies to employ against its ments yielded by the HSC algorithm, m + m ;
opponent agent in the game, and it had to choose the best                                   A    B
strategy against the opponent. The experiments compared 2. The number of formulas to be solved using heuristic B,
the agent’s achievements without using the HSC algorithm  if it prevails, F change · [T/µB];
and its achievements after executing this algorithm. We also 3. The number of formulas to be solved if heuristic A is not
compared these results with methods where a large set of ad- changed, (1 − F change) · [T/µA].
ditional experiments had been conducted. ([Selman et al.,
                                                      Here, δ is 1 as well, and cost(mA, mB) consists of the
1993] for instance conducted an unlimited number of exper-
                                                      number of ﬂips lost in each case, namely cost(mA, mB) =
iments in order to choose the best heuristic). These methods
                                                      F change · [(mAµA +  mBµB)/µB] + (1   − F change) ·
found the best heuristic more frequently than our algorithm.
                                                      [(mAµA  + mBµB)/µA]. Lastly, the HSC algorithm  will
Nevertheless, though the decisions made were slightly im-
                                                      RyieldR mA and mB that will maximize utility(mA, mB) =
proved, given our cost model, the approach applied by Sel-                           T
                                                             mA  +  mB  + F change ·   + (1 −  F change) ·
man yielded a huge loss in the number of experiments exe- µA µB                     µB
                                                       T  −  F change · mAµA+mB  µB  −  (1 −  F change) ·
cuted. For space reasons, these results are not detailed here. µA           RµB R
                                                      mAµA+mB  µB                        µB mB
  We ﬁrst executed preliminary runs to prepare an ofﬂine          dµBdµA  =        mB  −      +F change·
                                                           µA                µA µB        µA
database. The database comprised vital information, namely, T −µAmA T −µB mB
                                                      [        −         +  mA − mB]dµBdµA.
which option is the best, and moreover, the mean quality  µB        µA
of each option. Our hypothesis was that the execution of 4.1 Implementation issues
the HSC algorithm will indeed beneﬁt the agent. Moreover,
                                                      We constructed 305 different 3-SAT formulas, each consist-
by executing a small number of additional experiments the
                                                      ing of 100 different variables and 430 clauses. The formulas
agent’s utility will increase.
                                                      were tested in advance for the existence of a valid truth as-
                                                      signment. The GSAT algorithm restarted with a new random
4  The 3-SAT domain                                   truth assignment after 5,000 ﬂips, and the total number of
We assumed that each Truth-assignment ﬂip takes one unit of restarts was set at 18. The temperature of the SA algorithm
time. Thus, the MT scenario in this domain is a minimum was set at 2%, and the experiment was stopped after 100,000
problem: the agent must solve M formulas within a minimal unsuccessful ﬂips. The Random Walk with GSAT was im-
number of ﬂips. Without loss of generality, when comparing plemented using three different probabilities of Walk: 50%,    Heuristic GSAT Simulated Random Random Random
                  Annealing 50%    60%    80%
     Flips # 26.2   18.3    5.1    4.5    16.0

  Table 1: Ofﬂine results of number of ﬂips (in thousands)


                                                             Figure 2: 3-SAT MT scenario inﬂuence of M

                                                       4.2  2-heuristic experimental results
                                                       According to the results presented in table 1 we compared (1)
                                                       Random 60% to Random 50%; (2) SA to GSAT; (3) Random
                                                       80% to SA. Other pairs, although feasible, would not effec-
                                                       tively demonstrate the HSC algorithm’s performance: the dif-
Figure 1: 3-SAT % of choosing the best heuristic without the ference between their means is too large for the algorithm to
HSC algorithm, with the algorithm in the MT and with the improve. From empirical results we know that the algorithm
algorithm in the MF scenarios                          will not advise on further experiments in those cases, and sim-
                                                       ply yield the better heuristic according to the information the
                                                       agent has. Thus, it will not endure any loss for these other
60% and 80% (The three different heuristics of this algorithm pairs. The ofﬂine results revealed that the best heuristic in
will be denoted by Random 50%, 60% and 80%). In each ex- each pair was Random 60%, SA and Random 80%, respec-
periment the maximal number of ﬂips was set at 15,000 and tively. The total number of experiments in each scenario was
the number of restarts was set at ﬁve. We established all the 120, 40 experiments per pair.
algorithms’ parameters such that, on the one hand, they will In the MT scenario, when the agent did not use the HSC al-
have a reasonable potential to solve any formula, but on the gorithm, it always chose the heuristic with the better 5-game
other, their execution time will remain low.           average. In contrast, when the agent applied the HSC algo-
                                                       rithm, it mostly executed more experiments for both heuris-
  The preliminary experiments executed each of the ﬁve tics, and then either changed its mind or continued with the
heuristics on the 305 3-SAT formulas. Table 1 presents the better 5-game heuristic. On average, only 12 additional ex-
average number of ﬂips each heuristics obtained. The param- periments were executed for each heuristic. Figure 1 summa-
eters in the equations of section 2 were estimated using the rizes the percentage of experiments in which the agent chose
data accumulated during the preliminary experiments stage, the best heuristic, with and without the HSC algorithm across
since this data comprises our whole population. Thus, the a- the three heuristic pairs. As we expected, the algorithm im-
priori parameters ζA and ζB were estimated with the mean of proved the agent’s decision-making, and directed it to the best
all the ofﬂine results of all heuristics (55.2), and τA and τB heuristic in 80% of the experiments. Moreover, without the
were estimated by their standard deviation (22.14). In addi- HSC algorithm, the agent would have chosen the best heuris-
tion, σA and σB were estimated by heuristic A and heuristic tics only in 61% of the experiments. This improvement re-
B standard deviations, respectively. A sensitivity analysis of sulted in a gain in the number of ﬂips: the average number
these parameters determined that the results were not sensi- of ﬂips for 300 3-SAT formulas without the algorithm (4.20
tive to changes in the parameters. In the analyses we varied million) was signiﬁcantly higher than with the HSC algorithm
the values of σA and σB, of ζA and τA and of ζB and τB (4.06 million)(Wilcoxon pv=0.07).
and re-executed the experiments. Except for several extreme To show the inﬂuence of M on the average gain, we exe-
situations we obtained similar results for the speciﬁc values cuted 120 additional experiments, with 12 different values for
described above.                                       M. Figure 2 summarizes the average gain per formula solved
                                                       for M varying between 100 and 3000. As expected, the aver-
  Furthermore, in each experiment nA and nB were set to age gain is in linear relation to the size of M, since the HSC
ﬁve, M to 300, and T to 200,000. That is, in each experiment algorithm poses a greater potential beneﬁt as the number of
of both scenarios, ﬁve formulas chosen randomly from the formulas increases.
305 formulas mentioned above, were solved ﬁrst, and were
                                                         The agent’s task in the MF scenario was to solve as many
used as the agent’s preliminary units of information. Then, it
                                                       formulas as possible within 200,000 ﬂips. Here, on average
had to decide with which heuristic to proceed solving the re- nine additional experiments were executed for each heuris-
maining 300 formulas. We allowed the agent only ﬁve units tic. Figure 1 presents the percentage of experiments in which
of prior information since this is often the case in the real the agent chose the best heuristic, with and without using the
world - agents need to base their decision on only few ob- HSC algorithm (note that the percent of without the HSC al-
servations due to uncertainty in their environment or due to a gorithm is the same as in the MT scenario). In this scenario
cost associated with the information.                                                              Greedy SA Random Random Random
                                                                         80%   50%    60%
                                                      Without  1    2    2      16    19
                                                       With    0    0    1      9     30

                                                              Greedy SA Random Random Random
                                                                         80%   50%    60%
                                                      Without  1    2    2      16    19
                                                       With    0    0    0      11    29
      Figure 3: 3-SAT MF scenario inﬂuence of T
                                                 Table 2: MT Scenario chosen heuristic distribution, with and
                                                 without applying the HSC algorithm: (top) The binary tree
                                                 results; (bottom) The ﬁxed number of experiments results
as well, the agent succeeded in choosing the best heuristic
more frequently by using the algorithm (83% vs 61%). Con-
sequently, it managed to solve six more formulas on aver- 5 The CT domain
age: 212 formulas with the HSC algorithm in contrast to 206 We investigated the HSC algorithm in a two-player negoti-
without it. To demonstrate the inﬂuence of T, we executed ation game that uses the CT game [Grosz et al., 2004]. In
an additional 70 experiments for seven different values of T this game each player has a goal placed on the game board
varying from 2,500 to 100,000. As the average gain is depen- and certain resources to help it reach the goal. The players
dent on T (the larger the T the larger the gain), we normalized can exchange resources, and at the end of the game are as-
the gain by calculating the average gain per 2,500 ﬂips. Thus, signed a score that corresponds to their performance through-
when T was set to 5,000 the average gain was divided by 2, out the game. During the game the agents may negotiate
and when is was set to 100,000 the average gain was divided on resources, commit to resource exchanges and execute ex-
by 40. Figure 3 summarizes the average gain per 2,500 ﬂips. changes. However, the commitments made by an agent are
It supports our hypothesis that the gain of using the HSC al- not enforceable, and it might decide to back down on a com-
gorithms increases as T increases. It seems that the increase is mitment or even deceive an opponent agent by committing to
very steep at the beginning, but when T > 25, 000 it becomes an exchange it does not intend to keep.
moderate.                                          In [Talman et al., 2005], an automated agent able to play
                                                 repeated CT games was developed. The agent characterizes
4.3  k-heuristic experimental results            itself and its opponent in terms of cooperation and reliabil-
                                                 ity. The cooperation trait measures the willingness of an
                                                 agent to share its resources with others, whereas the relia-
We tested the generalized k-heuristic model using the binary bility trait measures the agent’s willingness to keep its com-
tree algorithm and the ﬁxed number of experiments algo- mitments in the game. Accordingly, the agent is capable of
rithm. The binary tree was built according to the pairs we employing seven strategies, differing in the level of cooper-
described in section 4.2. That is, the binary tree’s leaves were ation and reliability the strategy dictates. Each strategy is
Random 50% and Random 60%, then GSAT and SA and ﬁ- suitable to a different type of an opponent, but the optimal
nally Random 80%. The ﬁxed number of experiments proce- matching scheme is unknown. For example, it may prove
dure was executed according to the description presented in beneﬁcial to play a low-reliability strategy against a highly-
2.2. We repeated the procedures 40 times for both algorithms. cooperative opponent by deceiving it. On the other hand, per-
  Table 2 summarizes the number of times (of the forty ex- haps a more logical strategy against such an opponent will
periments) the agent chose each heuristic with and without be a high-cooperation strategy which promotes reciprocity in
each algorithm for the MT scenario. As expected, both algo- the game, and may beneﬁt the agent in the long run. Thus, in
rithms improved the agent’s decision-making, in the binary order to maximize the agent’s score in the game, it must de-
tree algorithm by 27.5% and in the ﬁxed number of experi- termine which strategy best-suits each opponent type. Each
ments algorithm by 25%. Nevertheless, the gain in the agent’s strategy trait can be low (L), medium (M) or high (H), and a
utility was not very impressive in the binary tree case: on strategy will be referred to by its cooperation-reliability level,
average 1,500 ﬂips were lost per experiment due to the exe- such as a low-cooperation medium-reliability strategy (or LM
cution of heuristics that although seemed promising in their strategy). Therefore, the seven possible strategies in the game
pair, were in fact time consuming in the overall aspect. On are LL, LM, LH, MM, MH, HM and HH. The remaining two
the other hand, in the ﬁxed number of experiments case 1,522 strategies, ML and HL are not applicable as an agent apply-
ﬂips were gained per experiment (the average total number of ing low reliability strategy has by deﬁnition a low cooperation
ﬂips per experiment was 4,800). The number of additional ex- level - when an agent almost never keeps it commitments, it
periments were 14 for each heuristic on average. Due to the is not willing to share its resources.
disappointing results of the binary tree method, for the MF We expect that our suggested model will assist the agent in
scenario we executed only the ﬁxed number of experiments its decision-making, which will result in higher score in the
method. Here, on average 1.6 formulas more were solved per game. Following ni games of each strategy the agent exe-
experiment when applying the algorithm.          cutes the HSC algorithm and determines which strategy best-