        AWA* - A Window Constrained Anytime Heuristic Search Algorithm

                         Sandip Aine      P. P. Chakrabarti     Rajeev Kumar
                            Department of Computer Science & Engineering
                            Indian Institute of Technology Kharagpur, India.
                    {sandip, ppchak, rkumar}@cse.iitkgp.ernet.in


                    Abstract                          by gradually decrementing the weight using a chosen Δ af-
                                                      ter each iteration [Likhachev et al., 2004]. Another algorithm
    This work presents an iterative anytime heuris-   was proposed to modify the weighted A* technique with se-
    tic search algorithm called Anytime Window A*     lective commitments [Kitamura et al., 1998]. One of the ma-
    (AWA*) where node expansion is localized within   jor disadvantages of using the weighted A* based techniques
    a sliding window comprising of levels of the search is that weighing the heuristic introduces non-admissibility.
    tree/graph. The search starts in depth-ﬁrst mode  As the basic characteristics of A* change substantially with
    and gradually proceeds towards A* by increment-   non-admissible heuristics [Pearl, 1984], it becomes very dif-
    ing the window size. An analysis on a uniform tree ﬁcult to establish a relation between node expansions and the
    model provides some very useful properties of this weighing factors (w,Δ). Therefore, to use a weighted A*
    algorithm. A modiﬁcation of AWA* is presented     technique, lengthy simulations are required to ﬁnd appropri-
    to guarantee bounded optimal solutions at each it- ate parameter settings. Also, there are applications where
    eration. Experimental results on the 0/1 Knapsack no speciﬁc heuristic for the remaining path is available, the
    problem and TSP demonstrate the efﬁcacy of the    search may be guided only by the g(n) information [Pearl,
    proposed techniques over some existing anytime    1984] or a lower bound f(n)-value may be used without dis-
    search methods.                                   tinct g(n) and h(n) components [Kumar et al., 1995].Insuch
                                                      cases, the weighted A* techniques are not directly applicable.
                                                        On the other hand, in beam search, a chosen number of
1  Introduction                                       most promising nodes at each level of the search tree/graph
Development of optimization methods that can work within are selected for further expansion, and the remaining nodes
time limitations is a major need for modern practitioners fac- are pruned. Several anytime variants of the basic beam
ing large-sized problems that are NP-hard in nature. This re- search technique [Zhang, 1998; Zhou and Hansen, 2005;
quirement has spawned research in the direction of designing Furcy, 2006] have been applied for different problems with
anytime algorithms [Dean and Boddy, 1988], i.e., the algo- reasonably good results. However, the lack of models, which
rithms which can work under any time limitations. Anytime can provide the user an apriori estimate about the quality-time
algorithms are expected to regularly produce solutions of im- trade-off, remains a problem for these algorithms as well.
proved quality and when suddenly terminated, return the best In this work, we present an alternative anytime heuristic
solution produced so far as the result.               search algorithm Anytime Window A* (AWA*), which local-
  Among heuristic search techniques, the depth-ﬁrst branch izes the global competition performed by A* within a ﬁxed-
and bound (DFBB) approach and its variants [Sarkar et size window comprising of levels of the search tree/graph.
al., 1991; Zhang, 1998] are by default anytime algorithms, The nodes which lie outside the active window are not con-
as they produce a stream of gradually improving solu- sidered for expansion at that point. The window is slid down-
tions. However, in general, the depth-ﬁrst techniques suf- wards to provide a depth-ﬁrst component to the best-ﬁrst
fer from excessive node expansions before they actually search that occurs within a window. The window size is set
reach the optimal solution.The best-ﬁrst anytime search algo- to 0 to start with (depth-ﬁrst mode) and is then increased iter-
rithms can be broadly classiﬁed into two categories, namely, atively proceeding towards pure best-ﬁrst search algorithms
weighted heuristic search [Pohl, 1970; Hansen et al., 1997; like A*. The solution quality is expected to improve for it-
Zhou and Hansen, 2002; Likhachev et al., 2004] and beam erations with a larger window size. While in AWA*, we use
search [Zhou and Hansen, 2005; Furcy, 2006].          the basic anytime principles of depth guided pruning and it-
  In weighted heuristic search, the heuristic estimate is mul- erative relaxation, the algorithm retains the admissibility of
tiplied by a weight, w (w ≥ 1), to yield f(n)=g(n)+   the heuristic estimate while it continues to search in best-ﬁrst
w ∗ h(n). In the anytime variants of weighted A*, a series of mode within the given window. It may be noted that AWA*
solutions are produced either by iterating with the same cho- does not need explicit decomposition of the evaluation func-
sen weight [Hansen et al., 1997; Zhou and Hansen, 2002] or tion (f(n))intog and h components. We analyze the char-

                                                IJCAI-07
                                                  2250acteristics of the presented algorithm using a uniform search Algorithm 1 Procedure Window A*
tree model. This reveals some very interesting properties of 1: CurLevel ←−1;
the algorithm in terms of heuristic accuracy versus solution 2: if OpenList is empty then
                                                      3:  Return BestSol;
quality and related search complexity.                4: end if
  We also present a bounded optimal version of the AWA* 5: Select node n which has the least f(n)-value node from OpenList;
algorithm, called Bounded Quality Anytime Window A*   6: Insert n into ClosedList;
                                                      7: if f(n) ≥ BestSol then
(BQAWA*), which is guaranteed to produce solutions within 8: Return BestSol;
a pre-speciﬁed sub-optimality bound, at each iteration. In 9: else if Level(n) ≤ CurLevel − WindowSizethen
                                                      10:  Remove n from ClosedList;Insertn into SuspendList;
BQAWA*, the search backtracks and dynamically adjusts the 11: goto line 2
window size within an iteration, such that the bound restric- 12: else if Level(n) >CurLevelthen
tion is adhered to.                                   13:  CurLevel ← Level(n)
                                                      14: end if
  We demonstrate the efﬁcacy of the presented strategies on 15: if IsGoal(n) then
two optimization problems, namely, the traveling salesman 16: BestSol ← f(n); Goal ← n;
problem (TSP) and the 0/1 knapsack problem. Compara-  17:  Return BestSol;
                                                      18: end if
                                                                         
tive results with the existing anytime techniques (like DFBB, 19: for Each successor node n of n do
                                                                   
                                                      20:  Calculate f(n );
ARA*, Beam-stack search) show considerable improvement       
                                                      21:  if n is not in OpenList or ClosedList or SuspendList then
                                                                              
in performance.                                       22:   Parent(n ) ← n; Level(n ) ← Level(n)+1;
                                                                 
                                                      23:   Insert n to OpenList;
                                                                
                                                      24:  else if n is in OpenList or SuspendList then
                                                                
                                                      25:   if f(n ) < previously calculated path estimation then
2  Anytime Window A* Algorithm                                                         
                                                      26:     Parent(n ) ← n; Update f(n ), Level(n );
                                                      27:   end if
The algorithm A* considers each node to be equivalent in        
                                                      28:  else if n is in ClosedList then
terms of information content and performs a global compe-       
                                                      29:   if f(n ) < previously calculated path estimation then
                                                                                       
tition among all the partially explored paths to select a new 30: Parent(n ) ← n; Update f(n ), Level(n );
                                                                  
node. In practice, the heuristic errors are usually distance de- 31: Insert n to OpenList;
       [         ]                                    32:   end if
pendent Pearl, 1984 . Therefore, the nodes lying in the same 33: end if
locality are expected to have comparable errors, where as the 34: end for
error may vary substantially for nodes which are distant to 35: goto line 2
each other. Thus, if the global competition performed by A*
is localized within some boundaries, tighter pruning can be size is increased in a pre-decided manner (we have used grad-
obtained. Also, as better nodes at a level are generally very ual increment by 1), and Window A* is called again. If the
competitive even within small localities, we can expect good algorithm is terminated through external interrupts, the best
quality solutions with localized expansions. This observation solution obtained thus far is returned.
forms the basis of AWA*, where we localize the node expan-
sions using a sliding window. The AWA* algorithm works in Algorithm 2 Anytime Window A* (AWA*)
two phases, in the inner loop it uses the Window A* (Algo. 1)
                                                      1: INPUT :: A search tree/graph and a start node s.
routine with a ﬁxed window size and computes the solution 2: ClosedList ← φ; SuspendList ← φ; WindowSize ← 0; Calculate
under the current restriction. In the outer loop (Algo. 2) the f(s); Level(s) ← 0;Inserts to OpenList; BestSol ←∞;
window restrictions are gradually relaxed to obtain iteratively 3: BestSol ← Window A*(); {Call Window A*}
                                                      4: if SuspendList is empty then
improving solutions.                                  5:  Terminate with BestSol as the Optimal Solution;
  The Window A* routine works with a pre-speciﬁed win- 6: else
                                                      7:  Add OpenList to ClosedList; Move SuspendList to OpenList;
dow size. This window size is used to restrict the active queue WindowSize ← WindowSize+1;
of the A* algorithm. For a given window size ω,theex- 8:  goto line 3
pansions are localized in the following way. When the ﬁrst 9: end if
node of any level (say l) is expanded, all nodes in the open
list which are from level (l − ω) or less will be suspended For an anytime algorithm, there are two desirable proper-
for this iteration. The window slides in a depth-ﬁrst man- ties, namely, interruptibility and progress. The ﬁrst property
ner when deeper nodes are explored. Window A* terminates ensures the anytime nature, i.e., the algorithm should pro-
when the ﬁrst goal node is expanded (A* like termination) vide a solution at pre-mature terminations. The second one
or if the open list does not contain any node having estimated states that the solution quality should improve with time, and
cost less than the cost of the best solution obtained in previous if enough time is allocated the algorithm should eventually
iterations.                                           return the optimal solution. For ﬁnite search spaces, AWA*
  The AWA* algorithm calls the Window A* routine multi- adheres to both the properties mentioned. It starts in a depth-
ple times with gradual relaxation of the window bounds. At ﬁrst mode (with no back-tracking) trying to obtain fast (possi-
the start, window size is set to 0 (depth-ﬁrst mode). Once bly sub-optimal) solutions, and then iteratively increases the
the Window A* routine terminates, AWA* checks whether window size to produce better quality solutions. Eventually,
there are any nodes suspended in the previous iteration. If the when the window size becomes such, that all the ’optimal-
suspended list is empty, the algorithm is terminated return- path’ nodes lie within the active window, the optimal solution
ing the optimal solution. Otherwise, the open list is emptied is returned.
and the suspended list is moved to the open list. The window While iterative algorithms provide opportunities to attain

                                                IJCAI-07
                                                  2251different trade-offs through anytime terminations, the search the window size is ω,
efforts can increase substantially if re-expansions are not con- = ( (   ) ≤    (Γ)) Γ=      ( +     )
trolled. For AWA* with a consistent heuristic, a node is ex- rg,j,ω P f ng,j Min    ,    min  l  ω,N
panded at most once within the Window A* routine, and it                                              (2)
can only be re-expanded in a later iteration if its g-value is Next, we use the search tree model to obtain the ex-
lowered. Thus AWA* requires minimal re-expansion.     pressions for the expected node expansions and convergence
  It may be noted, that in the generalized case AWA* does probability for a given ω. The expected node expansions and
not guarantee a solution at each iteration (Window A* loop). convergence probabilities (for a given window size) are pre-
If this property is a requirement; an easy way to attain this sented in the following lemmas,
is through back-tracking (i.e., by moving the suspended list Lemma 1. Expected number of node expansions EN(ω)
                2
to open list at line of Algo. 1), when no solution is found. (window size = ω)foram-ary uniform cost search tree T
However, we propose a better method for this in BQAWA* (with depth N) is given as,
(Sec. 4), which not only provides a solution but guarantees a   
                                                           ( )=    l=N   (    )+(    − 1)Σi=l  l−i ∗  (    )
bounded quality solution, at each iteration.           EN   ω      l=0 Pg g,l,ω    m      i=1m     Pn  i, l, ω
                                                                (     )=Πj=l
                                                       Where, Pg g,l,ω     j=0rg,j,ω, and,
3  Analyzing AWA* using a Search Tree Model               (    )=Πj=l       ∗ Πk=i−1
                                                       Pn i, l, ω   j=iqi,j,ω  k=0  rg,k,ω
In this section, we analyze the quality-time response of Win-                                         (3)
dow A* algorithm in terms of a uniform cost tree model (as Lemma 2. Expected probability of reaching the optimal goal
suggested in [Pearl, 1984]). We model the search space as a
                                                      node Ps(ω) (window size = ω)foram-ary uniform cost
uniform m-ary tree T , with a unique start state s. The leaf search tree T (with depth N) is given as,
nodes are grouped into two parts. There is a unique optimal
goal node g, situated at a distance N from s, and all other              ( )=Πj=N
                                                                       Ps ω     j=0 rg,j,ω            (4)
nodes at level N are sub-optimal terminal nodes. Window
A* terminates when either the optimal goal node or a sub- We try to estimate the expansion probability values
optimal terminal node is reached. The optimal solution path (qi,j,ω,rg,j,ω) from the heuristic information available. We
is given as (s, n 1..n ..n −1,g)wheren   denotes the  assume that the relative estimation errors (Eqn. 5), are inde-
             g,   g,i  g,N            g,i                                    (  )      (0 )       0 ≤  ≤
solution path node at level i. The trees T1...Ti...TN are ’off- pendent random variables Y n within ,e bound ( e
course’ sub-trees of T , one level removed from the solution 1) with an arbitrary distribution function.
path. An ’off-course’ node is labeled as ni,j ,wherej denotes     Y (n)=[h∗(n)  − h(n)]/h∗(n)         (5)
the level of that node and i denotes the root level of the ’off-
course’ sub-tree (Ti) to which the node belongs. Using this Now, for a node ni,j at depth j and belonging to ’off-course’
                                                      sub-tree Ti (Figure. 1), we have
                        S
                                                                   g(ni,j )=j,
                                                                    ∗                                 (6)
                    1      1                                       h (ni,j )=N + j +2(1−  i)

                              i1
                                                      Similarly, for an ’on-path’ node ng,j,wehave
                                 i
                           i
                  T 1
                                                                        g(ng,j)=j,
                                                                         ∗                            (7)
                                   N1                                   h (ng,j)=N  − j
                          T i
                                       G
                                TN                    Thus, with the chosen error model (Eqn. 5),

                                                        f(ni,j )=N +2(j − i +1)−  Y (n)(N + j +2(1−  i))
                                                        f(n  )=N   − Y (g)(N − j)
         Figure 1: Uniform binary tree (model)             g,j
                                                                                                      (8)
                                                      Where Y (n), Y (g) are identically distributed random vari-
model (Fig. 1), we try to quantify the expected node expan-
                                                      ables within (0,e) bounds. Combining Eqn. 8 with the earlier
sions and the probability of reaching the optimal goal node,
                                                      deﬁnitions, we obtain
for a speciﬁc window restriction. First we present few termi-
nologies,                                               qi,j,ω =1− P (Y (n) ≤ (j + α − Min(Γ))/α)
                                                                =    +  +2(1−    )    Γ=      ( +     )
Deﬁnition 1. q   : q  denotes the expansion probability where, α  N    j        i and,    min  l  ω,N
             i,j,ω i,j,ω                                                                              (9)
of an ’off-course’ node ni,j when it is in the open list and the
window size is ω,                                     and,
                                                              =1−    ( ( ) ≤ (  −      (Γ)) ( −  ))
  qi,j,ω = P (f(ni,j ) ≤ Min(Γ)) , Γ=min(l + ω,N)       rg,j,ω     P  Y n     N   Min     / N    j   (10)
  and, Min(l)=min(f(n)),  ∀n such that n ∈ open list                                                  ( )
           ( )=                                       From Eqns. 1 and 2, we observe that apart from the f n
   and level n  l.                                    value, the node expansion probability also depends on the
                                                (1)   Min(l + ω) value. Now, the Min(l) value at a given level is
Deﬁnition 2. rg,j,ω : rg,j,ω denotes the expansion probability bounded by the minimum and maximum possible f-values
of a ’on-path’ node ng,j when it is already in the open list and for that level. Also, for a consistent heuristic the Min(l)

                                                IJCAI-07
                                                  2252function should be non-decreasing with l. Thus, we obtain            1 1. e = 0.5, c = 0.1
                                                                      2. e = 0.5, c = 0.5
the following properties for Min(l).                                  3. e = 1.0, c = 0.1
                                                                    0.8 4. e = 1.0, c = 0.5 (1)
          N − e(N  − l) ≤ Min(l) ≤ 2l + N                           0.6
                                               (11)
          Min(l) ≥ Min(l − 1)                                       0.4
With these observations, we represent the Min(l) function as        0.2        (2) (4)
                                                                     0           (3)
follows,                                                           Expected probability of convergence  0  2  4  6  8  10
                                                                               Window Size 
     Min(l)=max(Min(l     − 1),
     N(1  − e(1 − s(l)) + l(e +(2− e)s(l))))   (12)   Figure 3: Convergence probability versus window size, for a
     where, s(l) is a function of l, 0 ≤ s(l) ≤ 1.    uniform binary tree of height 10 for AWA*
The Min(l) function denotes the minimum value of a set of
numbers. From statistics, we know that with the increase of         300
                                                                      1. Max density heuristic
the set cardinality the probability of the minimum converging       250  2. Fitting heuristic
                                                                                 (1)
to the lower limit increases exponentially. The implication         200
suggests that with increase in l, the probability of Min(l)         150

converging to the lower limit increases exponentially. Thus,        100
 ( )                                                                               (2)
s l can be approximated using a decreasing function with l,          50
and most probably the decrease will be exponential.                Average node expansions
                                                                      0    2   4    6    8   10
  Using the above presented results, we can estimate the ex-                   Window Size 
pected node expansion and the optimal convergence proba-
bilities for a chosen error distribution and s(l) function. We Figure 4: Average node expansions versus window size, 0/1
assume a simple linear distribution for Y (n) and approximate Knapsack problem
s(l) as an exponential function of l, i.e., we assume,
                     
                        0  (    0)
                         if x<                        size curves (Fig. 3) do not show much variability with dif-
       ( ( ) ≤  )=           (0 ≤   ≤  )
     P  Y n    x        x/e if    x   e        (13)   ferent errors. While the convergence probability for a given
                        1
                         otherwise                    window size increases with heuristic accuracy, the improve-
and,                                                  ment is marginal. However, the expected node expansions
                s(l)=cl,  0 ≤ c ≤ 1            (14)   for a given window increases substantially with more error
                                                      (Fig. 2). This observation can be explained by the fact that
Substituting the variables in Eqns. 3 and 4, we obtain the with more error, both the Min(l) and the f(n) values are
                                                      expected to decrease, causing relatively small changes in the

              35 1.  e = 0.5, c = 0.1                 qi,j,ω and rg,j,ω probabilities. The difference is even less pro-
               2.  e = 0.5, c = 0.5 (4)                                      ( )
               3.  e = 1.0, c = 0.1                   nounced for rg,j,ω as the f n values of the ’optimal-path’
              30
               4.  e = 1.0, c = 0.5                   nodes are expected to remain close to the Min(l) values. On
              25
                               (3)                    the other hand, the expected node expansions increase expo-
              20                                      nentially with the increase in qi,j,ω values (Eqn. 3). Thus,
              15               (2)                    with increased error more ’off-course’ nodes are expanded,

            Expected node expansions (1)
              10                                      causing appreciable variance in the node expansions.
               0    2    4   6    8    10
                        Window Size                     We conclude this section by presenting the experimental
                                                      results for average node expansions and convergence prob-
Figure 2: Expected node expansions versus window size, for abilities with changing window size, for the 0/1 Knapsack
a uniform binary tree of height 10 for AWA*           problem using two different heuristics h1 (max. density) and
                                                      h2 (ﬁtting based), such that h2 dominates h1 (Fig. 4 and 5).
expected values for node expansions as well as convergence
probabilities for different window constraints. In Figures 2
                                                                     1 1. Max density heuristic
and 3, we present the curves for expected node expansions                2. Fitting heuristic
and convergence probabilities with different window size, for       0.8
                           10
a uniform binary tree of height . The expected values are           0.6
computed using two error boundaries (e =0.5 and e =1.0),                           (1)
                                                                    0.4         (2)
and two chosen constants (c =0.1 and c =0.5). From the
curves, we observe that the node expansions and convergence         0.2


                                                                  Avergae  probability of convergence  0
probabilities obtained are almost independent of the choice           0    2   4    6    8   10
of c. This can be explained by the exponential nature of                       Window Size 
chosen s(l) function, which rapidly converges to the lower
limit. Investigating the trends with different heuristic errors, Figure 5: Average optimal convergence versus window size,
we observe that the convergence probability versus window 0/1 Knapsack problem

                                                IJCAI-07
                                                  2253The ﬁgures depict that the average node expansions and the (Algo. 4) works with the same principle of ARA* technique,
convergence probabilities with different heuristics show iden- i.e., the algorithm starts with a high initial bound () and iter-
tical trends to that obtained from the analytical model. atively reduces the bound to get improved solutions. It uses
                                                      the BQWA* routine (Algo. 3) to obtain intermediate solutions
4  BQAWA* - Anytime Window A* with                    within the chosen sub-optimality bound. After each iteration,
                                                      the suspended list is moved to the open list, weight bound
   Provable Quality Bounds                            is reduced and BQWA* is re-run to get improved solutions.
In AWA*, we have an anytime algorithm which attempts to The algorithm terminates if the suspended list is empty after
produce a stream of gradually improving solutions. However, an iteration.
no bound on the obtained solution is established. In this sec-
tion, we present a modiﬁed version of the AWA*, termed as 5 Experimental Results
Bounded Quality Anytime Window A* (BQAWA*), where     In this section, we present empirical results comparing the
each solution produced will be bounded within a pre-  performance of AWA* with some existing anytime tech-
speciﬁed factor () of the optimal solution, the bound is itera- niques (DFBB, ARA* and Beam-stack search). We per-
tively tightened to produce an iteratively improving stream of formed experiments with two optimization problems, namely
solutions.                                            0/1 Knapsack and Euclidean Traveling Salesman (TSP) prob-
                                                      lem. Since time spent is usually proportional with the number
Algorithm 3 Procedure Bounded Quality Window A*       of nodes expanded, we present the results in terms of node ex-
1: CurLevel ←−1; MinSus ←∞;                           pansions.
2: if OpenList is empty then
3:  goto line21
4: end if                                                           226
                       ( )                                             (4)
5: Select node n which has the least f n -value node from OpenList;       (3)
6: Insert n into ClosedList;                                        224       (2)
7: if f(n) ≥ BestSol or f(n) ≥ MinSus ∗  then                                  (1)
8:  goto line 18 {Update window size and back-track}                222
9: else if Level(n) ≤ CurLevel − WindowSizethen
                                                                    220
10: Move n from ClosedList to SuspendList; Update MinSus;
11: goto line 2                                                   Average  Cost
           ( )                                                      218            1. DFBB
12: else if Level n >CurLevelthen                                                  2. ARA*
    CurLevel ← Level(n)                                                      3. Beam-stack Search
13:                                                                 216            4. AWA*
14: end if
15: GOAL EVALUATION {Same as Window A*}                               0  500  1000  1500  2000  2500  3000  3500  4000
16: EXPANSION {Same as Window A*.}                                          n (n = nodes expanded x 50)
17: goto line 2
18: if f(n) <BestSolthen
19: Move n from ClosedList to OpenList;               Figure 6: Average cost versus node expansions - 0/1 Knap-
20: end if                                            sack problem
21: Merge SuspendList with OpenList; Increment WindowSize;
22: if OpenList is empty then
23: Return BestSol;                                     For 0/1 Knapsack, we estimated the heuristic by ﬁtting the
24: end if                                            remaining objects in decreasing order of their cost-densities.
25: goto line 1
                                                      We performed experiments on a set of 100 random instances
                                                      of the 50-object 0/1 Knapsack problem. Weight and costs of
  To achieve the pre-speciﬁed bounds at each iteration, we individual objects were generated randomly, while the con-
modify the Window A* routine to obtain Bounded Quality straint was chosen within 0.4 − 0.6 of the sum of weights. In
Window A* or BQWA* (Algo. 3). In BQWA* routine, at    Figure 6, we present the cost versus nodes expansion results
any intermediate point, the minimum f-value among the sus- obtained for DFBB, ARA*, Beam-stack search and AWA*.
pended nodes (min(fsus)) is noted. If the f-value of the Figure 7 includes the percentage optimal convergence versus
top node in the open list is greater than  ∗ min(fsus) (for node expansion curves. For ARA*, the weight limits were
minimization problems), the open list and the suspended list initialized to 2.0, and decreased by 0.1 per iteration.
are merged, and the search back-tracks after incrementing the For Euclidean TSP, we used the minimum spanning tree
window size. The ﬁrst solution produced by the algorithm is (MST) heuristic. We performed the experiments on a ran-
published with  bound guarantee. The BQAWA* algorithm domly generated set of 100 25-city problem instances. The
                                                      intermediate tour length and percentage optimality results
Algorithm 4 Bounded Quality Anytime Window A*         comparing AWA* with DFBB, ARA* and Beam-stack search
                                                      are included in Figures 8 and 9, respectively.
1: INPUT :: A search tree/graph, a start node s and an initial bound .
2: ClosedList ← φ; SuspendList ← φ; WindowSize ← 0; Calculate From the results obtained for both 0/1 Knapsack and TSP,
  f(s); Level(s) ← 0;Inserts to OpenList; BestSol ←∞; we observe that the performance of AWA*, is superior to all,
3: BestSol ← Bounded Quality Window A*();
4: if SuspendList is empty then                       DFBB, ARA* and Beam-stack search, both in terms of in-
5:  Terminate with BestSol as the Optimal Solution;   termediate solution qualities as well as percentage optimal
6: else                                               convergences.
7:  Add OpenList to ClosedList; Move SuspendList to OpenList;
8:   ←  − Δ;                                          In Table 1, we present the average node expansions for
9:  goto line 3                                       ARA* and BQAWA* applied to 0/1 Knapsack and TSP, with
10: end if                                            different sub-optimality bounds. The results shows for all the

                                                IJCAI-07
                                                  2254