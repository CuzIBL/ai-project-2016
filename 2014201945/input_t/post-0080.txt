                                     Motivated Agents 

                     Kathryn Kasmarik1, William Uther, Mary-Lou Maher1 
                          National ICT Australia∗, 1University of Sydney 
          kkas0686@it.usyd.edu.au, william.uther@nicta.com.au, mary@arch.it.usyd.edu.au 


 1 Introduction                                    Sensation transforms raw data received from sensors into 
                                                 structures called events.  Events encapsulate two recognis-
 This poster presents a model for motivated agents governed able occurrences among state variables values: increases and 
 by an internal, domain independent motivation process decreases between one state and the next.  Individual events 
 rather than domain specific rewards, goals, or examples and the current state n-tuple are incorporated into memory.  
 provided by an external teacher.  Internal motivation is de-  
 sirable in complex, dynamic environments where hard-
 coded domain theories can only approximate the true state         effectors 
 of the world and where pre-programmed goals can become                 action 
obsolete.              
  Early work with motivated agents focused on the use of             action 
 domain specific motives [Sloman and Croucher, 1981] or 
 environment modelling by focusing attention on situations 
 with the highest potential for learning [Kaplan and Oudeyer,             highest priority goal 
 2004].  More recent work with intrinsically motivated rein-
 forcement learning agents [Singh et al., 2005] has produced retrieved data  motivation 
agents that, rather than learning a model, learn new behav-
ioural options [Precup et al., 1998].  An option or simply a                new goals and 
                                                            retrieved data  
 behaviour is a whole course of action that achieves some                    behaviours 
sub-goal.  Our model extends previous work by defining 
general structures for events, attention focus and motivation.        memory 
                                                                        events and raw data 
 2  The Motivated Agent Model 
 Our model for a motivated agent is shown in Figure 1.  The        sensation 
 agent has three types of structures: sensors, memory and 
 effectors.  These structures are connected by three proc-              raw data 
 esses: sensation, motivation and action.  The agent functions 
                                                                    sensors 
 in a continuous sensation-motivation-action loop.                                           
  Sensors receive raw data in the form of an n-tuple of state Figure 1 – The motivated agent model. 
 variables {x1, x2, x3, ... xn} describing the current state of the  
 agent and its environment.  The n-tuple has a particular for- Memory is a cumulative record of events, actions, and 
 mat.  Variables x1 to xk comprise data about the state of the goals.  Initially, memory is empty save for a set of primitive 
agent.  Variables xk+1 to xn comprise data about other ob- actions that the agent can perform in the world.   
 jects sensed by the agent.  This includes absolute data as Motivation motivates agents to understand and repeat in-
 well as relative data such as the location of an object relative teresting events that occur in their environment.  We charac-
 to the agent.  This general format is domain independent and terise interesting events as changes in the world that occur 
 the inclusion of relative values makes it possible for the infrequently.  Agents are motivated to create goals to under-
 agent to learn general behaviours relative to itself. stand and repeat interesting events by identifying situations 
                                                 in which they can learn how to make the event recur using a 
                                                 temporal difference Q-learner (TDQL) [Sutton and Barto, 
                                                  2000].  Goals are masked so that events that occur with 
   ∗ National ICT Australia is funded by the Australian Govern- equal or lesser frequency than the one being pursued are 
 ment’s Backing Australia’s Ability initiative, in part through the ignored.  Masking increases learning efficiency by focusing 
 Australian Research Council.     attention and reducing the size of the state space.  Once an iour-2 in response to some goal G.  We then implemented a 
 agent can repeat an interesting event at will, it can encapsu- flat TDQL with a pre-programmed reward for achieving a 
 late its new knowledge as a behaviour.  Masking ensures similar behaviour that satisfies G.  Figure 3 shows that the 
that behaviours are independent of the situation in which motivated agent learns Behaviour-2 more quickly than a flat 
they were learned.  A behaviour can be reused either as a TDQL can learn a similar behaviour.  This is because the 
pre-planned course of action for achieving new goals similar motivated agent can make use of abstract behaviours such as 
to the one from which it was originally created or as a build- Behaviour-1while constructing Behaviour-2.    
ing block when learning to solve more complex goals.    

  Action reasons about which effectors will further the t 50000
agent’s progress towards its current goal.             45000
  Effectors are the means by which actions are achieved.  
                                                       40000
They allow the agent to cause a direct change to the world.    
                                                       35000
                                                       30000
 3  Motivated Agents in Practice  
                                                       25000                        Flat TDQL
 We demonstrate our agent model using a robot guide.  This 20000
 domain, based on Dietterich’s taxi domain [2000] and illus-
                                                       15000          Motivated 
                                                      with  95%  confidence
 trated in Figure 2, contains avatars who need to be guided to 10000    Agent
 particular locations.  Such situations are common in large 5000
 scale virtual worlds where new citizens can easily become 
                                                    Number ofNumber  primitive  actions in  solu 0
 lost.  There are four possible sources and destinations for 
                                                            0    2500000 5000000 7500000 10000000
 avatars.  The robot has twelve effectors controlling the 
movements of its legs and can choose to start or stop guid-   Number of primitive actions performed
                                                                                              
ing an avatar.  The robot’s sensors can perceive the absolute Figure 3 – Learning progress in a guide task.  
co-ordinates of the agent, the elevations of its legs, the co-
ordinates of its legs relative to its body and the absolute and 
relative co-ordinates of an avatar and its destination.   5 Conclusion  
                                                 Motivated agents are autonomous agents governed by inter-
                                                 nal, domain independent motivation.  In addition to being 
                                                 able to choose their own goals they are able to learn behav-
                                                 iours to satisfy their goals more quickly than agents using 
                                                 the TDQL algorithm without motivation. 

                                                 References 
                                                 [Dietterich, 2000] T. Dietterich. Hierarchical reinforcement 
                                                   learning with the MAXQ value function decomposition. 
                                                   Journal of Artificial Intelligence Research, 13:227-303, 
              Figure 2 – A guide robot.            2000.  
                                                  [Kaplan and Oudeyer, 2004] F. Kaplan and P-Y. Oudeyer. 
 After a period of exploration, the robot notices that in- Intelligent adaptive curiosity: a source of self-
 creases and decreases in its location and the location of development. In Proceedings of the 4th International 
other avatars are infrequent.  The agent is motivated to cre- Workshop on Epigenetic Robotics, pages 127–130, 2004.  
ate and pursue goals to repeat these changes.  Over time the 
agent learns walking behaviours, such as Behaviour-1, to  [Precup et al., 1998] D. Precup, R. Sutton, and S. Singh. 
change its location.  These behaviours are independent of Theoretical results on reinforcement learning with tem-
the location in which they were learned and the position of porally abstract options. In Proceedings of the 10th Eu-
other avatars. The agent uses these walking behaviours to ropean Conference on Machine Learning, 1998. 
develop path-following behaviours, such as Behaviour-2, to [Singh et al., 2005] S. Singh, A. G. Barto, and N. Chenta-
change the location of the avatar.                 nex. Intrinsically motivated reinforcement learning. To 
                                                   appear in Proceedings of Advances in Neural Informa-
 Behaviour-1 [lift left foot, move left foot forwards, put-down left foot, lift tion Processing Systems 17 (NIPS), 2005. 
  right foot, move right foot forwards, put-down right foot] [Sloman and Croucher, 1981] A. Sloman. and M. Croucher. 
 Behaviour-2 [guide, Behaviour-1, Behaviour-1, Behaviour-1, Behaviour- 1981. Why robots will have emotions, The 7th Interna-
  1, Behaviour-1, stop-guiding]                    tional Joint Conference on Artificial Intelligence, Van-
                                                   couver, Canada, pp. 197-202. 
 4 Empirical Results 
                                                 [Sutton and Barto, 2000] R. Sutton, and A. Barto.  Rein-
 We measured the performance of the robot guide by graph- forcement learning, an introduction. MIT Press, 2000. 
 ing the number of primitive actions taken to produce Behav-