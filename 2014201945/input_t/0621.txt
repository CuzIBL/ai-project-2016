          ROCCER: an Algorithm for Rule Learning Based on ROC Analysis

                     Ronaldo C. Prati                              Peter A. Flach
      Institute of Mathematics and Computer Science       Department of Computer Science
                  University of Sao˜ Paulo                      University of Bristol
                  Sao˜ Carlos (SP), Brazil                    Bristol, United Kingdom
                 prati@icmc.usp.br                      Peter.Flach@bristol.ac.uk

                    Abstract                            As decision tree induction necessarily builds complete
                                                      disjoint models, in some complex domains with high-
    We introduce a rule selection algorithm called    dimensional feature spaces these models can be quite com-
    ROCCER, which operates by selecting classiﬁca-    plex. In such cases, individual rule learning algorithms may
    tion rules from a larger set of rules – for instance be preferable since they are capable of inducing overlapping
    found by Apriori – using ROC analysis. Experi-    and simpler rule sets [van den Eijkel, 2003]. However, one
    mental comparison with rule induction algorithms  of the problems of set-covering rule learning is that rules are
    shows that ROCCER tends to produce considerably   found in isolation, although they are used in the context of the
    smaller rule sets with compatible Area Under the  others inside the classiﬁer. This issue can pose several prob-
    ROC Curve (AUC) values. The individual rules      lems for a rule learning algorithm. First, from the learning
    that compose the rule set also have higher support perspective, fewer and fewer examples are available as cov-
    and stronger association indexes.                 ering progresses. In latter stages of induction, this may lead
                                                      to fragmented training sets and rules with insufﬁcient statis-
                                                      tical support [Domingos, 1996]. Furthermore, each new rule
1  Introduction                                       is constructed in complete ignorance of the examples already
Classiﬁcation rule learning can be deﬁned as the process of, covered by the previously induced rules. If a bad rule has
given a set of training examples, ﬁnding a set of rules that can been introduced in the rule set, there is no chance of ﬁnding
be used for classiﬁcation or prediction. Almost all classiﬁca- a better rule for those examples (there is no backtracking).
tion rule learning algorithms belong to one of two families, In this work we present a new rule learning algorithm
namely separate-and-conquer and divide-and-conquer algo- named ROCCER, aimed to overcome such problems. The
rithms. The two families share a number of characteristics, main idea of the algorithm is to construct a convex hull in
most notably the assumption that the example space contains ROC space. We evaluate ROCCER on a broad set of bench-
large continuous regions of constant class membership. The mark domains from the UCI repository [Blake and Merz,
major differences are outlined below.                 1998] and compare it with other rule induction methods. The
  In the separate-and-conquer family of classiﬁcation rule paper is organized as follows. In Section 2 we discuss the
learning algorithms [Furnkranz,¨ 1999], the search procedure background related to this work. Section 3 presents our pro-
is generally an iterative greedy set-covering algorithm that on posed algorithm, and Section 4 contains the experimental
each iteration ﬁnds the best rule (according to a search cri- evaluation. In Section 5 we make some concluding remarks.
terion) and removes the covered examples. The process is
repeated on the remaining examples until all examples have 2 Related work
been covered or some stopping criterion has been met. In or- Several approaches have been proposed in the literature to
der to build a classiﬁer, the rules found in each iteration are overcome the fragmentation problem. [Liu et al., 1998] de-
gathered to form either an ordered rule list (a decision list) or couple the rule generation from the covering step. The ba-
an unordered rule set. In the former case the classiﬁcation is sic idea is to use an association rule algorithm to gather all
given by the ﬁrst rule in the list that ﬁres, while in the latter rules that predict the class attribute and also pass a mini-
case the predictions of rules that ﬁre are combined to predict mum quality criterion into the rule set. Although this ap-
a class.                                              proach might overcome some of the problems of the separate-
  This approach contrasts with the divide-and-conquer fam- and-conquer approach, and its performance was reported to
ily of learning algorithms [Quinlan, 1993], where a global outperform standard rule learning algorithms in some do-
classiﬁer is built following a top-down strategy by consec- mains, its main drawback is related to the number of gen-
utive reﬁnements of a partial theory. The result is gener- erated rules. Often they considerably outnumber the exam-
ally expressed as a decision tree, which completely divides ples, implying serious difﬁculties from a knowledge discov-
the instance space into non-overlapping axis-parallel hyper- ery point of view, since the understandability and usability of
rectangles.                                           the generated model would decrease and the risk of overﬁt-ting would increase. This idea has been extended in the Apri- class values, allowing us to deal with multi-class problems)
oriC and AprioriSD algorithms [Javanoski and Lavrac,ˇ 2001; and we perform a selection step based on the ROC curve.
Kavsekˇ et al., 2003] by adding an additional ﬁltering step to The basic idea is to only insert a rule in the rule list if the
remove some of the redundant rules. However, AprioriC still insertion leads to a point outside the current ROC convex hull
tends to build large rule sets and AprioriSD has been devel- (the current ROC convex hull is the upper convex hull of the
oped mainly for subgroup discovery.                   rules that are already in the rule list). Otherwise the rule is
  A different approach is to use weighted covering, which discarded. For a better understanding of how our algorithm
has been independently proposed by [Cohen and Singer, works, we ﬁrst describe it using an example.
1999] and [Weiss and Indurkhya, 2000]. Instead of com-  Rules are selected separately for each class, and are kept
pletely removing the examples covered by the best rule on in an ordered rule list. Let’s label the class we are selecting
each iteration, their weights are decreased, and in each itera- rules for positive; the label negative represents the (conjunc-
tion the covering algorithm concentrates on highly-weighted tion of) examples in the other class(es). First, we initialize
(i.e., infrequently covered) examples. [Lavracˇ et al., 2004] the rule list with a default rule, Rdefault, which always pre-
also discuss the use of weighted covering in a subgroup dis- dicts positive. The current ROC convex hull is formed then
covery context. Alternative methods to remove redundant by 2 points, (0,0) and (1,1) which means “ignore the de-
rules are based on pruning [Furnkranz¨ and Widmer, 1994; fault rule (classify everything as negative) or use the default
Cohen, 1995].                                         rule (classify everything as positive)”. We use fprRi, tprRi to
  Some authors propose the use of conﬁdence thresholds for refer to the rule Ri’s true and false positive rates, and tpri,
classiﬁcation. [Gamberger and Lavrac,ˇ 2000] include only fpri to refer to a point i in the ROC curve, representing the
rules with high conﬁdence in the rule set. The classiﬁer then corresponding rule list’s true and false positive rates. Sup-
refuses to classify a new instance if none of the rules cover pose now we are inserting a new rule R1. As the actual con-
it. [Ferri et al., 2004] extends this idea by retraining a new vex hull is formed only by the line segment (0,0) − (1,1),
classiﬁer on the unclassiﬁed examples.                R1 will only be inserted if the point formed by the rule’s
                                                      (fprR1,tprR1) is above the convex hull. Let’s say R1 is in-
3  The ROCCER      rule selection algorithm.          serted. The current convex hull is then updated, and con-
                                                      tains the points (0,0),(fpr1,tpr1),(1,1), where fpr1 = fprR1
Our approach relies on using ROC analysis for selecting rules and tpr1 = tprR1. This process is depicted in Figure 1.
instead of using a classical covering algorithm. Roughly
speaking, a ROC graph is a plot of the fraction of positive
examples misclassiﬁed — false positive rate (fpr) — on the x
axis against the fraction of positive examples correctly classi-
ﬁed — true positive rate (tpr) — on the y axis. It is possible to
plot in a ROC graph either a single rule, a classiﬁer (formed
by a rule set, or not) or even a partial classiﬁer (formed by a
subset of a rule set, for instance).
  For a threshold-based classiﬁer one can obtain several pairs
of points (fpri,tpri) by varying the threshold. If we trace a
line connecting those points we obtain a curve in the ROC Figure 1: R1 leads to a point outside the current convex hull
space that represents the behaviour of the classiﬁer over all (main diagonal) and is therefore inserted in the rule list.
possible choices of the respective threshold. In a rule learning
context, [Furnkranz¨ and Flach, 2005] show that rule learning Suppose we are now trying to insert a second rule R2. As
using a set covering approach can be seen as tracing a curve we have said before, in the standard set-covering approach
in ROC space. To see why, assume we have a empty rule list, the learning of a new rule does not take into account the rules
represented by the point (0,0) in ROC space. Adding a new already learnt. In our approach, we try to overcome this issue
rule R j to the rule list implies a shift to the point (fpr j,tpr j), using the ROC graph to analyze interactions among rules. We
where fpr j and tpr j is the tpr and fpr of the partial rule list (in- do this by comparing the rule we are trying to insert with the
terpreted as a decision list) containing all rules already learnt slopes of each of he line segments in the current convex hull
including R j. A curve can be traced by plotting all partial in the ROC graph. In our example, if the slope of the point
rule lists (fpr j,tpr j), for j varying from 0 to the total number formed by the origin and (fprR2,tprR2) is above the line from
n of rules in the ﬁnal rule list in the order they are learnt. A the origin to the point (fpr1,tpr1), we say that R2 “improves
ﬁnal default rule that always predicts the positive class can be in relation to” R1 and insert R2 in the rule list. This is similar
added at the end, connecting the point (fprn,tprn) to the point to if R2 had been learned before R1 in the set-covering ap-
(1,1).                                                proach. By comparing with the rules which are already in the
  Our approach is based on this observation, and the fact that rule list, our algorithm provides a kind of backtracking. This
the points which represent the optimum thresholds lie on the insertion will, of course, produce changes to the other points
upper convex hull of the ROC curve [Provost and Fawcett, in the ROC curve. The ﬁrst non-trivial point in the ROC curve
2001]. Rules come from a external larger set of rules (in our changes to (fprR2,tprR2). If R1 and R2 do not have any over-
implementation, we use the Apriori association rule learning lap, the second point will be (fprR2 + fprR1,tprR2 + tprR2).
algorithm, ﬁxing the head of the rules to each of the possible If R1 and R2 do have some overlap, however, we should dis-count the examples covered by both rules to calculate the sec- does not necessarily lead to a convex curve. The insertion
ond point.                                            of a new rule in the rule list may introduce concavities be-
  If R2 is not inserted at the ﬁrst iteration, we proceed by fore or after the point of insertion. If concavities occur after
comparing with the remaining line segments in the ROC con- an insertion, the inserted rule covers examples originally cov-
vex hull. Before we compare with the next line segment, we ered by subsequent rules, which decreases the latter’s preci-
update R2’s fpr and tpr by removing the examples covered by sion. In this case, we remove the rules where the concavity
R1 (the examples which evaluate to true for R1’s antecedent). occurs. Alternatively, if the concavity occurs before the in-
In fact, we are “interpreting” R2 as ¬R1 ∧ R2. If the updated sertion point, both rules share a region where they misclas-
position of R2 is above the line from R1 to Rdefault, R2 is sify some examples. In this case, it is unreasonable to use
inserted in the rule list after R1. This process is depicted in the partial rule list including the ﬁrst rule but excluding the
Figure 2. Otherwise (since no further rules remain) R2 is dis- second, because the corresponding ROC point is under the
carded. The pseudo-code of this algorithm is shown in Algo- convex hull. Therefore, we construct the disjunction of the
rithm 1.                                              two rules and treat it as a single rule.
                                                        In our implementation, rules presented to ROCCER are ini-
                                                      tially ordered, using the Euclidean distance to the point (0,1)
                                                      in the ROC space. However, due to the possibility to remove
                                                      a selected rule and the (implicit) backtracking, the order de-
                                                      pendence in selecting rules is lower than a decision list in
                                                      inducing rules. Experiments with another orderings would be
                                                      an interesting issue for further work, though.
                                                        This concludes the description of the training phase. For
                                                      classiﬁcation, we also use a ROC-based method. Bayes’ the-
                                                      orem states that the odds that a classiﬁer correctly classiﬁes
 (a) R2 does not improve the (b) ...but it does when com- an instance (posterior odds) is given by the likelihood ratio
 convex hull when compared to pared with Rdefault.    times the odds of the instance being of the predicted class
 R1...                                                (prior odds). In ROC space, the likelihood ratio can be in-
                                                      terpreted as tpr/fpr. Recall that we selected rules separately
      Figure 2: Finding the right point to insert R2. for each class. Thus, we have a ROC convex hull for each
                                                      class. To classify a new instance we also consider each class
                                                      separately, and, for each class, we determine the ﬁrst rule that
                                                      ﬁres in the respective ROC convex hull. This rule has an asso-
 Algorithm 1: The ROCCER algorithm.
                                                      ciated (tpr,fpr) in the ROC curve, which yields a likelihood
  Data: RSin: A (large) rule set for a given class    ratio. The posterior odds is then converted back to a prob-
  Result: RSout : A (smaller) rule list containing the ability (for ranking), or we select the class with maximum
         selected rules                               posterior odds (for classiﬁcation).
  RSout = {Rdefault};
  foreach Rule ∈ RSin do                              4   Experimental evaluation
     TryToInsertRule(Rule)
  endfch
  return RSout                                          In order to empirically evaluate our proposed approach, we
                                                      performed a experimental evaluation using 16 data sets from
  procedure TryToInsertRule(Rule)                     UCI [Blake and Merz, 1998]. We used only data sets with-
    RuleToCompare = ﬁrst rule in RSout ;              out missing values, as Apriori (the association rule algorithm
    repeat
                                                      we use to generate the rules for subsequent selection by ROC-
       if Rule’s (fpr,tpr) is outside convex hull then
                                                      CER) can’t handle them. Table 1 summarizes the data sets em-
          Insert Rule into RS before RuleToCompare
                          out                         ployed in this study. It shows, for each data set, the number of
       else
                                                      attributes (#Attrs), the number of examples (#Examples), and
          /*  shift to a new origin      */
          Remove all the examples from Rule which are percentage of examples in the majority class (%MajClass) –
          covered by RuleToCompare;                   although ROCCER can handle more than two classes, in order
          Actualize Rule’s (fpr,tpr);                 to calculate AUC values we restricted our experiments to two-
                                                      class problems. For data sets having more than two classes,
          RuleToCompare = next rule in RSout
       endif                                          we chose the class with fewer examples as the positive class,
                                                      and collapsed the remaining classes as the negative.
    until RuleToCompare 6= R   ;
                          default                       ROCCER’s results were compared with those obtained by
    if Rule is not inserted then                      the following rule learning systems:
       Discard Rule
    endif                                             CN2  This algorithm is a classical implementation of the
                                                          separate-and-conquer rule learning family. In its ﬁrst
                                                          version [Clark and Niblett, 1989] CN2 induces a deci-
  Due to overlapping coverage among rules, this process   sion list using entropy as a search heuristic. It has later  #     Data set   # Attrs # Examples  %MajClass      shown in Table 2. We also perform a two-tailed Dunett mul-
  1     Breast        10          683     65.00       tiple comparison with a control procedure1 using ROCCER
  2      Bupa           7         345     57.98       as control (the other results against ROCCER). Cells having
  3     E.Coli          8         336     89.58       AUC values statistically better than ROCCER are represented
  4      Flag         29          194     91.24       in dark gray while light gray is used to represent cells statis-
  5     German        21         1000     70.00       tically worse than ROCCER, both with 95% conﬁdence level.
  6      Glass        10          214     92.07         Table 2 shows relatively few statistically signiﬁcant differ-
  7    Haberman         4         306     73.53       ences. Comparing against C4.5, ROCCER achieved 4 wins
  8      Heart        14          270     55.55       and 1 loss. This is the same score if we compare ROCCER
  9   Ionosphere      34          351     64.10       against Ripper. Against Slipper, the results are 5 wins and no
 10    Kr-vs-Kp       37         3196     52.22       losses. A comparison of ROCCER against C4.5 without prun-
 11     Letter-a      17        20000     96.06       ing, CN2 and CN2 ordered yield 2 losses and no wins. We
 12   New-thyroid       6         215     83.72       believe these two losses are due to the high degree of class
 13     Nursery         9       12960     97.45       skew in those two datasets (they are the most skewed in our
 14      Pima           9         768     65.10       study). In order to allow Apriori to ﬁnd rules for both classes
 15    Satimage       37         6435     90.27       in these domains, the support parameter used in Apriori is
 16     Vehicle       19          846     76.48       very low. In these cases we have both a small number of
                                                      rules generated for the minority class and a large number of
     Table 1: UCI data sets used in our experiments.  rules generated for the majority class. Further improvements
                                                      should be made in ROCCER to cope with such situations (for
                                                      instance, it would be interesting to introduce different min-
    been modiﬁed to incorporate the induction of unordered imum support for each class). Taking into account all the
    rule sets and Laplace error correction as evaluation func- rule learning algorithms, the score is 12 wins and 9 losses (all
    tion [Clark and Boswell, 1991].                   losses are concentrated in the two skewed domains, though).
Ripper [Cohen, 1995] proposed Ripper in the Incremental Comparing with all the generated rules, ROCCER produced 6
    Reduced Error Pruning (IREP) [Furnkranz¨ and Widmer, wins and no losses. We also compute AUC value on selecting
    1994] context. It has features such as error-based prun- k (the same number as ROCCER)random rules and rules with
    ing and an MDL-based heuristic for determining how higher individual AUC values. Due to lack of space results
    many rules should be learned.                     are not shown in this paper, but they are in most of the cases
Slipper This algorithm is a further improvement of Ripper signiﬁcantly worst and never better than ROCCER. This indi-
    which uses a weighted set-covering approach [Cohen cates that ROCCER’s selection procedure is responsible for a
    and Singer, 1999].                                gain of performance over all the presented rules.
                                                        The good results with both versions of CN2, and the rel-
     [            ]
C4.5  Quinlan, 1993 ’s C4.5 is almost a standard in empiri- atively poor AUC ﬁgures for Ripper and Slipper, are worth
    cal comparison of symbolic learning algorithms. It is a noticing, and may be explained by the absence of pruning
    member of divide-and-conquer family. C4.5 uses infor- mechanisms. It has already been reported in the literature that
    mation gain as quality measure to build a decision tree non-pruned trees are better for probability prediction and thus
    and a post-pruning step based on error reduction. We produce higher AUC values [Provost and Domingos, 2003].
    can consider each branch in a decision tree as a rule. It might be expected that a similar phenomenon also would
Ripper and Slipper were used with -a option to generate rules occur with algorithms from the separate-and-conquer family.
for both classes. CN2 was used in its two versions, or- Ripper and Slipper are – at least conceptually – similar to
dered (CN2OR) and unordered (CN2). We also evaluated  CN2 but incorporate, respectively, rule pruning and weighted
both pruned (C45) and non-pruned (C45NP) trees induced coverage.
by C4.5. All other parameters were set to default values. In Table 3 presents the average size (in number of rules) of
order to calculate the AUC values we estimated probabilities the rule sets for each algorithm. Size 0 means that the clas-
of each rule using Laplace correction. For the unordered ver- siﬁer is formed only by the default rule. The picture here
sion of CN2, probabilities were estimated using all ﬁred rules. is more clear. Apart from some exceptions, ROCCER pro-
AUC values were estimated using the trapezoidal rule. We duces smaller rule sets than C4.5 without pruning, CN2 (both
used [Borgelt and Kruse, 2002]’s implementation of Apriori ordered and unordered), and Slipper. On the other hand,
to generate the large rule sets used by ROCCER. The parame- Ripper produced (signiﬁcantly) smaller rule sets in 7 out of
ters were set to 50% of conﬁdence and 1/3 of the percentage 16 domains, and there were 8 draws and 1 win. A further
of minority class as support. For ROCCER, the probabilities investigation involving the data sets where most of the al-
were estimated by the posterior odds (described in Section 3). gorithms produced smaller rule sets than ROCCER (Breast,
We also compare with a bagging of all rules generated by Heart, Ionosphere and Kr-vs-kp) might produce some in-
Apriori.
  We ran the experiments using 10-fold stratiﬁed cross-  1Multiple comparison is used to adjust the observed signiﬁcance
validation. The experiment is paired, i.e., all inducers were level for the fact that multiple comparisons are made. If we use a
given the same training and test ﬁles. The averaged AUC t-test, and as each comparison has up to 5% Type I error, then the
values (and respective standard deviations in brackets) are Type I error rate over the entire group can be much higher than 5%.   #    ROCCER      C45         C45NP       CN2         CN2OR       Ripper      Slipper      All
   1   98.63(1.88)  97.76(1.51) 98.39(1.30) 99.26(0.81) 99.13(0.92) 98.72(1.38) 99.24 (0.57) 99.07(0.87)
   2   65.30(7.93)  62.14(9.91) 57.44(11.92) 62.74(8.85) 62.21(8.11) 69.10(7.78) 59.84 (6.44) 65.38(10.63)
   3   90.31(11.56) 50.00(0.00) 90.06(7.75) 90.17(6.90) 85.15(11.38) 61.86(25.49) 74.78 (15.94) 16.50(10.43)
   4   61.83(24.14) 50.00(0.00) 68.68(17.22) 53.22(24.12) 42.78(24.43) 45.28(14.93) 52.35 (7.44) 62.11(23.96)
   5   72.08(6.02)  71.43(5.89) 67.71(4.12) 75.25(5.38) 70.90(4.70) 64.02(13.62) 71.32 (6.20) 73.37(4.84)
   6   79.45(12.98) 50.00(0.00) 81.50(12.65) 73.74(15.40) 79.64(13.24) 49.75(0.79) 50.00 (2.36) 35.62(18.93)
   7   66.41(11.54) 55.84(6.14) 64.33(13.58) 59.83(9.87) 59.28(10.13) 57.45(3.85) 50.40 (11.14) 66.52(5.94)
   8   85.78(8.43)  84.81(6.57) 81.11(7.91) 83.61(6.89) 82.25(6.59) 84.89(7.68) 84.03 (6.36) 90.72(6.28)
   9   94.18(4.49)  86.09(9.97) 90.91(6.03) 96.23(2.97) 92.18(7.54) 92.06(5.94) 93.95 (6.82) 90.14(5.32)
   10  99.35(0.36)  99.85(0.20) 99.86(0.20) 99.85(0.16) 99.91(0.17) 99.85(0.21) 99.91 (0.09) 92.67(1.60)
   11  96.08(0.52)  95.49(1.96) 99.33(0.46) 99.34(0.28) 99.44(0.63) 97.27(1.86) 98.82(0.44)  92.45(1.54)
   12  98.40(1.70)  87.85(10.43) 97.50(3.39) 99.14(1.19) 98.43(2.58) 94.95(9.94) 99.12 (1.25) 89.97(7.75)
   13  97.85(0.44)  99.42(0.14) 99.74(0.13) 100.00(0.00) 99.99(0.01) 99.43(0.26) 94.40(1.59) 97.79(0.65)
   14  70.68(5.09)  72.07(4.42) 72.60(6.50) 70.96(4.62) 71.97(5.44) 68.07(9.46) 70.02 (5.97) 70.37(5.01)
   15  89.39(2.38)  90.15(1.70) 91.31(1.32) 91.48(1.45) 91.48(0.90) 86.83(3.94) 89.06 (1.98) 79.62(4.95)
   16  96.42(1.47)  94.76(3.00) 96.99(1.44) 97.38(2.05) 96.49(2.41) 95.01(2.22) 93.99 (3.13) 93.37(3.05)
  Avg  85.13        77.98       84.84       84.51       83.2        79.03       80.08        75.98

Table 2: AUC values estimated with 10-fold cross-validation on the 16 UCI data sets described in Table 1, obtained with
ROCCER, C4.5, C4.5 without pruning, CN2 unordered, CN2 ordered (i.e., learning decision lists), Ripper, Slipper, and bagging
all rules found by Apriori. Numbers between brackets indicate standard deviations; dark gray indicates signiﬁcant wins over
ROCCER, and light gray indicates signiﬁcant losses against ROCCER.

sights for improvements to our approach.              plexity is O(n2), where n is the number of rules used as in-
  We conclude from Tables 2 and 3 that ROCCER combines, put. However, on average, the number of iterations is Ω(mn),
in a sense, the best of both worlds: it achieves AUC values where m is the number of rules selected by the algorithm.
that are comparable to those of unpruned decision trees and Due to lack of space we will not report runtime statistics for
CN2, but without the large number of rules induced by those all data sets. For most datasets the runtime ranges from a few
systems. Finally, Table 4 presents statistics of the individual seconds to 10 minutes per fold (on a Pentium 4 2.4Ghz ma-
rules that comprise the rule sets, which demonstrates another chine with 512MB of RAM). For these datasets, the number
advantage of the ROCCER approach. Support ranges from 0 of rules used as input is up to 1,000. For some domains (Kr-
to 100% and is a measure of the relative coverage of each vs-kp and Satimage) the number of rules generated by Apriori
rule. Weighted relative accuracy (WRAcc) ranges from 0 to is very high (more than 40,000). The runtime in these cases
0.25 and assesses the signiﬁcance of a rule, in terms of dif- is on average nearly 1.5 hours per fold.
ference between the observed and expected numbers of true
positives. The odds ratio ranges from 0 to ∞ and is a mea- 5 Conclusion
sure of strength of association. It can clearly be seen that the
                                                      We presented ROCCER, a rule selection algorithm based on
rules selected by ROCCER have considerably higher values
for all measures. This means that the rules are more mean- ROC analysis. ROCCER operates by selecting rules from a
ingful in isolation, without reference to the other rules in the larger set of rules by maintaining a ROC convex hull in the
                                                      ROC space. Featuers of ROCCER’s approach include implicit
rule set. Thus, ROCCER successfully overcomes one of the
main drawbacks of the set-covering approach.          backtracking and discovery of pairs of related rules. Exper-
                                                      imental results demonstrate AUC values that are compatible
                                                      with the best probability predictors such as unpruned deci-
           Support (%)    WRAcc        Odds Ratio     sion trees, achieved with considerably smaller rule sets. The
 ROCCER    13.67 (13.89) 0.0355 (0.018) 154.02 (337.33)
                                                      rules that compose the rule sets induced by ROCCER have
 C4.5       3.73 (6.01) 0.0094 (0.013) 44.55 (77.04)
 C4.5NP     1.19 (1.06) 0.0030 (0.003) 31.90 (56.68)  also higher values of support, weighted relative accuracy and
 CN2        3.90 (2.52) 0.0110 (0.009) 98.73 (138.64) odds ratio, and thus are more meaningful as individual rules.
 CN2OR      3.10 (2.18) 0.0085 (0.007) 95.07 (192.94)
 Ripper     5.96 (5.34) 0.0184 (0.012) 74.08 (103.88) Acknowledgments
 Slipper    1.92 (1.58) 0.0060 (0.006) 33.86 (50.41)
 All        8.07 (5.06) 0.0114 (0.014) 67.39 (111.72) This work is partially supported by the Brazilian Research
                                                      Concil CAPES (Process no. BEX1340/03-8), and was car-
                                                      ried out while the ﬁrst author was visiting the University of
Table 4: Support, weighted relative accuracy and odds ratio
                                                      Bristol.
averaged over all learned rules.

  A ﬁnal word should be said regarding computational com- References
plexity. ROCCER is, of course, computationally more expen- [Blake and Merz, 1998] C. L. Blake and C. J. Merz.
sive than the other algorithms. In the worst case, the com- UCI repository of machine  learning databases,