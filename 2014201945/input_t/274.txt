    Improving Speech Recognition on a Mobile Robot Platform through the use of 
                                          Top-Down Visual Queues 

                           Robert J, Ross, R.P.S. O'Donoghue and G.M.P. O'Hare 


                        Abstract                               ence. On a practical level the simplistic use of hand-held 
                                                               or head-mounted microphones can dramatically improve per•
     In many real-world environments, Automatic 
                                                               formance. Another approach uses dialog systems based 
     Speech Recognition (ASR) technologies fail to pro•
                                                               on finite state grammar models or frame systems to antic•
     vide adequate performance for applications such as 
                                                               ipate user utterances from dialog constructs [Matsui, 1999; 
     human robot dialog. Despite substantial evidence 
                                                               Bischoff, 2001]. Although these approaches do offer a sub•
     that speech recognition in humans is performed in 
                                                               stantial improvement on a purely black-box approach to ASR, 
     a top-down as well as bottom-up manner, ASR sys•
                                                               they impose tight constraints on the usability and flexibility of 
     tems typically fail to capitalize on this, instead rely•
                                                               the system as a whole. 
     ing on a purely statistical, bottom up methodology. 
                                                                 Naturally the partial failure of ASR technology in a noisy 
     In this paper we advocate the use of a knowledge 
                                                               environment does not only pose problems for the mobile 
     based approach to improving ASR in domains such 
                                                               robotics community. Problems with mono-modal speech 
     as mobile robotics. A simple implementation is 
                                                               recognition systems have been recognized for many years, 
     presented, which uses the visual recognition of ob•
                                                               and many attempts have been made to improve recogni•
     jects in a robot's environment to increase the prob•
                                                               tion quality through the use of low-level visual information. 
     ability that words and sentences related to these ob•
                                                               In [Chibelushi et al, 2002] Chibelushi, Deravi and Mason 
     jects will be recognized. 
                                                               present a comprehensive review of these approaches. Algo•
                                                               rithms providing sources of visual information center mostly 
1 Motivation & Proposal                                        on those which perform lip-reading or the analysis of facial 
Through the mapping of an acoustic signal to a string of       gesture. The techniques discussed in the review paper can 
words ASR systems are a key tool in the control of mobile      be characterized as being low-level or data driven in nature, 
devices such as robots and PDAs, particularly in cases where   with a division being drawn on whether information should 
manual control is not appropriate or feasible. However de•     be combined before integration into a model, or whether two 
spite the substantial improvements in ASR reliability which    separate low-level models should first be formed before a 
have been made in the last ten years, results can still be very general abstraction is made. 
poor in noisy environments. Most ASR systems are built           The trend towards integrating low level visual and audio 
around a statistical, data driven architecture which fails to  information reflects clear evidence of this phenonimon in hu•
capitalize on sources of information other than the input au•  mans. The McGurk effect is perhaps the best known example 
dio signal and static vocabulary. Many hybrid architectures    of this, where the audio presentation of the sound 'BA' along 
utilizing lip-reading algorithms have emerged in recent years  with the visual lip movements of the sound 'GA' typically 
[Chibelushi et al., 2002], but these are dependent on a user   results in a listener perceiving the sound 'DA [McGurk and 
directly facing a communication interface.                     MacDonald, 1976]. However there is also a growing body 
   ASR is becoming an increasingly more important tool in      of evidence that indicates that humans use high level context 
the development of service and mobile robots. This can gen•    and semantic effects to improve our speech recognition per•
erally be accredited to the ease of use a natural language in• formance [Tanenhaus et al., 1995; Simpson, 1994]. Further 
terface should bring to human-machine interactions. How•       more it is commonly observed that in conversation we will 
ever in practice the speech recognition systems available fail often reference particular themes, and discuss objects in his 
to produce sufficient accuracy for natural interactions. These or her local environment. 
failings result from a wide range of environmental distortions   Inspired by this evidence of context and high level seman•
and noise, including a) interference from the robot's drive    tic priming of speech recognition in humans, we propose the 
systems, b) reverberations c) multiple user interference (the  improvement of ASR systems on mobile robots through the 
so-called cocktail party effect).                              use of a top down context priming model, rather than relying 
  Users of ASR in the robotics community have generally        on workarounds based on strict dialog systems or user held 
taken two approaches to counteracting sources of interfer•     microphones. The initial model described below is based on 


POSTER PAPERS                                                                                                       1557  the premise that users commonly discuss objects in their local language Teanga [Rooney, 2001]. The internal design of the 
 environment. Specifically our model proposes that upon the    three main agent types will now be outlined, followed by a 
 visual recognition of an object in the environment, the prob• discussion of their typical interactions and usage. 
 ability of recognition of words associated with that object 
                                                                 Low Level Speech Recognition Agents - These agents 
 should be increased. This is achieved through direct com•
                                                               work at the level of traditional ASR systems to convert au•
 munication between software agents responsible for visual 
                                                               dio signal from the outside world to simple strings of text or 
 processing and speech recognition. Although such a model 
                                                               n-grams. A number of low level speech agents have been 
 is simplistic it acts as a stepping stone towards the further 
                                                               built using a range of speech recognition toolkits. Toolkits 
 improvement of ASR by context effects. 
                                                               employed included both well-known systems like Sphinx and 
                                                               Via Voice, as well as a less popular but potentially more pow•
 2 Implementation                                              erful and open hybrid systems [Carson-Berndsen and Walsh, 
 The research presented here has been conducted as part of the 2000]. 
 SAID (Speaking Autonomous Intelligent Devices) Project in       A problem encountered in using the more popular speech 
 University College Dublin [Ross et al., 2002]. The focus of   recognition toolkits, is that most operate using either a 13NF 
 this project is to examine how the two very different com•    grammar of predicted sentences, or in a completely free-form 
 putational paradigms of speech recognition and autonomous     dictation mode. A middle ground where the adjustment of 
 agents can benefit from each other in the production of intel• probabilities on words and sentences was not achieved easily. 
 ligent speech enabled devices. The model developed here has     Visual Object Recognition Agent - Each robot has on on 
 therefore been constructed for a complete mobile robot plat•  board color CCD camera. This camera is controlled by low 
 form, rather than having been implemented as a stand-alone    level software and drivers to provide video footage of the out•
 algorithm.                                                    side world. A Visual object recognition agent was built which 
   Although the model presented has been developed for a       employs color segmentation and edge-based feature detection 
mobile robot with a vision source, the basic principle is not  algorithms to scan for key objects in the robot's field of view. 
hard-wired to the mobile robot domain. The principle can be    The visual recognition agent passively scans the environment 
applied anywhere where there is a source of high level in•     for pre-defined objects such as chairs, colored balls, and large 
formation which can be used to dynamically prime a speech      office features. Upon detection of any such object this visual 
recognition systems. Specificly the Speech Priming agent       detection agent communicates its observation to any agents 
discussed below can be applied on any mobile device which      which have previously requested to be kept informed of its 
has access to spatial knowledge of the outside world (whether  findings. 
acquired through passive visual detection or not). 
                                                                 Speech Recognition Priming Agent - The third agent type 
2.1 Platform                                                   acts as the key filtering mechanism between the visual recog•
Experiments are carried out using a team of Nomaidc Scout      nition of objects and the adjustment of word recognition prob•
II robots, refitted with on-board computers, vision, and sound abilities in the lower-level speech recognition agents. 
systems. The control system for the robot is provided through    The agent employs a semantic network type structure to 
MARC, an experimental Multi-Agent System based architec•       produce strings and groups of probable words for recognition 
ture [Ross, 2002]. This architecture like many social robot    based on the priority of identified external objects. Speci•
architectures views individual robots as intelligent agents in ficly, the network has been prepared with details of objects 
a social community. However MARC is original in that all       commonly found in an office environment, along with asso•
aspects of any one robot's control, from high level planning   ciated verbs, prepositions and pseudonyms. When informed 
and user modeling to low level movement and reactive control   that there is a package in the robots proximity, this agent can 
are encapsulated through a community of intentional deliber•   provide a list of words and phrases which are likely to be spo•
ative agents (See [Wooldridge, 2000] for a good introduction   ken in connection with such a package e.g. deliver, box. The 
to the theoretical models, and [Collier, 2001] for details of  model can be altered to produce recommendations for varying 
a concrete development and runtime environment for these       periods and priorities based on relative importance of objects 
agents.). Although the robot control architecture contains a   and the robots current actions respectively. 
large number of diverse agents, discussion here will be lim•     A typical usage scenario involves the Speech Recognition 
ited to those agents specificly connected with speech recog•   Priming Agent making an initial request with the Visual Ob•
nition with visual priming.                                   ject Recognizer for reports on any key objects recognized in 
                                                               the environment. If and when provided with reports of objects 
2.2 The Agents                                                 in the robots vicinity, the Speech Recognition Priming Agent 
The speech recognition and visual priming system consists      will build up a revised list of words which it might expect 
of a number of agents which provide a) basic speech recogni•   a user to utter. The Speech Recognition Priming Agent will 
tion, b) passive visual object recognition and c) speech recog• then attempt to inform the speech recognition agent of this 
nition priming through the use of a simple semantic model.     revised list. The relevant speech recognition agent can then 
Despite the fact that these individual agents are internally   choose to take note of these values in speech recognition, or 
highly data driven, coupling between the agents is loose with  ignore them as it sees fit. 
all inter-agent communication via the agent communication        Based on the advice from the Speech Priming Agent, the 


1558                                                                                                   POSTER PAPERS Speech Recognition Agents can then interprete the incoming     Acknowledgements 
sound signals as appropriate, providing strings of recognized  We gratefully acknowledge the support of Enterprise Ireland 
text to any agents which have expressed an interest in its find• through grant No. IF/2001/02, SAID. 
ings e.g. social communication agents, command interpreter 
agents.                                                        References 
                                                               [Bischoff, 2001] Rainer Bischoff. Hermes - a humanoid ex•
2.3 Test Scenarios 
                                                                 perimental robot for mobile manipulation and exploration 
Experiments conducted are based on a user and robot situ•         services. IEEE International Conference on Robotics and 
ated in a room which has been furnished with a number of          Automation, apr 2001. Video Abstract. 
objects, some of which the robot is capable of visually rec•   [Carson-Berndsen and Walsh, 2000] Julie Carson-Berndsen 
ognizing. The user then issues a number of commands to the        and Michael Walsh. Interpreting multilinear representa•
robot. This set of commands is composed of instructions and       tions of speech. In Proceeedings of the 8th Australian 
questions about objects in the room. The command set in•          Conference on Speech Science and Technology, 2000. 
cludes both well formed commands and a number of garbled 
commands which are phonetically very similar to the well       [Chibelushi et al, 2002] C.C. Chibelushi, F. Deravi, and 
formed command. These incorrectly formed commands are            J.S.D. Mason. A review of speech-based bimodal recogni•
typical of slight mispronunciations, or environmental distor•     tion. IEEE Transactions on Multimedia, 4(l):23-37, mar 
tions. Badly pronounced utterances range from slight mis•         2002. 
pronunciations of nouns and verbs, to the replacement of one   [Collier, 2001] Rem W. Collier. Agent Factory: A Frame-
word with a different but proper word as with                     work for hte Engineering of Agent Oriented Applications. 
                 Where is the mall                                PhD thesis, University College Dublin, 2001. 
rather than                                                    [Herzog and Wazinski, 1994] G. Herzog and R Wazinski. 
                 Where is the ball                                Visual TRAnslator: linking perceptions and natural lan•
   In initial tests the utilization of the context priming model  guage descriptions. Artificial Intelligence Review, 8(2-
clearly produces more reliable results than those produced        3):175-187, 1994. 
when the vision system is disjoint from the speech recogni•    LMatsui, 1999] Toshihiro Matsui. Integrated natural spoken 
tion components.                                                 dialogue system of jijo-2 mobile robot for office services. 
                                                                  In AAAI/IAAJ, pages 621-627, 1999. 
3 Related Research                                             [McGurk and MacDonald, 1976] Harry McGurk and John 
                                                                  MacDonald. Hearing lips and seeing voices. Nature, 
The integration of audio and vision is becoming more pop•
                                                                  264:746-748, dec 1976. 
ular in recent years and is being approached from a number 
of angles. In addition to the low-level integration of audio   [Rooney, 2001] C.F.B. Rooney. A formal syntax & se•
and visual information as discussed earlier [Chibelushi et al,   mantics for teanga. Technical report, University College 
2002], related research on the integration of audio and vi•       Dublin, 2001. 
sual information includes Deb Roy's work on the learning of    [Ross et al, 2002] Robert J. Ross, Bryan McElency, Robert 
words from sights and sounds [Roy, 1999]., and the genera•        Kelly, Tarek Abu-Amer, Michael Walsh, Julie Carson-
tion of natural language from a visual representation [Herzog     Berndsen, and Gregory M.R O'Hare. Speaking au•
and Wazinski, 1994].                                             tonomous intelligent devices. In In Proc. 13th Irish 
                                                                  Conference on Artificial Intelligence & Cognitive Science 
4 Initial Conclusions & Future Work                               (AICS 02), 2002. 
                                                               [Ross, 2002] Robert Ross. Marc - the multi-agent robot con•
Although speech recognition systems can often produce in-         trol architecture. Technical Report, Oct 2002. 
accurate results in real-world environments, accuracy in the 
mobile robot domain can be improved through a knowledge        [Roy, 1999] Deb Kumar Roy. Learning Words from Sights 
based priming of speech systems. The model presented here        and Sounds: A Computational Model. PhD thesis, MIT, 
was based on the premise that users often discuss objects in      1999. 
their local environment. Through the visual recognition of     [Simpson, 1994] Greg B. Simpson. Context and the process•
objects ASR systems were 'primed' for specific topics of con•     ing of ambiguous words. In Handbook of Psycholinguis-
versation. Such a technique is novel and should be contrasted     tics., chapter 10, pages 359-374. Academic Press, 1994. 
with low level bi-modal speech recognition systems.            [Tanenhaus et al., 1995] M.K. Tanenhaus, M.J. Spivey-
  Although the model presented here is simple, the principle      Knowlton, K.M. Eberhard, and J.E. Sedivy. Integration of 
of using high level data sources to improve ASR performance       visual and linguistic information in spoken language com•
can easily be extended to other data sources and platforms.      prehension. Science, 268:1632-1634, 1995. 
Taking advantage of user and task domain modeling is an ob•
                                                               [Wooldridge, 2000] Michael Wooldridge. Reasoning about 
vious next step. With this in mind, immediate future work 
includes the expansion of the semantic model, and the addi•      Rational Agents. The MIT Press: Cambridge, MA, USA, 
tion of general task domains.                                     2000. 


POSTER PAPERS                                                                                                        1559 