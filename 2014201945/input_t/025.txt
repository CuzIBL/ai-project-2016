                 Body Movement Analysis of Human-Robot Interaction 

                Takayuki Kanda, Hiroshi Ishiguro, Michita Imai, and Tetsuo Ono 
                      ATR Intelligent Robotics & Communication Laboratories 
                  2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan 
                                           kanda@atr.co.jp 


                     Abstract                        oped. Furthermore, various communicative behaviors 
                                                     using a robot's body have been discovered, such as a 
    This paper presents a method for analyzing hu•
                                                     joint-attention mechanism [Scassellati et al., 2000]. 
    man-robot interaetion by body movements. Fu•
                                                       On the other hand, methods of analyzing social robots, 
    ture intelligent robots will communicate with 
                                                     especially with respect to human-robot interaction, are 
    humans and perform physical and communicative 
                                                     still lacking. To effectively develop any systems in general, 
    tasks to participate in daily life. A human-like 
                                                     it is essential to measure the systems' performance. For 
    body will provide an abundance of non-verbal 
                                                     example, algorithms arc compared with respect to time 
    information and enable us to smoothly commu•
                                                     and memory, and mechanical systems are evaluated by 
    nicate with the robot. To achieve this, we have 
                                                     speed and accuracy. Without analyzing current perform•
    developed a humanoid robot that autonomously 
                                                     ance, we cannot argue advantages and problems. For so•
    interacts with humans by speaking and making 
                                                     cial robots, no analysis method has yet been established, 
    gestures. It is used as a testbed for studying em•
                                                     thus, it is vital to determine what types of measurements 
    bodied communication. Our strategy is to analyze 
                                                     we can apply. Although questionnaire-based methods have 
    human-robot interaction in terms of body move•
                                                     been often used, they are rather subjective, static and 
    ments using a motion capturing system, which 
                                                     obtrusive (that is, we would interrupt the interaction when 
    allows us to measure the body movements in de•
                                                     we apply a questionnaire). Less commonly, human be•
    tail. We have performed experiments to compare 
                                                     haviors are employed for this purpose, such as distance 
    the body movements with subjective impressions 
                                                     [Hall, 1966], attitude [Reeves and Nass, 1996], eye gaze 
    of the robot. The results reveal the importance of 
                                                     (often used in psychology), and synchronized behaviors 
    well-coordinated behaviors and suggest a new 
                                                     [Ono et aL, 2001]. Although those methods are more dif•
    analytical approach to human-robot interaction. 
                                                     ficult to apply, the results are more objective and dynamic. 
                                                     Sometimes, interactive systems observe human behaviors 
1 Introduction                                       for synthesizing behaviors [Jebara and Pentland, 1999]. 
Over the past several years, many humanoid robots such as However, they are still fragments rather than a systematic 
Honda's [Hirai et aL, 1998] have been developed. We  analysis method applicable for human-robot interaction. 
believe that in the not-too-distant future humanoid robots In this paper, we present our exploratory analysis of 
will interact with humans in our daily life. Their hu• human-robot interaction. Our approach is to measure the 
man-like bodies enable humans to intuitively understand body movement interaction between a humanoid robot and 
their gestures and cause people to unconsciously behave as humans, and compare the results with traditional subjective 
if they were communicating with humans [Kanda et al, evaluation. We have developed an interactive humanoid 
2002a]. That is, if a humanoid robot effectively uses its robot that has a human-like body as the testbed of this 
body, people will naturally communicate with it. This embodied communication. Furthermore, many interactive 
could allow robots to perform communicative tasks in behaviors have been implemented. It encourages people to 
human society such as route guides.                  treat the robot as a human child. We employ a motion cap•
  Several researchers have investigated about social re• turing system for measuring time and space accurately. 
lationships between humans and robots. For example, 
Kismet was developed for studying early caregiver-infant 2 An Interactive Humanoid Robot 
interaction [Breazeal, 2001]. Also, a robot that stands in a 
line [Nakauchi et al., 2002] and a robot that talks with 2.1 Hardware 
multiple persons [Nakadai et al. , 2001] have been devel-
                                                     Figures 1 and 2 display an interactive humanoid robot 
                                                     "Robovie," which is characterized by its human-like body 
    * This research was supported in part by the Telecommunica• expression and various sensors. The human-like body 
   tions Advancement Organization of Japan. 


COGNITIVE ROBOTICS                                                                                     177  consists of eyes, a head and arms, which generate the 
 complex body movements required for communication. 
 The various sensors, such as auditory, tactile, ultrasonic, 
 and visual, enable it to behave autonomously and to in•
 teract with humans. Furthermore, the robot satisfies me•
 chanical requirements of autonomy. It includes all com•
 putational resources needed for processing the sensory 
 data and for generating behaviors. It can continually op•
 erate for four hours with its battery power supply. 

 2.2 Software 
 Using its body and sensors, the robot performs diverse 
 interactive behaviors with humans. Each behavior is gen-
 crated by a situated module, each of which consists of 
 communicative units. This implementation is based on a 
 constructive approach [Kanda et al, 2002b]: "combining 
 as many simple behavior modules (situated modules) as 
 possible." We believe that the complexity of the relations            toward the robot, it recognizes the human's action and reacts 
 among appropriate behaviors enriches the interaction and              by using reactive transition and reactive modules; that is, 
 creates perceived intelligence of the robot.                          some of the situated modules can catch the human's initiating 
                                                                       behaviors and interrupt its operations to react to them (as 
 Communicative Unit                                                    shown in Fig. 4: the second module TURN). 
 Previous works in cognitive science and psychology have                  A situated module consists of precondition, indication, 
 highlighted the importance of eye contact and arm                     and recognition parts (Fig. 3). By executing the precon•
 movement in communication. Communicative units are                    dition, the robot checks whether the situated module is in 
designed based on such knowledge to effectively use the                an executable situation. For example, the situated module 
 robot's body, and each unit is a sensory-motor unit that              that performs a handshake is executable when a human is 
 realizes certain communicative behavior. For example, we              in front of the robot. By executing the indication part, the 
have implemented "eye contact," "nod," "positional rela•               robot interacts with humans. With the handshake module, the 
tionship," "joint attention (gaze and point at object)."               robot says "Let's shake hands," and offers its hand. This be•
When developers create a situated module, they combine                 havior is implemented by combining communicative units of 
the communicative units at first. Then, they supplement it             eye contact and positional relationships (it orients its body to•
with other sensory-motor units such as utterances and                  ward the human), and by supplementing a particular utterance 
positional movements for particular interactive behaviors.             ("Let's shake hands") and a particular body movement (offering 
                                                                       its hand). The recognition part is designed to recognize 
Situated Modules                                                       several expected human reactions toward the robot's action. 
In linguistics, an adjacency pair is a well-known term for a           As for the handshake module, it can detect human hand-
unit of conversation where the first expression of a pair              shake behavior if a human touches its offered hand. 
requires the second expression to be of a certain type. For              The robot system sequentially executes situated mod•
example, "greeting and response" and "question and an•                 ules (Fig. 4). At the end of the current situated module 
swer" arc considered pairs. We assume that embodied                    execution, it records the recognition result obtained by the 
communication is materialized with a similar principle:                recognition part, and progresses to the next executable 
the action-reaction pair. This involves certain pairs of               situated module. The next module is determined by the 
actions and reactions that also include non-verbal expres•             results and the execution history of previous situated 
sions. The continuation of the pairs forms the communi•                modules, which is similar to a state transition model. 
cation between humans and a robot. 
   Although the action and reaction happen equally, the 
recognition ability provided by current computer science is 
not as powerful as that of humans. Thus, the robot takes the 
initiative and acts rather than reacting to humans actions. 
This allows the flow of communication to be maintained. 
Each situated module is designed to realize a certain ac•
tion-reaction pair in a particular situation (Fig. 3), where a 
robot mainly takes an action and recognizes the humans' 
reaction. Since it produces a particular situation by itself, it 
can recognize humans' complex reactions under limited 
conditions; that is, it expects the human's reaction. This pol•
icy enables developers to easily implement many situated 
modules. On the other hand, when a human takes an action 


 178                                                                                                             COGNITIVE ROBOTICS 2.3 Realized Interactive Behaviors                                     tem consisted of 12 pairs of infrared cameras and infrared 
We installed this mechanism on "Robovic." The robot's                  lights and markers that reflect infrared signals. These 
task is to perform daily communication as children do. The             cameras were set around the room. The system calculates 
number of developed situated modules has reached a                     each marker's 3-D position from all camera images. The 
hundred: 70 of which arc interactive behaviors such as                 system has high resolution in both time (120 Hz) and space 
handshake (Fig. 2, upper-left), hugging (Fig. 2, up•                   (accuracy is 1 mm in the room) 
per-right), playing paper-scissors-rock (Fig. 2, lower-left),            As shown in Fig. 5, we attached ten markers to the heads 
exercising (Fig. 2, lower-right), greeting, kissing, singing           (subjects wore a cap attached with markers), shoulders, 
a song, short conversation, and pointing to an object in the           necks, elbows, and wrists of both the robot and the sub•
surroundings; 20 are idling behaviors such as scratching              jects. By attaching markers to corresponding places on the 
its head, and folding its arms; and 10 are moving-around               robot and subjects, we could analyze the interaction of 
behaviors, such as pretending to patrol an area and going              body movements. The three markers on the subjects' head 
to watch an object in the surroundings.                                detect the individual height, facing direction, and potential 
   Basically, the transition among the situated modules is             eye contact with the robot. The markers on the shoulders 
implemented as follows: it sometimes asks humans for                   and neck are used to calculate the distance between the 
interaction by saying "Let's play, touch me," and exhibits             robot and subjects, and distance moved by them. The 
idling and moving-around behaviors until a human acts in               markers on the arms provide hand movement information 
response; once a human reacts to the robot (touches or                 (the relative positions of hands from the body) and the 
speaks), it starts and continues the friendly behaviors                duration of synchronized movements (the period where 
while the human reacts to these; when the human stops                  the movements of hands of the subject and robot highly 
reacting, it stops the friendly behaviors, says "good bye"             correlate). We also analyzed touching behaviors via an 
and re-starts its idling or moving-around behaviors.                   internal log of the robot's touch sensors. 

3 Body Movement Analysis                                               3.3 Results 
                                                                       Comparison between the body movements and the sub•
3.1 Experiment Settings                                               jective evaluations indicates meaningful correlation. From 
                                                                       the experimental results, well-coordinated behaviors such 
We performed an experiment to investigate the interaction              as eye contact and synchronized arm movements proved to 
of body movements between the developed robot and a                    be important. This suggests that humans make evaluations 
human. We used 26 university students (19 men and 7                    based on their body movements. 
women) as our subjects. Their average age was 19.9. First, 
they were shown examples how to use the robot, then they               Subjective Evaluation: "Evaluation Score" 
freely observed the robot for ten minutes in a rectangular            The semantic differential method is applied to obtain 
room 7.5 m by 10 m. As described in section 2.3, the robot             subjective evaluations with a l-to-7 scale, where 7 denotes 
autonomously tries to interact with subjects. At the be•               the most positive point on the scale. Since we chose the 
ginning of the free observation, the robot asks subjects to            adjective pairs that had high loadings as evaluation factors 
talk and play together, and then subjects usually start                for an interactive robot, the results of all adjective pairs 
touching and talking.                                                 represent subjective evaluation of the robot. Thus, we 
   After the experiment, subjects answered a questionnaire            calculated the evaluation score as the average of all ad•
about their subjective evaluations of the robot with five             jective-pairs' scores. Table 1 indicates the adjective pairs 
adjective pairs shown in Table 1, which was compared                  used, the averages, and standard deviations. 
with the body movements. We chose these adjective pairs 
                                                                      Correlation between Body Movements and Sub•
because they had high loadings as evaluation factors for an 
                                                                      jective Impressions 
interactive robot in a previous study [Kanda et al., 2002a]. 
                                                                      Table 2 displays the measured body movements. Re•
3.2 Measurement of Body Movements                                     garding eye contact, the average time was 328 seconds, 
We employed an optical motion capturing system to                      which is more than half of the experiment time. Since the 
                                                                      robot's eye height was 1.13 m and the average of subject 
measure the body movements. The motion capturing sys•


 COGNITIVE ROBOTICS                                                                                                                     179                                                          Adjective-pairs       Mean      Std. Dev. 
                                                         Good       Bad            4.88      0.95 
                                                         Kind       Cruel          4.85      1.29 
                                                         Pretty     Ugly           5.08      0.93 
                                                         Exciting   Dull           4.46      1.61 j 
                                                         Likable    Unlikable      4.77      1.03 
                                                         Evaluation score          4.81      0.92 
                                                     Table 1: The adjective-pairs used for subjective evaluation, 
                                                      and the mean and resulting mean and standard deviation 
                                                                                    Mean    Std. Dev. 
  Figure 5: Attached markers (left) and obtained 3-D nu•
     merical position data of body movement (right)     Distance (m)                 0.547    0.103 
 In the left figure, white circles indicate the attached markers, and the circles Eye contact (s) 328 61.8 
    in the right figure indicate the observed position of the markers Eye height (m)   1.55   0.124 
eye height was 1.55 m, which was less than their average Distance moved (m)           35.2     17.0 
standing eye height of 1.64 m, several subjects sat down or Distance moved by hands (m) 108    29.5 1 
stooped to bring their eyes to the same height as the robot's. Synchronized movements (s) 7.95 6.58 
The distance moved was farther than what we expected, 
and it seemed that subjects were always moving lit-     Touch (num. of times)         54.9     20.8 
tlc-by-little. For example, when the robot turned, the        Table 2: Results for body movement 
subjects would then correspondingly turn around the robot. as shown in Table 4. The obtained multiple linear regres•
Some subjects performed arm movements synchronized   sion is as follows: 
with the robot's behaviors, such as exercising. 
  Next, we calculated the correlation between the 
evaluation score and the body movements (Table 3). Since 
the number of subjects is 26, each correlation value whose 
absolute value is larger than 0.3297 is significant. We where DIST, EC, EH, DM, DMH, SM, and TOUCH are the 
highlight these significant values with bold face in the standardized values of the experimental results for the 
table. From the calculated results, we found that eye body movements. Since the evaluation was scored on a 
contact and synchronized movements indicated higher  l-to-7 scale, evaluation score E is between 1 and 7. The 
significant correlations with the evaluation score.  multiple correlation coefficient is 0.77, thus 59% of the 
                                                     evaluation score is explained by the regression. The va•
  According to the correlations among body movements, 
                                                     lidity of the regression is proved by analysis of variance 
the following items showed significant correlations: eye 
                                                     (F(7,18) = 3.71, P<0.05). 
contact - distance, eye contact - distance moved, syn•
chronized behaviors - distance moved by hands, and     The coefficients (Table 4) also indicate the importance 
synchronized behaviors - touch. However, these items of well-coordinated behaviors. Eye contact and synchro•
(distance, distance moved, distance moved by hands, and nized movements positively affected the evaluation score; 
touch) do not significantly correlate with the evaluation on the contrary, distance, distance moved and touch seem 
score. That is, only the well-coordinated behaviors cor• to have negatively affected the evaluation score. In other 
relate with the subjective evaluation. Isolated active body words, the subjects who just actively did something 
movements of subjects, such as standing near the robot, (standing near the robot, moved around, and touched re•
moving their hands energetically, and touching the robot peatedly), especially without cooperative behaviors, did 
repetitively, do not correlate to the subjective evaluation. not evaluate the robot highly. 
                                                       Because we can momentarily observe all terms involved 
Estimation of Momentary Evaluation: ''En-            in the body movements of the regression (l), we can es•
trainment Score"                                    timate a momentary evaluation score by using the same 
The results indicate that there are correlations between relations among body movements as follows: 
subjective evaluation and body movements. We performed 
multiple linear regression analysis to estimate the evalua•
tion score from the body movements, which confirms the 
above analysis and reveals how much each body move• where designations such as DIST(t) are the momentary 
ment affects the evaluation. We then applied the relations values of the body movements at time /. We named this 
among body movements to estimate a momentary evalua• momentary evaluation score the "entrainment score " with 
tion score called the entrainment score.            the idea that the robot entrains humans into interaction 
  As a result of the multiple linear regression analysis, through its body movements and humans move their body 
standardized partial regression coefficients were obtained, according to their current evaluation of the robot. The 


180                                                                                  COGNITIVE ROBOTICS evaluation score and entrainment score satisfy the fol•
 lowing equation, which represents our hypothesis that the 
evaluation forms during the interaction occurring through 
the exchange of body movements: 

                                                      (3) 
   Let us show the validity of the estimation by examining 
the obtained entrainment score. Figure 6 shows the en•
trainmcnt scores of two subjects. The horizontal axis in•
dicates the time from start to end (600 seconds) of the 
experiments. The solid line indicates the entrainment score 
E(t), while the colored region indicates the average of the 
entrainment score Eft) from the start to time t (this inte•
gration value grows the estimation of E at the end time). 
   The upper graph shows the score of the subject who 
interacted with the robot very well. She reported after the 
experiment that, "It seems that the robot really looked at 
me because of its eye motion. I nearly regard the robot as a 
human child that has an innocent personality." This en-
trainment-score graph hovers around 5 and sometimes 
goes higher. This is because she talked to the robot while 
maintaining eye contact. She performed synchronized 
movements corresponding to the robot's exercising be•
haviors, which caused the high value around 200 sec. 
   At the other extreme, the lower graph is for the subject          Figure 6: Illustration of entrainment score 
who became embarrassed and had difficulty in interacting      (upper: subject who treated the robot as if it were a humans child, lower: 
with the robot. The graph sometimes falls below 0. In                subject who was embarrassed by interacting with it) 
particular, at the end of the experiment, it became unstable 
                                                             action and exhibit behavior similar to the sleeping pose. 
and even lower. He covered the robot's eye camera, 
                                                             NOTTURN is the behavior for brushing off a human's 
touched it like he was irritated, and went away from the 
                                                             hand while saying "I'm busy" when someone touches on 
robot. We consider that those two examples suggest the 
                                                             its shoulder. The best modules were rather interactive 
validity of the entrainment score estimation. 
                                                             modules that entrain humans into the interaction. 
Evaluation of the implemented behaviors                      EXERCISE and CONDUCTOR produce the exercising 
                                                             and imitating of musical conductor behaviors, which in•
In the sections above, we explained the analysis of body 
                                                             duced human synchronized body movements. Other 
movement interaction. Here, we evaluate the implemented 
                                                             highly rated modules also produce attractive behaviors, 
behaviors. Although the application of this result is limited 
                                                             such asking and calling, which induce human reactions. 
to our approach, our findings also prove the validity and 
                                                             We believe that the entrainment scores provide plenty of 
applicability of the entrainment score. 
                                                             information for developing interactive behaviors of robots 
   We calculated the evaluation score of each situated 
                                                             that communicate with humans. 
module based on the average of the entrainment score 
while each module was being executed. Tables 5 and 6 
indicate the worst and best five modules, respectively, and  4 Discussions 
their scores. The worst modules were not so interactive.     The experiment reveals the correlation between humans' 
SLEEP_POSE and FULLY_FED do not respond to human             subjective evaluations and body movements. If a human 


 COGNITIVE ROBOTICS                                                                                                  181 