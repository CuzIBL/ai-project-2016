                        Dynamics of Temporal Difference Learning

                                         Andreas Wendemuth
                      Otto-von-Guericke-University, 39016 Magdeburg, Germany
                                       Cognitive Systems Group
                           andreas.wendemuth@e-technik.uni-magdeburg.de


Abstract                                              Smart 2004]. We adopt the following notation in the course
                                                      of this paper, following [Dayan and Abbott 2001]:
In behavioural sciences, the problem that a sequence of stim-
                                                        •
uli is followed by a sequence of rewards r(t) is considered. Stimulus u(t)
The subject is to learn the full sequence of rewards from the • Future reward r(t)
stimuli, where the prediction is modelled by the Sutton-Barto • Sum of future rewards R(t)
rule. In a sequence of n trials, this prediction rule is learned it- •
eratively by temporal difference learning. We present a closed Weights w(k)
formula of the prediction of rewards at trial time t within trial • Predicted reward v(t)
n. From that formula, we show directly that for n →∞the We want to compute, for all trials n of duration T ,andfor
predictions converge to the real rewards. In this approach, a any time of trial t, the predicted reward vn(t). The (ex-
new quality of correlation type Toeplitz matrices is proven. tended) stimulus u(t) is given at times tu,min ...tu,max ,the
We give learning rates which optimally speed up the learning (extended) reward r(t) is presented at times tr,min ...tr,max.
process.                                              Stimulus and reward do not overlap, i.e. tr,min >tu,max.
                                                        The subject is to learn the total remaining reward at time t,
                                                              T−t
1  Temporal Difference Learning                       R(t)=       r(t + τ) (only after stimulus onset!)
We consider here a mathematical treatment of a problem in     τ=0
                                                        The brackets  refer, in general, to stochastic values of
behavioural biology. This problem has been described e.g. in
                                                      R(t), if not exactly the same rewards are given at each trial,
[Dayan and Abbott 2001] as learning to predict a reward. It is
                                                      but when there are ﬂuctuations.
Pavlovian in the sense that classical conditioning is adressed,
                                                        All previous stimuli u(t) are weighted to give a linear pre-
however reward is not immediate, but after a series of stimuli
                                                      diction model [Sutton and Barto 1990]:
there is a latency time, followed by a series of rewards. Us-
                                                                            t
ing a simple linear rule, we will show that the subject is able
                                                                                      −               (1)
to predict the remaining rewards by that rule, repeating the         v(t)=     w(τ)u(t  τ)
                                                                            τ=0
same stimuli-reward pattern over a series of trials. A review
of learning theory related to these problems can be found in An update rule for the Sutton-Barto-formula can easily be de-
                                                      rived from the mean square error [Dayan and Abbott 2001]:
[Sutton and Barto 1998].                                                                           
                                                                         T−t         t              2
  There are recent experimental biological correlates with          2
this mathematical model, e.g. in the activity of primate R(t) − v(t) =     r(t + τ) −   w(τ)u(t−τ)
dopamine cells during appetitive conditioning tasks, together            τ=0          τ=0
with the psychological and pharmacological rationale for                                              (2)
studying these cells. A review was given in [Schultz 1998], using the partial derivative with respect to any weight w(α),
                                                                         2
the connection to temporal difference learning can be found ∂R(t) − v(t)
in [Montague et al. 1996].
                                                                ∂w(α)
  In this paper, the focus is on two mathematical issues: a) to                              
                                                                   T −t          t
provide a direct constructive proof of convergence by giving                   
                                                              −              −            −        −
an explicit dependence of the prediction error over trials, b) to = 2 r(t + τ)     w(τ)u(t  τ)  u(t  α)
minimize the learning time by giving a formula for optimal         τ=0          τ=0
setting of the learning rate. Hence, the paper contributes as Updates are made in discrete steps in the direction of the neg-
well to temporal difference learning as a purely mathematical ative gradient, providing the following rule:
issue, which may be valuable also without reference to be- Δw(τ)=εδ(t)u(t   − τ)                      (3)
                                                                                
havioural biology and reinforcement learning. It can also be          T−t
understood in a Dynamic Programming sense, see [Watkins    δ(t)=         r(t + τ)  − v(t)=R(t)−v(t)
1989] and later work, e.g. [Gordon 2001] and [Szepesvari and          τ=0

                                                IJCAI-07
                                                  1107            Obviously, the total future reward R(t) is not known to the This can be written in matrix notation as follows, with E the
            subject at time t. We can use R(t)=r(t)+R(t +1).Inthis unit matrix:
            formulation, again R(t +1)is not known. The subject can      ⎛   n+1            ⎞
                                                                            v   (tu,max)
            approximate the value by his prediction v(t +1). This will   ⎜   n+1            ⎟
                                                                         ⎜  v   (tu,max +1) ⎟
            provide the so-called                                        ⎜  .               ⎟
              Temporal difference rule at time t:                        ⎜  .               ⎟  =(E  − εGA)  ×
                                                                         ⎝  .               ⎠
                     w             u   −      ∀
                   Δ   (τ)=εδ(t)     (t  τ) ,   τ =0...t                     n+1
                                             −                              v   (tr,max)
                      δ(t)=r(t)+v(t     +1)    v(t)         (4)            ⎛                ⎞       ⎛           ⎞
                                                                               n                       0
            The updates can be made sequentially, after each time step t,     v (tu,max)
                                                                           ⎜   n            ⎟       ⎜  .        ⎟
            or in parallel, sampling the weight updates for all times in a ⎜  v (tu,max +1) ⎟       ⎜  .        ⎟
                                                                           ⎜  .             ⎟       ⎜           ⎟
            given trial, and then updating at the end of the trial. The two × ⎜ .           ⎟ +  εG ⎜  r(tr,min) ⎟
                                                                           ⎝  .             ⎠       ⎜           ⎟
            alternatives do not substantially differ in the ﬁnal result after                       ⎝  .        ⎠
                                                                               n                       .
            many trials. For computational reasons, we use the parallel       v (tr,max)
            version of the update rule. Other learning schedules may be                                r(tr,max)
            adopted, however we use eq. (4) since this is widely accepted where we shall now use the total time T = tr,max−tu,max+1:
            in the community, following [Dayan and Abbott 2001].Let
            us denote the trial number by superscripts n.                      tu,max−y
                                                                        g(y)=         u(k) u(y + k) , ∀ y =0...k  (8)
            2  Dynamics of temporal difference learning                        k=tu,min

            The parallel temporal difference rule eq. (4) was obtained, with any real values u(k) and u(tu,max) =0 ,andT × T -
            using an approximation to gradient descent. Therefore, con- matrices
            vergence is not guaranteed and shall be proven in the course
                                                                               ⎛            ···          0 ⎞
            of this paper, where a compact notation will be introduced.          g0     g1       gk
                                                                               ⎜                     .     ⎟
            We proceed as follows:                                             ⎜                 ···  ..   ⎟
                                                                               ⎜ g1     g0  g1          gk ⎟
              • Incorporate the rule into the prediction formula, yielding     ⎜ .      .   .    .       . ⎟
                                                 n                        G =  ⎜ .       .   .    .      . ⎟  and
                a recursive formula for all predictions v (t) at trial n.      ⎜ .        .   .   .      . ⎟
                                                                               ⎝ gk     ··· g1   g0     g1 ⎠
              • Make this a closed formula vn(t).
                                                                                      ..
              •                  n   →                                           0     . gk ···  g1     g0
                Show convergence v (t)  R(t) for large N.                      ⎛                      ⎞
                                                                                     −
              • Choose an optimal learning rate ε for maximally fast             1    1
                                                                               ⎜         −            ⎟
                convergence.                                                   ⎜     1     1          ⎟
                                      n+1                                      ⎜         .    .       ⎟
            We obtain for the predictions v (t) at trial n +1the fol-     A =  ⎜          ..   ..     ⎟           (9)
                                                                               ⎝                      ⎠
            lowing recursive formula. Summation limits explicitely state                      1   −1
            situations where stimuli and reward phases may overlap, we                            1
            will restrict to non-overlapping cases later.

              n+1      n           min(t,tu,max) n               where the ”0”inG  refers to the full remaining upper and
             v   (t) − v (t) t≥tu,min        Δw   (t − k)u(k)
                   ε                                ε             lower triangle, respectively, and in A all entries except for
                                     k=tu,min                     the two diagonals are 0. We can further write this in compact
               min(t,tu,max) min(tr,max−t+k,tu,max)
                                                 n                notation (vectors have component index t):
             =           u(k)                   δ (x + t − k)u(x)
                                                                               n+1               n
                 k=tu,min          x=tu,min                                  v    =(E  −  εGA)  v  + εGr
               min(tr,max−t,tu,max−tu,min)
                                      n
             =                       δ (t + y) g(t, y)            With v0 = 0 one has
                 y=t   −min(t,t   )
                    u,min     u,max                                                 N
                                                            (5)            vN+1         E −  GA   n Gr
            where δn(t)=r(t)+vn(t  +1)−  vn(t) and                              =  ε   (    ε    )
                                                                                    n=0
                       min(min(t,tu,max),tu,max−y)                                 −1                (N+1)
                                                                           = ε(εGA)   (E − (E − εGA)       )Gr
               g(t, y)=                      u(k)u(y + k)   (6)                                              
                                                                               −1        −1            (N+1)
                         k=max(tu,min,tu,min−y)                            = A    E  − G    (E − εGA)       G  r (10)

            Since we are interested whether the subject will make proper Only if (−GA) is a Hurwitz (stability) matrix (all eigenval-
            predictions after all stimuli have been given, we will restrict ues have negative real part), a suitable ε can be chosen such
            our analysis to times t>tu,max.Theng(t, y) will become               (N+1)
                                                                  that (E − εGA)       will vanish with large N. A similar
            independent of t,giving
                                                                  approach, relying on Hurwitz matrices, was followed in
                             min(tu,max,tu,max−y)                [Sutton 1988], however there the Hurwitz property was not
             g(y)=g(t, y)=                     u(k)u(y + k) (7)   shown immediately on the matrix structure of eq. (9). Our

                            k=max(tu,min,tu,min−y)                aim here is to provide an explicit proof, which will establish

                                                            IJCAI-07
                                                              1108as an interesting result in its own right a general property of hence xTGx can be written as
Toeplitz matrices with correlation entries.               N k            N  k k−j
                                                                    2 2
                                                        −       u(t) xi +2            u(t) u(t + j) xi xi+j
  We will show the Hurwitz property later, and we will give i=0 t=0         i=0 j=0 t=0
values for ε for which convergence is assured and for which it                                       (13)
is optimal. Continuing with the large N-behaviour, we obtain
     N→∞                                              Rearranging the inner sums with
vN+1  −→   A−1 r . This holds no matter what the stimulus!
                                                                        k k−j  k k−t
With              ⎛                  ⎞
                    11111                                                      =                     (14)
                  ⎜                  ⎟                                  j=0 t=0  t=0 j=0
                  ⎜     1111⎟
             −1   ⎜        .   .   . ⎟
           A   =  ⎜         ..  .. . ⎟                and using vectors u of dimension (k +1)with components
                  ⎝                . ⎠
                               11                     ut = u(t) leads to
                                   1                                N  k            N
                                                           T                   2 2         T ˜ (i)
we obtain                                                 x  Gx = −        u(t) xi +2     u X   u    (15)
                    T −t
                                                                    i=0 t=0          i=0
              ∞
            v  (t)=     r(t + τ)=R(t)                                  ˜ (i)
                    τ=0                               where the matrices X are (k +1)× (k +1)band matrices
                                                                 ˜ (i)
which is the desired result. This proves convergence in full with entries Xm,m+n = xixi+n for all m and n ≥ 0,and0
generality. We need to show that (−GA) is a Hurwitz ma- otherwise. Symmetrizing the matrix products, we can write
                                                                                                  
trix, i.e. that all eigenvalues of that matrix have negative real N       N               N
                                                             T  (i)     T       (i)      T       (i) T
part. Further, we want to give values for ε which provide 2 u  X˜ u = u       X˜   u + u       (X˜ )   u
maximum speed of convergence.                            i=0               i=0              i=0
                                                                                                     (16)
3  Proof of convergence                               The matrix elements of the second term are, for −k ≤ n ≤ 0:
                                                                  
In previous approaches in the literature, temporal difference N             N           N
                                                              ˜ (i) T
(TD) learning was extended to TD(λ) learning where the pa-   (X  )         =    xjxj−n =     xixi+n  (17)
rameter λ refers to an exponential weighting with recency. i=0      m,m+n    j=0          i=0
Learning and convergence issues were considered e.g. in
[Dayan 1992] and [Dayan and Sejnowski 1994].IntheTD(λ) where the summation limits in the last result were taken, for
framework, what we consider here is TD(0)-learning where convenience, larger than necessary for covering all cases of
in contrast to the mentioned literature we concentrate on a di- n for which the summand is nonzero (note that n is nonposi-
rect proof of convergence by establishing a general property tive), the extra terms being zero due to the zero padding con-
of Toeplitz matrices with correlation entries.        vention for the xi. As a result, the matrix elements of the
  The proof proceeds as follows: we will show ﬁrst that G ﬁrst and the second term in eq. (16) have identical format, for
is positive deﬁnite. Then, we will show that all eigenvalues nonnegative and nonpositive n, respectively.
of (GA) have positive real part. Hence, we will have estab- Making use of eq. (16) and eq. (17), and incorporating the
                                                      ﬁrst sum in eq. (15) leads to
lished the Hurwitz property required for convergence.                                  
  In order to see that G is positive deﬁnite, we consider any                    N
                              T                                      T        T       (i)
nonzero real vector x and show that x Gx is always positive.        x  Gx =  u      Xˆ    u          (18)
We ﬁrst extract from G the diagonal matrix GD and the upper                      i=0
triangular matrix G+.Thenwehave
                                                      where the matrices Xˆ (i) are band matrices with entries
    xTGx       xTG   x   xTG   x   xTGTx              ˆ (i)
            =       D  +      +  +      +             Xm,m+n  =  xixi+n for positive and negative n.
                 T         T         T  T   T
               x  GDx    x  G+x     x G   x             We can now write the sum of matrices in eq. (18) in a dif-
            =          +         +(     +  )                                   (i)
                 T          T                         ferent way. Denote vectors x˜ of dimension (k +1)with
            =  x  GDx  +2x   G+x                                  (i)
                                                      components x˜m = xi+m, m =0...k, where, as before, it is
               N            N    k
                        2                             understood that xj =0for j<0 and j>N. Hence, these
            =      g(0) xi +2    xi   g(j) xi+j (11)  vectors will have nonzero components only for i = −k...N.
                i=0          i=0   j=1                  Notice that the following covariance matrices are generated
where in order to keep summation limits easy it is understood by these vectors:
                                                                           
that xi =0for i<0 and i>N(”zero padding”). There are                (i) (i) T
                                                                   x˜ (x˜ )       = xi+m xi+n        (19)
2k+1 Toeplitz bands, k being the number of stimuli. Shifting                 (m,n)
the time index in eq. (8) from tu,min to 0, g(j) arises from the
k stimuli for u(k) =0 ,u(t)realas                    Then we have for (m =0...k,n=  −m...k−   m):
                                                                        
         k−|j|                                             N                    N+m
                                                                x˜(i) x˜(i) T
   g(j)=      u(t) u(t + |j|) , |j| =0...k ,∀ t (12)               (   )       =        xi xi+n−m    (20)
          t=0                                              i=−k           (m,n)  i=−k+m

                                                IJCAI-07
                                                  1109and                                                     Let y be any eigenvector of (GA) and λ be the corre-
                                                                             GA           y
     N                        N+m                   sponding eigenvalue. Since (  ) is real, and λ are ei-
          (i) (i) T                                   ther both real or both complex. In the complex case, there
         x˜ (x˜ )          =         xi xi+n   (21)                           ∗
                                                      exists a further eigenvector y and corresponding eigenvalue
     i=−k                     i=−k+m
                    (m,m+n)                           λ∗,where()∗ denotes complex conjugated and transposed.
                                                                    z   Ay
which establishes that                                  Let us denote =     and write
                                                                 z∗Gz    y∗A∗GAy       y∗A∗y
               N              N                                        =            = λ              (26)
                                                                                              ∗
                  x˜(i) x˜(i) T  Xˆ (i)               Also, with G real and symmetric, we have G = G and
                     (   )  =                  (22)                 ∗       ∗    ∗    ∗ ∗
             i=−k             i=0                                  z Gz =(z  Gz)   = λ y Ay          (27)
                                                      The summation of eqns. 26 and 27 gives
Then we can rewrite eq. (18), using eq. (22):                        ∗       ∗  ∗        ∗
                                                                   2z Gz  = y [λ A +  λA  ] y        (28)
            N                    N
                                                    With G real, symmetric and positive deﬁnite, we have for any
   xTGx         uTx˜(i) x˜(i) T u   |uTx˜(i)|2                      ∗
         =           (   )   =                 (23)   complex z that z Gz is real and positive. This can be used as
           i=−k                 i=−k                  follows.
                                                        Case 1: y and λ are both real.
This sum is certainly nonnegative. We will now show that it                A
cannot become zero.                                   Then we have, with real , from eq. (28) immediately
                                                                         ∗         T   T
                                                                0 < Re(2z Gz)=λy     A   + A  y      (29)
  The proof is by contradiction. Suppose the sum is zero. hence                               
                                                                                     T   T
Then all individual terms must be zero. Start with the ﬁrst sign(Re(λ)) = sign(λ)=sign(y A + A  y)   (30)
term. We have from the deﬁnition
                                                        Case 2: y and λ are both complex.
                     k                               Let us write y = v+iw and λ = g +ih. Then from eq. (28),
           uTx˜(−k)                                           A
                   =     ujx−k+j = ukx0        (24)   with real , we obtain              
                                                                     ∗         T    T
                     j=0                                 0  <   Re(2z Gz)=g   v   A   + A  v         (31)
                                                                                                
                                                                     T   T               T    T
This becomes zero, since uk =0 (eq. (12)), only for x0 =0.     +g w   A   + A  w − 2 h w   A  − A  v
                                                                                         
Now proceed with the second term:                                    ∗          T   T
                                                         0=Im(2z      Gz)=h   v   A   + A  v         (32)
                                                                                                
             k                                                      T   T               T    T
  T  (−k+1)                                                     +h w   A   + A  w +2g  w    A  − A  v
 u x˜      =     ujx−k+1+j = uk−1x0 + ukx1 = ukx1
             j=0                                      Combining eqns. 31 and 32 gives            
                                                               2   2    T   T            T   T
                                               (25)     0 <g(g  + h ) v   A   + A  v + w   A   + A  w
where the previous result x0 =0was used. This becomes                                                (33)
            
zero, since uk =0(eq. (12)), only for x1 =0. Continuing in from which follows with sign(Re( λ)) = sign(g):  
                                                                          T   T            T   T
this vein, after N steps, we end with the product     sign(Re(λ)) = sign v  A   + A  v + w   A   + A  w
                    k                                                                               (34)
        T  (−k+N)
       u x˜       =    ujx−k+N+j                      The results for both cases, eqns. 30 and 34, allow the
                    j=0                               following sufﬁcient condition:
                                                      Let G  be a real, symmetric and positive deﬁnite matrix,
       = u0xN−k + ...+ uk−1xN−1  + ukxN  = ukxN                                                 T
                                                      and A be a real square matrix. If the matrix A + A is
where all the previous results were used. Hence xN =0, positive deﬁnite, then the real parts of the eigenvalues of
which in total means that x = 0. This is a contradiction, (GA) are positive.
since x must be a nonzero vector.
  With this result, we have established that xTGx > 0,  Let us now turn our attention to the speciﬁc matrix A given
hence we have established:                            in eq. (9). We have⎛                     ⎞
  Symmetric Toeplitz matrices of the structure of G after               2    −1
                                                                     ⎜  −         −            ⎟
eq. (9), with correlation entries gj after eq. (12), are positive    ⎜    12       1           ⎟
deﬁnite.                                                     T       ⎜       .    .   .        ⎟
                                                           A   + A = ⎜        ..  ..   ..      ⎟
                                                                     ⎝                         ⎠     (35)
                                                                                  −12      −1
  We now turn our attention to showing that all eigenvalues                           −
of (GA)  have positive real part. We will look at any real                              12
A and any G which is real, symmetric and positive deﬁnite.
(We will not require the stronger condition that the structure It can be shown that the matrix Q = AT + A is positive def-
of G is after eq. (9), and we say nothing about the entries gj.) inite, by using the previous result of this paper: matrices of
  Under these premises, the proof will ﬁrst give a sufﬁcient the structure of G are positive deﬁnite. Noting that Q has the
condition for the sign of the real part of the eigenvalues of structure of G, using eq. (12) with k =1and u =(1,-1),im-
(GA). Then we will show that this sign is indeed positive mediately gives the desired result. This completes our proof
for the A at hand.                                    that (−GA)  is a Hurwitz matrix.

                                                IJCAI-07
                                                  11104Learningrateε       for fast convergence             along curves m(ε(k)), ensuring the condition that our chosen
The convergence behaviour of the temporal difference rule is k satisﬁes maxk m(ε(k)).
                                                        At ε =0,allm(ε(k)) =   1 and decreasing with ε.The
known from eq. (10). For large number of trials, convergence                                         ∗
                                        E −  GA       slope is given by −2gk < 0. We start by choosing the k for
speed will be dominated by the eigenvalue of ( ε  )          ∗
                                                      which k  =argmink   gk, which ensures that the condition
with the largest modulus, for which convergence speed is                                     ∗
slowest. We have for the eigenvalues (ev):            maxk  m(ε(k)) is satisﬁed by our choice of k . Our current
                                                      position is εc =0. Then we apply the following procedure:
 ev(E − εGA)=1−     ε ev(GA)=1−     ε (g + ih) (36)     [START]
                                                                         ∗
where the same notation for the eigenvalues of GAas in the Continuing on curve k , we would reach the minimum for
                                                                         ∗       gk∗
previous section, λ = g + ih, was used.                                 εk∗ =  2     2               (41)
  Hence, the ﬁrst task to do is to compute the eigenvalues of                 gk∗ + hk∗
GA
    which are only dependent on the given stimuli u(k) and if everywhere between our current position εc and the pro-
                         GA                                         ∗
on the total time T . Note that deﬁnes the special struc- posed position εk∗ the maximum condition maxk m(ε(k)) is
ture of an unsymmetric Toeplitz matrix with 2k +2bands. satisﬁed for k∗. In order to check this, we compute the inter-
It is well known that a closed solution for the eigenvalues of sections of our curve under inspection k∗ to all other curves
Toeplitz matrices exists only for not more than three bands, k.
they are given for symmetric and asymmetric cases e.g. in After eq. (37), these intersections are given at values εk
[Beam and Warming 1993]. Hence only for k =0can a     according to
closed solution been given, which we do in sec. 5. For k>0         2    2 2              2   2  2
                                                          (1 − εk gk) + εk hk =(1− εk gk∗ ) + εk hk∗ (42)
numerical values must be obtained, special methods for the
solution, and for the asymptotic structure of the eigenvalues or
                                                                               gk − gk∗
for large T ,aregivenin[Beam and Warming 1993].                     k
                                                                   ε =2    2 −  2    2 −  2          (43)
  The square modulus m of the eigenvalues is given by                     gk   gk∗ + hk  hk∗
                         2          2    2 2            Now we inspect the intersection which is closest to our
  m(ε)=|   ev(E − εGA)  | =(1−   εg)  + ε h    (37)
                                                      current position of εc,i.e.forwhichkˆ =argmink{ εk,
We will ﬁrst give a choice for ε for which convergence is εk >εc}.
                                                                ∗        ∗
always assured and which also serves as a good rough value If εkˆ >εk∗ ,thenεk∗ after eq. (41) is indeed the solution
for setting ε without much computational effort.      (free minimum). [STOP]
                                                                                                 ∗
  Note ﬁrst that m(ε =0)=1for all eigenvalues. Fur-     Else, there is an intersection prior to reaching εk∗ .Ifthe
ther, since (− GA) is Hurwitz, g>0 for all eigenvalues, curve that is intersecting is rising at ε(kˆ), this means that
and for small ε>0, m(ε) <  1 for all eigenvalues due to we are done, since we actually have reached the condition
eq. (37). Lastly, for large ε, m(ε) will increase again beyond of eq. (40). The solution is
all bounds for all eigenvalues, since m(ε) ∝ ε2 for large ε                                            
                                                                                 gk − gk∗
and all eigenvalues.                                    ˆ
                                                      ε(k)=mink    ε(k)=  2  2 −  2    2 −  2  ,ε(k) >εc
  After this sketch of the general behaviour of m(ε),itis                   gk  gk∗ + hk   hk∗
clear that for each eigenvalue k there is a ε(k) =0 for which                                       (44)
m(ε(k)) = 1 and m(ε(k)) is rising with ε(k). Computing (bounded minimum). [STOP]
                                                                                                   ˆ
these values from eq. (37) for eigenvalues λk = gk + ihk Else, if the curve that is intersecting is falling at ε(k),we
gives                                                 must continue on that new curve which now satisﬁes the max-
                           2 gk                       imum condition in eq. (40). To this end, we set εc = ε(kˆ) and
                  ε(k)=   2    2               (38)    ∗   ˆ
                         gk + hk                      k  = k and continue (change in falling curves with maximum
                                                      modulus). [GO TO START]
Hence, m(ε) < 1 for 0 <ε<mink ε(k), and a good choice
ε˜ for convergent behaviour is therefore the mean value of the Fig. 1 illustrates the behaviour with free minimum, ﬁg. 2
bounds,                                               with bounded minimum. Both ﬁgures have a change in falling
                            gk                        curves with maximum modulus.
                 ε˜ =mink  2    2              (39)     This algorithm terminates after ﬁnitely many steps, either
                          gk + hk
                                                      at the minimum of the curve k∗ or at an intersection of a
Note again that whenever hk =0 , there is an additional
            −                                         falling with a rising curve. It is clear that one of the two
eigenvalue gk  ihk for which the minimum in eq. (39) is possibilities exist since all curves eventually rise with large ε.
identical and needs not be computed.
                                     ∗
  We now turn to ﬁnding the optimal value ε for fastest con- 5 One stimulus
vergence. This is given by
             ∗                                        We look at the special case where only one stimulus u is
            ε  =argminε  maxk m(ε(k))          (40)   present, hence k =0. Then, eq. (10) takes a particularly
                                                                        E − A
This optimization problem has no closed solution. However, simple form, where ( ) is the unit shift matrix:
                                                                         
a simple line search algorithm can be given to ﬁnd the solu-       T                               
                                                          N+1           N          2 N−j    2        j
tion in ﬁnitely many (less than T )steps.              Av    = r −          (1 − εu )    εu  (E − A)   r
  The general idea is to start at ε =0and increase ε un-           j=0  j
til we reach ε∗. In all of this process, we select a k and go                                        (45)

                                                IJCAI-07
                                                  1111