           A Continuation Method for Nash Equilibria in Structured Games 

              Ben Blum                    Christian R. Shelton                 Daphne Koller 
         Stanford University               Stanford University               Stanford University 
         bblum@stanford.edu             cshelton@cs.stanford.edu           koller@cs.stanford.edu 


                     Abstract                            In this paper, we describe a set of algorithms that use con-
                                                       tinuation methods for solving structured games. These algo•
    We describe algorithms for computing Nash equilibria in rithms follow a trajectory of equilibria of perturbed games 
    structured game representations, including both graphi• until an equilibrium of the original game is found. Our algo•
    cal games and multi-agent influence diagrams (MAIDs). rithms are based on the recent work of Govindan and Wil•
    The algorithms are derived from a continuation method son [2002; 2003a; 2003b] (GW hereafter), which apply to 
    for normal-form and extensive-form games due to Govin-
                                                       standard game representations (normal-form and extensive-
    dan and Wilson; they follow a trajectory through the 
    space of perturbed games and their equilibria. Our algo• form). We show how the structure of the games can be ex•
    rithms exploit game structure through fast computation ploited to perform the key computational step of the algo•
    of the Jacobian of the game's payoff function. They are rithms of GW. 
    guaranteed to find at least one equilibrium of the game Our methods address both graphical games [Kearns et ai, 
    and may find more. Our approach provides the first exact 2001] and multi-agent influence diagrams (MAIDs) [Koller 
    algorithm for computing an exact equilibrium in graphi• and Milch, 2001]. We present the first algorithm for finding 
    cal games with arbitrary topology, and the first algorithm exact equilibria in graphical games of arbitrary structure. We 
    to exploit fine-grain structural properties of MAIDs. We also provide the first algorithm that can take advantage of the 
    present experimental results for our algorithms. The run•
                                                       fine-grained structure of MAIDs, and therefore can handle 
    ning time for our graphical game algorithm is similar to, 
    and often better than, the running time of previous ap• MAIDs which are significantly outside the scope of previous 
    proximate algorithms. Our algorithm for MAIDs can ef• methods. We provide experimental results demonstrating the 
    fectively solve games that arc much larger than those that efficiency of our approach relative to previous methods. 
    could be solved using previous methods. 
                                                       2   Game Representations and Equilibria 
1   Introduction                                       2.1  Game Theory 
                                                       We begin by briefly reviewing concepts from game theory 
Game theory is a mathematical framework that describes in• used in this paper, referring to Owen [1995] for a good intro•
teractions between multiple rational agents and allows for duction. A game defines an interaction between a set N of 
reasoning about their outcomes. However, the complexity agents. Each agent n e N has a set of available strategies 
of standard game descriptions grows exponentially with the , where a strategy determines the agent's behavior in the 
number of agents involved. For many multi-agent situations, game. The precise definition of the set depends on the 
this blowup presents a serious problem. Recent work in game representation, as we discuss below. A strategy pro-
artificial intelligence [La Mura, 2000; Kearns et al, 2001; file a defines a strategy for each Given a 
Koller and Milch, 2001 ] proposes the use of structured game strategy profile , the game defines a payoff for each 
representations that utilize a notion of locality of interaction; agent We use to denote the strategy profiles of 
these representations allow a wide range of complex games the agents N - _ . Similarly, we use to refer to 
to be represented compactly.                           the set of all strategy profiles of agents in  
  In this paper we consider the task of computing Nash equi• A solution to a game is a prescription of a strategy profile 
libria for structured games. A Nash equilibrium is a strategy for the agents. The agent's goal is to maximize its payoff. 
profile such that it is no agent's interest to deviate unilaterally. Thus, a basic desideratum for a solution profile is stability 
A naive approach to finding Nash equilibria is to convert the — it should not be in any agent's interests to deviate from 
structured game into a standard game representation, and ap• it. More precisely, the fundamental notion of a Nash equilib•
ply a standard game-theoretic solution algorithm iMcKelvey rium [Nash, 1951] is defined to be a strategy profile a such 
and McLennan, 1996]. This approach is, in general, infea- that, for all for all 
sible for all but the simplest games. We would like an algo• other strategies ' ~~ . Thus, if an agent knew that the oth•
rithm that exploits the structure in these game representations ers were playing according to an equilibrium profile, it would 
for efficient computation.                             have no incentive to deviate. 


MULTIAGENT SYSTEMS                                                                                     757    An e-equilibrium is a strategy profile such that no agent can From this representation, we can easily derive the probability 
 improve its expected payoff by more than by unilaterally      of taking an action at a particular information set. 
 changing its strategy. Unfortunately, finding an -equilibrium   The set of sequence form strategies for agent n is therefore 
 is not necessarily a step toward finding an exact equilibrium: a subset of , where is at most the number of leaves 
 the fact that is an -equilibrium does not guarantee the ex•   in the tree. The set of legal sequence form strategies 
 istence of an exact equilibrium in the neighborhood of a.     for agent n is defined by a set of linear constraints on vec-
                                                               tors in . The set of sequence form strategy profiles is 
 Normal-Form Games 
                                                               then defined as The payoff to agent n in an 
 A general-sum normal-form game defines a simultaneous-
                                                               extensive-form game can be shown to be 
 move multiagent scenario in which each agent independently 
 selects an action and then receives a payoff that depends on 
 the actions selected by all of the agents. More precisely, let                                                       (1) 
 G be a normal-form game with a set N of agents. Each agent 
 n N has a discrete action set Sn and a payoff array Gn        Thus, the payoffs arc a sum, over the leaves in the tree, of the 
 with entries for every action profile in                      payoff at a leaf times the product of the sequence form param•

   Equilibrium strategies often require that agents randomize  eters for that leaf.1 Importantly, this expression has a similar 
 their choice of action. A mixed strategy is a probability     multi-linear form to the payoff in a normal-form game, but us•
distribution over Sn. The set , is the set of all mixed strate• ing sequence form strategies rather than mixed strategies. In 
gies. The support of a mixed strategy is the set of actions in an extensive form game satisfying perfect recall, any mixed 
Sn that have non-zero probability. A strategy for agent n      strategy equilibrium can be represented using an essentially 
is said to be a pure strategy if it has only a single action in its equivalent sequence form strategy profile. 
support. The set of mixed strategy profiles is 
A mixed strategy profile is thus an m-vector, where            2.2   Structured Representations 
m = Every game is guaranteed to have at least 
one mixed-strategy equilibrium, and the number of equilibria   Graphical Games 
may be exponential in the number of agents.                    The size of the payoff arrays required to describe a normal-
                                                               form game grows exponentially with the number of agents. 
Extensive-Form Games                                           Kearns et al. 12001] introduced the framework ol\ graphical 
An extensive-form game is represented by a tree in which       games, which provide a more structured representation based 
each node represents a choice either of an agent or of nature. on probabilistic graphical models. Graphical games capture 
Each of nature's choice nodes is associated with a probability local structure in multi-agent interactions, allowing a com•
distribution over its outgoing branches. Each leaf of pact representation for scenarios where each agent's payoff is 
the tree is associated with a vector of payoffs , where        only affected by a small subset of other agents. Examples of 
       denotes the payoff to agent at leaf . The choices       interactions where this structure occurs include agents that in•
of the agents and nature dictate which path of the tree is fol• teract along organization hierarchies and agents that interact 
lowed and therefore the payoffs to the agents.                 according to geographic proximity. 
   The decision nodes belonging to each agent are partitioned    A graphical game is described like a normal-form game. 
into information sets, where each information set is a set of  The basic representation (slightly generalized) is a directed 

states among which the agent cannot distinguish. Thus, an      graph with one node for each agent. An edge from agent n1 
agent's strategy must take the same action at all nodes in the to agent n in the graph indicates that agent n's payoffs depend 

same information set. We define an agent history for a         on the action of agent n'. More precisely, we define Famn to 
node y in the tree and an agent n to be a sequence containing  be the set of agents consisting of n itself and its parents in the 
the information sets traversed in the path from the root to y, graph. The agent's payoff is an array indexed only by the 

and the action selected at each one. Thus, two nodes have      actions of the agents in Famn. Thus, the description of the 
the same agent-n history if the paths used to reach them are   game is exponential in the in-degree of the graph and not in 
indistinguishable to n. (The paths may differ in other ways,   the total number of agents. In this case, we use 

such as nature's decisions or the decisions of other agents.)  to refer to strategy profiles of the agent in Famn  
We make the common assumption of perfect recall: an agent 
does not forget information known nor the choices made at      Multi-agent Influence Diagrams 
previous decisions. More precisely, if two nodes y, y' are in  The description length of extensive-form games often also 
the same information set for agent then                        grows exponentially with the number of agents. In many 
   We need a representation of a strategy for an extensive-    situations this large tree can be represented more com•
form game; unlike the case of normal-form games, there are     pactly. Multi-agent influence diagrams (MAIDs) [Koller and 
several quite different choices. For our purposes, the most    Milch, 2001] allow a structured representation of games in•
appropriate representation is the sequence form [Koller and    volving time and information by extending influence dia•

Megiddo, 1992; von Stengel, 1996]. Here, the strategy an for   grams [Howard and Matheson, 1984] to the multi-agent case. 
an agent n is represented as a vector of real values of size hn, A MAID is represented as a directed acyclic graph over 
one for each distinct history for a leaf z in the tree.        nodes of three types: chance, decision, and utility. (Utility 
The number , abbreviated , is the product of 
the probabilities controlled by agent n along the history         1 For notational simplicity, Gn{z) includes nature's probabilities. 


758                                                                                             MULTIAGENT SYSTEMS nodes are assumed to have no children.) Chance nodes repre•   which point the corresponding value of w is a solution to the 
sent nature's actions, and a conditional probability distribu- original problem. A continuation method begins at the known 
tion (CPD) is associated with each such node, describing the  solution for = 1 . The null-space of the Jacobian at a 
distribution over outcomes of that variable conditioned on the current solution defines a direction, along which the 
values of its parents. Each decision node is associated with  solution is moved by a small amount. The process then re•
a single agent. The parents of a decision node represent the  peats, tracing the curve until = 0. The cost of each step in 
variables whose values are known to the agent when mak•       this computation, given the Jacobian, is cubic in the size of 
ing that decision. Thus, an agent's decision rule for a node  the Jacobian, due to the required matrix operations. 
can specify a different strategy for each assignment of values  Continuation methods involve the tracing of a dynamical 
to the node's parents. In effect, each such assignment corre• system through the continuous variation of the parameter . 
sponds to an information set. A randomizing decision rule for For computational purposes, discrete steps must be taken. As 
a decision node is simply a CPD: a distribution over its values a result, error inevitably accumulates as the path is traced. 
for each instantiation of its parents.                        One can use several techniques to reduce the error, which we 
  Each utility node is associated with an agent, and repre•   do not describe for lack of space. Unfortunately, these tech•
sents a component in that agent's payoff. The utility node    niques can potentially send the algorithm into a cycle, and in 
takes real values as a deterministic function of its parents. practice they occasionally do. If the algorithm cycles, random 
Thus, that component of the agent's payoff depends only on a  restarts and a decrease in step size can improve convergence. 
subset of the variables in the MAID. The agent's overall util•
ity is the sum of the utilities obtained at its different utility 4 Continuation Methods for Games 
nodes. It is easy to show that a MAID defines an extensive-
                                                              We now review the work of Kohlberg and Mertens [1986] 
form game. If we use CPDs and decision rules that are tree-
                                                              and GW on applying the continuation method to the task of 
structured, then the MAID representation is no larger than 
                                                              finding equilibria in games. These algorithms form the ba•
the corresponding extensive-form representation, and is expo•
                                                              sis for our extension to structured games, described in the 
nentially smaller in many cases. Note that a graphical game is 
                                                              next section. The continuation method perturbs the game by 
simply a MAID where each agent has a single decision node 
                                                              adding A times a fixed bonus to each agent's payoffs, such 
and a single utility node, and where the parents of an agent 
                                                              that an agent's bonus depends only on its own actions. If the 
n's utility node are the decision nodes for the agents in Fam . 
                                                        n     bonuses are large enough (and unique), the bonuses dominate 
                                                              the original game structure, and the agents need not consider 
3   Continuation Methods                                      their opponents' plays. Thus, for = 1, the perturbed game 
We begin with a high-level overview of continuation meth•     has only one equilibrium: each agent plays the action with the 
ods, referring the reader to [Watson, 2000] for a more detailed largest bonus. We then use the continuation method to follow 
discussion. Continuation methods work by solving a simpler    a path in the space of and equilibrium profiles for the re•
perturbed problem and then tracing the solution as the mag•   sulting perturbed game, decreasing until it is zero; at this 
nitude of the perturbation decreases, converging to a solution point, the corresponding strategy profile is an equilibrium of 
to the original problem.                                      the original game. We now make this intuition more precise. 
  More precisely, let be a scalar parameterizing a contin•
uum of perturbed problems. When = 0, the perturbed prob•      4.1   Normal Form Games 
lem is the original one; when = 1, the perturbed problem      In order to apply Eq. (2), we need to characterize the equi•
is one for which the solution is known. Let w represent the   libria of perturbed games as the zeros of a function F. We 
vector of real values of the solution. For any perturbed prob• first define an auxiliary function measuring the benefit of de•
lem defined by • we characterize solutions by the equation    viating from a given strategy profile. Specifically, is a 
         = 0, where F is a real-valued vector function (so    vector payoff function of the payoff to agent n for deviating 
that 0 is a vector of zeros). The function F is such that if  from the mixed strategy profile a by playing each action s: 
         = 0 holds then w is a solution to the problem per•
turbed by . 
  The continuation method traces solutions along the mani•
fold of solution pairs satisfying = 0. Specif•                  We now define a retraction operator R : to 
ically, if we have a solution pair we would like to           be an operator that maps arbitrary ra-vectors w to the point 
trace that solution to adjacent solutions. Differential changes in the space of mixed strategies which is nearest to w 
to w and must cancel out so that F remains equal to 0.        in Euclidean distance. As outlined in the structure theo•
Thus, locally, changes dw and dX along the path must obey     rem of Kohlberg and Mertens [1986], an equilibrium is 
                                     which is equivalent to   recoverable from by the retraction operator R: 
the matrix equation                                                                In fact, this condition is a full charac•
                                                              terization of equilibria. Thus, we can define an equilibrium 
                                                       (2)    as a solution to the equation Conversely, 
                                                              if and we have the equivalent 
  If the matrix has a null-space of rank 1 every•             condition that Thus, we can search 
where, the curve is uniquely defined. If properly constructed, for a point _ which satisfies this equality, in which 
the curve starting at = 1 is guaranteed to cross = 0, at      case is guaranteed to be an equilibrium. 


MULTIAGENT SYSTEMS                                                                                                  759    We now define the game perturbation and associated con•
tinuation method. For a target game G with payoff function 
 V, we create an easily soluble perturbed game by adding an 
rn-vector 6 of unique bonuses that agents receive for play•
 ing certain actions, independently of what all other agents 
do. In perturbing the game G by b, we arrive at a new game 
        in which for each , and for any t  
                                  If we make 6 sufficiently 
large, then has a unique equilibrium, in which each            Figure 1: (a) An abstract diagram of the path. The horizontal axis 
agent plays the pure strategy s for which is maximal.          represents and the vertical axis represents the space of strategy 
   Now, let V be the payoff function for the target game G.    profiles (actually multidimensional). The algorithm starts on the 
The induced payoff function is also perturbed from             right at =1 and follows the dynamical system until = 0 at 
         . Thus, the form of our continuation equation is:     point 1, where it has found an equilibrium of the original game. It 
                                                               can continue to trace the path and find the equilibria labeled 2 and 3. 
                                                        (3)    (b) Two-stage road building MAID tor three agents. 
We have that is the payoff function for the perturbed 
                                                               where is the set of leaves that are consistent with the 
game is zero if and only if is an 
                                                               sequences s  (for agent n ) and s  (for agent n )- We take 
equilibrium of the game is unperturbed, so                                 1            1       2             2
                                                                      to be the empty set (and hence if s          and s
                       is an equilibrium of G.                                                                   1      2 
                                                               are incompatible. Eq. (5) is precisely analogous to Eq. (4) for 
   The expensive step in the continuation method is the cal•
                                                               normal-form games. We have a sum, over outcomes, of the 
culation of the Jacobian required for the computation 
                                                               utility of the outcome multiplied by the strategy probabilities 
that maintains the constraint of Eq. (2). Here, we have that 
                                                               for all other agents. Note that this sum is over the leaves of 
                          , where I is the identity ma•
                                                               the tree, which may be exponential in the number of agents. 
trix. The hard part is the calculation of For pure strate•
                                                                 Zero-probability actions in extensive-form games give rise 
gies and , the value at location (s                   , s ) 
                                                      1  2     to an additional subtlety. Such actions induce a probability 
in is equal to the expected payoff to agent n        when 
                                                    1          of zero for entire trajectories in the tree, possibly leading to 
it plays the pure strategy s , agent plays the pure strategy 
                          1                                    equilibria based on unrealizable threats and other undesirable 
.s , and all other agents act according to the strategy profile a: 
  2                                                            phenomena. For us, they can also lead to bifurcations in the 
                                                               continuation path, preventing convergence. Thus, we con•
                                                               strain all sequence form parameters to be greater than or equal 
                                                               to e for some small e. This constraint ensures that the contin•
Computing Eq. (4) requires a number of multiplications         uation path is a 1-manifold. The algorithm thus finds an equi•
which is exponential in the game size; the sum is over the     librium to a perturbed game, where agents have a small prob•
exponentially large space                                      ability of choosing an unintended action. As c tends to zero, 
                                                               these equilibria converge to perfect equilibria of the original 
4.2 Extensive Form Games                                       game [Owen, 1995], a (nonempty) subset of all equilibria. 
The same method applies to extensive-form games, using the     For e small enough, continuity implies that there is always 
sequence form parameterization of strategies. We first need    an exact perfect equilibrium in the vicinity of the perturbed 
to define the bonus vector, and the retraction operator which  equilibrium, which can easily be found using local search. 
allows us to characterize equilibria. 
   The bonus vector in the extensive-form adds a bonus for     4.3 Path Properties 
each sequence of each agent. GW show that a sufficiently 
large bonus guarantees a unique equilibrium for the perturbed  In the case of normal-form games, the structure theorem of 
game. The retraction operator R takes a general vector and     Kohlberg and Mertens [1986] implies that, with probability 
projects it onto the valid region of sequence forms. As all of one over all choices for 6, the path of the algorithm is a one-
the constraints are linear, the projection amounts to a decom• manifold without boundary. GW provide an analogous struc•
posable quadratic program (QP) that can be solved quickly.     ture theorem that guarantees the same property for extensive-
We employ standard QP methods. The Jacobian of the retrac•     form games. Figure 1 (a) shows an abstract representation of 
tion is easily computable from the set of active constraints.  the path followed by the continuation method. The equilib•
                                                               rium for large positive is unique, so the one-manifold can•
   The solution for the sequence form is now surprisingly 
                                                               not double back to the side of = Furthermore, the 
similar to that of the normal-form. The key property of the 
                                                               perturbed games along the path can have only a finite number 
sequence form strategy representation is that the payoff func•
                                                               of discrete equilibria, so the path cannot travel back and forth 
tion is a multi-linear function of the extensive-form parame•
                                                               indefinitely. Therefore, it must cross the = 0 hyperplane 
ters, as shown in Eq. (1). The elements of the Jacobian 
                                                               at least once, yielding an equilibrium. In fact, the path may 
also have the same general structure. In particular the element 
                                                               cross multiple times, yielding many equilibria in a single run. 
corresponding to sequence s   for agent n  and sequence s
                            1           1                2     As the path must eventually continue to the side, it 
for agent n  is 
           2                                                   will find an odd number of equilibria when run to completion. 
                                                                 In both normal-form and extensive-form games, the path 
                                                               is piece-wise polynomial, with each piece corresponding to a 


760                                                                                             MULTIAGENT SYSTEMS different support set of the strategy profile. These pieces are locally, considering only n1 's family. More precisely, we can 
called support cells. The path is not smooth at cell bound•    consider two cases. If n2 Famn1, then 
aries, due to discontinuities in the Jacobian of the retrac•
tion operator, and hence in when the support changes. 
Thus, in following the path, care must be taken to step up to 
these boundaries exactly. 
   In the case of two agents, the path is piece-wise linear and, 
rather than taking steps, the algorithm can jump from "elbow" 
to "elbow" along this path. When this algorithm is applied to 
a two-agent game and a particular bonus vector is used, the 
steps from support cell to support cell that the algorithm takes 
are exactly equal to the pivots of the Lemke-Howson solution 
algorithm [Lemke and Howson, 1964] for two-agent games, 
and the two algorithms find precisely the same set of solu•
tions. Thus, the continuation method is a strict generalization 
of the Lemke-Howson algorithm that allows different pertur•
                                                                 Letting / be the maximal family size, and d the maximal 
bation rays and games of more than two agents. 
                                                               number of actions per agent, we have that the computation of 
4.4   Iterated Polymatrix Approximation                        the Jacobian requires time  
Because perturbed games may themselves have an exponen•        5.2   MAIDs 
tial number of equilibria, and the path may wind back and 
                                                               The Jacobian for MAIDs 
forth through any number of them, the continuation algorithm 
can take a while to trace its way back to a solution to the orig• To find equilibria in MAIDs, we extend the sequence form 
inal game. We can speed up the algorithm using an initializa•  continuation method of Section 4.2. As above, our key task is 
tion procedure based on the iterated polymatrix approxima•     computing the Jacobian of Eq. (5). The Jacobian has an entry 
tion (IPA) algorithm of GW. A polymatrix game is a normal-     for each pair of sequences in the game (one for each agent). 
form game where the payoffs to a agent n are equal to the sum  We therefore begin by noting that the sequence form repre•
of the payoffs from a set of two-agent games, each involving   sentation for MAIDs with perfect recall is no larger than the 
n and another agent. Polymatrix games can be solved quickly    agent's decision rules for that MAID. Due to perfect recall, 
using the Lemke-Howson algorithm [1964].                       the last decision node (in topological order) must have in•
   Given a normal-form game G and a strategy profile c we      coming edges from all of its previous actions and all parents 
                                                               of previous actions. Moreover, it must have an information 
can construct a polymatrix game Pa whose Jacobian at o is 
the same as the Jacobian of G's payoff function V at a. The    set for any distinct agent history. Thus, the agent's decision 
                                                               rule for that final decision has the same size as the sequence 
game Pa is a linearized approximation to G around cr, and 
can be computed efficiently from the Jacobian of G. GW pro•    form. Hence, the dimension m of the Jacobian ma•
vide an iterative algorithm that, in each step, takes a profile o trix is linear in the size of the MAID (where size, as usual, 
and improves it using the solution of the polymatrix approx•   includes the size of the parameterization). 
                                                                 We next turn to the computation of the Jacobian entries. 
imation Pa. This algorithm is not guaranteed to converge, 
but in practice, it quickly moves "near" a good solution. We   Eq. (5) can be rewritten as 
then construct a perturbed game close to the original game for 
which this approximate equilibrium is an exact equilibrium.                                                           (8) 
The continuation method is then run from this starting point 
to find an exact equilibrium of the original game. 
                                                               A leaf node z in the extensive-form game is simply an as•
5   Exploiting Structure                                       signment x to all of the variables in the MAID, and 
                                                               is n1's utility given x. The sequence probability is 
As mentioned above, the calculation of at each step of the     the product of the probabilities for the decisions of agent 
algorithm consumes most of the time. Both in normal-form       n in the assignment x. Thus, Eq. (8) is an expectation of 
and (in the worst case) in extensive-form games, it requires                          . The expectation is over the distribu•
time that is exponential in the number of agents. However,     tion defined by the Bayesian network whose structure is 
as we show in this section, when using a structured represen•  the same as the MAID, and where the agents' decision nodes 
tation such as a graphical game or a MAID, we can effec•       have CPDs determined by  
tively exploit the structure of the game to drastically reduce   The agent's utility is the sum of its utility nodes. 
the computational time required.                               Due to linearity of expectation, we can perform the computa•
                                                               tion separately for each of the agent's utility nodes, and then 
5.1   Graphical Games                                          simply add up the separate contributions. Thus, we assume 

Consider the computation of the normal-form Jacobian in        from here on, without loss of generality, that n1 has only a 
Eq. (4). The key insight is that the choice of strategy for an single utility node U. 

agent outside the family of n1 does not affect . This ob•        The value of depends only on the values 
servation allows us to compute the n1 entries in the Jacobian  of the set of nodes £>„, consisting of n1;'s decision nodes and 


MULTIAGENT SYSTEMS                                                                                                   761 