         Cho-k-NN:      A  Method     for Combining      Interacting    Pieces   of Evidence
                                     in Case-Based      Learning

                                           Eyke  Hullermeier¨
                   Otto-von-Guericke-Universitat¨ Magdeburg,   Fakultat¨ fur¨ Informatik
                            Universitatsplatz¨ 2, 39106 Magdeburg, Germany
                              eyke.huellermeier@iti.cs.uni-magdeburg.de

                    Abstract                          query) is actually requested, a property which qualiﬁes CBL
                                                      as a lazy learning method [Aha, 1997]. Predictions are then
    The case-based learning paradigm relies upon      derived by combining the information provided by the stored
    memorizing cases in the form of successful prob-  cases, primarily by those which are similar to the new query.
    lem solving experience, such as e.g. a pattern along
                                                        In fact, the concept of similarity plays a central role in
    with its classiﬁcation in pattern recognition or a
                                                      CBL.  The major assumption underlying CBL has already
    problem along with a solution in case-based rea-
                                                      been expressed, amongst others, by the philosopher DAVID
    soning. When it comes to solving a new problem,
                                                      HUME  ([Hume, 1999], page 116): “In reality, all arguments
    each of these cases serves as an individual piece of
                                                      from experience are founded on the similarity ... among natu-
    evidence that gives an indication of the solution to
                                                      ral objects. ... From causes, which appear similar, we expect
    that problem. In this paper, we elaborate on issues
                                                      similar effects.” This commonsense principle, that we shall
    concerning the proper combination (aggregation)
                                                      occasionally call the “similarity hypothesis” [Rendell, 1986],
    of such pieces of evidence. Particularly, we argue
                                                      serves as a basic inference paradigm in various domains of
    that cases retrieved from a case library must not be
                                                      application. For example, in a classiﬁcation context, it trans-
    considered as independent information sources, as
                                                      lates into the assertion that “similar objects have similar class
    implicitly done by most case-based learning meth-
                                                      labels”, and in CBR it suggests that “similar problems have
    ods. Focusing on the problem of prediction as a
                                                      similar solutions”.
    performance task, we propose a new inference prin-
    ciple that combines potentially interacting pieces  The similarity hypothesis, which is apparently of a heuris-
    of evidence by means of the so-called (discrete)  tic nature, has been put into practice by means of different in-
    Choquet-integral. Our method, called Cho-k-NN,    ference schemes that combine similarity and frequency infor-
    takes interdependencies between the stored cases  mation in one way or the other. For example, the well-known
    into account and can be seen as a generalization of nearest neighbor (NN) classiﬁer ﬁrst selects a neighborhood
    weighted nearest neighbor estimation.             around the query, consisting of the k most similar cases, and
                                                      then counts the occurrence of the different class labels. De-
                                                      spite of their simplicity, inference methods of such kind have
1  Introduction                                       proved to be quite successful in practical applications.
Case-based or instance-based learning algorithms have been In this paper, we suggest a further improvement based
applied successfully in ﬁelds such as, e.g., machine learn- on the idea of taking interdependencies between neighbored
ing and pattern recognition during the recent years [Aha et cases into account. In fact, in standard NN methods, these
al., 1991; Dasarathy, 1991]. The case-based learning (CBL) cases are implicitly considered as independent information
paradigm is also of central importance in case-based reason- sources. We argue that a corresponding assumption of in-
ing (CBR), a problem solving methodology which goes be- dependence is not always justiﬁed and propose a new infer-
yond the standard prediction problems of classiﬁcation and ence principle that combines interacting pieces of evidence
regression [Riesbeck and Schank, 1989; Kolodner, 1993]. in a more thorough way. This principle, which makes use
  As the term suggests, in CBL special importance is at- of non-additive measures for modeling interaction between
tached to the concept of a case. A case or an instance can cases and employs the so-called (discrete) Choquet-integral
be thought of as a single experience, such as a pattern (along as an aggregation operator, can be seen as a generalization of
with its classiﬁcation) in pattern recognition or a problem weighted NN estimation.
(along with a solution) in CBR.                         By way  of background, section 2 gives a concise review
  Rather than inducing a global model (theory) from the of the NN principle, which constitutes the core of the family
data and using this model for further reasoning, as inductive, of CBL algorithms. In section 3, we discuss the problem of
model-based machine learning methods typically do, CBL interaction between cases in CBL. A new approach to CBL,
systems simply store the data itself. The processing of the which takes such interactions into account, is then introduced
data is deferred until a prediction (or some other type of in section 4 and evaluated empirically in section 5.2  Nearest  Neighbor   Estimation
                                                                       x2           x1    x2
The well-known nearest neighbor (NN) estimation principle
is applicable to both classiﬁcation problems (prediction of
                                                                      x0               x0
class labels) and regression (prediction of numeric values).
  Consider a setting in which an instance space X is en-          x       x                x
dowed with a similarity measure sim : X × X → [0, 1]. An           1       3                3
instance corresponds to the description x of an object (usu- Figure 1: Different conﬁgurations of locations in two-
ally in attribute–value form). In the standard classiﬁcation dimensional space.
framework, each instance x is assumed to have a (unique) la-
bel y ∈ L. Here, L is a ﬁnite (typically small) set comprised
                                                      outcomes (see (2)). Thus, the neighbored cases are basically
of m class labels {` . . . ` }, and hx, yi ∈ X × L is a labeled
                1    m                                considered as independent information sources.
instance (case).
                                                        This assumption of independence between case-based ev-
  The NN  principle originated in the ﬁeld of pattern recog-
                                                      idence can thoroughly be called into question [Hullermeier¨ ,
nition [Dasarathy, 1991]. Given a sample S consisting of n
                                                      2002]. Indeed, it is not even in agreement with the similarity
labeled instances hx , y i, 1 ≤ ı ≤ n, and a novel instance
                 ı ı                                  hypothesis itself! Namely, if this hypothesis is true, then two
x  ∈ X  (a query), this principle prescribes to estimate the
 0                                                    neighbored cases that are not only similar to the query case
label y of the yet unclassiﬁed query x by the label of the
     0                           0                    but also similar among each other will probably provide sim-
nearest (most similar) sample instance. The k-nearest neigh-
                                                      ilar information regarding the query. In other words, when
bor (k-NN) approach is a slight generalization, which takes
                                                      taking the similarity hypothesis for granted, the information
the k ≥ 1 nearest neighbors of x0 into account. That is, an
          est                                         coming from the neighbored cases is at least not independent.
estimation y0 of y0 is derived from the set Nk(x0) of the k
nearest neighbors of x , usually by means of a majority vote. In particular, from a problem solving perspective, one
                  0                                   should realize that a set of cases can be complementary in
Besides, further conceptual extensions of the (k-)NN princi-
                                                      the sense that the experiences represented by the individual
ple have been devised, such as distance weighting [Dudani,
1976]:                                                cases complement or reinforce each other. On the other hand,
                                                      cases can also be redundant in the sense that much of the in-
         est                       I                  formation is already represented by a smaller subset among
        y0 =  arg max           ωx · (y = `)    (1)
                 `∈L                                  them. And indeed, as we said before, the similarity hypothe-
                     hx,yi∈NX (x0)
                           k                          sis suggests that similar cases are likely to be redundant.
where ωx is the weight of the instance x and I(·) the standard To illustrate this point by a simple example, consider the
{true, false} → {0, 1} mapping. (Throughout the paper, we problem of predicting student Peter’s grade in computer sci-
assume the weights to be given by ωx = sim(x, x0).)   ence, knowing that he has an A in French. The latter infor-
  The NN principle can also be used for regression problems, mation (i.e. the case hFrench, Ai) clearly suggests that Peter
i.e., for realizing a (locally weighted) approximation of real- is an excellent student and, hence, supports predicting an A
valued target functions x 7→ y = f(x) (in this case, L = R). or maybe a B. Yet, one cannot be sure that this prediction is
To this end, one reasonably computes the (weighted) mean of correct. In this situation, the additional information that Pe-
the k nearest neighbors of a new query point:         ter has a B in mathematics is probably more valuable than
                                                      the information that he has an A in Spanish as well. In fact,
                     hx,yi∈N (x0) ωx · y              the cases hFrench, Ai and hSpanish, Ai are partly redundant,
             yest =         k                   (2)
              0    P             ω                    since the two subjects are quite similar by themselves. As op-
                       hx,yi∈Nk(x0) x
                     P                                posed to this, the case hmathematics, Bi is complementary in
                                                      a sense, as it suggests that Peter is not only good in languages.
3  Interaction  Between   Cases in CBL                  Now, suppose that you know all three grades (mathematics,
Some case-based approaches completely rely on the (suppos- Spanish, French). Is A or B the more likely grade? Of course,
edly) most relevant piece of evidence, namely the observation mathematics is more similar to computer science than Span-
which is most similar to the query. In CBR, for example, it is ish and French. However, depending on the concrete spec-
common  practice to retrieve just a single case from the case iﬁcation of similarity degrees between the various subjects,
library, and to adapt the corresponding solution to the prob- it is quite possible that the weighted k-NN rule favors grade
lem under consideration. On the one hand, ignoring all but A since the two moderately similar A’s compensate for the
the most similar observation is conceptually simple and com- more similar B. Of course, this result might be judged criti-
putationally efﬁcient. On the other hand, this approach does cally, and one might wonder whether the grade A should re-
of course come along with a loss of information, because only ally count twice. In fact, one reasonable alternative is to con-
a very small part of the past experience is utilized. sider the two A’s as only a single piece of evidence, telling
  If several instead of only a single case are retrieved, as e.g. something about Peter’s achievements in languages, instead
in k-NN, an important question arises: How should the pieces of two pieces of distinct information. In any case, the close
of evidence coming from the different cases be combined? In connection between the two A’s should not be ignored when
k-NN classiﬁcation, the evidences in favor of a certain class combining the three observations.
label are simply added up (see (1)). Likewise, in regression As a second example, consider the problem of predicting
the estimation is a simple linear combination of the observed the yearly rainfall at a certain location (city). For instance,given the rainfall yı at location xı (ı = 1, 2, 3), what about the 4 The Cho-k-NN Method
                x
rainfall at location 0 in the two scenarios shown in Fig. 1? 4.1 Non-Additive Measures
The important point to notice is that even though the indi-
                                                      Let X be a ﬁnite set and ν(·) a measure 2X → [0, 1]. For any
vidual distances between x0 and the xı are the same in both
                                                      A ⊆ X, we interpret ν(A) as the weight or, say, the degree of
scenarios, the yı should not be combined in the same way. In
this example, this is due to the different arrangements of the relevance of the set of elements A.
neighbors [Zhang et al., 1997]: Simply predicting the arith- A standard assumption on the measure ν(·), which is made
                                                      e.g. in probability theory, is additivity: ν(A ∪ B) = ν(A) +
metic mean (y1 + y2 + y3)/3 seems to be reasonable in the
left scenario, while the same prediction appears questionable ν(B) for all A, B ⊆ X s.t. A ∩ B = ∅. Unfortunately, addi-
                                                      tive measures cannot model any kind of interaction between
in the scenario shown in the right picture. In fact, since x1 and
                                                      elements: Extending a set of elements A by a set of elements
x2 are closely neighbored in the latter case, information about
the rainfall at these locations will be partly redundant. Con- B always increases the weight ν(A) by the weight ν(B), re-
sequently, the weight of the (joint) evidence that comes from gardless of A and B.
                                                        Suppose, for example, that the elements of two sets A and
the observations hx1, y1i and hx2, y2i should not be twice as
                                                      B  are complementary in a certain sense. To illustrate, one
high as the weight of the evidence that comes from hx3, y3i.
                                                      might think of X as a set of workers in a factory, and of a
                                                            ν(A)                                    A
  The above examples show the need for taking interdepen- weight as the productivity of a team of workers . In
dencies between observed cases into account and, hence, pro- that case, complementarity means that the output produced
                                                                  A    B
vide a motivation for the method that will be proposed in the by two teams and is higher if they cooperate. Formally,
                                                                                              ν(A ∪ B) >
following section. Before proceeding, let us make two further this can be expressed as a positive interaction:
                                                      ν(A) + ν(B)
remarks: First, the above type of interaction between cases       . Likewise, elements can interact in a negative
                                                                                A    B
seems to be less important if the sample size is large and even way. For example, if two sets and are partly redundant
                                                                       ν(A ∪ B) < ν(A) + ν(B)
becomes negligible in asymptotic analyses of NN principles. or competitive, then             .
In fact, strong results on the performance of NN estimation The above considerations motivate the use of non-additive
can be derived [Dasarathy, 1991], but these are valid only measures, also called fuzzy measures, which are simply nor-
                                                                          [           ]
under idealized statistical assumptions and arbitrarily large malized and monotone Sugeno, 1974 :
sample sizes. Roughly speaking, if the sample size n tends to • ν(∅) = 0, ν(X) = 1
inﬁnity, the distance between the query and its nearest neigh- • ν(A) ≤ ν(B) for A ⊆ B
bors becomes arbitrarily small (with high probability). This
                                                        In order to quantify the interaction between subsets of a set
holds true even if the size k of the neighborhood is increased
                                                      X, as induced by a fuzzy measure ν(·), several indices have
too, as a function k(n) of n, provided that k(n)/n → 0 for
                                                      been proposed in the literature. For two individual elements
n →  ∞. Moreover, if the individual observations are inde-
                                                      x , x ∈ X, the interaction index is deﬁned by
pendent and identically distributed in a statistical sense, the ı 
neighborhood becomes “well distributed”. Under these as-        df           (|X| − |A| − 1)! |A|!
sumptions, it is intuitively clear that interdependencies be- Iν (xı, x) =                    (∆ıν)(A),
                                                                      X          (|X| − 1)!
tween observations will hardly play any role. On the other        A⊆X\{xı,x}
hand, it is also clear that statistical assumptions of such kind where |A| denotes the cardinality of A and
will almost never be satisﬁed in practice.
                                                                     df
                                                           (∆ıν)(A) = ν(A ∪ {xı, x}) − ν(A ∪ {xı}) −
  The second remark concerns related work. In fact, there               − ν(A ∪ {x }) + ν(A).
are a few methods that ﬁt into the CBL framework and that                         
allow for taking certain types of interaction between observa- The latter can be seen as the marginal interaction between
tions into account. Particularly, these are methods that make xı and x in the context A [Murofushi and Soneda, 1993].
assumptions on the statistical correlation between observa- The above index can be extended from pairs xı, x to any set
tions, depending on their distance [Lindenbaum et al., 1999]. U ⊆ X of elements [Grabisch, 1997]:
For example, in our rainfall example one could employ a
                                                               df       (|X| − |A| − |U|)! |A|!
method called kriging, which is well-known in geostatistics Iν (U) =                        (∆U ν)(A),
                                                                            (|X||U| + 1)!
[Oliver and Webster, 1990]. Usually, however, such methods        A⊆XX\U
are specialized on a particular type of application and, more-
                                                      where
over, make rather restrictive assumptions on the mathematical
                                                                        df        |U|−|V |
(metric) structure of the instance space. Our approach, to be (∆U ν)(A) =     (−1)      ν(V ∪ A).
detailed in the next section, is much more general in the sense           VX⊆U
that it only requires a similarity measure sim(·) to be given.
We do not make any particular assumptions on this measure 4.2 Modeling Interaction in Case-Based Learning
(such as symmetry or any kind of transitivity), apart from the Now, recall our actual problem of combining evidence in
fact that it should be normalized to the range [0, 1]. From an case-based learning. In this context, the set X of elements
application point of view, this seems to be an important point. corresponds to the neighbors of the query case x0:
In CBR, for example, cases are typically complex objects that
cannot easily be embedded into a metric space.                    X  = Nk(x0) = {x1, x2 . . . xk}     (3)Our comments so far have shown that fuzzy measures can  As can be seen, the joint weight of {x1, x2} is relatively
principally be used for modeling interaction between cases. low, reﬂecting the partial redundance of the two cases.
The basic question that we have to address in this connection Before going on, let us comment on the derivation of the
is the following: What is the evidence weight or simply the measure ν(·) from the similarity function sim(·). Firstly, even
weight, ν(A), of a subset A of the neighborhood (3)?  though the measure (6) captures our intuitive idea of decreas-
  First, the boundary conditions ν(∅) = 0 and ν(X) = 1 ing (increasing) the evidence weight of cases that are (dis-
should of course be satisﬁed, expressing that the full evidence )similar by themselves, we admit that it remains ad hoc to
is provided by the complete neighborhood X. Moreover, ac- some extent, and by no means we exclude the existence of
cording to our comments on the similarity hypothesis (sec- better alternatives. For example, an interesting idea is to de-
tion 3), the evidence coming from a set of cases A ⊆ X rive the measure in an indirect way: First, the interaction in-
should be discounted if these cases are similar among them- dices Iν (·) from section 4.1 are deﬁned, again based on the
selves. Likewise, the weight of A should be increased if similarity between cases. These indices can be seen as con-
the cases are “diverse” (hence complementary) in a certain straints on the measure ν(·), and the idea is to ﬁnd a measure
sense.1 To express this idea in a more rigorous way, we de- that is maximally consistent with these constraints.
ﬁne the diversity of a set of cases A by the average pairwise Secondly, even though the assumption that similar cases
dissimilarity:                                        provide redundant information is supported by the similarity
                                                      hypothesis, one might of course argue that the similarity be-
             df    2
      div(A) =                 1 − sim(xı, x)        tween the predictive parts of two cases, x and x , is not suf-
               |A|2 − |A|                                                                ı     
                         xıX,x∈A                     ﬁcient to call them redundant. Rather, the associated output
                                                      values y and y should be similar as well. Indeed, if y differs
(By deﬁnition, the diversity is 0 for singletons and the empty ı                                 1
                                                      drastically from y2, the ﬁrst two measurements in our rainfall
set.) We furthermore deﬁne the relative diversity by  example might better be considered as non-redundant. (In
            df       2 div(A)                         that case, the two measurements in conjunction suggest that
    rdiv(A) =                     − 1 ∈ [−1, 1]       there is something amiss ...) This conception of redundancy
               1 − minı, sim(xı, x)
                                                      can easily be represented by deriving ν(·) from an extended
Now, the idea is to modify the basic (additive) measure similarity measure sim0(·) which is deﬁned over X × L.2

                 df  −1                                 Anyway, the important point of this section is not so much
            µ(A) =  c       sim(x0, xı),        (4)   the speciﬁcation of a particular evidence measure, but rather
                        X
                       xı∈A                           the insight that non-additive measures can in principle be
                                                      used for modeling the interaction between cases in CBL.
where c =   xı∈X sim(x0, xı), by taking the diversity of A
into account.POf course, this can be done in different ways.
Here, we used the following approach:                 4.3  Aggregation  of Interacting Pieces of Evidence

                 df                                   So far, we have a tool for modeling the interaction between
           ν¯(A) = µ(A) · (1 + α rdiv(A))       (5)   different pieces of evidence in case-based learning. The next
As can be seen, the original measure µ(A) of a set of cases question that we have to address is how to combine these
A is increased if the diversity of A is relatively high, other- pieces of evidence, i.e., how to aggregate them in agreement
wise it is decreased. The parameter α ≥ 0 controls the extent with the evidence measure ν(·).
to which interactions between cases are taken into considera- For the time being we focus on the problem of regression.
tion: µ(A) can be modiﬁed by at most (100 α)%. For α = 0, Recall that in the standard approach to NN estimation, an
interactions are completely ignored and the original measure aggregation of the output values f(xı) = yı is realized by
µ(·) is recovered.                                    means of a simple weighted average:
  For the measure ν¯(·) as deﬁned in (5), the monotonicity          est
condition does not necessarily hold. To remedy this problem,       y0  =      µ({xı}) · f(xı),        (7)
we simply enforce this property by setting                                xXı∈X

                      df                                                                          −1
                 ν(A) = max  ν¯(B).             (6)   where µ({xı}) = sim(x0, xı)  xı∈X sim(x0, xı)  . In-
                        B⊆A                           terestingly, (7) is nothing else than Pthe standard Lebesgue in-
Finally, the boundary conditions are guaranteed by dividing tegral of the function f : X → R with respect to the additive
the measure thus obtained by ν(X).                    measure (4):
  To illustrate, consider again the rainfall example in the
                                                                          est
right picture of Fig. 1 and suppose that sim(xı, x0) = .5 (ı =           y   =    f dµ
                                                                          0    Z
1, 2, 3), sim(x1, x2) = .9, sim(x1, x3) = sim(x2, x3) = 0,                       X
    α = 1/2
With        in (5), we obtain the following weights:  In order to generalize this estimation, an integral with respect
    A     x1   x2   x3   x1, x2  x1, x3  x2, x3       to the non-additive measure ν(·) is needed: the Choquet in-
   ν(A)   .27  .27  .27    .33    .83     .83         tegral, a concept that originated in capacity theory [Choquet,
  1The idea of “diversity” of a set of cases plays also a role in 1954].
case-based retrieval [McSherry, 2002]. Here, however, the problem
is more to ﬁnd diverse cases, rather than to combine them. 2We didn’t explore this alternative in detail so far.  Let ν(·) be a fuzzy measure and f(·) a non-negative func- data set   k   weighted k-NN   Cho-k-NN
tion.3 The Choquet integral of f(·) with respect to ν(·) is then auto-mpg 5   12.21 (0.05) 11.56 (0.05)
deﬁned by                                                              7      12.18 (0.06) 11.53 (0.05)
              ch         ∞                                 bolts       5      47.07 (1.19) 38.77 (0.71)
                f dν =df   η([f > t]) dt                               7      51.36 (1.25) 39.94 (0.81)
            Z          Z0                                  housing     5      14.83 (0.08) 14.48 (0.08)
                                                                       7      14.99 (0.09) 14.62 (0.08)
     [f > t] = {x | f(x) > t}
where                       . The integral on the right-   detroit     5      16.02 (0.55) 14.90 (0.50)
hand side is the standard Lebesgue integral (with respect to
                                                                       7      15.93 (0.55) 14.71 (0.54)
the Borel measure on [0, ∞)). In our case, where X is a ﬁnite
                                                           echomonths  5      97.77 (7.17) 72.87 (3.87)
set, we can refer to the discrete Choquet integral which can
                                                                       7      99.03 (8.98) 74.80 (7.55)
be expressed in a rather simple form:
                                                           pollution   5       4.12 (0.05) 4.05 (0.05)
               k                                                       7       4.22 (0.05) 4.18 (0.05)
        est
       y0  =     f(xπ(ı)) · ν(Aı) − ν(Aı−1) ,   (8)
              Xı=1                                   Table 1: Estimation of expected relative estimation error and
                                                      its standard deviation.
where π(·) is a permutation of {1 . . . k} such that 0 ≤
f(xπ(1)) ≤ . . . ≤ f(xπ(k)), and Aı = {xπ(1) . . . xπ(ı)}.
  The discrete Choquet integral (8) can be seen as a special simply took the misclassiﬁcation rate as a performance index.
type of aggregation operator, namely a generalized arithmetic Moreover, we derived statistical estimations of the expected
mean. Indeed, (8) coincides with (7) if ν(·) is an additive performance of a method by repeating each experiment 100
measure (i.e. if ν(·) = µ(·)). Otherwise, it is a proper gener- times.
alization of the standard (weighted) NN estimation.     For the purpose of similarity computation, all numeric at-
  Coming back to our running example, suppose that we tributes have ﬁrst been normalized to the unit interval by lin-
have measured the following rainfalls: y1 = 100, y2 = 120, ear scaling. The similarity was then deﬁned by 1−distance
y3 = 200. According to (8), we then obtain the estimation for numeric variables and by the standard 0/1-measure in the
 est
y0  ≈  157. As the joint weight of the two locations with case of categorical attributes. The overall similarity sim(·)
less rainfall, x1 and x2, has been decreased, this estimation is was ﬁnally obtained by the average over all attributes. As
higher (closer to y3) than the standard weighted NN estima- the purpose of our study was to compare – under equal con-
            est
tion given by y0 = 140.                               ditions – weighted k-NN with Cho-k-NN in order to verify
  So far, we have focused on the problem of regression. In whether or not taking interactions into account is useful, we
the case of classiﬁcation, the Choquet integral cannot be ap- refrained from tuning both methods, e.g. by including fea-
plied immediately, since an averaging of class labels yı does ture selection or feature weighting (even though it is well-
not make sense. Instead, the Choquet integral can be de- known that such techniques can greatly improve performance
                                          I
rived for each of the indicator functions f` : y 7→ (y = `), [Wettschereck et al., 1997]). Results have been derived for
` = `1 . . . `m. As in (1), the evidence in favor of each class neighborhood sizes of k = 5 and k = 7; the parameter α in
label is thus accumulated separately. Now, however, the in- (5) has always been set to 1/2.
teraction between cases is taken into account. As usual, the The application of Cho-k-NN for regression has shown
estimation is then given by the label with the highest degree that it consistently improves weighted k-NN, sometimes only
of accumulated evidence.                              slightly but often even considerably. Some results are shown
                                                      in table 1. In particular, it seems that the smaller the size of
5  Empirical   Validation                             the data set, the higher the gain in performance. This ﬁnding
                                                      is intuitively plausible, since for large data sets the neighbor-
In order to validate the extension of NN estimation as pro-
                                                      hoods of a query tend to be more “balanced”; as already said,
posed in the previous section, we have performed several ex-
                                                      the neglect of interaction is likely to be less harmful under
perimental studies using benchmark data sets from the UCI
                                                      such circumstances.
repository4 and the StatLib archive.5
                                                        For classiﬁcation problems, it is also true that Cho-k-NN
  Experiments were performed in the following way: A data
                                                      consistently outperforms weighted k-NN; see table 2. Usu-
set is randomly split into a training and a test set of the same
                                                      ally, however, the gain in classiﬁcation accuracy is only small,
size. For each example in the test set, a prediction is derived
                                                      in many cases not even statistically signiﬁcant. Again, this is
using the training set in combination with weighted k-NN
                                                      especially true for large data sets, and all the more if the clas-
resp. Cho-k-NN. In the case of regression, an estimation yest
                                                0     siﬁcation error is already low for standard k-NN. Neverthe-
is evaluated by the relative estimation error |yest −y |·|y |−1,
                                     0    0   0       less, one should bear in mind that, in the case of classiﬁcation,
and the overall performance of a method by the mean of this
                                                      the ﬁnal prediction is largely insensitive toward modiﬁcations
error over all test examples. In the case of classiﬁcation, we
                                                      of the estimated evidences in favor of the potential labels. In
  3The Choquet integral can be extended to any real-valued func- fact, in this study we only checked whether the ﬁnal predic-
tion through decomposition into a positive and negative part. tion is correct or not and, hence, used a rather crude quality
  4http://www.ics.uci.edu/˜mlearn                     measure. More subtle improvements of an estimation such
  5http://stat.cmu.edu/                               as, e.g., the enlargement of an example’s margin [Schapire et