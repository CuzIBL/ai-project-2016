                               First-order probabilistic inference 
                                               David Poole 
                                    Department of Computer Science 
                                     University of British Columbia 
                                   Vancouver, B.C., Canada V6T 1Z4 
                                          poole@cs.ubc.ca 
                           http://www.cs.ubc.ca/spider/poole/ 

                     Abstract                            Probabilistic reasoning is more challenging than logical rea•
                                                       soning for a number of reasons: 
    There have been many proposals for first-order be•   • We have to use all of our information when making a 
    lief networks (i.e., where we quantify over individ•   probabilistic inference; new information can change old 
    uals) but these typically only let us reason about the conclusions. 
    individuals that we know about. There are many       • We don't want to double count evidence. If we have 
    instances where we have to quantify over all of the    some probabilistic information about all people and we 
    individuals in a population. When we do this the       use it for one particular individual, say Fred, then we 
    population size often matters and we need to rea•      can't reuse that information for Fred. 
    son about all of the members of the population (but  • There are cases where the size of the domain affects the 
    not necessarily individually). This paper presents     probability. In the example developed below, determin•
    an algorithm to reason about multiple individuals,     ing the probability that a person is guilty of a particular 
    where we may know particular facts about some of       crime depends on the population; the population size af•
    them, but want to treat the others as a group. Com•    fects the number of other people who could be guilty. 
    bining unification with variable elimination lets us The following example shows why we may need to reason 
    reason about classes of individuals without needing about the individuals we don't know about as well as the indi•
    to ground out the theory.                          viduals we do know about and shows why we need to consider 
                                                       population size. 
                                                       Example 1 A person in a town was seen committing a crime. 
1 Introduction                                         This person had the same (unusual) hair colour and car colour 
Belief networks or Bayesian networks [Pearl, 1988] are a pop• as Joe (both purple) and the person was very tall and we know 
ular representation for independencies amongst random vari• Joe has big feet (and being tall is correlated with having big 
ables. They are, however zeroth-order representations; to rea• feet). What is the probability that Joe is guilty? We need to 
son about multiple individuals, we have to make each property model the probabilities of the observations and their depen•
of each individual into a separate node.               dence, which would lead us to consider belief networks as a 
                                                       representation. The probability Joe is guilty also depends on 
  There have been proposals to allow for ways to lift belief 
                                                       the population. If the population of the town is very small 
networks to a first order representation. The first tradition 
                                                       then he is probably guilty (as it is unlikely there is anyone else 
[Breese, 1992; Horsch and Poole, 1990; Wellman, Breese and 
                                                       fitting this description). If the town was a city containing a 
Goldman, 1992] essentially allowed for parameterized belief 
                                                       large number of people then he is probably innocent (as we 
networks, which could be grounded for each individual. The 
                                                       would expect many other people to also fit this description). 
second tradition [Poole, 1993; Koller and Pfeffer, 1998; Pfef-
fer, Koller, Milch, and Takusagawa, 1999] allowed for richer This example points to a number of features that have been 
first order probabilistic representations that have belief net• ignored in first-order probabilistic models. Not only do we 
works as special cases. In all of these the only individuals need to take population sizes into account but we need to be 
assumed to exist are those that we know about.         able to reason about the individuals we know about as well 
                                                       as the individuals who we know exist, but we don't know 
  There are many cases where we want to reason about a set 
                                                       anything particular about. We don't have to ground out our 
of individuals as a group. We'd like avoid explicit reasoning 
                                                       theory and reason about millions of people in a city separately, 
about each individual separately. There was a great advance 
                                                       but we also cannot ignore the relevant information about the 
in theorem proving in the 1960s with the invention of of reso•
                                                       people we know something about. 
lution and unification [Robinson, 1965]. The big advance was 
that doing resolution on clauses that contain free variables 2 Representation 
implements a potentially unbounded number of resolutions 
on the grounded representation. The goal of the current work We assume that the domain is represented as a parametrized 
is to allow for similar savings in probabilistic reasoning. belief network, where the nodes are parametrized with 


PROBABILISTIC INFERENCE                                                                                985  domain-restricted types (or populations). Such an idea has 
 been explored previously |Horsch and Poole, 1990; Kersting 
 and De Raedt, 2000], and is similar to the plates of Buntine 
 [ 1994]. We will not dwell on the semantics in this paper. We 
just assume that the program means the grounding: the sub•
 stitution of constants for each individual in the domain of a 
 parameter. 
   This paper is explicitly Bayesian. In terms of the first-order 
 probability of Halpern [1990], the probabilities are all degrees 
 of belief, not statistical assertions about proportions of a pop•
 ulation. We are not quantifying over random individuals (as 
 does Bacchus [1990]), but over all individuals (i.e., with stan•
 dard universal quantification). All of the individuals about 
 which we have the same information have the same probabil•
 ity. It is this property that we exploit computationally. Figure 1: Robbing example parametrized belief network (Ex•
   This paper is built on two different traditions, namely that ample 2) using plates and parametrized random variables. 
of logic programming and theorem proving on one side and 
that of probabilistic reasoning on the other side. Unfortu•
nately they use the same terms or notations (e.g., "variable", logical connectives. 
"domain for very different things (or at least they are  A probabilistic assertion is of the form: 
used in very different ways in this paper). In order to avoid 
confusion, we use neutral terms, but use :he traditions of the 
                                                       where are parameters, dx are populations, C is a set of 
different communities as appropriate.                  inequality constraints on are parametrized 
  A population is a set of individuals. A population corre• propositions using only the parameters , /; specifies 
sponds to a domain in logic. The cardinality of the population a probability distribution over a. We omit the corresponding 
is called the population size. For example, the population syntactic constructs when " 
may be the set of people living in Vancouver, and the popula• A parametrized belief network consists of 
tion size is 2 million people. The population size can be finite a DAG where the nodes are parametrized random vari•
or infinite.                                               ables, 
  We allow for random variables to be parametrized, where  an association of a population to each parameter, 
parameters start with an upper case letter and constants start an assignment of a range to each functor, 
with a lowercase letter. Parameters correspond to logical vari• a set of probability assertions for each node given its 
ables (e.g., as a variable in Prolog). All of the parameters are parents. 
typed with a population. A parametrized random variable is Example 2 To formalize Example 1, consider the 
of the form where is a functor (either a function      parametrized belief network of Figure 1. Here we have 
symbol or a predicate symbol) and each is a parameter or a 
                                                       shown it using the plates  [Buntine, 1994] as well as 
constant. Each functor has a set of values called the range of                1
                                                       with parametrized random variables. We assume that the 
the functor. 
                                                       hair-colour of the different people are not independent; they 
  Examples of parametrized random variables 
                                                       depend on how conservative the town is. Associated with this 
are hair_colour(X), likes(X,Y), likes(joe,X) or 
                                                       network are conditional probabilistic assertions such as : 
townjeonservativeness (the latter being a functor of no argu•                                         2
ments), where hairjcolour, likes and townjeonservativeness 
are functors. 
  Given an assignment of a constant to each parameter, 
a parametrized random variable represents a random vari•
able. For example, suppose hair colour has range {black, 
blond, red, grey, purple, orange, none, multicoloured}. Then 
hair_colour{sam) is a random variable with domain (black, If we knew as background knowledge that sam was an ex•
blond, red, grey, purple, orange, none, multicoloured). Thus ception and has purple hair with probability 0.5, we would 
a parametrized random variable represents a set of random replace the last probabilistic assertion with: 
variables, one for each parameter assignment. Different pa•
rameter assignments for the variables in a parametrized ran•    P(hair_colour(X)=purple\conservative) = 0.001 
dom variable result in different random variables; for ex•
                                                         *Herc we use plates just as a visual help; the real meaning is as 
ample, hair_colour(jred) is a different random variable to probabilistic assertions. The plates get difficult to interpret when 
hairjcolour (joe).                                     there are complex interactions between the parameters, but we can 
  A parametrized primitive proposition is an expression always interpret these as probabilistic assertions. 
of the form which means that parametrized random         2We use conservative as an abbreviation for 
variable has value A parametrized proposition is built townjeonservativeness = conservative, male(X) as an abbreviation 
from the parametrized primitive propositions using the normal for sex(X) = male and very Jall (X) for height (X) = very jall. 


986                                                                             PROBABILISTIC INFERENCE      P(hair_colour(sam)=purple) = 0.5 
(We will not use this in our continuing example). 
  A grounding of a parametrized belief network is a belief 
network that consists of all instances of the parametrized ran•
dom variables where each parameter is replaced by an in•
dividual in the population (or a ground term denoting that 
individual). 
  Intuitively, a parametrized belief network represents a huge 
belief network where the parametrized random variables are Figure 2: We design FOVE so that we get the same answer as 
repeated for each individual in the population associated with if we had grounded the representation and carried out variable 
the parameter for which the constraint is true. The above elimination. 
example, if there were 10,000 people, would represent a belief 
network with 60,001 nodes. 
                                                      as a unit. We only instantiate parameters when we need to. In 
3 First order variable elimination                    general we reason with all of the individuals (except the ones 
                                                      that we know extra information about) as a unit. 
The problem that we consider is: given a parametrized belief A parametric factor or parfactor is a triple 
network, a set of observations, and a (possibly parametrized) where C is a set of constraints on parameters, V is a set of 
query to determine the conditional probability of the instances parametrized random variables and Ms a table representing a 
of a query given the observations. We assume that the evidence factor from the random variables to the non-negative reals. 
is a conjunction of existentially quantified parametrized prim• Intuitively the parametric factor represents all of the ground 
itive propositions.                                   instances of the factor where the instantiation of the parameters 
  We consider the problem of parametric probabilistic infer• satisfies the constraints. 
ence in two stages: first, where every parameter that appears 
in the parents of a node also appears in the node and where the Example 3 A parametric factor that represents one of the con•
observations are variable-free. In this case, when we ground ditional probability tables of Figure 1 is: 
out the theory, each random variable only has a limited number 
of parents. In Section 3.5 we present the second case where where t is the table that represents a function from hair_colour 
we have parents that contain extra parameters; in this case we and conservativeness into non-negative numbers, t is not in•
need a way to aggregate over populations. That section also dexed by X. t looks like: 
considers existential observations and queries.               hairjcolour  conservativeness Val 
  The algorithm is based on variable elimination, VE, [Zhang   purple      conservative     0.001 
and Poole, 1996], where we eliminate the non-observed non-    purple       liberal          0.01 
query random variables one at a time. For first-order VE      blue         conservative     0.1 
(FOVE) we eliminate all the instances of a functor at once3.  blue         liberal          0.05 
  There is a strong relationship between this work and lifting 
in theorem proving [Chang and Lee, 1973]: given a ground When there are two instances of the same parametrized ran•
proof procedure, construct a proof procedure with logical vari• dom variable in V; in this case we need to mark the random 
ables (or in our case with parametrized random variables). In variable in the table to distinguish these instances. 
general, correctness can be shown by proving that we get the Example 4 Suppose that whether person X is a friend of Y 
same answer as if we first grounded the theory and then carried 
                                                      depends on whether X likes Y and whether Y likes X. There are 
out variable elimination. See Figure 2. 
                                                      two distinct cases here, the first is when X = Y, in which case 
3.1 Parametric Factors                                friends{X, Y) has one parent, and the second is when 
                                                      in which case friends(X, Y) has two parents. To represent this, 
In VE, a factor is the unit of data used during computation. we could use 
A VE factor is a function from a set of random variables into 
a non-negative real number. The initial factors are the condi•
tional probabilities. The main operations are multiplying fac•
tors and summing out random variables from factors. After where the subscripts in likes1 and likesn represent different 
conditioning on the observed random variables and summing instances of likes in table t2- That is, is a factor on friends, 
out the non-observed, non-query random variables, we can likesi and likesi. When we eliminate all of the likes relation•
extract posterior probabilities from the remaining factors by ships, we have to consider likes\ and Ukes2. 
multiplying them and normalizing the remaining factor. 
  In first-order variable elimination, we use a generalization 3.2 Splitting 
of a factor where we want to treat the many instances of factors The foundation of parametric variable elimination is the split•
                                                      ting operation. Splitting plays the analogous role to applying 
  3In general, we can potentially eliminate some instances of a func•
tor, but not necessarily all instances, and leave the other instances to substitutions in theorem proving, except that we not only have 
be eliminated later. In this paper we will only do this when eliminat• to be concerned about the instance created, but also about the 
ing all instances except for a query instance.        instances left over. 


PROBABILISTIC INFERENCE                                                                               987 Definition 1 Suppose parametric factor contains pa•    3.4 Multiplying Parametric Factors 
rameter X. A split of on X =where y is either a        In VE, when we eliminate a variable, we multiply all of the 
constant or another parameter, and C does not contain  factors that contain that variable then sum out the variable 
           results in the two parametric factors:      from the resultant factor. 
                                                         As in variable elimination, we need to multiply the factors 
                                                       that contain the variable to be eliminated. In parametric vari•
where is the same as V\ but with y replacing every     able elimination, given two parametric factors, some instances 
instance of X (i.e., we substitute for X). The second para• may need to be multiplied, and some instances may not. Also, 
metric factor is called a residual parametric factor.  the dimension of the resulting factors can be different for dif•
                                                       ferent instances. 
  A substitution is of the form where the 
                                                         The product of two parametric factors, in general, results in 
  are distinct variables and the are terms. We assume that all 
                                                       a set of parametric factors. Intuitively, we keep splitting the 
substitutions are in normal form: does not contain for any 
                                                       parametric factors (and renaming parameters) until we can 
 and The substitutions resulting from standard unification 
                                                       guarantee that we get parametric factors that, if grounded, 
algorithms are in normal form [Chang and Lee, 1973]. If a 
                                                       result in the same factors as if we were to first ground the 
substitution is in normal form, we get the same result from 
                                                       factors and then multiply. 
replacing each X, by the corresponding f, sequentially (in any 
order) or in parallel.                                 Determining which parfactors to multiply 
  Instead of just applying substitutions as in normal theorem Suppose there are two parfactors and 
proving, we need to split. The general idea is that whenever                with variables renamed to be differ•
applying a substitution to a parfactor restricts the set of ground ent, wherepi and/?2 are unifiable instances of 
instances, we need to split the parfactor. We don't need to where is to be eliminated. Let We will 
split when just renaming variables or substituting a value for split on and split on putting all the residuals in the set 
a variable that doesn't appear in the parfactor.         of all parfactors. All instances of the resulting non-residual 
  We can split a parfactor on substitution by totally  parfactors would be multiplied in VE. 
ordering the elements of the substitution4,              The following abstract example is designed to show what 
by carrying out the following procedure which results in a final needs to be considered when multiplying parfactors. It isn't 
instance of and a set of residual parfactors:          meant to be meaningful. 
For i from 1 to k                                      Example 6 Suppose we were to eliminate p and multiply the 
                                                       two parametric factors: 
                                                                                                       (1) 
                                                                                                       (2) 
                                                       If we were to ground the parameters some of the instances of 
                                                       these would be multiplied in VE and some of them wouldn't. 
Note that the final value of is the same as if we had applied Unification finds the most general instances that are identical. 
the substitution to , but we also create residuals.                                resulting in the substitution 
  There is a close relationship between the splitting on equal•
ity in this paper and the splitting on the value of a variable in We can split parametric factor (1) on resulting in 
contextual variable elimination fPoole and Zhang, 2003].                                               (3) 
3.3 Observations                                                                                       (4) 
                                                       Parametric factor (4) is a residual parametric factor. No in•
When we observe a ground value, we carry out the analogous stance of parametric factor (4) ever needs to be multiplied by 
operation to VE. We project the tables onto the observed val• any instance of parametric factor (2) when eliminating and 
ues. However we must first split to ensure that we only affect doesn't participate further in the product. 
the appropriate ground variables.                        Similarly, we can split (2) on 0 resulting in: 
Example 5 if we condition on the fact that Joe has purple hair,                                        (5) 
a purple car, and a shoe size of 12, we now reason separately 
about Joe than we do about the other individuals who can be 
treated as a group.                                                                                    (6) 
  The parametric factor of example 3 becomes the two para• Parametric factor (6) is a residual factor and doesn't partici•
metric factors:                                        pate further in the elimination of p. All instances of (3) and 
                                                       (5) would be multiplied together when eliminating p(b, a) in 
                                                       variable elimination. 
                                                       Determining the dimensionality of the product 
                                                       In Example 6, all instances of parametric factors (3) and (5) 
                                                       would be multiplied if we were to ground parametric factors 

  4The total order does not affect the instance created but does affect (1) and (2) and carry out VE. However, not all of the product 
which residuals are created.                           factors have the same dimension; some have two different 


988                                                                             PROBABILISTIC INFERENCE instances, and some have one. We need to do more splitting 
to ensure that all of the products have the form of parametric 
factors. 
  Suppose we have parfactors 
          that need to be multiplied. If for all and 
for all either /?' and p" are identical or non-unifiable 
or if mgu(p\ p") is incompatible with the constraints, we know 
that all instances of their product has the same dimension. If 
there is a p' and p"V" that are not identical but unify 
with mgu consistent with C' and C", we can split and on 
mgu(p\ /?"). The resulting instances and residuals either have 
identical instances of// and/?" or non-unifiable instances. We 
then multiply each instance created from by each instance 
created from 0". 
  When we know all the instances 
          have the same dimension, we create the parfactor 
                       where is the product of the 
tables for where we maintain one dimension for each 
member of Thus, those members in common in V 
and are treated as the same variable in the product, but 
those members that don't unify, even with the same functor, 
in and are treated as different variables in the product 
table. 
Example 7 When we need to multiple parfactors (3) and 
(5), we notice that and unify (with unifier 
             We thus split parametric factor (5) on 
and rename W to K, replacing (5) with: 
                                                 (7) 
                                                 (8) 
We can now multiply parametric factors (3) and (7) producing: 

                                                 (9) 
where is the product of factors. Parametric factor 
(9) represents all of the factors of dimension four created by 
multiplying factors that are instances of parametric factors (1) 
and (2). 
  We also multiply parametric factors (3) and (8) producing: The effective population size is the product of the populations 
                                                       of the parameters less the number excluded by the constraints. 
                                                (10)   Thus we have to take each element in the table and put it to 
                                                       the power of the effective population size. 
where is the factor , but with abelled as q\ and is the 
factor but with labelled as Thus is a factor 
on Parametric factor (10) represents all of the        3.5 Aggregation over populations 
factors of dimension five created by multiplying factors that When we have parameters in the parents of a node and not in 
are instances of parametric factors (1) and (2).       the node itself, the number of parents grows with the popula•
  While this may seem very complicated, remember that para• tion size, and we need to specify how a node is a function of 
metric factor (10) represents mr(m — l)2 factors (assuming its parents. There are many possibilities, such as a node being 
that all populations have size Even if is 10, this is 8100 the "logical or" of it's parents, the "logical and", the max of 
factors.                                               its parents, the average if its parents, true iff greater than k 
  When we have multiplied all of the appropriate parfactors of its parents are true, or the vote of its parents (according to 
we are ready to sum out the variables being eliminated. We some voting scheme, for example when the majority wins), or 
must remember that when we are eliminating p           some other function. Zhang and Poole [1996] give an analysis 
we are not just eliminating one random variable, but we are where there is an arbitrary associative and commutative oper•
eliminating a number of variables equal to the product of the ator between the parents. In order to make the presentation 
population sizes of If some parameters only ap•        simpler, in this paper we assume that the operator is a logical 
pear in the parametrized random variable that is being elimi• "or" [Diez and Galan, 2002]. It is straightforward to use the 
nated, in the grounding we are multiplying the number of fac• techniques of Zhang and Poole [1996] to extend this to other 
tors equal to the effective population size of those parameters. operators. 


PROBABILISTIC INFERENCE                                                                                989 