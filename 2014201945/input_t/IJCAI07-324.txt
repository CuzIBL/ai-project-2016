             Memory-Bounded Dynamic Programming for DEC-POMDPs

                       Sven Seuken∗                              Shlomo Zilberstein
       Division of Engineering and Applied Sciences       Department of Computer Science
                    Harvard University                       University of Massachusetts
                  Cambridge, MA 02138                           Amherst, MA 01003
                 seuken@eecs.harvard.edu                       shlomo@cs.umass.edu

                    Abstract                          than single-agent control and provably intractable. Even ε-
                                                      approximations are hard [Rabinovich et al., 2003]. Due to
    Decentralized decision making under uncertainty
                                                      these complexity results, optimal algorithms have mostly the-
    has been shown to be intractable when each agent
                                                      oretical signiﬁcance. Consequently, researchers have started
    has different partial information about the domain.
                                                      to focus on approximate algorithms to solve signiﬁcantly
    Thus, improving the applicability and scalability
                                                      larger problems. But scalability remains a major research
    of planning algorithms is an important challenge.
                                                      challenge and the main question – how to use the available
    We present the ﬁrst memory-bounded dynamic pro-
                                                      memory and time most efﬁciently – remains unanswered. A
    gramming algorithm for ﬁnite-horizon decentral-
                                                      detailed survey of existing formal models, complexity results
    ized POMDPs. A set of heuristics is used to iden-
                                                      and planning algorithms is available in [Seuken and Zilber-
    tify relevant points of the inﬁnitely large belief
                                                      stein, 2005].
    space. Using these belief points, the algorithm
    successively selects the best joint policies for each
    horizon. The algorithm is extremely efﬁcient, hav-  This paper presents a fundamentally new approach to over-
    ing linear time and space complexity with respect come the time and space complexity of existing optimal and
    to the horizon length. Experimental results show  approximate algorithms. Most of the previous algorithms
    that it can handle horizons that are multiple orders cannot solve problems with a horizon larger than 10. Our
    of magnitude larger than what was previously pos- goal has been to increase this horizon barrier by several orders
    sible, while achieving the same or better solution of magnitude. The approach is based on the ﬁrst exact dy-
    quality. These results signiﬁcantly increase the ap- namic programming (DP) algorithm for ﬁnite-horizon DEC-
    plicability of decentralized decision-making tech- POMDPs  [Hansen et al., 2004]. The original DP algorithm,
    niques.                                           presented in Section 3, builds the ﬁnal solution bottom-up,
                                                      starting with a 1-step policy for the last time step and succes-
1  Introduction                                       sively working towards the ﬁrst time step. In every iteration,
                                                      it eliminates unnecessary strategies to reduce the total number
For over 50 years, researchers in artiﬁcial intelligence and of policies kept in memory. Unfortunately, the algorithm runs
operations research have been working on decision making out of memory very quickly. The key observation is that this
under uncertainty. The Markov decision process (MDP) has algorithm keeps too many policies in memory, even though
been proved to be a useful framework for centralized deci- only very few of them are actually necessary for optimal or
sion making in fully observable stochastic environments. In near-optimal behavior. Thus, the main idea is to identify a
the 1960’s, partially observable MDPs (POMDPs) were in- small set of policies that are actually useful for the construc-
troduced to account for imperfect state information. An even tion of good joint policies. This is achieved by using a set
more general problem results when two or more agents have of top-down heuristics to identify tentative policies that are
to cooperate to optimize a joint reward function, while hav- not necessarily optimal, but can be used to explore the belief
ing different local observations. This problem arises in many space. Section 4 describes how a set of relevant belief states
application domains such as multi-robot coordination, manu- can be identiﬁed using these heuristics. The DP algorithm
facturing, information gathering and load balancing.  can then select the optimal joint policy trees for those belief
  The  decentralized partially observable MDP (DEC-   states, leading to a bounded number of good policy trees. Sec-
POMDP) framework is one way to model these problems.  tion 5 presents the details of our memory-bounded dynamic
It has been shown that ﬁnite-horizon DEC-POMDPs are   programming algorithm and proves its linear time and space
NEXP-complete  [Bernstein et al., 2000]. Thus, decen- complexity with respect to the horizon length. We have ap-
tralized control of multiple agents is signiﬁcantly harder plied our algorithm to a set of standard test problems from
  ∗This work was done while the author was a graduate student the literature. Section 6 details the results and shows that the
in the Computer Science Department of the University of Mas- algorithm can solve problems with horizons that are multiple
sachusetts, Amherst.                                  orders of magnitude larger than what was previously possible.

                                                IJCAI-07
                                                  20092  Related Work                                         • P is a Markovian transition probability table. P (s|s,a)
                                                          denotes the probability that taking joint action a in state
Over the last ﬁve years, researchers have proposed a wide                            
                                                          s results in a transition to state s .
range of optimal and approximate algorithms for decentral-
ized multi-agent planning. In this paper we focus on ﬁnite- • Ωi is a ﬁnite set of observations available to agent i and
                                                          
horizon problems; inﬁnite-horizon problems often require  Ω=⊗i∈I   Ωi is the set of joint observations, where
different solution techniques. One important class of ﬁnite- o = o1, ..., on denotes a joint observation.
horizon algorithms is called “Joint Equilibrium-based Search • O is a table of observation probabilities. O(o|a, s)
for Policies” (JESP), where the algorithms do not search for denotes the probability of observing joint observation o
the globally optimal solution, but instead aim for local opti- given that joint action a was taken and led to state s.
mality [Nair et al., 2003]. The authors introduce the idea of
                                                                                              
focusing on the reachable belief states, but the approach still • R : S × A →is a reward function. R(a, s ) denotes
                                                                                                  
leads to exponential complexity.                          the reward obtained from transitioning to state s after
  The approach that is closest to our work is called Point- taking joint action a.
Based Dynamic Programming for DEC-POMDPs  [Szer and     • T denotes the total number of time steps.
Charpillet, 2006]. That algorithm computes the policies
based on a subset of the reachable belief states. But in con- Because the underlying system state of a DEC-POMDP is
trast to our work, this approach does not employ top-down not available to the agents during execution time, they must
heuristics to identify the most useful belief states and it is not base their actions on beliefs about the current situation. In a
memory-bounded. It still leads to double-exponential worst- single-agent setting, the belief state, a distribution over states,
case complexity.                                      is sufﬁcient for optimal action selection. In a distributed set-
                                                      ting, each agent must base its actions on a multi-agent belief
  A DEC-POMDP can also be seen as a partially observ-
                                                      state – a distribution over states and over the other agents’
able stochastic game (POSG) with common payoffs [Emery-
                                                      policies. Unfortunately, there is no compact representation
Montemerlo et al., 2004]. In this approach, the POSG is ap-
                                                      of this multi-agent belief state. It only exists implicitly for
proximated as a series of smaller Bayesian games. Interleav-
                                                      each agent if the joint policy is known and the entire history
ing planning and execution, this algorithm ﬁnds good solu-
                                                      of local observations is taken into consideration.
tions for short horizons, but it still runs out of memory after
horizon 10.                                           Deﬁnition 2 (Local and Joint Policies for a DEC-POMDP)
  Another way of addressing the high complexity of DEC-
                                                      A local policy for agent i, δi, is a mapping from local histo-
POMDPs is to develop algorithms for problem classes that
                                                      ries of observations oi = oi1 ···oit over Ωi, to actions in Ai.
exhibit certain structure [Becker et al., 2004; Goldman and
                                                      A joint policy, δ = δ1, ..., δn, is a tuple of local policies,
              ]
Zilberstein, 2005 . However, such techniques still have expo- one for each agent.
nential complexity unless the interaction between the agents
is very limited. A model that is particularly useful if the Solving a DEC-POMDP means ﬁnding a joint policy that
agents do not share the same payoff function is the I-POMDP maximizes the expected total reward. A policy for a sin-
          [                           ]
framework Gmytrasiewicz and Doshi, 2005 . I-POMDPs    gle agent i can be represented as a decision tree qi, where
have similar complexity barriers, leading the authors to in- nodes are labeled with actions and arcs are labeled with
                                                                                              t
troduce an approximate algorithm based on particle ﬁltering observations (a so called policy tree). Let Qi denote the
[Doshi and Gmytrasiewicz, 2005]. This approach addresses set of horizon-t policy trees for agent i. A solution to a
the belief space complexity, but the policy space complexity DEC-POMDP with horizon t can then be seen as a vec-
remains too high and limits scalability.              tor of horizon-t policy trees, a so called joint policy tree
                                                       t     t  t    t
                                                      δ  =(q1,q2, ..., qn), one policy tree for each agent, where
                                                       t    t
3  Solution Techniques for DEC-POMDPs                 qi ∈ Qi. These policy trees can be constructed in two differ-
                                                      ent ways: top-down or bottom-up. If the goal is the optimal
We formalize the problem using the DEC-POMDP frame-
                                                      policy, both techniques lead to the same ﬁnal policy. But if
work [Bernstein et al., 2000]. Our results, however, apply
                                                      the goal is an approximate solution, the different characteris-
to equivalent models such as the MTDP or the COM-MTDP
                                                      tics of the construction processes can be exploited.
[Pynadath and Tambe, 2002].
Deﬁnition 1 (DEC-POMDP)  A ﬁnite-horizon decentralized Top-down Approach   The ﬁrst algorithm that used a top-
partially observable Markov decision process is a tuple down approach, MAA∗, makes use of heuristic search tech-
I,S,b0, {Ai},P,{Ωi},O,R,T where                     niques [Szer et al., 2005]. It is an extension of the standard A∗
  • I is a ﬁnite set of agents indexed 1,...,n.       algorithm where each search node contains a joint policy tree.
                                                      For example, if δ2 is a horizon-2 joint policy tree, an expan-
  • S
      is a ﬁnite set of states.                       sion of the corresponding search node generates all possible
                                                                                                       δ2
  • b0 ∈ ΔS is the initial belief state (state distribution). joint policy trees of horizon 3 that use the joint policy tree
                                                      for the ﬁrst two time steps. Using a set of heuristics that are
  • Ai is a ﬁnite set of actions available to agent i and
                                                     suitable for DEC-POMDPs, some parts of the search tree can
    A = ⊗i∈I Ai is the set of joint actions, where    be pruned. But the algorithm runs out of time very quickly,
    a = a1, ..., an denotes a joint action.        because the search space grows double exponentially.

                                                IJCAI-07
                                                  2010Bottom-up Approach: Dynamic Programming    The ﬁrst                                        Top−down
non-trivial algorithm for solving DEC-POMDPs used a                                        Heuristics
                 [                ]                                     a
bottom-up approach Hansen et al., 2004 . Policy trees were           o   o
constructed incrementally, but instead of successively com-           12
                                                                    a      b
ing closer to the frontiers of the trees, this algorithm starts    o  o  o  o
at the frontiers and works its way up to the roots using dy-       1  2   1  2
                                                                   b  b  a   a
namic programming (DP). The policy trees for each agent are                       Identify Relevant Belief Points
kept separately throughout the construction process. In the
end, the best policy trees are combined to produce the optimal
joint policy. The DP algorithm has one iteration per step in
the horizon. First, all policies with horizon 1 are constructed. baa
                                                           o  o   o  o   o  o
In each consecutive iteration, the DP algorithm is given a set 112 1 2      2              Bottom−up
                      t                       t+1          b   a  b  a   b  b
of horizon-t policy trees Qi for each agent i. The set Qi is                                   DP
then created by an exhaustive backup. This operation gener-
ates every possible depth-t+1policy tree that makes a transi- Figure 1: The construction process of a single-agent policy
tion, after an action and observation, to the root node of some tree combining the top-down and the bottom-up approach.
depth-t policy tree. After the exhaustive backup, the size of
                        t+1         t |O|             4   Top-Down Heuristics for DEC-PODMPs
the set of policy trees is |Qi | = |A||Qi| . If all pol-
icy trees are generated for every step in the horizon, the total Even though the agents do not have access to the belief state
number of complete policy trees for each agent is of the order during execution time, it can be used to evaluate the bottom-
        T
O(|A|(|O| )). This double exponential blow-up is the reason up policy trees computed by the DP algorithm. Policy trees
why the naive algorithm would quickly run out of memory. that are good for a centralized belief state are often also good
                                                      candidates for the decentralized policy. Obviously, a belief
  To alleviate this problem, the algorithm uses iterated elimi-
                                                      state that corresponds to the optimal joint policy is not avail-
nation of dominated policies. For two agents i and j, a policy
                                                      able during the construction process. But fortunately, a set of
tree qi ∈ Qi is dominated, if for every possible multi-agent
                                                      belief states can be computed using multiple top-down heuris-
belief state mb ∈ Δ(S ×Qj) there is at least one other policy
                                                      tics – efﬁcient algorithms that ﬁnd useful top-down policies.
tree qk ∈ Qi \ qi that is as good as or better than qi. This test
                                                      Once a top-down heuristic policy is generated, the most likely
for dominance is performed using a linear program. Remov-
                                                      belief state can be computed. Obviously, this does only lead
ing a dominated policy tree does not reduce the value of the
                                                      to exactly one belief state. But depending on the speciﬁc
optimal joint policy. If used in every iteration, this technique
                                                      problem, the bottom-up DP algorithm can handle up to a few
signiﬁcantly reduces the number of policy trees kept in mem-
                                                      dozen belief state candidates (at most 50 in our experiments).
ory, because for every eliminated policy tree q, a whole set of
                                                      A whole set of reachable belief states can be identiﬁed using
policy trees containing q as a subtree is implicitly eliminated
                                                      standard sampling techniques. The rest of this section de-
as well. But, even with this pruning technique, the number of
                                                      scribes several useful heuristics to identify these belief states.
policy trees still grows quickly and the algorithm runs out of
memory even for very small problems.                  The MDP Heuristic   A DEC-POMDP can be turned into a
  A more detailed analysis of the construction process shows fully-observable MDP by revealing the underlying state af-
that most of the policy trees kept in memory are useless. ter each step. A joint policy for the resulting MDP assigns
These policy trees could be eliminated early on, but they are joint actions to states, thus this policy cannot be used for the
not. One reason is that a policy tree can only be eliminated real DEC-POMDP. But during the construction process, it has
if it is dominated for every belief state. But for many DEC- proven very useful to identify reachable belief states that are
POMDPs, only a small subset of the belief space is actually very likely to occur.
reachable. Another reason is that a policy tree can only be The Inﬁnite-Horizon Heuristic Inﬁnite-horizon DEC-
eliminated if it is dominated for every possible belief over the POMDPs are signiﬁcantly different from ﬁnite-horizon DEC-
other agents’ policies. But obviously, during the construc- POMDPs. Instead of maximizing expected reward over T
tion process, the other agents also maintain a large set of pol- time steps, the agents maximize the reward over an inﬁnite
icy trees that will eventually prove to be useless. Inevitably, time period. Obviously, optimal policies for ﬁnite-horizon
these policy trees are also considered in the test for domi- problems with a very short horizon may be very different
nance which inhibits the elimination of a large set of policies. from those for inﬁnite-horizon problems. But the longer
  Unfortunately, these drawbacks of the pruning process can- the horizon, the more similar the solutions become in gen-
not be avoided. Before the algorithm reaches the root of the eral. Thus, inﬁnite-horizon policies can be used as top-down
policy trees, it cannot predict which beliefs about the state heuristics to identify useful belief states for problems with
and about the other agents’ policies will eventually be useful. sufﬁciently long horizons. Inﬁnite-horizon DEC-POMDPs
This observation leads to the idea of combining the bottom- are undecidable and thus it is generally impossible to de-
up and top-down approaches: Using top-down heuristics to termine when optimal policies for ﬁnite and inﬁnite horizon
identify relevant belief states for which the dynamic program- problems match. But because the rewards are discounted over
ming algorithm can then evaluate the bottom-up policy trees time, a bound on the difference in value can be guaranteed.
and select the best joint policy. Figure 1 illustrates this idea. Recently, approximate algorithms have been introduced that

                                                IJCAI-07
                                                  2011can solve those problems very efﬁciently [Bernstein et al., Algorithm 1: The MBDP Algorithm
2005]. The resulting policies can be used to simulate mul-
                                                      1 begin
tiple runs where the execution is simply stopped when the 2 maxT rees ← max number of trees before backup
desired horizon and a corresponding belief state are reached. 3 T ← horizon of the DEC-POMDP
                                                           H  ←                               h ∈ H
The Random Policy Heuristic The MDP heuristic and the 4         pre-compute heuristic policies for each
                                                           Q1,Q1  ←
inﬁnite-horizon heuristic can be modiﬁed by adding some ε- 5 i  j   initialize all 1-step policy trees
exploration – choosing random actions with probability ε in 6 for t=1 to T do
                                                               Qt+1,Qt+1 ←          Qt          Qt
every step. This helps cover a larger area of the belief space, 7 i  j     fullBackup( i), fullBackup( j )
                                                                 t+1    t+1
in particular if the other heuristics are not well suited for the 8 Seli ,Selj ← empty
problem. For the same reason, a completely random heuristic 9  for k=1 to maxTrees do
can also be used – choosing actions according to a uniform 10     choose h ∈ H and generate belief state b
                                                                              t+1      t+1
distribution. This provides a set of reachable belief states and 11 foreach qi ∈ Qi ,qj ∈ Qj do
is not dependent on the speciﬁc problem. Obviously, those 12         evaluate each pair (qi,qj ) with respect to b
random policies achieve low values. But this is not a problem,                        t+1      t+1
                                                      13          add best policy trees to Seli and Selj
because only the belief states and not the policies are used.                             t+1     t+1
                                                      14          delete these policy trees from Qi and Qj
                                                                t+1  t+1     t+1    t+1
Heuristic Portfolio The usefulness of the heuristics and, 15   Qi  ,Qj  ←  Seli ,Selj
more importantly, the computed belief states are highly de-                      T       T   T
                                                      16   select best joint policy tree δ from {Qi ,Qj }
pendent on the speciﬁc problem. For some problems, the so-       T
lution computed with the MDP heuristic might be very simi- 17 return δ
lar to the optimal policy and accordingly the computed belief 18 end
states are very useful. But for other problems, this heuristic
might focus the algorithm on the wrong parts of the belief
space and consequently it may select poor policy-tree candi- Theorem 1. The MBDP algorithm has a linear space com-
dates for the decentralized case. This problem can be alle- plexity with respect to the horizon length.
viated by using what we call a heuristic portfolio. Instead
of just using one top-down heuristic, a whole set of heuris- Proof: Once the variable maxT rees is ﬁxed, the number of
tics can be used to compute a set of belief states. Thus, each policy trees per agent is always between k and K. The lower
heuristic is used to select a subset of the policy trees. Learn- limit k is equal to maxT rees, which is the number of trees
ing algorithms could optimize the composition of the heuris- after the heuristics have been used to select the k-best pol-
tic portfolio, but this aspect is beyond the scope of this paper. icy trees. The upper limit K is equal to |A||maxT rees||O|,
                                                      which is the number of policy trees after a full backup. By
5  Memory-Bounded Dynamic Programming                 choosing maxT rees appropriately, the desired upper limit
                                                      can be pre-set. In every iteration of the algorithm, no more
The MBDP algorithm combines the bottom-up and top-down
                                                      than K policy trees are constructed. For the construction of
approaches. The algorithm can be applied to DEC-POMDPs
                                                      the new policy trees, the algorithm uses pointers to the k pol-
with an arbitrary number of agents, but to simplify notation,
                                                      icy trees of the previous level and is thus able to potentially
the description in Algorithm 1 is for two agents, i and j. The
                                                      attach each subtree multiple times instead of just once as il-
policy trees for each agent are constructed incrementally us-
                                                      lustrated in Figure 2. Due to this efﬁcient pointer mechanism,
ing the bottom-up DP algorithm. But to avoid the double ex-
                                                      a ﬁnal horizon-T policy tree can be represented with k · T de-
ponential blow-up, the parameter maxT rees is chosen such
                                                      cision nodes. Thus, the amount of space grows linearly with
that a full backup with this number of policy trees of length
                                                      the horizon length. For n agents, this is O(n(k ·T +K)).
T − 1 does not exceed the available memory. Every iteration
of the algorithm consists of the following steps. First, a full
backup of the policies from the last iteration is performed.   1
This creates policy tree sets of size |A||maxT rees||O|. Next,
top-down heuristics are chosen from the portfolio and used
to compute a set of belief states. Then, the best policy tree  2
pairs for these belief states are added to the new sets of policy
trees. Finally, after the T th backup, the best joint policy tree
for the start distribution is returned.                        T−2

5.1  Theoretical Properties                                    T−1
Overcoming the complexity barriers of DEC-POMDPs is
challenging, because there is a double exponential number
of possible policy trees, each containing an exponential num-  T
ber of nodes in the time horizon. Accordingly, an important
feature of the MBDP algorithm is that it can compute and Figure 2: An exponential size policy can be represented using
represent the policy trees using just linear space.   linear space by re-using policy trees. Here maxT rees =5.

                                                IJCAI-07
                                                  2012  In general, the idea of the pointer mechanism could be ap-       MABC Problem         Tiger Problem
                                                                                             σ
plied to other algorithms and improve them as well. It re- Horizon Value   Time(s)  Value        Time(s)
duces the number of nodes used for one ﬁnal policy tree from 3     2.99     0.01     5.19    0     0.19
O(|O|T ) to O(k · T ). However, if k grows double exponen- 4       3.89     0.01     4.80    0     0.46
tially – as in the optimal DP algorithm – using the pointer 5      4.79     0.02     5.38   0.42   0.72
mechanism only has marginal effects. For an improvement in 10      9.29     0.08    13.49   1.76   2.19
                                   k                     100      90.29     0.26    93.24   7.79  25.84
space complexity it is also necessary that – the number of 1,000  900.29     2.5    819.01  20.5  289.3
possible policy trees kept in memory – be sufﬁciently small. 10,000 9,000.29 50      7930   52.5  4092
Theorem  2. The MBDP algorithm has a linear time com-   100,000  90,000.29  3000    78252   87.9  44028
plexity with respect to the horizon length.
                                                            Table 1: Performance of the MBDP algorithm.
Proof: The main loop of the algorithm (lines 6-15) depends
linearly on the horizon length T . Inside this loop, all oper- the algorithm as the entire computational process including
ations are independent of T once k and K are ﬁxed. The all recursive calls. The best solution found during one trial
remaining critical operation is in line 4, where the heuristic run is returned as the ﬁnal solution. Because the algorithm
policies are pre-computed. Currently, the only heuristic that is partly randomized, it is obvious that on average, the higher
guarantees linear time is the random policy heuristic. The the recursion depth, the better the solution value. The third
other heuristics have higher, but still polynomial-time com- parameter controls the selection of heuristics from the portfo-
plexity. In practice, the time used by the heuristics is negli- lio. In the following experiments we used a uniform selection
gible. If a top-down heuristic is used that can be computed method (giving the same weight to all heuristics) because it
in linear time, the time complexity of the whole algorithm is performed well without ﬁne-tuning. We also experimented
linear in the horizon length.                         with the pruning technique described in Section 5.3, but it
                                                      had little impact on the runtime or solution quality. Hence,
5.2  Recursive MBDP                                   we decided not to include it in the following experiments.
As pointed out earlier, the effectiveness of any top-down
heuristic may depend on the speciﬁc problem. To alleviate 6.1 Benchmark Problems
this drawback, we use a heuristic portfolio. But once the al- We have applied the MBDP algorithm to two benchmark
gorithm has computed a complete solution, we have a joint problems from the literature: the multi-access broadcast
policy that deﬁnitely leads to relevant belief states when used channel (MABC) problem involving two agents who send
as a heuristic. This is exactly the idea of Recursive MBDP. messages over a shared channel and try to avoid colli-
The algorithm can be applied recursively with an arbitrary sions [Hansen et al., 2004], and the multi-agent tiger prob-
recursion-depth. After the ﬁrst recursion has ﬁnished, the ﬁ- lem, where two agents have to open one of two doors, one
nal joint policy can be used as a top-down heuristic for the leading to a dangerous tiger and the other to a valuable trea-
next run and so on.                                   sure [Nair et al., 2003]. Table 1 presents performance results
                                                      of the MBDP algorithm for the two test problems. Shown
5.3  Pruning Dominated Policy Trees                   are solution value and computation time, which are both av-
                                                      eraged over 10 trial runs. For the tiger problem, we also show
As described in Section 3, the exact DP algorithm uses iter-             σ
ated elimination of dominated strategies to reduce the size of the standard deviation , because in this case the algorithm
the policy tree sets. Interestingly, this pruning technique can achieved different solution values over the 10 trials. For the
                                                      MABC problem, we used  maxT rees =3and a recursion
be combined with the MBDP algorithm in two different ways.                                 maxT rees =7
First, after a full backup is performed, the pruning algorithm depth of 1. For the tiger problem we used
can be used to eliminate dominated policy trees, which re- and a recursion depth of 5.
duces the set of remaining policy trees. Consequently, no To evaluate the MBDP algorithm we compared it to the op-
dominated policy tree will be selected using the top-down timal DP algorithm [Hansen et al., 2004], the JESP algorithm
heuristics. Second, the pruning technique can be applied af- [Nair et al., 2003], the Point-Based Dynamic Programming
ter the heuristics have been used to select the policy trees for algorithm (PBDP) [Szer and Charpillet, 2006] and a random-
the next iteration. Pruning dominated policy trees at this step policy generating algorithm. These are all ofﬂine planning
leads to a more efﬁcient use of memory in the next iteration. algorithms. The computation is performed in a centralized
                                                      way and the ﬁnal solution is a complete joint policy tree for
                                                      the desired horizon, which can then be executed by multi-
6  Experiments                                        ple agents in a decentralized way. In contrast, the Bayesian
We have implemented the MBDP algorithm and performed  Game Approximation algorithm [Emery-Montemerlo et al.,
intensive experimental tests. The algorithm has three key 2004] interleaves planning with execution. Thus, it allows for
parameters that affect performance. One parameter is the some amount of decentralized re-planning after each step. To
maximum number of trees, maxT rees, which deﬁnes the  perform a consistent comparison, we also created an online
memory requirements; runtime is quadratically dependent on version of MBDP and in fact it outperformed the other algo-
maxT rees. Its effect on the value of the solution is analyzed rithm. But a discussion of the online algorithms is beyond the
in Section 6.2. The second parameter is the depth of the recur- scope of this paper and thus the following performance com-
sion, which affects runtime linearly. We deﬁne a trial run of parison does not include results for the online algorithms.

                                                IJCAI-07
                                                  2013