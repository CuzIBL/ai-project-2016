                    Stepwise Nearest Neighbor Discriminant Analysis∗

                                       Xipeng Qiu and Lide Wu
                               Media Computing & Web Intelligence Lab
                           Department of Computer Science and Engineering
                                   Fudan University, Shanghai, China
                                       xpqiu,ldwu@fudan.edu.cn

                    Abstract                            A major drawback of LDA is that it often suffers from the
                                                      small sample size problem when dealing with the high dimen-
    Linear Discriminant Analysis (LDA) is a popu-
                                                      sional data. When there are not enough training samples, Sw
    lar feature extraction technique in statistical pat- may become singular, and it is difﬁcult to compute the LDA
    tern recognition. However, it often suffers from  vectors. For example, a 100 × 100 image in a face recog-
    the small sample size problem when dealing with   nition system has 10000 dimensions, which requires more
    the high dimensional data. Moreover, while LDA
                                                      than 10000 training data to ensure that Sw is nonsingular.
    is guaranteed to ﬁnd the best directions when each Several approaches[Liu et al., 1992; Belhumeur et al., 1997;
    class has a Gaussian density with a common co-    Chen et al., 2000; Yu and Yang, 2001] have been proposed
    variance matrix, it can fail if the class densities to address this problem. A common problem with all these
    are more general. In this paper, a new nonpara-   proposed variant LDA approaches is that they all lose some
    metric feature extraction method, stepwise nearest discriminative information in the high dimensional space.
    neighbor discriminant analysis(SNNDA), is pro-
                                                        Another disadvantage of LDA is that it assumes each class
    posed from the point of view of the nearest neigh-
                                                      has a Gaussian density with a common covariance matrix.
    bor classiﬁcation. SNNDA ﬁnds the important
                                                      LDA guaranteed to ﬁnd the best directions when the distri-
    discriminant directions without assuming the class
                                                      butions are unimodal and separated by the scatter of class
    densities belong to any particular parametric fam-
                                                      means. However, if the class distributions are multimodal
    ily. It does not depend on the nonsingularity of the
                                                      and share the same mean, it fails to ﬁnd the discriminant
    within-class scatter matrix either. Our experimental
                                                      direction[Fukunaga, 1990]. Besides, the rank of S is c − 1,
    results demonstrate that SNNDA outperforms the                                              b
                                                      where c is the number of classes. So the number of extracted
    existing variant LDA methods and the other state-
                                                      features is, at most, c − 1. However, unless a posteriori prob-
    of-art face recognition approaches on three datasets
                                                      ability function are selected, c − 1 features are suboptimal in
    from ATT and FERET face databases.
                                                      Bayes sense, although they are optimal with regard to Fisher
                                                      criterion [Fukunaga, 1990].
1  Introduction                                         In this paper, a new feature extraction method, step-
The curse of high-dimensionality is a major cause of the wise nearest neighbor discriminant analysis(SNNDA), is pro-
practical limitations of many pattern recognition technolo- posed. SNNDA is a linear feature extraction method in or-
gies, such as text classiﬁcation and object recognition. In der to optimize nearest neighbor classiﬁcation (NN). Near-
the past several decades, many dimensionality reduction tech- est neighbor classiﬁcation [Duda et al., 2001] is an efﬁcient
niques have been proposed. Linear discriminant analysis method for performing nonparametric classiﬁcation and of-
(LDA) [Fukunaga, 1990] is one of the most popular super- ten used in the pattern classiﬁcation ﬁeld, especially in ob-
vised methods for linear dimensionality reduction. In many ject recognition. Moreover, the NN classiﬁer has a close
applications, LDA has been proven to be very powerful. relation with the Bayes classiﬁer. However, when nearest
  The purpose of LDA is to maximize the between-class scat- neighbor classiﬁcation is carried out in a high-dimensional
ter while simultaneously minimizing the within-class scatter. feature space, the nearest neighbors of a point can be very
It can be formulated by Fisher Criterion:             far away, causing bias and degrading the performance of the
                                                      rule [Hastie et al., 2001]. Hastie and Tibshirani [Hastie and
                            T
                         W   SbW                      Tibshirani, 1996] proposed a discriminant adaptive nearest
                JF (W ) =  T      ,             (1)
                         W   SwW                      neighbor (DANN) metric to stretch the neighborhood in the
                                                      directions in which the class probabilities don’t change much,
where W is a linear transformation matrix, Sb is the between- but their method also suffers from the small sample size prob-
class scatter matrix and Sw is the within-class scatter matrix. lem.
  ∗The support of NSF of China (69935010) and (60435020) is SNNDA can be regarded as an extension of nonparametric
acknowledged.                                         discriminant analysis[Fukunaga and Mantock, 1983], but itdoesn’t depend on the nonsingularity of the within-class scat- is obviously suboptimal due to discarding much discrimina-
ter matrix. Moreover, SNNDA ﬁnds the important discrimi- tive information.
nant directions without assuming the class densities belong to Liu et al. [Liu et al., 1992] modiﬁed Fisher’s criterion by
any particular parametric family.                     using the total scatter matrix St = Sb + Sw as the denom-
  The rest of the paper is organized as follows: Section 2 inator instead of Sw. It has been proven that the modiﬁed
gives the review and analysis of the current existing variant criterion is exactly equivalent to Fisher criterion. However,
LDA methods. Then we describe stepwise nearest neighbor when Sw is singular, the modiﬁed criterion reaches the max-
discriminant analysis in Section 3. Experimental evaluations imum value, namely 1, for any transformation W in the null
of our method, existing variant LDA methods and the other space of Sw. Thus the transformation W cannot guarantee the
                                                                                 T
state-of-art face recognition approaches are presented in Sec- maximum class separability |W SbW | is maximized. Be-
tion 4. Finally, we give the conclusions in Section 5. sides, this method still needs to calculate an inverse matrix,
                                                      which is time consuming. Chen et al. [Chen et al., 2000]
2  Review and Analysis of Variant LDA                 suggested that the null space spanned by the eigenvectors of
                                                      Sw with zero eigenvalues contains the most discriminative in-
   Methods                                            formation. A LDA method (called NLDA)in the null space of
The purpose of LDA is to maximize the between-class scatter Sw was proposed. It chooses the projection vectors maximiz-
while simultaneously minimizing the within-class scatter. ing Sb with the constraint that Sw is zero. But this approach
  The between-class scatter matrix Sb and the within-class discards the discriminative information outside the null space
scatter matrix Sw are deﬁned as                       of Sw. Figure 1(a) shows that the null space of Sw probably
                                                      contains no discriminant information. Thus, it is obviously
                  Xc
                                        T             suboptimal because it maximizes the between-class scatter in
          Sb  =      pi(mi − m)(mi − m)         (2)
                                                      the null space of Sw instead of the original input space. Be-
                  i=1
                   c                                  sides, the performance of the NLDA drops signiﬁcantly when
                  X                                   N  − c is close to the dimension D, where N is the number
         S    =      p S ,                      (3)
          w           i i                             of samples and c is the number of classes. The reason is that
                  i=1                                 the dimensionality of the null space is too small in this situ-
where c is the number of classes; mi and pi are the mean ation and too much information is lost [Li et al., 2003]. Yu
vector and a priori probability of class i, respectively; m = et al. [Yu and Yang, 2001]proposed a direct LDA (DLDA)
Pc
  i=1 pimi is the total mean vector; Si is the covariance ma- algorithm, which ﬁrst removes the null space of Sb. They as-
trix of class i.                                      sume that no discriminative information exists in this space.
  LDA method tries to ﬁnd a set of projection vectors W ∈ Unfortunately, it be shown that this assumption is incorrect.
 D×d
R     maximizing the ratio of determinant of Sb to Sw, Fig.1(b) demonstrates that the optimal discriminant vectors
                                                      do not necessarily lie in the subspace spanned by the class
                             T
                          |W   SbW |                  centers.
             W  = arg max    T      ,           (4)
                       W  |W  SwW  |
                                                           0.2                  0.2

                                                                     NLDA       0.15       LDA 
where D and d are the dimensionalities of the data before and 0.15                             DLDA 
                                                           0.1                  0.1

after the transformation respectively.                                                         Class 1 
                                                          0.05                  0.05         m  
                                                                                             1
                                                                Class 1 Class 2 
  From Eq.(4), the transformation matrix W must be consti- 0                     0
                          −1                                                 LDA 
                                                          −0.05                 −0.05    m  Class 2 
tuted by the d eigenvectors of Sw Sb corresponding to its ﬁrst                            2
d largest eigenvalues [Fukunaga, 1990].                   −0.1                  −0.1

                                                          −0.15                 −0.15
  However, when the small sample size problem occurs, Sw
                                                          −0.2                  −0.2
                    −1                                     −10 −8 −6 −4 −2 0 2 4 6 8 10 −1.5 −1 −0.5 0 0.5 1 1.5
becomes singular and Sw does not exist. Moreover, if the            (a)                   (b)
class distributions are multimodal or share the same mean (for
example, the samples in (b),(c) and (d) of Figure 2), it can Figure 1: (a) shows that the discriminant vector (dashed line)
fail to ﬁnd the discriminant direction[Fukunaga, 1990]. Many of NLDA contains no discriminant information. (b) shows
methods have been proposed for solving the above problems. that the discriminant vector (dashed line) of DLDA is con-
In following subsections, we give more detailed review and strained to pass through the two class centers m1 and m2.
analysis of these methods.                            But according to the Fisher criteria, the optimal discriminant
                                                      projection should be solid line (both in (a) and (b)).
2.1  Methods Aimed at Singularity of Sw
In recent years, many researchers have noticed the problem
about singularity of Sw and tried to overcome the computa-
tional difﬁculty with LDA.                            2.2  Methods Aimed at Limitations of  Sb
  To avoid the singularity of Sw, a two-stage PCA+LDA ap- When the class conditional densities are multimodal, the class
proach is used in [Belhumeur et al., 1997]. PCA is ﬁrst used separability represented by Sb is poor. Especially in the case
to project the high dimensional face data into a low dimen- that each class shares the same mean, it fails to ﬁnd the dis-
sional feature space. Then LDA is performed in the reduced criminant direction because there is no scatter of the class
PCA subspace, in which Sw is non-singular. But this method means[Fukunaga, 1990].  Notice the rank of Sb is c − 1, so the number of extracted where wn is the sample weight deﬁned as
features is, at most, c − 1. However, unless a posteriori prob-
                                                                               ||∆I ||α
ability function are selected, c − 1 features are suboptimal in                   n
                                                                    wn  =    I α      E α ,          (11)
Bayes sense, although they are optimal with regard to Fisher              ||∆n|| + ||∆n ||
criterion [Fukunaga, 1990].                           where α is a control parameter between zero and inﬁnity. This
  In fact, if classiﬁcation is the ultimate goal, we need sample weight is introduced to deemphasize the samples in
only estimate the class density well near the decision the class center and give emphases to the samples near to the
boundary[Hastie et al., 2001].                        other class. The sample that has a larger ratio between the
  Fukunaga and Mantock [Fukunaga and Mantock, 1983]   nonparametric extra-class and intra-class differences is given
presented a nonparametric discriminant analysis (NDA) in an an undesirable inﬂuence on the scatter matrix. The sample
attempt to overcome these limitations presented in LDA. In weights in Eq.(11) take values close to 0.5 near the classiﬁca-
nonparametric discriminant analysis the between-class scat- tion boundaries and drop to zero as we move to class center.
ter Sb is of nonparametric nature. This scatter matrix is gen- The control parameter α adjusts how fast this happens. In this
erally full rank, thus loosening the bound on extracted fea- paper, we set α = 6.
                                                                                             E
ture dimensionality. Also, the nonparametric structure of this From the Eq.(7) and (8), we can see that ||∆n || represents
matrix inherently leads to the extracted features that preserve the distance between the sample xn and its nearest neighbor
                                           [                                    I
relevant structures for classiﬁcation. Bressan et al. Bressan in the different classes, and ||∆n|| represents the distance be-
and Vitria,` 2003] explored the nexus between nonparametric tween the sample xn and its nearest neighbor in the same
discriminant analysis (NDA) and the nearest neighbors (NN) class. Given a training sample xn, the accuracy of the nearest
classiﬁer and gave a slight modiﬁcation of NDA which ex- neighbor classiﬁcation can be directly computed by examin-
tends the two-class NDA to a multi-class version.     ing the difference
  Although these nonparametric methods overcomes the lim-                    E  2     I 2
                                                                     Θn = ||∆  || − ||∆ || ,         (12)
itations of Sb, they still depend on the singularity of Sw(or                n        n
                                                              E      I
Sˆw). The rank of Sˆw must be no more than N − c.     where ∆   and ∆  are nonparametric extra-class and intra-
                                                      class differences and deﬁned in Eq.(7) and (8).
3  Stepwise Nearest Neighbor Discriminant               If the difference Θn is more than zero, xn will be correctly
   Analysis                                           classiﬁed. Otherwise, xn will be classiﬁed to the false class.
                                                      The larger the difference Θn is, the more accurately the sam-
In this section, we propose a new feature extraction method,
                                                      ple xn is classiﬁed.
stepwise nearest neighbor discriminant analysis(SNNDA). Assuming that we extract features by the D × d linear pro-
SNNDA also uses nonparametric between-class and within- jection matrix W with a constraint that W T W is an identity
class scatter matrix. But it does not depend on singularity of matrix, the projected sample xnew = W T x. The projected
within-class scatter matrix and improves the performance of nonparametric extra-class and intra-class differences can be
NN classiﬁer.                                         written as δE = W T ∆E and δI = W T ∆I . So we expect to
                                                                                            E 2     I 2
3.1  Nearest Neighbor Discriminant Analysis           ﬁnd the optimal W to make the difference ||δn || − ||δn|| in
     Criterion                                        the projected subspace as large as possible.
Assuming a multi-class problem with classes ωi(i =                         XN
                                                              c                     E 2     I 2
1, . . . , c), we deﬁne the extra-class nearest neighbor for a W = arg max    wn(||δn || − ||δn|| ). (13)
                                                                        W
sample x ∈ ωi as                                                           n=1
      E     0        0                                  This optimization problem can be interpreted as: ﬁnd the
     x  = {x ∈/ ωi| ||x − x|| ≤ ||z − x||, ∀z∈ / ωi}. (5)
                                                      linear transform that maximizes the distance between classes,
  In the same fashion, the set of intra-class nearest neighbors while minimizing the expected distance among the samples
are deﬁned as                                         of a single class.
      I     0        0
     x  = {x ∈ ωi| ||x − x|| ≤ ||z − x||, ∀z ∈ ωi}. (6) Considering that,
  The nonparametric extra-class and intra-class differences XN
                                                                    E  2    I 2
are deﬁned as                                                  wn(||δn || − ||δn|| )
                                                            n=1
                 ∆E   =   x − xE,               (7)
                                                            XN                      XN
                   I           I                                    T  E T   T  E            T  I T  T  I
                 ∆    =   x − x .               (8)     =      wn(W  ∆n ) (W  ∆n ) −    wn(W  ∆n)  (W  ∆n)
.                                                           n=1                     n=1
                                                              XN
  The nonparametric between-class and within-class scatter             T E    T  E T
matrix are deﬁned as                                    =   tr(  wn(W   ∆n )(W ∆n ) )
                                                              n=1
                    XN
             ˆ               E    E T                           XN
            Sb   =      wn(∆n )(∆n ) ,          (9)                     T  I    T I T
                                                            −tr(   wn(W  ∆n)(W   ∆n) )
                    n=1
                                                                n=1
                     N
                    X                                             XN
            ˆ                I   I T                            T        E   E T
            Sw   =      wn(∆n)(∆n)   ,         (10)     =   tr(W (   wn∆n  (∆n ) )W )
                    n=1                                           n=1                          1
 0.15                                                 and intra-class differences in its current dimensionality. Thus,
                          0.8
 0.1
                          0.6                         we keep the consistency of the nonparametric extra-class and
 0.05                     0.4                         intra-class differences in the process of dimensionality reduc-
                          0.2
  0                                                   tion.
                          0

 −0.05
                         −0.2                           Figure 3 gives the algorithm of stepwise nearest neighbor

                  Class 1 −0.4
 −0.1                                        Class 1  discriminant analysis.
                  Class 2                    Class 2
                  LDA    −0.6                LDA
 −0.15            NNDA                       NNDA
                         −0.8

 −0.2                     −1
  −5 −4 −3 −2 −1 0 1 2 3 4 5 −8 −6 −4 −2 0 2 4 6 8
            (a)                      (b)                 • Give D dimensional samples {x1, · · · , xN }, we expect
 15                       5                                to ﬁnd d dimensional discriminant subspace.
                          4

 10
                          3                              • Suppose that we ﬁnd the projection matrix Wc via T
                          2
 5                                                         steps, we reduce the dimensionality of samples to dt in
                          1                                step t, and d meet the conditions: d > d > d  ,
 0                        0                                           t                   t−1    t    t+1

                         −1                                d0 = D and dT = d.
 −5
                  Class 1 −2
                  Class 2                    Class 1
                  LDA    −3                  Class 2     • For t = 1, · · · , T
 −10              NNDA                       LDA
                         −4                  NNDA            1. Calculate the nonparametric between-class Sˆt and
 −15                     −5                                                                          b
 −15 −10 −5  0   5  10  15 −15 −10 −5 0  5   10  15
            (c)                      (d)                                              ˆt
                                                               within-class scatter matrix Sw in the current dt−1
                                                               dimensionality,
Figure 2: First projected directions of NNDA (solid) and
                                                             2. Calculate the projection matrix Wc , Wc is d ×d
LDA (dashed) projections, for four artiﬁcial datasets.                                     t   t   t−1  t
                                                               matrix.
                                                             3. Project the samples by the projection matrix Wct,
             XN                                                    cT
           T         I  I T                                    x = Wt  × x.
     −tr(W   (  wn∆   (∆ ) )W )
                     n  n                                                             c    QT   c
             n=1                                         • The ﬁnal transformation matrix W = t=1 Wt.
          T            T
  =  tr(W  SˆbW ) − tr(W SˆwW )
          T ˆ   ˆ
  =  tr(W  (Sb − Sw)W ),                         (14) Figure 3: Stepwise Nearest Neighbor Discriminant Analysis

where tr(·) is the trace of matrix, Sˆb and Sˆw are the non-
parametric between-class and within-class scatter matrix, as
deﬁned in Eq.(9) and (10).                            3.3  Discussions
  So Eq.(13) is equivalent to                         SNNDA has an advantage that there is no need to calculate
                                                      the inverse matrix, so it is a more efﬁcient and stable method.
                           T
          Wc = arg max tr(W (Sˆb − Sˆw)W ).    (15)   Moreover, though SNNDA optimizes the 1-NN classiﬁcation,
                   W                                  it is easy to extend it to the case of k-NN.
  We call Eq.(15) the nearest neighbor discriminant analysis However, a drawback of SNNDA is the computational in-
criterion(NNDA).                                      efﬁciency in ﬁnding the neighbors when the original data
  The projection matrix Wc must be constituted by the d space is high dimensionality. A improved method is that PCA
eigenvectors of (Sˆb − Sˆw) corresponding to its ﬁrst d largest is ﬁrst used to reduce the dimension of data to N −1 (the rank
eigenvalues.                                          of the total scatter matrix) through removing the null space of
  Figure 2 gives comparisons between NNDA and LDA.    the total scatter matrix. Then, SNNDA is performed in the
When the class density is unimodal ((a)), NNDA is approxi- transformed space. Yang et al. [Yang and Yang, 2003] shows
mately equivalent to LDA. But in the cases that the class den- that no discriminant information is lost in this transformed
sity is multimodal or that all the classes share the same mean space.
((b),(c) and (d)), NNDA outperforms LDA greatly.
                                                      4   Experiments
3.2  Stepwise Dimensionality Reduction
                                                      In this section, we apply our method to face recognition
In the analysis of the nearest neighbor discriminant analysis and compare it with the existing variant LDA methods and
criterion, notice that we calculate nonparametric extra-class the other state-of-art face recognition approaches, such as
                         E      I
and intra-class differences (∆ and ∆ ) in original high di- PCA [Turk and Pentland, 1991], PCA+LDA [Belhumeur et
mensional space, then project them to the low dimensional al., 1997], NLDA [Chen et al., 2000], NDA [Bressan and
       E     T  E      I     T  I
space (δ = W  ∆   and δ =  W  ∆  ), which does not ex- Vitria,` 2003] and Bayesian [Moghaddam et al., 2000] ap-
actly agree with the nonparametric extra-class and intra-class proaches. All the experiments are repeated 5 times indepen-
differences in projection subspace except for the orthonor- dently and the average results are calculated.
mal transformation case, so we have no warranty on distance
preservation. A solution for this problem is to ﬁnd the projec- 4.1 Datasets
tion matrix Wc by stepwise dimensionality reduction method. To evaluate the robustness of SNNDA, we perform the ex-
In each step, we re-calculate the nonparametric extra-class periments on three datasets from the popular ATT facedatabase [Samaria and Harter, 1994] and FERET  face   The recognition rate of SNNDA have reached 100% on two
database [Phillips et al., 1998]. The descriptions of the three FERET dataset surprisedly when the dimensionality of sam-
datasets are below:                                   ples is about 20, while the other methods have poor perfor-
ATT Dataset This dataset is the ATT face database (for- mances in the same dimensionality. Moreover, SNNDA does
    merly ‘The ORL   Database of Faces’), which con-  not suffer from overﬁtting. Except SNNDA and PCA, the
    tains 400 images (112 × 92) of 40 persons, 10 im- rank-1 recognition rates of the other methods have a descent
    ages per person. The images are taken at differ-  when the dimensionality increases continuously.
    ent times, varying lighting slightly, facial expres- Fig. 6 shows cumulative recognition rates on the three dif-
    sions (open/closed eyes, smiling/non-smiling) and fa- ferent datasets. From it, we can see that none of the cumula-
    cial details (glasses/no-glasses). Each image is linearly tive recognition rates can reach 100% except SNNDA.
    stretched to the full range of pixel values of [0,255]. When dataset contains the changes of lighting condition
    Fig.4 shows some face examples in this database. The (such as FERET Dataset 1), SNNDA also has obviously bet-
    set of the 10 images for each person is randomly parti- ter performance than the others.
    tioned into a training subset of 5 images and a test set of Different from ATT dataset and FERET dataset 1, where
    the other 5. The training set is then used to learn basis the class labels involved in training and testing are the same,
    components, and the test set for evaluate.        the FERER dataset 2 has no overlap between the training
                                                      set and the galley/probe set according to the FERET proto-
                                                      col [Phillips et al., 1998]. The ability of generalization from
                                                      known subjects in the training set to unknown subjects in the
                                                      gallery/probe set is needed for each method. Thus, the result
                                                      on FERET dataset 2 is more convincing to evaluate the robust
                                                      of each method. We can see that SNNDA also gives the best
                                                      performance than the other methods on FERET dataset 2.
       Figure 4: Face examples from ATT database        A major character, displayed by the experimental results,
                                                      is that SNNDA always has a stable and high recognition rates
                                                      on the three different datasets, while the other methods have
FERET Dataset 1 This dataset is a subset of the FERET unstable performances.
    database with 194 subjects only. Each subject has 3
    images: (a) one taken under controlled lighting condi- 5 Conclusion
    tion with a neutral expression; (b) one taken under the
    same lighting condition as above but with different fa- In this paper, we proposed a new feature extraction method,
    cial expressions (mostly smiling); and (c) one taken un- stepwise nearest neighbor discriminant analysis(SNNDA),
    der different lighting condition and mostly with a neutral which ﬁnds the important discriminant directions without as-
    expression. All images are pre-processed using zero- suming the class densities belong to any particular paramet-
    mean-unit-variance operation and manually registered ric family. It does not depend on the nonsingularity of the
    using the eye positions. All the images are normal- within-class scatter matrix either. Our experimental results
    ized by the eye locations and are cropped to the size of on the three datasets from ATT and FERET face databases
    75 × 65. A mask template is used to remove the back- demonstrate that SNNDA outperforms the existing variant
    ground and the hair. Histogram equalization is applied LDA methods and the other state-of-art face recognition ap-
    to the face images for photometric normalization. Two proaches greatly. Moreover, SNNDA is very efﬁcient, accu-
    images for each person is randomly selected for training rate and robust. In the further works, we will extend SNNDA
    and the rest one is used for test.                to non-linear discriminant analysis with the kernel method.
                                                      Another attempt is to extend SNNDA to the k-NN case.
FERET Dataset 2 This dataset is a different subset of the
    FERET database. All the 1195 people from the FERET
    Fa/Fb data set are used in the experiment. There are References
    two face images for each person. This dataset has no [Belhumeur et al., 1997] P.N. Belhumeur, J. Hespanda, and
    overlap between the training set and the galley/probe set D. Kiregeman. Eigenfaces vs. Fisherfaces: Recognition
    according to the FERET protocol [Phillips et al., 1998]. using class speciﬁc linear projection. IEEE Transactions
    500 people are randomly selected for training, and the on Pattern Analysis and Machine Intelligence, 19(7):711–
    remaining 695 people are used for testing. For each test- 720, 1997.
    ing people, one face image is in the gallery and the other
                                                      [                    ]
    is for probe. All images are pre-processed by using the Bressan and Vitria,` 2003 M. Bressan and J. Vitria.`
    same method in FERET Dataset 1.                      Nonparametric discriminant analysis and   nearest
                                                         neighbor classiﬁcation. Pattern Recognition Letters,
4.2  Experimental Results                                24:2743C2749, 2003.
Fig. 5 shows the rank-1 recognition rates with the different [Chen et al., 2000] L. Chen, H. Liao, M. Ko, J. Lin, and
number of features on the three different datasets. It is shown G. Yu. A new LDA-based face recognition system which
that SNNDA outperforms the other methods. The recogni-   can solve the small sample size problem. Pattern Recog-
tion rate of SNNDA can reach almost 100% on ATT dataset. nition, 33(10):1713–1726, 2000.