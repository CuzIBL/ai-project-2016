           Probabilistic Reasoning with Hierarchically Structured Variables

                       Rita Sharma                                  David Poole
             Department of Computer Science               Department of Computer Science
              University of British Columbia               University of British Columbia
           http://www.cs.ubc.ca/spider/rsharma/          http://www.cs.ubc.ca/spider/poole/
                    rsharma@cs.ubc.ca                             poole@cs.ubc.ca

                    Abstract                          values of LT (i.e., millions of species) can be classiﬁed ac-
                                                      cording to Linnaean taxonomy hierarchy.1 Living things are
    Many practical problems have random variables     divided into kingdoms (e.g., plantae, animalia), classes (e.g.,
    with a large number of values that can be hierarchi- mammals, birds, ﬁsh), all the way down to species.
    cally structured into an abstraction tree of classes. In this paper, we look at two related problems with hierar-
    This paper considers how to represent and exploit chical variables in Bayesian networks. The ﬁrst is the compact
    hierarchical structure in probabilistic reasoning. We representation of conditional probability distributions. The
    represent the distribution for such variables by spec- second is how to exploit that representation in probabilistic
    ifying, for each class, the probability distribution inference for computational gain. We assume that the tree hi-
    over its immediate subclasses. We represent the   erarchy of the values is ﬁxed and does not change with the
    conditional probability distribution of any variable context.
    conditioned on hierarchical variables using inher-  To compute a posterior probability, given some evidence
    itance. We present an approach for reasoning in   in a Bayesian network that has both simple and hierarchi-
    Bayesian networks with hierarchically structured  cal variables, we construct a ﬂat Bayesian network by col-
    variables that dynamically constructs a ﬂat Bayesian lapsing the hierarchies and including only those values neces-
    network, given some evidence and a query, by col- sary to answer the query. We can answer the query from the
    lapsing the hierarchies to include only those values ﬂat Bayesian network using any standard inference algorithm.
    necessary to answer the query. This can be done   The domain size of the variables in the ﬂat Bayesian network is
    with a single pass over the network. We can answer independent of the size of the hierarchies; it depends on how
    the query from the ﬂat Bayesian network using any many of the classes in the hierarchy are directly associated
    standard probabilistic inference algorithm such as with the evidence and query.
    variable elimination or stochastic simulation. The
    domain size of the variables in the ﬂat Bayesian net- 2 Hierarchical Variables
    work is independent of the size of the hierarchies;
    it depends on how many of the classes in the hierar- We divide the discrete random variables into two cate-
    chies are directly associated with the evidence and gories: simple variables and hierarchical variables.We
    query. Thus, the representation is applicable even call Bayesian networks that have both simple and hierarchical
    when the hierarchy is conceptually inﬁnite.       variables hierarchical Bayesian networks.2 We use upper case
                                                      letters to denote simple random variables (e.g., X1, X2, X) and
                                                      the actual value of these variables by small letters (e.g., a, b,
1  Introduction                                       x1). The domain of X, written Val (X), is the set of values that
                                                      X can take on.
Many problem domains have discrete variables with a large A hierarchical variable is a variable in which subsets of the
number of values that can be represented a priori as an abstrac- values of the variable are represented hierarchically in a tree
tion hierarchy (or an is-a hierarchy or taxonomic hierarchy) [Pearl, 1986]. We refer to a node in the tree as a class. The
[                               ]
Pearl, 1986; Mackworth et al., 1985 . We call these vari- nodes in the tree represent subsets of the values of the variable.
ables hierarchical variables. Taxonomic hierarchies allow us The root of the tree represents the set of all the values of the
to manage effectively the large state space of a variable be- variable. The children of a node correspond to a partitioning
cause they allow facts and regularities to be revealed both at of the set of values of the node. Thus, the subsets represented
high and low levels of abstraction. In taxonomic hierarchies by the children of a class must be mutually disjoint and any
the information from high-level abstractions are automatically class represents the union of its children in the tree.
inherited by more speciﬁc concepts. Abstraction hierarchies
also allow us to answer more abstract queries. As an example 1http : //www.wikipedia.org/wiki/Linnaean_taxonomy
of a hierarchical variable, consider a random variable LT that 2The term hierarchical Bayesian network used in this paper is
describes the species of living things. The large number of different than the term hierarchical Bayesian model used in statistics.                       livingthing                    We can specify the probability of an immediate subclass of
                                                      mammal  given mammal, with probabilities such as:
                            plant
                   animal                                 P(bat|mammal) = 0.1

           reptile            fish                    We can compute the prior probability of any class in a recur-
                                                      sive manner by multiplying the probability of class given its
                            bird
              mammal   insect                         immediate superclass and the probability of its immediate su-

           platypus               penguin             perclass. The root has probability 1 (as it represents the set
                 cat bat    sparrow                   of all values). For example, given the probabilities as above,
                                                      P(bat) can be computed as follows:
                                                        P(bat)  =   P(bat|mammal) × P(mammal|animal) ×
  Figure 1: Part of a taxonomic hierarchy of living things          P(animal|livingthing)
                                                                =   0.012
  If a class Cj is a child of Ci, we say that Cj is an immediate In this representation, computing the probability of any class
subclass of Ci. The subclass relation is the symmetric transi- is linear in the depth of the class in the hierarchy and otherwise
tive closure of the immediate subclass relation. That is, Cj is is not a function of the hierarchy’s size.
a subclass of Ci, written Cj ≤ Ci,ifCj is a descendant of Ci in
the abstraction hierarchy (or is Ci). The inverse of subclass is 3.2 Hierarchical Variable Is the Parent of a Simple
superclass. The conjunction of two classes Cj and Ck, written Variable
as Cj ∧ Ck, denotes the set intersection of Cj and Ck. This is
empty unless one is a superclass of the other.        Suppose that H is a hierarchical variable, F is a simple vari-
  We denote the hierarchical variables using boldface upper- able, and H is the only parent of F. We can exploit any hier-
case letters (e.g., LT). For example, a part of the tree hierarchy archical structure by assuming that the inﬂuence on F is local
                                        (  )          in the hierarchy of H. To specify the conditional probabil-
of values of variable LT is shown in Figure 1. tree LT denotes            ( |  )
the tree hierarchy of LT.                             ity distribution (CPD) P F H we use the notion of a default
                                                      probability distribution. The default probability distribution
                                                                             ( |  )
3  Conditional Probabilities of Hierarchical          of F for class Cj, written Pd F Cj , is assumed to be inherited
                                                      by all subclasses of Cj, unless overridden by a more speciﬁc
   Bayesian Networks                                  default probability distribution. We can represent P(F|H) by
                                                                 (  | )
There are two main issues to be considered to represent hier- specifying Pd F Cj for some classes Cj in the hierarchy of H.
archical Bayesian networks:                           We call such classes Cj the exceptional classes of H. The idea
                                                      of using “inheritance” with abstraction hierarchies is similar
C1: specifying the conditional probability for hierarchical to the use of “context speciﬁc independence” for the compact
    variables;                                        representation of the CPTs in a Bayesian network [Boutilier
C2: specifying how variables are conditioned on hierarchical et al., 1996].
    parents.                                            To make this properly deﬁned, we need to ensure that every
  The simplest case of C1 is how to specify the prior prob- terminal node in the tree hierarchy of H is covered by some
ability of a hierarchical variable. The simplest case of C2 is default probability distribution. The union of all of the sets
                                                      of values associated with the exceptional classes must equal
how to specify the conditional probability of a simple variable ( )
conditioned on a hierarchical variable. The more complicated Val H . This condition holds trivially if the root class is an
cases are built from these simpler cases.             exceptional class for F. If the root class is not an exceptional
                                                      class, there must be an exceptional class along every path from
3.1  Prior Probability for Hierarchical Variables     the root.
                                                        The conditional probability of F for any class C of H,
The prior probability of a hierarchical variable can be speci-                                    k
                                                      P(F|C ), can be computed as follows:
ﬁed in a top-down manner. For each class that has subclasses, k
we specify a probability distribution over its immediate sub- •ifCk does not have any exceptional subclasses, P(F|Ck)
classes. For example, suppose we want to specify the prior is inherited from the default probability distribution of F
over the hierarchical variable LT as shown in Figure 1. The for the lowest exceptional superclass Cj of Ck. Thus,
root of the hierarchy is the class livingthing. We specify the
                                                                         P(F|C ) = P (F|C )
probability distribution over its immediate subclasses:                       k     d    j

    P(animal|livingthing) = 0.4                          • otherwise, P(F|Ck) can be derived from its children
    P(plant|livingthing) = 0.6                                              
                                                              P(F|C ) =             P(F|C ) × P(C |C )
We can specify the probability distribution over the immedi-        k                    i       i k
                                                                        ∀ ∈      ( )
ate subclasses of animal. Suppose we have as part of this                Ci children Ck
distribution:                                         Example: Consider a Bayesian network in which a hierarchi-
    P(mammal|animal) = 0.3                            cal variable LT is the only parent of a simple variable FLYING
    P(bird|animal) = 0.2, etc.                        with domain {ﬂying, ¬ﬂying}. To represent P(FLYING|LT),we need to deﬁne the default probability distributions over                               location
                                                             S      LOC
FLYING for the exceptional classes of LT. For example, we                               land  ocean
can state that livingthings have a low probability of ﬂying by

default, but bird and insect are exceptional because they have                     northern  southern
                                                                                        equatorial hemisphere
a high probability of ﬂying. From the children of class mam-     LT               hemisphere

mal,abat is exceptional because it has a high probability of                  N−temperate N−polar S−temperate S−polar
ﬂying. From the children of class bird,apenguin is excep- FLYING     LAYS_EGGS
tional because it has a very low probability of ﬂying. Thus, to   (a)           Part of tree hierarchy of location on the earth
represent P(FLYING|LT) we could have:                                                   (b)
    P (ﬂying|livingthing) = 0.00001
     d                                                Figure 2: (a)A Bayesian network with simple and hierarchical
    P (ﬂying|bird) = 0.5
     d                                                variables (b) A part of the tree hierarchy for LOC
    Pd(ﬂying|bat) = 0.3
    Pd(ﬂying|penguin) = 0.000001
    Pd(ﬂying|insect) = 0.4                            (a). The hierarchical variable LOC represents the location on
Note that P(ﬂying|livingthing) = 0.00001 as the class liv- earth. The simple variable S represents the season. Figure 2
ingthing contains bird and bat that each have a much higher (b) shows the part of the hierarchy of the location. We consider
probability of ﬂying.                                 that S has only two values: (northern hemisphere) summer
                                                      (May – October) and winter (November – April). The prob-
3.3  The General Case                                 ability distribution of class animal of LT over its immediate
                                                      subclasses can be conditioned on some of its parents. Suppose
The general case is when a random variable (hierarchical or the distribution of class animal in the equatorial region is in-
simple) can have any number and any kind of variables as dependent of season, so for parent context X = (equatorial)
parents. To represent the CPD of a hierarchical variable H we could have a default distribution such as:
conditioned on its parents, we assume that each class in the
                                                             (     |      ∧  ) =  .
hierarchy of H has a probability distribution over its immediate Pd reptile animal X 0 1
                                                             (    |      ∧   ) = .
subclasses that is conditioned on (some of) the parents of H. Pd insect animal X 0 4
                                                             (       |     ∧   ) = .
We can treat each of these classes as a simple variable. Thus, Pd mammal animal X 0 2, etc.
the problem of representing a (conditional) probability dis- In the polar regions the distribution of class animal over its
tribution over a hierarchical variable reduces to the problem children depends on the season, so for parent context X =
of representing a (conditional) probability distribution over (N − polar) we could have a default distribution such as:
simple variables.
                                                         Pd(reptile|animal ∧ S = summer ∧ X) = 0.01
  Let F be a simple variable that has both simple and hier- P (mammal|animal ∧ S = summer ∧ X) = 0.04
                        s( )                              d
archical parents. Suppose pa F denotes the simple parents P (insect|animal ∧ S = summer ∧ X) = 0.7, etc.
of F and suppose pah(F) denotes the hierarchical parents of d
F. Let X denote an assignment of a class to each hierarchical
                                     =   1,..., n     4   Construction of a Flat Bayesian Network
parent of F; we call X a parent context. X cx  cx ,
      i                                th             An observation about a hierarchical variable H can be posi-
where cx is a class in the tree hierarchy of the i hierarchical
parent of F.                                        tive or negative. An observation about H is positive when we
  To represent P F|pas (F) ∧ pah (F) we specify the default observe that H is Cobs, where Cobs is a class in the hierarchy
                                                                                                   ∈
probability distribution over F for different combinations of of H. This means that H has a value vh such that vh Cobs.
values for simple parents, pas(F), and some parent contexts. An observation about H is negative when we observe that H is
                                                      ¬                                            ∈
The given parent contexts (parent contexts for which the de- Cobs. This means that H has a value vh such that vh Cobs.
fault distribution over F has been deﬁned) can be represented Without loss of generality, we can assume that there is always
                                                                                H            C
in a concept hierarchy Cp of partial ordering Cp = (Zp, ≤), one positive observation about , denoted by pos. For ex-
                                                                                                C     C
where Zp is the set of given parent contexts. A parent context ample, suppose we have two positive observations 1 and 2
X ∈ Z is a super parent context of parent context Y, written about H. If classes C1 and C2 are disjoint then observations
     p                                                                                              ∧
Y ≤ X,if∀i,  ci ≤ ci . The conditional distribution over F about H are not consistent. If C1 is a subclass of C2, C1 C2
             y    x                                   equals C1. If there are only negative observations about H,
for a parent context X is inherited from the default probability
                                                      we assume root class is the positive observation. There can be
distribution of F for the3 super parent context Z of X in Z
                                                 p    multiple negative observations about H that are descendants
that is not overridden by a more speciﬁc default probability
                                                      of C   in the hierarchy of H.
distribution of F for parent context Y such that X ≤ Y < Z. pos
                                                        We assume a query asks either for a distribution over a
Example: Consider the Bayesian network shown in Figure 2 simple variable or for the probability of a particular class in a
                                                      hierarchical variable.
  3We assume that there is only one most-speciﬁc super parent con-
                                                        Given some evidence (a set of observations) and a query,
text. If there are more than one most-speciﬁc compatible parent con-                   f
texts, with different default probability distributions, there must be a we construct a ﬂat Bayesian network, B , from a hierarchical
                                                                       h
default distribution that disambiguates the two default distributions. Bayesian network B by replacing each hierarchical variable
We are thus forcing the user to disambiguate cases in which there with a simple variable whose domain includes only those val-
would otherwise be a problem of multiple inheritance. ues necessary to answer the query. The state space of a hi-erarchical variable H can be abstracted because, for a given
                                                                              R1 Croot
problem, only certain classes of H are supported directly by               (true)
                                             f
the evidence or relevant to the query. To construct B from                    C1
 h                                           h                         R3           C2
B  given some evidence and a query, we traverse B from              (false)
the leaves upwards, prune those variables irrelevant to an-            C3
swer the query, and abstract the hierarchical variables. We             C4
abstract the hierarchical variables as part of a pass to remove                 Cpos

variables that are irrelevant to the query. From the bottom-               R2         C6
up, you can recursively prune any variable that has no chil-                 C5  (????)
dren and is not observed or queried [Geiger et al., 1990;              C1neg            C2neg
Pearl, 1988] as well as abstract hierarchical variables when
their children have been abstracted. We can then answer the
query from the ﬂat Bayesian network using any standard prob-
                                                                                      H
abilistic inference algorithm.                        Figure 3: The abstraction hierarchy of is divided into three
                                                      regions R1, R2, and R3 by positive (Cpos) and negative (C1neg,
4.1  Abstraction of Hierarchical Variables            C2neg) observations about H
Let H be a hierarchical variable with values Val (H). The ab-
straction of the domain of H, denoted by part(H), is a partition with respect to each of these children (i.e., classes that are
of Val (H). That is, part(H) is a set of subsets of Val (H) such associated with the evidence or the query). The domain of
                (  )                                  Ha is the set of non-empty abstract values for each excep-
that all sets in part H are mutually disjoint and the union                                            a
of all the sets equals Val (H). We refer to a set in part(H) tional class. The algorithm computes the abstract value, v ,
as an abstract value of H.Anabstraction of hierarchical for every exceptional class Ck as follows:
                     a                                     a
variable H, denoted by H , is a simple variable with domain • v = Ck,ifCk does not have any exceptional strict sub-
Val (Ha) = part(H).                                       classes.
                                                         • Otherwise, va = C − C − ...− C , where C ,...,C
4.2  Finding a Safe Abstraction                                           k   1        m        1      m
                                                          are the highest exceptional strict subclasses of Ck. This
In this section we describe a simple algorithm Flat_Bayes set difference represents the set of all of the values that
                                   f
for constructing a ﬂat Bayesian network B from a hierarchi- are in Ck and are not covered by other abstract values.
                    h
cal Bayesian network B given some evidence and a query. The abstract values that are empty are removed from the do-
A safe abstraction has the same posterior distribution over the main of Ha. To recursively abstract the parents of H, the
               f      h
query variable in B as in B . To develop a safe abstraction for algorithm ﬁnds those strict super classes of the exceptional
efﬁcient inference, the following constraints are followed in classes Ck, which are affected by H’s parents.
Flat_Bayes: 1. For every evidence or query, there must be an
                                                      Case2: H is observed
abstract value that directly associates with that evidence/query
                                                      The observations about H divide its tree hierarchy into three
and conveys its effect. 2. All abstract values must be mutually
                                                      regions R1, R2, and R3. R1 includes the classes that are true for
disjoint and exclusive. The algorithm Flat_Bayes consists of
                                                      the observation (superclasses of C ), R2 includes the classes
three phases:                                                                     pos
                                                      that we know nothing about, and R3 includes the classes that
Phase0:                                               we know are false. For example, suppose we have one positive
If a query asks for the probability of a particular class in (Cpos) and two negative (C1neg, C2neg) observations about H
hierarchical variable H, we create an extra binary child Q as shown in Figure 3. The regions R1, R2, and R3 are shown
of H, which is true exactly when the query is true (i.e., by the bold lines in Figure 3. We do not need to distinguish
  (  =     |    ) =        (  =     |  ) =
Pd Q    true Croot  0 and Pd Q   true Cq   1, where   between the values of H that we know are false. The values of
Croot is the root class and Cq is the query class in the tree hi- H can be partitioned into two disjoint and exclusive subsets:
erarchy of H). The variable Q has the same probability as the one (vfalse) corresponds to all the values that are false and an-
query class. We thus reduce the problem to one of computing other (vnotknow) corresponds to all the values we know nothing
the probability of a simple variable.
                                                      about. Thus,                                
Phase1: Abstract
                                                           vfalse = Croot − Cpos ∪ (negative observations)
In this phase Flat_Bayes traverses Bh from the leaves up-                                     
                                                           v      =  C   −  (negative observations)
wards.  It prunes a variable that is not queried or ob-     notknow   pos       
                                                              ( a) =        ,
served and does not have any children [Geiger et al., 1990; Val H     vnotknow vfalse
Pearl, 1988]. For each unpruned hierarchical variable H, the If H also has children, we need to partition its abstract value
algorithm computes its abstraction Ha as follows:     vnotknow into subsets based on the evidence from its children.
Case1: H is not observed                              We need to ﬁnd the exceptional classes Ck of H with respect
                                                      to its children that are in region R2 in the hierarchy of H.We
The hierarchical variable H can be a parent of several simple                      a
        4                                             can compute the abstract value vo that corresponds to each
variables. We need to ﬁnd the exceptional classes Ck of H
                                                      exceptional class Ck in the same way as described in Case1,
  4Note that because of the bottom-up nature of the algorithm, H but we need to remove all the false values. Thus,
                                                        a    a
can only have simple children at this stage of the algorithm. vo = v − {negative observations that are subclasses ofCk}       a,...,  a                                             ,...,
  Let vo1    vok be the non-empty abstract values that cor- Let X1 Xs be the non-query non-observed random vari-
                                   =   a ∪ ...∪  a             h   (  )
respond to exceptional classes Ck. Let vr vo1  vok. ables of B . pa Xi denotes the parents of variable Xi. The
        ( a) =    a,..., a,       −   ,               marginal P(Q ∧ E) can be computed as follows:
Then, Val H     vo1    vok vnotknow vr vfalse
  To recursively abstract the parents of H, the algorithm ﬁnds                  n
                                                            (  ∧   ∈  ) =   ...        (  | (  ))
those strict super classes of exceptional classes Ck and Cpos P Q E  e               P  Xi pa Xi {E∈e}
that are affected by H’s parents.
                                                                          Xs    X1 i=1
Phase2: Construct tables
                                                      Lemma 4.1  The posterior probability P(Q|E) computed
The algorithm constructs the CPT for variables Xa of a ﬂat
                                                      from a Bayesian network after replacing its variables X by
network. Let paa(Xa) denote the abstracted parents of Xa.                                            k
                                                      their abstractions Xa is the same as if it is computed with-
We compute P(Xa = va|pas(Xa) = V ∧ paa(Xa) = V) for                    k
                                s                     out replacing X , if the Xa are constructed from the partitions
each value va of Xa and for each assignment V of pas(Xa)           k       k
                                        s             part(X ) that have the following property :
and V of paa(Xa) for the following two cases:               k
Case1: Xa is not abstracted but it has abstracted parents    ∀Xk, ∀Yj ∈ children(Xk), ∀V ∈ part(pa(Yj)),
                          (  a =  a|  s( a) =    ∧
As discussed in Section 3.3, P X v  pa X      Vs       ∀v1, v2 ∈ V, P(Yj|pa(Yj) = v1) = P(Yj|pa(Yj) = v2) (1)
paa(Xa) = V) is inherited from the default distribution of Xa
                                                      Proof Sketch Summing over all the values of X is equivalent
for the parent context C, C ∈ Zp, V ≤ C and  ∃Y ∈ Zp such                                   k
                                                      to summing over the partition of the values of X . Thus,
that V ≤ Y < C, where Zp is the set of parent contexts for                                    k
                            a
which the default distribution of X has been deﬁned.                           j
        a
Case2: X is an abstracted variable                            P(Xk = v|pa(Xk)) ×   P (Yl|pa (Yl))     (2)
                           s a           a  a
Let  denote the assignment pa (X ) = V ∧ pa (X ) = V.  ∈  ( )                  =
                                   s                   v Val Xk               l 1       j
As discussed in Phase1, va = C − C − ...− C . Then,
                          k    1         m               =              P(X  = v|pa(X )) ×   P (Y |pa (Y ))
        a   a                                                              k         k          l     l
    P (X = v |) = P(Ck|) − ...− P(Cm|)
                                                             V∈part(Xk ) v∈V              l=1
  As discussed in Section 3.1, to compute P(Ck|) we need
the probability distribution of all the super classes of Ck over If (1) is true, we can distribute the product out of the inner-
their immediate subclasses for parent context V. As discussed most sum from the RHS of the above equality, leaving the
                                                                 (   =  | (  ))                ( |  (  ))
in Section 3.3, we can treat each of these classes as a simple term v∈V P Xk v pa Xk , which is equal to P V pa Xk .
variable. Thus, the probability distribution of a class over its Thus (2) is equal to
immediate subclasses for parent context V can be computed                            j
in the same way as described in Case1.                                 a
                                                                   P(X  = V|pa(Xk)) ×   P (Yl|pa (Yl))
  The evidence for observed hierarchical variable H is trans-         k
                                                           V∈part(X )                 l=1
lated into the corresponding abstract variable Ha of Bf . The    k
              a    f
evidence Eh for H in B is the disjunction of all those abstract It is straightforward to incorporate evidence E and query Q
values of H that are not false for the observation.   into the proof.
                                  f
  The domain size of the variables in B is independent of Lemma 4.2 The algorithm Flat_Bayes partitions the values
the size of the hierarchies; it depends on how many classes in of the hierarchical variables X of Bh such that all its children
                                                 5                             k
their hierarchy are exceptional with respect to their relevant satisﬁes the Equation (1).
children in Bh. The running time to construct Bf depends on
                                                      Proof Sketch Let part(Xk) denote the abstraction of the do-
both the depth and number of the exceptional classes. Af-                                               a
ter constructing Bf we can answer the query from Bf using main of hierarchical variable Xk computed by Flat_Bayes. Xk
any standard probabilistic inference algorithm, for example, denotes the abstraction of Xk.IfXk is observed, Flat_Bayes
                                                                       ∈    (  )
variable elimination algorithm [Zhang and Poole, 1994] or removes a set V, V part Xk , such that all values in V are
                                                      false. Flat_Bayes constructs the conditional probability tables
stochastic simulation.                                            a
                                                      for variables Xk of a ﬂat Bayesian network using “inheritance”
4.3  Correctness of Flat Bayesian Network             for the following two cases:
                                                              a
                                                      Case1: Xk is not abstracted but some of its parents are
    h                                                                         (  a =   a|  s( a) =     ∧
Let B be a hierarchical Bayesian network that has n discrete As discussed in Phase2, P Xk v pa Xk   Vs
random variables, X = {X1,...,Xn}, and we want to an-   a( a) =  )    ∈    (  h(  ))
                                                      pa  Xk    V , V   part pa Xk , is inherited from the de-
                           P (Q|E)       Q, E ⊆ X                       a
swer some probabilistic query,   , where          .   fault distribution of X for the parent context C ∈ Zp, V ≤ C
Q            query variables                E                           k
  denotes the             . We observed that   is in  and  ∃Y ∈ Zp such that V ≤ Y < C. Thus,
the set e, e ⊆ domain(E), E ∈ e is the evidence. From ∀   ∈       ( a|  s( a) =      ∧   a( a)  =   )  =
                                                       v     V,  P Xk pa Xk       Vs   pa  Xk       v
Bh                                                            s
   we can recursively prune any variable that has no chil- Pd(Xk|pa (Xk) = Vs ∧ C). This implies Equation (1).
dren, and is not observed or queried [Geiger et al., 1990;    a
                                                      Case2: Xk is abstracted
Pearl, 1988].                                                                          ( a =  a|  s( a) =
                                                      As discussed in Phase2, to compute P Xk v pa Xk
                                                         ∧   a( a) =  )
                         P (Q ∧ E ∈ e)                Vs   pa Xk     V  we need the conditional distribution of
             P(Q|E ∈ e) =                                    C  C  ∈ tree(X )
                           P (E ∈ e)                  classes j, j        k , over their immediate subclasses.
                                                      We can treat a class Cj, Cj ∈ tree(Xk), as a simple variable.
  5Variables irrelevant to the query are pruned by Flat_Bayes. Thus, the proof follows from Case1.