                   Logistic Regression Models for a Fast CBIR Method
                                   Based on Feature Selection
                          R. Ksantini1,D.Ziou1,B.Colin2, and F. Dubeau2
                                        University of Sherbrooke
                                   (1) Computer Science Department
                             {riadh.ksantini, djemel.ziou}@usherbrooke.ca
                                      (2) Mathematic Department
                           {bernard.colin, francois.dubeau}@usherbrooke.ca

                    Abstract                          information about the relative relevances of the featurebase
                                                      feature vectors and because of the noise in these vectors, dis-
    Distance measures like the Euclidean distance have tance measures can fail and irrelevant features might hurt re-
    been the most widely used to measure similarities trieval performance. Probabilistic approaches are a promis-
    between feature vectors in the content-based image ing solution to this CBIR problem, that when compared to
    retrieval (CBIR) systems. However, in these sim-  the standard CBIR methods based on the distance measures,
    ilarity measures no assumption is made about the  can lead to a signiﬁcant gain in retrieval accuracy. In fact,
    probability distributions and the local relevances these approaches are capable of generating probabilistic sim-
    of the feature vectors. Therefore, irrelevant fea- ilarity measures and highly customized metrics for comput-
    tures might hurt retrieval performance. Probabilis- ing image similarity based on the consideration and distinc-
    tic approaches have proven to be an effective solu- tion of the relative feature vector relevances. As to previ-
    tion to this CBIR problem. In this paper, we use  ous works based on these probabilistic approaches, [Peng et
    a Bayesian logistic regression model, in order to al., 2004] used a binary classiﬁcation to classify the database
    compute the weights of a pseudo-metric to improve color image feature vectors as relevant or irrelevant, [Cae-
    its discriminatory capacity and then to increase im- nen and Pauwels, 2002] used the classical quadratic logistic
    age retrieval accuracy. The pseudo-metric weights regression model, in order to classify database image feature
    were adjusted by the classical logistic regression vectors as relevant or irrelevant, [Aksoy et al., 2000] used
    model in [Ksantini et al., 2006]. The Bayesian
                                                      weighted L1 and L2 distances, in order to measure the simi-
    logistic regression model was shown to be a sig-  larity degree between two images and [Aksoy and Haralick,
    niﬁcantly better tool than the classical logistic re- 2001] measure the similarity degree between a query image
    gression one to improve the retrieval performance. and a database image using a likelihood ratio derived from a
    The retrieval method is fast and is based on feature Bayesian classiﬁer.
    selection. Experimental results are reported on the In this paper, we investigate the effectiveness of a Bayesian
    Zubud and WANG color image databases proposed     logistic regression model based on a variational method, in
       [                  ]
    by  Deselaers et al., 2004 .                      order to adjust the weights of a pseudo-metric used in [Ksan-
                                                      tini et al., 2006], and then to improve its discriminatory ca-
1  Introduction                                       pacity and to increase image retrieval accuracy. This pseudo-
                                                      metric makes use of the compressed and quantized versions
The rapid expansion of the Internet and the wide use of               8
digital data in many real world applications in the ﬁeld of of the Daubechies- wavelet decomposed feature vectors, and
medecine, security, communications, commerce and acad- its weights were adjusted by the classical logistic regression.
emia, increased the need for both efﬁcient image database We will show that thanks to the variational method, the used
creation and retrieval procedures. For this reason, content- Bayesian logistic regression model is a signiﬁcantly better
based image retrieval (CBIR) approach was proposed. In this tool than the classical logistic regression model to compute
approach, each image from the database is associated with a the pseudo-metric weights and to improve the querying re-
feature vector capturing certain visual features of the image sults. The retrieval method is fast, efﬁcient and based on
such as color, texture and shape. Then, a similarity measure feature selection. The evaluation of the retrieval method us-
is used to compare these feature vectors and to ﬁnd similar- ing both models, separately, is performed using precision and
                                                                             [                  ]
ities between images with the assumption that images that scope curves as deﬁned in Kherﬁ and Ziou, 2006 .
                                                        In the next section, we brieﬂy deﬁne the pseudo-metric. In
are close to each other in the feature space are also visu-  3
ally similar. Distance measures like the Euclidean distance section , we brieﬂy describe the pseudo-metric weight com-
have been the most widely used for feature vector compar- putation using the classical logistic regression model, while
ison in the CBIR systems. However, these similarity mea- showing the limitations of this latter and that the Bayesian
sures are only based on the distances between feature vec- logistic regression model is more appropriate for the pseudo-
tors in the feature space. Therefore, because of the lack of metric weight computation. Then, we detail the Bayesian lo-

                                                IJCAI-07
                                                  2790                                                                                                 ˜
gistic regression model. Moreover, we will describe the data the two feature vectors are intended to be similar. X0,i is the
training performed for both models. The feature selection absolute value of the difference between the scaling factors
based image retrieval method and the feature vectors used to of the Daubechies-8 wavelets decomposed, compressed and
                                              4                                                      J−1
represent the database images are presented in section .Fi- quantized versions of the two feature vectors and {Xk,i}k=0
nally, in section 5, we will perform some experiments to val- are the numbers of mismatches between the J resolution level
idate the Bayesian logistic regression model and we will use coefﬁcients of these latter. We suppose that we have n0 pairs
the precision and scope, in order to show the advantage of the of similar feature vectors and n1 pairs of dissimilar ones.
Bayesian logistic regression model over the classical logistic Thus, the class Ω0 contains n0 explanatory vectors and their
                                                                                     r  r     n0
regression one, in terms of querying results.         associated binary target variables {Xi ,Si =0}i=1 to repre-
                                                      sent the pairs of the similar feature vectors, and the class Ω1
2  The pseudo-metric                                  contains n1 explanatory vectors and their associated binary
                                                                    { ir  ir =1}n1
Given a query feature vector Q and a featurebase of |DB| target variables Xj ,Sj j=1 to represent the pairs of
                                       J              the dissimilar feature vectors. The pseudo-metric weights w˜0
feature vectors Tk (k =1, ..., |DB|) having 2 components
                                                          {  }J−1
each, our aim is to retrieve in the featurebase the most similar and wk k=0 and an intercept v are chosen to optimize the
feature vectors to Q. To achieve this, Q and the |DB| feature following conditional log-likelihood.
vectors are Daubechies-8 wavelets decomposed, compressed                      n0         n1
                                                                                      r           ir
to m coefﬁcients each and quantized. Then, to measure the L(˜w0,w0, ..., wJ−1,v)= log(pi )+  log(pj ), (4)
similarity degree between Q and a target feature vector Tk                    i=1         j=1
of the featurebase, we use the one-dimensional version of the
                                                             r     ir
pseudo-metric used in [Ksantini et al., 2006] and given by where pi and pj are the relevance and irrelevance probabili-
the following expression                              ties, respectively, and given by
                           
      =˜|  ˜[0]− ˜ [0]|−            ( ˜c[ ]= ˜c [ ])                            J−1
 Q, Tk    w0 Q    Tk            wbin(i) Qq i  Tkq i ,          r             ˜ r          r
                                                              pi  =   F (−w˜0X0,i −   wkXk,i − v),
                         i:Q˜c[i]=0
                           q                                                      k=0
                                                (1)
                                                                                 J−1
where                                                         ir           ˜ ir          ir
                                                          pj   =   F (˜w0X0,j +  wkXk,j +  v),
                                ˜c     ˜c
        ˜c     ˜c         1  if Qq[i]=Tkq[i]                                     k=0
       Qq[i]=Tkq[i]  =                          (2)
                          0  otherwise,                              ex
                                                      where F (x)=  1+ex is the logistic function. For this rea-
˜        ˜                                    ˜c
Q[0] and Tk[0] are the scaling factors of Q and Tk, Qq[i] son, standard optimization algorithms such as Fisher scoring
    ˜c                                                and gradient ascent algorithms [Clogg et al., 1991], can be
and Tkq[i] represent the i-th coefﬁcients of their Daubechies-
                                                      invoked. However, in several cases, especially because of
8 wavelets decomposed, compressed to m coefﬁcients and
                                                      the exponential in the likelihood function or because of the
quantized versions, w˜0 and the wbin(i)’s are the weights to
                                  ()                  existence of many zero explanatory vectors, the maximum
compute, and the bucketing function bin groups these lat- likelihood can fail and estimates of the parameters of interest
ters according to the J resolution levels, such as    (weights and intercept) may not be optimal or may not ex-
                                        J
   bin(i)=log2(i)    with   i =1, ..., 2 − 1. (3)   ist or may be on the boundary of the parameter space. Also,
                                                      as there is complete or quasicomplete separation between Ω0
                                                          Ω
3  The weight computation                             and  1, the function L is made arbitrarily large and standard
                                                      optimization algorithms diverge [Krishnapuram et al., 2005].
In order to improve the discriminatory power of the pseudo- Moreover, as Ω0 and Ω1 are large and high-dimensional,
                                     J−1
metric, we compute its weights w˜0 and {wk}k=0 using a clas- these standard optimization algorithms have high computa-
sical logistic regression model and a Bayesian logistic regres- tional complexity and take long time to converge. The ﬁrst
sion model, separately. We deﬁne two classes, the relevance two problems can be solved by smoothing the parameter of
class denoted by Ω0 and the irrelevance class denoted by Ω1, interest estimates, assuming a certain prior distribution for
in order to classify the feature vector pairs as similar or dis- the parameters, thereby reducing the parameter space, and
similar. The basic principle of using the Bayesian logistic re- the third problem can be solved by using variational trans-
gression model and the classical logistic regression one is to formations which simplify the computation of the parameter
allow a good linear separation between Ω0 and Ω1,andthen of interest estimates [Jaakkola and Jordan, 2000]. This mo-
to compute the weights which represent the local relevances tivates the adoption of a Bayesian logistic regression model
of the pseudo-metric components.                      based on variational methods.
3.1  The classical logistic regression model          3.2  The Bayesian logistic regression model
In this model, each feature vector pair is represented by an ex- In the Bayesian logistic regression framework, there are three
planatory vector and a binary target variable. Speciﬁcally, for main components which are a chosen prior distribution over
the i-th feature vector pair, we associate an explanatory vec- the parameters of interest, the likelihood function and the pos-
         ˜                       J
tor Xi =(X0,i,X0,i, ..., XJ−1,i, 1) ∈ R ×{1} and a binary terior distribution. These three components are formally com-
target Si which is either 0 or 1, depending on whether or not bined by Bayes’ rule. The posterior distribution contains all

                                                IJCAI-07
                                                  2791                                                                                          
                                                           ∝     |   =0    =1      1       1    (  )
the available knowledge about the parameters of interest in  P W  S0    , S1   , i i=0, qi i=0 π W
the model. Among many priors having different distributional
                                                      where
forms, gaussian prior has the advantage of having low com-                         
putational intensity and of smoothing the parameter estimates | =0  =1      1       1   =
                                                      P W  S0    , S1  ,  i i=0, qi i=0
toward a ﬁxed mean and away from unreasonable extremes.          
                                     
However, when the likelihood function is not conjugate of                                         
                                                      
           P    Eq [H ]− P
                                                         1           1    i i  i −  1   ϕ( ) E [H2]−2
the gaussian prior, the posterior distribution has no tractable     i=0    2       i=0   i   qi i  i
form and its mean computation involves high-dimensional    F (i) e                                     ,
integration which has high computational cost. According i=0

to [Jaakkola and Jordan, 2000], it’s possible to use accurate where Eq0 and Eq1 are the expectations with respect to the
                                                                                                   i
variational transformations in order to approximate the like-                                 tanh( 2 )
                                                                  0     1              ( i)=
                                                      distributions q and q , respectively, ϕ  4i   and
lihood function with a simpler tractable exponential form. In  1
this case, thanks to the conjugacy, with a gaussian prior dis- i i=0 are the variational parameters. Therefore, the ap-
tribution over the parameters of interest combined with the proximation of the posterior distribution is considered as an
likelihood approximation, we obtain a closed gaussian form adjustable lower bound and as a proper Gaussian distribu-
approximation to the posterior distribution. However, as the tion with a posterior mean μpost and covariance matrix Σpost
number of observations is large, the number of variational which are estimated by the following Bayesian update equa-
parameters updated to optimize the posterior distribution ap- tions
proximation is also large, thereby the computational cost is
                                                                              1                
high. In the Bayesian logistic regression model that we pro- −1         −1                     t
                                                      (Σpost)    =(Σ)     +2      ϕ(i)Eqi [xi(xi) ] , (5)
pose, we use variational transformations and the Jensen’s in-                 i=0
equality in order to approximate the likelihood function with            
                           
                                                                                   1      1       
tractable exponential form. The explanatory vectors are not      =Σ       (Σ)−1  +      ( −  )   [ ]
observed but instead are distributed according to two speciﬁc μpost   post      μ       i   2 Eqi xi  . (6)
distributions. The posterior distribution is also approximated                     i=0
with a gaussian which depends only on two variational pa- The weight and intercept computation algorithm is in two
rameters. The computation of the posterior distribution ap- phases. The ﬁrst phase is the initialization of q0, q1 and
proximation mean is fast and has low computational com- the gaussian prior π(W), and the second phase is iterative
plexity. In this model, we denote the random vectors whose and allows the computation of Σpost and μpost through the
                                      { r}n0
realizations represent the explanatory vectors Xi i=1 of the Bayesian update equations (5) and (6), respectively, while
             Ω                          {  ir}n1
relevance class 0 and the explanatory vectors Xj j=1 of using an EM type algorithm [Jaakkola and  Jordan, 2000],in
                 Ω        =(˜                    1)                                        1
the irrelevance class 1,byX0  X0,0, X0,0, ..., XJ−1,0, order to ﬁnd the variational parameters i i=0 at each iter-
          ˜
and X1 =(X0,1, X0,1, ..., XJ−1,1, 1), respectively. We sup- ation to have an optimal approximation to the posterior dis-
                                                      tribution. In the initialization phase, q0 and q1 are chosen to
pose that X0 ∼ q0(X0) and X1 ∼ q1(X1),whereq0 and q1
                                                      model Ω0 and Ω1, respectively, and because of the absence of
are two chosen distributions. For X0 we associate a binary
                                                      prior knowledge about the weights and the intercept, π(W) is
random variable S0 whose realizations are the target vari-
       r      n0                                      chosen univariate with zero mean and large variances [Con-
ables {Si =0}i=1,andforX1   we associate a binary ran-
                                                      gdon, 2001]. The values of μpost components are the desired
dom variable S1 whose realizations are the target variables                                       J−1
  ir     n1                                                                            ˜     {   }
{Sj =1}j=1.WesetS0   equal to 0 for similarity and we set estimates of the pseudo-metric weights w0 and wk k=0 and
S1 equal to 1 for dissimilarity. Parameters of interest (weights the intercept v. Once the parameters of the posterior distrib-
                                                      ution approximation are computed, its magnitude is given by
and intercept) are considered as random variables and are de- 1
noted by the random vector W =(˜w0,w0, ..., wJ−1,v).We the term i=0 F (i). This latter becomes very close to 1 as
assume that W ∼ π(W),whereπ   is a gaussian prior with Ω0 and Ω1 are linearly separated or quasi separated and tends
prior mean μ and covariance matrix Σ. Using Bayes’ rule, towards 0 as Ω0 and Ω1 become more and more overlapped.
the posterior distribution over W is given by         Analogically, in the classical logistic regression model, the
                                                            2L                                   1
                                                      term e  has almost the same characteristics as i=0 F (i)
P (W|S0 =0, S1 =1)=
ˆ                                            ˜        [Caenen and Pauwels, 2002]. These two terms will be used
 P           Q1
                  P (S = i|X = xi, W)qi(X = xi) π(W)  to perform feature selection in the retrieval method.
   x0∈Ω0,x1∈Ω1 i=0   i     i           i          ,
                  P (S0 =0, S1 =1)
                                        t             3.3  Training
where P (Si = i|Xi = xi, W)=F ((2i − 1)W xi) for each
i ∈{0, 1}. Using a variational approximation [Jaakkola and Let us consider a color image database which consists of sev-
Jordan, 2000] and the Jensen’s inequality, the posterior distri- eral color image sets such that each set contains color images
bution is approximated as follows                     which are perceptually close to each other in terms of object
                                                      shapes and colors. In order to compute the pseudo-metric
P (W|S0 =0, S1 =1)                                    weights and the intercept by the classical logistic regression
                                                                                             Ω
                          1    1                 model, we have to create the relevance class 0 and the ir-
       P W|S0 =0, S1 =1,  i    , qi    π(W)          relevance class Ω1. To create Ω0, we draw all possible pairs
    ≥                        i=0     i=0     ,
                  P (S0 =0, S1 =1)                    of feature vectors representing color images belonging to the

                                                IJCAI-07
                                                  2792same database color image sets, and for each pair we com- 4. The similarity degrees between the query color image
pute an explanatory vector and we associate to this latter a and the database color images are represented by a re-
binary target variable equal to 0. Similarly, to create Ω1,we sulted array TotalScore,suchas,TotalScore[i]=
                                                          N
draw all possible pairs of feature vectors representing color l=1 γlScorel[i] for each i ∈{1, ..., |DB|},where
images belonging to different database color image sets, and  N
                                                          {γl}l=1 are weightfactors used to down-weight the fea-
for each pair we compute an explanatory vector and we asso-                                           2L
                                                          ture which has low discriminatory power. γl = e l
ciate to this latter a binary target variable equal to 1.Forthe
                                         Ω      Ω         when the weights are computed by the classical logis-
Bayesian logistic regression model, we create the 0 and 1                           =   1    ( l)
with the same way, but instead of associating a binary target tic regression model, and γl i=0 F i when the
                                                          weights are computed by the Bayesian logistic regres-
variable value to each explanatory vector of Ω0 and Ω1,we
                                                          sion model.
associate a binary target variable S0 equal to 0 to all Ω0 ex-
planatory vectors and we associate a binary target variable S1 5. Organize the database color images in order of increas-
equal to 1 to all Ω1 explanatory vectors.                 ing resulted similarity degrees of the array TotalScore.
                                                          The most negative resulted similarity degrees corre-
4  Color image retrieval method                           spond to the closest target images to the query image.
                                                          Finally, return to the user the closest target color images
The querying method is in two phases. The ﬁrst phase is a
                                                          to the query color image and whose number is denoted
preprocessing phase done once for the entire database con-
                                                          by RI and chosen by the user.
taining |DB| color images. The second phase is the querying
phase.                                                4.3  Used feature vectors
4.1  Color image database preprocessing               In order to describe the luminance, colors and the edges of a
We detail the preprocessing phase done once for all the data- color image, we use luminance histogram and weighted his-
                                                      tograms. The image texture description is performed by kur-
base color images before the querying in a general case by                                 ×
the following steps.                                  tosis and skewness histograms. Given an M N pixel LAB
                                                      color image, its luminance histogram hL contains the number
 1. Choose N feature vectors for comparison.          of pixels of the luminance L, and can be written as follows
                                    ( ∈{1        })
 2. Compute the N  feature vectors Tli l   , ..., N                      M−1 N−1
    for each i-th color image of the database, where i ∈           ( )=           (  (  ) −  )
    {1    |   |}                                                 hL c            δ IL i, j  c ,       (7)
      , ..., DB .                                                        i=0 j=0
 3. The feature vectors representing the database color
    images are Daubechies-8 wavelets decomposed, com- for each c ∈{0, ..., 255},whereIL is the luminance image
                                                                                  0
    pressed to m coefﬁcients each and quantized.      and δ is the Kronecker symbol at . The weighted histograms
                                                      are the color histogram constructed after edge region elimina-
 4. Organize the decomposed, compressed and quantized
                                  Θl      Θl     =    tion and the multispectral gradient module mean histogram.
    feature  vectors into search arrays + and − l     The former is given by
    1, ..., N which are used to optimize the pseud-metric
                                                               M−1  N−1                           
    computation process [Ksantini et al., 2006].                   
                                                         h( )=          (  (   ) − )          (   )
                                  l       l J−1         hk c           δ Ik i, j  c χ[0,η] λmax i, j , (8)
 5. Adjustment of the metric weights w˜0 and {wk}k=0 for        i=0 j=0
    each featurebase Tli (i =1, ..., |DB|) representing the
    database color images, where l ∈{1, ..., N}.      and the latter is given by
                                                                                  e ( )
4.2  The querying algorithm                                             ¯e ( )=  hk c
                                                                        hk c        ( ),              (9)
We detail the querying algorithm in a general case by the fol-                  Np,k c
lowing steps.                                         where Np,k(c) is the number of the edge region pixels and is
 1. Given a query color image, we denote the feature vectors  deﬁned as
    representing the query image by Ql l =1, ..., N .           M−1 N−1                            
 2. The feature vectors representing the query image are Np,k(c)=        δ(Ik(i, j) − c)χ]η,+∞[ λmax(i, j) ,
    Daubechies-8 wavelets decomposed, compressed to m            i=0 j=0
                                                                                                     (10)
    coefﬁcients each and quantized.              
                                                      and
 3. The similarity degrees between Ql l =1, ..., N
                                                 =     e ( )=
    and the  database color image feature vectors Tli l hk c
    1       ( =1     |   |)
     , ..., N i , ..., DB  are represented by the arrays M−1 N−1                                  
             =1                      [ ]=       
    Scorel l    , ..., N such that Scorel i Ql,Tli             δ(Ik(i, j) − c)λmax(i, j) χ]η,+∞[ λmax(i, j) ,
    for each i ∈{1, ..., |DB|}. These arrays are returned by  i=0 j=0
                                l   l
    the procedure Retrieval(Ql, m, Θ+, Θ−) l =1, ..., N ,                                            (11)
    respectively. The procedure Retrieval is used to optimize for each c ∈{0, ..., 255} and k = a, b,whereλmax represents
    the querying process [Ksantini et al., 2006].     the multispectral gradient module [Ksantini et al., 2006], η is

                                                IJCAI-07
                                                  2793a threshold deﬁned by the mean of the multispectral gradient them follows a truncated poisson distribution at its greatest
modules computed over all image pixels, Ia and Ib are the realization, to have a best ﬁt. Analogically, we make the
images of the chrominances a red/green and b yellow/blue, same choice for (X0,1, ..., XJ−1,1). Also, we assume that the
                                                                     ˜
respectively, and χ is the characteristic function. The multi- random variable X0,0 whose realizations are positive reals,
spectral gradient module mean histogram provides informa- follows a gaussian mixture distribution, which is the same
tion about the overall contrast in the chrominance and the     ˜
                                                      choice for X0,1. Generally, to carry out an evaluation in the
edge region elimination allows the avoidance of overlappings image retrieval ﬁeld, two principal issues are required: the
or noises between the color histogram populations caused by acquisition of ground truth and the deﬁnition of performance
the edge pixels. The LAB color image kurtosis and skewness criteria. For ground truth, we use human observations. In
histograms are given by                               fact, three external persons participate in the below evalu-
                M−1 N−1                             ation. Concerning performance criteria, we represent the
          κ                κ
         hk (c)=        δ(Ik (i, j) − c),      (12)   evaluation results by the precision-scope curve Pr = f(RI),
                 i=0 j=0                              where the scope RI is the number of images returned to the
and                                                   user. In each querying performed in the evaluation experi-
                M−1 N−1                             ment, each human subject is asked to give a goodness score to
          s ( )=         ( s(   ) − )                 each retrieved image. The goodness score is 2 if the retrieved
         hk c           δ Ik i, j  c ,         (13)                                  1
                 i=0 j=0                              image is almost similar to the query, if the retrieved image
                                                      is fairly similar to the query and 0 if there is no similarity
respectively, for each c ∈{0, ..., 255} and k = L, a, b,where
 κ  κ      κ                                          between the retrieved image and the query. The precision
IL, Ia and Ib are the kurtosis images of the luminance L                       =
                                          s   s       is computed as follows: Pr the sum of goodness scores
and the chrominances a and b, respectively, and IL, Ia and                                       =  (   )
 s                                                    for retrieved images/RI. Therefore, the curve Pr f RI
Ib are the skewness images of these latter. They are obtained gives the precision for different values of RI which lie
by local computations of the kurtosis and skewness values at between 1 and 20 when we perform the querying evaluation
the luminance and chrominance image pixels. Then, a linear on the WANG database, and lie between 1 and 5 when we
interpolation is used to represent the kurtosis and skewness perform the querying evaluation on the ZuBuD database.
values between 0 and 255. Since each used feature vector is a When the human subjects perform different queryings in the
histogram having 256 components, we set J equal to 8 in the evaluation experiment, we compute an average precision for
following section.                                    each value of RI, and then we construct the precision-scope
                                                      curve. In our evaluation experiment, each color image of
5  Experimental results                               the WANG and Zubud databases is represented by N =11
                                                                              h   h ¯e  ¯e  κ   κ  κ   s
                                                      histograms which are hL, ha, hb , ha, hb, hL, ha, hb , hL,
In this section, we will discuss the choices of the distributions s s
q0 and q1, in order to validate the Bayesian logistic regression ha and hb. In order to evaluate the querying in the WANG
model in the image retrieval context. Finally, we will use database, each human subject is asked to formulate a query
the precision and scope as deﬁned in [Kherﬁ and Ziou, from the database and to execute a querying, using weights
2006], to evaluate the querying method using both models computed by the classical logistic regression model, and
separately. The choices of the distributions q0 and q1 and the to give a goodness score to each retrieved image, then to
querying evaluation will be conducted on the WANG and reformulate a query from the database and to execute the
Zubud color image databases proposed by [Deselaers et al., querying, using weights computed by the Bayesian logistic
2004]. The WANG database contains |DB| = 1000 color   regression model, and to give a goodness score to each
images which were selected manually to form 10 sets (e.g. retrieved image. Each human subject performs the querying
Africa, beach, ruins, food) of 100 images each. The Zurich ﬁfty times by choosing a new query from the database
Building Image Database (ZuBuD) contains a training part each time. We repeat this experience for different orders of
of |DB| = 1005 color images and query part of 115 color compression m ∈{30, 20, 10}. To evaluate the querying in
images. The training part consists of 201 building image sets, the ZuBuD database, each human subject is asked to follow
where each set contains 5 color images of the same building the preceding steps, while formulating the queries from the
taken from different positions. Before the feature vector database query part. For the WANG and Zubud databases,
extractions, we represent the WANG and Zubud database the resulted precision-scope curves are given in Figure 1
color images in the perceptually uniform LAB color space. for compression orders m ∈{30, 20, 10}. The Figure 2
Since from each color image of the Zubud and WANG     illustrates two retrieval examples in the Zubud database
databases we extract N =11histograms which are given  comparing the performances of the regression models for
by (7), (8), (9), (12) and (13) respectively, each database is m =30. In each example the query is located at the top-left
represented by eleven featurebases. The choices of q0 and q1 of the dialog box.
will be separately performed for each featurebase. For each
                        ˜
featurebase, we assume that X0,0 and (X0,0, ..., XJ−1,0) are
                                           ˜
independent. We make the same assumption for X0,1 and
(X0,1, ..., XJ−1,1). Moreover, we suppose that the random
vector (X0,0, ..., XJ−1,0) random variables whose realiza-
tions are positive integers, are independent and each one of

                                                IJCAI-07
                                                  2794