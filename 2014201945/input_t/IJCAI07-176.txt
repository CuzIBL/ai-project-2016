          Ensembles of Partially Trained SVMs with Multiplicative Updates

                                   Ivor W. Tsang      James T. Kwok
                           Department of Computer Science and Engineering
                     Hong Kong University of Science and Technology, Hong Kong
                                       {ivor,jamesk}@cse.ust.hk


                    Abstract                            A much simpler approach is to use multiplicative updates,
                                                      which was ﬁrst explored in [Cristianini et al., 1999].How-
    The training of support vector machines (SVM) in- ever, its convergence is sensitive to a learning rate that has to
    volves a quadratic programming problem, which is  be ﬁxed manually. When the learning rate is too large, over-
    often optimized by a complicated numerical solver. shooting and oscillations occur; while when it is too small,
    In this paper, we propose a much simpler approach convergence is slow.
    based on multiplicative updates. This idea was ﬁrst A second problem with the multiplicative update rule is
    explored in [Cristianini et al., 1999], but its con- that it can only work with the hard-margin SVM. However,
    vergence is sensitive to a learning rate that has to on noisy data, the hard-margin SVM has poor performance
    be ﬁxed manually. Moreover, the update rule only  and the soft-margin SVM, which can trade off complexity
    works for the hard-margin SVM, which is known     with training error, is usually preferred. However, the choice
    to have poor performance on noisy data. In this pa- of this tradeoff parameter, which can be from zero to inﬁn-
    per, we show that the multiplicative update of SVM ity, is sometimes critical. Various procedures have been pro-
    can be formulated as a Bregman projection prob-   posed for its determination, such as by using cross-validation
    lem, and the learning rate can then be adapted au- or some generalization error bounds [Evgeniou et al., 2004].
    tomatically. Moreover, because of the connection  However, they are typically very computationally expensive.
    between boosting and Bregman distance, we show      Another possibility that avoids parameter tuning com-
    that this multiplicative update for SVM can be re- pletely is by using the ensemble approach. [Kim et al., 2003]
    garded as boosting the (weighted) Parzen window   combines multiple SVMs with different parameters, and ob-
    classiﬁers. Motivated by the success of boosting, tains improved performance. However, as a lot of SVMs still
    we then consider the use of an adaptive ensemble  have to be trained, this approach is also very expensive.
    of the partially trained SVMs. Extensive experi-
                                                        In this paper, we ﬁrst address the learning rate problem by
    ments show that the proposed multiplicative update
                                                      formulating the multiplicative update of SVM as a Bregman
    rule with an adaptive learning rate leads to faster
                                                      projection problem [Collins and Schapire, 2002]. It can then
    and more stable convergence. Moreover, the pro-
                                                      be shown that this learning rate can be adapted automatically
    posed ensemble has efﬁcient training and compa-
                                                      based on the data. Moreover, because of the connection be-
    rable or even better accuracy than the best-tuned
                                                      tween boosting and Bregman distance [Collins and Schapire,
    soft-margin SVM.
                                                      2002; Kivinen and Warmuth, 1999], we show that this mul-
                                                      tiplicative update can be regarded as a boosting algorithm in
                                                      which the (weighted) Parzen window classiﬁers are the base
1  Introduction                                       hypotheses, and each base hypothesis added to the ensemble
Kernel methods, such as the support vector machines   is a partially trained hard-margin SVM. Now, in the boosting
(SVMs), have been highly successful in many machine learn- literature, it is well-known that the whole ensemble performs
ing problems. Standard SVM training involves a quadratic better than a single base hypothesis. Hence, to address the
programming (QP) problem, which is often solved by a com- possibly poor performance of the hard-margin SVM, we con-
plicated numerical solver. Moreover, while a general-purpose sider using the whole ensemble of partially trained SVMs for
QP solver can be used, they are inefﬁcient on this particu- prediction. Note that this comes at little extra cost in the mul-
lar type of QPs. Consequently, a lot of specialized optimiza- tiplicative updating procedure. Experimentally, this ensemble
tion techniques have been developed. The most popular ap- is observed to have comparable or even better accuracy than
proach is by using decomposition methods such as sequential the best-tuned soft-margin SVM.
minimization optimization (SMO) [Platt, 1999],whichin-  The rest of this paper is organized as follows. The learn-
volves sophisticated working set selection strategies, advance ing rate problem in multiplicative update is addressed in Sec-
caching schemes for the kernel matrix and its gradients, and tion 2. The connection between this update process and
also heuristics for stepsize prediction.              boosting, together with the proposed ensemble of partially

                                                IJCAI-07
                                                  1089trained hard-margin SVMs, is then discussed in Section 3. 2.2 Bregman Projection
Experimental results are presented in Section 4, and the last In this section, we assume that we have access to the optimal
section gives some concluding remarks.                value of ρ∗. We will return to the issue of determining ρ∗ in
                                                      Section 2.3.
                                                                             m             
2  Multiplicative Updating Rule of SVM                  As α  ∈ Pm  = {α  ∈ R   | α ≥ 0, α 1 =1},them-
                                                      dimensional probability simplex, a natural choice of the Breg-
In this paper, we focus on a particular variant of the hard- man distance is the (unnormalized) relative entropy. At the t-
margin SVM, called ρ-SVM in the sequel, which will be th iteration, we minimize the Bregman distance between the
introduced in Section 2.1. We then develop in Sections 2.2                              (t)     (t) 
                                                      new α and the current estimate αt =[α1 ,...,αm ] , while
                                                                           ∂u(α ,ρ∗)
and 2.3 an iterative multiplicative updating procedure based  α          α    t      α K˜ α − ρ∗1
on the Bregman projection [Collins and Schapire, 2002; requiring to satisfy   ∂α    =   (    t     )=0,
Kivinen and Warmuth, 1999]. The convergence of this it- which is similar to the constraint in (5). We then have the
                                                      following entropy projection problem:
erative procedure is shown in Section 2.4.
                                                               Xm “     α            ”
                                                          min      α ln  i − α  + α(t) : ρ∗ = αK˜ α .
2.1  Hard-Margin   ρ-SVM                                            i    (t)   i   i              t    (7)
                                                         α∈Pm           α
                       m               d                       i=1       i
Given a training set {xi,yi}i=1,wherexi ∈ R and yi ∈ R,
                                 
the ρ-SVM ﬁnds the plane f(x)=w   ϕ(x) in the kernel-   Introducing Lagrange multipliers for the constraints ρ∗ =
induced feature space (with feature map ϕ) that separates the αK˜ α α1
                             ρ/w                          t and    =1(it will be shown that the remaining
two classes with maximum margin    :                  constraint α ≥ 0 is inactive and so no Lagrange multiplier
              2                                      is needed) and on setting the derivative of the Lagrangian
 max 2ρ −w     : yiw ϕ(xi) ≥ ρ, i =1,...,m.   (1)   
  w,ρ                                                   m        αi       (t)      ∗     ˜        
                                                        i=1 αi ln (t) −αi +αi −ηt ρ − α Kαt   −λ(α  1−1)
                                                                αi
The corresponding dual is:                                                            (t)
                                                      w.r.t. α to zero, we have αi = αi exp(−ηtyift(xi)+
                                                      λ                                           α1
            αK˜ α    α ≥ 0,  α1    ,                 ) on using (3).  Substituting this back into    =
         minα       :             =1            (2)                     λ     −   Z  η          Z  η
                                                      1, we then obtain   =    ln  t( t),where  t( t)=
                                                        m  α(t)    −η  y f x
where 0 and 1 are vectors of zero and one respectively, α = i=1 i exp( t i t( i)), and thus
                                              ˜
[α1,...,αm] is the vector of Lagrange multipliers, and K =      (t+1)   (t)
                                                              α        α      −ηtyift xi /Zt ηt .     (8)
[yiyjk(xi, xj )]. It can be easily shown that                   i   =   i exp(       (  ))   ( )
         Xm                    Xm                     Such an update is commonly known as multiplicative up-
     w =    αiyiϕ(xi), and f(x)=   αiyik(xi, x). (3)  date [Cristianini et al., 1999] or exponentiated gradient (EG)
         i=1                    i=1                   [Kivinen and Warmuth, 1997], with ηt playing the role of
                                                      the learning rate. Notice that by initializing2 α1 > 0,then
Because of the zero duality gap, the objectives in (1) and (2) α > 0
                  1                                     t     in all subsequent iterations. Hence, as mentioned
are equal at optimality , i.e.,                       earlier, α ≥ 0 in (7) is an inactive constraint. Finally, it can
                    ∗    ∗   ∗                       be shown that ηt can be obtained from the dual of (7), as:
                   ρ = α   K˜ α .               (4)
                                                                                       ∗
                                                                   max  − ln(Zt(ηt)exp(ρ ηt)).        (9)
Denote                                                              ηt
                    1   ˜       
           u(α, ρ¯)= α Kα  − ρ¯(α 1 − 1),               As mentioned in Section 1, a similar multiplicative updat-
                    2
                                                      ing algorithm for the standard hard-margin SVM is also ex-
     ρ
where ¯ is the Lagrangian multiplier for the dual constraint plored in [Cristianini et al., 1999]. However, the learning rate
α1
    =1. Moreover, the Karush-Kuhn-Tucker (KKT) condi- η there is not adaptive and the performance is sensitive to the
tions lead to:                                                       η                        ρ
                                                     manual choice of . On the other hand, for our -SVM vari-
                     ∂u α, ρ∗                        ant, the value of ηt can be automatically determined from (9).
            0 ≤ α∗⊥     (  ¯ )   ≥ 0,
                        ∂α                     (5)   The improved convergence properties of our adaptive scheme
                               α∗                     will be experimentally demonstrated in Section 4.1.
where ⊥ denotes that the two vectors are orthogonal, and
                                                      2.3  Estimating the Value of ρ∗
 ∂u(α, ρ¯)                                                                                   ∗
         = K˜ α − ρ¯1 =[y f(x ) − ρ,...,y¯ f(x ) − ρ¯], We now return to the question of estimating ρ . Recall that
   ∂α                 1   1         m   m       (6)                        1                   ∗
                                                      we initialize α as α1 = m 1. From (2) and (4), ρ is equal to
                               ρ                      the optimal dual objective (which is to be minimized), and so
on using (3). Moreover, the optimal ¯ (i.e., the dual variable    ∗
of the dual) is indeed equal to the optimal primal variable ρ: the optimal α should yield a smaller objective than that by
                                                                               1   ˜
                                                      α1 (which is equal to ρ¯1 = m2 1 K1). Moreover, from (4),
                     ρ∗   ρ∗,                                 ∗                     ∗
                      ¯ =                             we have ρ ≥ 0 as K  0. Hence, ρ ∈ [0, ρ¯1].
                                                        In the following, we consider using a decreasing sequence
  ρ∗   ρ∗α∗1   α∗K˜ α∗  ρ∗                                                   ∗
as ¯ =¯       =         =    on using (4), (5) and (6). of ρt’s such that ρ¯1 ≥ ρt ≥ ρ . Consequently, the constraint
  1                                             ∗        2                     α  =  1 1
   Here, variables at optimality are denoted by the superscript . In this paper, we initialize as 1 m .

                                                IJCAI-07
                                                  1090                                ∗      ˜
in (7) has to be replaced by ρt ≥ ρ = α Kαt,andthe    Algorithm 1 Training the ρ-SVM.
entropy projection problem becomes                     1: Input: S = {(xi,yi)}i=1,...,m, tolerance 1 and .
       Xm “                  ”                                           (1)
                αi         (t)           
  min      α  ln    − α + α     :  ρ ≥ α K˜ α .        2: Initialize t =1: αi =1/m for all i =1,...,m,and
             i   (t)   i   i        t       t  (10)
 α∈Pm           α                                                ˜
       i=1       i                                        ρ¯1 = α1Kα1.
           ˜                                                             ft xi
Let ρ¯t ≡ αtKαt. We ﬁrst consider the feasibility of (10). 3: Use (3) to compute ( ).
                                                       4: Set ρt =¯ρt/(1 + t).
Proposition 1. For any 1 >t > 0,ifρ¯t satisﬁes
                                                       5: Set ηt =argminη≥0  Zt(η)exp(ρtη).Ifηt <  0,then
                   1 − t   ∗
                 ρ       ≥ ρ  >  ,                        t = t/2, and goto Step 8 to test whether t becomes too
                 ¯t            0              (11)
                   1+  t                                  small; else if ηt =0,thensetT = t − 1 and terminate.
                                 ρ¯t     ˜
                α ∈ Pm              ≥  α Kαt           6: Update the Lagrangian multipliers αi’s using (8), as
then there exists an    such that 1+t       .
                                                          α(t+1)   α(t)    −η y f x   /Z  η
            ˜               ˜                             i    =   i exp (  t i t( i))  t( t), and compute
Proof. Since K  0,wehavev  Kv  ≥ 0 for any vector v.               ˜
              ∗                                           ρ¯t+1 = αt+1Kαt+1.
Put v = αt − α ,then
                                                       7: If ρ¯t ≥ ρ¯t+1,thent+1 = t and t ← t +1and goto
         α K˜ α ≥ α∗K˜ α − α∗K˜ α∗.
          t    t  2       t                    (12)       Step 3; else t = t/2 and check whether t <.
From (4), the condition                                8: If t ≥ , then goto Step 4; else set T = t − 1 and
      1 − t    ∗        ˜   1 − t   ∗ ˜ ∗             terminate.
    ρ¯t     ≥  ρ   ⇒   αtKαt       ≥  α  Kα  . (13)              f      x    f    x
      1+t                    1+t                     9: Output: SVM(   )=   T +1( ).
Summing (12) and (13), we have
                 ρ¯t     ∗ ˜
                     ≥ α  Kαt,                 (14)   2.4  Convergence
               1+t
             ∗                                        In this section, we will design an auxiliary function, which is
and thus α = α satisﬁes the condition.
                                                      then used to lower bound the amount that the loss decreases at
               1−t    ∗
  Hence, when ρ¯t 1+ ≥ ρ , a feasible solution of (10) ex- each iteration [Collins and Schapire, 2002]. Denote the rela-
                  t                                                                                  ·, ·
          αt+1              α                         tive entropy (which is our Bregman distance here) by Δ( ).
ists. We set   as the optimal of (10). Notice that (10)                     ∗
differs from (7) only in that the equality constraint in (7) now From (14), the optimal α solution of (2) is in the intersec-
becomes an inequality constraint. Hence, optimization of tion of all the hyperplanes deﬁned by the linear constraints
                                                             ˜
(10) is almost exactly the same as that in Section 2.2, except ρt ≥ α Kαt for all t’s. Using (14), (15) and (16), we have
that optimization of ηt now has an extra constraint η ≥ 0.              ˜       ∗ ˜
                                ˜                                  αt+1Kαt  ≥ α  Kαt.               (18)
  Subsequently, we set ρ¯t+1 = αt+1Kαt+1 and ρt+1 ac-
cording to                                            Now, the decrease in loss at two consecutive iterations is:
                          ρ¯t
                    ρ         .                               α∗, α   −   α∗, α
                     t =                      (15)         Δ(      t)  Δ(     t+1)
                        1+   t                               m                    
                                   ˜                       
From the KKT condition of (10), ηt(αt+1Kαt − ρt)=0.              ∗     (t+1)     (t)           
                                                        =      αi  ln αi   − ln αi   (as αt1 = αt+11 =1)
                3
This, together with ηt > 0, leads to                        i=1
                      ˜                                        m
                  αt+1Kαt  =  ρt.              (16)             
                                                            −η     α∗y f  x  −   Z  η
Using (3), then                                         =      t    i i t( i)  ln t( t) (using (8))
                                                               i=1
                 α  K˜ α
                  t+1   t      1     ρ¯t                        m
     cos ωt,t+1 = √ √     =              ,     (17)                 (t+1)
                  ρ¯t ρ¯t+1  1+t   ρ¯t+1               ≥−ηt       αi   yift(xi) − ln Zt(ηt)(using (18))
where ωt,t+1 is the angle between wt and wt+1. Recall that      i=1
                                                            m                       
this entropy projection procedure is used to solve the dual in   (t+1)    (t+1)    (t)
(2). When (2) starts to converge, the angle ωt,t+1 tends to =  αi     ln αi   − ln αi  ,  (using (8))
zero, and cos ωt,t+1 → 1.Ast > 0,ifρ¯t+1 > ρ¯t, from       i=1
(17), the optimization progress becomes deteriorated. This
implies that the current t is too large, and we have overshot or, for all t,
and this solution is discarded. This is also the case when (10) ∗     ∗
                                                       Δ(α  , αt) − Δ(α , αt+1) ≥ Δ(αt+1, αt) ≡ A(αt), (19)
is infeasible. Instead, we can relax the constraint in (10) by
setting t ← t/2 to obtain a larger ρt. This is repeated until where A(·) is the auxiliary function. As A(·) never increases
                                 ∗
ρ¯t+1 ≤ ρ¯t. Note that we always have ρ ≤ ··· ≤ ρ¯t+1 ≤ ρ¯t and is bounded below by zero (as Δ(·, ·) ≥ 0), the sequence
          
  ρ∗    α∗ K˜ α∗    α∗                                of A(αt)’s converges to zero. Since α ∈ Pm is compact, by
as   =          and    is optimal. Optimization of (10)                                   ∗
                      αt+1    ρt+1                    the continuity of A and the uniqueness of α , the limit of this
then resumes with the new  and    . The whole process                                                  ∗
           t                                        sequence of minimizers converges to the global optimum α .
iterates until is smaller than some termination threshold .                                      
The complete algorithm4 is shown in Algorithm 1.        Moreover, using (15) and (16), we have αt+1 K˜ αt =
                                                                
                                                       ρ¯t      t ˜ t                      ˜     tρ¯t
  3                                                       .Asα   Kα   =¯ρt,then|(αt − αt+1) Kαt| =    .In
   When ηt =0, no process can be made as we achieve the global 1+t                               1+t
                                          ∗                                               ˜
optimal. Hence, we terminate the algorithm and αt = α . the special case where ft(xi) ∈ [−1, 1], Kαt∞ ≤ 1.Us-
  4                                                                                                  tρ¯t
                         =0.005          1 =0.1                                    αt+1 − αt1 ≥
   In the experiments, we use   and initialize   .    ing the H¨older’s inequality, we have         1+t .

                                                IJCAI-07
                                                  1091                                                                       (t)                 (t)
We then apply the Pinsker’s inequality [Fedotov et al., 2001] are identical, with αi playing the role of di (compare (8)
and obtain                                            and (21)), and ηt the role of ct (compare (9) and (20)). This is
                             2 2                      a consequence of the fact that boosting can also be regarded
                            t ρ¯t
              αt+1, αt >          .                                      [                        ]
           Δ(        )          2                    as entropy projection Kivinen and Warmuth, 1999 .From
                         2(1 + t)                     this boosting perspective, we are effectively using base hy-
           tρ¯t                                      pothesis of the form (3), with α ∈ Pm (not necessarily equal
Let ν =min1+  , and summing up (19) at each step, then
              t                                       to α∗). This base hypothesis can be regarded as a variant of
                                T                    the Parzen window classiﬁer, with the patterns weighted by
       ∗           ∗
   Δ(α  , α1) − Δ(α , αT +1) ≥     Δ(αt+1, αt),       α                 6                        t
                                                        . Note that the edge of the base hypothesis at the -th itera-
                                t=1                           m         m    (t)               ˜
                                                      tion is i=1 αiyi  j=1 αi yik(xi, xj ) = α Kαt. Thus,
       ⇒    m  ≥    α∗, α   >Tν2/     ,
          ln     Δ(      1)          2                the constraint in (7) is the same as requiring that the edge of
                                                                                                 ∗
and so the number of iterations required is at most T = the base hypothesis w.r.t. the αi distribution to be ρ .
 2lnm                                                                                              ρ
 ν2  for a ﬁxed . This gives a faster ﬁnite convergence The same correspondence also holds between the -SVM
                                                                                             ∗         7
result than the linear asymptotic convergence of the SMO al- algorithm (Algorithm 1) and the AdaBoostν algorithm ,
       [        ]                                     with additionally that ρt in ρ-SVM is analogous to t in
gorithm Lin, 2001 .                                           ∗
                                  t                   AdaBoostν. Moreover,  in the ρ-SVM is similar to ν in
  Assuming that (11) is satisﬁed for all and following the    ∗
convergence proof here5, the solution of the ρ-SVM’s dual, AdaBoostν in controlling the approximation quality of the
       ∗                                ˜            maximum margin. Note, however, that ν is quite difﬁcult to
αt → α   in at most T iterations such that αT +1KαT +1 →
 ∗                                                                                   
ρ . Thus, the ﬁnal solution is the desired ρ-SVM classiﬁer. set in practice. On the other hand, our t can be set adaptively.
                                                        There have been some other well-known connections be-
3  Ensemble of Partially Trained SVMs                 tween SVM  and boosting (e.g., [R¨atsch, 2001]). However,
                                                      typically these are interested in relating the combined hypoth-
There is a close resemblance between the update rules of the esis with the SVM, and the fact that boosting uses the L1
ρ-SVM  with variants of the AdaBoost algorithm. In partic- norm while SVM uses the L2 norm. Here, on the other hand,
ular, we will show in Section 3.1 that the algorithm in Sec-
              ∗                                       our focus is on showing that by using the weighted Parzen
tion 2.2 (with ρ assumed to be known) is similar to the window classiﬁer as the base hypothesis, then the last hy-
AdaBoost algorithm (Algorithm 2); while that in Section 2.3
      ∗                                 ∗             pothesis is indeed a SVM.
(with ρ unknown) is similar to the AdaBoostν algorithm
[R¨atsch and Warmuth, 2005]. Inspired by this connection, 3.2 Using the Whole Ensemble for Prediction
in Section 3.2, we consider using the output of the boosting As mentioned in Section 1, the hard-margin SVM (which is
ensemble for improved performance. The time complexity the same as the last hypothesis) may have poor performance
will be considered in Section 3.3.                    on noisy data. Inspired by its connection with boosting above,
                                                      we will use the boosting ensemble’s output for prediction:
Algorithm 2 AdaBoost [R¨atsch and Warmuth, 2005]
                                                                       T
                                                                              ηt
 1: Input: S = {(xi,yi)}i=1,...,m; Number of iteration T .                         f x ,
                                                                           T       t( )
             (1)                                                                ηr
 2: Initialize: di =1/m for all i =1,...,m.                            t=1   r=1
 3: for t =1,...,T do                                 which is a convex combination of all the partially trained
 4:  Train a weak classiﬁer w.r.t. the distribution di for each SVMs. Note that this takes little extra cost. As evidenced
     pattern xi to obtain a hypothesis ht ∈ [−1, 1].  in the boosting literature, the use of such an ensemble can
 5:  Set                                              lead to better performance, and this will also be experimen-
                ct =argminNt(c)exp(c),        (20)
                        c≥0                           tally demonstrated in Section 4.
                   
                     m   (t)                          3.3  Computational Issue
     where Nt(c)=    i=1 di exp (−cyiht(xi)).
                                                                                            2
 6:  Update distributions di:                         In (8), each step of the EG update takes O(m ) time, which
          (t+1)      (t)                              can be excessive on large data sets. This can be alleviated by
         di     =   di  exp (−ctyiht(xi)) /Nt(ct). (21) performing low-rank approximations on the kernel matrix K
                                                      [Fine and Scheinberg, 2001],whichtakesO(mr2) time (r is
 7: end for
                  T      c                           the rank of K). Then, the EG update and computation of ρt
           f x          P  t  h  x .                                                                   ¯
 8: Output: ( )=    t=1  T  c  t( )                                               2
                         r=1 r                        both take O(mr), instead of O(m ), time.
                                                      4   Experiments
3.1  ρ-SVM Training vs Boosting                       In this section, experiments are performed on ﬁve small
                                                                                                    8
                             α(t)   η                 (breast cancer, diabetis, german, titanic and waveform) and
By comparing the update rules of i and t in Section 2.2                               P
            (t)                                          6                    h   γ =   m  d(t)y h (x )
with those of di and ct in AdaBoost, we can see that they The edge of the hypothesis t is t i=1 i i t i .
                                                         7            ∗
                                                         In the AdaBoostν algorithm,   in Algorithm 2 is replaced by
  5    1−t    ∗     ∗      ˜     ∗     2t
     ρ¯t   <ρ       ρ  ≤ α  Kαt <ρ(1 +      )          t =minr=1,...,t γr − ν  γt           ht
   If  1+t    ,then       t             1−t which                      ,where  is the edge of .
satisﬁes the loose KKT condition for the ρ-SVM.          8http://ida.ﬁrst.fraunhofer.de/projects/bench/benchmarks.htm

                                                IJCAI-07
                                                  1092                                                                               2
seven medium-to-large real-world data sets (astro-particles, 1 m        1   m    
                                                      β =  m   i=1 xi − m   j=1 xj . Each small data set in
adult1, adult2, adult3, adult, web1 and web2)9 (Table 1).
                                                      Table 1 comes with 100 realizations, and the performance is
                                                      obtained by averaging over all these realizations. However,
        Table1:Asummaryofthedatasetsused.             for each large data set, only one realization is available.
                                                        The ensemble is compared with the following:
     data set dim   # training patterns # test patterns
     cancer    9         200            77              1. SVM (BEST): The best-performing SVM among all the
     diabetis  8         468            300               C-SVMs obtained by the regularization path algorithm10
     german    20        700            300               [Hastie et al., 2005];
     titanic   3         150           2,051            2. SVM (LOO): The C-SVM which has the best leave-one-
    waveform   21        200           4,600              out-error bound [Evgeniou et al., 2004] among those ob-
      astro    4        3,089          4,000
                                                          tained by the regularization path algorithm;
     adult1   123       1,605         30,956
     adult2   123       2,265         30,296            3. the Parzen window classiﬁer, which is the ﬁrst hypothe-
     adult3   123       3,185         29,376              sis added to the ensemble;
      adult   123       32,561        16,281            4. the (hard-margin) ρ-SVM, which is the last hypothesis;
      web1    300       2,477         47,272
      web2    300       3,470         46,279            5. the proposed ensemble.
                                                      All these are implemented in Matlab11 and run on a 3.2GHz
                                                      Pentium–4 machine with 1GB RAM.
4.1  Adaptive Learning Rate
We ﬁrst demonstrate the advantages of the adaptive learning Table 2: Testing errors (in %) on the different data sets.
rate scheme (in Section 2.2). Because of the lack of space, we
                                                                  SVM     SVM    Parzen    ρ-   proposed
only report results on breast cancer and waveform. Similar
                                                        data set (BEST)  (LOO)   window  SVM    ensemble
behavior is observed on the other data sets. For comparison, cancer 23.1  26.1    27.4    33.5    28.2
we also train the ρ-SVM using the multiplicative updating
             η                                          diabetis  24.8    28.9    33.4    34.5    24.8
rule (with ﬁxed )in[Cristianini et al., 1999].          german    22.6    24.9    29.8    29.3    23.7
  As can be seen from Figure 1, the performance of [Cris- titanic 22.7    25.5    23.3    45.2    22.5
tianini et al., 1999] is sensitive to the ﬁxed choice of η.When waveform 11.2 18.8 22.0   11.2    10.0
η is small (e.g., η =0.01 or smaller), the objective value de- astro 5.8   9.3     7.0    22.1    3.4
creases gradually. However, when η is slightly larger (say, adult1 16.3   17.7    24.1    19.4    15.9
at 0.02), the estimate will ﬁnally over-shoot (as indicated by adult2 16.0 17.4   24.0    22.8    15.8
the vertical lines) and convergence can no longer be obtained. adult3 16.1 16.2   24.1    19.7    15.9
On the other hand, our proposed adaptive scheme always con- web1   2.3     2.8     3.0    14.4    2.8
verges much faster.                                      web2      2.1     2.2     3.0    3.4     2.1


    0                        0
  10                        10                          As can be seen from Table 2, the ensemble performs much
               ρ-SVM(adaptive)          ρ-SVM(adaptive) better than the (hard-margin) ρ-SVM. Indeed, its accuracy is
    -1         SVM(η=0.005)             SVM(η=0.005)
  10
               SVM(η=0.01)   -1         SVM(η=0.01)
                            10                        comparable to or sometimes even better that of SVM(BEST).
               SVM(η=0.02)              SVM(η=0.02)
    -2                                                                      12
  10           SVM(η=0.04)              SVM(η=0.04)   Table 3 compares the time for obtaining the ensemble with
                             -2
  objective                objective 10               that of running the regularization path algorithm. As can be
    -3
  10                                                  seen, obtaining the ensemble is much faster, and takes a small
    -4                       -3                       number of iterations. To illustrate the performance on large
  10 0      1     2      3  10 0     1     2      3
    10    10     10     10   10     10    10     10
            #iterations              #iterations      data sets, we experiment on the full adult data set in Table 1.
         (a) cancer              (b) waveform         The regularization path algorithm cannot be run on this large
Figure 1: Comparing the EG update (with ﬁxed η) with our data. So, we use instead a ν-SVM (using LIBSVM13)for
adaptive scheme.                                      comparison, where ν (unlike the C parameter in the C-SVM)
                                                        10Alternatively, one can ﬁnd the best-tuned SVM by performing
                                                      a grid search on the soft-margin parameter. However, as each grid
4.2  Accuracy and Speed                               point then requires a complete re-training of the SVM, the regular-
Next, we show that the proposed ensemble of partially trained ization path algorithm is usually more efﬁcient [Hastie et al., 2005].
hard-margin SVMs (Section 3.2) has comparable perfor-   11The inner QP in the regularization path algorithm is solved by
mance as the best-tuned soft-margin SVM (C-SVM). We use using the optimization package Mosek (http://www.mosek.com).
                                          2             12
the Gaussian kernel k(xi, xj)=exp(−xi − xj /β), with   This includes the time for computing the kernel matrix, which
                                                      can be expensive on large data sets. Moreover, time for computing
  9astro-particles    is      downloaded       from   the leave-one-out bound is not included in the timing of SVM(LOO).
http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/, while 13Version 2.8.1 (C++ implementation)  from
others are from http://research.microsoft.com/users/jplatt/smo.html. http://www.csie.ntu.edu.tw/ cjlin/libsvm/.

                                                IJCAI-07
                                                  1093