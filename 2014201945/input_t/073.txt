        When Discriminative Learning of Bayesian Network Parameters Is Easy 


                 Hannes Wettig*, Peter Griinwald0, Teemu Roos*, Petri Myllymaki*, and Henry Tirri* 
              * Complex Systems Computation Group (CoSCo) ° Centrum voor Wiskunde en Informatica (CWI) 
             Helsinki Institute for Information Technology (HUT) P.O. Box 94079 
          University of Helsinki & Helsinki University of Technology NL-1090 GB Amsterdam, The Netherlands. 
                   P.O. Box 9800, FIN-02015 HUT, Finland Peter.Grunwald®cwi.nl 
                       {Firstname}. {Lastname} @hiit.fi 


                        Abstract                               model, for which the likelihood surface is known to be con•
                                                               cave. However, in some cases the parameters of this logis•
     Bayesian network models are widely used for dis•          tic regression model are not allowed to vary freely. In other 
     criminative prediction tasks such as classification.      words, the Bayesian network model corresponds to a subset 
     Usually their parameters are determined using 'un•        of a logistic regression model rather than the full model. 
     supervised' methods such as maximization of the             Our main result (Thm. 3 below) provides a general condi•
     joint likelihood. The reason is often that it is un•      tion on the network structure under which, as we prove, the 
     clear how to find the parameters maximizing the           Bayesian network model is mapped to a full logistic regres•
     conditional (supervised) likelihood. We show how          sion model with freely varying parameters. Therefore, in the 
     the discriminative learning problem can be solved         new parametrization the conditional log-likelihood becomes 
     efficiently for a large class of Bayesian network         a concave function of the parameters that under our condi•
     models, including the Naive Bayes (NB) and tree-
                                                               tion are allowed to vary freely over the convex set Rk. Now 
     augmented Naive Bayes (TAN) models. We do this            we can find the global maximum in the conditional likelihood 
     by showing that under a certain general condition         surface by simple local optimization techniques such as hill 
     on the network structure, the discriminative learn•       climbing. 
     ing problem is exactly equivalent to logistic regres•
                                                                 The result still leaves open the possibility that there are no 
     sion with unconstrained convex parameter spaces. 
                                                               network structures for which the conditional likelihood sur•
     Hitherto this was known only for Naive Bayes mod•
                                                               face has local, non-global maxima. This would make our con•
     els. Since logistic regression models have a con•
                                                               dition superfluous. Our second result (Thm. 4 below) shows 
     cave log-likelihood surface, the global maximum 
                                                               that this is not the case: there are very simple network struc•
     can be easily found by local optimization methods. 
                                                               tures that do not satisfy our condition, and for which the con•
                                                               ditional likelihood can exhibit local, non-global maxima. 
1 Introduction                                                   Viewing Bayesian network (BN) models as subsets of lo•
                                                               gistic regression models is not new; it was done earlier in 
In recent years it has been recognized that for discriminative papers such as [Heckerman and Meek, 1997a; Ng and Jor•
prediction tasks such as classification, we should use a 'su•  dan, 2001; Greiner and Zhou, 2002]. Also, the concavity 
pervised* learning algorithm such as conditional likelihood    of the log-likelihood surface for logistic regression is known. 
maximization [Friedman et al., 1997; Ng and Jordan, 2001;      Our main contribution is to supply the condition under which 
Kontkanen et al., 2001; Greiner and Zhou, 2002]. Neverthe•     Bayesian network models correspond to logistic regression 
less, for Bayesian network models the parameters are cus•      with completely freely varying parameters. Only then can 
tomarily determined using ordinary methods such as maxi•       we guarantee that there are no local maxima in the likelihood 
mization of the joint (unsupervised) likelihood. One of the    surface. As a direct consequence of our result, we show for 
main reasons for this discrepancy is the difficulty in finding the first time that the supervised likelihood of, for instance, 
the global maximum of the conditional likelihood. In this pa•  the tree-augmented Naive Bayes (TAN) model has no local 
per, we show that this problem can be solved, as long as the   maxima. 
underlying Bayesian network meets a particular additional        This paper is organized as follows. In Section 2 we in•
condition, which is satisfied for many existing Bayesian-      troduce Bayesian networks and an alternative, so-called L-
network based models including Naive Bayes (NB), TAN           parametrization. In Section 3 we show that this allows us 
(tree-augmented NB) and 'diagnostic' models [Kontkanen et      to consider Bayesian network models as logistic regression 
al., 2001].                                                    models. Based on earlier results in logistic regression, we 
  We consider domains of discrete-valued random variables.     conclude that in the L-parametrization the supervised log-
We find the maximum conditional likelihood parameters by       likelihood is a concave function. In Section 4 we present 
logarithmic reparametrization. In this way, each conditional   our main results, giving conditions under which the two 
Bayesian network model is mapped to a logistic regression      parametrizations correspond to exactly the same conditional 


LEARNING                                                                                                             491 distributions. Conclusions are summarized in Section 5; 
 proofs of the main results are given in Appendix A. 

2 Bayesian Networks and the L-model 
We assume that the reader is familiar with the basics of the 
theory of Bayesian networks, see, e.g., iPearl, 988]. 
   Consider a random vector 
where each variable Xi takes values in {1,..., ni}. Let B be 
a Bayesian network structure over A", that factorizes P(X) 
into 
                                                       (1) 


where is the parent set of variable Xt 
inB. 
   We are interested in predicting some class variable Xm 
for some m conditioned on all 
Without loss of generality we may assume that m = 0 (i.e., 
Xo is the class variable) and that the children of A'0 in B 
are for some. For instance, in the 
so-called Naive Bayes model we have M — M' and the chil•
dren of the class variable Ao are independent given the value 
of XQ. The Bayesian network model corresponding to B is 
the set of all distributions satisfying the conditional indepen•
dencies encoded in B. It is usually parametrized by vectors 
    with components of the form defined by 

                                                       (2) 

where Pai is any configuration (set of values) for the par•
ents Whenever we want to emphasize that 
each pai is determined by the complete data vector x = 
              , we write pat(x) to denote the configuration 
of given by the vector x. For a given data vec•
tor , we sometimes need to consider 
a modified vector where X0 is replaced by x'0 and the other 
entries remain the same. We then write for the 
configuration of given by 

   We let MB be the set of conditional distributions 
                             corresponding to distributions 
                        satisfying the conditional indepen•
dencies encoded in B. The conditional distributions in 
can be written as 


                                                       (3) 

extended to TV outcomes by indenendence. 
  Given a complete data-matrix , the con•
ditional log-likelihood, with parameters is 
given by                                                      3 The L-model Viewed as Logistic Regression 
                                                       (4)    Although L-models are closely related to and in some cases 
                                                              formally identical to Bayesian network models, we can also 
where                                                         think of them as predictors that combine the information of 
                                                       (5)    the attributes using the so-called softmax rule [Heckerman 
                                                              and Meek, 1997b; Ng and Jordan, 2001]. In statistics, such 
  Note that in (3), and hence also in (4), all with models have been extensively studied under the name of lo•
       (standing for nodes that are neither the class variable gistic regression models, see, e.g. [McLachlan, 1992, p.2551. 


492                                                                                                          LEARNING  More precisely, let and let                                             future data. In addition, as discussed in [Heckerman and 
be real-valued random variables. The multiple logistic re•               Meek, 1997a], they can be used to perform model selec•
gression model with dependent variable XQ and covariates                 tion among several competing model structures using, e.g., 
 Y\,..., Yk is defined as the set of conditional distributions           the BIC or (approximate) MDL criteria. In [Heckerman and 
                                                                         Meek, 1997a] it is stated that for general conditional Bayesian 

                                                                 (7)     network models MB, "although it may be difficult to deter•
                                                                         mine a global maximum, gradient-based methods [...] can be 
                                                                         used to locate local maxima". Theorem 2 shows that if the 
 where the are allowed to take on all values in R This 
                                                                         network structure B is such that the two models are equiva•
 defines a conditional model parameterized in . Now, for                 lent, , we can find even the global maximum of 
i and pa                                        in the set of parent 
                                              x                          the conditional likelihood by reparametrizing MB to the L-
configurations of Xt, let                                                model, and using some local optimization method. Thus, the 
                                                                         question under which condition . becomes cru•
                                                                 (8)     cial. It is this question we address in the next section. 

The indicator random variables thus obtained can                         Remark. Because the log-transformation is continuous, it 
 be lexicographically ordered and renamed l,...,fc, which                follows (with some calculus) that, if . , then all 
 shows that each L-model corresponding to a Bayesian net•                maxima of the (concave) conditional likelihood in the L-
 work structure B as in (6) is formally identical to the logistic        parameterization are global (and connected) maxima also in 
 model (7) with dependent variable A'o and covariates given              the original parametrization. Nevertheless, the likelihood sur•
 by (8). So, for all network structures B, the corresponding L-          face as a function of has some unpleasant proper•
 model ML is the standard multiple logistic model, where the             ties (see [Wettig et al., 2002]): it is not concave in general 
 input variables for the logistic model are transformations of           and, worse, it can have 'wrinkles': by these we mean con•
 the input variables to the L-model, the transformation being            vex subsets such that, under the constraint that 
determined by the network structure B.                                                 the likelihood surface does exhibit local, non-
   It turns out that the conditional log-likelihood in the L-            global maxima. This suggests that it is computationally pre-
parametrization is a concave function of the parameters:                 ferrable to optimize over rather than . Empirical evi•
Theorem 2. The parameter set is convex, and the con•                     dence for this is reported in [Greiner and Zhou, 2002]. 
ditional log-likelihood is concave, though not 
strictly concave.                                                        4 Main Result 
                                                                         By setting , it follows that each distribu•
 Proof. The first part is obvious since each parameter can take 

values in Concavity ofis a direct con•                                   tion in MB is also in ML (Thm. 1). This suggests that, by 
 sequence of the fact that multiple logistic regression models           doing the reverse transformation 
 are exponential families; see, e.g., [McLachlan, 1992, p.2601.                                                                          (9) 
 For an example showing that the conditional log-likelihood is 
 not strictly concave, see [Wettig et at, 20021.                         we could also show that distributions in are also in 
                                                                              . However, contains distributions that violate the 
                                                                         'sum-up-to-one constraint', i.e., for some we have 
 Remark. Non-strictness of the proven concavity may pose 
                                                                                                    for some i and pa                      . 
a technical problem in optimization. This can be avoided by                                                                                i
assigning a strictly concave prior on the model parameters               Then the corresponding is not in . But, since the L-
and then maximizing the 'conditional posterior' [Grunwald                parameterization is redundant (many different index the 
et «/., 2002; Wettig et a/., 2002] instead of the likelihood.            same conditional distribution it may still 
One may also prune the model of weakly supported parame•                 be the case that the distribution indexed by 
ters and/or add constraints to arrive at a strictly concave con•         is in , -. . Indeed, it turns out that for some network struc•
ditional likelihood surface. Our experiments [Wettig et al,              tures B, the corresponding M L is such that each distribution 
2002] suggest that for small data samples this should be done            in , can be expressed by a parameter vector _ such that 
in any case, in order to avoid over-fitting; see also Section 5.                                                               and/Mi. In 
Any constraint added should of course leave the parameter 
                                                                         that case, by (9), we do have MB — ML. Our main result is 
space a convex set, e.g. a subspace of the full 0L.                      that this is the case if B satisfies the following condition: 
Corollary 1. There are no local, non-global, maxima in the 
likelihood surface of an L-model.                                        Condition 1. For all there exists 
   The conditions under which a global maximum exists are                     such that ~ ~ 
discussed in, e.g., [McLachlan, 1992] and references therein. 
A possible solution in cases where no maximum exists is to               Remark. Condition 1 implies that the class must be a 
introduce a strictly concave prior as discusssed above.                  'moral node', i.e., it cannot have a common child with a node 
   The global conditional maximum likelihood parameters                  it is not directly connected with. But Condition 1 demands 
obtained from training data can be used for prediction of                more than that; see Figures 1 and 2. 


LEARNING                                                                                                                                493                                                                now implies that, for example, the conditional likelihood sur•
                                                               face of TAN models has no local maxima. Therefore, a global 
                                                               maximum can be found by local optimization techniques. 
                                                                 But what about the case in which Condition 1 does not 
                                                               hold? Our second result, Theorem 4 (proven in Appendix A) 
 Figure 1: A simple Bayesian network (the class variable is    says that in this case, there can be local maxima: 
 denoted by Xo) satisfying Condition 1 (left); and a network   Theorem 4. be the network 
 that does not satisfy the condition (right).                  structure depicted in Figure J (right). There exist data sam•
                                                               ples such that the conditional likelihood has local, non-global 
                                                               maxima over, 
                                                                 The theorem implies that Thus, Condition 1 
                                                               is not superfluous. We may now ask whether our condition 
                                                               is necessary for having . that is, whether 

                                                               MB for all network structures that violate the condition. We 
 Figure 2: A tree-augmented Naive Bayes (TAN) model satis•     plan to address this intriguing open question in future work. 
 fying Condition 1 (left); and a network that is not TAN (right). 
Even though in both cases the class variable X0 is a moral     5 Concluding Remarks 
node, the network on the right does not satisfy Condition 1. 
                                                               We showed that one can effectively find the parameters max•
Example 1. Consider the Bayesian networks depicted in Fig•     imizing the conditional (supervised) likelihood of NB, TAN 
ure 1. The leftmost network, B  , satisfies Condition 1, the   and many other Bayesian network models. We did this by 
                                1                              showing that the network structure of these models satisfies 
rightmost network, B2, does not. Theorem shows that the 
conditional likelihood surface of can have local max•          our 'Condition 1', which ensures that the conditional distri•
                                                               butions corresponding to such models are equivalent to a par•
ima, implying that in this case 
                                                               ticular multiple logistic regression model with unconstrained 
   Examples of network structures that satisfy Condition 1 are parameters. An arbitrary network structure can always be 
the Naive Bayes (NB) and the tree-augmented Naive Bayes        made to satisfy Condition 1 by adding arcs. Thus, we can 
(TAN) models [Friedman et a/., 1997]. The latter is a gen•     embed any Bayesian network model in a larger model (with 
eralization of the former in which the children of the class   less independence assumptions) that satisfies Condition 1. 
variable are allowed to form tree-structures; see Figure 2.      Test runs for the Naive Bayes case in [Wettig et al, 2002] 
                                                               have shown that maximizing the conditional likelihood in 
Proposition 1. Condition J is satisfied by the Naive Bayes     contrast to the usual practice of maximizing the joint (unsu•
and the tree-augmented Naive Bayes structures.                 pervised) likelihood is feasible and yields greatly improved 
                                                               classification. Similar results are reported in [Greiner and 
Proof. For Naive Bayes, we have for all                        Zhou, 2002]. Our conclusions are also supported by theo•
             For TAN models, all children of the class vari•   retical analysis in [Ng and Jordan, 2001]. Only on very small 
able have either one or two parents. For children with only    data sets we sometimes see that joint likelihood optimization 
one parent (the class variable) we can use the same argument   outperforms conditional likelihood, the reason apparently be•
as in the NB case. For any child Xj with two parents, let Xi   ing that the conditional method is more inclined to over-
be the parent that is not the class variable. Because Xi is also fitting. We conjecture that in such cases, rather than resorting 
a child of the class variable, we have I to maximizing the joint instead of the conditional likelihood, 
                                                               it may be preferable to use a simpler model or simplify (i.e. 
   Condition 1 is also automatically satisfied if Xo only has  prune or restrict) the model at hand and still choose its param•
incoming arcs1 ('diagnostic' models, see [Kontkanen et ai,     eters in a discriminative fashion. In our setting, this would 
2001 ]). For Bayesian network structures B for which the con•  amount to model selection using the L-parametrization. This 
dition does not hold, we can always add some arcs to arrive    is a subject of our future research. 
at a structure B' for which the condition does hold (for in•
stance, add an arc from in the rightmost network               References 
in Figure 2). Therefore, Mb is always a subset of a larger 
model for which the condition holds. We are now ready          [Friedman et al, 1997] N. Friedman, D. Gcigcr, and M. Gold-
to present our main result (for proof see Appendix A):           szmidt. Bayesian network classifiers. Machine Learning, 
                                                                 29:131-163, 1997. 
Theorem 3. if B satisfies Condition 1, then 
                                                               [Greiner and Zhou, 2002] R. Greiner and W. Zhou. Structural ex•
  Together with Corollary 1, Theorem 3 shows that Condi•         tension to logistic regression: Discriminant parameter learning 
tion 1 suffices to ensure that the conditional likelihood sur•   of belief net classifiers. In Proceedings of the Eighteenth Annual 
face of has no local (non-global) maxima. Proposition 1          National Conference on Artificial Intelligence (AAA1-02), pages 
                                                                 167-173, Edmonton, 2002. 
   'it is easy to see that in that case the maximum conditional like• [Griinwald et al, 2002] P. Griinwald, P. Kontkanen, P. Myllymaki, 
lihood parameters may even be determined analytically.           T. Roos, H. Tirri, and H. Wettig. Supervised posterior distri-


494                                                                                                           LEARNING    butions, 2002. Presented at the Seventh Valencia International          We first show that, for all possible vectors x and the corre•
   Meeting on Baycsian Statistics, Tenerife, Spain.                      sponding parent configurations, no matter how the are 
 [Hcckcrman and Meek, 1997a] D. Heckcrman and C. Meek. Em•               chosen, it holds that 
   bedded bayesian network classifiers. Technical Report MSR-TR-
   97-06, Microsoft Research, 1997.                                                                                                    (12) 
[Heckcrman and Meek, 1997b) D. Hcckerman and C. Meek. Mod•
   els and selection criteria for regression and classification. In 
   D. Geiger and P. Shenoy, editors, Uncertainty in Arificial Intel­     To derive (12) we substitute all terms of by their 
   ligence 13, pages 223-228. Morgan Kaufmann Publishers, San            definition (11). Clearly, for all , there is ex•
   Mateo, CA, 1997. 
                                                                         actly one term of the form that appears in the sum with 
 [Kontkanen et al, 2001] P. Kontkanen, P. Myllymaki, and H. Tirri.       a positive sign. Since for each ■ there exists 
   Classifier learning with supervised marginal likelihood. In           exactly onewith mj = i, it must be the case 
   J. Breese and D. Koller, editors, Proceedings of the 17th In­
                                                                         that for all , a term of the form 
   ternational Conference on Uncertainty in Artificial Intelligence 
   (UAI'01). Morgan Kaufmann Publishers, 2001.                           appears exactly once in the sum with a negative sign. By (10) 
                                                                         we have . Therefore all terms that 
[McLachlan, 1992] G.J. McLachlan. Discriminant Analysis and              appear once with a positive sign also appear once with a neg•
   Statistical Pattern Recognition. John Wiley & Sons, New York,         ative sign. It follows that, except for all terms 
    1992. 
                                                                         cancel. This establishes (12). By plugging in (12) into (6), it 
[Ng and Jordan, 2001] A.Y. Ng and M.I. Jordan. On discriminative         follows that 
   vs. generative classifiers: A comparison of logistic regression and 
                                                                           Now set, for all x, and pai 
   naive Bayes. Advances in Neural Information Processing Sys­
   tems, 14:605-610,2001. 
[Pearl, 19881 J. Pearl. Probabilistic Reasoning in Intelligent Sys­
   tems: Networks of Plausible Inference. Morgan Kaufmann Pub•           We show that we can determine the constants such that 
   lishers, San Mateo, CA, 1988.                                         for all and pa,, the 'sum up to one' constraint 
                                                                         is satisfied, i.e., we have 
lWettig et al.,2002] H. Wettig, P. Grunwald, T. Roos, P. Myl-
   lymaki, and H. Tirri. On supervised learning of Bayesian net•
   work parameters. Technical Report HIIT-2002-1, Helsinki In•                                                                         (14) 
   stitute for Information Technology (HUT), 2002. Available at 
   http://cosco.hiit.fi/Articles/hiit2002- .ps. 
                                                                         We achieve this by sequentially determining values for 
A Proofs                                                                 in a particular order. 
                                                                           We need some additional terminology: we say 'ci, is deter­

Proof of Theorem 3. We introduce some more notation. For                 mined' if for all configurations pai of Pai we have already 

                      , let mj be the maximum number in                  determined t. We say V, is undetermined' if we have de•

{(),..., A/} such that                                                   termined for no configuration of . We say ci, is 

Such an mj exists by Condition 1. To see this, note that                 ready to be determined' if Ci is undetermined and at the same 

thementioned in Condition 1 must lie in the set                          time all with mj = i have been determined. 

                       (otherwise , so                                     We note that as long as some ct with {0,..., A/} arc 

                      , contradiction).                                  undetermined, there must exist ci that are ready to be de•
   Condition 1 implies that is completely determined by                  termined. To see this, first take any {0,..., A/} with 

the pairWe can therefore introduce functions                             ct undetermined. Either ci itself is ready to be determined 

Qj mapping) to the corresponding paj. Hence,                             (in which case we are done), or there exists {1,... M) 

for all andwe have                                                       with mj = i (and hence such that cj is undeter•
                                                                         mined. If C   is ready to be determined, we are done. Oth•
                                                               (10)                   j
                                                                         erwise we repeat the argument, move forward in B restricted 
   We introduce, for all and for each con•                               to and (because B is acyclic) within M steps 
figuration , a constant and define, for any                              surely rind a C. that is ready to be determined. 
                                                                           We now describe an algorithm that sequentially assigns 
                                                                         values to such that (14) is satisfied. We start with all c, 
                                                               (ID       undetermined and repeat the following steps: 
                                                                         WHILE there exists , that is undetermined 
                                                                         DO 
The parameters constructed this way are combined to 
                                                                           1. Pick the largest i such that ci is ready to be determined. 
a vector which is clearly a member of 
                                                                           2. Set, for all configurations of such that 
   Having introduced this notation, we now show that no mat•

ter how we choose the constants Ci|pa,, for all 0L and corre•
sponding we have                                                         DONE 


LEARNING                                                                                                                                495 