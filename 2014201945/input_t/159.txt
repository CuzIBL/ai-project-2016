   Approximating Optimal Policies for Agents with Limited Execution Resources 

                                   Dmitri A. Dolgov and Edmund H. Durfee 
                          Department of Electrical Engineering and Computer Science 
                                                University of Michigan 
                                                Ann Arbor, MI 48109 
                                           {ddolgov, durfee}@umich.edu 


                        Abstract                               where the current resource amounts are unobservable and 
                                                               cannot be easily estimated by the agent, or where modeling 
     An agent with limited consumable execution re•
                                                               resource amounts in the state description is computationally 
     sources needs policies that attempt to achieve good 
                                                               infeasible. In an aircraft scenario, some resources and situa•
     performance while respecting these limitations. 
                                                               tions where such methods arc beneficial could include an air•
     Otherwise, an agent (such as a plane) might fail 
                                                               plane with a broken fuel gauge (fuel amount is unobservable), 
     catastrophically (crash) when it runs out of re•
                                                               pilot fatigue (attention is a resource that cannot be easily es•
     sources (fuel) at the wrong time (in midair). We 
                                                               timated), or a combination of non-critical resources (ex. var•
     present a new approach to constructing policies for 
                                                               ious refreshments) that should nevertheless not be exhausted, 
     agents with limited execution resources that builds 
                                                               but explicit modeling of which unnecessarily complicates the 
     on principles of real-time Al, as well as research 
                                                               optimization problem and increases policy size. In the rest of 
     in constrained Markov decision processes. Specif•
                                                               the paper, we will use the fuel example, simply because it is 
     ically, we formulate, solve, and analyze the pol•
                                                               a very intuitive instance of a consumable resource. 
     icy optimization problem where constraints are im•
                                                                 However, as we will explain later, standard risk-neutral 
     posed on the probability of exceeding the resource 
                                                               CMDP optimization techniques are not applicable to prob•
     limits. We describe and empirically evaluate our 
                                                               lems where violating the constraints can have negative or, in 
     solution technique to show that it is computation•
                                                               the limit, catastrophic  consequences. The main contribution 
     ally reasonable, and that it generates policies that                           1
                                                               of this work is that it extends the standard CMDP techniques 
     sacrifice some potential reward in order to make the 
                                                               to handle the types of hard constraints that naturally arise in 
     kinds of precise guarantees about the probability of 
                                                               problems involving critical resources. In particular, we for•
     resource overutilization that are crucial for mission-
                                                               mulate an optimization problem where constraints are im•
     critical applications. 
                                                               posed on improbability of resource overutilization, and show 
                                                               how the problem can be solved using standard linear pro•
1 Introduction                                                 gramming algorithms. Our formulation yields sub-optimal 
                                                               solutions to the constrained problem because it sacrifices po•
Optimality is the gold standard in rational decision making 
                                                               tential reward to make guarantees about the probability of 
(e.g., [Russell and Subramanian, 1995]), and, consequently, 
                                                               violating the resource constraints. As we later show, when 
the problem of constructing optimal policies for autonomous 
                                                               violating constraints incurs dire costs, these guarantees are 
agents has received considerable attention over the years. 
                                                               worthwhile ([Musliner et al, 1995] and references therein). 
Traditionally, this problem has been viewed separately from 
                                                                 We introduce our model in section 2, where we review 
the problem of actually carrying out the policies. However, 
                                                               Markov models, introduce notation, and specify our assump•
real agents have limitations as to what they can execute, and, 
                                                               tions about the problem domain. Section 3 describes and 
clearly, a policy is less useful if an agent might run out of 
                                                               compares several candidate solutions for addressing aspects 
resources while carrying out the policy. 
                                                               of our problem. We then present in section 4 our new method, 
  In this paper, we present a new approach for construct•
                                                               and empirically evaluate it in section 5. We conclude with a 
ing policies for agents that have limited consumable re•
                                                               discussion about the strengths and limitations of our results, 
sources where running out of the resources can have neg•
                                                               and about our future directions. 
ative consequences. Whereas AI research has mostly fo•
cused on (PO)MDP [Boutilier et ai, 1999; Dean et ai,1993; 
Howard, 1960; Puterman, 1995] methods for formulating          2 The Model 
policies for agents without emphasizing constraints on their   The stochastic properties of the environments in the prob•
execution resources, the Operations Research literature has    lems that we are addressing lead us to formulate our op-
developed constrained MDP (CMDP) [Altaian, 1999; Put•

erman, 1995] techniques that can account for resource con•        !The term catastrophic is, of course, relative. We assume that the 
straints. CMDP methods are particularly useful for domains     system designer is willing to accept certain risks to receive payoff. 


RESOURCE-BOUNDED REASONING                                                                                          1107  timization problem as a stationary, discrete-time, Markov     justifiable). On the other hand, the problem does not natu•
 decision process. In this section, we review some well-       rally fit the definition of the infinite-horizon model, because 
 known facts from the theory of standard [Puterman, 1995;      the plane obviously cannot keep on flying forever. 
 Boutilier et a/., 1999] and constrained [Altman, 1999] fully-   This leads us to make a slightly different and less common 
 observable Markov decision processes and also discuss the     (although, certainly, not novel) assumption about how much 
 assumptions that are particular to the class of problems that time the agent spends executing its policy. We assume that 
 we address in this work. This section provides the neces•     there is no predefined number of steps that the agent spends 
 sary background for the subsequent sections, where we dis•    in the system, but that optimal policies always yield transient 
 cuss resource-constrained optimization problems.              Markov processes (decision problems of this type were exten•
                                                               sively studied by Kallenberg f 1983]). A policy is said to yield 
 2.1 Markov Decision Processes                                 a transient Markov process if the agent executing that policy 
 A standard Markov decision process can be defined as a tuple  will eventually leave the corresponding Markov chain, after 
              where S is a finite set of states that the agent spending a finite number of time steps in it. Given a finite 
can be in, is a finite set of actions that the agent can execute, state space, this assumption implies that there has to be some 
                        R defines the transition function      "leakage" of probability out of the system, i.e. there have 
 is the probability that the agent will go to state if it executes to exist some state-action pairs for which 
action a in state i), and is the reward                        One particular case where the above assumption holds is in 
function (agent receives a reward of ria for executing action  a system in which all trajectories lead to absorbing states. 
  in state                                                     Once an agent enters an absorbing state, it has finished (or 
   Clearly, the total probability of transitioning out of a    failed to finish) some task and has nowhere else to go, i.e. the 
state, given a particular action, cannot be greater than 1, i.e. probability of transitioning out of an absorbing state i is zero: 
             As we discuss below, we are actually interested                    In the plane-flying example, all trajectories 
in domains where there exist states for which                  lead to either a safe landing or a crash, and once the agent 
   A policy is defined as a procedure for selecting an ac•     enters one of these states, the probability of transitioning to 
tion in each state. A policy that makes its choices accord•    other states is zero. 
ing to a probability distribution over the set of actions is     The transient nature of our problems leads us to adopt the 
called randomized and can be described as a mapping of         expected total reward as the policy evaluation criterion. Given 
states to probability distributions over actions:              that an agent receives a reward whenever it executes an action, 
                A deterministic policy that always chooses     the total expected utility of a policy can be expressed as: 
the same action for a state is, of course, a special case 
of a randomized policy. It can be seen (similarly to the 
                                                                                                                      (2) 
case of standard CMDPs, as described in [Kallenberg, 1983; 
Puterman, 1995]) that under our optimization criterion and 
constraints (section 4), deterministic policies can be subop-  where T is the number of steps during which the agent accu•
timal. Therefore, in this work, we focus on approximating      mulates utility. For a transient system with bounded rewards, 
optimal policies in the class of randomized ones.              the above sum converges for any T. 
   If, at time 0, the agent has an initial probability distribution 
          over the state space, a Markov system follows the    3 Related Work 
following trajectory: 
                                                               In this section, we briefly survey several approaches (based 
                                                               on well-known techniques) to solving the problem of finding 
                                                        (1) 
                                                               optimal policies for agents with limited resources, and point 
                                                               out the assumptions, strengths, and limitations of these meth•
where is the probability distribution at time                  ods. This establishes a landscape of solution algorithms, to 
                                                               which we can compare our method (presented in the next sec•
2.2 Assumptions 
                                                               tion) in terms of complexity, efficiency, and solution quality. 
Typically, Markov decision processes are divided into two 
categories: finite-horizon problems, where the total number    3.1 Fully Observable MDPs 
of steps that the agent spends in the system is finite and 
                                                               The most straightforward way of handling resource con•
is known a priori, and infinite-horizon problems, where the 
                                                               straints in a MDP framework is to explicitly model the re•
agent is assumed to stay in the system forever (see [Puterman, 
                                                               sources by making their current amounts a part of the state 
1995] for a detailed discussion of both types of models). 
                                                               description. This yields a standard FOMDP, which can be 
   In this work we concentrate on dynamic real-time domains, 
                                                               solved by a wide variety of efficient methods. The benefit 
where agents have tasks to accomplish. For example, con•
sider an agent flying a plane, whose goal is to safely get to its An alternative way of handling these states is to treat them as 
destination and land there. This example does not naturally    infinitely-recurrent, i.e. once the agent gets there, it infinitely tran•
correspond to a finite-horizon problem, because the duration   sitions back to itself. We do not adopt this model, because it is less 
of executing various policies is not predetermined (unless we  natural for our domains and also leads to unnecessary complications 
artificially impose such a finite duration, which is not easily in the optimization problems. 


1108                                                                                RESOURCE-BOUNDED REASONING  of this approach is that it allows one to make use of all rele• munication domain [1988]) is that a standard CMDP imposes 
 vant information to construct the best possible policy, as the constraints on the expected amounts. Clearly, this method 
 agent is able to base its action choice on the current state and does not work for critical resources, whose overutilization has 
 resource amounts. The downside of this approach is that it    negative consequences. Indeed, an agent that pilots aircraft 
 requires an a priori discretization of resource amounts to be would not be satisfied with a policy that on average uses an 
 made when the world model is constructed. Also, there is an   amount of fuel that does not exceed the tank capacity. 
 additional burden of specifying the rewards and state transi•
 tions as functions of current resource amounts. Furthermore,  3.3 Sample Path MDPs 
 in this model, the size of the state space and, consequently, As just mentioned, Ross and Chen pointed out the weak•
the policy size, explodes exponentially in the number of re•   ness of the CMDP approach with constraints on the expected 
 sources, as compared to the state space where the resource    amounts. As a possible solution, Ross and Varadarajan [1989; 
 amounts are not folded into the state description.            1991] propose an approach where constraints are placed on 
   The FOMDP approach relies on the fact that the agent can    the actual sample-path costs. In their work, the space of feasi•
 observe the exact amounts of all resources at runtime. How•   ble solutions is constrained to the set of policies whose proba•
 ever, this may not hold, especially in multiagent domains with bility of violating the constraints (overutilizing the resources) 
 shared resources, when an agent does not know what other      in the long run is zero. However, their work concentrates on 
 agents have been doing and how much of the shared resources   the average per-step costs and rewards, whereas we are inter•
 they have been consuming.                                     ested in the total amounts, whose distributions are not easily 
                                                               calculable. The approach of Ross and Varadarajan has the 
3.2 Constrained MDPs                                           same benefits as the standard CMDP method, i.e. no explicit 
An alternative to the FOMDP approach described above is to     modeling of resources is required, and the state space and 
 treat the problem as a constrained Markov decision process    policies are small. In addition, unlike the standard CMDP, 
 [Altman, 1999], where the resources are not explicitly mod•   this method is suitable for problems with critical resources. 
 eled, but rather are treated as constraints that are imposed on However, a weakness of this approach is that for some prob•
the space of feasible solutions (policies).                    lems it might be too restrictive, in that it allows no possibility 
   A constrained MDP for a resource-limited agent differs      of overutilizing the resources. Indeed, policies produced by 
 from a standard MDP in that the agent, besides getting re•    the sample path method might have significantly lower pay•
wards for executing actions, also incurs resource costs. Con•  off, as compared to policies that have some probability of 
sequently, a constrained MDP (CMDP) can be described as a      resource overutilization. Furthermore, for some domains it 
tuple (S, A, P, R, C, Q), where C = _                          might be desirable for the system designer to be able to con•
defines a vector of actions' costs units of resource           trol the probability of resource overutilization, as a means of 
            are used when action is executed in state i),      balancing optimality and risk. 
and Q is the vector of amounts of available 
resources (there is units of resource initially available).    3.4 MDPs With Constraints on Variance 
   The benefit of the CMDP is that it does not require one     Another approach to handling deviations from the expecta•
to explicitly model how the resources affect the world. In•    tion in Markov models is to impose additional constraints on 
deed, if all policies satisfy the resource constraints, one does (or to assign additional cost to) the variance. Sobel [1985] 
not have to worry about what happens when the resources        proposed to constrain the expected cost and to maximize the 
are overutilized. Consequently, the state space and the re•    mean-variance ratio of the reward. Huang and Kallenberg 
sulting policies are exponentially smaller than the ones in the [1994] proposed a unified approach to handling variance via 
FOMDP model. The standard CMDP formulation constrains          an algorithm based on parametric linear programming. These 
the expected usage of all resources to be below a certain limit approaches have the same benefits as the standard CMDP and 
and can be formulated as the following linear program:         the sample-path methods (compared to the FOMDP formula•
                                                               tion) in terms of state space and policy size, as well as the 
                                                        (3)    complexity of constructing the initial world model. Addi•
                                                               tionally, they allow one to somewhat balance payoff and the 
where the variables have the interpretation of the expected    deviation from the expected. However, these methods do not 
number of times action a is executed in state i.               allow one to make hard guarantees about the probability of 
   A weakness of this approach is that it yields policies that overutilizing the resources. 
can be suboptimal, as compared to the ones constructed by 
the FOMDP method, because the agent does not base its de•      4 Linear Approximation 
cision on the current resource amounts, but rather completely  As hinted at in the previous sections, we would like to be able 
ignores that aspect of the state. However, as mentioned in the to constrain the feasible solution space to the set of policies 
previous section, in domains where the resources are not ob•   whose probability of overutilizing the resources is below a 
servable, or if the policy size is of vital importance (for exam• user-specified threshold In other words, we 
ple if the agent's architecture imposes constraints on policies would like to be able to solve the following math program: 
that it can store), this approach could prove fruitful. 
   However, the biggest problem with this approach (as                                                                (4) 
pointed out, for example, by Ross and Chen in the telecom-


RESOURCE-BOUNDED REASONING                                                                                          1109  where is the total amount of resource that is used by the       To reduce the bias that might arise from using a small num•
 policy, and is the upper bound on that resource (as before).  ber of hand-crafted test cases, we have instead used a large 
   The trouble is that the optimization is in the space of     number of randomly-generated constrained MDPs. All of the 
 which can be interpreted as the expected number of times for  generated problems shared some common properties, among 
 executing the actions in the corresponding states. However,   which the most interesting ones are the following (the values 
 it is difficult to express as a simple function of            for our main experiments are given in parentheses): 
 the optimization variables because the latter contain no                   The total number of states, actions, and re•
 information about the probability distribution of the random       sources, respectively. (20, 20, 2) 
 variable of the number of times the action is actually executed 
                                                                                Maximum reward. Rewards are assigned 
 in the corresponding state - only the expected values.3 In 
 this section, we present a linear approximation to the above       from a uniform distribution on (10) 
program, based on the Markov inequality:4                     Mc = max(C) Maximum action cost. Resource costs are 
                                                                   assigned to state-action pairs from based on a 
                                                        (5)        distribution of R and (C, R) (described below). (10) 
 Using this inequality and the fact that the expected resource                 Correlation between rewards and resource 
usage can be expressed as the opti•                                costs; better actions are typically more costly. 
mization problem can be formulated as a linear program: 
                                                                                            Upper bounds on resource 
                                                                   amounts are assigned according to a uniform distribu•
                                                                   tion from this range. ([200, 300]) 
                                                                   Dissipation of probability - the probability that the agent 
                                                                   exits the system at each time step. ([0.95,0.99]) 
   A potential weakness of this approach is that the Markov 
inequality gives a very rough upper bound, and the policies    The last parameter was used to ensure a transient chain. In•
that correspond to solutions to this LP can be too conserva•   stead of providing a small number of sink states, we have 
tive, in that their real probability of overutilizing the resources chosen to use a uniform dissipation of probability, in order 
can be significantly lower than However, if making hard        to avoid additional randomization in our experiments, as the 
guarantees about the probability of overutilizing the resources latter choice provides a more stable domain. 
is of vital importance, this method might prove valuable, as it  Our main concern was the behavior of our approxima•
yields policies that, in general, would have higher payoff than tion, as a function of the probability threshold Therefore, 
the ones obtained by the sample-path method (as the latter is  we have run a number of experiments for various values of 
often too restrictive). On the other hand, unlike the standard            To be more precise, we have gradually increased 
CMDPs that impose constraints on the expected resource us•       from 0 to 1 in increments of 0.05, and for each value, gen•
age, and the MDPs that constrain the variance, this method     erated 50 random models and solved them using the three 
allows one to explicitly bound the allowable probability of re• methods: 1) an unconstrained MDP without any regard for 
source overutilization, and to make precise guarantees about   resource limitations, 2) a standard CMDP with constraints on 
the behavior of the system in that respect.                   the expected usage of resources, and 3) our CMDP with con•
   It is also worth noting that, unlike the sample-path method straints on the probability of overutilizing the resources (eq. 
or the methods that constrain on variance, this method re•     6). We then evaluated (using a Monte-Carlo simulation) each 
lies on solving a standard linear program, whereas the for•    of the three solutions (policies) in terms of expected reward 
mer require solving quadratic or parametric linear programs.   and probability of overutilizing the resources. 
Therefore, the above formulation appears to be a reasonable      Figure 1 shows a plot of the actual probability of overuti•
approximation, because it should be no harder to solve than    lizing the resources for the policies obtained via the three 
the standard CMDP (see section 5 for experimental results),   methods as a function of the probability threshold . The 
while providing a means of balancing solution quality with    data points are averaged over the runs for a particular value 
the precisely quantifiable risk of resource overutilization.  of po. The curve that corresponds to the method that bounds 
                                                              the overutilization probability also shows the standard devia•
5 Evaluation                                                  tion for the runs. The other data have very similar variance, 
                                                              so we will use the plots of means (averaged over the runs for 
To verify our hypotheses about the properties of the approxi• a given po) for our analyses. 
mation described in the previous section, we have performed      Obviously, po has no effect on the unconstrained and the 
a set of numerical experiments that compare its behavior to   standard CMDP (which maintain more or less a constant 
a standard CMDP with constraints on the expected resource     probability of overutilization), but it does affect to a large ex•
amounts (section 3), and to an unconstrained MDP.             tent the solutions to the problem with constraints on overuti•
                                                              lization probability. One can see that the overutilization prob•
   3Generally speaking, the total cost is a sum of a random num•
ber of differently-distributed random variables, and calculating its ability for the solutions produced by our approach is always 
probability distribution is a nontrivial task.                below (as it should be). Also, it is worth noting that when 

   4This inequality only holds for nonnegative random variables. approaches 1, our approximation yields the same results as 
However, for a transient system, we can make the assumption that the other methods, which is good, since setting should 
all costs are nonnegative, without any loss of generality.    not constrain the space of feasible solutions. 


1110                                                                               RESOURCE-BOUNDED REASONING        Figure 1: Probability of resource overutilization.                Figure 3: Average rewards with penalties for overutilization. 

                                                                         Moreover, for large penalties (IV = -220), the conservative 
                                                                         policy outperforms the others for all values of; . Note that 
                                                                         here, the policy is re-evaluated post-factum. Section 6 briefly 
                                                                         discusses an approach that explicitly models the penalties in 
                                                                         the optimization program. 
                                                                            As we mentioned in section 4, the linear approximation 
                                                                         should be no harder to solve than the standard constrained 
                                                                         MDP, because both are formulated as linear programs with 
                                                                         the same number of constraints. To experimentally verify 
                                                                         this, we have timed the runs of all our experiments. Figure 
                                                                         4 contains a plot of the times that it took to solve the prob•
                                                                         lems in all our experiments. One can see that the running 

                                                                         times for all three methods are not appreciably different.5 In 
                                                                         particular, the average ratio of the running time for the stan•
Figure 2: Average rewards for solutions to the three problems. 
                                                                         dard CMDP to the running time of the unconstrained method 
                                                                         is 1.25; the ratio of the running time of our linear approxi•
                                                                         mation with constraints on overutilization probability to the 
   The rewards obtained by these policies are shown in Fig•
                                                                         running time of the unconstrained method is 1.06. The slight 
ure 2. These actual rewards do not necessarily equal the ex•
                                                                         downward curvature of the plot of the running time of our 
pected rewards (which are used during the optimization pro•
                                                                         approximation method appears to be a consequence of the 
cess). This is because only the runs that did not overutilize 
                                                                         specific implementation of the linear programming algorithm 
the resources were included in the average, and policies were 
                                                                         that we used in our experiments. 
not penalized for violating the resource constraints. This also 
explains why the total rewards received by solutions to the 
standard CMDPs were sometimes greater than the ones ob•                  6 Discussion and Future Work 
tained by solutions to the unconstrained problems. 
                                                                         Our experiments substantiate the claims that our approxima•
   However, realistically, an agent always incurs a penalty for          tion provides an effective and efficient method that agents 
overutilizing a critical resource, where the penalty amount is           can use to formulate policies that not only consider limita•
based on the consequences of overutilization. For example,               tions on execution resources, but that also explicitly bound 
if the agent is flying a plane, and the resource in question is          the probability of resource overutilization. Our new approxi•
fuel, the consequences of trying to use too much of that re•             mation achieves the constraints on overutilization, and is es•
source are catastrophic, so the penalty is very high. If we              sentially no more expensive to use than more standard CMDP 
take this into account by assigning a fixed penalty to poli•             and unconstrained MDP methods. Because our method con•
cies that overutilize the resources, we can update the graph in          structs policies that are more careful about avoiding resource 
Figure 2 to get a more realistic picture. Figure 3 shows the             overutilization, the rewards associated with its policies when 
average rewards for solutions obtained via the three meth•               resources are not overutilized tend to be less than the re•
ods, recalculated to reflect the penalties: new rewards are              wards for the other methods' policies when resources are not 
                                where is the overutilization             overutilized. However, as we illustrated in Figure 3, when 
probability, R is the average reward for successful runs of the          overutilization incurs penalties our new method can outper-
given policy (as in Figure 2), and W is the penalty for overuti•

lization. We see that, if we take the penalty into account, there           5Here we have to note that we used the linear programming ap•
is an interval of po where the conservative policy produced              proach to solving the unconstrained problem. A different algorithm 
by our linear approximation outperforms the other policies.              such as value or policy iteration might be more efficient. 


RESOURCE-BOUNDED REASONING                                                                                                             1111 