     Constructing Career Histories: A Case Study in Disentangling the Threads

                                             Paul R. Cohen
                                   USC Information Sciences Institute
                                       Marina del Rey, California


                    Abstract                          a new thread). To avoid false advertising, I must say immedi-
                                                      ately that the algorithm I present in this paper produces trees
    We present an algorithm for organizing partially- in which every publication has only one parent (e.g., it can-
    ordered observations into multiple “threads,” some not produce nodes like the one labeled a, b). An algorithm for
    of which may be concurrent., The algorithm is ap- generating graphs like the one in Figure 1 is under develop-
    plied to the problem of constructing career histories ment.
    for individual scientists from the abstracts of pub-
    lished papers. Because abstracts generally do not
    provide rich information about the contents of pa-
    pers, we developed a novel relational method for
    judging the similarity of papers. We report four               a       b
    experiments that demonstrate the advantage of this
                                                                   a
    method over the traditional Dice and Tanimoto co-                              c
    efﬁcients, and that evaluate the quality of induced                    b
    multi-thread career histories.                                                         d
                                                                       a,b         c
1  Introduction
                                                                                   c
Like most researchers, I work on several problems at once, so          a,b
my publications represent several research “threads.” Some
threads produce several papers, others lie dormant for months
or years, and some produce infrequent, irregular publications. Figure 1: A publication tree. Each node represents a publi-
Some papers clearly represent new lines of research (for me) cation. Threads are identiﬁed by letters. The depth of a node
while others are continuations. Although my research has represents its publication date.
changed over the years it is difﬁcult to identify when these
changes occurred. Yet, in retrospect, it is usually clear to me Trees or graphs of publications are not models of the pro-
that one paper is a direct descendent of another, while a third cesses that generate publications, but nor are they entirely
is unrelated to the other two.                        descriptive. Inference is required to generate them, in par-
  How should we model the processes that produce research ticular, research threads and parent-child relationships must
and publications? A single researcher can be viewed as a bun- be inferred. The inference problem is as follows: Given an
dle of non-independent processes, each of which produces ordered or partially-ordered sequence of observations, iden-
one kind of research. For example, some of my papers are tify the processes that generated each observation, and assign
about modeling cognitive development on robots and some each observation to at least one process. Some processes may
are about ﬁnding structure in time series, and although these not be active at the beginning of the sequence. I call this the
threads are distinct, they are not independent. Occasionally problem of disentangling the threads.
two or more threads go through one paper, or, to say it dif-
ferently, one paper can descend from two or more parents.
Within a thread, papers may be ordered roughly by their pub- 2 The PUBTREE Algorithm
lication dates, although some appear months or years after
they were written. Threads sometimes appear with no appar- The PUBTREE algorithm greedily adds papers to a growing
ent connection to previous work.                      tree of papers:
  My publications can be arranged in a graph like the one
                                                        1. Arrange the papers in chronological order, p1,p2, ...pn,
in Figure 1. Later publications are represented as children least to most recent.
of earlier ones or, when they are unrelated to earlier publica-
tions, as children of the root (e.g., the node labeled d begins 2. Create a root node r. Make p1 a child of r.

                                                IJCAI-07
                                                  2713 3. For each remaining paper pi, ﬁnd the most similar paper entirely on shared words. If α =0then S(A, B) is the re-
    in the tree and make pi its child. If pi is not similar lational similarity measure, R, and depends entirely on un-
    enough to any paper in the tree, make it the child of r. shared words.
  The last step is managed with the help of a threshold. For To illustrate the value of the relational similarity measure,
                                                      R, consider two abstracts of papers about the same piece of
two papers, A and B,ifS(A, B) >Tthread, treat them as
similar enough for B to be A’s child, otherwise make B the research, a system for automating exploratory data analysis.
child of r.                                           After non-content words are removed, the ﬁrst abstract is rep-
                                                      resented by: automate, capture, complexities, contribute, ex-
2.1  Relational Similarity                            ploratory, EDA, IGOR, knowledge-based, manage, Phoenix,
                                                      relies and script-based. The second abstract, by the same au-
Clearly, the PUBTREE algorithm relies on a good similarity
metric. This section describes a novel approach to judging thors, represents another paper about a different aspect of the
the similarity of the abstracts of papers. Two abstracts, A and system, with these words: Aide, assistant, ARPAROME, be-
                                                      gins, captured, contracts, data-driven, descriptive, extracted,
B, contain sets of words WA and WB, of sizes NA = WA
                                                      examined, enormous, explored, EDA, exploratory, informal,
and NB = WB  respectively. For example, we might have
                                                      presentation, plays, prelude, researcher, subtle, understand-
WA  = (tree search optimize constraint restart) and WB =
                                                      ing, word. The Dice coefﬁcient for these abstracts is just
(graph search vertex constraint). Note that the abstracts have D =0.11
two common or shared words, search and constraint, and ﬁve     because, although the abstracts are from very sim-
                                                      ilar papers, they share only the words. exploratory and EDA.
words that are not common, or unshared. Shared and un-                       R  =0.49
shared words each contribute a component to our measure of The relational coefﬁcient is , a much more satisfac-
similarity of abstracts.                              tory representation of the similarity of these papers.
  A prevalent way to compare abstracts is to count the num-
ber of shared words and divide by the total number of words 3 Related work
in both abstracts:
                                                      PUBTREE  is neither a clustering algorithm nor a sequence
                       2 ·W   ∩ W                   analysis algorithm but has some attributes of both. Like clus-
             D(A, B)=        A     B
                                                (1)   tering, PUBTREE forms groups of objects, that is, it groups pa-
                          NA + NB
                                                      pers together into threads. Yet clusters are generally unstruc-
This is called the Dice coefﬁcient. Its value will be one when
W   = W                                               tured, or hierarchically structured, while PUBTREE’s threads
  A     B and zero when the abstracts share no words. Sev- are organized sequentially by publication dates. Another dif-
eral other coefﬁcients have been proposed (e.g., Jaccard, Co- ference is that, generally, an item may be added to a cluster if
         [                        ]
sine, see Grossman and Frieder, 1998 ). The differences it is similar to all items in the cluster (usually via comparison
between them were slight in the experiments that follow. to the cluster centroid) whereas an item may be added to a
  For unshared words, we deﬁne a simple relational measure thread if it is similar to the last item on the thread.
of similarity. Let occ(wi,wj) be the number of abstracts in
                                                        Sometimes, adding an item to a thread causes the thread to
the corpus in which words wi and wj co-occur. Two words
                                                      split into two or more threads (i.e., threads are organized into
are said to be related if they co-occur in a super-threshold
                                                      trees). A thread A, B, C... splits under node B when a paper
number of abstracts:                                  D                          B       C
                                                       is considered more similar to than to . Recall, a thread
                                                      is supposed to represent a process that produces research pa-
                      1   occ(w ,w ) >T
       rel(w ,w )=             i  j    occ            pers, and a researcher is thought of as a bundle of such pro-
            i  j      0   Otherwise             (2)
                                                      cesses, so splitting a thread is analogous to changepoint anal-
                                                      ysis (e.g., [Lai, 1995]), or ﬁnding a point in a sequence where
For two abstracts, A and B, with words WA and WB, a relat-
edness coefﬁcient is                                  a process changes. One can imagine a kind of PUBTREE al-
                                                      gorithm that repeatedly samples sequences from the author’s
                                                    list of papers, builds Markov chain models of some kind, and
                                   rel(wi,wj)
               wi∈WA   wj ∈WB ,wj =wi                identiﬁes threads as sequences that do not feature change-
  R(A, B)=                                      (3)
                  (NANB)  −WA   ∪ WB                points. The challenge, of course, is to model the production
                                                      of papers as a stochastic process. We do not know of any
  R                      rel
    is the average value of  for all pairs of unshared way to do this, which is why we built the similarity-based
words. Its value will be one when every pair of unshared PUBTREE algorithm. It would be nice to model the author as
words is related, that is, occurs in a super-threshold number of a Hidden Markov Model (HMM), where the hidden state is
                R
abstracts. Note that does not require two abstracts to share the thread identiﬁer, but, again, we do not have a stochastic
                                       W   ∩ W
any words. That is, the measure works even if A B is  framework for the production of research papers. Nor do we
empty.                                                know a priori how many threads (i.e., hidden states) an author
        D     R
  Putting  and  together we get:                      will produce, as ﬁtting HMMs requires.
       S(A, B)=αD(A, B)+(1−      α)R(A, B)              Genter and her colleagues introduced the term relational
                                                (4)   similarity to emphasize that situations may be similar by
This measure, a weighted sum of similarity measures on merit of the relationships that hold among their elements,
shared and unshared words, also will range from zero to one. even though the elements themselves are different [Gentner
If α =1then  S(A, B) is the Dice coefﬁcient and depends and Forbus, 1995; Gentner and Markman, 1997; Goldstone et

                                                IJCAI-07
                                                  2714al., 1997]. This notion of similarity requires situations to be false positives. Figure 2 shows curves for three levels of α,
structured; there are no relationships between elements in a that is, for three degrees of mixing of the Dice and relational
bag. However, as this paper shows, elements wi and wj from similarity measurements. The curve labeled R is for rela-
unstructured bags of words WA and WB, respectively, can tional similarity alone (α =0) and the curve labeled D is for
be related by their appearances in other bags WC ,WD, .... the Dice coefﬁcient alone (α =1). Between them lies a curve
Clearly, this is a weak kind of relational similarity, but the for α = .5.
term seems apt, nonetheless.
  Our kind of relational similarity is a bit like query expan-
sion in information retrieval. The idea of query expansion is
                                                                                           R
that words in queries are rarely sufﬁcient to ﬁnd the right doc- 1
uments, so the query is “expanded” or augmented with addi-                                Sim
tional words. Local query expansion [Xu and Croft, 1996]    0.75
uses the query to retrieve some documents and the words                                    D
in those documents to expand the query. So, in query ex-     0.5
pansion, wi ∈ WA causes WC ,WD, ... to be retrieved, and
wj ∈  WC ,wk  ∈ WD, ... expand the query, which might      Hit  Rate
                                                            0.25
cause WB to be retrieved. Whereas, in our technique WA
and WB are judged to be similar if wi ∈ WA and wj ∈ WB
and wi,wj ∈ WC  ; wi,wj ∈ WD, .... Clearly these aren’t        0
identical inferences, but they are related.

                                                                     0     .25     .5    .75     1
4  Experiments                                                             False Positive Rate
The experiments reported here were performed on a corpus
of 11672 papers from the Citeseer database. The papers were Figure 2: ROC curve for classifying whether two papers are
selected by starting with my own Citeseer entries, adding all written by one author or two, based on the decision criterion
those of my coauthors, and all those of their coauthors. Each S(A, B) >TS . Points are for different values of TS .
paper’s abstract was processed to remove common and dupli-
cate words so each paper was represented by a set of words.
(Unlike in most information retrieval applications, no spe- The relational similarity measure R has a slightly better
cial effort was taken to identify discriminating words, such ROC curve than the others because it is a more nuanced met-
as ranking words by their tf/idf scores.) The mean, median ric than the Dice coefﬁcient, D. In fact, D has a fairly high
and standard deviations of the number of words in an abstract accuracy; for example, if one says two abstracts are written
were 21.25, 21, and 10.6, respectively.               by different authors when D =0and by the same author
                                                      when D>0, then one’s overall accuracy (true negatives plus
4.1  Experiment 1: Same/different author              true positives) is 79%. However, roughly 25% of the pairs of
                                                                                           D  =0
Our ﬁrst experiment tested the relative contributions of the abstracts that are written by one author have , and it is
                                                                 R
D and R coefﬁcients to performance on a classiﬁcation task. here that the coefﬁcient provides additional resolution.
The task was to decide whether two papers were written by A followup experiment compared not individual papers but
the same author given their abstracts. The procedure is: Se- the entire oeuvres of authors. The protocol was as follows:
lect abstract A at random from the corpus; with probability p Select two authors A and B at random from the corpus. Select
select another abstract, B, from the papers of the same author random pairs a, b in which a and b are papers of A’s and B’s,
as abstract A, and with probability 1 − p select B at ran- respectively. Calculate S(a, b) for each such pair and add
dom from the corpus; record whether A and B have the same the mean of these scores (i.e., the mean pairwise similarity of
author; record D(A, B) and R(A, B). (For this experiment, papers by authors A and B) to the distribution of between-
Tocc =5.) Now, for a given value of α, compute the simi- author scores. Select random pairs of papers ai,aj and bi,bj,
larity score S(A, B) and compare it to a threshold, TS , and calculate their similarity scores and add the mean similarity
if S(A, B) >TS decide that A and B have the same author. scores to the distribution of within-author scores. Finally, re-
Because we recorded whether the papers were truly written peatedly select a mean score from either the within-author or
by one author or two, we know whether a “single-author” de- between-author distribution and, if the score exceeds a thresh-
cision is a correct (a hit) or a incorrect (a false positive). Re- old T assert that it is a within-author mean score (as these are
peating this process for many pairs of papers (1000 in these expected to be higher than between-author scores). Calcu-
experiments) yields a hit rate and a false positive rate for each late the classiﬁcation accuracy of these assertions for differ-
value of TS , and by varying TS we get a ROC curve.   ent values of T to get an ROC curve. The result is that the
  The ROC curve represents how good a classiﬁer is. Ideally, area under the ROC curve is 0.955, which is nearly perfect
the curve should have a vertical line going from the origin to classiﬁcation. In other words, the mean similarity scores for
the top-left corner. Such a curve would indicate that the false- pairs of papers by the same author are very much higher, in
positive rate is zero for all levels of hit rate up to 1.0. More general, than the mean similarity scores of pairs of papers by
commonly, as TS decreases, one gets more hits but also more different authors.

                                                IJCAI-07
                                                  27154.2  Experiment 2: Author trees and intrusions        contrast, when the similarity metric is R, the relational simi-
The second experiment tests how the similarity measure in larity coefﬁcient, the error rate associated with joining extant
Equation 4 performs in the PUBTREE algorithm. It is difﬁcult threads is highest. An intermediate error rate is evident for
                                                      α = .5
to get a gold standard for performance for PUBTREE. The al- .
gorithm is intended to organize the work of a research into
“threads,” but we lack an objective way to assess the threads
that characterize researchers’ work (see the following section   0.2
for a subjective assessment). However, if we build a graph
from two researchers’ abstracts, then we would hope the PUB-    0.15
TREE algorithm would not mix up the abstracts. Each thread
                                                                 0.1
it discovers should belong to one author and should include                    alpha = .5
no abstracts by the other. This is our proxy for PUBTREE’s
ability to disentangle research threads, and it can be evalu-   0.05
ated objectively.                                                   alpha = 1
                                                                  0

  The procedure for Experiment 2 is: Select ten abstracts at   Actual  / Potential Intrusions alpha = 0
random from the work of a random author. Select another ten
                                                                   .1 .2 .3 .4 .5 .6  .7 .8 .9 1
abstracts from a second author. Run PUBTREE to produce a           Proportion of Author 2 abstracts that join 
tree for the ﬁrst author. Now, have the algorithm add the pa-      extant threads
pers of the second author to this tree. To score performance,
recall that every abstract can be added as the child of one al-
ready in the tree, extending a thread; or it can be added as a
child of the root, starting a new thread. Good performance Figure 3: The relationship between the number of threads and
has two aspects: there should be no “intrusions” of the sec- the proportion of opportunities to include intrusions that were
ond author into threads of the ﬁrst author, and the number of taken, for three levels of α and ten levels of Tthread
new threads should be small. If the number of new threads
is large, then PUBTREE is saying, essentially, “every abstract Experiment 2 shows that the PUBTREE algorithm does a
looks different to me, I can’t ﬁnd any threads.” Thus, each good job in most conditions of keeping the abstracts of one
replicate of the experiment returns two numbers: The propor- author from intruding on the threads of another. This result
tion of all the abstracts, from both authors, that join extant provides some evidence that PUBTREE can separate abstracts
threads; and the proportion of the second author’s abstracts into threads and keep the threads relatively “pure.” Of course,
that intrude on the threads of the ﬁrst author. Fifty replicates this experiment actually tested whether PUBTREE would con-
                                      T
were run at each of ten levels of the threshold thread, which fuse the abstracts of two different authors, it did not test
is the threshold for allowing an abstract to extend a thread. whether PUBTREE correctly ﬁnds the threads in the papers
The entire experiment was repeated three times for different of one author.
mixing proportions of D and R, namely, α =[1.0, 0.5, 0]
  The results of this experiment are shown in Figure 3. The 5 Experiment 3: One author with coauthor
axes are the levels of the two dependent measures described
in the previous paragraph. On the horizontal axis is the pro- indicator
portion of all abstracts that join extant threads, and on the ver- The third experiment uses coauthor information as a gold
tical axis is the proportion of abstracts of the second author standard for the performance of PUBTREE. The idea is
that intrude into threads of the ﬁrst author. Each point on the that papers within a thread often have the same coauthors,
graph represents the mean of ﬁfty replicates associated with whereas papers in different threads are less likely to. We
one level of α and one level of Tthread. In general, higher would have reason to doubt whether PUBTREE ﬁnds real
values of Tthread cause new threads to be formed because if threads if the degree of overlap between coauthors within
an abstract is not similar enough to an abstract in an extant a thread is no different than the overlap between coauthors
thread to exceed Tthread, then it starts a new thread. If an ab- across threads.
stract begins a new thread then it cannot intrude in an extant The procedure for Experiment 3 is replicated many times,
one, so there is a positive relationship between the tendency once for each author of ten or more abstracts. For each such
to join an extant thread and the number of intrusions. author, build a publication tree with PUBTREE. Then for ev-
  Although the relationship between these variables is not ery pair of abstracts in the tree, calculate the overlap between
linear, we can interpret the slopes of the lines in Figure 3 coauthor lists with our old friend, the Dice coefﬁcient (Eq. 1),
very roughly as the error rate, in terms of intrusions, of join- substituting lists of authors for WA and WB. Next, the over-
ing extant threads. This error rate is highest for the relational lap between papers within each thread is calculated in the
similarity measure, R, and appears to be lowest for the Dice same way. Thus we generate two lists of overlap scores, one
coefﬁcient D. In fact, almost all of the data for α =1— for all pairs of papers, the other for papers in threads. Finally,
which is equivalent to the Dice coefﬁcient — is grouped near a two-sample t test is run on these lists and the t statistic is
the origin because D =0so often, as we noted in the pre- reported. (Note we are not using the t test for any inferential
vious section. Said differently, when the similarity metric is purpose, merely using the t statistic as a descriptive measure
D, intrusions are avoided by starting a lot of new threads. In of the difference between two samples.)

                                                IJCAI-07
                                                  2716                                                      and count the number of inappropriate parent/child relation-
                                                      ships. For example, consider a thread of three papers, A, B
                                                      and C. Two papers, B and C might be misplaced: B might
                                                      not be a child of A and C might not be a child of B. The latter
                                                      question is settled independently of the former, so even if B is
                                                      not appropriately placed under A, C might be appropriately
                                                      placed under B. One ought to ask whether C is appropriately
                                                      placed under A, but this question made the task of scoring
                                                      publication trees considerably harder for the volunteers and
         -2 0  2  4  6 8 10 12 14 16 18 20            was not asked.
                                                        The trees for the three authors contained 44, 71, and 74 pa-
                                                      pers, respectively. The authors all have written considerably
                                                      more papers, so PUBTREE was working with incomplete data.
                                                      The ﬁrst author reported that 32 of her papers, or 73% were
Figure 4: The distribution of t statistics for 165 authors of at placed correctly. Of the remaining 12 papers, three should
least 10 papers. A publication tree is built for each author and have started entirely new threads (i.e., should have been chil-
the degree of overlap between coauthors of pairs of papers dren of the root node), and two had incorrect Citeseer dates
within and between threads is calculated. The between-thread and might have been placed wrongly for this reason. The sec-
and within-thread samples are compared with a two-sample t ond author reports a slightly higher proportion of misplaced
test and this is the distribution of t statistics.    papers (24, or 34%) but notes that for more than half of these,
                                                      the publication date was unknown and so was not used by the
                                                      algorithm. The third author reported that 14 of the 74 papers
  Virtually all the authors showed the same pattern: Coau-
                                                      had corrupted titles and were duplicates of others in the set.
thors of papers on a thread overlapped more than coauthors
                                                      Of the remaining 60 papers, 10, or 17% were placed wrongly.
of papers on different threads. The distribution of t statistics
                                                      Three started new threads but should have been added to ex-
is shown in Figure 4. Values greater than zero indicate more
                                                      tant threads, four had valid dates and were placed under the
within-thread coauthor overlap, values less than zero indicate
                                                      wrong parents, and three had no dates and were placed under
more between-thread overlap. Values greater than 2 would be
                                                      the wrong parents.
signiﬁcant at the .01 level if we were testing the hypothesis
that within-thread overlap is no different than between-thread These results are not dramatically good, but nor are they
overlap. Roughly 2% of the authors had values less than zero dramatically bad, especially when one considers that every
and 81% had values greater than 2.0. On average, the overlap paper might have been placed below any of two dozen (for
coefﬁcients for papers within and between threads were .73 the ﬁrst author) or three dozen (for the second and third au-
and .48, respectively. Said differently, papers within threads thor) papers, on average. One of the authors said, “[The] tree
had 64% more overlap among coauthors (calculated per pri- did surprisingly well, given the subtle differences between
mary author and then averaged) than papers between threads. papers. ...The tree made most but not all of the right con-
                                                      nections. I noticed that some papers could be thought of as
We may safely conclude that PUBTREE’s threads, which it
constructs with no knowledge of coauthors, satisfy one intu- having more than one parent in reality, which would compli-
itive feature of threads, namely, that papers on threads often cate threads, but of course that can’t be addressed in a single
share coauthors.                                      tree.”

6  Experiment 4: Subjective judgments                 7   Discussion and Future Work
The ﬁrst three experiments report proxies for the subjective As the author noted correctly in the previous section, PUB-
notion of a research thread. Experiment 1 showed that the TREE builds trees, not graphs, and so it cannot give a paper
similarity measure S(A,B) does pretty well as a classiﬁer of more than one parent. This is unfortunate because some pa-
whether abstracts A and B are written by the same author. pers truly are the children of two or more research threads.
Experiment 2 showed that PUBTREE tends to segregate pa- A version of the algorithm that produces graphs is under de-
pers written by different authors into different threads of a velopment. Another likely way to improve the algorithm is to
publication tree. Experiment 3 used exogenous information include additional information about papers, such as coauthor
about coauthors to show that abstracts in PUBTREE’s threads lists.
tend to share coauthors. None of these is a direct test of PUBTREE and its successors might be applied to several
whether PUBTREE builds trees that represent threads in au- current problems. One is to track the development of ideas
thors’ research. Research threads are subjective entities and in the Blogosphere. Blogs are like abstracts of research pa-
the accuracy with which they are recovered by PUBTREE can- pers in many ways and it will be interesting to look for blog
not be assessed objectively with the information available in threads in the writings of individual authors, and perhaps even
Citeseer. The best we can do is ask authors to assess subjec- more enlightening to ﬁnd threads in amalgamations of several
tively the publication trees constructed for them by PUBTREE. authors’ blogs. Email threading is a related problem.
Three authors volunteered for this arduous job. Each was in- The relational similarity measure that underlies PUBTREE
structed to examine every thread of his or her publication tree also has numerous applications, notably to problems that re-

                                                IJCAI-07
                                                  2717