                       Evaluating an NLG System using Post-Editing 

                    Somayajulu G. Sripada and Ehud Reiter and Lezan Hawizy 
                                   Department of Computing Science 
                                        University of Aberdeen 
                                       Aberdeen, AB24 3UE, UK 
                               {ssripada,ereiter,lhawizy}@csd.abdn.ac.uk 

                    Abstract                            Table 1 shows a small extract from the NWP data for 12-
                                                      06-2002, and Table 2 shows part of the textual forecast that 
    Computer-generated texts, whether from Natural    SumTime-Mousam generates from the NWP data. 
    Language Generation (NLG) or Machine Transla-
                                                       
    tion (MT) systems, are often post-edited by hu-
                                                          Field           Text 
    mans before being released to users.  The fre-        WIND(KTS) 10M   W 8-13 backing SW by mid after-
    quency and type of post-edits is a measure of how                     noon and S 10-15 by midnight. 
    well the system works, and can be used for evalua-    WIND(KTS) 50M   W 10-15 backing SW by mid after-
    tion.  We describe how we have used post-edit data                    noon and S 13-18 by midnight. 
    to evaluate SUMTIME-MOUSAM,  an NLG  system        
    that produces weather forecasts.                  Table 2. Extract from SUMTIME-MOUSAM Forecast Produced from 
                                                      NWP data in Table 1. 
1   Introduction                                       
In this paper we describe an evaluation technique, which Weathernews uses SumTime-Mousam  to generate draft 
looks at how much humans need to post-edit texts generated forecasts; we call them ‘Pre-edit Texts’. The forecasters 
by an NLG system before they are released to users.  Post- then post-edit them to produce ‘Post-edit texts’.  When the 
edit evaluations are common in machine translation [Hut- forecaster is done, the complete forecast is sent to the cus-
chins and Somers, 1992], but we believe that ours is the first tomer. 
large-scale post-edit evaluation of an NLG system. Mitkov 
and An Ha [2003] reported a small scale post-edit evalua- 2 Post-Edit Evaluation  
tion of their NLG system.                             The evaluation was carried out on 2728 forecasts, collected 
  The system being evaluated is SUMTIME-MOUSAM [Sri-  during period June to August 2003.  Each forecast was 
pada et al, 2003], an NLG system, which generates weather roughly of 400 words, so there are about one million words 
forecasts from Numerical Weather Prediction (NWP) data. in all in the corpus. 
The forecasts are marine forecasts for offshore oilrigs.  For each forecast, we have the following data: 
SUMTIME-MOUSAM    is operational and is used by Weath-
                                                        • Data: The final edited NWP data 
ernews (UK) Ltd to generate 150 draft forecasts per day, 
which are post-edited by Weathernews forecasters before • Pre-edit text: The  draft forecast produced  by 
being released to clients.                                 SUMTIME-MOUSAM. 
   
                                                        • Post-edit text: The manually post-edited forecast, 
 Time  Wind Dir  Wind Spd  Wind Spd   Gust    Gust 
                     10m       50m     10m    50m          which was sent to the client. 
 06:00       W       10.0      12.0    12.0   16.0      • Background information: includes date, location, and 
 09:00       W       11.0      14.0    14.0   17.0         forecaster 
 12:00    WSW        10.0      12.0    12.0   16.0 
                                                        The following procedure is performed automatically by a 
 15:00      SW        7.0       9.0     9.0   11.0 
                                                      software tool: 
 18:00     SSW        8.0      10.0    10.0   12.0 
                                                        •
 21:00       S        9.0      11.0    11.0   14.0        First, we break sentences up into phrases, where each 
 00:00       S       12.0      15.0    15.0   19.0         phrase describes the weather at one point in time. 
                                                        • The second step is to align phrases from the previous 
Table 1. Weather Data produced by an NWP model for 12-Jun  step as a preparation for comparison in the next step. 
2002                                                       Our alignment procedure first generates an exhaustive 
      list of possible alignments and uses a scoring scheme the cost of using SUMTIME-MOUSAM, and hence of the 
     to select aligned phrases.                       attractiveness of the system to users. 
  •                                                       (A) however was perhaps less true than we had hoped. 
    The third step is to compare aligned phrases and label Wagner [1998] also described post-edited texts in MT as at 
     each pre-edit/post-edit pair as match, replace, add, or times noisy. During the development of SUMTIME-
     delete.                                          MOUSAM, our analysis of manually written forecasts [Reiter 
  For example, A and B of Figure 1 are analyzed as in Ta- and Sripada, 2002] had highlighted a number of “noise” 
ble 3 where phrases are shown separated by a shaded row. elements that made it more difficult to extract information 
                                                      from such corpora. While collecting the post-edit data, we 
   A. Pre-edit Text: SW 20-25 backing SSW 28-33 by midday, assumed that people would only post-edit mistakes, where 
   then gradually increasing 34-39 by midnight.       the generated text was wrong or sub-optimal, and hence 
   B. Post-edit Text: SW 22-27 gradually increasing SSW 34-39. post-edit data would be better for evaluation purposes than 
                                                      corpus comparisons. 
Figure 1. Example pre-edit and post-edit texts from the post-edit   In fact, however, there were many justifications for post-
corpus                                                edits.  Some post-edits fixed problems in the generated texts 
                                                      (such as overuse of then); some post-edits refined/optimized 
  POS           A           B           label         the texts (such as using for a time); some post-edits reflected 
  Direction     SW          SW          match         individual preferences (such as easing vs decreasing); and 
  Speed         20-25       22-27       replace       some post-edits were downstream consequences of earlier 
                                                      changes (such as introducing SSW before ‘34-39’ in B, in 
  Conjunction   then        <none>      delete        the example of Section 2).  We wanted to use our post-edit 
  Adverb        gradually   gradually   match         data to improve the system, not just to quantify its perform-
  Verb          increasing  increasing  match         ance, and we discovered that we could not do this without 
  Direction     <none>      SSW         add           attempting to analyze why post-edits were made.  Probably 
  Speed         34-39       34-39       match         the best way of doing this was to discuss post-edits with the 
  Time          by midnight <none>      delete        forecasters. 
 
Table 3. Detailed Edit Analysis                       4   Conclusion 
 
  We  processed 2728 forecast pairs (pre-edited and post- We have used analysis of post-edits, a popular evaluation 
edited). These were divided into 73041 phrases. Out of technique in machine translation, to evaluate SumTime-
these, the alignment procedure failed to align 7608 (10%) Mousam, an NLG system that generates weather forecasts. 
phrases. Out of the successfully aligned phrases, 43914 While we encountered some problems, such as the need to 
(60%) are perfect matches, and the remaining 21519 (30%) identify why post-edits were made, on the whole we found 
are mismatches.  Table 4 summarizes the mismatches and post-editing to be a useful evaluation technique which gave 
suggests that the major problem is ellipsis.  Most (25235 out us valuable insights as to how to improve our system. 
of 35874, 70%) of these errors are deletions, where the fore-
caster deletes words SumTime-Mousam’s texts.          References 
                                                      [Hutchins and Somers, 1992] John Hutchins and Harold L. 
  S. No.   Mismatch Type          Freq.   %              Somers. An Introduction to Machine Translation, Aca-
  1.       Ellipses (word additions 35874 65             demic Press, London, 1992. 
           and deletions) 
  2.       Data Related Replacements 10781 20         [Mitkov and An Ha, 2003] Ruslan Mitkov and Le An Ha. 
           (range and direction re-                      Computer-Aided Generation of Multiple-Choice Tests, 
           placements)                                   In Proc. of the HLT-NAACL03 Workshop  on Building 
  3.       Lexical Replacements   8264    15             Educational Applications Using NLP, Edmonton, Can-
           Total                  54919                  ada, pages 17-22, 2003. 
                                                      [Reiter and Sripada, 2002] Ehud Reiter and Somayajulu G. 
Table 4. Results of the evaluation showing summary categories Sripada. Human Variation and Lexical Choice. Compu-
and their frequencies                                    tational Linguistics, 28:545-553, 2002. 
                                                      [Sripada et al., 2003] Somayajulu G. Sripada, Ehud Reiter, 
3   Discussion of Post-Edit Evaluations                  and Ian  Davy. SUMTIME-MOUSAM:       Configurable 
We  were attracted to post-edit evaluation because we be- Marine Weather Forecast Generator. Expert Update, 
lieved that (A) people would only edit things that were  6(3):4-10, 2003. 
clearly wrong; and (B) post-editing was an important use-  [Wagner, 1998] Simone Wagner. Small Scale Evaluation 
fulness metric from the perspective of our users.        Methods In Proc. of the Workshop on Evaluation of the 
Looking back, (B) was certainly true.  The amount of post- Linguistic Performance of Machine Translation Systems 
editing that generated texts require is a crucial component of at the KONVENS-98, Bonn, pages 93-105, 1998. 