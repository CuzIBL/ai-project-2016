                 A General Model for Online Probabilistic Plan Recognition 

                                                     Hung H. Bui 
                                              Department of Computing 
                                          Curtin University of Technology 
                                    PO Box U1987, Perth, WA 6001, Australia 
                                     URL: http://www.cs.curtin.edu.aurbuihh 
                                           Email: buihh@cs.curtin.edu.au 

                        Abstract                               network, making exact inference intractable even when the 
                                                               DBN has a sparse structure. Thus, online plan recognition 
     We present a new general framework for online 
                                                               algorithms based on exact inference will run into problems 
     probabilistic plan recognition called the Abstract 
                                                               when the belief state becomes too large, and will be unable to 
     Hidden Markov Memory Model (AHMEM). The 
                                                               scale up to larger or more detailed plan hierarchies. 
     new model is an extension of the existing Abstract 
                                                                 In our previous work, we have proposed a framework for 
     Hidden Markov Model to allow the policy to have 
                                                               online probabilistic plan recognition based on the Abstract 
     internal memory which can be updated in a Markov 
                                                               Hidden Markov Models (AHMM) [Bui et a/., 2002]. The 
     fashion. We show that the AHMEM can repre­
                                                               AHMM is a stochastic model for representing the execution 
     sent a richer class of probabilistic plans, and at the 
                                                               of a hierarchy of contingent plans (termed policies). Scal­
     same time derive an efficient algorithm for plan 
                                                               ability in policy recognition in the AHMM is achieved by 
     recognition in the AHMEM based on the Rao-
                                                               using an approximate inference scheme known as the Rao-
     Blackwellised Particle Filter approximate inference 
                                                               Blackwellised Particle Filter (RBPF) [Doucet et al 20001. 
     method. 
                                                               It has been shown that this algorithm scales well w.r.t. the 
                                                               number of levels in the plan hierarchy. 
1 Introduction                                                   Despite its computational attractiveness, the current 
The ability to perform plan recognition can be very useful in  AHMM is limited in its expressiveness, in particular, its in­
a wide range of applications such as monitoring and surveil­   ability to represent an uninterrupted sequence of plans and 
lance, decision supports, and team work. However the plan      actions. This is due to the fact that each policy in the AHMM 
recognizing agent's task is usually complicated by the uncer­  is purely reactive on the current state and has no memory. 
tainty in the plan refinement process, in the outcomes of ac­  This type of memoryless policies cannot represent an unin­
tions, and in the agent's observations of the plan. Dealing    terrupted sequence of sub-plans since they have no way of 
with these issues in plan recognition is a challenging task, es­ remembering the sub-plan in the sequence that is currently 
pecially when the recognition has to be done online so that    being executed. In other words, the decision to choose the 
the observer can react to the actor's plan in real-time.       next sub-plan can only be dependent on the current state, and 
   The uncertainty problem has been addressed by the sem­      not on the sub-plans that have been chosen in the past. Other 
inal work [Charniak and Goldman, 19931 which phrases           models for plan recognition such that the Probabilistic State 
the plan recognition problem as the inference problem in       Dependent Grammar (PSDG) [Pynadath and Wellman, 2000; 
a Bayesian network representing the process of executing       Pynadath, 1999] are more expressive and do not have this 
the actor's plan. More recent work has considered dy­          limitation. Unfortunately, the existing exact inference method 
namic models for performing plan recognition online [Py-       for the PSDG in [Pynadath, 1999] has been found to be flawed 
nadath and Wellman, 1995; 2000; Goldmand et a/., 1999;         and inadequate [Bui, 2002]. 
Huber et ai, 1994; Albrecht et a/., 1998]. While this offers a   The main motivation in this paper is to extend the existing 
coherent way of modelling and dealing with various sources     AHMM framework to allow for policies with memories to be 
of uncertainty in the plan execution model, the computational  considered. We propose an extension of the AHMM called 
complexity and scalability of inference is the main issue, es­ the Abstract Hidden Markov mEmory Model (AHMEM). 
pecially for dynamic models.                                   The expressiveness of the new model encompasses that of the 
   Inference in dynamic models such as the Dynamic             PSDG [Pynadath and Wellman, 2000], thus the new model 
Bayesian Networks (DBN) [Nicholson and Brady, 1994] is         removes the current restriction of the AHMM. More impor­
more difficult than in a static model. Inference in a static   tantly, we show that the RBPF approximate inference method 
network utilizes the sparse structure of the graphical model   used for the AHMM can be extended to the more general 
to make it tractable. In the dynamic case, the DBN belief      AHMEM as well, ensuring that the new generalized model 
state that we need to maintain usually does not preserve the   remains computationally attractive. To the best of our knowl­
conditional independence properties of the single time-slice   edge, we are the first to provide a scalable inference method 


USER MODELING                                                                                                        1309  for this general type of hierarchical probabilistic plan hierar• tion rules of the form X YX and A" are allowed. 
 chy.                                                          The first rule models the adoption of a lower level policy Y 
   The paper is structured as follows. Section 2 provides      by a higher level policy A', while the second rule models the 
 a more detailed discussion of the AHMM, PSDG and re•          termination of the policy X. The PSDG model considered 
 lated models for online probabilistic plan recognition. The   by Pynadath and Wellman allows for more general rules of 
 AHMEM is introduced in section 3 and the algorithms for       the form X i.e., the recursion symbol must 
 plan recognition are presented in section 4. Experimental re• be located at the end of the expansion. Thus in a PSDG, a 
 sults with a prototype system are provided in section 5. Fi•  policy might be expanded into a sequence of policies at the 
 nally, we conclude and discuss directions for future work in  lower level which will be executed one after another before 
section 6.                                                     control is returned to the higher level policy. 
                                                                 Although more expressive than the AHMM, the existing 
2 Related Models for Online Probabilistic                      computational method for inference with the PSDG remains 
     Plan Recognition                                          inadequate. Pynadath proposed an exact method for updat•
                                                               ing the belief state of the PSDG in a "compact" closed form. 
In the AHMM [Bui et al, 2000; 2002], an agent's probabilis•    The proposed algorithm seemingly gets around the exponen•
tic plan is modeled by an abstract Markov policy (AMP). An     tial blow up in the size of the belief state. Unfortunately, the 
AMP is an extension of a policy in Markov Decision Pro•        derivation of the algorithm is based on a flawed assumption 
cesses (MDP) defined within a subset of the environment state  that the higher levels in the belief state are independent of the 
space so that it can select other more refined AMPs and so     lower levels given the current level. For more details about 
on to form a hierarchy of policies. The AMP is thus simi•      the flaw in the inference algorithm for PSDG, interested read•
lar to a contingent plan that prescribes which sub-plan should ers are referred to [Bui, 20021. 
be invoked at each applicable state of the world. The noisy      The AHMM, PSDG, and the proposed AHMEM are re•
observation about the environment state can be modelled by     lated to the Hierarchical Abstract Machines (HAM) [Parr, 
making the state "hidden", similar to the hidden state in the  1998] used in abstract probabilistic planning. In this model, 
Hidden Markov Models [Rabiner, 1989]. The stochastic pro•      the policy is represented by a stochastic finite automaton, 
cess resulting from the execution of an AMP is termed the Ab•  which can call other automata at the lower level. Despite their 
stract Hidden Markov Model. Intuitively, the AHMM models       representational similarity, the computational techniques for 
how an AMP causes the adoption of other policies and ac•       AHMEM and related models are intended for plan recogni•
tions at different levels of abstraction, which in turn generate tion whereas the HAM model is used for speeding up the pro•
a sequence of states and observations. In the plan recogni•    cess of finding an optimal policy for MDP. 
tion task, an observer is given an AHMM corresponding to         If we ignore the state dependency, the DBN structure of the 
the actor's plan hierarchy, and is asked to infer about the cur• AHMEM and PSDG is similar to the structure of the Hierar•
rent policy being executed by the actor at all levels of the   chical Hidden Markov Model (HHMM) [Fine et al, 1998; 
hierarchy, taking into account the sequence of observations    Murphy and Pashkin, 2001]. However, while the HHMM is 
currently available. This problem is termed policy recogni•    a type of Probabilistic Context Free Grammar (PCFG), the 
tion [Bui et al, 2002].                                        AHMEM and PSDG are not due to the state dependency in 
   Scalability of policy recognition in the AHMM is            the model. 
achieved by using a hybrid inference method, a variant 
of the Rao-Blackwellised particle filter (RBPF) [Doucet        3 The Abstract Hidden Markov Memory 
et al, 2000]. When applied to DBN inference, Rao-
Blackwellisation [Casella and Robert, 1996] splits the net•        Models 
work into two sets of variables: the set of variables that need 
                                                               This section introduces the Abstract Hidden Markov Mem•
to be sampled (termed the Rao-Blackwellising (RB) vari•
                                                               ory Models (AHMEM), an extension of the AHMM where 
ables), and the set of remaining variables whose belief state 
                                                               the policy can have internal memory. Our main aim is to 
conditioned on the RB variables need to be maintained via 
                                                               construct a general model for plan recognition whose expres•
exact inference (termed the Rao-Blackwellised (RB) belief 
                                                               siveness encompasses that of the current AHMM and PSDG 
state). The RBPF thus allows us to combine sampling-based 
                                                               models, while retaining the computational attractiveness of 
approximate inference with exact inference to achieve effi•
                                                               the AHMM framework. We first define the AHMEM in sub-
ciency and improve accuracy. 
                                                               section 3.1. The DBN structure of the new model is given in 
  The Probabilistic State-Dependent Grammar (PSDG) [Py-        subsection 3.2. 
nadath, 1999; Pynadath and Wellman, 2000] can be described 
as the Probabilistic Context Free Grammar (PCFG) [Jelinek 
                                                               3.1 The Model 
et al, 1992], augmented with a state space, and a state transi•
tion probability table for each terminal symbol of the PCFG.   Consider an MDP-like model with S representing the states 
In addition, the probability of each production rule is made   of the environment, and A representing the set of primitive 
state dependent. As a result, the terminal symbol now acts     actions available to the agent. Each action a € A is defined 
like primitive actions and the non-terminal symbol chooses     by its transition probability from the current state s to the next 
its expansion depending on the current state. The AHMM         state s': oa(s,s'). The set of abstract policies will include 
is equivalent to a special class of PSDG where only produc•    every primitive actions. Furthermore, the AHMM [Bui et al, 


1310                                                                                                   USER MODELING 2000] defines higher level abstract policies on top of a set of distributions, i.e.  
policies as follows:                                           l,and 1. 
                                                                 When an AMPE is executed from a state .s, it first 
                                                               initialises its memory value m according to the distribution 
                                                                        Then a policy at the lower level will be selected 
                                                               according to the distribution This policy will 
                                                               be executed until it terminates at some state At the new 
                                                               state .s', the policy itself will terminate with probability 
                                                                         If it does not terminate, the memory variable will 
                                                               be given a new value m' according to the transition probabil­
                                                               ity Then a new policy at the lower level is 
                                                               selected with probability and so on. 
                                                                 Using the AMPEs, we can construct a hierarchy of abstract 
                                                               policies in the same way as in the AHMM. We start with a set 
   An AMP as defined above is purely reactive, in the sense    of primitive actions = A, and for k = 1,..., K build a set 
that it selects the next policy at the lower level based only on of new AMPEs on top of Then, if a top-level policy 
the current state s. This restricts the set of behaviours that     is executed, it invokes a sequence of level-(K-l) policies, 
an AMP can represent. For example, it will not be able to      each of which invokes a sequence of leveI-(K-2) policies and 
represent a plan consisting of a few sub-plans, one followed   so on. A level-1 policy will invoke a sequence of primitive 
by another regardless of the state sequence. To represent this actions which leads to a sequence of states. We can then in­
kind of plans, the agent needs to have some form of inter­     troduce the hidden states and model the noisy observation of 
nal memory to remember the current stage of execution. Let     the state by an observation model The 
M be a set of possible internal memory states. We first ex­    dynamic process of executing a top-level AMPE is termed the 
tend the definition of our policy to include a memory variable Abstract Hidden Markov mEmory Model (AHMEM). 
which takes on values in M and is updated after each stage of    Some special cases of the AHMEM are worth mention­
execution of the policy.1                                      ing here. First, the AHMM itself is a special AHMEM. The 
Definition 2 (Abstract Markov Policy with Memory               memoryless policies of the AHMM are equivalent to AM­
(AMPE)). Let II be a set of AMPEs, an AMPE * over II is       PEs where the dependency on the memory variable is ignored 
defined as a tuple where: (e.g. when M is a singleton set). The class of PSDG con­
                                                               sidered in [Pynadath, 1999] can also be easily converted to 
                     is the set of applicable states. 
                                                               an AHMEM. The terminal symbols in the PSDG are equiv­
                      is the set of destination states.        alent to the primitive actions. Each non-terminal symbol is 
                            (0,1] is the terminating proba-    equivalent to a memoryless policy. In addition, each sequence 
                       is the probability that the policy                        encountered on the RHS of a production 
     would stop if the current state is d and the current mem­                                           is equivalent to a 
     ory value is 777.                                         policy whose memory m taking on the values 1,... ,n. Y 
                                                               then simply selects the (memoryless) policy  
                                 is the policy selection prob­
                                                                 Note that in the AHMEM definition, we assume a balanced 
     ability, a (.s, m,) is the probability that selects the 
                                                               policy hierarchy for the ease of presentation (all the actions 
     policy  at the state s and memory value  
                                                               must appear at the same bottom level). However, we can also 
                          [0,1] is the initial distribution of specify an unbalanced hierarchy by introducing some dummy 
     memory values. is the probability that the ini­           policies which arc equivalent to primitive actions at the higher 
     tial memory is m if * commences at state s.              levels in the hierarchy. 
                                      is the memory transi­
     tion probability. is the probability that the             3.2 DBN Representation of the AHMEM 
     next memory value is m' given that the current memory 
     value is m and the current state is .s. 
   Subsequently, we will drop the subscript if there is no 
confusion about the policy in the context. Note that all the 
states in D \ S are called terminal states and thus  
                         Also, the policy selection, memory 
initial and transition probability have to be proper probability 

   'One can argue that wc can always incorporate the memory vari­
able in the environment state, and hence we gain no extra represen­
tational power just by introducing the memory variables. However, 
incorporating the memory variables in the state variable would blow 
up the size of the state space and thus defeat our purpose of keeping 
the model computationally feasible. 


USER MODELING                                                                                                       1311                                                                at the start: either through the starting state or the starting 
                                                               time. Thus, the conditional independence theorem for poli­
                                                               cies in the AHMM still holds in this more general setting. 
                                                               We state the theorem for the AHMEM below. The proof for 
                                                               the AHMM [Bui et ai, 2002] can be directly extended to this 
                                                               general case using the context specific independence proper­
                                                               ties described in the previous subsection. 


                                                                  Let Note that knowingis equivalent to 
                                                               knowing precisely when each of the policies starts and ends. 
                                                               Therefore, given the starting time and state of every 
                                                               current policy are known. The following corollary is thus a 
                                                               direct consequence of theorem 1. 

                                                               Corollary 1. Let Ct represent the conditional joint distribu•
                                                               tion Then Ct has the following 
 Policy Termination and Selection                              Bayesian network factorization: 
 The policy termination and selection model for the AHMEM 
 is essentially the same as in the AHMM, except for the depen­
 dency on the value of the current memory variable We 
 note that the same context specific independence [Boutilier et 
 a/., 1996] properties of the AHMM still hold. For example, if   Ct thus also has the following undirected network repre­
       = F then = F and becomes independent of the             sentation. We first form the set of cliques:  
 remaining parents of this variable. Now consider the variable                                        Note that the set of 
            = F then and is independent of the                 cliques CO-K form a chain of cliques in this order, therefore we 
 remaining parents; otherwise,                                 term Ct the policy-clique chain. This extends the concept of 
                        and is independent of                  the policy chain in the memoryless case of the AHMM [Bui 
                                                               et al, 2000]. Ct can be factored into the product of poten-
 Memory Update 
 Consider the variable There are two parents of this node 
 that act like context variables: and If = F, 
 the policy at the lower level has not terminated and memory   network factorization, the potentials are said to be in canoni•
 is not updated at this time. Thus, and be­                    cal form. Any potential representation of the clique chain can 
 comes independent of all the remaining parents. If = T        be canonicalized by first perform message passing (exact in­
                                                               ference) to compute the marginal at each clique. The canoni­
                                                               cal form can then be computed directly from these marginals. 
                                                               Later on we will use the undirected representation of Ct for 
                                                               exact inference, and the canonical form (directed representa­
                                                               tion) of Ct for obtaining samples from the joint distribution 
                                                               using simple forward sampling. 
                                                               4 Approximate Inference for AHMEM 
                                                               In this section, we look at the online inference prob­
                                                               lem in the AHMEM. Assume that at time f, we have 
3.3 Independence Properties in the AHMEM                       a sequence of observations about the environment state 
 Even though AMPEs are more expressive than memoryless                 = We need to compute the be­
 policies, they remain "autonomous", in the sense that the     lief state of the DBN which is the joint distribution of 
 higher layers have no influence over the state of an AMPE     all the current variables given this observation sequence: 
during its execution. The only way the higher layers can influ­                                , From this, we can answer 
ence the current state of an AMPE is through the conditions    various queries about the current status of the plan execution. 


 1312                                                                                                  USER MODELING For example, the marginal probability of tells us about 
the current policy the actor is executing at some level k; the 
probability Prtells us about the current stage of ex­
ecution of a policy the probability Pr tells us if 
   will end after the current time, etc. 
  Since there is no compact closed form representation 
for the above belief state, exact inference in the structure 
of the AHMEM is intractable when K is large. How­
ever, theorem 1 suggests that we can apply the Rao-
Blackwellised Particle Filter (RBPF) to this problem in a sim­
ilar way as in the AHMM [Bui et al., 2002], i.e. by us­
ing rt as the Rao-Blackwellising (RB) variables. The Rao-
Blackwellised (RB) belief state is then similar to the origi­
nal belief state of the AHMEM, except that now are 
known: = Pr Note that 
the RB belief state can be obtained directly from the 
policy-clique chain Ct by adding in the network represent­
ing the conditional distribution of ot and , — 
Pr Pr(oi|st)Ct(seeFig.2). 
  Two main steps in the RBPF procedure are: (1) updating            Figure 2: Two time slices of the RB belief state 
the RB belief state using exact inference, and (2) sampling 
the RB variable rt from the current RB belief state. 
                                                              same as the RBPF procedure for AHMM [Bui et al, 2002]. 
4.1 Updating the RB Belief State                              At each time step t, the algorithm maintains a set of N sam­
Fig. 2 shows the modified 2-time-slice DBN when the RB        ples, each consists of a value for st~i and //_i, and a para­
variables are known We note that all the                      metric representation of Ct The differences are in the details 
nodes from above level / remain unchanged, while all the      of how to obtain new samples and update the RB belief state. 
links across time slices from level / and below can be re­      To obtain each new sample (st,lt), we first need to canoni-
moved. This greatly simplifies the network structure, allow­  calize the potentials of Ct. However, assuming that Ct-\ is in 
ing the updating operations to be performed efficiently.      canonical form, we only have to canonicalize the potentials 
                                                              of Ct between level 0 and level I + 1 with complexity O(1). 
  Since the Bt can be obtained directly from Ct, all we have 
                                                              Thus the complexity of the sampling step is O(Nl). Since 
to do is to compute from Ct. The procedure for updating 
as usual has two stages: (1) absorbing the new evidence, i.e. the complexity of the updating step is also O(Nl), the overall 
                                                              complexity of the algorithm at each time step is O(Nl). Fur­
from Ct, we need to compute — Pr  
and (2) projecting to the new time slice, i.e. from we thermore, the distribution for I usually decays exponentially, 
                                                              thus the average complexity is only O(N). 
need to compute Ct+\. 
  In principle, we apply a simplified version of the junction-  If at some time t, an estimation e.g. Pr 
tree algorithm Uensen, 1996] on the undirected network rep­   required, we need to compute h = for each sam­
resentation of Ct to perform the update step. This is in fact ple. This involves performing message passing for the entire 
a generalization of the arc-reversal procedure which operates chain Ct. Thus the complexity for this time step is 0(NK). 
on directed network representation of the policy chain in the Other types of queries are also possible, as long as the proba­
AHMM [Bui et al, 2002]. The algorithm for updating the        bility required can be computed from the RB belief state. For 
                                                              example, we can ask the question: if the actor is currently 
RB belief state is given in Fig. 3. The absorbing step involves 
                                                              executing what is the current stage of execution of this 
simply incorporating the evidence likelihood into the poten­
                                                              policy? To answer this query, we need to compute the con­
tials of Ct to obtain the potentials for Ct+. The projecting 
                                                              ditional probability This can easily be 
step involves first adding time-slice t + 1 to Ct+ (see Fig. 2), 
                                                              achieved by replacing the ft function in the algorithm with 
then marginalizing the now redundant variables 
in the old time slice. The marginalization is done by perform­
ing message passing between the cliques of the 2-time-slice 
network shown in Fig. 2.                                      5 Experimental Results 

  Since the potentials of Ct from level / + 2 to level K' stay We have implemented the above algorithm in a surveillance 
unchanged, the complexity of the algorithm is 0(1) where / is domain to demonstrate its working in a practical application. 
the highest level of termination. Furthermore, if one of these The environment consists of a spatial area which has two sep­
potentials is in canonical form, it remains in canonical form arate rooms and a corridor monitored by a set of 5 cameras 
after the updating procedure.                                 (Fig. 5). The monitored area is divided into a grid of cells, 
                                                              and the cell coordinates constitute the overall state space 5. 
4.2 RBPF for AHMEM                                            The coordinates returned by the cameras are modelled as the 
The full RBPF algorithm for policy recognition in the         noisy observations of the true coordinates of the tracked per­
AHMEM is provided in Fig 4. The general structure is the      son, and over time, provide the sequence of observations o\ ;t. 


USER MODELING                                                                                                      1313 