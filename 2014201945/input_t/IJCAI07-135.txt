 Real Boosting      a la Carte   with an Application to Boosting Oblique Decision Trees

          Claudia Henry             Richard Nock                   Frank Nielsen
             U. Antilles-Guyane / CEREGMIA                       SONY   CS Labs Inc.
           97275 Schoelcher, Martinique, France                Tokyo 141-0022, Japan
          {chenry,rnock}@martinique.univ-ag.fr                 Nielsen@csl.sony.co.jp


                    Abstract                          a simple assumption, a corresponding boosting algorithm.
                                                      Using a recent result due to [Nock and Nielsen, 2006],this
    In the past ten years, boosting has become a major algorithm uses real-valued weak hypotheses, but it does not
    ﬁeld of machine learning and classiﬁcation. This  face the repetitive minimization of the exponential loss faced
    paper brings contributions to its theory and algo- by previous Real boosting algorithms [Friedman et al., 2000;
    rithms. We ﬁrst unify a well-known top-down de-   Schapire and Singer, 1999]. When the classiﬁer is an oblique
                                    [
    cision tree induction algorithm due to Kearns and decision tree, the algorithm obtained has three key features:
                 ]                      [
    Mansour, 1999 , and discrete AdaBoost Freund      it is fast, simple, and the ﬁrst provable boosting algorithm
                     ]
    and Schapire, 1997 , as two versions of a same    that fully exploits this class. This is important, as the prob-
    higher-level boosting algorithm. It may be used   lem of inducing oblique decision trees has a longstanding his-
    as the basic building block to devise simple prov- tory, and the algorithms available so far are either time con-
    able boosting algorithms for complex classiﬁers.  suming or complex to handle formally [Breiman et al., 1984;
    We provide one example: the ﬁrst boosting al-     Cant´u-Paz and Kamath, 2003; Heath et al., 1993; Murthy et
    gorithm for Oblique Decision Trees, an algorithm  al., 1994; Shawe-Taylor and Cristianini, 1998]. The follow-
    which turns out to be simpler, faster and signiﬁ- ing Section presents deﬁnitions; it is followed by a Section on
    cantly more accurate than previous approaches.    our theoretical results, and a Section that discusses them and
                                                      gives our boosting algorithm for oblique decision trees; two
1  Introduction                                       last Sections discuss experiments, and conclude. For the sake
Loosely speaking, a boosting algorithm repeatedly trains of clarity, proofs have been postponed to an appendix.
moderately accurate learners from which it gets its weak hy-
potheses, and combines them to output a strong classiﬁer 2 Deﬁnitions and problem statement
which boosts the accuracy to arbitrary high levels [Kearns Bold-faced variables such as w and x, represent vectors.
and Valiant, 1989]. A pioneering paper [Schapire, 1990] Unless otherwise stated, sets are represented by calligraphic
proved the existence of such boosting algorithms, and another upper-case alphabets, e.g. X , and (unless explicitely stated)
one drew the roots of their most popular representative: Ad- enumerated following their lower-case, such as {xi : i =
aBoost, that builds a linear combination of the predictions of 1, 2, ...} for vector sets, and {xi : i =1, 2, ...} for other sets.
the weak hypotheses [Freund and Schapire, 1997]. Another Consider the following supervised learning setting. Let X
paper due to [Kearns and Mansour, 1999] proved that some of denote a domain of observations of dimension n,suchasRn,
the most popular top-down decision tree induction algorithms {0, 1}n, etc. . We suppose a set S of |S| = m examples, with
                                [
are also boosting algorithms in disguise Breiman et al., 1984; |.| the cardinal notation, deﬁned as S = {si = xi,yi∈
Quinlan, 1993]. These two kinds of algorithms are outwardly X×{−1, +1} : i =1, 2, ..., m}, onto which a discrete dis-
different from each other: while AdaBoost repeatedly modi- tribution w1 is known. “+1” is called the positive class, and
ﬁes weights on the training examples and directly minimizes “-1” the negative class. Our primary objective is related to
aconvexexponential loss, top-down decision tree induction building an accurate classiﬁer (or hypothesis) H : X→R.
algorithms do not modify weights on the examples, and they The goodness of ﬁt of H on S may be evaluated by two quan-
minimize the expectation of a concave, so-called permissible tities:
function [Kearns and Mansour, 1999].
                                                                     w
  This explains why the starting point of this paper is a quite ε(H,  1)=Ew1      (1[sign(H(x))=y]) , (1)
surprising result, as we prove that these two kinds of algo-         w                −     x
                                                              εexp(H, 1)=Ew1      (exp( yH(  ))) .    (2)
rithms are in fact the same algorithm, whose induction is per-
                                                                           ≥      −
formed on two different classiﬁer “graphs”. The apparent per- Here, sign(a)=+1iff a 0 and 1 otherwise, 1[.] is the 0/1
ceptual differences in the algorithms stem only from different indicator variable, and E.() is the expectation. (1) is the con-
structural properties of the classiﬁers. This suggests a generic ventional empirical risk, and (2) is an upperbound, the expo-
induction scheme which gives, for many classiﬁers that meet nential loss [Schapire and Singer, 1999]. We are interested in

                                                IJCAI-07
                                                   842   Input: S, w1;                           Input: S, w1;                  Input: S, w1;
   for t =1, 2, ..., T                     for t =1, 2, ..., T            for t =1, 2, ..., T

     ht ← Tree(S, wt);                       let  ∈ leaves(Ht−1) containing let  ∈ leaves(Ht−1) containing
        ←                    −      x
     αt    arg minα∈R Ewt (exp( yαht( )));         examples of both classes;       examples of both classes;

     for i =1, 2, ..., m                     ht ←  Stump(S, w1);            ht ← StumpLS(S, w1);

        wt+1,i ← wt,i exp(−αtyiht(xi))/Zt;    ←  ht;                         ← ht;
   AdaBoost + Trees                        TopDown  DT (CART, C4.5)       TopDown  ODT (OC1, SADT)

Figure 1: An abstraction of 3 core greedy procedures of some popular induction algorithms (see text for references and details).

the stagewise, greedy ﬁtting of H to S, from scratch. A large Figure 1 gives an abstract view of some of the most pop-
number of classes of classiﬁers have popular induction algo- ular induction algorithms, or at least of their core procedure
rithms whose core procedure follows this scheme, including which induces a large classiﬁer: AdaBoost with trees [Freund
Decision Trees (DT) [Breiman et al., 1984; Quinlan, 1993], and Schapire, 1997; Schapire and Singer, 1999],C4.5[Quin-
Branching Programs (BP) [Mansour and McAllester, 2002] lan, 1993],CART[Breiman et al., 1984],OC1[Murthy et
and Linear Separators (LS) [Freund and Schapire, 1997; al., 1994], SADT [Heath et al., 1993]; the ﬁrst algorithm for
Nock and Nielsen, 2006]. Virtually all these procedures share the induction of ODT dates back to the mid-eighties; it was a
another common-point: the components that are used to grow descendant of CART [Breiman et al., 1984]. Most of the in-
H are also classiﬁers. These are the DTs for AdaBoost with duction algorithms for DT or ODT integrate a pruning stage;
trees, the internal node splits for the DTs and BPs (akin to here, we are only interested in the greedy induction scheme.
decision stumps), etc.                                WL has various forms: it induces a decision tree on the whole
  Now, given some ﬁxed class of classiﬁers in which we ﬁt sample S and on a distribution which is repeatedly modiﬁed
H, here is the problem statement. A weak learner (WL) is for AdaBoost; it induces a decision tree with a single internal
assumed to be available, which can be queried with a sam- node (a stump) on the subset of S that reaches leaf  (noted
                                   
ple S and (discrete) distribution w on S ; the assump- S) and on a distribution which is not modiﬁed for CART and
tion of its existence is called a Weak Learning Assump- C4.5; it replaces this ordinary, axis-parallel stump, by a linear
tion (WLA). It returns in polynomial time1 a classiﬁer h separator stump (also called stump for the sake of simplicity)
on which only the following can be assumed: ε(h, w) ≤ for OC1 and SADT. The choice of a split for DT and ODT re-
1/2 − γ for some γ>0     [Schapire and Singer, 1999]. lies on the repetitive minimization of an “impurity” criterion
Given 0 <<1, is it possible to build in polynomial   which is the expectation, over the leaves of the current tree
time some H with ε(H, w1) ≤  , after having queried T H, of a so-called permissible function [Kearns and Mansour,
times WL for classiﬁers h1,h2, ..., hT ,forsomeT>0 ?  1999]:
Provided additional assumptions are made for its generaliza-                                   
                                                                                            w+
tion abilities (see Section “Discussion” below), this algorithm w                            1,S
                                                         εimp(H,  1,f)=E∼leaves(H)     f            ; (3)
is called a boosting algorithm [Freund and Schapire, 1997;                                  w1,S
Kearns and Valiant, 1989].
                                                                                     
                                                                                   S    w        +
                                                      here, w1,S is the total weight of in 1 and w.  is its
3  Unifying boosting properties                       weight further restricted to the positive class. In the expec-
For the sake of clarity, we now plug T in subscript on tation, the weight of a leaf  is w1,S . The permissible func-
                                             x        tion f :[0, 1] → [0, 1] has to be concave, symmetric around
H; An element of LS is a classiﬁer HT with HT ( )=
  T       x          ∈ R                              1/2, and with f(0) = f(1) =  0 and f(1/2) =  1.Ex-
  t=1 αtht( ),whereαt    is a leveraging coefﬁcient that amples of permissible functions are f(z)=4z(1 − z) for
may be interpreted as a conﬁdence in ht. An element of DT is Gini index [Breiman et al., 1984], f(z)=−z log z − (1 −
a rooted directed tree in which each internal node supports a z) log(1 − z) for the entropy [Quinlan, 1993] (log base-2), or
single boolean test over some observation variable, and each       
                                                                        −                         [
leaf is labeled by a real. The classiﬁcation of some obser- even f(z)=2 z(1 z) for the optimal choice of Kearns
                                                      and Mansour, 1999]. Remark that we have εimp(H, w1,f) ≥
vation proceeds by following, from the root, the path whose  w        {    −  }          w
tests it satisﬁes, until it reaches a leaf which gives its class. εimp(H, 1, 2min z,1 z )=2ε(H, 1), for any per-
                                                      missible , and so minimizing (3) amount to minimizing
Figure 2 (left) presents an example of DT (n =2), where       f
                                                      the empirical risk of H as well. The reason why this is
v1,v2 are Boolean description variables. Finally, Oblique De-
cision Trees (ODT) generalizes DT, as linear combinations of a better choice than focusing directly on the empirical risk
                                                                                                  [
variables are authorized for the internal nodes, which allows comes from the concavity of the permissible function Kearns
                                                                      ]
splits that are oblique instead of just axis-parallel [Breiman et and Mansour, 1999 . Though seemingly very different from
al., 1984].                                           each other, AdaBoost and DT induction algorithms from the
                                                      CART-family were independently proven to be boosting al-
  1For the sake of simplicity, polynomial time means polynomial gorithms [Freund and Schapire, 1997; Kearns and Mansour,
in the relevant parameters throughout the paper.      1999; Schapire and Singer, 1999]. Sofar,nosuchformal

                                                IJCAI-07
                                                   843                                       h
                                         1                Input: S, w1;
        v2 =0     v2 =1          v2 =0     v2 =1
                                                          for t =1, 2, ..., T
                                h                                   S
                    +1            2          h3             compute   t;
 v1 =0    v1 =1           v1 =0    v1 =1
                                                            ht ← WL(St,  wt);
                                                            for i =1, 2, ..., m
    +1    −1                 h4    h5                                        −      x   
                                                                            1 (μtyiht( i)/ht ) ∈S
                                                                                  − 2      if si  t
                                                              w    ←w    ×       1 μt                  ;
                        (α1,h1)=(−2, +1)                       t+1,i   t,i                     ∈S\S
                        (α2,h2)=(3, +1)(α3,h3)=(3, +1)                            1        if si     t
                        (α4,h4)=(0, +1)(α5,h5)=(2, −1)    for t =1, 2, ..., T
                                                               ←                      −
                                                            αt    (1/(2ht )) ln((1 + μt)/(1 μt));
Figure 2: A DT with 3 leaves and 2 internal nodes (left) and GenericGreedy (G2)
an equivalent representation which ﬁts to (A) (right).

                                                      Figure 3: The generic greedy induction of HT . The compu-
boosting algorithm or proof exist for ODT that would take tation of St depends on the decision graph of Ht.
full advantage of their structure combining oblique stumps

and decision trees. For this objective, let us shift to a more                                    
                                                      We also extend the weight notation, and let wt ,Sp =
general standpoint on our problem, and address the following                         
                                                             w    and w  St =        w    (with p ∈Pand
question: what kind of classiﬁers may be used to solve it ? si∈Sp t ,i t ,      si∈St  t ,i
                                                        ≤   ≤                                 |  x  |∈
Consider the following assumption, that represents some 1 t   T +1). We also deﬁne ht =maxsi∈St  ht( i)
                                  HT                  R                                 S
kind of local linear classiﬁer:                         , the maximal absolute value of ht over t.After[Nock and
                                                      Nielsen, 2006], we deﬁne two notions of margins:theﬁrstis
    (assumption A) ∀T>0, denote HT = {h1,h2, ..., hT }
                                                      the normalized margin ht over S:
    the set of outputs of WL; then, there exists a function               
                                                                     1
    which maps any observation to a non-empty subset of    μ   =              w   y h (x ) ∈ [−1, +1] . (6)
    H         X→     HT \{∅}  ∀x ∈X        x                t                  t,i i t i
      T , gHT :     2       .        , gHT ( ) is re-              h wt,St
                                                                    t     si∈St
    quired to be computable polynomially in n and the size
    of HT ; it describes the classiﬁcation function of HT ,in With the help of this deﬁnition, Figure 3 presents the generic
    the following way:                                greedy induction of HT . Remark that when HT is a linear
                                                     separator, G2 is the AdaBoostR boosting algorithm of [Nock
        HT (x)=              αtht(x) , ∀x ∈X .        and Nielsen, 2006]. The second margin deﬁnition is the mar-
                      ∈    x                                             x  
                    ht gHT ( )                        gin of HT on example ,y  [Nock and Nielsen, 2006]:

                                                           νT (x,y) =   tanh(yHT (x)/2) ∈ [−1, +1] . (7)
By means of words, the classiﬁcation of HT is obtained by
                                   H
summing the outputs of some elements of T . Many classi- This margin comes to mind from the relationships between
ﬁers satisfy (A), such as decisition trees, decision lists, linear boosting and the logistic prediction HT (x) = log(Pr[y =
separators, etc.                                      +1|x]/Pr[y = −1|x]) [Friedman et al., 2000]. In this case,
                                                                     x                    |x −
Deﬁnition 1 The decision graph of HT (on X )isanoriented (7) becomes νT ( ,y )=y(Pr[y =+1     ]   Pr[y =
                                                     −  |x
graph G =(HT  , E);anarc(ht,ht ) ∈Eiff t<t and there   1  ]), whose expectation is the normalized margin (6) of
          x ∈X                 ∈     x                                     −
exists some      such that ht,ht  gHT ( ) and no ht   the Bayesian prediction in [ 1, +1] (also called gentle lo-
                   x                                                  [                   ]
with t<t  <t  is in gHT ( ).                          gistic approximation Friedman et al., 2000 ). Following
                                                      [Nock and Nielsen, 2006],wedeﬁnethemargin error of HT
Remark that G is acyclic, and g T maps each observation of
                          H                           (∀θ ∈ [−1, +1]):
X to some path of G.WealsodeﬁneP   as representing the
                               X
set of paths of G that is mapped from by gHT :                   νw1,HT ,θ =   Ew1 (1[νT (s)≤θ]) ,    (8)
      P       { ⊆H     ∃x ∈X           x }                              w    ≤
          =    p    T :       ,p=  gHT ( ) .          and we have ε(HT ,  1)    νw1,HT ,0. Thus, minimizing
                                                                                           w
                                                      νw1,HT ,0 would amount to minimize ε(HT , 1) as well. The
 The simplest case of G is obtained when g T (x)=
                                         H            following Theorem shows much more, as under some weak
HT , ∀x ∈X: HT  is a linear separator, and G asinglepath
                                                      conditions, we can show that νw1 T is minimized for all
from h to h . Examples of classes of classiﬁers different                          ,H ,θ
      1    T                                          values of θ ∈ [−1, +1) (and not only for θ =0).
from linear separators and that ﬁt to (A) include DT and ODT.
In this case, G is a tree and weak hypotheses are in fact con- Theorem 1 After T ≥ 1 queries of WL, the classiﬁer ob-
                                                      tained, H , satisﬁes:
stant predictions on the tree nodes. Figure 2 (right) displays T          
the way we can represent a DT so as to meet (A). Remark                                
   
                                                                  ≤  1+θ    ×                  −   2
that there exists many possible equivalent transformations of νw1 T             w    Sp       1  μ   , (9)
                                                             ,H ,θ   1 − θ        T +1,            t
a DT; the way we induce HT shall favor a single one out of                   p∈P        ht∈p
all, as follows. We deﬁne two types of subsets of S (with
                                                      for any θ ∈ [−1, +1].
p ∈Pand  1 ≤ t ≤ T +1):
                                                      (proof in appendix). To read Theorem 1, consider the follow-
        S       {x   ∈S            x  }
         p  =      i,yi    : p = gHT ( i) ,     (4)   ing WLA for real-valued weak hypotheses, borrowed from
        S       ∪       S
         t  =    p∈P:ht∈p p .                   (5)   [Nock and Nielsen, 2006]:

                                                IJCAI-07
                                                   844           | |≥                  ∀ ≥                                   −    x                   2
    (WLA)  μt   γ,forsomeγ>0    ( t  1).              arg minR Ew1 (exp( yht( ))) (Figure 1), and G matches
                                                      exactly discrete AdaBoost [Freund and Schapire, 1997].
Under the WLA, Theorem    1 states that νw1 T    ≤
                                          ,H ,θ         Thus, decision trees and linear separators are somehow ex-
K  exp(− min ∈P |p|γ2/2), with K constant whenever θ
  θ         p                  θ                      tremal classiﬁers with respect to G2. Finally, when H is a
is a constant ∈ [−1, +1). In other words, provided the in-                                          T
                                                      linear separator without restriction on the weak hypotheses,
duction in G2 is performed so as to keep paths with roughly
                                                        2 specializes to AdaBoostR [Nock and Nielsen, 2006].
equal size, such as by a breadth-ﬁrst induction of the decision G
graph, the margin error is guaranteed to decrease exponen- 4.2 All boosting algorithms
tially fast. To see how G2 ﬁts to tree-shaped decision graphs,
consider the following assumption:                    In the original boosting setting, the examples are drawn in-
                           ∈H                         dependently according to some unknown but ﬁxed distribu-
    (assumption B) (i) each ht T is a constant, and (ii) tion D over X , and the goal is to minimize the true risk
    G is a rooted tree.
                                                      ε(HT ,D) with high probability, i.e. we basically wish that
Assumption (B) basically restricts HT to be a tree of any arity, ε(HT ,D) ≤  with probability ≥ 1 − δ over the sampling of
still in which any kind of classiﬁer may be used to split the S [Freund and Schapire, 1997; Kearns and Valiant, 1989].
                                    +/−
internal nodes. As in (3), we use notation w. as the index’ Two sufﬁcient conditions for a polynomial time induction
weight for class “+1” or “-1”, respectively.          algorithm to satisfy this constraint are (i) return HT with
                                                      ε(H  , w )=0, and (ii) prove that structural parameters of
Theorem 2 Suppose that (A) and (B) hold. Then:            T   1
                                                      the class of classiﬁers to which HT belongs satisfy particular
                               +                      bounds [Freund and Schapire, 1997; Shawe-Taylor and Cris-
                          1   w1,St
              HT (x)=       ln −    ,          (10)   tianini, 1998]. Theorem 1 is enough to prove that (i) holds
                          2   w                                                               2
                               1,St                   under fairly general conditions for algorithm G in Figure 3
                                                      provided WLA holds. For example, T =(2/γ2)log(1/) it-
∀x ∈Xand     h is the leaf of g T (x). Furthermore, (2)                           2
              t             H                                                  2/γ
simpliﬁes as:                                         erations for LS and T =(1/) for DT are enough to have
                                                            w   ≤
                                                   ε(HT ,  1)   from Theorem 1. Fixing <mini w1,i easily
                                                     yields (i). The bound for LS is the same as AdaBoost (dis-
                           w+          w+
         w               ×     1,St   −   1,St       crete or real) [Schapire and Singer, 1999], while that for DT
 εexp(HT , 1)=      w1,St  2         1         (11).
                              w1,St      w1,St                                           [
             ht leaf of G                             improves upon the exponent constant of Kearns and Man-
                                                     sour, 1999]. Finally, (ii) is either immediate, or follows from
                         w         −
             =   εimp(HT , 1, 2 z(1  z)) .     (12)   mild assumptions on G and WL. As a simple matter of fact,
                                                      (i) and (ii) also hold when inducing ODT with G2.
(proof in appendix).
                                                      4.3  Recursive Boosting and Oblique Decision
4  Discussion                                              Trees
4.1  Kearns and Mansour’s algorithm, AdaBoost         The preceeding Subsections suggest that G2 could be used
           2
     and G                                           not only to build HT , but also as the core procedure for WL.
The similarity between (12) and (3) with f(z)=2 z(1 − z) For example, it comes from Theorem 2 that AdaBoost + trees
                                                                                             2
is immediate, and quite surprising as it shows the identity be- without pruning (Figure 1) is equivalent to G growing lin-
                                                                                   2
tween a convex loss and the expectation of a concave loss. ear separators, in which WL is G growing a decision tree,
However, this is not a coincidence. Indeed, Theorem 2 shows in which WL returns any constant weak hypothesis. In the
a rather surprising result: the choice of the weak hypotheses general case, we would obtain a recursive/composite design
                                                      of the “master” G2,viaG2 itself, and the recursion would
does not impact at all on HT (see (10)). When (A)and(B)
                                                      end until we reach a WL that can afford exhaustive search for
hold, the only way to modify HT is thus through its deci-
sion graph, i.e. on the choice of the splits of the tree. There simple classiﬁers (e.g. axis-parallel stumps, constant classi-
                                                                                       2            2
is a simple way to choose them, which is to do the same ﬁers, etc.), instead of calling again G .However,G can
thing as the most popular LS boosting algorithms [Friedman also be used to build the decision graph of HT ,inthesame
et al., 2000; Schapire and Singer, 1999]: repeatedly mini- recursive fashion. Consider for example the class ODT. The
mize the exponential loss in (2). Because of Theorem 2, this internal nodes’ splits are local classiﬁers from LS that decide
amounts to the minimization of the impurity criterion in (3) the path based on the sign of their output, or equivalently, on
                   −                                  the class they would give. This suggest to build the tree with
with f(z)=2    z(1   z).Thisisexactly the DT induc-     2
                                                      G  on both the linear separators in the internal nodes of HT
tion algorithm proposed by [Kearns and Mansour, 1999] that S
meets the representation optimal bound.               (use   to split leaf , where the linear separator uses ordi-
                                                      nary decision stumps), and on the tree shape as well. Call
  On the other hand, when  HT  is a linear separator,
there is no inﬂuence of the decision graph on the induc- BoostODT this ODT induction algorithm. It turns out that it
tion of H  as it is a single path from h to h .The    brings a boosting algorithm, that takes full advantage of the
         T                           1     T          ODT structure. However, this time, it is enough to assume the
only way to modify HT  is thus through the choice of
the weak hypotheses. Suppose that each weak hypothe-  WLA  one level deeper, i.e. only for the stumps of the linear
sis has output restricted to the set of classes, {−1, +1}. separators, and not for the splits of the oblique decision tree.
In this case, αt =(1/2) ln((1 − ε(ht, wt))/ε(ht, wt)) = Theorem 3 BoostODT is a boosting algorithm.

                                                IJCAI-07
                                                   845 100              100             100
          Xd6              Xd6             Xd6         Domain               BoostODT       OC1   AdaBoost
  80              80               80                                     T1 =50  T1 = 200          +C4.5
  60              60               60                  Adult-strech         0.00     0.00  0.15      0.00
  40              40               40                  Breast-cancer       28.67    27.27 37.76     33.92
  20              20               20                  Breast-cancer-w.     3.72     3.86  6.01      4.43
  0                0               0                   Bupa                28.12    28.12 37.97     31.59
  -1  -0.5  0  0.5  1 -1 -0.5  0  0.5  1 -1 -0.5  0  0.5  1 Colic          17.66    18.21 23.91     18.21
                                                       Colic.ORIG          13.86    15.76 16.03     32.07
                                                       Credit-a            14.64    14.93 20.43     15.51
Figure 4: Margin distributions for BoostODT (T1 =50)on Credit-g            26.50    25.40 31.10     28.40
domain XD6, with 10% (left), 20% (center) and 30% class Diabetes           25.91    23.44 34.64     29.82
noise (right). Stair-shaped (bold plain) curves are the theoret- Hepatitis 20.65    18.71 20.65     18.71
ical margins for the logistic model (see text).        Ionosphere           7.41     6.27  9.12      7.41
                                                       Kr-vs-kp             3.38     3.35  3.69      0.69
                                                       Labor               10.53    10.53 15.79     12.28
The proof, omitted due to the lack of space, builds upon The- LEDeven      15.25    14.75  9.75     10.50
orem (1) plus Lemma 2.1 in [Mansour and McAllester, 2002], LEDeven+17      22.75    21.50 38.25     30.75
and structural arguments in [Freund and Schapire, 1997; Monks1             25.36    25.36  0.00      2.16
Shawe-Taylor and Cristianini, 1998]. The proof emphasizes Monks2            1.00     0.67  0.33     26.96
the relative importance of sizes: suppose that each linear sep- Monks3      1.44     2.17  2.71      2.53
                                                       Mushroom             0.00     0.00  0.05      0.00
arator contains the same number of weak hypotheses (T1),
and the tree is complete with T internal nodes; then, having Parity        47.66    47.27 46.88     44.14
                         2                             Sick                 2.15     1.91  2.41      1.09
                              2
          T1 log T2 =   Ω((1/γ ) log(1/))             Sonar               13.94    12.50 33.17     18.27
                                                       Vot e                3.91     3.45  4.37      5.29
is enough to have ε(HT , w1) ≤ . From an experimental XD6                 20.57    18.77  5.33      5.40
standpoint, this suggests to build trees with T1  T2. Yellow-small         0.00     0.00  0.00      0.00
                                                       #Wins (T1 =50)   17(10/4)           5(3)      8(3)
5  Experiments                                         #Wins (T1 = 200)          18(10/7)  5(3)      8(3)
We have performed comparisons on a testbed of 25 domains
with two classes, most of which can be found on the UCI Table 1: Results on 25 domains. For each domain, bold
repository of ML databases [Blake et al., 1998]. Comparisons faces indicate the lowest errors. In each row “#Wins
are performed via ten-fold stratiﬁed cross-validation, against (T1 = z)”, bold faces denote the number of times the
OC1 and AdaBoost in which the weak learner is C4.5 un- corresponding algorithm in column is the best over three
                               ∈{       }
pruned. BoostODT was ran for T1   50, 200 (the weak   columns: BoostODT(T1  =  z), OC1 and AdaBoost+C4.5
hypotheses of the linear separators are decision stumps) and (z ∈{50, 200}). Furthermore, the four numbers in parenthe-
T2 =4. To make fair comparisons, we ran AdaBoost for  ses in each row are the number of signiﬁcant wins (student
a total number of T =5boosting iterations. This brings paired t-test, p = .05), for BoostODT vs OC1, BoostODT vs
fair comparisons, as an observation is classiﬁed by 5 nodes AdaBoost+C4.5, and OC1 vs BoostODT, AdaBoost+C4.5 vs
(including leaves) in BoostODT, and 5 unpruned trees in Ad- BoostODT (from left to right).
aBoost. Before looking at the results, BoostODT has proven
to be much faster than OC1 in practice (ten to hundred times
                                                      4 its margin error curves on domain XD6 with variable class
faster). OC1’s time complexity is O(nm2 log m) [Murthy
                                                                   [                    ]
et al., 1994], without guarantee on its result, while Boos- noise (see e.g. Nock and Nielsen, 2006 for a description of
                                                      the domain), averaged over the test folds [Nock and Nielsen,
tODT’s is O(nm) under the WLA, with the guarantee to
                                                          ]
reach empirical consistency in this case. Complexity reduc- 2006 .Themargin curve obtained is compared to that of the
                                                                        [                   ]
tions have almost the same order of magnitude with respect to logistic prediction of Friedman et al., 2000 , which can be
SVM based induction of ODT [Shawe-Taylor and Cristianini, computed exactly. The approximation of the logistic model
1998]. Table 1 summarizes the results obtained. With rejec- by BoostODT is quite remarkable. Indeed, its margin curves
tion probabilities p ranging from less than .05 to less than display the single stair-shape of a theoretical logistic model
                                                      for a domain XD6 with 8-13% additional class noise, uni-
.005 for the hypothesis H0 that BoostODT does not perform
better, the four sign tests comparing both runs of BoostODT formly distributed among the ODT leaves.
to its opponents are enough to display its better performances,
and this is conﬁrmed by student paired t-tests. There is more: 6Conclusion
we can tell from simulated domains that BoostODT performs Perhaps one main contribution of this paper is to show that
as better as the domain gets harder. It is the case for the formal boosting is within reach using the same uniﬁed algo-
Monks domains, and the LEDeven domains. BoostODT is   rithm, for a wide variety of formalisms not restricted to the
indeed beaten by both opponents on LEDeven, while it beats most popular included in this paper (such as decision lists
both on LEDeven+17 (=LEDeven +17 irrelevant variables). [Rivest, 1987], simple rules [Nock, 2002], etc. ). Another
  Looking at these simulated domains, we have drilled down contribution, quite surprising, is to show that a boosting al-
the results of BoostODT. Using (8), we have plotted on Figure gorithm follows immediately even for complex combinations

                                                IJCAI-07
                                                   846