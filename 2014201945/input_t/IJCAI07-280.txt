  Morphological Annotation of a Large Spontaneous Speech Corpus in Japanese

                               Kiyotaka Uchimoto     and Hitoshi Isahara
                   National Institute of Information and Communications Technology
                    3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
                                     {uchimoto,isahara}@nict.go.jp


                    Abstract                          words because each long word consists of one or more short
                                                      words. Approximately one-eighth of the words have been
    We propose an efﬁcient framework for human-       manually detected, and morphological information such as
    aided morphological annotation of a large sponta- part-of-speech (POS) categories and conjugation types have
    neous speech corpus such as the Corpus of Spon-   been assigned to them. Human annotators tagged every mor-
    taneous Japanese. In this framework, even when    pheme in the one-eighth of the CSJ that had been tagged, and
    word units have several deﬁnitions in a given cor- other annotators checked them. The human annotators dis-
    pus, and not all words are found in a dictionary or in cussed their disagreements and resolved them. The accura-
    a training corpus, we can morphologically analyze cies of the manual tagging of short and long words in the
    the given corpus with high accuracy and low labor one-eighth of the CSJ were both approximately 99.9%. The
    costs by detecting words not found in the dictionary accuracies were evaluated by random sampling. Because it
    and putting them into it. We can further reduce la- took over two years to tag one-eighth of the CSJ accurately,
    bor costs by expanding training corpora based on  tagging the remainder with morphological information would
    active learning.                                  take about twenty years. Therefore, the remaining seven-
                                                      eighths of the CSJ were tagged semi-automatically. In this
1  Introduction                                       paper, we describe methods for detecting the two types of
The “Spontaneous Speech: Corpus and Processing Technol- word segments and corresponding morphological informa-
ogy” project sponsored the construction of a large sponta- tion. We also describe how to accurately tag a large spon-
neous Japanese speech corpus, the Corpus of Spontaneous taneous speech corpus. We propose an efﬁcient framework
Japanese (CSJ) [Maekawa et al., 2000]. The CSJ is a collec- for human-aided morphological annotation of a large sponta-
tion of monologues and dialogues, the majority being mono- neous speech corpus.
logues such as academic presentations and simulated public We collectively call short and long words morphemes.We
speeches. The simulated public speeches are short speeches use the term morphological analysis for the process of seg-
presented speciﬁcally for the corpus by paid non-professional menting a given sentence into a row of morphemes and as-
speakers. The CSJ includes transcriptions of the speeches signing to each morpheme morphological attributes such as a
as well as audio recordings of them. One of the goals of POS category.
the project is to detect two types of word segments and cor-
responding morphological information in the transcriptions. 2 Framework of Morphological Annotation
The two types of word segments were deﬁned by the mem-
bers of the National Institute for Japanese Language and are We call a series of processes for morphological analysis and
called short words and long words.Thetermshort word ap- maintenance of a corpus morphological annotation.Our
proximates an item found in an ordinary Japanese dictionary, framework of morphological annotation is illustrated in Fig-
and long word represents various compounds. The length and ure 1. The purpose of this framework is, given an annotated
morphological information of each are different, and every corpus and a large raw corpus, to improve the quality of the
short word is included in a long word, which is shorter than a annotated morphological information in both corpora with
Japanese phrasal unit, a bunsetsu. For example, the short and low labor costs. In this framework, a corpus-based morpho-
long words in “keitaisokaiseki ni tsuite ohanasi itashimasu” logical analyzer is used because the deﬁnition of a set of part-
(I would like to talk about morphological analysis) are repre- of-speech categories and that of word units are often changed
sented as shown in Table 1. In this table, each line indicates in the middle of constructing a corpus, and a morphological
a short word, and ten short words and four long words are analyzer must be robust to such changes.
shown.                                                  The framework of morphological annotation consists of
  Approximately 7.5 million short words were detected in three parts: maintenance of an annotated corpus, analysis of
the CSJ, which makes it the largest spontaneous speech cor- a large raw corpus, and enhancement of linguistic resources.
pus in the world. On the other hand, there were fewer long They are described in more detail in the following sections.

                                                IJCAI-07
                                                  1731                               Table 1: Example of morphological analysis results.
                       Short word                    Label                   Long word
OT   DicForm Lemma   PT   POS ConjType ConjForm Other     OT    DicForm Lemma  POS ConjType ConjForm Other
keitai keitai keitai (form) keˆtai Noun               Ba  keitaiso- keitaiso- keitaisokaiseki Noun
so   so    so  (element) so Sufﬁx                      I  kaiseki kaiseki (morphological
kaiseki kaiseki kaiseki (analysis) kaiseki Noun       Ia              analysis)
ni   ni    ni        ni   PPP              case marker B  nitsuite nitsuite nitsuite PPP         case marker
tsui tsuku tsuku (relate) tsui Verb KA-GYO ADF euphonic change I      (about)                    & compound
te   te    te        te   PPP              conjunctive I                                         word
oo         o         oPreﬁx                           B   ohanashi- ohanashi- ohanashiitasu Verb SA-GYO ADF
hanashi hanasu hanasu (talk) hanashi Verb SA-GYO ADF  Ia  itashi itasu (talk)
itashi itasu itasu   itashi Verb SA-GYO ADF           Ia
masu masu  masu      masu AUX       ending form       Ba  masu  masu  masu     AUX       ending form
OT: orthographic transcription, DicForm: dictionary form written in kanji and hiragana characters, Lemma: lemma written
in kanji and hiragana characters, POS: part-of-speech, PT: phonetic transcription written in katakana characters, ConjType:
conjugation type, ConjForm: conjugation form. PPP: post-positional particle, AUX: auxiliary verb, ADF: adverbial form.

                                                      corpus. This method was initially applied to the CSJ, and the
                                                      morphological information replaced by the method was man-
                                                      ually examined.

                                                      2.2  Analysis of a Large Raw Corpus and
                                                           Enhancement of Linguistic Resources
                                                      Enhancement of the Dictionary
                                                      When a given raw corpus including unknown words that are
                                                      in neither the dictionary nor the training corpus, detection er-
                                                      rors tend to occur on the unknown words and words to the left
                                                      and right of the unknown words. In many cases, not a single
                                                      unknown word but a series of unknown words are found, or
                                                      an unknown word may consist of known words and unknown
                                                      characters. In this case, the accuracy of morphological infor-
                                                      mation assigned to a raw corpus can be increased by detecting
    Figure 1: Framework of morphological annotation.  and putting the unknown words into the dictionary and man-
                                                      ually examining words whose probability is estimated as low
                                                      [Uchimoto et al., 2003].
2.1  Maintenance of an Annotated Corpus                 The cost of manual examination is high when word seg-
                                                      ments and their POS categories have several deﬁnitions. Be-
In general, when an annotated corpus or a training corpus cause the CSJ has two types of word deﬁnition, the cost would
has many errors, a corpus-based analyzer easily overﬁts the double. However, when one type of word segment consists of
errors, and the analysis accuracy often decreases. There- compounds of other types of words, the manual examination
fore, the errors should be detected and corrected to avoid of the shortest word segments improves the morphological
overﬁtting them. The simplest way to detect and correct analysis accuracy for other types of words if we use a method
the errors in a training corpus is to examine the difference based on a chunking model [Uchimoto et al., 2003].
between the original annotated corpus and the corpus auto- Besides the above methods, in the morphological analy-
matically annotated by a corpus-based analyzer trained by sis of the CSJ, the number of unknown words was further
using the original annotated corpus. In previous studies, reduced by expanding conjugational words in a dictionary
a boosting method and an anomaly detection method were based on a conjugation chart developed by the members of
applied to detect errors in a corpus [Abney et al., 1999; the National Institute for Japanese Language.
Eskin, 2000], and a method for detecting and correcting er-
rors in a corpus was proposed [Murata et al., 2002].Inthe Active Sampling
method for detecting and correcting errors, the difference be- In general, a corpus-based morphological analysis system re-
tween the annotated labels of the original corpus and those of quires a large training corpus. However, the accuracy does
the automatically annotated corpus is ﬁrst detected. For each not improve in proportion to the simple increase of the train-
difference, the probabilities of the labels in the original cor- ing corpus. This is because a model for morphological analy-
pus and the automatically annotated corpus are calculated by sis usually considers the relationship between adjacent words,
a model trained using the original annotated corpus. Then, and the simply supplemented data rarely includes relation-
the labels in the original corpus whose probabilities are lower ships that were not found in the initial training corpus. There-
than those in the automatically annotated corpus are replaced fore, when expanding a training corpus, data must be se-
with the corresponding labels in the automatically annotated lected that include as many sequences of words as possible

                                                IJCAI-07
                                                  1732that are difﬁcult for the morphological analysis model to ana- [Uchimoto et al., 2001]. Their method uses a model that es-
lyze. The expansion should be done so that a big improve- timates how likely a string is to be a morpheme as its proba-
ment can be achieved with as small a supplement as pos- bility, and thus, it has the potential to overcome the unknown
sible. Argamon-Engelson et al. reported that data that can word problem. Therefore, we used their method to morpho-
usefully be added to training data can be selected by extract- logically analyze the CSJ.
ing sentences whose analysis results obtained by using ran- In this paper, we assume that two types of word segments,
domly selected models include a great amount of consistency short and long words, are deﬁned and every long word con-
[Argamon-Engelson and Dagan, 1999].However,theyas-    sists of one or more short words, as in the CSJ.
sumed that word boundaries were given and that the data had
                                                      Method Based on a Morpheme Model
no unknown words. Their method cannot be simply applied
to Japanese sentences when word boundaries are inconsistent Given a tokenized test corpus, namely, a set of strings, the
because no blank spaces are used between words in Japanese problem of Japanese morphological analysis can be reduced
sentences. In our research, we assumed that word boundaries to that of assigning one of two tags to each string in a sen-
were not given and that the data had unknown words. This tence. A string is tagged with a 1 or a 0 to indicate whether it
section describes an active sampling method using a single is a morpheme. When a string is a morpheme, a grammatical
statistical model for expanding a training corpus.    attribute is assigned to it. A tag of 1 is thus assigned one of a
                                                      number, n, of grammatical attributes assigned to morphemes,
  The active sampling is conducted as follows, under three                                         0
assumptions. Under these assumptions, a set of sentences that and the problem becomes assigning an attribute (from to n)
                                                     to every string in a given sentence.
minimize   p is extracted, and words in the extracted sen-
tences whose probabilities are below a threshold are exam- We deﬁne a morpheme model that estimates the likeli-
                                                     hood that a given string is a morpheme and has a grammat-
ined. Here, p is the product of the probabilities estimated        (1 ≤   ≤   )
by the morpheme model for all words in a set of sentences. ical attribute, i i n . We implemented this model
                                                                                      [
The three assumptions are as follows.                 within an ME modeling framework  Jaynes, 1957; 1979;
                                                      Berger et al., 1996]. The model is represented by Eq. (1):
 1. Similar errors tend to appear in the same speech.                                      
                                                                         exp            (   )
    This is because speciﬁc words or sequences of words                         i,j λi,j gi,j a, b
                                                             pλ(a|b)=                                 (1)
    may only appear in a certain speech. Therefore, the data                     Zλ(b)
    should be compiled from as varied speeches as possible                      ⎛              ⎞
                                                                                
    to avoid examining the same erroneous words. That is        ( )=        exp ⎝          (   )⎠
    why x% of words in each speech were examined. The         Zλ b                   λi,j gi,j a, b , (2)
    maximum number of examined words was chosen to be                     a       i,j
    n×x
     100 when the number of words in a speech was n.  where a (called a “future”) is one of the categories for clas-
 2. The sentence will have more errors if the product of siﬁcation, and it can be one of (n +1)tagsfrom0ton; b
    probabilities estimated for its words are low.    (called a “history”) is the contextual or conditioning infor-
    Therefore, we preferred sentences with low probabili- mation that enables us to make a decision among the space
    ties.                                             of futures; and Zλ(b) is a normalizing constant determined
                                                      by the requirement that a pλ(a|b)=1for all b.Thecom-
 3. Any word whose probability is over a certain threshold        ( | )
    can be considered correct.                        putation of pλ a b in any ME model is dependent on a set
                                                      of “features”, which are binary functions of the history and
    Therefore, words whose probabilities are over a thresh- future. For instance, one of our features is
    old are ignored when  p mentioned above is calcu-              
                                                                      1:ifhas(b, fj)=1&a    =  ai
    lated. The threshold should be set rather high to reduce                                         
    the chance that errors will be ignored. In our prelimi- gi,j (a, b)=  fj =“POS(−1)(Major) : verb  , (3)
    nary experiments, the threshold was set at 95% because            0: otherwise.
    the accuracy obtained using this threshold was 99% or
                                                      Here “has(b, fj)” is a binary function that returns 1 if the his-
    higher.
                                                      tory, b, has feature fj. The features used in our experiments
2.3  Models and Algorithms for Morphological          are described in Section 3.
                                                        Given a sentence, the probabilities of n tags from 1 to n are
     Analysis                                         estimated for each length of string in that sentence by using
One of the most important problems in morphological analy- the morpheme model. From all possible divisions of mor-
sis is that posed by unknown words, which are words found phemes in the sentence, an optimal one is found by using the
in neither a dictionary nor a training corpus. Two statisti- Viterbi algorithm or a branch and bound method. Each di-
cal approaches have been applied to this problem. One is to vision is represented as a particular division of morphemes
ﬁnd unknown words from corpora and put them into a dic- with grammatical attributes in a sentence, and the optimal di-
tionary (e.g., [Mori and Nagao, 1996]), and the other is to vision is deﬁned as a division that maximizes the product of
estimate a model that can identify unknown words correctly the probabilities estimated for each morpheme in the division.
(e.g., [Kashioka et al., 1997; Nagata, 1999]). Uchimoto et In the CSJ, transcriptions consist of basic forms and pro-
al. used both approaches. They proposed a morphological nunciations. The pronunciation is transcribed separately from
analysis method based on a maximum entropy (ME) model the basic form written in kanji and hiragana characters.

                                                IJCAI-07
                                                  1733Speech sounds are faithfully transcribed in katakana char- set of labels is obtained by ﬁnding a division that maximizes
acters as the pronunciation and represented as basic forms the product of the probabilities estimated for each label as-
in kanji and hiragana characters. The text we targeted for signed to each short word. The model is represented by Eq.
morphological analysis is the transcription in the CSJ. When (1). In the equation, a can be one of the four labels. The
all possible divisions of morphemes in a sentence are ob- optimal set of labels is found by using the Viterbi algorithm
tained, they are matched with the pronunciation part in each or a branch and bound method. On the other hand, SVM is
line representing a bunsetsu by using a dynamic programming a binary classiﬁer. Therefore, we expanded it to a multi-class
method. Then, morphemes whose phonetic transcription can- classiﬁer by using multi-class methods such as a pairwise
didates do not match the aligned pronunciation part are elim- method and a one-versus-rest method. The optimal label de-
inated before searching for the optimal division of the mor- termined using an SVM model is deterministically assigned
phemes.                                               to each short word. The features used in our experiments are
                                                      described in Section 3.
Method Based on the Chunking Model and
Transformation Rules                                    The transformation rules are automatically acquired from
                                                      the training corpus by extracting long words with con-
Long word segments and their POS information are detected
                                                      stituents, namely, short words, that are labeled only “B” or
by using a method described below after detecting short word
                                                      “I”. A rule is constructed by using the extracted long word
segments and their POS information by using a morpheme
                                                      and the adjacent short words on its left and right. For exam-
model.
                                                      ple, the rule shown in Figure 2 was acquired from the exam-
  Given the two types of word segments, the longer of which
                                                      ple shown in Table 1. This rule indicates that when the labels
consists of compounds of the shorter, the problem of detect-
                                                      “B”, “I”, and “I” are assigned to “ni” (post-positional parti-
ing long word segments and their POS information can be
                                                      cle), “tsui” (verb), and “te” (post-positional particle), respec-
reduced to the problem of assigning one of four labels, as ex-
                                                      tively, the combination “nitsuite” is transformed into a long
plained below, to each short word. We call the model that
                                                      word having the morphological information, a post-positional
estimates the likelihood of the four labels a chunking model.
                                                      particle, case marker, and compound word. If several differ-
We implemented this model within ME or support vector ma-
                                                      ent rules have the same antecedent part, only the rule with
chine (SVM) based modeling frameworks. The four labels
                                                      the highest frequency is chosen. If no rules can be applied to
are as follows.
                                                      a long word segment, rules are generalized in the following
Ba: The beginning of a long word, and the POS information steps.
    of the long word agrees with that of the short word.
                                                        1. Delete the posterior context
Ia: The middle or end of a long word, and the POS informa-
    tion of the long word agrees with that of the short word. 2. Delete the anterior and posterior contexts
B: The beginning of a long word, and the POS information 3. Delete the anterior and posterior contexts and lexical
    of the long word does not agree with that of the short information such as orthographic transcriptions, dictio-
    word.                                                 nary forms, lemmas, and phonetic transcriptions
I: The middle or end of a long word, and the POS informa- If no rules can be applied to a long word segment in any step,
    tion of the long word does not agree with that of the short the POS category of the leftmost constituent of the long word
    word.                                             is assigned to the long word.
The label assigned to the leftmost constituent of a long word The dictionary form and lemma of a long word are usually
is “Ba” or “B”. The labels assigned to the other constituents generated by concatenating those of short words. As for a
of long words are “Ia” and “I”. A short word that “Ba” or spoonerism, for example, in case of a long word “ipponbari”
“Ia” is assigned to has the same POS information as its cor- consisting of three short words, “ichi”, “hon”, and “hari”,
responding long word. Here, the POS information represents information on the phonetic transcriptions of the short words
a set of a POS category, conjugation type, conjugation form, is used to generate the dictionary form and the lemma of the
and other detailed information on POS, as shown in Table 1. long word when concatenating the short words.
For example, in the table, the labels (in the “label” column)
are assigned to the short words. If these labels are correctly
detected, the POS information of the long words can be ob- 3 Experiments and Discussion
tained from the short words to which “Ba” or “Ia” is assigned. 3.1 Experimental Conditions
In the example, the POS information can be detected for all
the long words except “nitsuite”. On the other hand, only the In our experiments, we used 868,243 short words and 721,978
long word segment information can be assigned to “nitsuite” long words for training and 63,039 short words and 51,699
even if the assigned labels for its constituents are correct be- long words for testing. Those words were extracted from the
cause it has additional POS information as a compound word one-eighth of the CSJ that already had been manually tagged.
that is different from the POS information of its constituents, The training corpus consisted of 377 speeches and the test
“ni”, “tsui”, and “te”. In this case, the POS information can corpus consisted of 19 speeches.
be obtained by using the transformation rules mentioned later. In our experiments, we used the basic forms and pronunci-
  A given sentence is labeled from its beginning/end of to its ations of transcriptions as the input for morphological analy-
end/beginning. When using an ME-based model, the optimal sis.

                                                IJCAI-07
                                                  1734           Anterior context        Before transformation       Posterior context    After transformation
  OT       kaiseki        ni         tsui           te         o                 nitsuite
  DicForm  kaiseki        ni         tsuku          te         o                 nitsuite
  Lemma    kaiseki        ni         tsuku          te         o                 nitsuite
  POS      Noun           PPP        Verb           PPP        Preﬁx         ⇒   PPP
  ConjType                           KA-GYO
  ConjForm                           ADF
  Other                   case marker euphonic change conjunctive                case marker, compound word
  Label    Ia             BI                        IB
                                     Antecedent part                                  Consequent part

                                   Figure 2: Example of transformation rule.

  Because the sentences in the corpus do not have boundaries In our experiments, SVM-based models were used to ana-
between them, we selected the places in the CSJ that were au- lyze long words because better results were obtained using
tomatically detected as pauses of 500 ms or longer and des- SVM-based models than ME-based models in our prelim-
ignated them as sentence boundaries. In addition to these, inary experiments. A SVM-based chunker, YamCha[Kudo
we used utterance boundaries as sentence boundaries. These and Matsumoto, 2001], was used to assign the four chunking
were automatically detected at places where short pauses (be- labels.
tween 50 and 200 ms) follow the typical sentence-ending We selected the following parameters for YamCha based
forms of predicates such as verbs, adjectives, and copulas. on our preliminary experiments.
  In the CSJ, bunsetsu boundaries, which are the boundaries • Degree of polynomial kernel: 3rd
of Japanese phrasal units, were manually detected. Fillers and •
disﬂuencies were marked with the labels (F) and (D). In the Analysis direction: Right to left
experiments, we eliminated the ﬁllers and disﬂuencies, but • Dynamic features: Two preceding chunk labels
we did use their positional information as features. We also • Multi-class method: One-versus-rest
used as features bunsetsu boundaries and the labels (M), (O),
(R), and (A), which were assigned to particular morphemes We used the following information as features of target
such as personal names and foreign words. Thus, the input words:
sentences for training and testing were character strings with- • Morphological information: orthographic transcription,
out ﬁllers and disﬂuencies, and both boundary information dictionary form, lemma, phonetic transcription, POS
and various labels were attached to them. Candidate mor-  category, conjugation type, conjugation form, and other
phemes that crossed bunsetsu boundaries were ignored be-  detailed POS information
cause morphemes do not cross them. The output was a se- • Boundary information: bunsetsu and various labels such
quence of morphemes with grammatical attributes, as shown as “ﬁller”.
in Table 1. The candidate morphemes whose POS informa-
                                                        •
tion corresponded to that of the correct morphemes were used The same information as for the target word for the four
as positive examples, and the others were used as negative ex- closest words, the two on the left and the two on the right
amples. We used the POS categories in the CSJ as grammati- of the target word.
cal attributes. We obtained 14 major POS categories for short Morphological information for approximately 10% of the
and long words. Therefore, a in Eq. (1) can be one of 15 tags long words was generated by applying transformation rules.
from 0 to 14 for short words.
  Next, we describe the features used with the morpheme 3.2 Results and Discussion
models in our experiments. Each feature consists of a type The results of the morphological analysis obtained by us-
and a value, and it corresponds to j in the function gi,j (a, b) ing morpheme models are shown in Table 2. In the table,
in Eq. (1). As the feature functions, we used the combinations OOV indicates the out-of-vocabulary rates, which were cal-
of features and futures that appeared three times or more in culated as the proportion of word and morphological infor-
the training corpus. The features used in our experiments are mation pairs that were not found in a dictionary to all pairs
basically the same as those that Uchimoto et al. used[Uchi- in the test corpus. The morphological information included
moto et al., 2003]. The main differences are as follows: orthographic transcriptions, dictionary forms, lemmas, POS
                                                      categories, conjugation types, conjugation forms, and other
Strings following to the right: One- and two-character detailed POS information. Recall is the percentage of mor-
    strings directly to the right of the target word, their phemes in the test corpus for which the segmentation and all
    character types, and the combinations of them and the morphological information were identiﬁed correctly. Preci-
    dictionary information on the target word were used as sion is the percentage of all morphemes identiﬁed by the sys-
    features.                                         tem that were identiﬁed correctly. The F-measure is deﬁned
Fine-grained categories of conjugation types and forms: as the harmonic mean of recall and precision.
    Conjugation types and forms were divided into ﬁne-  Table 2, except the bottom line, shows the results obtained
    grained categories according to the context.      when the output of a short word analysis was used as the input

                                                IJCAI-07
                                                  1735