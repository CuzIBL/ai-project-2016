      Feature Mining and Neuro-Fuzzy Inference System for Steganalysis of LSB 
                        Matching Stegangoraphy in Grayscale Images 

                                  Qingzhong Liu1, Andrew H. Sung1, 2
                                     1Department of Computer Science
                             1,2Institute for Complex Additive Systems Analysis
                               New Mexico Tech, Socorro, NM 87801, USA
                                         {liu, sung}@cs.nmt.edu
                   Abstract                               The literature does provide a few detectors for LSB
                                                        matching steganography. One of the first papers on
    In this paper, we present a scheme based on
                                                        detection of embedding by noise adding is the paper by
    feature mining and neuro-fuzzy inference system
                                                        Harmsen and Pearlman [Harmsen and Pearlman, 2003],
    for detecting LSB matching steganography in         wherein the measure of histogram characteristic function
    grayscale images, which is a very challenging
                                                        center of mass (HCFCOM), is extracted and a Bayesian
    problem in steganalysis. Four types of features
                                                        multivariate classifier is applied. Based on  the
    are proposed, and a Dynamic Evolving Neural
                                                        contribution of Harmsen and  Pearlman [2003], Ker
    Fuzzy Inference System (DENFIS) based feature
                                                        [2005b] proposes two novel ways of applying the HCF:
    selection is proposed, as well as the use of
                                                        calibrating the output using a down-sampled image and
    Support Vector Machine  Recursive Feature           computing the adjacency histogram instead of the usual
    Elimination (SVM-RFE)   to  obtain  better
                                                        histogram. The  best discriminators are Adjacency
    detection accuracy. In comparison with other
                                                        HCFCOM     (A.HCFCOM)    and Calibrated Adjacency
    well-known  features, overall, our features
                                                        HCFCOM (C.A.HCFCOM) to improve the probability of
    perform the best. DENFIS outperforms some
                                                        detection for LSB matching in grayscale images. Farid
    traditional learning classifiers. SVM-RFE and
                                                        and  Lyu describe an  approach to detecting hidden
    DENFIS  based  feature selection outperform         messages  in   images  by   using  a  wavelet-like
    statistical significance based feature selection
                                                        decomposition to build high-order statistical models of
    such as t-test. Experimental results also indicate
                                                        natural images [Lyu and Farid, 2004 and 2005]. Fridrich
    that it remains very challenging to steganalyze
                                                        et al. [2005] propose a Maximum   Likelihood (ML)
    LSB  matching  steganography in  grayscale
                                                        estimator for estimating the number of embedding
    images with high complexity.
                                                        changes for non-adaptive Â±K embedding in images. Based
                                                        on the stego-signal estimation, Holotyak et al. [2005]
1  Introduction                                         present a blind steganalysis classifying on high order
Steganalysis is the science and art of detecting the    statistics of the estimation signal.
presence of hidden data in digital images, audios, videos Unfortunately, the publications mentioned above did
and other media. In steganography or the hiding of secret not fully address the issue of image complexity that is
data in digital media, the most common cover is digital very important in evaluating the detection performance
images. To   this date, many   steganographical or      (though Fridrich et al. [2005] report that the ML estimator
embedding methods, such as LSB  embedding, spread       starts to fail to reliably estimate the message length once
spectrum steganography, F5 algorithm and some other     the variance of sample exceeds 9, indicating that the
JPEG  steganography, have  been  very successfully      detection performance decreases with the increase in the
steganalyzed [Fridrich et al., 2003; Ker, 2005a; Fridrich et image complexity).
al. 2002; Harmsen and Pearlman 2003; Choubassi and        Recently, the shape parameter of Generalized Gaussian
Moulin 2005; Liu et al., 2006a]. Several other embedding Distribution (GGD) in the wavelet domain is introduced
paradigms, include stochastic modulation [Fridrich and  by the authors to measure the image complexity in
Goljan, 2003; Moulin and Briassouli, 2004] and LSB      steganalysis [Liu et al., 2006b]; although the method
matching [Sharp 2001], however, are much more difficult proposed therein is successful in detecting LSB matching
to detect.
                                                        steganography in color images and outperforms other
                                                        well-known methods, its performance is not so good in
                                                        grayscale images, which is generally more difficult.
                                                         On the other side, many steganalysis methods are based
    Support for this research was received from ICASA (of
New Mexico Tech) and a DoD IASP Capacity Building grant on  feature mining and machine learning. In feature


                                                IJCAI-07
                                                  2808mining, besides feature extraction, another general     shape parameter. The GGD model contains the Gaussian
problem is how to choose the good measures from the     and Laplacian PDFs as special cases, using  =2and =
extracted features. Avcibas et al. [2003] propose a     1, respectively.
steganalysis using image quality metrics. In their method,
they apply analysis of variance (ANOVA) to feature      2.2 Entropy and    High order Statistics of the
selection, the higher the F statistic, the lower the p value, Histogram of the Nearest Neighbors
and the better the feature will be. Essentially, the    There is evidence that adjacent pixels in ordinary images
ANOVA applied by Avcibas et al. [2003] is significance- are highly correlated [Huang and Mumford, 1999; Liu et
based feature selection like other statistics such as T-test, al., 2006a]. Consider the histogram of the nearest
etc. But these statistics just consider the significance of neighbors, denote the grayscale value at the point (i, j)as
individual feature, not the interaction of features. There x, the grayscale value at the nearest point (i+1, j)inthe
has been little research that addresses in depth the feature horizontal direction as y, and the grayscale value at the
selection problem with specific respect to steganalysis. nearest point (i, j+1) in the vertical direction as z.The
  In this paper, we propose four types of features and a variable H(x, y, z) denotes the occurrence of the pair (x,
Dynamic  Evolving Neural Fuzzy   Inference System       y, z), or the histogram of the nearest neighbors (NNH).
[Kasabov and Song, 2002; Kasabov, 2002] (DENFIS)-         The entropy of NNH (NNH_E) is calculated as follows:
based feature selection to the steganalysis of LSB                           
                                                            NNH_E =       log 2                      (2)
matching steganography in grayscale images. The four          
types of features consist of the shape parameter of GGD Where   denotes the distribution density of the NNH.
                                                                     
in the wavelet domain to measure the image complexity,    The symbol   H denotes the standard deviation of H.
the entropy and the high order statistics in the histogram The rth high order statistics of NNH is given as:
                                                                                                    r
of the nearest neighbors, and correlation features. We also            N1 N1 N1    N1 N1 N1 
                                                                     1               1            
adopt the well-known gene selection method of Support                      H (x, y, z) H (x, y, z)
                                                                     N 3             N 3
Vector Machine Recursive Feature Elimination (SVM-       NNH_HOS(r)=   x=0 y=0 x=0 
   x=0 y=0 z=0   (3)
                                                                                    r
RFE) [Guyon et al., 2002] for choosing good measures in                             H
steganalysis.                                           Where N  is the number of possible gray scales of the
  Comparing against other well-known methods in terms   image, e.g., for 8-bit grayscale image, N=256.
of steganalysis performance, our feature set, overall,
performs the best. DENFIS outperforms some traditional  2.3  Correlation Features
learning classifiers. SVM-RFE  and  DENFIS-based        The following three correlation features are extracted.
feature selection outperform statistical significance based 1. The correlation between the Least Significant Bit
feature selection such as T-test in steganalysis.       Plane (LSBP) and the second Least Significant Bit Plane
                                                        (LSBP2) and the autocorrelation in the LSBP: M1(1:m,
2  Feature Mining                                       1:n) denotes the binary bits of the LSBP and M2(1:m,1:n)
                                                        denotes the binary bits of the LSBP2.
2.1 Image Complexity                                                            Cov(, M M  )
                                                            CcorMM1(,)==              12              (4)
Several papers [Srivastava et al., 2003; Winkler, 1996;                 12        
                                                                                    MM12
Sharifi and Leon-Garcia, 1995] describe the statistical
                                                        where 22==Va r()and M          Va r (). M
models of images such as probability models for images          MM1212
based on  Markov   Random  Field models  (MRFs),          C(k, l), the autocorrelation of LSBP is defined as:
Gaussian Mixture Model (GMM) and GGD     model in           Ckl(,)=  cor ( X , X )                    (5)
transform domains, such as DCT, DFT, or DWT.                                kl
                                                                ==++
  Experiments show that a good Probability Density      where XMkl11(1: mknlXMk ,1: ); ( 1: ml , 1: n ).
Function (PDF) approximation for the marginal density of
                                                          2. The autocorrelation in the image histogram: The
coefficients at a particular subband produced by various                                          (  
types of wavelet transforms may   be achieved by        histogram probability density is denoted as 0, 1,
                                                             )
adaptively varying two parameters of the GGD [Sharifi    2â¦  N-1 . The histogram probability densities, He,Ho,
and Leon-Garcia, 1995; Moulin and Liu, 1999], which is  Hl1, and Hl2 are denoted as follows:
                                                                (        )         (         )
defined as                                                  He =  0, 2, 4â¦ N-2 ,Ho    =  1, 3, 5â¦  N-1 ;
                                                        H  = ( ,  ,  â¦ ),H=    ( ,  ,  â¦   ).
     px(; , )=        e(|x |/ )             (1)            l1   0  1  2   N-1-l   l2   l  l+1 l+2  N-1
               2(1/)                                   The autocorrelation coefficients C2andCH(l),wherel
                                                       is the lag distance, are defined as follows:
Where  ()zetdtz=> tz1 , 0 is the Gamma function.
             0                                               C2 =cor(He,Ho)                           (6)
Here  models the width of the PDF peak, while  is
inversely proportional to the decreasing rate of the peak;   CH(l) = cor (Hl1,Hl2)                    (7)
 is referred to as the scale parameter and  is called the 3. The correlation in the difference between the image
                                                        and the denoised version: The original cover is denoted as


                                                IJCAI-07
                                                  2809                                                                      N
                                                                          2
F, the stego-image is denoted as FÂ´, D (Â·) denotes some       SC(ec)=                                (9)
                                                                        cor() eci ,e
denoising function, the differences between the image and             i=1

the denoised are:                                       Where, ec  C, ei  FN (i = 1, 2â¦ N). The ec with the
     EF =FâD(F)                                         minimal value of SC(ec) is chosen as eN+1.Wecallthis
                                                        feature selection DENFIS-MSC (for Minimum of SC).
     EFÂ´ =FÂ´-D(FÂ´)
  Generally, the statistics of EF and EFÂ´ are different. The 4 Experiments
correlation features in the difference domain are extracted
as follows. Firstly, the test image is decomposed by
wavelet transform. Find the coefficients in HL, LH and  4.1  Experimental Setup
HH  subbands with the absolute value smaller than the   The original images in our experiments are 5000 TIFF
threshold value, t, set these coefficients to zero, and raw format digital pictures from Olympus C740. These
reconstruct the image using the inverse wavelet transform images are 24-bit, 640480 pixels, lossless true color and
on the updated wavelet coefficients. The reconstructed  never compressed. According to the method in [Lyu and
image is treated as denoised image. The difference      Farid, 2004 and 2005], we cropped the original images
between test image and reconstructed version is Et,where into 256256 pixels in order to get rid of the low
t is the threshold value.                               complexity parts of the images. The cropped color images
                =                                       are converted into grayscales and we hid data in these
     CtklEtktl(; , ) corE (,, , E )           (8)
                                                        grayscales with different hiding ratio. LSB matching
where     =                =++The
       EEmknlEEkmlntk,, t(1: ,1: ); tl t ( 1: , 1: ).   stego-images are produced. The hidden data in different
variables k and l denote the lag distance.              covers are different. The hiding ratio is 12.5%.
                                                        4.2  Feature Extraction and Comparison
3   Neuro-Fuzzy Inference System Based
                                                        The following features are extracted:
    Feature Selection                                     1. Shape parameter  of GGD of HH wavelet subband
Neuro-fuzzy inference systems and evolving neuro-fuzzy  to measure image complexity, described in (1).
inference systems are introduced in [Kasabov, 2002]. The  2. Entropy of the histogram of the nearest neighbors,
dynamic  evolving  neuro-fuzzy  system  (DENFIS)        NNH_E, defined in (2)
proposed by [Kasabov and Song, 2002] uses the Takagi-     3. The high order statistics of the histogram of the
Sugeno type of fuzzy inference method [Takagi and       nearest neighbors, NNH_HOS(r)in(3). r is set from 3 to
Sugeno, 1985].                                          22, total 20 high order statistics.
  To improve the detection performance, based on our      4. Correlations features consist of C1in(4),C(k,l)in
previous work [Liu and Sung, 2006], we propose a feature (5), C2in(6),CH(l) in (7), and CE(t; k,l).
selection method based on  the DENFIS   supervised        We set the following lag distance to k and l in C(k,l)
learning, described as follows:                         and get 14 features:
  1. Each individual feature is ranked in the order from  a.  k =0,l =1,2,3,and4;l =0,k =1,2,3,and4.
the highest train accuracy to the lowest train accuracy   b.  k =1,l =1;k =2,l =2; k=3, l =3;k =4andl =4.
with the use of DENFIS.                                   c.  k =1,l =2; k=2, l =1.
  2. The feature with the highest train accuracy is chosen In (7), l is set to 1, 2, 3, and 4. In (8), we set the
as the first feature. After this step, the chosen feature set,
                                                        following lag distance to k and l in C (t; k,l) and get
F , consists of the best feature, e , corresponding to                                     E
 1                             1                        following pairs: C (t;0,1),C(t;0,2),C(t;1,0), C (t;2,0),
feature dimension one.                                                 E        E        E        E
             st                                         CE(t;1,1),CE(t;1,2),andCE(t; 2,1). t isset1,1.5,2,2.5,
  3. The (N+1) feature set, FN+1 = {e1, e2 ,â¦, eN , eN+1}
is produced by adding e into the present N-dimensional  3, 3.5, 4, 4.5, and 5.
                    N+1                                   Henceforth, we use CF to denote the fourth type of
feature set, FN ={e1, e2,â¦,eN}accordingtothefollowing
                                                        correlation features; and use EHCC (for Entropy, High
method: Each feature ei (i  1, 2, â¦, N) outside of FN is
                                                        order statistics, Complexity, and Correlation features) to
added into FN; the classification accuracy of each feature
set FN +{ei} is compared, the ec with the highest train denote types 1 to 4 features.
accuracy is put into the set of candidates, C.The         To compare EHCC with other well-known features, the
candidate set C generally includes multiple features, but Histogram Characteristic Function Center of Mass
only one feature will be chosen. The strategy is to     (HCFCOM) features [Harmsen and Pearlman, 2003] are
measure the similarity of chosen features and each of the extracted because the hiding process of LSB matching
candidates. Pearsonâs correlation between the candidate steganography can be modeled in the context of additive
ec, ec  C and the element ei, ei  FN (i = 1, 2â¦ N)is  noise. We extend HCFCOM  feature set to the high order
calculated. To measure the similarity, the sum of the   moments. HCFHOM stands for HCF center of mass High
square of the correlation (SC) is defined as follows:   Order Moments; HCFHOM(r) denotes the     rth order
                                                        statistics. In our experiments, the HCFHOM feature set


                                                IJCAI-07
                                                  2810consists of HCFCOM and HCFHOM(r)(r  =2,3,and4).         Fig. 1  also shows  that the detection performance
Based on Harmsen and Pearlmanâs work, Ker [2005b]       decreases as the image complexity increases.
proposed A.HCFCOM and C.A.HCFCOM. Additionally,
Farid and Lyu [2004, 2005] described an approach to     Table 1. Detection performances (mean value Â± standard deviation, %)
                                                            on the feature sets with the use of different classifiers. In different
detecting hidden messages in images by building High-       image complexity, the highest test accuracy is in bold.
Order Moment statistics in Multi-Scale decomposition
domain (we call the features HOMMS), which consists of         Classifier
                                                                          SVM    ADABOOST   NBC    QDC
72-dimension features in grayscale images.                   Feature Set
                                                                EHCC    86.9 Â± 1.1 84.5 Â± 1.1 77.5 Â± 1.6 57.9 Â± 0.7

4.3 Detection Performance on Feature Sets                        CF      85.9 Â± 1.0 82.0 Â± 1.2 77.0 Â± 2.1 80.9 Â± 1.7

To compare the detection performances on these feature    <    HCFHOM    60.9 Â± 1.3 57.6 Â± 1.5 57.5 Â± 1.5 53.4 Â± 1.0
sets with different classifiers, Besides DENFIS, we apply 0.4  HOMMS     53.6 Â± 1.0 50.6 Â± 2.0 46.9 Â± 1.7 42.1 Â± 1.4
the following classifiers to each feature sets. These
                                                              C.A.HCFCOM 55.3 Â± 0.6 54.3 Â± 1.1 53.8 Â± 1.1 55.4 Â± 1.1
classifiers are Naive Bayes Classifier (NBC), Support
Vector Machines  (SVM), Quadratic  Bayes  Normal              A.HCFCOM   55.6 Â± 0.9 55.4 Â± 1.8 54.7 Â± 1.4 55.5 Â± 1.1
Classifier (QDC), Nearest Mean   Scaled Classifier              EHCC    81.4 Â± 0.7 74.3 Â± 0.8 68.2 Â± 0.8 60.9 Â± 0.5
(NMSC), K-Nearest Neighbor classifier (KNN) and                  CF      77.6 Â± 0.4 72.2 Â± 1.0 67.6 Â± 1.3 70.6 Â± 1.3
Adaboost that produces a classifier composed from a set  0.4-  HCFHOM    58.4 Â± 0.6 56.6 Â± 1.1 56.1 Â± 0.9 54.5 Â± 0.6
of weak rules [Vapnik, 1998; Schlesinger and Hlavac,     0.6   HOMMS     48.8 Â± 1.6 47.6 Â± 1.0 47.1 Â± 0.8 44.0 Â± 1.5

2002; Heijden et al., 2004; Webb, 2002; Schapire and          C.A.HCFCOM 58.1 Â± 0.7 57.0 Â± 1.5 57.8 Â± 1.1 57.9 Â± 0.8
Singer, 1999; Friedman et al., 2000].
                                                              A.HCFCOM   57.3 Â± 0.6 56.6 Â± 0.9 56.8 Â± 0.7 56.6 Â± 0.6
  Thirty experiments are done on each feature set using
each classifier. Training samples are chosen at random          EHCC7    2.4 Â± 1.0 64.3 Â± 1.2 61.4 Â± 1.0 58.3 Â± 0.5
and the remaining samples are for test. The ratio of             CF      66.7 Â± 0.7 63.9 Â± 1.2 62.1 Â± 1.1 62.3 Â± 1.2
training samples to test samples is 2:3. The average     0.6-  HCFHOM    57.6 Â± 0.9 55.3 Â± 1.1 54.2 Â± 1.3 53.1 Â± 0.7
classification accuracy is compared.                     0.8   HOMMS     47.3 Â± 0.7 43.7 Â± 1.3 45.4 Â± 1.2 40.6 Â± 2.4

  Table 1 compares the detection performances (mean           C.A.HCFCOM 56.0 Â± 1.1 56.4 Â± 1.0 55.8 Â± 1.0 56.2 Â± 0.8
values and standard deviations) on each feature set with
the use of different classifiers. In each category of image   A.HCFCOM   56.6 Â± 0.6 54.9 Â± 1.2 55.2 Â± 1.1 55.5 Â± 1.2
complexity, the highest test accuracy is in bold. Table 1       EHCC     63.6 Â± 1.2 57.8 Â± 1.3 56.9 Â± 1.2 56.1 Â± 0.5
indicates that, regarding the classification accuracy of         CF      60.0 Â± 1.0 57.4 Â± 1.8 57.8 Â± 1.5 57.5 Â± 1.6

feature sets, on the average, EHCC  performs best,       0.8-  HCFHOM    53.9 Â± 1.2 52.0 Â± 1.6 53.2 Â± 1.4 51.7 Â± 0.6
followed by CF, HOMMS performs worst; regarding the       1    HOMMS       /      42.0 Â± 1.5 44.5 Â± 0.8 41.6 Â± 2.8
classification performance of classifiers, SVM is the best;
                                                              C.A.HCFCOM 52.4 Â± 0.7 52.6 Â± 1.5 52.1 Â± 1.3 53.1 Â± 1.2
regarding image complexity, the test accuracy decreases
while the image  complexity increases. In the low             A.HCFCOM   53.3 Â± 1.0 50.3 Â± 1.3 51.8 Â± 1.2 50.8 Â± 1.6
complexity of  < 0.4, the highest test accuracy is 86.9%;      EHCC     62.9 Â± 1.6 58.6 Â± 1.9 58.1 Â± 1.4 56.8 Â± 1.4
in the high complexity of  > 1, the highest test accuracy       CF      59.7 Â± 1.7 58.9 Â± 2.3 57.1 Â± 1.5 58.4 Â± 1.3
is 62.9%. It shows that image complexity is an important       HCFHOM    54.4 Â± 0.8 52.7 Â± 1.6 51.9 Â± 1.7 53.2 Â± 1.8
                                                         >1
factor to the detection performance.                           HOMMS       /      46.7 Â± 1.8 50.4 Â± 1.4 43.1 Â± 1.5
  To obtain a higher detection performance, we combine
                                                              C.A.HCFCOM 54.7 Â± 0.5 52.7 Â± 1.7 53.1 Â± 1.4 54.4 Â± 0.9
EHCC, HCFCOM, HOMMS, C.A. HCFCOM, and A.
HCFCOM    features, and apply three feature section           A.HCFCOM   54.3 Â± 0.3 51.2 Â± 1.6 51.6 Â± 2.0 53.5 Â± 1.4
methods, SVM-RFE [Guyon et al., 2002], DENFIS-MSC,
and T-test (here we apply T-test instead of ANOVA
because cover samples and steganography samples are       Fig. 2 compares the three feature selections in the
                                                                            
unpaired in each category of image complexity) to the   image complexity of   < 0.4 with the use of SVM,
features, and compare different classifiers and the three DENFIS, and NMSC. It indicates that the  feature
feature selections in the feature dimensions 1 to 40.   selections SVM-RFE and DENFIS-MSC are better than
                                                        T-test. Furthermore, applying SVM and DENFIS to
  Fig. 1 compares the detection performance on the
                                                        SVM-RFE    and DENFIS-MSC    feature sets, the test
SVM-RFE feature set with the use of DENFIS, SVM,
                                                        accuracies are better than the highest value of table 1.
NBC, NMSC, and KNN.     It indicates that in the low    Due to the page limit, we donât list comparison of the
image complexity, DENFIS and SVM are the best; in the   feature selections in the high image complexity. Our
mediate and high image complexity, DENFIS is the best.  experiments also indicate that SVM-RFE and DENFIS-
                                                        MSC outperform T-test.


                                                IJCAI-07
                                                  2811           (a)   <0.4                      (b) 0.4 <  <0.6                   (c) 0.6 <  <0.8


          (d) 0.8 <  <1                        (e)  >1

Fig. 1. Comparison of the detection performance on SVM-RFE feature set with the use of different classifiers.


Fig. 2. Comparison of feature selections DENFIS-MSC, SVM-RFE, and T-test in the image complexity of  <0.4.

                                                      challenging for the  steganalysis of LSB   matching
5  Conclusions                                        steganography in grayscale images with high complexity.
In this paper, we present a scheme of steganalysis of LSB
matching steganography in grayscale images based on   References
feature mining and neuro-fuzzy inference system. Four [Avcibas et al., 2003] I. Avcibas, N. Memon and B. Sankur.
types of features are extracted, a DENFIS-based feature  Steganalysis using Image Quality Metrics. IEEE Trans.
selection is used, and SVM-RFE is used as well to obtain Image Processing, 12(2):221â229.
better detection accuracy. In comparison with other features [Choubassi and Moulin, 2005] M. Choubassi and P. Moulin.
of HCFHOM, HOMMS, A.HCFCOM, and C.A.HCFCOM,              A  New  Sensitivity Analysis Attack. Proc. of SPIE
overall, our features perform the best. DENFIS outperforms Electronic Imaging, vol.5681, pp.734â745.
some  traditional learning classifiers. SVM-RFE and   [Fridrich, 2002] J. Fridrich, M. Goljan, D. Hogea.
DENFIS   based feature selection outperform statistical
                                                         Steganalysis of JPEG   Images: Breaking  the F5
significance based feature selection such as T-test.
                                                         Algorithm. Lecture  Notes  in Computer   Science,
Experimental results also indicate that it is still very


                                                IJCAI-07
                                                  2812