                   Predicting and Preventing Coordination Problems in
                                Cooperative Q-learning Systems
                                    Nancy Fulda    and  Dan Ventura
                                     Computer Science Department
                                       Brigham Young University
                                            Provo, UT 84602
                                  fulda@byu.edu, ventura@cs.byu.edu

                    Abstract                          ber of agents in the system increases. Some of them rely
                                                      on global perceptions of other agents’ actions or require a
    We present a conceptual framework for creating Q- unique optimal equilibrium, conditions that do not always ex-
    learning-based algorithms that converge to optimal ist in real-world systems. As reinforcement learning and Q-
    equilibria in cooperative multiagent settings. This learning are applied to real-world problems with real-world
    framework includes a set of conditions that are suf- constraints, new algorithms will need to be designed.
    ﬁcient to guarantee optimal system performance.     The objective of this paper is to understand why the algo-
    We demonstrate the efﬁcacy of the framework by    rithms cited above are able to work effectively, and to use this
    using it to analyze several well-known multi-agent understanding to facilitate the development of algorithms that
    learning algorithms and conclude by employing it  improve on this success. We do this by isolating three factors
    as a design tool to construct a simple, novel multi- that can cause a system to behave poorly: suboptimal indi-
    agent learning algorithm.                         vidual convergence, action shadowing, and the equilibrium
                                                      selection problem. We prove that the absence of these three
1  Introduction                                       factors is sufﬁcient to guarantee optimal behavior in cooper-
Multiagent reinforcement learning systems are interesting be- ative Q-learning systems. Hence, any algorithm that effec-
cause they share many beneﬁts of distributed artiﬁcial in- tively addresses all three factors will perform well, and sys-
telligence, including parallel execution, increased autonomy, tem designers can select means of addressing each problem
and simplicity of individual agent design [Stone and Veloso, that are consistent with the constraints of their system.
2000; Cao et al., 1997]. Q-learning [Watkins, 1989] is a natu-
ral choice for studying such systems because of its simplicity 2 Background and Terminology
and its convergence guarantees. Also, because the Q-learning The simplicity of the Q-learning algorithm [Watkins, 1989]
algorithm is itself so well-understood, researchers are able to has led to its frequent use in reinforcement learning research
focus on the unique challenges of learning in a multiagent and permits a clear, concise study of multiagent coordination
environment.                                          problems in simple environments. Multiagent coordination
  Coaxing useful behavior out of a group of concurrently- problems are not a direct consequence of the Q-learning al-
learning Q-learners is not a trivial task. Because the agents gorithm, however. Thus, although we focus on Q-learning
are constantly modifying their behavior during the learning in this paper, the analysis presented should be applicable in
process, each agent is faced with an unpredictable environ- other reinforcement learning paradigms as well.
ment which may invalidate convergence guarantees. Even  A Q-learning agent may be described as a mapping from a
when the agents converge to individually optimal policies, the state space S to an action space A. The agent maintains a list
combination of these policies may not be an optimal system of expected discounted rewards, called Q-values, which are
behavior.                                             represented by the function Q(s, a) where s ∈ S and a ∈ A
  Poor group behavior is not the universal rule, of course. are the current state and chosen action. The agent’s objective
Many researchers have observed emergent coordination in is to learn an optimal policy π∗ : S → A that maximizes
groups of independent learners using Q-learning or similar expected discounted reward over all states s. At each time
algorithms [Maes and Brooks, 1990; Schaerf et al., 1995; step, the agent chooses an action at ∈ A, receives a reward
             ]
Sen et al., 1994 . However, failed coordination attempts oc- r(st,at), and updates the appropriate Q-value as
cur frequently enough to motivate a host of Q-learning adap-
tations for cooperative multiagent environments. [Claus and
                                                       ΔQ(st,at)=α[r(st,at)+γ   max Q(st+1,a) − Q(st,at)]
Boutilier, 1998; Lauer and Riedmiller, 2000; Littman, 2001;                      a
Wang and Sandholm, 2002]. These algorithms are critical
steps towards a better understanding of multiagent Q-learning where 0 <α≤ 1 is the learning rate and 0 ≤ γ<1
and of multiagent reinforcement learning in general. How- is the discount factor. At any point in time, the agent’s
ever, most of these algorithms become intractable as the num- best estimate of the optimal policy is its learned policy

                                                IJCAI-07
                                                   780πˆ(s) = argmaxa{Q(s, a)}. The learned policy may differ

from the optimal policy because it is based on Q-value es- ∗
                                                        Q  (si , a)=ri (si , a)
timates. In Q-learning, the learned policy also differs from i  
the exploration policy executed during learning (off-policy        t                           ∗
                                                         +        γ p(si(t)|si(t−1 ), at−1 )ri (si(t), Π (si(t))))
learning). Note that other approaches might employ identi-                                     i
                                                            t  s
cal learned and exploration policies (on-policy learning).     i(t)
  Under certain conditions [Tsitsiklis, 1994], Q-learning will where t = {1, 2, ...∞} and p(si(t)|si(t−1), at−1) is the prob-
converge to a set of optimal Q-values                 ability of transitioning to individual state si(t) given the pre-
                                                      vious state and action. An agent’s preferred joint policy
                                                    can be described in terms of the optimal joint Q-values,
Q∗ s, a   r s, a        γtp s |s  ,a    r s ,π∗ s       ∗                  ∗
  (   )=   (   )+          ( t t−1  t−1) ( t  ( t))   Πi (si) = argmaxa{Qi  (si , a)}.
                   t s
                      t                               Deﬁnition 1. A system of Q-learning agents is cooperative if
                                                                                           ∗
                                                      and only if any joint action that maximizes Q si , ai for one
where t = {1, 2, ...∞} and p(st|st−1,at−1) is the probability                              i (    )
                                                      agent also maximizes it for all agents.
of transitioning to state st given the previous state and action.
If the agent has converged to an optimal Q-value set, then the This deﬁnition of cooperation does not require that the
optimal and learned policies are identical: π∗(s)=ˆπ(s)= agents share the same reward signal for all joint actions; it
argmaxa{Q(s, a)}.                                     requires only that the agents share a set of mutually-preferred
                                                      joint actions. It thus allows for scenarios where the agents’
2.1  Q-learning in Multiagent Systems                 preferences differ, but where the agents’ needs can (and must)
                                                      be simultaneously satisﬁed.
Let Si and Ai represent the state and action space of the ith We consider a solution to be system optimal if it is both a
agent in an n-agent system. The state of the system can be ex- Nash equilibrium and Pareto-optimal. In a cooperative set-
pressed as a vector of individual agent states s =[s1, ..., sn ], ting, this deﬁnition of optimality can be restricted to a subset
si ∈ Si, and the combination of the agents’ individual actions of Pareto-optimal Nash equilibria called coordination equi-
is a =[a1, ..., an ], ai ∈ Ai, resulting in joint state and action libria. A coordination equilibrium is strict if the inequality
spaces, S and A.                                      in Deﬁnition 2 is strict (the inequality will be strict if there
  The combined  policy for the system is a mapping    is exactly one joint action that results in all agents receiving
Π:  S →   A, where Π(st)=[π1(s1(t)), ..., πn(sn(t))].In their max reward; in other cases there are multiple such joint
each time step, a joint action a =Π(st) is executed, and each actions and some sort of coordinated equilibrium selection
agent receives a reward ri(si , a) and updates the correspond- becomes necessary).
          Q  s ,a
ing Q-value i( i i). Note that the rewards the agents re- Deﬁnition 2. Joint action  a∗   is   a   coordi-
ceive are based on the joint action (and individual state), but nation equilibrium for state s if and only if
the agents update Q-values for their corresponding individual           ∗     ∗    ∗    ∗     ∗
                                                      ∀{a, i | ai ∈ a and ai ∈ a },Qi (si , ai ) ≥ Qi (si , ai ).
action (and state). Thus, multiple joint actions are aliased to
a single action perception of the agent.              3   Factors That Can Cause Poor System
  It is sometimes useful to describe system behavior in terms
of which joint policy an agent would prefer if it maintained Behavior
a separate Q-value for each joint action. This preferred joint We now identify three factors that can cause poor system be-
                   ∗
policy is a mapping Πi : Si → A and represents the joint havior. The ﬁrst and third factors are represented in various
policy which provides maximum discounted reward for agent guises in the literature. The second factor, action shadowing,
i over all si. An agent’s joint Q-values Qi(si, a) can then is likely to be less familiar to readers. In the next section, we
be deﬁned as the average (over all joint states that contain si) will show that the absence of these three factors is sufﬁcient
expected reward received by agent i when joint action a is to guarantee that the system will perform optimally.
                 ∗
executed in si and Πi is followed thereafter. The relationship
between the joint Q-values and the agent’s actual Q-value set 3.1 Poor Individual Behavior
Qi(si,ai) is described by                             An  agent has learned  an individually optimal policy
                                                       ∗                    ∗
                                                      π  si              {Q   si,ai }
                                                      i ( ) =   argmaxai   i (   )  if its behavior is a best
                                                      response to the strategies of the other players. Q-learning is
           Qi(si,ai)=    p(a|ai )Qi (si , a)
                       a                              guaranteed to converge to an optimal Q-value set, and hence
                                                      to an optimal policy, in the individual case. However, this
where p(a|ai) is the probability of joint action a being exe- convergence guarantee breaks down in multiagent systems
cuted when agent i selects individual action ai. Note that the because the changing behavior of the other agents creates a
probability is conditional because for some a (those that do non-Markovian environment.
not contain ai) the probability is 0 but for others it is not. For Despite this loss of theoretical guarantees, Q-learning
those a that do contain ai, the probability depends on the ac- agents often converge to optimal policies in multiagent set-
tions of the other agents (in other words, p(a|ai) is a function tings because (1) the agents do not necessarily need to con-
of the joint exploration policy Π(s,t)). The agent’s optimal verge to an optimal Q-value set in order to execute an optimal
joint Q-values are deﬁned as                          policy and (2) if all agents are playing optimally, they must

                                                IJCAI-07
                                                   781settle to a Nash equilibrium, and Nash equilibria tend to be will depend on the complete reward structure of the task as
self-reinforcing. Individual behavior of Q-learning agents is well as on the exploration strategy used by the system. Par-
well-studied in the literature and in what follows we focus tially exploitive exploration strategies have proven particu-
particularly on the following two potential problems. larly effective at encouraging convergence to a set of mutually
                                                      compatible individual policies [Claus and Boutilier, 1998;
3.2  Action Shadowing                                 Sen and Sekaran, 1998].
Action shadowing occurs when one individual action appears
better than another, even though the second individual ac- 4 Achieving Optimal Performance
tion is potentially superior. This can occur because typical
Q-learning agents maintain Q-values only for individual ac- Cooperating Q-learners will behave optimally if each agent
tions, but receive rewards based on the joint action executed learns an individually optimal policy and if maximal action
by the system. As a consequence, the agent’s optimal pol- shadowing and the equilibrium selection problem are absent.
icy may preclude the possibility of executing a coordination Theorem 1. For any cooperative Q-learning system,
                                                        n
equilibrium.                                          ∪i=1πˆi(si) is a system optimal solution if the following con-
Deﬁnition 3. A joint action a† is shadowed by individ- ditions hold:
          a         s              a     π∗ s                            ∗
ual action ˆi in state if and only if ˆi = i ( i) and     (1) ∀i, πˆi(si)=πi (si)
            ∗    †      ∗
∀a|aˆi ∈ a, Qi (si, a ) > Qi (si, a).                           †          ∗                   ∗     †
                                                          (2) (a , aˆi )|aˆi = πi (si) and ∀a|aˆi ∈ a, Qi (si , a ) >
                                                            ∗
  A special case is maximal action shadowing, which occurs Qi (si , a)
when the shadowed joint action provides maximal possible         1  2     1                    ∗    j
                                                          (3) (a , a =  a )|∀(i, j ∈{1, 2}, a)Qi (si , a ) ≥
reward for the affected agent:                              ∗
                                                          Qi (si , a)
Deﬁnition 4. An agent i experiences maximal action shad-
                                   ∗
owing in state s if and only if there exist a and aˆi such that      n
 ∗                             ∗    ∗      ∗          Proof. Let aˆ = ∪i=1πˆi(si) be the joint action selected by the
a is shadowed by aˆi and ∀a ∈ A, Qi (si , a ) ≥ Qi (si , a)
                                                      learned joint policy of the system. Then ∀i, aˆi =ˆπi(si) and
                                                                             ∗
  The cause of the action shadowing problem lies in the Q- by Condition (1) ∀i, aˆi = πi (si).
value update function for agents in a multiagent system. In We know from Condition (2) that there cannot be a joint
                                    r s ,                    †                    ∗    †      ∗
each time step, each agent receives a reward i( i a) based on action a such that ∀a|aˆi ∈ a, Q (si , a ) > Q (si , a). This
                                          Q  s ,a                                 i           i
the joint action space, but it updates the Q-value i( i i), implies that aˆi enables one or more joint actions that maxi-
based on its individual action selection. Consequently, agent mize agent i’s joint Q-value function: ∃{a1, ..., am }|∀(j ∈
i                                                                       j      ∗    j      ∗
 is unable to distinguish between distinct rewards that are all {1, ..., m}, a), aˆi ∈ a and Qi (si , a ) ≥ Qi (si , a).
aliased to the same individual action. Action shadowing is Because the system is cooperative, any joint action that
a consequence of the well-known credit assignment problem maximizes expected discounted reward for one agent must
but is more precisely deﬁned (and thus addressable).  maximize it for all other agents as well. Hence, we
  Action shadowing is sometimes prevented by on-policy have a set of joint actions {a1, ..., am } such that ∀(i, j ∈
                                                                    ∗     j     ∗
learning – agents seeking to maximize individual reward {1, ..., m}, a), Qi (si , a ) ≥ Qi (si , a).
will tend to gravitate toward coordination equilibria. On- From Condition (3) we know that there can be at most one
policy learning does not assure that an action shadowing joint action that maximizes the expected discounted reward
problem will not occur, however. Maximal action shadow- for all agents. It follows that m =1and there is a unique
                                                                 1                 ∗    1      ∗
ing is likely to occur despite on-policy learning in situations joint action a such that ∀(i, a), Qi (si , a ) ≥ Qi (si , a).
where failed coordination attempts are punished, as in penalty Since each agent’s individual action aˆi enables a joint ac-
games [Claus and Boutilier, 1998]. Maximal action shadow- tion that maximizes its expected discounted reward, it must
ing will always cause a coordination problem when agent in- be the case that ˆa = a1. Because it maximizes expected dis-
terests do not conﬂict.                               counted reward for every agent, ˆa is a (strict) coordination
3.3  Equilibrium Selection Problems                   equilibrium (by Deﬁnition 2) and hence must be a system op-
                                                      timal solution.
An equilibrium selection problem occurs whenever coordi-
nation between at least two agents is required in selecting Naturally, these are not the only possible sufﬁcient con-
between multiple system optimal solutions. This is a some- ditions to guarantee optimal system behavior. However, the
what stricter deﬁnition than the standard game-theoretic term, conditions that make up our framework are preferable over
which refers to the task of selecting an optimal equilibrium many other possibilities because they can be addressed by
from a set of (possibly suboptimal) potential equilibria. modifying the learning algorithm directly, without placing
Deﬁnition 5.  An  equilibrium selection problem oc-   additional constraints on the cooperative learning environ-
curs whenever ∃(a1, a2  =   a1)|∀(i, j ∈{1,  2}, a)   ment.
  ∗    j      ∗
Qi (si , a ) ≥ Qi (si , a).
  The existence of an equilibrium selection problem does 5 Improving System Performance
not necessarily result in suboptimal system behavior. How- Given the framework imposed by Theorem 1, we con-
ever, an equilibrium selection problem creates a potential that sider various approaches to preventing coordination prob-
the agents will mis-coordinate. Whether or not this happens lems. Topics are grouped according to two factors that affect

                                                IJCAI-07
                                                   782system behavior: action shadowing, and equilibrium selec- 5.3 Task-oriented Approaches for Equilibrium
tion. The third factor, suboptimal individual convergence, is Selection
quite prevalent in the Q-learning literature, and is too broad a
                                                      Unique Optimal Solution: The simplest way to prevent the
topic to be examined here.
                                                      equilibrium selection problem is to design a system that has
  There are two basic ways to improve system performance: only a single optimal solution. When this is the case, the
control the task or modify the learning algorithm. In task- agents do not need to coordinate in selecting between mul-
oriented approaches, reward structures are constrained so that tiple optimal equilibria. This is the premise behind the con-
action shadowing and the equilibrium selection problem are vergence proofs in [Hu and Wellman, 1998] and [Littman,
not present for any agent. Algorithm-oriented approaches 2001]. It is also the only possibility for avoiding an equilib-
attempt to design algorithms that cope effectively with the rium selection problem using WoLF [Bowling, 2004] (note
above-mentioned problems. In general, algorithm-oriented that the WoLF variants have focused on adversarial general
approaches are superior because they enable the creation of sum games, and not at all on cooperative ones).
general-purpose learners that learn effective policies regard-
less of the reward structure. Still, it is useful to be acquainted 5.4 Algorithm-oriented Approaches for
with task-oriented approaches because algorithms designed  Equilibrium Selection
for constrained environments will not need to explicitly ad-
dress issues that are implicitly resolved by the environment. Emergent Coordination: Emergent Coordination describes
                                                      the tendency of a set of non-communicating reinforcement
5.1  Task-oriented Approaches for Action              learners to learn compatible policies because each agent is
     Shadowing                                        constantly seeking a best response to the other agents’ ac-
                                                      tions. This has been demonstrated, for example, in hexapedal
Dominant Strategies: A dominant strategy is a policy that robot locomotion [Maes and Brooks, 1990], network load
maximizes an agent’s payoff regardless of the actions of the balancing [Schaerf et al., 1995], and a cooperative box-
other agents. If the task is structured in a way that creates pushing task [Sen et al., 1994].
dominant strategies for all agents, no agent can experience Social Conventions: A social convention is a pre-arranged
action shadowing.                                     constraint on behavior that applies to all agents, such as driv-
                                                      ing on the right side of the street. This is the premise be-
5.2  Algorithm-oriented Approaches for Action         hind social learners [Mataric, 1997] and homo egualis agents
     Shadowing                                        [Nowe et al., 2001], and it has also been used as a coordina-
                                                      tion mechanism in Q-learning systems [Lauer and Riedmiller,
Joint Action Learning: If each agent is able to perceive the 2000].
actions of its counterparts, then it will be able to distinguish Strategic Learners: Strategic learners are agents that model
between high and low payoffs received for different joint ac- their counterparts and select an optimal individual strategy
tions, rather than indiscriminately attributing the payoffs to a based on that model. One commonly-used model in games
single individual action. This technique is often called joint where the agents can see each others’ actions is ﬁctitious
action learning [Claus and Boutilier, 1998]. Other exam- play [Claus and Boutilier, 1998] and its variant, adaptive play
ples of joint action learning include Friend-or-Foe Q-learning [Wang and Sandholm, 2002; Young, 1993]. Another exam-
[Littman, 2001], Nash Q-learning [Hu and Wellman, 2003], ple is that of concurrent reinforcement learners [Mundhe and
and cooperative learners [Tan, 1997].                 Sen, 2000].
  Optimistic Updates and Optimistic Exploration: For de-
terministic environments, distributed reinforcement learning 6 Applying the Framework: Incremental
[Lauer and Riedmiller, 2000] has the same effect as joint ac-
tion learning, but without giving the agents any extra infor- Policy Learning
mation – agents optimistically assume that all other agents In this section we describe a simple learning algorithm de-
will act to maximize their reward, and thus store the maxi- signed by addressing each of the conditions of Theorem
mum observed reward for each action as that action’s utility. 1. This algorithm, called Incremental Policy Learning, ad-
A variation for stochastic domains uses a weighted sum of dresses the issues of optimal individual convergence, action
the actual Q-value and an heuristic to select an action for ex- shadowing and the equilibrium selection problem. It consis-
ecution [Kapetanakis and Kudenko, 2002] . This heuristic tently learns to play a coordination equilibrium in determin-
results in effective convergence to optimal equilibria in some istic environments.
stochastic climbing games, but does not do so in all stochastic Achieving Optimal Individual Behavior: Incremental Pol-
environments.                                         icy Learning achieves optimal individual behavior by using a
  Variable Learning Rate: Another approach to addressing standard Q-learning update equation to estimate Q-values.
the problem is to minimize the effect that the learning of other Preventing Action Shadowing: Following the example of
agents has on a given agent’s own learning. This is the ap- [Claus and Boutilier, 1998], Incremental Policy Learning pre-
proach taken by WoLF variants [Bowling, 2004], in which a vents action shadowing by learning Q-values over the entire
variable learning rate for updating the Q-values has the effect joint action space. Each agent can perceive the action selec-
of holding some agents’ policies constant while others learn tions of its counterparts (but only its own reward signal) and
against the (temporarily) stationary environment.     uses this information to learn Q-values for all possible joint

                                                IJCAI-07
                                                   783actions. This enables the agents to clearly determine which
individual actions may lead to coordination equilibria.
  Addressing the Equilibrium Selection Problem: Incre-
mental Policy Learning uses a sequence of incremen-
tal policy adjustments to select between multiple optimal
equilibria. Each agent maintains a probability distribu-
tion P =  {p(a1), ..., p(am)} over its available action set
A  =  {a1, ..., am}. These probabilities are initialized and
modiﬁed according to the algorithm described below.
6.1  The Incremental Policy Learning Algorithm
  • Initialization
    ∀i, p(ai)=vi, where v is arbitrarily chosen such that
    n
      i=0 vi =1and ∀i, vi > 0.
  • Action Selection                                  Figure 1: Incremental Policy Learning performance as a func-
    In each time step t, the agent selects an action a(t) ∈ A tion of α
    according to probability distribution P . The agent exe-
                                       r t
    cutes this action, receives a reward signal ( ), updates policy are learned simultaneously, the agents do not always
                              P
    its joint Q-values, and updates as described below. achieve their maximum expected rewards. This occurs be-
  • Probability Updates                               cause the agents’ policies sometimes settle before the Q-value
    Let rmax be the maximum Q-value stored in the joint estimates for the coordination equilibria are large enough to
    action table                                      be distinguished from the Q-values of less desirable actions.
    Let 0 <α≤  1.                                     As expected, the algorithm performs better with lower values
                                                        α             α
    If r(t) ≥ rmax then ∀i:                           of  . The smaller is, the more likely it is that the joint Q-
           a t   a      p a    p a    α   − p a       values will converge to their correct values before the agents’
        if ( ( )= i) then ( i)= ( i)+   (1   ( i))    policies settle, which in turn enables the agents to easily learn
        if (a(t) =  ai) then p(ai)=p(ai) − αp(ai)     a coordination equilibrium. Interestingly, even when α ap-
  Here rmax is the reward for the target equilibrium (the re- proaches 1, the performance of the algorithm degrades rather
ward for the preferred joint action). Intuitively, whenever gracefully.
r
 max is received as a reward, the action selection probabil- 6.3 Discussion
ity distribution is skewed somewhat toward the action that
resulted in its receipt.                              The methods used by Incremental Policy Learning are simple,
  A proof sketch that the IPL algorithm meets the criteria but the principle demonstrated is powerful. An algorithm that
of Theorem 1 (and thus will result in optimal system perfor- successfully achieves individual optimal performance, avoids
mance) proceeds as follows. Condition 1 is met because the maximal action shadowing, and addresses the equilibrium se-
individual agents use standard Q-learning. Condition 2 is met lection problem will learn an optimal group behavior in coop-
because the agents are allowed to see the joint action space. erative environments. Incremental Policy Learning satisﬁes
                                                                                                     α
An argument for meeting condition 3 is given in [Fulda and these requirements in deterministic environments when is
Ventura, 2004].                                       sufﬁciently small. In fact, the algorithm performs well even
                                                      when these requirements are violated.
6.2  Results                                            Incremental Policy Learning is particularly suited to en-
We ﬁrst allow the agents to select random actions until their vironments with small numbers of interacting agents. If the
joint Q-values converge, and only then use the coordination number of agents becomes very large, a method of addressing
mechanism described above. This results in the agents con- the action shadowing problem other than joint action learning
sistently learning to play a coordination equilibrium. How- would be required. A possible alternative is to represent only
                                                                                                [
ever, such strictly controlled situations are of limited interest. signiﬁcant subsets of the joint action space, as in Fulda and
                                                                  ]
  We next experiment with two agents learning Q-values and Ventura, 2003 .
the coordination policy simultaneously. These agents repeat-
edly play a (stateless) single-stage game in which each agent 7 Conclusion
has ﬁve possible action selections. Each cell of the payoff We have identiﬁed a set of conditions sufﬁcient to guarantee
matrix was randomly initialized to an integer between 0 and optimal performance for systems of cooperative, concurrently
24 (different random payoffs were assigned to each agent), learning agents. Each condition can be met in multiple differ-
with the exception of ﬁve randomly placed coordination equi- ent ways, thus enabling the creation of learning algorithms
libria whose payoff was 25 for both agents. The algorithm that are suited to the constraints of a particular environment
was tested in both deterministic and stochastic environments or task. As an example, a learning algorithm has been pre-
(each reward signal was summed with Gaussian noise).  sented that addresses each of the conditions.
  Figure 1 shows the algorithm’s performance as a function The major advantage of our framework is that the con-
of α, averaged over 100 trials. Because the Q-values and the ditions can all be satisﬁed through algorithm-oriented ap-

                                                IJCAI-07
                                                   784