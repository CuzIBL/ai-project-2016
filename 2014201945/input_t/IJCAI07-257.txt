               Word Sense Disambiguation through Sememe Labeling 

                           *Xiangyu Duan, *Jun ZHAO, **BO XU 
                      Institute of Automation, Chinese Academy of Sciences 
                           National Laboratory of Pattern Recognition 
                         Eastern Road of ZhongGuan Cun, 95#, 100080 
                      *{xyduan, jzhao}@nlpr.ia.ac.cn, **xubohitic.ia.ac.cn 

                  Abstract                      been proposed, including GAMBL (a cascaded mem-
                                                ory-based classifiers) [Decadt et al., 2004], SenseLearner 
   Currently most word sense disambiguation (WSD) 
                                                [Mihalcea and Csomai, 2005], Naïve Bayes methods [Yuret, 
   systems are relatively individual word sense ex- 2004].  
   perts. Scarcely do these systems take word sense   Most of these methods still based on individual word 
   transitions between senses of linearly consecutive 
                                                experts while our motivation is to model word sense transi-
   words or syntactically dependent words into con- tions. This is a different view of globally modeling word 
   sideration. Word sense transitions are very impor- sense assignments. In a sentence with no syntactic informa-
   tant. They embody the fluency of semantic expres-
                                                tion, word sense transitions take the form of linear chain. 
   sion and avoid sparse data problem effectively. In That is, transitions are from left to right, word by word. In a 
   this paper, HowNet knowledge base is used to de- sentence with dependency syntactic information, word sense 
   compose every word sense into several sememes. 
                                                transitions are from all children to their parent. Although 
   Then one transition between two words’ senses linear chain word sense transitions had been studied, by 
   becomes multiple transitions between sememes. using simulated annealing [Cowie et al., 1992] and unsu-
   Sememe transitions are much easier to be captured 
                                                pervised graph-based algorithm [Mihalcea, 2005], the num-
   than word sense transitions due to much less se- ber of word sense transitions is so tremendous that they are 
   memes. When sememes are labeled, WSD is done. not easy to be captured. In order to circumvent this short-
   In this paper, multi-layered conditional random 
                                                coming, we adopt HowNet (http://www.keenage.com) 
   fields (MLCRF) is proposed to model sememe   knowledge base to decompose every word sense into several 
   transitions. The experiments show that MLCRF sememes (usually no more than 6 sememes). HowNet de-
   performs better than a base-line system and a 
                                                fines a closed set of sememes that are much less than word 
   maximum entropy model. Syntactic and hypernym senses. But various combinations of these sememes can 
   features can enhance the performance significantly. describe various word senses. It is more practical to model 
                                                sense transitions through sememes than through pure word 
1 Introduction                                  senses.
Word sense disambiguation (WSD) is one of the important   Then one transition between two words’ senses be-
tasks in natural language processing. Given the context of a comes multiple transitions between sememes. In this paper, 
word, a WSD system should automatically assign a correct we propose multi-layered conditional random fields 
sense to this word. WSD has been studied for many years. (MLCRF) to model sememe transitions. As we know, con-
Current word sense disambiguation systems are mainly in- ditional random fields (CRF) can label nodes of structures 
dividual word sense experts [Ide et al., 1998]. In these sys- like sequences and trees. Consecutive labels in CRF also 
tems, there are mainly two categories of the target word’s constitute label transitions. Therefore, the problem of mod-
contexts. One category is the words in some windows sur- eling sememe transitions can also be viewed as sememe 
rounding the target word, the other category is the relational labeling problem. 
information, such as syntactic relation, selectional prefer-   There are researches on HowNet-based WSD. Wong 
ences, etc. But senses of contextual words, which may also and Yang [2002] have done a similar work to ours. Just like 
need to be disambiguated, attract little attention. In this pa- POS tagging, they regarded WSD as word sense tagging and 
per, senses of contextual word and the target word are si- adopted a maximum entropy approach to tag senses. In sec-
multaneously considered. We try to incorporate sense tran- tion 4.2, we made some comparisons. The experiments 
sitions between every contextual word and the target word show that MLCRF performs better than a base-line system 
into one framework, and our target is to disambiguate all and Wong and Yang’s maximum entropy model. Syntactic 
words through determining word sense transitions. and hypernym features can enhance the performance sig-
   Some scholars have done relative researches on glob- nificantly.
ally modeling word sense assignments. Some methods have 


                                           IJCAI-07
                                            1594 2  An Introduction of HowNet                    of sememes denoted by yi1 ,...yim , where m is an empirical 
                                                 parameter depending on how much layers of sememes 
 HowNet [Zhendong Dong et, 2006] is a bilingual com- bound together can differentiate word senses from each 
 mon-sense knowledge base. One important component of 
                                                 other. Please note that yi1 ,...yim  are ordered by decreas-
 HowNet is the knowledge dictionary, which covers over ing importance. For example, y  is the first sememe in 
 6,500 words in Chinese and close to 7,500 English equiva-                i1
                                                 the sense definition of xi . With some simplification of the 
 lents. We use this knowledge dictionary to generate candi- HowNet knowledge dictionary, word senses can distinguish 
 date senses of a word. In HowNet, each candidate sense of each other by 2 layers of sememes. If sememes are taken as 
 one word is a combination of several sememes. For example, 
                                                 labels, every word xi  has 2 layers of labels. Word sense 
 a sense definition of the Chinese word “research institute” is disambiguation becomes a 2-layered labeling problem. Se-
 as follows:                                     meme labeling can be carried on flat sentences (sequences) 
    DEF=InstitutePlace, *research, #knowledge    or dependency syntactic trees. This is illustrated in Figure 1. 
    where word sense (DEF) is split into sememes by 
 commas. The sememes in this example are “InstitutePlace”,  yi 1,2    yi,2      yi 1,2
 “research”, “knowledge”. Symbols preceding sememes rep-
 resent relations between the entry word and the correspond-
 ing sememe. In this example, symbols are “*” and “#”.      yi 1,1    yi,1      yi 1,1
 Symbol “*” represents agent-event relation, “#” represents 
 co-relation. This word sense (DEF) can be glossed as: “re-
 search institute” is an “InstitutePlace”, which acts as an xi 1      xi        xi 1
 agent of a “researching” activity, and it has a co-relation           (a)
 with “knowledge”. 
    Sememes are the most basic semantic units that are 
 non-decomposable. It is feasible to extract a close set of            yi,2
 sememes from Chinese characters. This is because each 
 Chinese character is monosyllabic and meaning bearing.     y
                                                             j,2                   y
 Using this closed set of sememes, HowNet can define all               yi,1         k,2
 words’ senses through various combinations of these se-
 memes.                                                     y
    A word sense is defined by sememes according to an       j,1       xi          yk,1
 order. The first sememe in a word sense definition repre-
sents the main feature of the sense of that word, and this 
                                                            x j                    x
main feature is always the category of that sense. In the               (b)         k
example mentioned above, the main feature of the word 
“research institute” is “InstitutePlace”. Other sememes in a Figure 1. Graphical representation of words and sememe layers. (a) 
word sense are organized by an order of importance from is the sequence representation, and (b) is the tree representa-
 the HowNet’s point of view. In our current system, symbols tion. Solid lines show sememe transitions, dashed lines show 
 representing relations are omitted.                 related links. Please note that the direction of sememe transi-
    Totally, HowNet contains 1,600 sememes, which are tion is different for sequences and trees. For sequences, the 
 classified into 7 main classes, including “entity”, “event”, direction is from left to right. For trees, the direction is from 
 “attribute”, “attribute value”, “quantity”, “quantity value”, child node to its parent node. Although sememes can depend 
 “secondary feature”. These 7 main classes are further classi- on any word, higher layer sememes can depend on lower 
 fied hierarchically. In the end, all of 1,600 sememes are layer sememes, for clarity, we do not show these cross links. 
 organized as 7 trees. Each sememe besides the root sememe For more detailed description, see the feature set of section 
 has a hypernym (A is a hypernym of B if B is a type of A). 3.5.
 For example, “InstitutePlace” has a hypernym “organiza-   Sememes per layer constitute a label configuration of 
 tion”, “organization” has a hypernym “thing”, “thing” has a the sentence. These m layer configurations of the sentence 
 hypernym “entity”. The hypernym feature shows usefulness 
                                                 are denoted by 1,... m . Our aim is to find: 
 in our word sense disambiguation system.               ˆ  ˆ
                                                        1... m arg max ( 1... m | )  (1)
 3 System Description                                           Y1...Ym
                                                 We propose a layer-by-layer strategy “MLCRF” to solve 
 3.1 Task Definition                             this equation. Current layer sememe labeling can use labeled 
                                                 pre-layer sememes as features. The order of the strategy is 
 Our task is to disambiguate word senses through sememe bottom up, from 1st layer to mth layer. To introduce MLCRF, 
 labeling. Here is the formal description of our task. In what we introduce conditional random fields in the following 
 follows, X denotes a sentence. The ith word of X is denoted section. 
 by xi , and the sentence’s length is n. According to 
HowNet, the sense of each xi  is decomposed into m layers 


                                            IJCAI-07
                                             1595 3.2  Introduction of Conditional Random Fields                         [    ]     (7)
                                                                , 1 ,... m 1 c m
 Conditional random fields (CRF) [Lafferty et al., 2001] are           m
                                                 where    denotes the pair of mth layer labels of the words 
undirected graphical models used to calculate the condi- c,m
tional probability of a set of labels given a set of input val- of clique c.
ues. We cite the definitions of CRF in [McCallum, 2003]. It 
                                                      ( | ) , ( |  ) ,…, (  |  ,..., , )  represent 1,
defines the conditional probability proportional to the prod- 1           m  m 1 1
uct of potential functions on cliques of the graph, 2, …, mth layer probabilities respectively. Each layer CRF 
                  1                              model can be trained individually , using seque ntial CRF or 
          ( |  )         c ( c , c ) (2)         tree CRF. 
                    c ( , )                       Sutton et al. [2004] use dynamic CRF to perform mul-
where X is a set of input random variables and Y is a set of tiple, cascaded labeling tasks. Rather than perform ap-
random labels.           is the clique potential on 
               c ( c , c )                       proximate inference, we perform exact inference. To avoid 
clique c. C(Y,X) is the set of all cliques of the graph. In CRF, 
                                                 exponential increase, we propose a multi-layered Viterbi 
 clique is often an edge with a node on each end. c  repre-
 sents labels of the two end nodes. Clique potential is often algorithm for decoding. 
 defined as:                                         To illustrate multi-layered Viterbi, let us look back on 
                     R                           single layer decoding. For single layer, the most probable Y

        c ( c , c ) exp( r f r ( c , c )) (3)    is as follows: 
                    r 1
                                                     ^
where f r is an arbitrary feature function over its argu- arg max ( ( | )) arg max ( ( , )) (8)
ments,  r   is a learned weight for each feature function, R
 is the total number of feature functions of the clique. 
                                                 where  ( , )    ( , ) . The denominator is om it-
 is a normalization factor over all output values,  c               c
                                                             c
                         '
                      c ( c , c ) (4)            ted because  is the sam e for all Y. But in m ul ti-layered 
                 ' c ( ', )
Two dynamic programming algorithms are employed: case, the next layer decod ing will use pre layer labels as 
Viterbi for decoding and a variant of the forward-backward features, so the denominators are no longer the same. 
algorithm [Sha and Pereira, 2003] for the computing of ex-   For the ease of explanation, let us consider linear chain 
pectations and normalization.                    CRF of single layer. If the denominator  is not omitted 
    Two kinds of CRF are used in our system: sequential for one layer, it can also be expressed as: 
CRF and tree CRF. Sequential CRF is like traditional linear 
chain CRF, while tree CRF is slightly different. In linear       sum( 2)   sum( n)
                                                           sum( 1)                   (9)
 chain CRF, one clique is composed of current word and its        sum( 1)  sum( n 1)
 pre-word. In tree CRF, one clique is composed of current where  denotes the forward vector at the ith position of 
 word and one child of it. Tree CRF adopts sum-product al- i
 gorithm [Pearl, 1988], which is an inference algorithm in the chain. sum( ) denotes the sum of the vector ’s
 graphical models. Cohn and Blunsom [2005] have used tree 
                                                 elements. In CRF, the sum of the forward vector i ’s ele-
 CRF to solve semantic role labeling. But the algorithm was ments is the sum of all possible paths up to the ith position. 
 not presented. In this paper, we take sum-product algorithm 
                                                 So          is equal to  .
 into a forward-backward frame. Each node in the tree has sum( n )
 several sub forward (backward) vectors. The detailed de-   Then the linear chain probability is as follows: 
 scription of the algorithm is not included in this paper for 
                                                                         sum
 the limit of the paper length.                       ( | ))    ( 1, 1) log( ( 1))
                                                           ( , )  {log(sum( )) log(sum( ))} ...
 3.3  Multi-layered Conditional Random Fields               2  2          2         1
                                                           ( ,  ) {log(sum( )) log(sum( ))} (10)
 In multi-layered conditional random fields, the conditional n n          n          n 1
probability is as follows:                       Equation 8 shows that the linear chain probability can be 
                                                 computed dynamically. At each time step, we can get prob-
    ( 1... m | ) ( 1 | ) ( 2 | 1, ) ( m | m 1,..., 1, )
                                                 ability up to that time. Based on this single layer equation, 
            [   ] [   ]     [   ]
               c 1    c 2      c m    (5)        we can derive the multi-layered Viterbi algorithm. 
                                                     With some simplification of the HowNet knowledge 
                     ,       , ,...,
                      1       1 m 1              dictionary, word senses can distinguish each other by 2 lay-
 where [  ]  denotes the product of clique potentials of 
          c i                                    ers of sememes. We present a 2-layer Viterbi for sequential 
 the ithlayer.  etc denotes normalization factor. For ex- CRF in Figure 2. Multi-layered Viterbi can be extended 
 ample,                                          from 2-layer Viterbi. In Figure 2, all forward vectors 
                 R                               and transition matrices M are only about second layer. S
                     f                                                                       i
 [   c ]m   exp(   r  r ( c,m , c,m 1,... c,1, )) (6) denotes a combined label of 2 layers at ith position, S
           c    r 1                                                                         i,2


                                            IJCAI-07
                                             1596 denotes the second layer label at ith position. Suppose that DefSpace and the other is ExDefSpace. In the following, we 
                                                 will introduce how to apply these two methods to first layer 
the label set of each layer contains k elements, then Si has 
 k*k kinds of 2-layer combinations.              CRF in detail. 
    In 2-layer Viterbi, we do not need transition ma trices DefSpace
                   st
and forward vectors of 1  layer because  is the same for For DefSpace, candidate first layer sememes of a word are 
              nd                                 the first layer sememes appeared in the definitions of this 
all 1 . But for 2  layer, the normalizing factor is ,
                                            , 1  word in HowNet. DefSpace generates a real space that only 
 where  is the 1st layer sememes that are related to 1st
       1                                         includes the labels (sememes) listed in the definitions of the 
 layer decoding. So forward vectors and transition matrices entry word. For example, Chinese word fazhan has 3 senses 
 are neede d to calculate normalization factor for the 2nd layer in the HowNet dictionary: “CauseToGrow”, “grow” and 
only.            is the forward vector of 2nd layer at ith “include”. Since all these senses are single sememes, candi-
       i (si 1,2 , si,2 )
                                      nd         date sememes of the first layer are these 3 sememes for 
 position. M (s , s )   is the transition matrix of 2  layer at 
          i i 1,2 i,2                            DefSpace.
 ith position. [ ]  and [ ]   are the logarithm value 
               i,1       i,2                     ExDefSpace
of 1st layer and 2nd lay er’s ith clique potential respectively.  ExDefSpace is the extended version of DefSpace. Suppose 
                                                 there are two words a and b. According to the HowNet dic-
                                                 tionary, a has 2 candidate first layer sememes sem_1 and 
  Initial value:Val(s0 ) 0 , s0 is the initial state 
   '                                             sem_2, b has 2 candidate first layer sememes sem_2 and 
    0 (s ) [1...0]
     0,2                                         sem_3. Suppose a appears in the training corpus while b
  for every positio n i:                         does not appears. Considering training examples of a with 

    for every candidate 2-layer label si :       sem_1 or sem_2, for DefSpace, the training procedure only 
                '                                discriminates sem_1 and sem_2, while sem_2 and sem_3 
      (s  , s ) i 1(s )M (s , s )
     i i 1,2 i,2   i 1,2 i i 1,2 i,2             are not discriminated in this discriminative training. Then 
                                                 when b appears in test data, it can’t be disambiguated. To 
    Val(si ) max{Val(si 1) pair(si 1,si )}
           si 1                                  overcome this shortcoming of DefSpace, candidate se-
                                                 memes of a word are extended. For every sememe in 
     pair(si 1,si ) [ ]i,1 [ ]i,2
                           '                     HowNet, we build a candidate sememe list. Concretely 
    {log[sum( (s ,s ))] log[sum( i 1(s ))]}
           i i 1,2 i,2       i 1,2               speaking, for every sememe s, we search the whole diction-
                                                 ary entries to find sememes of first layer that together with s
     '
    s i 1 argmax{Val(si 1) pair(si 1,si )}       define a common polysemous word. All such sememes are 
           si 1                                  put into the candidate sememe list of s. Then during training, 
     ' (s ) (s' ,s )                             the candidate sememes of a word are generated according to 
     i i,2  i i 1,2 i,2                          the hand labeled sememe’s candidate list. For above exam-
  till the end node                              ple, sem_3 is added to the candidate lists of sem_1 and 
                                                 sem_2. Then word a provides a positive example of sem_1 
                                            '
 Figure 2. 2-Layer V iterbi algorithm for Sequential CRF. si 1 or sem_2, but a negative example of sem_3. Also, if one 
    record the optimal path.                     sem_3 is tagged in the training corpus, it provides a negative 
                                                 example of sem_1 or sem_2. 
    The time complexity of 2-layer Viterbi for one sen- Finally, in a sememe’s candidate sememe list, there are 
                                   st
 tence is O( J 2 K 4T ), where J is t he number of 1  layer labels, average 11.76 candidate sememes. 
 K is the number of 2nd layer labels, T is the length of the Candidate second layer sememes of a word are gener-
sentence. Figure 2 mainly deals with sequence. When ap- ated according to this word’s entries in HowNet whose first 
plying it to trees, sub s for every tree node should be layer sememes are the same as the labeled first layer se-
introduced. In our problem for WSD, S  is the candidate meme. This is like DefSpace used in first layer CRF. 
                                i                    In the phase of decoding (multi-layered Viterbi), only 
 sense at position i, which is less than 2-layer combinations 
                                                 multiple senses of a word are regarded as candidate senses 
 of entire labels.                               of this word. No special measure like ExDefSpace is taken.
 3.4  Reducing the Number of Candidate Labels    3.5 Feature Set 
 In common CRF, every input variable xi   has several can- In MLCRF, training is taken layer by layer. For first layer,
 didate labels. There are 1,600 sememes in HowNet. For our we use the following factored representation for features. 
WSD problem, it is impractical to treat all these sememes as 
                                                           f        p  c q  '
candidate labels for the training of single layer CRF.      ( ,  )   ( , ) ( c , c ) (11)
    Using the HowNet knowledge dictionary we can gen- where p( ,c)  is a binary predicate on the input X and 
erate candidate sememes much less. We propose two meth-           '
ods to reduce the number of candidate sememes. One is current clique c, q( c , c ) is a binary predicate on pairs 


                                            IJCAI-07
                                             1597 of labels of curre nt clique c. For instance, p( ,c)  might annotations, and the figure is 0.88. This corpus is included 
                                                 in ChineseLDC. 
 be “ whether word at position i is de”.
                                                     For comparison, we build a base-line system and du-
    Table 1 presents the categories of p( ,c)  and 
    '                                            plicate Wong and Yang’s method [Wong et al., 2002]. The 
 q( c , c ) .                                    base-line system predicts word senses according to the most 
  q( ' , )     p(X ,c)   q( ' , )   p(X ,c)      frequ ent sense of the word in the training corpus. For words 
     c c seq         seq   c  c tree      tree   that do not appear in the training corpus, we choose the first 
                                                 sense entry in the HowNet dictionary. We also imitate 
  s_i_0        true     s_n_0       true         Wong’s method. Their method is at a word level. The sense 
  s_i-1_0               s_c_0                    of a word is the first sememe optionally combined with the 
  s_i_0,s_i-1_0         s_c_0,s_n_0              second sememe. They defined this kind of word sense as 
 ^ s_i_0                ^s_n_0                   categorical attribute (and also a semantic tag). Then an 
 ^ s_i-1_0              ^s_c_0                   off-the-shelf POS tagger (MXPOST) is used to tag these 
                                                 senses.
  s_i_0        uni_w    s_n_0       uni_w            In our system, named entities have been mapped to 
               bi_w                 bi_w         predefined senses. For example, all person named entities 
               uni_p                uni_p        have the sense “human| ”. For those words that do not 
               bi_p                 bi_p         have entries in dictionary, we define their sense by referring 
               tri_p                tri_p        to their synonyms that have entries. In the end, polysemous 
               (from i-2            (from ff to  words take up about 1/3 of the corpus, and there are average 
               to i+2)              cl and cr)   3.7 senses per polysemous word. 
                                                     In the following sections, precisions of polysemous 
      Table 1. feature set of sequ ential CRF and tr ee CRF words are used to measure the performance. Since all words 
                                                 have entries in dictionary or can be mapped to some senses, 
    For first layer, a label of the clique c is the first layer 
                             st                  recall is the same with precision. Note that for single layer 
 sememe. In table 1, s_i_0 denotes 1 layer sememe of ith sememe labeling, precision is the rate of correct sememes of 
                                  st
word in sequential CRF and s_n_0 denotes 1  layer sem eme the corresponding layer. Sense precision is the rate of cor-
 of cu rrent node n in tree CRF. “^” denotes hypernym, uni, bi rect senses, not sememes. 
 and tri denote unigrams, bigrams and trigrams respectively, 
 w denotes word, p denotes POS. The subscript seq and tree 4.2  Results of MLCRF in WSD 
 represent sequential CRF and tree CRF respectively. For       1st layer 2nd layer   Sense
 tree CRF, c denotes the child of n in current clique. cl and cr c +h c +h
 are the left and right sibling of node c. f denotes the parent 
                                                              881.0 78. 8 7.2  85.1   79.9
 node of n, ff denotes grandfather of n.             Seq Def
    For second layer, s_i_0 (s_n_0) is replaced by s_i_1      80.6  78.5 - -          79.3 
                                                     Seq ExDef
 (s_n_1). p(X,c) excluding true are appended by s_i_0 
                                                              82.6  84.0 90.0  95.9   85.9
(s_n_0). For true value, when q is about s_i_1 (s_n_1) or Tree Def
s_i-1 _1 (s_c_1), additional feature s_i_0 (s_n_0) is added to
                                                    TreeExDef 82.0  83.5   -     -    85.1
p(X,c).
                                                 Table 2. Perf orm ance  of ML CR F in WS D. Nu mbers a re 
 4  Experiments and Results                          precisions of po lys emous w ord s wi th  resp ect to first 
                                                     layer, second layer and whole word sense. 
 4.1 Experimental Settings                       Table 2 shows the performance of MLCRF in WSD. “c”
 Sememe (HowNet) based disambiguation is widely used in represents common features that presented in table 1 except 
 Chinese WSD such as Senseval 3 Chinese lexical sample hypernym features. “+h” means common features  plus hy-
 task. But full text disambiguation at sememe level is pernym features. Seq  (Seq ) denotes sequential 
 scarcely considered. Furthermore, the full text corpus tagged    Def     ExDef

 with WordNet like thesaurus is not publicly available. The CRF using DefSpace (ExDefSpace). Tree Def  (TreeExDef )
 sentences used in our experiments are from Chinese Lin- denotes tree CRF using DefSpace (ExD efSpace). From table 
 guistic Data Consortium (http://www.chineseldc.org) that 2, we can see that tree CRF performs better than sequentia l
 contains phrase structure syntactic trees. We can generate CRF, which shows that sense transitions in trees are easie r
 gold standard dependency trees using head rules. We se-
                                                 to be captured than in sequences. Hypernym features en-
 lected some dependency trees and hand tagged senses on 
 them with over 7,000 words for training and over 1,600 hance performance of tree CRF partly because it avoids 
 words for testing. There are two taggers to tag the corpus sparse data problem, while it adds noise to sequential CRF. 
 and an adjudicator to decide the tagging results. The agree- ExDefSpace performs similarly as DefSpace, which maybe 
 ment figure is the ratio of matches to the total number of due to our relatively small corpus. 


                                            IJCAI-07
                                             1598