                              Improving Author Coreference by
                Resource-bounded Information Gathering from the Web
                            Pallika Kanani, Andrew McCallum, Chris Pal
                                    Department of Computer Science
                                  University of Massachusetts Amherst
                                       Amherst, MA 01003 USA
                                {pallika, mccallum, pal} @ cs.umass.edu

                    Abstract                            The speciﬁc entity resolution domain we address is re-
                                                      search paper author coreference. The vertices in our coref-
    Accurate entity resolution is sometimes impossi-  erence graphs are citations, each containing an author name
    ble simply due to insufﬁcient information. For ex- with the same last name and ﬁrst initial.1 Coreference in this
    ample, in research paper author name resolution,  domain is extremely difﬁcult. Although there is a rich and
    even clever use of venue, title and co-authorship complex set of features that are often helpful, in many sit-
    relations are often not enough to make a conﬁ-    uations they are not sufﬁcient to make a conﬁdent decision.
    dent coreference decision. This paper presents sev- Consider, for example, the following two citations both con-
    eral methods for increasing accuracy by gather-   taining a “D. Miller.”
    ing and integrating additional evidence from the
                                                        • Mark Orey and David Miller, Diagnostic Computer Sys-
    web. We formulate the coreference problem as one
                                                          tems for Arithmetic, Computers in the School, volume
    of graph partitioning with discriminatively-trained
                                                          3, #4, 1987
    edge weights, and then incorporate web informa-
    tion either as additional features or as additional • Miller, D., Atkinson, D., Wilcox, B., Mishkin, A., Au-
    nodes in the graph. Since the web is too large to     tonomous Navigation and Control of a Mars Rover, Pro-
    incorporate all its data, we need an efﬁcient pro-    ceedings of the 11th IFAC Symposium on Automatic
    cedure for selecting a subset of web queries and      Control in Aerospace, pp. 127-130, Tsukuba, Japan,
    data. We formally describe the problem of re-         July 1989.
    source bounded information gathering in each of     The publication years are close; and the titles both relate
    these contexts, and show signiﬁcant accuracy im-  to computer science, but there is not a speciﬁc topical over-
    provement with low cost.                          lap; “Miller” is a fairly common last name; and there are no
                                                      co-author names in common. Furthermore, in the rest of the
                                                      larger citation graph, there is not a length-two path of co-
1  Introduction                                       author name matches indicating that some of the co-authors
Machine learning and web mining researchers are increas- here may have themselves co-authored a third paper. So there
ingly interested in using search engines to gather information is really insufﬁcient evidence to indicate a match despite the
for augmenting their models, e.g. [Etzioni et al., 2004], [Mc- fact that these citations do refer to the same “Miller”.
Callum and Li, 2003], [Dong et al., 2004]. However, it is In this paper, we present two different mechanisms for aug-
impossible to query for the entire web, and this gives rise to menting the coreference graph partitioning problem by incor-
the problem of efﬁciently selecting which queries will pro- porating additional helpful information from the web. In both
vide the most beneﬁt. We refer to this problem as resource- cases, a web search engine query is formed by conjoining the
bounded information gathering from the web.           titles from two citations. The ﬁrst mechanism changes the
  We examine this problem in the domain of entity resolu- edge weight between the citation pair by adding a feature in-
tion. Given a large set of entity names (each in their own con- dicating whether or not any web pages were returned by the
text), the task is to determine which names are referring to query. The second mechanism uses one of the returned pages
the same underlying entity. Often these coreference merging (if any) to create an additional vertex in the graph, for which
decisions are best made, not merely by examining separate edge weights are then calculated to all the other vertices. The
pairs of names, but relationally, by accounting for transitive additional transitive relations provided by the new vertex can
dependencies among all merging decisions. Following previ- provide signiﬁcant helpful information. For example, if the
ous work, we thus formulate entity resolution as graph par- new vertex is a home page listing all of an author’s publica-
titioning with edge weights based on many features with pa- tions, it will pull together all the other vertices that should be
rameters learned by maximum entropy [McCallum and Well- coreferent.
ner, 2004], and in this paper explore a relational, graph-based 1Future work will address the problem of correctly merging
approach to resource-bounded information gathering.   names with typographic errors in the ﬁrst initial and last name.

                                                IJCAI-07
                                                   429  Gathering such external information for all vertex pairs in can then be expressed as
                                                                            
the graph is prohibitively expensive, however. Thus, meth-           1        
ods that acknowledge time, space and network resource lim- P (y|x)=     exp      λ f (x ,x ,y )+
                                                                   Z(x)           l l  i j  ij
itations, and effectively select just a subset of the possible                i,j,l
queries are of interest. Learning and inference under resource                                 
                                                                            
limitations has been studied in various forms. For example,
                                                                               λ∗f∗(y  ,y ,y  ) ,     (2)
the value of information, as studied in decision theory, mea-                        ij  jk ik
sures the expected beneﬁt of queries [Zilberstein and Lesser,               i,j,k
1996]. Budgeted learning, rather than selecting training in- where y = {yij : ∀i,j} and
stances, selects new features [Kapoor and Greiner, 2005].                                              
                                                      Z(x)=      exp     λ f (x ,x ,y )+   λ  f (y ,y ,y  )
Resource-bounded reasoning studies the trade offs between                 l l i  j  ij       ∗ ∗ ij  jk ik
computational commodities and value of the computed re-       y       i,j,l             i,j,k
sults [Zilberstein, 1996]. Active learning aims to request hu-                                        (3)
                                                                                                    λ
man labeling of a small set of unlabeled training examples As in Wellner and McCallum [2002], the parameters can
[Thompson et al., 1999], for example, aiming to reduce label be estimated in local fashion by maximizing the product of
entropy on a sample [Roy and McCallum, 2001].         Equation 1 over all edges in a labeled graph exibiting the true
                                                      partitioning. When fl(xi,xj, 1) = −fl(xi,xj, 0) it is possi-
  In this paper we employ a similar strategy, and com- ble to construct a new undirected and fully connected graph
pare it with two baseline approaches, showing on 7 dif-
                                                      consisting of nodes for mentions, edge weights ∈ [−∞, ∞]
ferent data sets that leveraging web queries can reduce F1 deﬁned by λ (x ,x ,y ) and with sign deﬁned by the
error by 13.03%, and furthermore that, by using our pro-          l l  i  j  ij
                                                      value of yij. In our work here we deﬁne a graph in a sim-
posed resource-bounded approach, 53.5% of this gain can be ilar fashion as follows.
achieved with about 1% of the web queries. We also suggest
                                                        Let G0 =<V0,E0   > be a weighted, undirected and fully
that our problem setting will be of interest to theoretical com-
                                                      connected graph, where V0 = {v1,v2, ..., v } is the set of
puter science, since it is a rich extension to correlational clus-                          n
                                                      vertices representing mentions and E0 is the set of edges
tering [Bansal et al., 2002; Demaine and Immorlica, 2003].
                                                      where ei =<vj,vk >  is an edge whose weight wij is given
                                                      by P (yij =1|xi,xj) − P (yij =0|xi,xj)  or the differ-
                                                      ence in the probabilities that that the citations vj and vk are
2  Conditional Entity Resolution Models               by the same author. Note that the edge weights deﬁned in
                                                      this manner are in [−1, +1]. The edge weights in E0 are
We are interested in obtaining an optimal set of coreference noisy and may contain inconsistencies. For example, given
assignments for all mentions contained in our database. In our the nodes v1, v2 and v3, we might have a positive weight
approach, we ﬁrst learn maximum entropy or logistic regres- on <v1,v2 > as well as on <v2,v3 >, but a high neg-
sion models for pairwise binary coreference classiﬁcations. ative weight on <v1,v3 >. Our objective is to partition
We then combine the information from these pairwise mod- the vertices in graph G0 into an unknown number of M non-
els using graph-partitioning-based methods so as to achieve a overlapping subsets, such that each subset represents the set
good global and consistent coreference decision. We use the of citations corresponding to the same author. We deﬁne our
                                                                        F =      w  f(i, j)    f(i, j)=1
term, “mention” to indicate the appearance of an author name objective function as ij ij  where
                                                           x     x                          −1
in a citation and use xi to denote mention i =1,...,n. Let when i and j are in the same partition and otherwise.
yij represent a binary random variable that is true when men- [Bansal et al., 2002] provide two polynomial-time approx-
tions xi and xj refer to the same underlying author “entity.” imation schemes (PTAS) for partitioning graphs with mixed
For each pair of mentions we deﬁne a set of l feature func- positive and negative edge weights. We obtain good empir-
tions fl(xi,xj,yi,j ) acting upon a pair of mentions. From ical results with the following stochastic graph partitioning
these feature functions we can construct a local model given technique, termed here N-run stochastic sampling.
by
                                                        Algorithm 1. – N-Run Stochastic Sampling:
                      1                                                                      G   P (w ) ∝
       P (y |x ,x )=     exp(λ f (x ,x ,y )),   (1)     We deﬁne a distribution over all edges in 0, i
          i,j i  j   Z        l l i  j  ij            e− wi
                       x                                 T where T acts as temperature. At each iteration, we draw
                                                     an edge from this distribution and merge the two vertices.
      Z  =     exp(λ f (x ,x ,y ))                    Edge weights to the new vertex formulated by the merge are
where  x      y     l l  i j  ij . In McCallum and
Wellner [2003] a conditional random ﬁeld with a form similar set to the average of its constituents and the distribution over
to (1) is constructed which effectively couples a collection of the edges is recalculated. Merging stops when no positive
pairwise coreference models using equality transitivity func- edges remain in the graph. This procedure is then repeated
                                                      r =1...N times and the partitioning with the maximum F is
tions f∗(yij,yjk,yik) to ensure globally consistent conﬁgura-
tions. These functions ensure that the coupled model assigns then selected.
zero probability to inconsistent conﬁgurations by evaluating
to −∞  for inconsistent conﬁgurations and 0 for consistent 3 Coreference Leveraging the Web
conﬁgurations. The complete model for the conditional dis- Now, consider that we have the ability to augment the graph
tribution of all binary match variables given all mentions x with additional information using two alternative methods:

                                                IJCAI-07
                                                   430    (A)..., H. Wang, ... Background Initialization..., ICCV,...2005.
    (B)..., H. Wang, ... Tracking and Segmenting People..., ICIP, 2005.
    (C)..., H. Wang, ... Gaussian Background Modeling..., ICASSP, 2005.
    (D)..., H. Wang, ... Facial Expression Decomposition..., ICCV, 2003.
    (E)..., H. Wang, ... Tensor Approximation..., SIGGRAPH. 2005.
    (F)..., H. Wang, ... High Speed Machining..., ASME, (JMSE), 2005.

            Figure 1: Six Example References

(1) changing the weight on an existing edge, (2) adding a
new vertex and edges connecting it to existing vertices. This Figure 2: Extending a pairwise similarity matrix with additional
new information can be obtained by querying some external web mentions. A..F are citations and 1..10 are web mentions.
source, such as a database or the web.
  The ﬁrst method may be accomplished in author corefer-
                                                                                 i     E   ⊂ E
ence, for example, by querying a web search engine as fol- responding piece of information i. Let s 0, be this set
                                                          I
lows. Clean and concatenate the titles of the citations, issue and s be the subset of information obtained that corresponds
                                                                            E             E
this query and examine attributes of the returned hits. In this to each of the elements in s. The size of s is determined
case, a hit indicates the presence of a document on the web by the amount of resources available. Our objective is to ﬁnd
                                                               E                            F         G
that mentions both these titles and hence, some evidence that the subset s that will optimize the function on graph 0
                                                                   I
they are by the same author. Let f be this new boolean fea- after obtaining s and applying graph partitioning.
                            g                                                                G
ture. This feature is then added to an augmented classiﬁer Similarly, in the case of expanded graph , given the
                                                                                        V  ⊂ V
that is then used to determine edge weights.          constraint on resources, we must select s 1, to add to
  In the second method, a new vertex can be obtained by the graph. Note that in the context of information gathering
                                                                   |V |
querying the web in a similar fashion, but creating a new ver- from the web, 1 is in the billions. Even in the case when
                                                      |V |
tex by using one of the returned web pages as a new mention. 1 is much smaller, we may choose to calculate the edge
                                                                               E      E  ⊂ E
Various features f(·) will measure compatibility between the weights for only a subset of 1. Let s 1 be this set.
                                                                  V     E
other “citation mentions” and the new “web mention,” and The sizes of s and s are determined by the amount of
                                                                                                      V 
with similarly estimated parameters λ, edge weights to the resources available. Our objective is to ﬁnd the subsets s
                                                          E                            F          G
rest of the graph can be set.                         and  s that will optimize the function on graph 0 by
                              G                       applying graph partitioning on the expanded graph. We now
  In this case, we expand the graph 0, by adding a new set                               E
of vertices, V1 and the corresponding new set of edges, E1 to present the procedure for the selection of s.
create a new, fully connected graph, G. Although we are not
                                                         Algorithm 2. – Centroid Based Resource Bounded In-
interested in partitioning V1, we hypothesize that partitioning
G                               F     G              formation Gathering and Graph Partitioning For each clus-
   would improve the optimization of on 0. This can   ter of vertices that have been assigned the same label under a given
                        v1,v2V0 v3V 1
be explained as follows. Let    ,      , and the edge partitioning, we deﬁne the centroid as the vertex vc with the largest
<v1,v2  > has an incorrect, but high negative edge weight. sum of weights to other members in its cluster. Denote the subset
However, the edges <v1,v3 > and <v2,v3  > have high   of vertex centroids obtained from clusters as Vc. (We can also op-
positive edge weights. Then, by transitivity, partitioning the tionally pick multiple centroids from each cluster.) We begin with
       
graph G will force v1 and v2 to be in the same subgraph and graph G0 obtained from the base features of the classiﬁer. We use
improve the optimization of F on G0.                  the following criteria for ﬁnding the best order of queries: expected
  As an example, consider the references shown in Fig.1. Let entropy, gravitational force, uncertainty-based and random. The un-
us assume that based on the evidence present in the citations, certainty criteria uses the entropy of the binary classiﬁer for each
we are fairly certain that the citations A, B and C are by H. edge. For each of these criteria, we follow this procedure.
Wang 1 and that the citations E and F are by H. Wang 2. Let 1. Partition graph G0 using N-run stochastic sampling.
                                                                                            ∗
us say we now need to determine the authorship of citation 2. From the highest scoring partitioned graph Gi , ﬁnd the subset
D. We now add a set of additional mentions from the web,  of vertex centroids Vc
{1, 2, .. 10}. The adjacency matrix of this expanded graph
                                                        3. Construct Es as the set of all edges connecting centroids in Vc.
is shown in Fig. 2. The darkness of the circle represents the
                                                                    E            I
level of afﬁnity between two mentions. Let us assume that 4. Order edges s into index list based on the criteria.
the web mention 1 (e.g. the web page of H. Wang 1) is found 5. Using index list I, for each edge ei ⊂ Es
to have strong afﬁnity to the mentions D, E and F. Therefore, (a) Execute the web query and evaluate additional features
by transitivity, we can conclude that mention D belongs to the from result
group 2. Similarly, values in the lower right region could also (b) Evaluate classiﬁer for edge ei with the additional features
help disambiguate the mentions through double transitivity.   and form graph Gi from graph Gi−1
                                                           (c) Using graph Gi, perform N-run stochastic sampling and
4  Resource Bounded Web Usage                                 compute performance measures
Under the constraint on resources, however, we must select Criterion 1 - Expected Entropy
only a subset of edges in E0, for which we can obtain the cor- 1. Force merge of the vertex pair of ei to get a graph Gp

                                                IJCAI-07
                                                   431 2. Peform N-run stochastic sampling on Gp. This gives the prob- Corpus # Sets # Authors # Citations # Pairs
    abilities pi for each of the edges in Gp             DBLP      18      103         945      43338
 3. CalculateP the entropy, Hp of the graph Gp as follows: Rexa    8       289        1459     207379
    Hp = −   i Pi log Pi                                  Penn     7       139        2021     455155
 4. Force split of the vertex pair of ei to get a graph Gn Rbig    18      103        1360     126205
 5. Repeat steps 2-3 to calculate entropy, Hn for graph Gn
                                                                Table 1: Summary of Data set properties.
 6. The expected entropy, Hi for the edge ei is calculated as:
    H    (Hp)+(Hn)
     i =     2    (Assuming equal probabilities for both out-
    comes)                                              The ’Rbig’ corpus consists of a collection of web docu-
Criterion 2 - Gravitational Force                     ments which is created as follows. For every dataset in the
This selection criteria is inspired by the inverse squared law of DBLP corpus, we generate a pair of titles and issue queries
the gravitational force between two bodies. It is deﬁned as F = to Google. Then, we save the top ﬁve results and label them
 M1∗M2                     M      M
Γ  d2  , where Γ is a constant, 1 and 2 are analogous to to correspond with the authors in the original corpus. The
masses of two bodies and d is the distance between them. This number of pairs in this case corresponds to the sum of the
criteria ranks highly partitions that are near each other and large, products of the number of web documents and citations in
and thus high-impact candidates for merging. Let vj and vk be the each dataset.
two vertices connected by ei. Let Cj and Ck be their corresponding All the corpora are split into training and test sets roughly
clusters. We calculate the value of F as described above, where M1 based on the total number of citations in the datasets. We
and M2 are the number of vertices in Cj and Ck respectively. We keep the individual datasets intact because it would not be
     d    1       w                      e    x
deﬁne  = xwi , where i is the weight on the edge i and is a possible to test graph partitioning performance on randomly
parameter that we tune for our method.                split citation pairs.

5  Theoretical Problem                                6.2  Baseline, Web Information as a Feature and
There has been recent interest in the general problem of cor- Effect of Graph Partitioning
relational clustering [Bansal et al., 2002; Demaine and Im-
morlica, 2003]. We now present a new class of problems that The maximum entropy classiﬁer described in Section 2 is
are concerned with resource bounded information gathering built using the following features. We use the ﬁrst and middle
under graph partitioning.                             names of the author in question and the number of overlap-
  Consider the matrix as presented in Fig.2. Suppose we care ping co-authors. The US census data helps us determine how
about the partitioning in the upper left part of the matrix, and rare the last name of the author is. We use several differ-
all the values in the upper right part of the matrix are hidden. ent similarity measures on the titles of the two citations like
As we have seen before, obtaining these values would impact the cosine similarity between the words, string edit distance,
the graph partitioning in the section that we care about. TF-IDF measure and the number of overlapping bigrams and
  Now, suppose, we have access to an adversarial oracle, trigrams. We also look for similarity in author emails, in-
who unveils these values in the requested order. In the worst stitution afﬁliation and the venue of publication if available.
case, no useful information is obtained till the last value is un- We use a greedy agglomerative graph partitioner in this set of
veiled. In the best case, however, requesting a small fraction experiments and are interested in investigating the effect of
of the values, leads to perfect partitioning in the section that using a stochastic partitioner.
we care about. The question that we now ask is, what is the The baseline column in Table 2 shows the performance of
best possible order to request these values. Making reason- this classiﬁer. Note that there is a large number of negative
able assumptions about the nature of the oracle and imposing examples in this dataset and hence we prefer pairwise F1 over
restrictions on the edge weights makes this problem interest- accuracy as the main evaluation metric. Table 2 shows that
ing and useful. This is one way to formulate this problem. graph partitioning signiﬁcantly improves pairwise F1. We
This paper opens up many such possibilities for the theory also use area under the ROC curve for comparing the per-
community.                                            formance of the pairwise classiﬁer, with and without the web
                                                      feature.
6  Experimental Results                                 Note that these are some of the best results in author coref-
6.1  Dataset and Infrastructure                       erence and hence qualify as a good baseline for our experi-
We use the Google API for searching the web. The data sets ments with the use of web. It is difﬁcult to make direct com-
used for these experiments are a collection of hand labeled parison with other coreference schemes [Han et al., 2005] due
citations from the DBLP and Rexa corpora (see table 1). The to the difference in the evaluation metrics.
portion of DBLP data, which is labeled at Pennstate Univer- Table 2 compares the performance of our model in the ab-
sity is referred to as ’Penn’. Each dataset refers to the cita- sence and in the presence of the Google title feature. As
tions authored by people with the same last name and ﬁrst described before, these are two completely identical models,
initial. The hand labeling process involved carefully segre- with the difference of just one feature. The F1 values improve
gating these into subsets where each subset represents papers signiﬁcantly after adding this feature and applying graph par-
written by a single real author.                      titioning.

                                                IJCAI-07
                                                   432 Method             AROC    Acc    Pr   Rec    F1          Method            Precision Recall   F1
 Baseline    class.  .847   .770  .926  .524  .669         Merge Only
 DBLP        part.    -     .780  .814  .683  .743         Expected Entropy    73.72    87.92  72.37
 W/ Google   class.  .913   .883  .907  .821  .862         Gravitational Force 63.10    92.37  64.55
 DBLP        part.    -     .905  .949  .830  .886         Uncertainty         64.95    87.83  63.54
 Baseline    class.  .866   .837  .732  .651  .689         Random              63.97    89.46  64.23
 Rexa        part.    -     .829  .634  .913  .748         Merge and Split
 W/ Google   class.  .910   .865  .751  .768  .759         Expected Entropy    76.19    58.56  60.90
 Rexa        part.    -     .877  .701  .972  .814         Gravitational Force 64.10    53.06  53.56
 Baseline    class.  .688   .838  .980  .179  .303         Uncertainty         66.56    54.45  55.32
 Penn        part.    -     .837  .835  .211  .337         Random              66.45    50.47  52.27
 W/ Google   class.  .880   .913  .855  .672  .752         No Merge
 Penn        part.    -     .918  .945  .617  .747         Expected Entropy    91.46    38.46  51.06
                                                           Gravitational Force 91.53    37.84  50.47
Table 2: Effect of using the Google feature. Top row in each corpus Uncertainty 87.01   41.91  52.70
indicates results for pairwise classiﬁcation and bottom row indicates Random   86.96    43.77  54.03
results after graph partitioning.
                                                      Table 4: Area Under Curve for different Resource Bounded Infor-
                                                      mation Gathering criteria
6.3  Expanding the Graph by Adding Web
     Mentions
In this case, we augment the citation graph by adding docu- and pick top 20% tightly connected vertices in each cluster.
ments obtained from the web. We build three different kinds We experiment with ordering these query candidates accord-
of pairwise classiﬁers to ﬁll the entries of the matrix shown in ing to the four criteria: expected entropy, gravitational force,
Fig 2. The ﬁrst classiﬁer, between two citations, is the same uncertainty-based and random. For each of the queries in the
as the one described in the previous section. The second clas- proposed order, we issue a query to Google and incorporate
siﬁer, between a citation and a web mention, predicts whether the result into the binary classiﬁer with an additional feature.
they both refer to the same real author. The features for this If the prediction from this classiﬁer is greater than a thresh-
second classiﬁer include, occurrence of the citation’s author old (t = 0.5), we force merge the two nodes together. If lower,
and coauthor names, title words, bigrams and trigrams in the we have two choices. We can impose the force split, in ac-
web page. The third classiﬁer, between two web mentions, cordance with the deﬁnition of expected entropy. We call this
predicts if they both refer to the same real author or not. Due approach “split and merge”. The second choice is to not im-
to the sparsity of training data available at this time, we set pose the force split, because, in practice, Google is not an
the value of zero in this region of the matrix, indicating no oracle and absence of co-occurence of two citations on the
preference. We now run the greedy agglomerative graph par- web is not an evidence that they refer to different people. We
titioner on this larger matrix and ﬁnally, measure the results call this approach “merge only”. The third choice is to simply
on the upper left matrix.                             incorporate the result of the query into the edge weight.
  We compare the effects of using web as a feature and web After each query, we rerun the stochastic partitioner and
as a mention on the DBLP corpus. We use the Rbig corpus for note the precision, recall and F1. This gives us a plot for a
this experiment. Table 3 shows that the use of web as a men- single dataset. Note that the number of proposed queries in
tion improves the performance on F1. Note that alternative each dataset is different. We get an average plot by sampling
query schemes may yield better results.               the result of each of the datasets for a ﬁxed number of points,
                                                      n (n = 100). We interpolate when queries fewer than n are
     Data          Acc.    Pr.    Rec.    F1          proposed. We then average across these datsets and calculate
     Baseline      .7800  .8143  .6825  .7426         the area under these curves, as shown in Table 4.
     Web Feature   .9048  .9494  .8300  .8857           These curves measure the effectiveness of a criteria in
     Web Mention   .8816  .8634  .9462  .9029         achieving maximum possible beneﬁt with least effort. Hence,
                                                      a curve that rises the fastest, and has the maximum area under
Table 3: DBLP Results when using Web Pages found by Google as the curve is most desired. Expected entropy approach, gives
Extra Mentions(Rbig).                                 the best performance on F1 measure, as expected.
                                                        It is interesting to note that the gravitational-force-based
                                                      criteria does better than the expected entropy criteria on re-
6.4  Applying the Resource Bounded Criteria for       call, but worse on the precision. This is because this ap-
     Selective Querying                               proach captures the sizes of the two clusters and hence tends
We now turn to the experiments that use different criteria for to merge large clusters, without paying much attention to the
selectively querying the web. We present the results on test ’purity’ of the resulting clusters. The expected entropy ap-
datasets from DBLP and Rexa corpora. As described ear- proach, on the other hand, takes this into account and hence
lier in Section 4, the query candidates are the edges connect- emerges as the best method. The force-based approach is a
ing centroids of initial clustering. We use multiple centroids much faster approach and it can be used as a heuristic for

                                                IJCAI-07
                                                   433