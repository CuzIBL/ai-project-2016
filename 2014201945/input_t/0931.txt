                   Compiling Bayesian Networks with Local Structure

                                Mark Chavira     and  Adnan Darwiche
                                     Computer Science Department
                                 University of California, Los Angeles
                                     Los Angeles, CA 90095-1596
                                    {chavira,darwiche}@cs.ucla.edu

                    Abstract                          and then immediately extracts the AC from the CNF factor-
                                                      ization. The beneﬁt of this logical approach is twofold. First,
    Recent work on compiling Bayesian networks has    it allows one to encode local structure in the form of deter-
    reduced the problem to that of factoring CNF en-  minism and context speciﬁc independence (CSI) [Boutilier et
    codings of these networks, providing an expressive al., 1996]. For example, using this approach, it became prac-
    framework for exploiting local structure. For net- tical to compile some Bayesian networks with binary vari-
    works that have local structure, large CPTs, yet no ables and excessive determinism (induced by relational mod-
    excessive determinism, the quality of the CNF en- els) and having treewidths in excess of 200 by compiling the
    codings and the amount of local structure they cap- AC in minutes and evaluating them in seconds [Chavira et al.,
    ture can have a signiﬁcant effect on both the ofﬂine 2004]. The second advantage of this approach is its ability to
    compile time and online inference time. We ex-    accommodate different representations of conditional prob-
    amine the encoding of such Bayesian networks in   ability tables (CPTs) (decision trees, rules, noisy–or, etc.),
    this paper and report on new ﬁndings that allow us without the need for algorithmic change. In this paper, we
    to signiﬁcantly scale this compilation approach. In consider only tabular representations.
    particular, we obtain order–of–magnitude improve-   The critical computational step in the above approach is
    ments in compile time, compile some networks      clearly that of factoring/compiling the CNF, which is done us-
    successfully for the ﬁrst time, and obtain orders– ing an exhaustive and reﬁned version of the DPLL algorithm
    of–magnitude improvements in online inference for [Davis et al., 1962; Darwiche, 2004]. The key observation
    some networks with local structure, as compared to underlying this paper is that the efﬁciency of the factoring
    baseline jointree inference, which does not exploit step—both factoring time and size of factorization—can be
    local structure.                                  signiﬁcantly improved through careful CNF encodings which
                                                      capture as much local structure as possible, and by passing
                                                      additional information about these CNFs to the factoring al-
1  Introduction                                       gorithm. This becomes especially true when handling net-
It was shown recently that compiling Bayesian networks cor- works that have local structure, large CPTs, yet no excessive
responds to factoring multi–linear functions (MLFs) [Dar- determinism. We do indeed propose a particular CNF en-
wiche, 2003]. In particular, each Bayesian network can be coding which appears quite effective on networks with such
characterized by an MLF of exponential size, whose eval- properties. We also identify a key semantic property of the
uation and differentiation solves the exact inference prob- resulting CNFs, which we exploit in the CNF factoring al-
lem.  Moreover, the MLF can be factored into an arith- gorithm. By incorporating these ﬁndings, we show dramatic
metic circuit (AC) whose size is not necessarily exponen- improvements in the both ofﬂine compile time and online in-
tial, allowing one to use ACs as a compiled representation ference. In the ofﬂine compilation phase, we get an order–of–
of Bayesian networks. Interestingly, [Park and Darwiche, magnitude improvement in some cases and an ability to com-
2003] has shown that building a jointree [Jensen et al., 1990; pile some networks for the ﬁrst time. In the online phase, we
Shenoy and Shafer, 1986] for a Bayesian network corre- observe orders–of–magnitude improvements on some well
sponds in a precise sense to a process of factoring the MLF known benchmarks, such as Pathﬁnder, Munin1, and Water,
into an AC, which is embedded by the jointree structure. over online inference by the baseline jointree algorithm.
  These ﬁndings created new possibilities for performing ex-
act inference, as they provided a new computational frame- 2 Factoring Multi–linear Functions
work based on factoring MLFs. In fact, a speciﬁc MLF factor- Our investigation is based on three technical observations
ing method was proposed in [Darwiche, 2002], which can ex- [Darwiche, 2003]: that every Bayesian network can be inter-
ploit the structure inherent in network parameters. According preted as an exponentially–sized MLF whose evaluation and
to this approach, one encodes the MLF using a propositional differentiation solves the exact inference problem; that such
theory in Conjunctive Normal Form (CNF), factors the CNF, an MLF can be factored into an AC whose size may not be ex-                  row  A    B   C   Pr(c | a, b)        One way to factor an MLF into an AC is by encoding the
                  1    a    b   c   θ      = 0
                        1   1    1   c1|a1b1          MLF into CNF and then factoring the CNF. To illustrate the
                  2    a    b   c   θ      = 0.5
   A         B          1   1    2   c2|a1b1          encoding scheme, consider again the MLF f in Figure 2 over
                  3    a1   b1  c3  θc3|a1b2 = 0.5
                  4    a    b   c   θ      = 0.2      real–valued variables a, b, c, d. The basic idea is to specify
                        1   2    1   c1|a1b2          this MLF using a propositional theory that has exactly four
                  5    a1   b2  c2  θ      = 0.3
        C                            c2|a1b1                                  f
                  6    a1   b2  c3  θc |a b = 0.5     models, one for each term in . Speciﬁcally, the propositional
                                      3 1 1           theory ∆  = V  ∧ (V  ⇒  V ) ∧ (V ⇒  V ) over Boolean
                  7    a2   b1  c1  θc1|a2b2 = 0              f     a    b     d     c     b

                  8    a2   b1  c2  θc2|a2b2 = 0      variables Va, Vb, Vc, Vd has exactly four models and encodes

                  9    a2   b1  c3  θc3|a2b1 = 1      f as follows:
                  10   a2   b2  c1  θc |a b = 0.2
                                      1 2 1                Model  Va    Vb    Vc    Vd    encoded term
                  11   a2   b2  c2  θc2|a2b2 = 0.3
                                                           σ1     true  false false false a
                  12   a2   b2  c3  θc3|a2b2 = 0.5
                                                           σ2     true  false false true  ad
                                                           σ3     true  true  false true  abd
Figure 1: A small Bayesian network with one of its CPTs,   σ4     true  true  true  true  abcd
showing local structure in the form of determinism and CSI.
                                                      That is, model σ encodes term t since σ(Vj) = true precisely
                                                      when term t contains the real–valued variable j.
                                 *                      By factoring/compiling the CNF ∆f as discussed in [Dar-
                                         +            wiche, 2004], one can immediately extract an AC represen-
  f    =    a + ad+      factor                       tation of the MLF f in time and space proportional to the
                                   *                  factored CNF [Darwiche, 2002]. We will discuss this process
            abd + abcd                                later, but we ﬁrst provide more detail on the encoding step.
                                           +          We start here with the baseline encoding [Darwiche, 2002],
                                                      which we refer to as PREV.
                               a   b   d   c   1        The CNF has one Boolean variable Vλ for each indicator
                                                      variable λ, and one Boolean variable Vθ for each parameter
                                                      variable θ. For brevity though, we will abuse notation and
        Figure 2: An MLF factored into an AC.         simply write λ and θ instead of Vλ and Vθ. CNF clauses
                                                      fall into three sets. First, for each network variable X with
ponential; and that the MLF factoring process can be reduced domain x1, x2, . . . , xn, we have:
to factoring a CNF encoding of the MLF.
                                                             Indicator clauses : λx1 ∨ λx2 ∨ . . . ∨ λxn
  The MLF for a network contains two types of variables.                      ¬λx  ∨ ¬λx , for i < j
For each value x of each network variable X, there is an in-                      i     j
dicator variable λx. For each network parameter Pr(x|u), For example, variable C in Figure 1 generates:
there is a parameter variable θ . The MLF contains a term
                         x|u                          λ  ∨ λ   ∨ λ , ¬λ   ∨ ¬λ  , ¬λ  ∨ ¬λ  , ¬λ   ∨ ¬λ
for each instantiation of the network variables, and the term is c1 c2 c3 c1  c2    c1     c3    c2    c3
the product of all indicators and parameters that are consistent These clauses ensure that exactly one indicator variable for
with the instantiation. For the network in Figure 1, variables C appears in each term of the MLF. The remaining sets of
A and B have two values, and variable C has three values. clauses correspond to network parameters. In particular, for
The MLF corresponding to this network is as follows:
                                                      each parameter θxn|x1,x2,...,xn−1 , we have:
  λ  λ  λ  θ  θ θ      +  λ  λ λ  θ  θ  θ      +
   a1 b1 c1 a1 b1 c1|a1,b1 a1 b1 c2 a1 b1 c2|a1,b1       IP clause : λ ∧ λ ∧ . . . ∧ λ ⇒ θ
  . . .                                                            x1    x2        xn    xn|x1,x2,...,xn−1
                                                         PI clauses : θxn|x1,x2,...,xn−1 ⇒ λxi , for each i
  λa2 λb2 λc2 θa2 θb2 θc2|a2,b2 + λa2 λb2 λc3 θa2 θb2 θc3|a2,b2

To compute the probability of evidence e, we evaluate the For example, parameter θc1|a1,b1 in Figure 1 generates:
MLF after setting indicators that contradict e to 0 and other
                                                        λa  ∧ λb ∧ λc ⇒  θc |a ,b
indicators to 1. For example, to compute Pr(a2, b1), we set 1   1    1     1 1 1                      (1)
                                                        θc1|a1,b1 ⇒ λa1 , θc1|a1,b1 ⇒ λb1 , θc1|a1,b1 ⇒ λc1
indicators λa1 and λb2 to 0, set other indicators to 1, and eval-
uate the reduced MLF: Pr(a2, b1) =                      The models of this CNF are in one–to–one correspondence
                                                      with the terms of the MLF. In particular, each model of the
 θa2 θb1|a2 θc1|a2,b1 + θa2 θb1|a2 θc2|a2,b1 + θa2 θb1|a2 θc3|a2,b1
                                                      CNF will correspond to a unique network variable instantia-
  As is obvious from the above example, the MLF has an ex- tion, and will set to true only those indicator and parameter
ponential size. Yet the MLF can be factored into an AC whose variables which are compatible with that instantiation.
size may not be exponential, leading one to formulate the ex-
act inference problem as a problem of factoring MLFs into
ACs. An AC is a DAG with internal nodes labeled with mul- 3 Encoding Techniques
tiplications/additions and leaves labeled with variables and The PREV encoding as discussed does not encode information
constants; see Figure 2.1                             about parameter values (local structure). However, it is quite
  1Representations such as ADDs and their variations are more re- stricted representations of MLFs (they can be unfolded into ACs).        Table 1: —: jointree ran out of memory.        Table 3: CNFs generated by PREV (determinism encoded).
                 Max   AC Inference Improvement           Network     Vars  Parm Vars Clauses  Literals
     Network   Cluster    Time (s)     Over JT            pathﬁnder  55229     54781  300576   821814
     bm-5-3       23        0.0068       4,028            water       6630      6514   49367   152686
     stud-3-2     25        0.0052       1,181            mildew     38540     37924  683552  1958952
     mm-4-8-3     26        0.0516       1,114            munin1      9551      8556   49363   129358
     mm-3-8-5     54        0.6835         —              munin4     48864     43216  247582   641839
     bm-22-3     104        4.7000         —              diabetes  113527    108845  814412  2196008
     stud-6-24   233       13.0000         —

                                                      3.1  How the CNF factoring/compilation process
easy to encode information about determinism within this en- works

coding. Consider Figure 1 and the parameter θc1|a1,b1 = 0, We provide in this section a sketch of the CNF factoring pro-
which generates the four clauses in (1). These clauses en- cess. We note, however, that this section may be skipped on

sure that the parameter θc1|a1,b1 appears in an MLF term iff a ﬁrst reading of the paper as it is not strictly needed for the

that term contains the indicators λa1 , λb1 and λc1 . How- following sections.
ever, given that this parameter is known to be 0, all terms Consider again the CNF ∆f = Va ∧ (Vb ⇒  Vd) ∧
that contain this parameter must vanish. Therefore, we can (Vc ⇒ Vb) from the previous section, and the MLF f =
suppress the generation of a Boolean variable for this param- a + ad + abd + abcd that it encodes. We now brieﬂy de-
eter, and then replace the above clauses by a single clause: scribe the CNF factoring process which allows us to produce
¬λa1 ∨¬λb1 ∨¬λc1 . This clause has the effect of eliminating the AC shown in Figure 2. First, the output of the factor-
all CNF models which correspond to vanishing terms, those ing process is shown on the left of Figure 3: It is a logi-

containing the parameter θc1|a1,b1 .                  cal form known as negation normal form (NNF) which sat-
  Armed with determinism, the PREV encoding can produce isﬁes decomposability (conjunctions do not share variables),
impressive results when applied to networks with only bi- determinism (disjuncts must be logically incompatible), and
nary variables, that contain small CPTs, and that contain large smoothness (disjuncts must mention the same sets of vari-
numbers of 0 parameters. For example, [Chavira et al., 2004] ables). Such a factorization is generated using an exhaustive
reports on networks with such properties, generated from re- version of the DPLL procedure [Davis et al., 1962]. In par-
lational models, and Table 1 reviews some of these results. As ticular, the algorithm will pick a variable x in the CNF, will
is clear from the table, one gets exponential improvements factor ∆|x and ∆|¬x separately, and then combine the re-
over the standard jointree method, which does not take ad- sults into x ∧ factor(∆|x) ∨ ¬x ∧ factor(∆|¬x). To im-
vantage of network determinism.2 Similar results have also prove performance, the algorithm keeps a cache that stores
been reported in [Darwiche, 2002] with respect to Bayesian CNFs that have been factored and their factorizations and
networks corresponding to digital circuits.           checks this cache before trying to factor a CNF Γ. Finally, be-
  For Bayesian networks with local structure, large CPTs, yet fore picking a variable x to split Γ on, the algorithm checks
no excessive determinism, the encoding of determinism alone the CNF to see if it can be broken into disconnected com-
may not be so effective. Table 2 lists a set of benchmark net- ponents, say α1 and α2. In that case, the algorithm factors
works, some having variables with large cardinalities, others the components separately and combines their results into
having very large CPTs, and where the amount of determin- factor(α1) ∧ factor(α2). The factoring algorithm we use
ism is not necessarily excessive. Table 3 provides statistics on [Darwiche, 2004], utilizes a decomposition tree (dtree) to
the CNFs generated for some of these networks, according to manage this decomposition process. In particular, a dtree for
the PREV encoding, while also encoding determinism as dis- a CNF is a binary tree whose leaves correspond to the CNF
cussed above. These CNFs are quite large, but the striking clauses. Moreover, each node in the dtree is associated with
property they have is the large percentage (up to 99% in some a set of variables whose instantiation is guaranteed to decom-
cases) of Boolean variables that represent parameters versus pose the CNF into two independent components.
those representing indicators. Some of these CNFs proved The above procedure generates NNFs that are decompos-
challenging to factor, some taking too long and others run- able and deterministic. Smoothness can be established easily
ning out of memory. There are two key observations, how- by a postprocessing step. Given an NNF that satisﬁes the
ever, that allowed us to handle these networks successfully, required properties, we can extract an AC by simply replac-
leading to signiﬁcant improvements in both ofﬂine compile ing conjunctions with multiplications, disjunctions with ad-
time and online inference time. We explain each of these in ditions, and negative literals with the constant 1. Positive lit-
some detail next, but after providing an overview of the CNF erals are replaced by the real–valued variables they encode.
factoring/compilation algorithm of [Darwiche, 2004].  This decoding process is shown in Figure 3; see [Darwiche,
                                                      2002] for more details.
                                                        This factoring algorithm is indeed a logical version of the
  2The technique of “zero compression” can be employed to ex- recursive conditioning (RC) algorithm [Darwiche, 2001b].3
ploit determinism in jointrees, but it requires inference on the full
jointree ﬁrst, which is prohibitive in this case.        3Similar algorithms have been used recently to solve probabilis-                              Table 2: The networks with which we experimented.
     Network   Max Clust  Vars  Card  Ave Card  Total Parms Max CPT Parms Ave CPT Parms  %Det   %DP
     alarm          7.2    37    2-4       2.8        752            108            20     0.9  24.6
     bm             20.0 1005    2-2       2.0       6972              8             7    99.6 100.0
     diabetes       17.2  413   3-21      11.3     461069           7056          1116    78.2  17.6
     hailﬁnder      11.7   56   2-11       4.0       3741           1188            67    15.7  26.9
     mildew         21.4   35   3-100     17.6     547158         280000         15633    93.2  25.1
     mm             23.0 1220    2-2       2.0       8326              8             7    98.7  75.0
     munin1         26.8  189   1-21       5.3      19466            600           103    66.5  61.2
     munin2         18.6 1003   2-21       5.4      83920            600            84    63.3  69.5
     munin3         17.8 1044   1-21       5.4      85855            600            82    63.1  71.3
     munin4         21.4 1041   1-21       5.4      98183            600            94    64.5  65.3
     pathﬁnder      15.0  109   2-63       4.1      97851           8064           898    56.1   5.1
     pigs           17.4  441    3-3       3.0       8427             27            19    56.2  23.9
     students       22.0  376    2-2       2.0       2616              8             7    90.7  79.3
     tcc4f          10.0  105    2-2       2.0       3236            512            31     0.4  35.6
     water          19.9   32    3-4       3.6      13484           3072           421    54.0  57.0

               ^                                *                               *
                       v                                +                                +

                    ^      ^                         *     *                     *
                                Decode                            Reduce

                ^    v    v   ^                  *    +   +    *                           +


        Va  Vb  Vd ¬Vd  Vc  ¬Vc  ¬Vb     a   b   d   1    c   1    1      a   b   d       c    1

               Figure 3: An NNF (left), its encoded AC (middle) and a simpliﬁcation of the AC (right).

One can in principle use RC to compile a Bayesian network duction to CNF allows one to more naturally accommodate
directly into an AC (bypassing the CNF encoding), but the other types of CPT structures (e.g., decision trees and rules)
approach we use allows us to capitalize on the state of the without the need for algorithmic change. The reduction to
art in logical reasoning for handling determinism and CSI, CNF factoring and the corresponding techniques lead to quite
even though it may incur more overhead in the factoring pro- a bit of overhead in some cases, but this is justiﬁed since
cess. In particular, our CNF factoring algorithm uses unit it will only be done once when compiling the network into
resolution to propagate logical constraints; conﬂict directed an AC. If the application of these techniques lead to smaller
backtracking to recover more efﬁciently when conditioning ACs, the compile time can then be amortized over all online
on variable settings that lead to contradictions; in addition to queries. We will illustrate this beneﬁt more concretely in the
clause learning as a means for avoiding such contradictions experimental results section.
early on in future conditioning. It also provides a more ﬂexi-
ble framework for exploiting CSI through the use of two tech- 3.2 Encoding parameter equality / CSI
niques. First, it can detect non–structural decompositions,
that is, subproblems that become independent due to condi- The CPT depicted in Figure 1 has 12 parameters, yet only 5
tioning on speciﬁc variable values—such independence can- of these are distinct. Some of the equalities among parame-
not be detected based only on structural considerations (net- ters imply context–speciﬁc independence; others do not. For
work topology). This is done by removing clauses that be- example, the equality between parameters in rows 4 − 6 with
come subsumed due to conditioning, therefore, disconnecting those in rows 10 − 12 imply that C is independent of A given
subsets of the CNF under speciﬁc variable values. Second, it B = b2. However, the equality between parameters in rows 2
uses a non–structural caching scheme which allows one to and 3 do not imply a CSI.
prove the equivalence of subproblems under speciﬁc variable From a purely encoding viewpoint, one would clearly want
values (therefore, avoiding multiple factorings of the same to exploit parameter equality, at least to reduce the number of
subproblem). Again, these equivalences cannot be proven if Boolean variables one must generate. Table 2 shows the ex-
we were to only use structural considerations. Finally, the re- tent to which parameter equality can help in this regard. In
                                                      particular, the table reports as %DP the percentage of distinct
tic inference using #SAT [Bacchus et al., 2003].      parameters among non–extreme parameters. That is, the per-centage of parameters that would remain if, for each CPT, 3.3 A more informed factoring algorithm
we collapsed equal, non–extreme parameters into a single pa- The CNF factoring algorithm employs two key techniques as
rameter. The dramatic example here is pathﬁnder: about half discussed earlier. The ﬁrst is variable splitting, which can be
of its parameters are extreme, and among the other half, only thought of as doing case analysis. The second is caching, so
about 5% are distinct within their CPTs. In addition to gener- that one can avoid factoring the same CNF subset multiple
ating smaller CNFs, encoding parameter equality allows the times. Which variables the algorithm ends up splitting on can
compiler to run with less overhead, and to generate smaller very much affect its running time, and the size of factoriza-
ACs since parameter equality provides more opportunities for tions it generates. Moreover, the complexity of the caching
factoring, which immediately translates to gains in online in- scheme is proportional to the number of variables appearing
ference.                                              in the cached CNF subset, as the state of such variables are
  A key observation here is that no two parameters in the used to generate keys that uniquely deﬁne CNFs. The fol-
same CPT can ever appear in the same MLF term, as they lowing observations state interesting properties of our CNF
correspond to incompatible network instantiations. This ob- encodings, which if passed to the factoring algorithm can sig-
servation suggests that we can use the same Boolean variable niﬁcantly improve both the splitting and caching processes.
to represent multiple parameters, assuming that such param- First, if two clauses share a parameter variable, then they
eters have equal values and appear in the same CPT. How- must also share indicators over the same network variable.
ever, the idea will not work when applied to PREV. Consider This property, and the presence of indicator clauses, allow the
again the CPT in Figure 1. If we use the same variable θ to CNF factoring algorithm to restrict its splitting to indicator
represent parameters θc2|a1,b1 and θc3|a1,b2 , which are both variables, which would be sufﬁcient to decompose the prob-
equal to .5, we would get the following two PI clauses in the lem into independent components (hence, no splitting/case
CNF: θ ⇒ λb1 and θ ⇒ λb2 , which is inconsistent with other analysis is needed on parameter variables). Second, given
clauses. More generally, PI clauses assert that a parameter the structure of indicator and IP clauses, the state of indicator
implies the corresponding family instantiation. Therefore, if variables are sufﬁcient to characterize the state of parameter
we simply use the same Boolean variable to represent equal variables. This property allows us to only involve indicator
parameters, we would be implying inconsistent family instan- variables when generating CNF keys during the caching pro-
      4
tiations.                                             cess. Both of the above optimizations can be exploited by
  The solution we adopt is to drop PI clauses from the encod- simply identifying parameter variables to the factoring algo-
ing! Note that dropping PI clauses introduces additional (un- rithm.
intended) models into the CNF, allowing MLF terms which Another technique that we have used in some of the ex-
contain multiple parameters from the same CPT. These unin- periments involves the construction of a decomposition tree
tended models/terms, however, can be easily ﬁltered during (dtree) for the given Bayesian network, and then converting it
the decoding process given the following.             into a dtree for its CNF encoding. A dtree for a Bayesian net-
Theorem 1 Consider a Bayesian network with n variables. work is simply a binary tree whose leaves correspond to the
Let ∆ be a CNF encoding which includes the indicator, IP network CPTs [Darwiche, 2001b]. A dtree for a CNF is also
and PI clause sets, and let Γ be the CNF encoding which in- a binary tree, but its leaves correspond to the CNF clauses.
cludes the indicator and IP clause sets only. Then ∆’s models Since each clause in the CNF encoding is generated by a CPT,
have cardinality 2n and are a subset of Γ’s models. Moreover, we can convert a network dtree into a CNF dtree by simply
if σ is a model of Γ but not ∆, then σ’s cardinality is > 2n. unfolding the dtree node corresponding to a CPT into a sub-
Therefore, unintended models have a higher cardinality than tree whose leaves correspond to the clauses generated by that
original models (which all have the same cardinality). As it CPT. The main point of this technique is to more efﬁciently
turns out, if Γ is an NNF which satisﬁes decomposability, de- generate dtrees for very large CNF encodings that are gener-
terminism and smoothness, one can in linear time obtain an- ated by Bayesian networks with a small number of CPTs (this
other NNF Γ0 whose models are exactly the minimum cardi- happens when the network contains very large CPTs).
nality models of Γ and which satisﬁes the required properties 3.4 Other Optimizations
[Darwiche, 2001a]. Therefore, we can safely drop PI clauses
as long we minimize the resulting NNF before we decode the Our CNF encodings utilize some additional enhancements,
AC.                                                   two of which are described next. First, we deﬁne a new type
  By including only indicator and IP clauses, we can now of clause, called an eclause, which has the same syntax as a
safely represent all equal parameters within the same CPT regular clause but stronger semantics: it asserts that exactly
by a single Boolean variable in the CNF encoding. For the one of it literals is true. We use eclauses for representing in-
Pathﬁnder network for example, this drops the number of dicator clauses, therefore reducing the size of CNFs consid-
Boolean variables needed to represent non–extreme param- erably in networks having multi–valued variables. Moreover,
eters from 42, 946 to 2, 186, a 95% reduction! Similar re- we outﬁt the DPLL procedure used in factoring the CNF to
ductions are obtained for many other networks; see Table 2. work directly with eclauses, without having to unfold them
As we show later, not only does this technique improve the into regular clauses. For another optimization example, the
compilation time, but can lead to signiﬁcantly smaller ACs. indicators and parameters corresponding to the same state of
                                                      a root variable are logically equivalent, making it possible to
  4A restricted case of encoding parameter equality was discussed delete the parameter variables and the corresponding IP and
in [Darwiche, 2002].                                  PI clauses, which establish the equivalence.