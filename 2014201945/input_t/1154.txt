            Optimal Nonmyopic Value of Information in Graphical Models –
                         Efﬁcient Algorithms and Theoretical Limits

                      Andreas Krause                               Carlos Guestrin
                Carnegie Mellon University                    Carnegie Mellon University


                     Abstract                          guarantees. In this paper, we present efﬁcient algorithms,
                                                       which guarantee optimal nonmyopic value of information
    Many real-world decision making tasks require us to
                                                       in chain graphical models such as Hidden Markov Models
    choose among several expensive observations. In a sen-
                                                       (HMMs). We address two settings: subset selection, where
    sor network, for example, it is important to select the sub-
                                                       the optimal subset of observations is obtained in an open-loop
    set of sensors that is expected to provide the strongest re-
                                                       fashion, and conditional plans, a closed-loop plan where the
    duction in uncertainty. It has been general practice to use
                                                       observation strategy depends on the actual value of the ob-
    heuristic-guided procedures for selecting observations. In
                                                       served variables (c.f. Fig. 1(a)). To our knowledge, these are
    this paper, we present the ﬁrst efﬁcient optimal algorithms
                                                       the ﬁrst optimal and efﬁcient algorithms for these tasks for this
    for selecting observations for a class of graphical models
                                                       class of graphical models. For both settings, we address the
    containing Hidden Markov Models (HMMs). We provide
                                                       ﬁltering and the smoothing versions: Filtering is important in
    results for both selecting the optimal subset of observa-
                                                       online decision making, where our decisions can only utilize
    tions, and for obtaining an optimal conditional observation
                                                       observations made in the past. Smoothing arises for example
    plan. For both problems, we present algorithms for the ﬁl-
                                                       in structured classiﬁcation tasks, where there is no temporal
    tering case, where only observations made in the past are
                                                       dimension in the data, and hence all observations can be taken
    taken into account, and the smoothing case, where all ob-
                                                       into account. We evaluate our algorithms empirically on three
    servations are utilized. Furthermore we prove a surpris-
                                                       real-world datasets, and also show that they are well-suited for
    ing result: In most graphical models tasks, if one designs
                                                       interactive classiﬁcation of sequential data.
    an efﬁcient algorithm for chain graphs, such as HMMs,
                                                         Most problems in graphical models, such as probabilistic in-
    this procedure can be generalized to polytrees. We prove
                                                       ference and the most probable explanation, that can be solved
    that the value of information problem is NPPP-hard even
                                                       efﬁciently for chain-structured graphs, can also be solved efﬁ-
    for discrete polytrees. It also follows from our results that
                                                       ciently for polytrees. We prove that the problem of maximizing
    even computing conditional entropies, which are widely                    PP
    used to measure value of information, is a #P-complete value of information is NP -hard even for discrete polytree
    problem on polytrees. Finally, we demonstrate the effec- graphical models, giving a complexity theoretic classiﬁcation
                                                                                              PP
    tiveness of our approach on several real-world datasets. of a core artiﬁcial intelligence problem. NP -hard prob-
                                                       lems are believed to be signiﬁcantly harder than NP-complete
                                                       or even #P-complete problems commonly arising in the con-
1  Introduction                                        text of graphical models. As a special case, we also prove that
In probabilistic reasoning, where one can choose among sev- computing conditional entropies is #P-complete even in the
eral possible but expensive observations, it is often a central case of discrete polytrees. This is a surprising result about a
issue to decide which variables to observe in order to most ef- measure of uncertainty that is frequently used in practice.
fectively increase the expected utility. In a medical expert sys-
tem [14], for example, multiple tests are available, and each
test has a different cost. In such systems, it is thus important to 2 Optimization criteria
decide which tests to perform in order to become most certain In order to maximize value of information, our objective func-
about the patient’s condition, at a minimum cost. Occasionally, tions should depend on probability distributions over variables.
the cost of testing can even exceed the value of information for Let S = {X1, . . . , Xn} be a set of discrete random variables.
any possible outcome.                                  We consider a class of local reward functions R, which are de-
  The following running example motivates our research and ﬁned on the marginal probability distributions of the variables.
is empirically evaluated in Section 7. Consider a temperature This class has the computational advantage that local rewards
monitoring task, where wireless temperature sensors are dis- can be evaluated using probabilistic inference techniques. The
tributed across a building. The task is to become most certain total reward will then be the sum of all local rewards.
about the temperature distribution, whilst minimizing energy Let O be a subset of S. Then P (Xj | O = o) denotes the
expenditure, a critically constrained resource [4].    marginal distribution of variable Xj conditioned on observa-
  Many researchers have suggested the use of myopic    tions o. For classiﬁcation purposes, it can be more appropriate
                                                                                   max
(greedy) approaches to select observations [13; 15; 5; 1]. Un- to consider the max-marginals P (Xj = xj | O = o) =
fortunately, this heuristic does not provide any performance maxx P (X = x, Xj = xj | O = o), that is, for Xj set to value                                                               Tmorn =high?
                                                             no      yes
x , the probability of the most probable assignment to all other
 j                                                        T  =high? T =high?
random variables conditioned on the observations o. The local noon  eve
reward Rj is a functional on the probability distribution P or (a) Example conditional plan. (b) Decomposition of the reward.
 max
P    over Xj. We write                                       Figure 1: Example, and decomposing reward idea
                 X                                       The set of random variables S = {X1, . . . , Xn} forms a
     R (X  | O) ,    P (O = o)R (P (X | O = o))
      j   j                    j     j                 chain graphical model (a chain), if Xi is conditionally inde-
                   o
                                                       pendent of Xk given Xj whenever i < j < k. We can assume
as an abbreviation to indicate expected local rewards, where that the joint distribution is speciﬁed by the prior P (X1) and
the expectation is taken over all assignments o to the observa- conditional probability distributions P (Xi+1 | Xi). The time
tions O. Important measures for value of information include: series model for the temperature measured by one sensor in
                                                       our example can be formulated as a chain graphical model.
  • Entropy. If we set Rj(P (Xj | O)) = −H(Xj | O) =
    P                                                    Chain graphical models originating from time series have
          P (xj, o) log P (xj | o), the objective in the opti-
      xj ,o                                            additional, speciﬁc properties: In a system for online decision
    mization problem becomes to minimize the sum of resid-
                                                       making, only observations from the past and present time steps
    ual entropies. We choose this reward function in our run-
                                                       can be taken into account, not observations which will be made
    ning example to measure the uncertainty about the tem-
                                                       in the future. This is in general referred to as the ﬁltering prob-
    perature distribution.
                                                       lem. In this setting, the notation P (X | O) will refer to the
  • Maximum expected utility. The concept of local reward                              i
                                                       distribution of X conditional on observations in O prior to and
    functions also includes the concept of utility nodes in in-     i
                                                       including time i. For structured classiﬁcation problems as dis-
    ﬂuence diagrams. If U : A × dom X →  R is a utility
                       j   j         j                 cussed in Section 7, in general observations made anywhere in
    function mapping an action a ∈ A and an outcome x ∈
                                 j                     the chain must be taken into account. This situation is usually
    dom X  to a reward, then the maximum expected utility
          j                                            referred to as the smoothing problem. We will provide algo-
    principle states that actions should be selected as to max-
                       P                               rithms both for ﬁltering and smoothing.
    imize EUj(a | o) =   x P (x | o)Uj(a, x). The more
    certain we are about Xj, the more economically we can We will now describe the key insight, which allows for ef-
    choose our action. Hence we can deﬁne our local reward ﬁcient optimization in chains. Consider a set of observations
                           P                           O ⊆ S. If the j variable is observed, i.e., Xj ∈ O, then the
    function R(P (Xj | O)) = o P (o) maxa EUj(a | o).
  • Margin.  We can also consider the margin of conﬁ-  local reward is simply R(Xj | O) = R(Xj | Xj). Now
                max              P         max   ∗     consider Xj ∈/ O, and let Oj be the subset of O contain-
    dence: Rj(P    (Xj  | O)) =    o P (o)[P   (xj |
         max               ∗              max          ing the closest ancestor (and for the smoothing problem also
    o)−P     (¯xj | o)], where x = argmaxxj P (xj | o)
                          max                          the closest descendant) of Xj in O. The conditional inde-
    and x¯ = argmax    ∗ P   (xj | o), which describes
                   xj 6=x                              pendence property of the graphical model implies that, given
    the margin between the most likely outcome and the clos-
                                                       Oj, Xj is independent of the rest of the observed variables,
    est runner up. This reward function is very useful for
                                                       i.e., P (Xj | O) = P (Xj | Oj).  Thus, it follows that
    structured classiﬁcation purposes, as shown in Sec. 7.
                                                       R(Xj | O) = R(Xj | Oj).
These examples demonstrate the generality of our notion of lo- These observations imply that the expected reward of some
cal reward. One can generalize the algorithms even more, e.g., set of observations decomposes along the chain. For simplicity
to measure the total entropy or the margin between the most of notation, we add two independent dummy variables X0 and
probable explanation and its runner up. Details are omitted Xn+1, where R0 = C0 = β0 = Rn+1 = Cn+1 = βn+1 = 0.
here due to space limitations.
                                                       Let O =  {Xi0 , . . . , Xim+1 } where il < il+1, i0 = 0 and
  We also want to capture the constraint that observations are im+1 = n + 1. Using this notation, the total reward R(O) =
expensive. This can mean that each observation Xj has an P
                                                         j Rj(Xj | O) for the smoothing case is given by:
associated positive penalty Cj that effectively decreases the
reward. In our example, we might be interested in trading                                               
                                                       m                         iv+1−1
off accuracy with sensing energy expenditure. Alternatively, X                    X
it is also possible to deﬁne a budget B for selecting observa- Riv (Xiv | Xiv ) − Civ + Rj(Xj | Xiv , Xiv+1 ) .
tions, where each one is associated with an integer cost βj. v=0                 j=iv +1
Here, we want to select observations whose sum cost is within
the budget, but these costs do not decrease the reward. In
                                                       In ﬁltering settings, we simply replace Rj(Xj | Xiv , Xiv+1 )
our running example, the sensors could be powered by solar by R (X | X ). Figure 1(b) illustrates this decomposition.
power, and regain a certain amount of energy per day, which j j   iv
allows a certain amount of sensing. Our formulation of the Consider now a Hidden Markov Model unrolled for n time
optimization problems allows both for penalties and budgets. steps, i.e., S can be partitioned into the hidden variables
To simplify notation we also write C(O) = P  C  and    {X1, . . . , Xn} and the emission variables {Y1, . . . , Yn}. In
                                        Xj ∈O  j
       P                                               HMMs, the Yi are observed and the variables Xi form a chain.
β(O) =        βj to extend C and β to sets.
         Xj ∈O                                         In many applications, some of which are discussed in Sec-
                                                       tion 7, we can observe some of the hidden variables, e.g., by
3  Decomposing Rewards                                 asking an expert, in addition to observing the emission vari-
In the following Sections 4 and 5, we present efﬁcient algo- ables. In these cases, the problem of selecting expert labels
rithms for two problems of optimizing value of information in also belongs to the class of chain graphical models addressed
the class of chain graphical models.                   by this paper.4  Subset Selection
                                                        Input: Budget B, rewards Rj, costs βj and penalties Cj
In the subset selection problem, we want to ﬁnd a most in- Output: Optimal selection O of observation times
formative subset of the variables to observe in advance, i.e., begin
before any observations are made. In our running example, we for 0 ≤ a < b ≤ n + 1 do compute La:b(0);
would, before deploying the sensors, identify k time points that for k = 1 to B do
are expected to provide the most informative sensor readings  for 0 ≤ a < b ≤ n + 1 do
according to our model.                                          sel(−1) := La:b(0);
  First, deﬁne the objective function L on subsets of S by       for j = a + 1 to b − 1 do sel(j) :=
                    n                                            −Cj  + Rj(Xj | Xj) + La:j(0) + Lj:b(k − βj);
                   X
           L(O) =     R (X  | O) − C(O).       (4.1)             La:b(k) = maxa<j<b sel(j);
                        j  j                                     Λ   (k) = argmax      sel(j);
                   j=1                                             a:b           a<j<b
                                                              end
The subset selection problem is to ﬁnd the optimal subset  end
                                                           a := 0; b := n + 1; k := B; O := ∅;
               O∗ =   argmax   L(O)
                     O⊆S,β(O)≤B                            repeat
                                                              j := Λa:b(k);
maximizing the sum of expected local rewards minus the        if j ≥ 0 then O := O ∪ {Xj}; k := k − βj;
penalties, subject to the constraint that the total cost must not until j = −1;
exceed the budget B.                                    end
  We solve this optimization problem using a dynamic pro-
gramming algorithm, where the chain is broken into sub-chains   Algorithm 1: Optimal subset selection.
using the insight from Sec. 3. Consider a sub-chain from vari-
able Xa to Xb. We deﬁne La:b(k) to represent the expected since the subset problem is open-loop and the order of the ob-
total reward for the sub-chain Xa, . . . , Xb, where Xa (and Xb servations is irrelevant, we only need to consider split points
in the smoothing case) is observed, and with a budget level of where the ﬁrst sub-chain receives zero budget.
k. More formally:                                        A pseudo code implementation is given in Alg. 1. If we do
                       Xb−1                            not consider different costs β, we would simply choose βj = 1
La:b(k) =     max          Rj (Xj | O ∪ {Xa}) − C(O),  for all variables and compute La:b(N). Alg. 1 uses the quanti-
         O⊂{X   ...X }
             a+1   b−1 j=a+1                              Λ
             β(O)≤k                                    ties a:b to recover the optimal subset by tracing the maximal
                                                       values occurring in the dynamic programming equations. Us-
for the ﬁltering version, and                          ing an induction proof, we obtain:
                       Xb−1                            Theorem 1. The dynamic programming algorithm described
                                                                                                     1 3
La:b(k) =    max           Rj (Xj | O∪{Xa, Xb})−C(O),  above computes the optimal subset with budget B in ( n +
        O⊂{X   ...X  }                                                                               6
             a+1  b−1 j=a+1                               2
            β(O)≤k                                     O(n ))B evaluations of expected local rewards.
                                                         If the variables X are continuous, our algorithm is still ap-
for the smoothing version.  Note that L     (B)   =                    i
                                        0:n+1          plicable when the integrations and inferences necessary for
max         L(O), as in Eq. (4.1), i.e., by computing the
    O:β(O)≤B                                           computing the expected rewards can be performed efﬁciently.
values for La:b(k), we compute the maximum expected total
reward for the entire chain.                           5  Conditional Plan
  We can compute La:b(k) using dynamic programming. The
base case is simply:                                   In the conditional plan problem, we want to compute an opti-
                                                       mal querying policy: We sequentially observe a variable, pay
                       b−1
                      X                                the penalty, and depending on the observed values, select the
             La:b(0) =     Rj(Xj | Xa),                next query as long as our budget sufﬁces. The objective is
                     j=a+1                             to ﬁnd the plan with the highest expected reward, where, for
for ﬁltering, and                                      each possible sequence of observations, the budget B is not
                                                       exceeded. For ﬁltering, we can only select observations in the
                     b−1
                     X                                 future, whereas in the smoothing case, the next observation can
           La:b(0) =     Rj(Xj  | Xa, Xb),             be anywhere in the chain. In our running example, the ﬁlter-
                    j=a+1                              ing algorithm would be most appropriate: The sensors would
                                                       sequentially follow the conditional plan, deciding on the most
for smoothing. The recursion for La:b(k) has two cases: we
can choose not to spend any more of the budget, reaching the informative times to sense based on the previous observations.
base case, or we can break the chain into two sub-chains, se- Fig. 1(a) shows an example of such a conditional plan.
                                                                                                 J
lecting the optimal observation X , where a < j < b:     The formal deﬁnition of the objective function is given
                            j                          recursively. The base case considers the exhausted budget:
  La:b(k) = max{La:b(0),   max    {−Cj+
                       j:a<j<b,β ≤k                                       X
                               j                             J(O = o; 0) =    Rj(Xj  | O = o) − C(O).
           + Rj(Xj  | Xj) + La:j(0) + Lj:b(k − βj)}}.                     Xj ∈S
At ﬁrst, it may seem that this recursion should consider the op- The recursion, J(O = o; k), represents the maximum expected
timal split of the budget between the two sub-chains. However, reward of the conditional plan for the chain where O = o hasbeen observed and the budget is limited to k:          Theorem  2. The algorithm for smoothing presented above
                                                       computes an optimal conditional plan in d3·B2·( 1 n3+O(n2))
 J(O  = o; k) = max{J(O = o; 0), max {                                                         6
                               Xj ∈/O                  evaluations of local rewards, where d is the maximum domain
   X                                                   size of the random variables X , . . . , X . In the ﬁltering case,
      P (X  = y | O = o) · J(O = o, X = y; k − β )}}.                           1      n
           j                       j         j         or if no budget is used, the optimal plan can be computed us-
    y                                                      3     1 3      2      3  1  4      3
                                                       ing d · B · ( 6 n + O(n )) or d · ( 6 n + O(n )) evaluations
The optimal plan has reward J(∅; B).                   respectively.
  We propose a dynamic programming algorithm for obtain- The faster computation for the no budget case is obtained by
ing the optimal conditional plan that is similar to the subset observing that we do not require the third maximum computa-
algorithm presented in Sec. 4. Again, we utilize the decompo- tion, which distributes the budget into the sub-chains.
sition of rewards described in Section 3. The difference here
is that the observation selection and budget allocation now de- Input: Budget B, rewards Rj, costs βj and penalties Cj
                                                                                     (π   σ
pend on the actual values of the observations.          Output: Optimal conditional plan a:b, a:b)
                                                        begin
  We again consider sub-chains Xa, . . . , Xb. The base case
deals with the zero budget setting:                        for 0 ≤a < b≤ n + 1, xa ∈ dom Xa, xb ∈ dom Xb do
                                                           compute Ja:b(xa, xb; 0);
                      Xb−1                                 for k = 1 to B do
          Ja:b(xa; 0) =   Rj (Xj | Xa = xa),
                                                              for 0≤a<b≤n+1, xa   ∈dom  Xa, xb ∈dom Xb do
                     j=a+1
                                                                 sel(−1) := Ja:b(0);
 for ﬁltering, and                                               for a < j < b do
                                                                 sel(j) := −Cj + Rj(Xj | Xj);
                   Xb−1
                                                                 for a < j < b, xj ∈ dom Xj do
     Ja:b(xa, xb; 0) = Rj (Xj | Xa = xa, Xb = xb),
                                                                    for 0 ≤ l ≤ k − βj do ;
                  j=a+1                                             bd(l) :=
for smoothing.     The  recursion deﬁnes  Ja:b(xa; k)                 Ja:j(xa, xj; l) + Jj:b(xj, xb; k − l − βj);
(Ja:b(xa, xb; k) for smoothing), the expected reward for            sel(j) := sel(j) + P (xj | xa, xb) · maxl bd(j);
the problem restricted to the sub-chain Xa, . . . , Xb condi-       σ(j, xj) = argmaxl bd(j);
tioned on the values of Xa (and Xb for smoothing), and with      end
budget limited by k. To compute this quantity, we again iterate  Ja:b(k) = maxa<j<b sel(j);
through possible split points j, such that a < j < b. Here
                                                                 πa:b(xa, xb; k) = argmaxa<j<b sel(j);
we observe a notable difference between the ﬁltering and         for x ∈ dom X       do
the smoothing case. For smoothing, we now must consider               j        πa:b(k)
all possible splits of the budget between the two resulting      σa:b(xa, xb, xj; k) = σ(πa:b(k), xj);
sub-chains, since an observation at time j might require us to end
                                                           end
make an additional, earlier observation:                end
 Ja:b(xa, xb; k) = max{Ja:b(xa, xb; 0), max {−Cj +        Algorithm 2: Computation of optimal conditional plan.
       X                       a<j<b
                                                        Input: Budget k, observations Xa = xa, Xb = xb, σ, π
          P (Xj = xj | Xa = xa, Xb = xb){Rj (Xj | Xj )+ begin
        xj
                                                           j := πa:b(xa, xb; k);
         max  [Ja:j (xa, xj ; l) + Jj:b(xj , xb; k − l − βj )]}}}. if j ≥ 0 then
       0≤l≤k−βj
                                                              Observe Xj = xj;
Looking back in time is not possible in the ﬁltering case, hence l := σa:b(xa, xb, xj; k);
the recursion simpliﬁes to                                    Recurse with k := l, a := a, b := j;
                                                              Recurse with k := k − l − βj, a := j, b := b;
     Ja:b(xa; k) = max{Ja:b(xa; 0), max {−Cj +
                              a<j<b:βj ≤k                  end
            X                                           end
               P (Xj = xj | Xa = xa){Rj (Xj | Xj )+
                                                         Algorithm 3: Observation selection using conditional plan.
            xj
           Ja:j (xa; 0) + Jj:b(xj ; k − βj )}}}.       6  Theoretical Limits
The optimal reward is obtained by J0:n+1(∅; B) = J(∅; B). Many problems that can be solved efﬁciently for discrete chain
Alg. 5 presents a pseudo code implementation for the smooth- graphical models can also be efﬁciently solved for discrete
ing version – the ﬁltering case is a straight-forward modiﬁca- polytrees. Examples include probabilistic inference and the
tion. The plan itself is compactly encoded in the quantities πa:b most probable explanation (MPE). Surprisingly, we prove that
which determines the next variable to query and σa:b, which for the optimization problems discussed in this paper, this gen-
determines the allocation of the budget. Considering the ex- eralization is not possible, unless P = NP. All proofs in this
ponential number of possible sequences of observations, it is section are stated in the Appendix.
remarkable that the optimal plan can even be represented us- In order to solve the optimization problems, we will most
ing only polynomial space. Alg. 5 indicates how the computed likely have to evaluate the objective function, i.e., the expected
plan can be executed. The procedure is recursive, requiring the local rewards. Our ﬁrst result states that this problem is in-
parameters a := 0, xa := 1, b := n + 1, xb := 1 and k := B tractable even for discrete polytrees.
for the initial call. Again, by induction, we obtain:         10
                  Optimal conditional plan 1 Mean margin of optimal subset 1
                                          Mean margin of greedy heuristic
         8                                                            0.98
                                        0.9                           0.96            Mean F1 score
         6
                                                                      0.94
                                        0.8                           0.92      Mean margin
         4                                     Mean accuracy of
                                                optimal subset         0.9
         2                              0.7
                                                      Mean accuracy of 0.88
        Percent  improvement Optimal subset            greedy heuristic
         0            Greedy heuristic                                0.86
                                        0.6
          1  4   8   12  16  20 24        1   2    3   4    5   6        0   10   20  30   40  50
               Number of observations          Number of observations         Number of observations
      (a) Temperature data: Improvement over (b) CpG island data set: Effect of increas- (c) Part-of-Speech tagging data set: Ef-
      the uniform spacing heuristic. ing the number of observations on margin fect of increasing the number of observa-
                                     and classiﬁcation accuracy.     tions on margin and F1 score.
                                        Figure 2: Experimental results.

Theorem  3. The computation of expected local rewards for 60 minutes, and it was discretized into 10 bins of 2 degrees
discrete polytrees is #P-complete.1                    Kelvin. To avoid overﬁtting, we used pseudo counts α = 0.5
  This negative result can be specialized to the conditional en- when learning the model. Using parameter sharing, we learned
tropy, one of the most frequently used reward function to char- four sets of transition probabilities: from 12 am - 7am, 7 am -
acterize the residual uncertainty in value of information prob- 12 pm, 12 pm - 7 pm and 7 pm - 12 am. Combining the data
lems.                                                  from three adjacent sensors, we got 53 sample time series.
                                                         The goal of this task was to select k out of 24 time points
Corollary 4. The computation of conditional entropy for dis- during the day, during which sensor readings are most infor-
crete polytrees is #P-complete.                        mative. The experiment was designed to compare the perfor-
  Since evaluating local rewards is #P-complete, it can be mance of the optimal algorithms, the greedy heuristic, and a
suspected that the subset selection problem is at least #P- uniform spacing heuristic, which distributed the k observations
hard. We show that it is even NPPP-complete2, a complexity uniformly over the day. Fig. 2(a) shows the relative improve-
class containing problems that are believed to be signiﬁcantly ment of the optimal algorithms and the greedy heuristic over
harder than NP or #P complete problems. This result pro- the uniform spacing heuristic. The performance is measured
vides a complexity theoretic classiﬁcation of value of informa- in decrease of expected entropy, with zero observations as the
tion, a core AI problem.                               baseline. It can be seen that if k is less than about the half of
                                                       all possible observations, the optimal algorithms decreased the
Theorem 5. Subset Selection is NPPP-complete even for dis-
                                                       expected uncertainty by several percent over both heuristics.
crete polytrees.
                                                       The improvement gained by the optimal plan over the subset
  For our running example, this implies that the generalized selection algorithms appears to become more drastic if a large
problem of optimally selecting k sensors from a network of number of observations (over half of all possible observations)
correlated sensors is most likely computationally intractable is allowed. Furthermore, for a large number of observations,
without resorting to heuristics. A corollary extends the hard- the optimal subset and the subset selected by the greedy heuris-
ness of subset selection to the hardness of conditional plans. tic were almost identical.
Corollary 6. Computing conditional plans is NPPP-hard
even for discrete polytrees.                           7.2  CpG-Island detection
                                                       We then studied the bioinformatics problem of ﬁnding CpG
7  Experiments                                         islands in DNA sequences. CpG islands are regions in the
                                                       genome with a high concentration of the cytosine-guanine se-
In this section, we evaluate the proposed methods for several quence. These areas are believed to be mainly located around
real world data sets. A special focus is on the comparison of the promoters of genes, which are frequently expressed in
the optimal methods with the greedy heuristic and other heuris- the cell. In our experiment, we considered the gene loci
tic methods for selecting observations, and on how the algo- HS381K22, AF047825 and AL133174, for which the Gen-
rithms can be used for interactive structured classiﬁcation. Bank annotation listed three, two and one CpG islands each.
7.1  Temperature time series                           We ran our algorithm on a 50 base window at the beginning
                                                       and end of each island, using the transition and emission prob-
The ﬁrst data set consists of temperature time series collected abilities from [6] for our Hidden Markov Model, and we used
                                                 [ ]
from a sensor network deployed at Intel Research Berkeley 4 the sum of margins as reward function.
as described in our running example. Data was continuously The goal of this experiment was to locate the beginning and
collected for 19 days, linear interpolation was used in case of ending of the CpG islands more precisely by asking experts,
missing samples. The temperature was measured once every whether or not certain bases belong to the CpG region or not.
  1#P  contains problems such as counting the number of satisfying Fig. 2(b) shows the mean classiﬁcation accuracy and mean
assignments to a Boolean formula.                      margin scores for an increasing number of observations. The
  2NPPP   is natural for AI planning problems [9]. A complete results indicate that, although the expected margin scores are
problem is EMAJSAT , where one has to ﬁnd an assignment to the similar for the optimal algorithm and the greedy heuristic, the
ﬁrst k variables of a 3CNF formula, such that the formula is satisﬁed mean classiﬁcation performance of the optimal algorithm was
under the majority of assignments to the remaining variables. still better than the performance of the greedy heuristic.