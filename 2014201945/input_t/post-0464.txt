                        Language Learning in Multi-Agent Systems

            Martin Allen                 Claudia V. Goldman                 Shlomo Zilberstein
   Computer Science Department        Caesarea Rothschild Institute    Computer Science Department
    University of Massachusetts            University of Haifa          University of Massachusetts
     Amherst, MA 01003, USA        Mount Carmel, Haifa 31905, Israel     Amherst, MA 01003, USA
       mwallen@cs.umass.edu                clag@cri.haifa.ac.il            shlomo@cs.umass.edu

                    Abstract                          sharing information. However, where agents utilize different
                                                      sets of messages, and do not fully understand one another,
    We present the problem of learning to communi-    message-passing alone is not enough. Rather, agents need to
    cate in decentralized and stochastic environments, learn how to respond to the messages that are passed between
    analyzing it formally in a decision-theoretic context them—in a sense, learning what those messages mean.
    and illustrating the concept experimentally. Our ap-
                                                      Deﬁnition 1 (Translation). Let Σ and Σ0 be sets of mes-
    proach allows agents to converge upon coordinated                                       0
    communication and action over time.               sages. A translation, τ, between Σ and Σ is a probabil-
                                                      ity function over message-pairs: for any messages σ, σ0,
                                                      τ(σ, σ0) is the probability that σ and σ0 have the same mean-
                                                           +                                        0
1  Introduction                                       ing. τΣ,Σ0 is the set of all translations between Σ and Σ .
Learning to communicate in multi-agent systems is an emerg- Agents may need to consider multiple possible translations
ing challenge AI research. Autonomous systems, developed between messages; that is, agents possess beliefs as to which
separately, interact more and more often in contexts like dis- translation is correct given their present situation.
tributed computing, information gathering over the internet,
                                                      Deﬁnition 2 (Belief-state). Let agents α , α use sets of
and wide-spread networks of machines using distinct proto-                                1   2
                                                      messages Σ1, Σ2. A  belief-state for αi is a probability-
cols. As a result, we foresee the need for autonomous systems                    +
                                                      function βi over translation-set τ (i =6 j). For translation
that can learn to communicate with one another in order to                       Σi,Σj
                                                      τ ∈ τ +  , β τ is the probability that τ is correct.
achieve cooperative goals. We make some ﬁrst steps towards Σi,Σj i( )
solving the attendant problems.                         Updating beliefs about translations is thus an important
  Coordination among agents acting in the same environ- part of the overall process of learning to communicate.
ment while sharing resources has been studied extensively, Agents act based upon local observations, messages received,
particularly by the multi-agent systems community. While and current beliefs about how to translate those messages.
such coordination may involve communication, typically Their actions lead to new observations, causing them to up-
there is no deliberation about the value of communication, date beliefs and translations. The procedure governing these
resulting in systems with no communication or ones allow- updates comprises the agent’s language-model, a function
ing free communication of well-understood messages. In from actions, messages, and observations to distributions over
contrast, we study decentralized systems that require agents translations. Such models may be highly complex, or difﬁcult
to adapt their communication language when new situations to compute, especially where languages are complicated, or
arise or when mis-coordination occurs.                the environment is only partially observable. Here we con-
                                                      centrate upon special—but interesting—cases for which gen-
2  The Decentralized Learning Framework               erating these probabilities is much more straightforward.
We study the problem in the context of decentralized Markov
Decision Processes [Bernstein et al., 2002] with communi- 3 Formal Properties of the Problem
cation (Dec-MDP-Com). Such a process is a multi-agent ex- Our main formal results isolate conditions under which Dec-
tension of a common MDP in which each agent αi observes MDP-Coms reduce to simpler problems, and present a proto-
only its own local portion of the state-space, and can attempt col for learning to communicate in such reduced problems.
to communicate with others using the set of messages Σi.
Decentralization makes Dec-MDPs, with communication or Reduction to MMDPs
not, signiﬁcantly harder to solve than regular MDPs; for their Boutilier [1999] deﬁnes multiagent MDPs (MMDPs), con-
complexity properties, see [Goldman and Zilberstein, 2004]. sisting of a set of agents operating in a fully- and commonly-
  If agents in a system share the same language, optimal lin- observed environment; transitions between states in that envi-
guistic action is a matter of deciding what and when to com- ronment arise from joint actions of all agents, and a common
municate, given its cost relative to the projected beneﬁt of reward is shared by the system as a whole. While we can       100        % of Reward Earned                  Claim 2. Given an inﬁnite time-horizon, agents acting ac-
                 % of Language Learned                cording to the elementary action protocol in a suitable Dec-
        80                                            MDP-Com will eventually converge upon a joint policy that
                                                      is optimal for the states they encounter from then on.
        60

        40                                            4   Empirical Results and Conclusions
     Pct.  Learning Progress                          To explore the viability of our approach, we implemented our
        20                                            language-learning protocol for a reasonably complex (but still
                                                      suitable) Dec-MDP-Com, involving two agents in joint con-
        0
         0   2000  4000  6000  8000  10000 12000      trol of a set of pumps and ﬂow-valves in a factory setting.
                   Time−Steps in Learning Process
                                                        Our results show the elementary protocol converging on
                                                      optimal policies in a wide range of problem-instances. Fig-
   Figure 1: Reward accumulated as language is learned. ure 1 gives an example, for a problem-instance featuring 100
                                                      vocabulary-items for each agent, showing the percentage of
calculate an optimal joint policy for such a process ofﬂine, total accumulated reward, and total shared vocabulary, at each
this is not the same thing as implementing it. Unless agents time-step in the process of learning and acting. As can be
can coordinate their actions, there is no guarantee of a jointly seen, the learning process (top, dotted line) proceeds quite
optimal policy, since communication is not allowed, or is un- steadily. Reward-accumulation, on the other hand, grows
reliable. Boutilier thus deﬁnes coordination problems, which with time before ﬁnally stabilizing. Initially, language learn-
arise when agents may each take an individual action that is ing outpaces reward gain given that knowledge, as agents still
potentially optimal, but which combine in sub-optimal fash- ﬁnd many of the other’s actions and observations hard to de-
ion. We show that certain, putatively more complex, Dec- termine. As time goes on, the rate of accumulated reward nar-
MDP-Coms in fact reduce to MMDPs for which such prob- rows this gap considerably; agents now know much of what
lems do not arise. This is notable, as Dec-MDPs are generally they need to communicate, and spend more time accumu-
intractable, while MMDPs can be solved efﬁciently.    lating reward in familiar circumstances, without necessarily
                                                      learning anything new about the language of others.
Deﬁnition 3 (Fully-describable). A Dec-MDP-Com is fully- These experimental results conform with intuition, show-
describable if and only if each agent αi possesses a language ing that while a small amount of language learning does little
Σi that is sufﬁcient to communicate both: (a) any observation to help agents in choosing their actions, they are capable of
it makes, and (b) any action it takes.                very nearly optimal action even in the presence of an under-
Deﬁnition 4 (Freely-describable). A Dec-MDP-Com  is   standing that is still less than perfect. This opens the door
freely-describable if and only if for any agent αi and mes- for further study into approximation in these contexts. We
sage σ ∈ Σi, the cost of communicating that message is 0. continue to investigate and compare other approaches to the
Claim 1. A Dec-MDP-Com is equivalent to an MMDP with- problem, including analysis of the differences between possi-
out coordination problems if (a) it is both fully- and freely- ble optimal ofﬂine techniques and online learning methods.
describable; and (b) agents share a common language.
                                                      Acknowledgments
Suitability and Convergence
                                                      This work was supported in part by the National Science
For any freely- and fully-describable Dec-MDP-Com, agents Foundation under grant IIS-0219606 and by the Air Force Of-
can calculate an optimal joint policy, under the working as- ﬁce of Scientiﬁc Research under grant F49620-03-1-0090.
sumption that all agents share a common language and that
all relevant information is shared. Where agents must in References
fact learn to communicate, however, implementation of such
policies requires cooperation from the environment, so that [Bernstein et al., 2002] D. S. Bernstein, R. Givan, N. Immer-
agents can update translations appropriately over time. The man, and S. Zilberstein. The complexity of decentralized
full deﬁnition of a suitable Dec-MDP-Com cannot be in-   control of Markov decision processes. Mathematics of Op-
cluded here; we simply note that in such problems, the prob- erations Research, 27:819–840, 2002.
ability that each agent assigns to the actual prior observations [Boutilier, 1999] C. Boutilier. Sequential optimality and co-
and actions of others following some state-transition is strictly ordination in multiagent systems. In Procs. of IJCAI-99,
greater than that of the observations and actions considered pages 478–485, Stockholm, Sweden, 1999.
most likely before that transition (unless those entries were [Goldman and Zilberstein, 2004] C. V. Goldman and S. Zil-
actually correct). Suitable Dec-MDP-Coms provide enough  berstein. Decentralized control of cooperative systems:
information to ensure that others’ actual actions and observa- Categorization and complexity analysis. Journal of Ar-
tions are more likely than mistaken ones.                tiﬁcial Intelligence Research, 22:143–174, 2004.
  We extend work of Goldman et al. [2004] (where agents
                                                      [                 ]
communicate states but not actions), to give an elementary Goldman et al., 2004 C. V. Goldman, M. Allen, and S. Zil-
action protocol. Using such a protocol for action and belief- berstein. Decentralized language learning through acting.
update, agents move towards optimality, based upon the ob- In Procs. of AAMAS-04, pages 1006–1013, New York City,
served consequences of action in a suitable problem-domain. NY, 2004.