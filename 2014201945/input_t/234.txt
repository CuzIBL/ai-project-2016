                 Integrating Background Knowledge Into Text Classification 

                         Sarah Zelikovitz                                      Haym Hirsh 
                College of Staten Island - CUNY                             Rutgers University 
                        2800 Victory Blvd                                110 Frelinghuysen Road 
                     Staten Island, NY 11694                              Piscataway, NJ 08854 
                zelikovitz@postbox.csi.cuny.edu                            hirsh@cs.rutgers.edu 


                        Abstract                                 We present three methods of incorporating background 
                                                               knowledge into the text classification task. Each of these 
     We present a description of three different algo•
                                                               methods uses the corpus of background knowledge in a dif•
     rithms that use background knowledge to improve 
                                                               ferent way, yet empirically, on a wide variety of text clas•
     text classifiers. One uses the background knowl•
                                                               sification tasks we can show that accuracy on test sets can 
     edge as an index into the set of training exam•
                                                               be improved when incorporating background knowledge into 
     ples. The second method uses background knowl•
                                                               these systems. We ran all three methods incorporating back•
     edge to reexpress the training examples. The last 
                                                               ground knowledge on a range of problems from nine differ•
     method treats pieces of background knowledge as 
                                                               ent text classification tasks. Details on the data sets can be 
     unlabeled examples, and actually classifies them. 
                                                               found at (www.cs.csi.cuny.edu/~zelikovi/datasets; each var•
     The choice of background knowledge affects each 
                                                               ied on the size of each example, the size of each piece of 
     method's performance and we discuss which type 
                                                               background knowledge, the number of examples and number 
     of background knowledge is most useful for each 
                                                               of items of background knowledge, and the relationship of 
     specific method. 
                                                               the background knowledge to the classification task. 

1 Using Background Knowledge                                   2 Methods 
Supervised learning algorithms rely on a corpus of labeled     In our first approach we use Naive Bayes and EM as in 
training examples to produce accurate automatic text clas•     [Nigam et al, 2000]. We can substitute more general back•
sifiers. An insufficient number of training examples often     ground knowledge for unlabeled examples, and obtain im•
results in learned models that are suboptimal when classi•     provements in accuracy on text classifiers that arc created us•
fying previously unseen examples. Numerous different ap•       ing both the training set and the set of background knowledge. 
proaches have been taken to compensate for the lack of train•  Naive Bayes classifiers make the assumption that examples 
ing examples. These include the use of unlabeled exam•         (both labeled and unlabeled) have been generated by a mix•
ples [Bennet and Demiriz, 1998; Blum and Mitchell, 1998;       ture model that has a one-to-one correspondence with classes. 
Nigam et al, 2000; Goldman and Zhou, 2000], the use of test    Even if this assumption is true for the labeled data and the test 
examples [Joachims, 19991, and choosing a small set of spe•    data, by its very nature, background knowledge should not fit 
cific unlabeled examples to be manually classified [Lewis and  this assumption at all. However, the interesting observation 
Gale, 1994].                                                   that we make is that to gain leverage out of unlabeled exam•
   Our approach does not assume the availability of either un• ples, the unlabeled data that we have need not be specifically 
labeled examples or test examples. As a result of the explo•   and accurately unlabeled examples. As long as the vocabu•
sion of the amount of data that is available, it is often the  lary and classification structure closely resembles the train•
case that text, databases and other sources of knowledge that  ing/test data, background knowledge can improve classifica•
are related to the text classification task are readily available tion accuracy in textual data using the EM algorithm. 
from the World Wide Web. We incorporate such "background         A second approach that we take is based upon a near•
knowledge" into different learners to improve classification   est neighbor text classifier using WHIRL [Cohen, 1998; 
of unknown instances. The use of external readily available    Cohen and Hirsh, 1998]. Instead of simply comparing a test 
textual resources allows learning systems to model the do•     example to the corpus of training examples, we use the items 
main in a way that would be impossible by simply using a       of background knowledge as "bridges" to connect each new 
small set of training instances. For example, if a text classi• example with labeled training examples. A labeled training 
fication task is to determine the sub-discipline of physics that example is useful in classifying an unknown test instance if 
a paper title should belong to, background knowledge such      there exists some set of unlabeled background knowledge that 
as abstracts, physics newsgroups, and perhaps even book re•    is similar to both the test example and the training example. 
views of physics books can be used by learners to create more  We call this a "second-order" approach to classification [Ze•
accurate classifiers.                                          likovitz and Hirsh, 2000; 2002], in that data are no longer di-


1448                                                                                                   POSTER PAPERS  rectly compared but rather, are compared one step removed,    References 
 through an intermediary. 
                                                               [Bennet and Demiriz, 19981 K. Bennet and A. Demiriz. 
   Finally we use the background knowledge to redescribe          Semi-supervised support vector machines. Advances 
 both the training and the test examples. To do this, we          in Neural Information Processing Systems, 12:368—374, 
add the background knowledge documents to the training            1998. 
 set, to create a large, sparse term-by-document (t x d) ma•
                                                               [Blum and Mitchell, 1998] A. Blum and Tom Mitchell. 
trix. We then use Latent Semantic Indexing (LSI) [Deer-
                                                                  Combining labeled and unlabeled data with co-training. In 
 wester et al., 19901 to automatically redescribe textual data 
                                                                  Proceedings of the 11th Annual Conference on Computa•
 in a new smaller semantic space using singular value de•
                                                                  tional Learning Theory, pages 92-100, 1998. 
composition. The original space is decomposed into lin•
early independent dimensions or "factors", and the terms and   [Cohen and Hirsh, 1998] William Cohen and Haym Hirsh. 
documents of the training and test examples are then repre•       Joins that generalize: Text categorization using WHIRL. 
sented in this new vector space [Zelikovitz and Hirsh, 2001;      In Proceedings of the Fourth International Conference on 
 2002J. Documents with high similarity no longer simply           Knowledge Discovery and Data Mining, pages 169-173, 
share words with each other, but instead are located near each    1998. 
other in the new semantic space. Since this semantic space     [Cohen, 1998] William Cohen. Integration on heterogeneous 
 was created by incorporating the background knowledge, the       databases without common domains using queries based 
model of the domain that it creates reflects both the training    on textual similarity. In Proceedings of ACM-SIGMOD 
set and the background knowledge.                                 98, 
                                                               [Deerwesteref al., 1990] S. Deerwester, S. Dumais, G. Fur•
3 Comparison of Approaches                                        nas, and T. Landauer. Indexing by latent semantic analysis. 
                                                                  Journal for the American Society for Information Science, 
                                                                  41(6):391-407, 1990. 
Different types of background knowledge are most useful for 
each of these three systems. The system based upon WHIRL       [Goldman and Zhou, 2000] S. Goldman and Y. Zhou. En•
performs best on the problems where the form and size of          hancing supervised learning with unlabeled data. In Pro•
the background knowledge is substantially different than the      ceedings of the Seventeenth International Conference on 
training and test data. For example, we classify names of         Machine Learning, 2000. 
companies by area using Yahoo! pages as background knowl•      [Joachims, 1999] T. Joachims. Transductive inference for 
edge. These background pieces of data are not really classi•      text classification using support vector machines. In Pro•
fiable, in the sense that they do not necessarily belong to any   ceedings of the Sixteenth International Conference on Ma•
specific class. Since this WHIRL-based method does not at•        chine Learning, pages 200-209, 1999. 
tempt to classify the background knowledge, but merely uses 
                                                               [Lewis and Gale, 1994] David D. Lewis and William A. 
it to index into the training corpus, it makes the best use of 
                                                                  Gale. A sequential algorithm for training text classifiers. 
this background knowledge. 
                                                                  In SIGIR94: Proceedings of the Seventeenth Annual Inter•
   For the data sets where the background knowledge fits very     national ACM-SIGIR Conference on Research and Devel•
closely to the training and test classification task, EM outper•  opment in Information Retrieval, pages 3-12, 1994. 
forms the other systems. For example, EM performed best 
                                                               [Nigam et al, 2000] Kamal Nigam, Andre Kachites Mccal-
when classifying physics papers by subdiscipline using ab•
                                                                  lum, Sebastian Thrun, and Tom Mitchell. Text classifi•
stracts as background knowledge. This is consistent with the 
                                                                  cation from labeled and unlabeled documents using EM. 
way EM makes use of background knowledge. Since EM 
                                                                  Machine Learning, 39(2/3): 103-134,2000. 
actually classifies the background knowledge, and uses the 
background knowledge to decide on the parameters of its gen•   [Zelikovitz and Hirsh, 20001 S. Zelikovitz and H. Hirsh. Im•
erative model, the closer the background knowledge is to the      proving short text classification using unlabeled back•
training and test sets, the better EM will perform. Ideally,      ground knowledge to assess document similarity. In Pro•
for EM, we wish the background knowledge to be generated          ceedings of the Seventeenth International Conference on 
from the same model as the training and test sets.                Machine Learning, pages 1183-1190,2000. 
   Reexpressing the data and background with LSI seems to      [Zelikovitz and Hirsh, 2001] S. Zelikovitz and H. Hirsh. Us•
be most effective when there is very limited training data. On    ing LSI for text classification in the presence of back•
the smallest data sets, it outperforms all the other methods      ground text. In Proceedings of the Tenth Conference for 
in many domains. When very few training examples exist,           Information and Knowledge Management, 2001. 
this method can still build a space that correctly models the  [Zelikovitz and Hirsh, 2002] S. Zelikovitz and H. Hirsh. In•
domain by using the available background knowledge.               tegrating background knowledge into nearest-Neighbor 
   We are currently looking at methods to evaluate sets of        text classification. In Proceedings of the 6th European 
background knowledge to determine the amount of back•             Conference on Case Based Reasoning, 2002. 
ground knowledge as well as the measure of relevance that 
it must have to the training set to be useful for each of these 
learners. 


POSTER PAPERS                                                                                                       1449 