                    Dynamic Mixture Models for Multiple Time Series

              Xing Wei                        Jimeng Sun                       Xuerui Wang
   Computer Science Department       Computer Science Department       Computer Science Department
    Univeristy of Massachusetts       Carnegie Mellon University        Univeristy of Massachusetts
        Amherst, MA 01003                 Pittsburgh, PA 15213              Amherst, MA 01003
        xwei@cs.umass.edu                 jimeng@cs.cmu.edu                xuerui@cs.umass.edu

                    Abstract                          variance directions. This construction often seems not intu-
                                                      itive to the end users due to the reiﬁcation of the mathemat-
    Traditional probabilistic mixture models such as  ical properties of the SVD techniques. In particular, some
    Latent Dirichlet Allocation imply that data records streams may have negative coefﬁcients for hidden variables
    (such as documents) are fully exchangeable. How-  even when all the measurements are nonnegative. This draw-
    ever, data are naturally collected along time, thus back is intrinsic due to the Gaussian distribution assumption
    obey some order in time. In this paper, we present on the input streams.
    Dynamic Mixture Models (DMMs) for online pat-
    tern discovery in multiple time series. DMMs do     Using mixture of hidden variables for data representation
    not have the noticeable drawback of the SVD-based has recently been of considerable interest in text modeling.
    methods for data streams: negative values in hid- One early example is the Latent Semantic Indexing (LSI)
    den variables are often produced even with all non- model, which is also based on the SVD technique. More
    negative inputs. We apply DMM models to two       recently, Hoffman presents the probabilistic Latent Seman-
                                                                                 [             ]
    real-world datasets, and achieve signiﬁcantly better tic Indexing (pLSI) technique Hoffman, 1999 . This ap-
    results with intuitive interpretation.            proach uses a latent variable model that represents documents
                                                      as mixtures of topics in a probabilistic framework. Latent
                                                      Dirichlet Allocation (LDA) [Blei et al., 2003], which resem-
1  Introduction                                       bles the generative process of pLSI but overcomes some of
Multiple co-evolving time series or data streams are ubiqui- its drawbacks, has quickly become one of the most popular
tous in many different real-world applications. Considering a probabilistic text modeling techniques in machine learning
sensor network, multiple sensors continuously collect differ- and has inspired a series of research works in this direction
ent measurements over time (e.g., chlorine concentration at [Girolami and Kaban, 2005; Teh et al., 2004]. In particular,
different locations in a water distribution system; temperature LDA has been shown to be effective in some text-related tasks
or light measurements of different rooms; host status of dif- such as document classiﬁcation [Blei et al., 2003] and infor-
ferent machines in a data center; etc.). A central site usually mation retrieval [Wei and Croft, 2006], but its effectiveness
analyzes those measurements for main trends and anomaly on continuous data and time-series remains mostly unknown.
detection. There has been success in mining multiple streams. Most topic models mentioned above assume that data
However, the existing methods on analyzing such streams still records (e.g., documents) are fully exchangeable, which are
have signiﬁcant weaknesses:                           not true in time-series model from many real-world appli-
  First, existing monitoring softwares require considerable cations. Exactly because of that, topic modeling over time
time and expertise to be properly conﬁgured despite the fact starts to receive more and more attentions. In the recent Dy-
they treat the streams as independent. For each data stream namic Topic Models [Blei and Lafferty, 2006], topic evolu-
that an administrator intends to monitor, he/she must make tions are modeled through collections sliced into certain peri-
decisions upon proper thresholds for the data values. That ods of time. However, the time dependency of individual data
is, he/she must deﬁne, for each data stream, what constitutes records (documents) inside a collection/period are not consid-
normal behaviors. The correlations across streams are usually ered. In the Topic over Time (TOT) model [Wang and McCal-
not captured.                                         lum, 2006], continuous time stamps are put into an LDA-style
  Second, the state-of-the-art algorithms on automatically topic model as observations. But drawing time stamps from
summarizing multiple streams often adopt matrix decomposi- one distribution as in the TOT model is often not enough for
tions, like Singular Value Decomposition (SVD) and its vari- dealing with bursty data, common in data streams.
ants. For example, Papadimitriou et al. uses online SVD In this paper, we present a Dynamic Mixture Model
tracking to summarize the multiple streams incrementally (DMM), a latent variable model that takes into considera-
through a small number of hidden variables [Papadimitriou tion the time stamps of data records in dynamic streams. The
et al., 2005]. The hidden variables computed there are lin- values in streams are represented as mixtures of hidden vari-
ear combinations of all streams projecting onto the maximum ables, and for each time stamp, the mixture of hidden vari-

                                                IJCAI-07
                                                  2909ables for all streams is dependent on the mixture of the pre- α α      α       α
vious time stamp. In this way we can capture the short-term
                                                       θ       θ       θ       θ  ··· θ   θ    θ   ··· θ
dependencies. Compared to the SVD techniques, it does not                      1      t−1  t   t+1     D

have their common drawback: non-intuitive negative values z    z       z       z      z    z    z      z
of hidden variables, and thus has a more interpretable expla-
nation within a probabilistic framework. Compared to the w     w       w       w      w   w    w      w
                                                        Nd      Nd      Nd      N1    Nt−1 Nt   Nt+1   ND
static LDA-style models, it can take advantage of the time D     D       D
dependency of multiple streams. Finally, we want to point
                                                       β       β       β                   φβ
out that topic models have been mainly designed for discrete             K                  K
data. To apply the LDA-style mixture models to continuous
data, the standard way is discretization, which we show in Figure 1: Graphical model representation of Dynamic Topic
Section 4 works well in DMM.                          Model (left, 3 slices) and Dynamic Mixture Model (right)

2  Related Work                                        SYMBOL      DESCRIPTION
Data streams, containing much more interesting information K       number of hidden components / topics
than static data, have been extensively studied in recent years. D number of snapshots / documents
Evidence has shown that temporal information plays a cru-  V       number of parameters / words
cial role in capturing many meaningful and interesting pat- Nt     sum of parameter values in the tth snapshot /
terns. The main objective of temporal analysis is to efﬁciently    number of words in the tth document
discover and monitor patterns when data are streaming into t       snapshot index / time stamp of documents
a system. A recent survey [Muthukrishnan 2005] has dis-    θ       mixture distribution of hidden components
cussed many data streams algorithms. Among of these, SVD           for a snapshot / document
and PCA are popular tools for summarizing multiple time    φ       mixture distribution over parameters / words
series into a number of hidden variables [Papadimitriou et         for a hidden component /topic
al., 2005]. However, it assumes independency across time   z       hidden component / topic
stamps; the results are often lack of probabilistic argument w     one unit value of a parameter / word token
and intuitive interpretation.                              α       hyperparameter for the multinomial θ1
                                                           β                       φ
  Probabilistic mixture models [Hoffman, 1999; Blei et al.,        hyperparameter for
2003] are widely used to summarize data based on statistical
frameworks. To capture time evolution, the usage of time         Table 1: Notation correspondence
within probabilistic mixture models has been around for a
while. For example, time can be taken care of in a post-
hoc way. One could ﬁrst ﬁt a time-unaware mixture model, distribution, TOT is not appropriate for data streams, which
and then order the data in time, slice them into discrete sub- are usually bursty and multi-modal.
sets, and examine the mixture distributions in each time-slice
[Grifﬁths and Steyvers, 2004]. Alternatively, non-joint mod- 3 Dynamic Mixture Models
eling can also pre-divide the data into discrete time slices, ﬁt Mixture models have been widely used in modeling various
a separate mixture model in each slice and possibly align the complex data with very competitive results. But the usage of
mixture components across slices [Wang et al., 2006]. latent mixture model in streaming data has not been explored,
  More systematic ways to take advantage of the temporal in- and largely remains unknown. We now present a dynamic
formation in mixture models can be put into two categories. mixture model (DMM) incorporating temporal information
First, the dynamic behaviors are driven by state transitions. in data.
This kind of models generally make a Markov assumption, The graphical model representation of the DMM is shown
i.e., the state at time t +1or t +Δt is independent of all in Figure 1 (right). As other mixture models for discrete
other history given the state at time t. For instance, Blei and data, the DMM can be regarded as a topic model as well.
Lafferty present dynamic topic models (DTMs) in which the For presentation convenience, we explain this model in the
alignment among topics across collections of documents is background of both text and the streaming data (Section 4 de-
modeled by a Kalman ﬁlter [Blei and Lafferty, 2006]. DTMs scribes the streaming data in detail). Table 1 summarizes the
target on topic evolution over sets of large amounts of data notation correspondences for both text and data streams. In
records and local dependencies between two records are not particular, each snapshot corresponds to a document in topic
captured. Second, the other type of models does not em- models; each stream corresponds to a word occurrence over
ploy a Markov assumption over time, but instead treats time time, e.g., the number of occurrences of word wi over time
as an observed continuous variable such as the topics over represents a single stream wi, which could be the chlorine
time (TOT) models [Wang and McCallum, 2006]. This helps concentrations over time, or the light intensities over time of
capture long-range dependencies in time, and also avoids a ith sensor.
Markov model’s risk of inappropriately dividing a mixture The generative process of the DMM is similar to Latent
component (topic) in two when there is a brief gap in its ap- Dirichlet Allocation [Blei et al., 2003], but the mixture dis-
pearance. However, with all time stamps drawn from one tribution θ for each document does not have a Dirichlet prior

                                                IJCAI-07
                                                  2910  (except the very ﬁrst one); instead, θt is dependent on the 3.1 Dirichlet Dynamic Mixture Models
                                       θt−1
  mixture distribution of the previous snapshot .       As we have described, θt is generated from a distribution with
    First, we assume there are strong evolution dependencies. θt−1 as the expectation. There are no strict requirements for
  The continuous stream data are evenly sampled in time as this distribution, and we just want to build up connections be-
  successive snapshots that reﬂect the intrinsic dependencies tween two successive snapshots, i.e., the mixture distribution
  between the values along time series.                 of the current snapshot is dependent on the one of its previ-
    Second, the time dependency is complicated and the  ous snapshot. In this setting, a continuous one-modal distri-
  changes hardly follow a continuous time distribution as in the bution is desired. Gaussian distribution is a straightforward
  TOT model [Wang and McCallum, 2006]. Thus neighboring selection when θ is represented by natural parameters, which
  dependency (Markov assumption) is more appropriate.   was adopted by [Blei and Lafferty, 2006] to model the depen-
    Compared to the TOT model, the Dynamic Mixture Model dency between two consecutive α (Figure 1, left). However,
  is constructed on discrete time stamps and assumes depen- Gaussian distribution is not conjugate to the multinomial dis-
  dencies between two consecutive snapshots. Although the tribution that is used to generate hidden components / top-
  TOT model captures both short-term and long-term changes ics, which makes inference more difﬁcult. Although, to the
  by treating time stamps as observed random variables, DMM best of our knowledge, there is no conjugate prior for Dirich-
  is capable to capture more detailed changes and to model the let distribution; Dirichlet distribution is conjugate to multino-
  dependency between any two consecutive time shots, which mial distribution; and we use a Dirichlet distribution to model
  is especially appropriate for many streaming data when the the dependency between consecutive θ. Expectation only is
  data are equally sampled in time.                     not enough to make the proposed distribution identiﬁable,
    Compared to Dynamic Topic Models (DTM, see Figure 1, since there are inﬁnitely many Dirichlet distribution having
  left), DMM captures the evolution between snapshots (doc- the same expectation. Thus we introduce another precision
  uments) instead of between snapshot-groups (collections in constraint that all the parameters of the Dirichlet distributions
  text model). In both DTM and LDA, documents in a col- sum to ψ that is also equal to the sum of all parameters α in
  lection and words in a document are fully exchangeable; in the ﬁrst Dirichlet distribution. A Dirichlet distribution can
  DMM, snapshots of multiple time series, which correspond be completely parameterized by the mean and precision pa-
  to documents in text model, have very strong temporal order rameters [Minka, 2003]. For notation convenience, we deﬁne
  and exchanges of snapshots can lead to very different results. θ0 = α/ψ. That is, θt|θt−1 ∼ Dir(ψθt−1).
  From this perspective, DMM is a true online model. Note that
  SVD also treats time series as vectors, thus the permutation Inference
  of snapshots will not make a difference.              Inference can not be done exactly in complex graphical mod-
    We model the dependency by setting the expectation of the els such as DMMs. The non-conjugacy between Dirichlet
  distribution of θt to be θt−1, i.e., θt is generated from a dis- distributions makes standard Gibbs sampling methods [Ge-
  tribution with θt−1 as the 1st order moment (we will discuss man and Geman, 1994] harder in approximate inference of
  the concrete distribution in Section 3.1). The process of gen- DMMs. Here, we use a simple, but effective iterated sam-
  erating time series streams in DMM is as follows:     pling procedure considering that streaming data are very dif-
                                                        ferent from traditional data sets such as large text collections:
                                φ
   1. Pick a multinomial distribution z for each topic (hidden ﬁrst, massive amounts of data arrive at high rates, which
                z
      dimension) from a Dirichlet distribution with parame- makes efﬁciency the most concern; second, users, or higher-
         β
      ter ;                                             level applications, require immediate responses and cannot
   2. For time shot t =0, sample a multinomial distribution afford any post-processing (e.g., in network intrusion detec-
      θt from a Dirichlet distribution with parameter α, tion) [Papadimitriou et al., 2005]. In summary, the algorithm
                                                        is expected to be efﬁcient, incremental, scalable, and possi-
                      t>0
   3. For each time shot   , sample a multinomial distri- bly scaling linearly with the number of streams. As shown in
            θt                              θt−1
      bution  from a distribution with expectation ,    [Grifﬁths and Steyvers, 2004], we deﬁne mz,w to be the num-
   4. Sample a hidden variable / topic z ∈{1, ···,K} from a ber of tokens of word w assigned to topic z, and the posterior
                                                                    φ
      multinomial distribution with parameter θt,       distribution of z can be approximated by
                                w                                               m    + β
   5. Add a unit value to parameter picked from a multi-             φˆ   =       z,w   w    .
                                   φ               w                  z,w   V
      nomial distribution with parameter zw / Pick a word                         (m    + β )
                                              φ                               w=1   z,w    w
      from a multinomial distribution with parameter zw .
                                                                                          θ
  Thus, the likelihood of generating a data set of multiple data To estimate the posterior distribution of t in streaming data,
  streams is:                                           we use a technique that is commonly used in mean-ﬁeld vari-
                                                        ational approximation: assume each latent variable to be in-
                                   
                                       K               dependent to the others. Then we use an iterative procedure
 P (Snapshot1, ···, SnapshotD|α, β)=      p(φz|β)       to update all the θ periodically. We deﬁne nt,z to be the num-
                                       z=1              ber of tokens in document t assigned to topic z. Thus, we
        D          D Nt K                           have,
×p(θ |α)   p(θ |θ  )          (P (z |θ)P (w |φ )) dθdφ                        n   + ψθˆ
    1         t t−1                i      i zi                      ˆ          t,z    t−1,z
                                                                    θt,z =                   .
        t=2         t=1 i=1 zi=1                                             K          ˆ
                                                                             z=1(nt,z + ψθt−1,z)

                                                  IJCAI-07
                                                    2911  In each sample, we draw in turn zt,i according to a prob-
                   θˆ   × φˆ                 θˆ
ability proportional to t,zt,i zt,i,wt,i , and update after
each iteration.
  Streams are very special data and require the corresponding
algorithms to be highly efﬁcient in order to react in a timely
manner. It could be certainly possible to derive a variational
approximation from the scratch, however, we have empiri-
cally found that our procedure is very effective and efﬁcient
for streaming data presented in Section 4. With this iter-
ated sampling algirithm, we keep the information from ofﬂine
training, and run sampling only for the coming snapshot with
a couple of iterations. The parameter estimation is updated
after each new snapshot arrives.
                                                      Figure 2: Chlorine reconstruction; reconstruction based on 2
4  Experiments                                        hidden variables are very close to the original value.
In this section we present case studies on real and realistic
datasets to demonstrate the effectiveness of our approach in
discovering the underlying correlations among streams.
4.1  Chlorine Concentrations
Description: The Chlorine dataset was generated by EPA-
NET2.01 that accurately simulates the hydraulic and chem-
ical phenomena within drinking water distribution systems.
Given a network as the input, EPANET tracks the ﬂow of
water in each pipe, the pressure at each node, the height
of water in each tank, and the concentration of a chemical
species throughout the network, during a simulation period
comprised of multiple time stamps. We monitor the chlo-
rine concentration level at all the 166 junctions in the network
shown in Figure 5 for 4310 time stamps during 15 days (one Figure 3: Hidden variables for chlorine data; the ﬁrst captures
time tick every ﬁve minutes). The data was generated by us- the daily cycles; the second reﬂects the time shift of different
ing the input network with the demand patterns, pressures, sensors.
ﬂows speciﬁed at each node.
  Data characteristics: The two key features are: 1) A clear the constraint that the probability mass over all parameters
global periodic pattern (daily cycle, dominating residential equals one. Unlike the SVD where a hidden variable is a lin-
demand pattern). Chlorine concentrations reﬂect this, with ear combination of all streams, our method interprets the hid-
few exceptions. 2) A slight time shift across different junc- den variable as a generating process behind all 166 streams
tions, which is due to the time it takes for fresh water to ﬂow which is more intuitive (see Figure 4). Notice that there is the
down the pipes from the reservoirs.                   high probability mass on stream 1-7 because those sensors
  Thus, most streams exhibit the same sinusoidal-like pat- (highlighted on Figure 5) are close to the reservoir and on the
tern, except with gradual phase shifts as we go further away main pipe of the water distribution network.
from the reservoir.
  Reconstruction: Our method can successfully summarize 4.2 Light Measurements
the data using just two numbers (hidden variables) per time Description: This dataset consists of light intensity measure-
tick (see Figure 3), as opposed to the original 166 numbers. ments collected using Berkeley Mote sensors, at several dif-
Figure 2 shows the reconstruction for one sensor (out of 166). ferent locations in a lab (see Figure 6) over a period of a
The reconstructions of the other sensors achieve similarly re- month [Deshpande et al., 2004]. We simulate streams with
sults. Note that only two hidden variables give very good this dataset by cutting the set into two even parts, and process
reconstruction.                                       the snapshots in the second part one by one in an online man-
  Hidden variables: The two hidden variables (see Figure 3) ner with the trained model from the ﬁrst part. The results we
reﬂect the two key dataset characteristics: 1) The ﬁrst hidden present of this dataset are all from stream simulation running.
variable captures the global, periodic pattern; 2) The second Data characteristics: The main characteristics are: 1) A
one also follows a very similar periodic pattern, but with a clear global periodic pattern (daily cycle). 2) Occasional big
slight phase shift.                                   spikes from some sensors (outliers). Reconstruction: Similar
  Interpretation of hidden variables: Each hidden variable to the chlorine data, the reconstruction error on light data is
follows a multinomial distribution with 166 parameters with also very small.
                                                        Hidden variable: The ﬁrst hidden variable exhibits the
  1http://www.epa.gov/ORD/NRMRL/wswrd/epanet.html     daily periodicity as shown in Figure 8. The probability distri-

                                                IJCAI-07
                                                  2912                                                      Figure 6: Sensor map, the highlighted region receives more
Figure 4: The distribution of 1st hidden variable; Note that sunshine.
sensor 1 to 7 have signiﬁcantly higher mass because they are
on the main pipe of water distribution.


                                                      Figure 7: Light reconstruction; reconstruction based on 4 hid-
                                                      den variables are very close to the original value.
Figure 5: Water distribution network: Sensor 1-7 are high-
lighted since they are close to reservoir (red arrow) and on mixture models such as Latent Dirichlet Allocation, but in
the main pipe.                                        those models, the order of the data is ignored, thus all data
                                                      records are assumed to be fully exchangeable. The DMMs,
                                                      on the contrary, take into consideration the temporal informa-
bution (see Figure 9) concentrates on the sensor 37-44 (high- tion (e.g., order) implied in the data . Also, compared to the
lighted in Figure 6) since they receive more sunshine and state-of-the-art SVD-based methods for data streams, DMMs
close to window.                                      naturally give positive values of hidden variables, so the re-
  Comparison with static mixture modeling: To show the sults from DMMs are much more intuitive and interpretable.
effectiveness of modeling mixtures dynamically, and model- We believe that probabilistic mixture modeling is a promis-
ing the dependency between mixtures of hidden variables, ing direction for streaming data, especially Dynamic Mixture
we compare our streaming modeling based on time series Models have been shown to be a very effective model for mul-
to the non-time dependency latent variable modeling tech- tiple time series application.
niques, e.g., the LDA model. The sums of reconstruction er- For future work, we plan to apply DMMs to larger datasets
rors over time are compared and signiﬁcant less (p<1e − 20 and improve both of its performance and efﬁciency on on-
under t-test) error rate were achieved with the Dynamic Mix- line streaming data. Modeling the dependencies using other
ture Model from all of 1 to 3 iterations, as shown in Figure distributions other than Dirichlet distribution will also be in-
10.                                                   teresting.

5  Conclusion and Future Work                         Acknowledgments
In this paper, we have presented Dynamic Mixture Models This work was supported in part by the Center for Intelligent
(DMMs) for online pattern discovery in multiple time series, Information Retrieval, in part by the National Science Foun-
and shown interesting results on two data sets. The generative dation under Grants No. IIS-0209107, IIS-0205224, INT-
process of DMMs is similar to traditional, static probabilistic 0318547, SENSOR-0329549, IIS-0326322, in part by the

                                                IJCAI-07
                                                  2913