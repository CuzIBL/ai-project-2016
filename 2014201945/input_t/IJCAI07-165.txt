                                     Deictic Option Schemas

       Balaraman Ravindran                 Andrew G. Barto                    Vimal Mathew
  Dept. of Computer Science and Engineering Dept. of Computer Science Dept. of Computer Science and Engineering
            IIT Madras, India         University of Massachusetts, Amherst     IIT Madras, India
           ravi@cse.iitm.ac.in               barto@cs.umass.edu                vml@cse.iitm.ac.in


                    Abstract                          problem. We present a Bayesian algorithm for learning the
                                                      correct conﬁguration of pointers and validate our approach
    Deictic representation is a  representational     on a simulated game domain inspired by Agre and Chapman
    paradigm, based on selective attention and point- [1987].
    ers, that allows an agent to learn and reason       We employ Markov Decision Processes (MDPs) as our
    about rich complex environments. In this article  basic modeling paradigm. First we present some notation
    we present a hierarchical reinforcement learning  regarding factored MDPs (Section 2), a brief summary of
    framework that employs aspects of deictic repre-  relativized options (Section 3) and introduce deictic option
    sentation. We also present a Bayesian algorithm   schemas( Section 4). Then we present our Bayesian algo-
    for learning the correct representation for a given rithm to choose the right pointer conﬁguration to apply to a
    sub-problem and empirically validate it on a      sub-task (Section 5). We relate our approach to an existing
    complex game environment.                         family of deictic algorithms in Section 6. In Section 7 we de-
                                                      scribe some related work, and we conclude in Section 8 by
1  Introduction                                       discussing some future directions for research.
Agre [1987; 1988] introduced deictic representations to the
AI community as a way of reasoning and learning in large 2 Notation
complex domains. It is a representational paradigm based on A structured (ﬁnite) MDP is described by the tuple
using a small set of pointers that lets the agent focus on parts S, A, Ψ,P,R,whereS is a ﬁnite set of states, A is a ﬁnite
of the state space relevant to solving the problem at hand. set of actions, Ψ ⊆ S × A is the set of admissible state-action
  Deictic pointers might be simple physical locators or they pairs, P :Ψ×S → [0, 1] is the transition probability function
might encode complex semantics. Agre and Chapman [1987] with P (s, a, s) being the probability of transition from state
employ pointers that let the agent precisely locate important s to state s under action a,andR :Ψ→ IR is the expected
components of the system, but their method needs substan- reward function, with R(s, a) being the expected reward for
tial pre-processing and domain knowledge. While solving performing action a in state s. The state set S is given by
                                                                                M
the arcade game Pengo, their agent Pengi employs complex M features or variables, S ⊆ i=1 Si,whereSi is the set
pointers such as bee-attacking-me, ice-cube-next-to-me,etc. of permissible values for feature i. Thus any s ∈ S is of the
The actions of the agent are then deﬁned with respect to these form s = s1,...,sM ,wheresi ∈ Si for all i.
pointers, for example, push ice-cube-next-to-me toward bee- The transition probability function P is usually described
attacking-me. In general, deictic representations can be used by a family of two-slice temporal Bayesian networks (2-
in rich environments with incredible amounts of detail. They TBNs), with one TBN for each action. A 2-TBN is a two
are also useful in systems where there are physical limita- layer directed acyclic graph whose nodes are {s1,...,sM}
                                                                 
tions on the sensory capabilities so that some form of atten- and {s1,..., sM}.Heresi denotes the random variable repre-
                                 [                                                      
tional mechanism has to be employed Minut and Mahade- senting feature i at the present state and si denotes the random
van, 2001].                                           variable representing feature i in the resulting state. Many
  There has been considerable interest in employing deictic classes of structured problems may be modeled by a 2-TBN
representations in a reinforcement learning (RL) framework in which each arc is restricted to go from a node in the ﬁrst
[Whitehead and Ballard, 1991; McCallum, 1995; Finney et set to a node in the second. The state-transition probabilities
al., 2002]. The attractions of such a synthesis are obvious and can be factored as:
can lead to a trial-and-error approach that can work in large
                                                                          M
environments. In this article we develop a hierarchical deictic                           
                                                              P (s, a, s )=  Prob(si|Parents(si,a)),
RL framework based on relativized options [Ravindran and
                                                                          i=1
Barto, 2003a]. Speciﬁcally we extend our earlier approach to
                                                                                                       
factored MDPs and show that certain aspects of deixis can be where Parents(si,a) denotes the parents of node si
modeled as ﬁnding structured homomorphic reductions of the in the 2-TBN corresponding to action a and each

                                                IJCAI-07
                                                  1023              
Prob(si|Parents(si,a)) is given by a conditional probability When the option is invoked, a particular instantiation of the
                             
table (CPT) associated with node si. In computing the condi- schema is chosen by binding to the appropriate resources.
tional probabilities it is implicitly assumed that the nodes in Given a set of possible bindings, [Ravindran and Barto,
        
Parents(si,a) are assigned values according to s.     2003a] presented a Bayesian approach to choosing the right
  An option (or a temporally extended action) [Sutton et al., binding to apply in a given context based on experience gath-
1999] in an MDP M  =  S, A, Ψ,P, R is deﬁned by the ered in solving the task. It assumed that the set of bindings
tuple O = I,π,β, where the initiation set I⊆S is the were given by a family of transformations, H, applied to Ψ
set of states in which the option can be invoked, π is the op- and maintained a heuristic weight vector, wn(h, ψ(s)),which
tion policy, and the termination function β : S → [0, 1] gives is a measure of the likelihood of transformation h ∈Hbeing
the probability of the option terminating in any given state. the right transformation in the context represented by ψ(s).1
The option policy can in general be a mapping from arbitrary The weight vectors are updated using:
sequences of state-action pairs (or histories) to action proba-                       
                                                                     PO((f(s),gs(a),f(s ))wn−1(h, ψ(s))
bilities. We restrict attention to Markov sub-goal options in wn(h, ψ(s)) =                           (1)
which the policies are functions of the current state alone,                        K
and an option terminates on reaching a set of pre-deﬁned                                  
                                                      where PO(s, a, s )=max(ν, PO(s, a, s )),andK    =
(sub)goal states. The states over which the option policy is    (( ( )  ( ) ( ))   (   ( ))
deﬁned is known as the domain of the option. In such cases h∈H PO f s ,gs a ,f s wn−1 h ,ψ s   is a nor-
the option policy can be deﬁned in a sub-MDP, known as the malizing factor. Since the projected transition probability is
option MDP, consisting of the states in the domain of the op- lower bounded by ν, this approach works even when the ho-
tion.                                                 momorphic image is not exact. In this article we extend
                                                      our earlier work to cases where the bindings are speciﬁed by
                                                      deictic pointers.
3  Relativized Options
A relativized option combines MDP homomorphisms [Ravin- 4 Deictic Option Schemas
dran and Barto, 2003b] with the options framework to com- The set of candidate transformations, H, can be speciﬁed by
pactly represent a family of related options. An MDP ho- a set of deictic pointers together with their possible conﬁg-
                                         M
momorphism is a transformation from an MDP  to a re-  urations. The agent learns to place the pointers in speciﬁc
           M                      M
duced model    such that a solution to yields a solu- conﬁgurations to effect the correct bindings to the schema.
      M
tion to . Notationally, an MDP homomorphism, h,isde-  We call such option schema together with the set of pointers
                     Ψ    Ψ
ﬁned as a surjection from to , given by a tuple of surjec- a deictic option schema. Formally a deictic option schema is
tions f,{gs|s ∈ S}, with h((s, a)) = (f(s),gs(a)),where
                                                   deﬁned as follows:
f : S → S  and gs : As → Af(s) for s ∈ S. M is called
the homomorphic image of M under h. An optimal policy Deﬁnition: A  deictic option schema of a factored MDP
in M when lifted to M yields an optimal policy in M. M   =  S, A, Ψ,P,R is the tuple D,O,whereO   =
                                                        M    I                           M        D  =
  In a relativized option, the policy for achieving the op- h, O, ,β , is a relativized option in ,and
                                                      {       ···    }
tion’s sub-goal is deﬁned in the image of a partial homo- D1,D2, ,DK   is the set of admissible conﬁgurations of
                                                      the deictic pointers, with K being the number of deictic point-
morphism deﬁned only over a subset of S. This image                             {1,···,M}
is called the option MDP. When the option is invoked, the ers available. For all i, Di ⊆ 2 is the collection of all
current state is projected onto the option MDP, MO =  possible subsets of indices of the features that pointer i can
SO,AO, ΨO,PO,RO, and the policy action is lifted to the project onto the schema, where M is the number of features
original MDP based on the states in which the option is in- used to describe S.
voked. A relativized option is deﬁned as follows:
                                            M    =      The set Di indicates the set of objects that pointer i can
Deﬁnition:  A  relativized option of an MDP           point to in the environment. For example, in a blocks world
S, A, Ψ,P,R is the tuple O = h, MO, I,β,whereI⊆
                      :    →  [0 1]                   domain this is the set of all blocks. In our example in the next
S is the initiation set, β SO   ,  is the termination section, it is the set of all possible adversaries.
function and h = f,{gs|s ∈ S} is a partial homomorphism
                 Ψ                     M             Once a deictic option schema is invoked the decision mak-
from the MDP S, A, ,P,R   to the option MDP  O with   ing proceeds in two alternating phases. In the ﬁrst phase the
R chosen based on the sub-task.
                            M                         agent picks a suitable pointer conﬁguration, using a heuris-
In other words, the option MDP O is a partial homomor- tic weight function similar to the one given in Section 3. In
phic image of an MDP with the same states, actions and tran-
                 M                                    the second phase, the agent picks an action, based on the per-
sition dynamics as  but with a reward function chosen ceptual information obtained from the pointers, using a Q-
based on the option’s sub-task. The homomorphism condi- function. This calling semantics is similar to that followed by
tions hold only for states in the domain of the option O.The Whitehead and Ballard [1991].
option policy π :ΨO → [0, 1] is obtained by solving MO
                                                      Each member of  H has a state transformation of the form
by treating it as an episodic task. When lifted to M, π is K            ∈
                                        Ψ               i=1 ρJi ,whereJi   Di for all i and ρJ , is the projec-
suitably transformed into policy fragments over .     tion of S onto the subset of features indexed by J.Ifh is
  A relativized option can be viewed as an option schema
where a template of an option policy is speciﬁed using a 1Frequently, ψ(s) is some simple function of the state space, like
parameterized representation of a family of sub-problems. a projection onto a few features.

                                                IJCAI-07
                                                  1024known a priori then the pointer conﬁgurations can be chosen
appropriately while learning. In the absence of prior knowl-
edge the Bayesian algorithm developed in [Ravindran and
Barto, 2003a] can be used to determine the correct bindings to
the schema from among the possible pointer conﬁgurations.
But, the algorithm is not entirely suitable for deictic option
schemas for the following reason.
  The algorithm assumes that the candidate transformations
are not structured and maintains a monolithic weight vector,
wn(·, ·). In the case of deictic option schemas the transfor-
mations are structured and it is advantageous to maintain a
                                  1      2
“factored” weight vector, wn(·, ·)=wn(·, ·),wn(·, ·), ···.
Ideally each component of the weight vector should be the Figure 1: A game domain with interacting adversaries and
likelihood of the corresponding pointer being in the right con- stochastic actions. The task is to collect the black diamond.
ﬁguration. But usually there is a certain degree of dependence The adversaries are of three types—benign (shaded), retriever
among the pointers and the correct conﬁguration of some (white) and delayers (black). See text for more explanation.
pointers might depend on the conﬁguration of other pointers.                              
  Therefore, three cases need to be considered. Assume that   l                   l     
                                                      where PO(s, a, s )=maxν, PO(s, a, s ) , ψ(s) is again
there are only two pointers, i and j, for the following dis- a function of s that captures the features of the states
cussion, but the concepts generalize to arbitrary number of
                                                      necessary to distinguish the particular sub-problem under
pointers.                                                                            l  i    i    i 
                                                      consideration, and K =  hi∈H PO(f (s),gs (a),f (s ))
                                   ∈                         i                               l      
 1. Independent pointers: For every J Di, ρJ satisﬁes wn−1(h  ,ψ(s)) is the normalizing factor. PO(s, a, s ) is
                                                                               
    the homomorphism condition on transition probabilities. a “projection” of PO(s, a, s ) computed as follows. Let
    Then, the right assignment for pointer i is independent J be the set of features that is required in the com-
                                                                   l  i
    of the other pointers and there is one component of the putation of wn(h ,ψ(s)). This is determined as de-
                                                                                              l      
    weight vector corresponding to pointer i and the updates scribed above for the various cases. Then PO(s, a, s )=
                                                                       
    for this components depends only on the features in- Prob(sj |Parents(sj,a)).
    dexedbysomeJ   ∈ Di.                              j∈J

 2. Mutually dependent pointers: For each J ∈ Di and
                                                     5   Experimental Illustration in a Game
    J  ∈ Dj, ρJ × ρJ satisﬁes the homomorphism condi-
    tions. But ρJ and ρJ do not satisfy the homomorphism Environment
                                  
    conditions for some J ∈ Di and J ∈ Dj. Thus, they We now apply a deictic option schema to learning in a mod-
    cannot be treated separately and the composite projec- iﬁed version of the game environment introduced in [Ravin-
    tions given by their cross-products has to be considered. dran and Barto, 2003a]. The layout of the game is shown in
    There is one component of the weight vector that corre- Figure 1. The environment has the usual stochastic gridworld
    sponds to this cross-product projection. The update for dynamics. There is just one room in the world and the goal
    this component will depend on the features indexed by of the agent is to collect the diamond in the room and exit it.
                    
    some J ∈ Di and J ∈ Dj.                           The agent collects a diamond by occupying the same square
                                                     as the diamond. A Boolean variable have indicates posses-
 3. Dependent pointer: For each J ∈ Di and J ∈ Dj, ρJ ×
                                                      sion of the diamond.
    ρJ satisﬁes the homomorphism conditions, as does ρJ .
                                                        The room also has 8 autonomous adversaries. The adver-
    But ρJ does not satisfy the homomorphism conditions
                                                     saries may be of three types—benign, delayer or retriever. If
    for at least some value of J ∈ Dj. This means pointer
    i is an independent pointer, while j is a dependent one. the agent happens to occupy the same square as the delayer it
    There is a separate component of the weight vector that is captured and is prevented from moving for a random num-
    corresponds to pointer j, but whose update depends on ber of time steps determined by a geometric distribution with
                                                     parameter hold. When not occupying the same square, the
    the features indexed by some J ∈ Di and J ∈ Dj.
                                                      delayer pursues the agent with probability chase. The benign
  The weight vector is chosen such that there is one com- robots execute random walks in the room and act as mobile
ponent for each independent pointer, one for each dependent obstacles. The retriever behaves like the benign adversary till
pointer and one for each set of mutually dependent pointers. the agent picks up the diamond. Once the agent picks up the
Let the resulting number of components be L. A modiﬁed diamond, the retriever’s behavior switches to that of the de-
version of the update rule in [Ravindran and Barto, 2003a] is layer. The main difference is that once the retriever occupies
used to update each component l of the weight independently the same square as the agent, the diamond is returned to the
of the updates for the other components:              original position and the retriever reverts to benign behavior.
                                                      The retriever returns to benign behavior if the agent is also
                l (( i( ) i ( ) i( )) · l ( i ( ))   “captured” by the delayer. None of the adversaries leave the
 l ( i  ( )) = PO f  s ,gs a ,f s    wn−1 h ,ψ s
wn h ,ψ s                      K                      room, and thus the agent can “escape” from the room by exit-
                                                (2)   ing to the corridor. The agent is not aware of the types of the

                                                IJCAI-07
                                                  1025Figure 2: The option MDP corresponding to the sub-task get-
object-and-leave-room for the domain in Figure 1. There is
just one delayer and one retriever in this image MDP.

                                                      Figure 3: Average number of steps per episode taken by both
individual adversaries.
                                                      agents for solving the task shown in Figure 1.
  The option MDP (Figure 2) is a symmetrical room with just
two adversaries—a delayer and a retriever with ﬁxed chase
                                                                 1
and hold parameters. The features describing the state space                  Transformation 21
of the option MDP consists of the x and y coordinates relative
to the room of the agent and of the adversaries and a boolean   0.8
variable indicating possession of the diamond. The room in
the world does not match the option MDP exactly and no ad-      0.6    Transformation  16
versary in the world has the same chase and hold parameters
as the adversaries here.                                        0.4
                                                             Transformation  weights

  The deictic agent has access to 2 pointers: a delayer pointer     Transformation  22
that projects one of the adversaries onto the delayer in the im-  0.2
age MDP and a retriever pointer that projects one of the ad-
                                                                 0
versaries onto the retriever in the image MDP. The delayer        0      500     1000    1500    2000
pointer is an independent pointer and the retriever pointer                     Steps
is dependent on the delayer pointer. The sets Ddelayer and
Dretriever are given by the 8 pairs of features describing the Figure 4: Typical evolution of a subset of weights of the
adversary coordinates.                                monolithic agent on the task shown in Figure 1.
  In addition to the pointers the agent also has access to some
background information, such as its own location (which can
be formalized as a self pointer) and whether it has the dia- a single typical run). Figure 5 shows that typically the deic-
mond or not. Note that since the option MDP is an approxi- tic agent identiﬁes the delayer quite rapidly. In fact it takes
mate homomorphic image, the homomorphism conditions are only an average of 52 update steps to identify the delayer,
not strictly met by any of the projections. Therefore, in com- over the 10 independent runs. The monolithic agent takes
puting the weight updates, the inﬂuence of the features not much longer to identify the right transformation (number 21
used in the construction of the image MDP are ignored by in our encoding), as seen from Figure 4. On an average it
marginalizing over them.                              takes around 2007 update steps. As Figure 6 shows, identi-
                                                      fying the retriever is harder and the deictic agent takes about
5.1  Experimental Results                             3050 update steps on an average.
The performance of the deictic agent is compared with a rela- This result is not surprising, since the correct position
tivized agent (monolithic agent) that employs the same option for the retriever depends on position of the delayer pointer.
MDP but chooses from a set H of 64 monolithic transforma- Therefore, while the delayer is being learned, the weights for
tions, formed by the cross product of the 8 conﬁgurations of the retriever receive inconsistent updates and it takes a while
the deictic pointers. Both agents employ hierarchical SMDP for the weights to get back on track. For much of the time we
Q-learning, with the learning rates for the option and the main are interested in only identifying the delayer correctly and
task set to 0.1. The agents are both trained initially in the hence the deictic agent seems to have lower variability in its
option MDP to acquire an approximate initial option policy performance. Overall a single run with the composite agent
that achieves the goal some percentage of the trials, but is not takes around 14 hours of CPU time on a Pentium IV, 3.4 GHz
optimal. Both agents use  greedy exploration. The results machine, while a single run of the deictic agent takes around
reported are averaged over 10 independent runs.       4 hours. Further, the monolithic agent considers all possi-
  On learning trials both agents perform similarly (Figure 3), ble combinations of pointer conﬁgurations simultaneously.
but the monolithic agent has a marginally better initial perfor- Therefore, while it takes fewer update steps to converge to
mance. To understand this we look at the rates at which the the right weights, the deictic agent makes far fewer number
transformation weights converge (Figures 4, 5, and 6 are for of weight updates: 130,000 vs. 85,000 on an average.

                                                IJCAI-07
                                                  1026          1
                      Robot 2                         phases: in the perceptual phase the agent looks around the
                                                      environment to ﬁnd a consistent representation of the under-
         0.8                                          lying state. A consistent representation [Whitehead and Bal-
            Robot  4                                  lard, 1991] of a state is one such that all states that map to
         0.6                                          the representation have the same optimal action-value func-
                                                      tion. In the overt phase the agent picks an action to apply to
         0.4

       Delayer  Weights                               the environment based on the current sensory input. Learning
                                                      takes place in both phases. The Lion algorithm [Whitehead
         0.2   Robot 5                                and Ballard, 1991] is an example of a consistent representa-
                                                      tion algorithm. Here Q-learning is used in the overt phase and
          0                                           a simple learning rule based on one step error information is
           0     20     40    60     80    100
                          Steps                       used to train the sensory phase. If the one step error in the
                                                      Q update rule for a particular conﬁguration is negative then
Figure 5: Typical evolution of a subset of the delayer weights that representation is considered perceptually aliased and is
of the deictic agent on the task shown in Figure 1.   ignored in the future. This simple rule limits the applicability
                                                      of this algorithm to deterministic settings alone.
                                                        If the representation used is a homomorphic image then
          1
                      Robot 5                         it is a consistent representation, as mentioned in [Ravindran
           Robot 0                                    and Barto, 2003b]. By restricting the deﬁnition of deictic op-
         0.8
                                                      tion schema to employ partial homomorphic images as option
                                                      MDPs, it is guaranteed that a consistent representation is al-
         0.6     Robot  1
                                                      ways employed. In the absence of knowledge of the option
                                                      homomorphism, ﬁnding the right transformation to employ
         0.4
       Retriever  Weights                             constitutes the search for a consistent representation and we
                                                      employ Bayesian learning in this phase. As with the Lion
         0.2                                          algorithm, a form of Q-learning is used in the overt phase.

          0
           0     200    400   600    800   1000
                          Steps                       7   Related Work
                                                      Deixis originates from the Greek word deiknynai which
Figure 6: Typical evolution of a subset of the retriever means to show or to point out. It is employed by linguists
weights on the task shown in Figure 1.                to denote the pointing function of certain words, like here
                                                      and that, whose meaning could change depending on the con-
                                                      text. Deixis was introduced to the AI community by Agre.
Comment                                               As mentioned earlier, [Agre and Chapman, 1987] used deic-
The algorithm used above updates the weights for all the tic representations to design an agent, Pengi, that plays the
transformations after each transition in the world. This is arcade game Pengo. Pengi was designed to play the game
possible since the transformations are assumed to be math- from the view point of a human player and hence used visu-
ematical operations and the agent could use different trans- als from a computer screen as input.
formations to project the same transition onto to the option [Whitehead and Ballard, 1991] were the ﬁrst to use de-
MDP. But deictic pointers are often implemented as physi- ictic representations in a RL system, with their Lion algo-
cal sensors. In such cases, this is equivalent to sensing ev- rithm. Unfortunately, the method the Lion algorithm employs
ery adversary in the world before making a move and then to determine consistency works only in deterministic environ-
sensing them after making the move, to gather the data re- ments. [McCallum, 1995] takes a more direct approach to
quired for the updates. Since the weights converge fairly overcoming perceptual aliasing. He employs deixis to solve
rapidly, compared to the convergence of the policy, the time a car driving task and models the problem as a partially ob-
the agent spends “looking around” would be a fraction of the servable MDP. He uses a tree structure, known as U-trees,
total learning time.                                  for representing “states” and identiﬁes the necessary distinc-
                                                      tions so that the resulting representation is consistent. But
6  Perceptual Aliasing and Consistent                 his approach is not divided into explicit perceptual and overt
                                                      phases. There has not been much work on using hierarchical
   Representations                                    RL and deixis. The only work we are aware of is by [Minut
The power of deixis arises from its ability to treat many per- and Mahadevan, 2001]. They develop a selective attention
ceptually distinct states in the same fashion, but it is also the system that searches for a particular object in a room. It oper-
chief difﬁculty in employing deictic representations. This ates by identifying the most salient object in the agent’s visual
phenomenon is known as perceptual aliasing [Whitehead and ﬁeld and shifting its visual ﬁeld to center and focus on that
Ballard, 1991]. One approach to overcome perceptual alias- object. They employ an option to identify the most salient
ing is a class of methods known as Consistent Representation object in the current visual ﬁeld. Though they do not state it
Methods. These methods split the decision making into two thus, this is a “deictic” option, whose effect depends on the

                                                IJCAI-07
                                                  1027