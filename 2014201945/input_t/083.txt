                                                Spectral Learning 

         Sepandar D. Kamvar Dan Klein Christopher D. Manning 
                  SCCM Computer Science Dept. Computer Science Dept. 
           Stanford University Stanford University Stanford University 
       Stanford, CA 94305-9040 Stanford, CA 94305-9040 Stanford, CA 94305-9040 
       sdkamvar@cs.stanford.edu klein@cs.stanford.edu manning@cs.stanford.edu 

                        Abstract 2 The "Interested Reader" Model 

     We present a simple, easily implemented spectral          We propose a Markov chain model similar in spirit to the 
     learning algorithm which applies equally whether          "random surfer" model of [Page et al.% 1998].1 This descrip•
     we have no supervisory information, pairwise link         tion is motivated in the context of text categorization, but the 
     constraints, or labeled examples. In the unsuper•         model depends only on notions of pairwise data similarity 
     vised case, it performs consistently with other spec•     and is completely general. In the model, there is a collection 
     tral clustering algorithms. In the supervised case,       of documents, each of which has some (possibly unknown) 
     our approach achieves high accuracy on the cate•          topic. A reader begins with some document of interest and 
     gorization of thousands of documents given only           continues to read successive documents. When she chooses 
     a few dozen labeled training documents for the 20         the next document to read, she tries to read another document 
     Newsgroups data set. Furthermore, its classifica•         on the same topic, and hence will prefer other documents 
     tion accuracy increases with the addition of unla•        which are similar to her current document. Some mapping 
     beled documents, demonstrating effective use of           between similarities and transition probabilities must be cho•
     unlabeled data. By using normalized affinity ma•          sen; we describe a specific choice in Section 3. 
     trices which are both symmetric and stochastic, we          These transition probabilities define a Markov chain 
     also obtain both a probabilistic interpretation of our    among the documents in the collection. If there exist dis•
     method and certain guarantees of performance.             tinct topic areas in the document set (or, generally, if there 
                                                               are clusters in the data), this Markov chain will be composed 
                                                               of subsets that have high intra-set transition probabilities, and 
 1 Introduction                                                low inter-set transition probabilities. We will refer to these 
                                                               subsets as cliques. Each of the cliques corresponds to a topic 
Spectral algorithms use information contained in the eigen•    in the text clustering problem. 
vectors of a data affinity (i.e., item-item similarity) matrix   Of course, the natural clusters in the data need not be per•
to detect structure. Such an approach has proven effective     fectly compatible with document labels, and we have said 
on many tasks, including information retrieval [Deerwester     nothing about the use of supervision information. In Sec•
et al. , 1990], web search [Page et al, 1998; Kleinberg, 1998], tion 4, we use supervision to override the similarity-based 
image segmentation LMeila and Shi, 2000], word class detec•    transition probilities. For example, we will disallow transition 
tion [Brew and Schulte im Walde, 2002] and data clustering     between two documents which are known to be differently-
[Ng et al., 2002]. But while spectral algorithms have been     labeled, regardless of their pairwise similarity. 
very useful in unsupervised learning (clustering), little work 
has been done in developing spectral algorithms for super•     3 Spectral Clustering Algorithms 
vised learning (classification). 
   In this work, we consider the adaptation of spectral cluster• In this section, we discuss the process of turning an affinity 
ing methods to classification. We first present a method for   matrix A of pairwise document similarities into a normalized 
combining item similarities with supervision information to    Markov transition process N. The eigenvectors of N are then 
produce a Markov transition process between data items. We     used to detect blocks or or near-blocks in N, which will cor•
call this Markov process the "interested reader" model by ap•  respond to clusters of the data. 
peal to the special case of text clustering/classification. Our 
                                                                    Note that there is an important difference between the way these 
algorithm incorporates supervisory information whenever it        1
                                                               two models are used; the random surfer model is used for the first 
is available, either in the form of pairwise constraints or la• left eigenvector of the transition matrix, which indicates the relative 
beled data (or both). Empirically, our algorithm achieves high amount of time the process spends at each data item. On the other 
accuracy when supplied either small amounts of labeled data    hand, we are interested in right eigenvectors of our transition matrix, 
(Section 4) or small numbers of pairwise constraints (Sec•     which more straightforwardly relate to (near-)block structure in the 
tion 5).                                                       transition matrix. 


LEARNING                                                                                                              561                                                                                      Spectral Learning k-means 
                                                                      3 NEWS 0.84 0.20 
                                                                      20 NEWS 0.36 0.07 
                                                                      LYMPHOMA 0.50 0.10 
                                                                      SOYBEAN 0.41 0.34 
                                                                   Table 2: A comparison of Spectral Learning and k-mcans. 

                                                               well, but some of the details differ; our algorithm is shown 
 For clustering: 
                                                               in Figure 1. This algorithm is most similar to the algorithm 
   6. Treating each row of X as a point in Rk, cluster into k clusters presented in [Ng et al, 2002], which we call NJW after its au•
     using k-means or any other sensible clustering algorithm. thors. In fact, the only difference is the type of normalization 
   7. Assign the original point xi, to cluster j if and only if row i of used. There are two differences between our algorithm and 
     X was assigned to cluster j.                              MNCUT from [Meila and Shi, 2001]; the normalization of 
 For classification:                                           A is again different, and, additionally, MNCUT docs not row 
                                                               normalize X (step 5). Table 1 describes the different types of 
  6. Represent each data point i by the row Xi of X. 
                                                               normalizations and mentions some algorithms that use them. 
  7. Classify these rows as points in Rk using any reasonable clas•
     sifier, trained on the labeled points.                      It should be noted that for data sets where there are distant 
                                                               outliers, additive normalization can lead to very poor perfor•
  8. Assign the data point i the class c that X-  was assigned. 
                                       %                       mance. This is because, with additive normalization, the out•
             Figure 1: Spectral Learning Algorithm.            liers become their own clique. Therefore, the clusters will 
                                                               represent outliers rather than true clusters. In a dataset where 
Algorithm Normalization v(A, D)                                there are distant outliers, divisive normalization is likely to 
MNCUT Divisive N = Di1 A                                       lead to better performance. 
NJW Symmetric Divisive N = D /2AD~]/2 
LSA None N = A                                                 3.2 Parameter Selection 
SL Normalized Additive N — (A 4- d            l — D)/d
                                           max        max      The importance of parameter selection is often overlooked 
        Table 1: Normalizations used by spectral methods.      in the presentation of standard spectral clustering methods. 
                                                               With different values of a, the results of spectral clustering 
3.1 Calculating the Transition Matrix                          can be vastly different. In [Ng et al., 2002], the parameter a 
In order to fully specify the data-to-data Markov transition   is chosen based on that value of a that gives the least distorted 
matrix, we must map document similarities to transition prob•  clusters. 
abilities. Let A be the affinity matrix over documents whose     In our text experiments, the data B was a term-document 
elements Aij are the similarities between documents / and      matrix, and the similarity function f gave the pairwise cosine 
                                                               similarities, with an entry A  set to zero if neither i was one 
j. When we are given documents / as points xi,- and a                                     ij
                                                               of the top k nearest-neighbors of j nor the reverse. Threshold•
distance function , a common definition is Aij --
                                                               ing the affinity matrix in this manner is very useful, as spectral 
               where a is a free scale parameter. In LSA       methods empirically work much better when there are zeros 
[Deerwester et al.t 1990], we are given a row-normalized       in the affinity matrix for pairs of items that are not in the same 
term-document matrix B, and A is defined to be B1 B (the       class. For our experiments, we chose k = 20; however, one 
cosine similarity matrix [Salton, 1989]).                      may learn the optimal k in the same manner that [Ng et al., 
   We may map document similarities to transition probabili•   2002] learn the optimal scale factor 
ties in several of ways. We can define [Meila and 
Shi, 2001], where D is the diagonal matrix whose elements      3.3 Empirical Results 
               This corresponds to transitioning with proba•
                                                               We compared the spectral learning algorithm in Figure 1 to 
bility proportional to relative similarity values. Alternatively, 
                                                               k-means on 4 data sets: 
we can define [Fiedler, 1975; 

Chung, 1997], where dmax is the maximum rowsum of A.             • 20 NEWSGROUPS a collection of approximately 1000 
Here, transition probabilities are sensitive to the absolute sim•  postings from each of 20 Usenet newsgroups.2 
ilarity values. For example, if a given document is similar to   • 3 NEWSGROUPS 3 of the 20 newsgroups: sci.crypt, 
very few others, the interested reader may keep reading that       talk.politics.mideast, and soc.religion.christian. 
document repeatedly, rather than move on to another docu•
                                                                 • LYMPHOMA gene expression profiles of 96 normal and 
ment. While either of these normalizations are plausible, we 
                                                                   malignant lymphocyte samples. There are 2 classes: 
chose the latter, since it had slight empirical performance ben•
                                                                   Diffuse Large B-Cell Lymphoma (42 samples), and 
efits for our data. 
                                                                   Non-DLCBL (54 samples) [Alizadeh, 2000]. 
  In [Meila and Shi, 2001], it is shown that a probability 
transition matrix N for a Markov chain with k strong cliques 
                                                                 2From http://www.ai.mit.edu/~jrennie/20Newsgroups/; a total of 
will have k piecewise constant eigenvectors, and they suggest  18828 documents. Documents were stripped of headers, stopwords, 
clustering by finding approximately equal segments in the top  and converted to lowercase. All numbers were discarded. All words 
k eigenvectors. Our algorithm uses this general method as      that occur in more than 150 or less than 2 documents were removed. 


562                                                                                                           LEARNING    • SOYBEAN is the SOYBEAN-LARGH data set from the              1. Define the affinity matrix A as in the previous algo•
     UCI repository. 15 classes.3                                   rithms. 
The results are shown in Table 2. The numbers reported are       2. First, for each pair of points (/, j) that are in the same 
adjusted Rand Index values [Hubert and Arabie, 1985] for            class, assign the values AtJ = Aj, — 1. 
the clusters output by the algorithms. The Rand Index is fre•    3. Likewise, for each pair of points (/', j) that are in differ•
quently used for evaluating clusters, and is based on whether       ent classes, assign the values Aij — Aji — 0. 
pairs are placed in the same or different clusters in two par-   4. 
titionings. The Adjusted Rand Index ranges from — 1 to 1, 
and its key property is that the expected value for a random     This gives us a symmetric Markov matrix describing the 
clustering is 0. The result that spectral methods generally per• "interested reader" process which uses supervisory infor•
form better than k-means is consistent with the results in [Ng mation when present, and data similarities otherwise. A 
et ai, 2002; Brew and Schulte im Walde, 2002]. In some         strength of this model lies in the fact that it incorporates unla•
cases, the poor performance of k-means reflects its inabil•    beled data, whereas the majority of classification models deal 
ity to cope with noise dimensions (especially in the case of   strictly with the labeled data. A benefit of additive normal•
the text data) and highly non-spherical clusters (in the case  ization is that, after the affinities are adjusted, same-labeled 

of the composite negative cluster for LYMPHOMA).4 How•         pairs will always have a higher (or equal) mutual transition 
ever, spectral learning outperforms k-means on the SOYBEAN     probability than unlabeled pairs. This will not necessarily be 
datasct as well, which is a low-dimensional, multi-class data  the case with other normalization schemes. 
set. 
                                                               4.2 A Spectral Classification Algorithm 
4 Spectral Classification                                      Again, if natural classes occur in the data, the Markov 
                                                               chain described above should have cliques. Furthermore, the 
In the previous section, we described clustering a data set by cliques will become stronger as the number of labeled doc•
creating a Markov chain based on the similarities of the data  uments increases. Given this model, we wish to categorize 
items with one another, and analyzing the dominant eigenvec•   documents by assigning them to the appropriate clique in the 
tors of the resulting Markov matrix. In this section, we show  Markov chain. The spectral clustering methods given in Sec•
how to classify a data set by making two changes. First, we    tion 3 can be adapted to do classification by replacing the final 
modify the Markov chain itself by using class labels, when     few steps (clustering in spectral space) with the steps shown 
known, to override the underlying similarities. Second, we     in Figure 1 (which classify in spectral space). 
use a classification algorithm in the spectral space rather than The key differences between the spectral classifier and the 
a clustering algorithm.                                        clustering algorithm are (a) that our transition matrix A incor•
                                                               porates labeling information, and (b) wc use a classifier in the 
4.1 Modifying the "Interested Reader" Model                    spectral space rather than a clustering method. What is novel 
The model described in Section 2 can be modified to incor•     here is that this algorithm is able to classify documents by 
porate labeled data in the following simple manner. If the     the similarity of their transition probabilities to known sub•
interested reader happens to be at a labeled document, the     sets of B. Because the model incorporates both labeled and 
probability that she will choose another labeled document of   unlabeled data, it should improve not only with the addition 
the same category is high, while the probability that she will of labeled data, but also with the addition of unlabeled data. 
choose a labeled document of a different category is low (or   We observe this empirically in Section 4.3. 
zero). Transition probabilities to unlabeled documents are 
still proportional to their similarity to the current source doc• 4.3 Empirical Results 
ument, whether the current document is labeled or not.         Cliques 
   Wc wish to create a Markov matrix that reflects this mod•   It was suggested in Section 4.2 that the Markov chain de•
ified model. We propose doing this in the following manner,    scribed above will have cliques, that is, subsets of nodes in 
using the normalization introduced in Section 3. For most      the graph that are internally fast-mixing, but are not mutu•
similarity functions, the maximum pairwise similarity value    ally fast-mixing. Figure 4 shows the thresholded sparsity pat•
is 1, and the minimum similarity is 0. Therefore, we would     tern for the affinity matrices for the 3 Newgroups data set, as 
like to say that two points in the same class are maximally    labeled data is added. The left matrix is the affinity matrix 
similar, and two points in different classes are minimally sim• for 1% labeled data. Even the underlying similarities show 
ilar:                                                          block-like behavior, if weakly. To the extent that the unla•
                                                               beled data gives a block-like affinity matrix, clusters natu•
   3Thcrc arc has 562 instances, 35 features, and 15 different rally exist in the data; this is the basis for spectral clustering. 
classes. It is nominal; Hamming distance was used.             The subsequent matrices have increasing fractions of data la•
   4For some of these sets, the k-means numbers are low. This  beled. The effect of adding labeled data is to sharpen and 
is partially illusory, due to the zeroed expectation of the adjusted 
                                                               coerce the natural clusters into the desired classes. As more 
Rand index, and partially a real consequence of the sparse high-
dimensionality of the text data. Better k-means results on text typ• labels are added, the blocks become clearer, the cliques be•
ically require some kind of aggressive dimensionality reduction, come stronger, and, in the limit of 100% labeled data, the 
(usually LSA, another spectral method) or careful feature selection interested reader will never accidently jump from a document 
(or both).                                                     of one topic to one of another. 


LEARNING                                                                                                              563                                                                documents to incorporate. 
                                                                  Figure 2(b) shows the effect of supplying increasingly 
                                                               large fractions of the 3 NEWSGROUPS data set as labeled 
                                                               training instances, and using the remainder of the data set 
                                                               as test instances. The spectral classifier outperforms Naive 
                                                               Bayes, more substantially so when there is little labeled data. 
                                                               Figures 3(a) and (b) show the same graphs for the 20 NEWS•
                                                               GROUPS data set. Again, spectral classification performs 
                                                               well, especially when less than 10% of the data is labeled. 
                                                               It should be noticed that, for this data set, Naive Bayes out•
 Figure 2: Categorization accuracy on the 3 NEWSGROUPS task as performs the spectral classifier in the strongly supervised case 
 the number of (a) unlabeled and (b) labeled points increases. In (a), (>15% labeled data). The strength of Spectral Learning lies 
 12 labeled documents and the given number of unlabeled docments in incorporating unlabeled data, and, for the strongly super•
 were used as a training set. In (b), the training set is all of 3 NEWS• vised case, this is of less value. 
 GROUPS, with the given fraction labeled. In both cases, the test set 
 for a given run consisted of all documents in 3 NEWSGROUPS whose 
 labels were not known during training for that run.           Spectral Space 
                                                               To further investigate the behavior of the spectral classi•
                                                               fier, wc performed the following experiment. We took the 
                                                               3 NEWSGROUPS data and labeled various fractions of each 
                                                               of the 3 classes. We then plotted each document's position 
                                                               in the resulting 3-dimensional spectral space (the space of 
                                                               the rows of the matrix X as defined by our spectral learn•
                                                               ing algorithm). Figure 4 shows dimensions 2 and 3 of these 
                                                               plots. With no labels, the data does not tightly cluster. As 
                                                               we add labels (circled points), two things happen. First, the 
                                                               labeled points move close to same-labeled points, away from 
                                                               different-labeled points, and generally towards the outside, 
 Figure 3: Categorization accuracy on the 20 NEWSGROUPS task as since they are "hubs" of the Markov process. Second, they 
 the (a) amount of unlabeled data and (b) fraction of labeled data pull the unlabeled points out radially along with them. This is 
 increases. 
                                                               effective in that it seems to pull the classes apart, even though 
                                                               the classes were not naturally very strong clusters in the un•
 Accuracies 
                                                               labeled data. 
 To validate the utility of spectral classification, we performed 
 the following experiments on the 20 NEWSGROUPS data set. 
   We built two batch classifiers. The first was a standard    4.4 Related Work 
 multinomial Naive Bayes (NB) classifier with standard add-
                                                               In [Yu and Shi, 2001], a spectral grouping method, which 
 one smoothing. The second was a spectral classifier as de•
                                                               they call "Grouping with Bias", is presented that allows for 
 scribed above, which used a single-nearest neighbor classifier 
                                                               top-level bias, as in labeled data. They formulate the problem 
 to perform the final classification in the spectral space. The 
                                                               as a constrained optimization problem, where the optimal par•
 affinity matrix A was the thresholded cosine similarity be•
                                                               tition is sought, subject to the constraint that the normalized 
 tween documents.   We note here that the thresholding is im•
                  5                                            cut values of any two nodes that are preassigned to the same 
portant, since it weakens the effect of outliers. Furthermore, it 
                                                               class should be the same. 
 saves space and computation time, since the resulting affinity 
 matrix is sparse.                                               The main drawback with the algorithm in [Yu and Shi, 
   We split the data into a labeled training set and an unla•  2001] is that it only constrains same-labeled data points. The 
beled test set. For classification, the spectral classifier pro• algorithm we present here benefits from the zeros in the spar-
cessed both the training and test set, but was evaluated on the sity pattern introduced by differently-labeled pairs. Further•
test set only.                                                 more, it should be noted that, in the multi-class case, labeled 
   Figure 2(a) shows the effect of using a sample of 12 docu•  sets combinatorialy tend to embody more differently-labeled 
 ments from the 3 NEWSGROUPS data as a labeled training        pairs than same-labeled pairs. The other drawback to not us•
 set, with an increasing number of unlabeled documents as      ing the information given by differently-labeled points is that 
a test set. The accuracy of the NB classifier is, of course,   the trivial partition (all points in one cluster) will satisfy the 
constant up to sampling variation, since it discards unlabeled constraints, even when many points are labeled. In fact, when 
data. The spectral classifier is more accurate than the NB     all the data is labeled, it is likely that the partition found by 
classifier when given sufficiently many additional unlabeled   the Grouping with Bias algorithm will be the trivial partition. 
                                                               Figure 6(a) shows that our Spectral Classifier outperforms the 

   5For each document, we take the most similar 20 documents, and Grouping with Bias algorithm for the 3 NEWSGROUPS data 
put those similarities in the appropriate row and column. All other set. In fact, Grouping with Bias started performing slightly 
entries are 0.                                                 worse when a large fraction of the data was labeled. 


564                                                                                                           LEARNING  Figure 4: Three classes of the 20 NEWSGROUPS data set in spectral space with increasing mounts of labeled data. The classes are sci.crypt 
 (pluses), talk.politics.midcast (dots), and soc.religion.christian (squares). The labeled points are circled. The bottom graphs show the sparsity 
 patterns of the associated affinity matrices. 


5 Constrained Spectral Clustering 1. Must-links: two items are known to be in the same class. 
                                                                 2. Cannot-links: two items are in different classes. 
Recently, there has been interest in constrained clustering Constrained clustering allows one to do exploratory data 
[Wagstaff and Cardie, 2000; Klein et al, 2002], which in- analysis when one has some prior knowledge, but not class la-
volves clustering with two types of pairwise constraints: bels. The classifier presented in section 4 can be easily modi-


LEARNING                                                                                                             565 