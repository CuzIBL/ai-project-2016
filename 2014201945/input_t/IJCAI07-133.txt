                      Continuous Time Associative Bandit Problems∗
        Andras´  Gyorgy¨ 1 and  Levente Kocsis1   and  Ivett Szabo´ 1,2 and Csaba Szepesvari´ 1,3
               1Computer and Automation Research Institute of the Hungarian Academy
                            of Sciences, Machine Learning Research Group
               gya@szit.bme.hu, kocsis@sztaki.hu, ivett7@gmail.hu, szcsaba@sztaki.hu
            2 Budapest University of Technology and Economics, Department of Stochastics
                       3University of Alberta, Department of Computing Science

                    Abstract                          continuous values, hence the time instants when the decisions
    In this paper we consider an extension of the multi- can take place take continuous values, too. The supercom-
    armed bandit problem. In this generalized setting, puter has a ﬁxed cost of running, whilst the centre’s income
    the decision maker receives some side information, is based on the quality of solutions delivered. At any given
    performs an action chosen from a ﬁnite set and then time only a single task can be executed on the supercomputer.
    receives a reward. Unlike in the standard bandit  Admittedly, this assumption looks absurd at the ﬁrst sight in
    settings, performing an action takes a random pe- the context of our example, however, we think that our results
    riod of time. The environment is assumed to be sta- can be extended to the more general case when the number of
    tionary, stochastic and memoryless. The goal is to algorithms that can run simultaneously is bounded by a con-
    maximize the average reward received in one unit  stant without much trouble. Hence we decided to stick to this
    time, that is, to maximize the average rate of return. simplifying assumption.
    We consider the on-line learning problem where      An allocation rule decides, based on the side information
    the decision maker initially does not know anything available about the task just received, which algorithm to use
    about the environment but must learn about it by  for processing it, the goal being to maximize the return rate.
    trial and error. We propose an “upper conﬁdence   Note that this criterion is different from maximizing the total
    bound”-style algorithm that exploits the structure reward. In fact, since processing a task takes some time dur-
    of the problem. We show that the regret of this al- ing which no other tasks can be processed, the rate maximiza-
    gorithm relative to the optimal algorithm that has tion problem cannot be solved by selecting the algorithm with
    perfect knowledge about the problem grows at the  the highest expected payoff: Some tasks may look so difﬁcult
    optimal logarithmic rate in the number of decisions to solve that the best thing could be to drop them, which re-
    and scales polynomially with the parameters of the sults in no payoff, but in exchange the learner does not suffer
    problem.                                          any loss due to not processing other, possibly more rewarding
                                                      tasks. (Note that this would not be possible without the pres-
                                                      ence of side information; in the latter case the problem would
1  Introduction                                       simplify to the usual multi-armed bandit problem where one
Multi-armed bandit problems ﬁnd applications in various needs to ﬁnd the best option with highest reward rate.) This
ﬁelds, such as statistics, control, learning theory or eco- example illustrates that a learner whose aim is to quickly learn
nomics. They became popular with the seminal paper by a good allocation strategy for rate maximization must solve
Robbins [1952] and since then they enjoy perpetual popular- two problems simultaneously: Predicting the long-term val-
ity.                                                  ues of the available algorithms given the information about
  The version of the bandit problem we consider here is mo- the task to be processed and balancing exploration and ex-
tivated by the following example: Imagine that a sequence ploitation so that the loss due to selecting inferior options
of tasks arrive for processing in a computer center that has (i.e., the regret) is kept at minimum. The problem we con-
a single supercomputer. For each of the tasks a number of sider can be thought of as a minimalistic example where the
alternative algorithms can be applied to. Some information learner faces these two problems simultaneously.
about the tasks is available that can be used to predict which Bandit problems in continuous time have been studied ear-
of the algorithms to try. The processing time depends on the lier by a number of authors (see e.g. [Kaspi and Mandel-
task at hand and also on the algorithm selected and may take baum, 1998; Karoui and Karatzas, 1997] and the references
  ∗This research was supported in part by the Ministry of Econ- therein). These earlier results concern the construction of op-
omy and Transport, Hungary (Grant No. GVOP-3.1.1.-2004-05- timal allocation policies (typically in the form of Gittins in-
0145/3.0), the Hungarian National Science Foundation (Grant No. dexes) given some parametric form of the distributions of the
T047193), and by the J´anos Bolyai Research Scholarship of the random variables involved. In contrast, here we consider the
Hungarian Academy of Sciences.                        agnostic case when no particular parametric form is assumed,

                                                IJCAI-07
                                                   830but the environment is supposed to be stationary and stochas- and
tic. The agnostic (or non-parametric) case has been studied             δi(x)=E  [δi1(x)]
extensively in the discrete time case. In fact, this problem denote the expected reward and delay, respectively, when op-
was ﬁrst considered by Robbins [1952], who introduced a tion i is chosen at the presence of the side-information x.
certainty-equivalence rule with forcing. In the same article The exact protocol of decision making is as follows: De-
Robbins showed that this rule is asymptotically consistent in cision making happens in discrete trials. Let τ0 =0and
the sense that the frequency of the time instants when the best let τt denote the time of the beginning of the t-th trial. At
arm is selected converges to one almost surely. More recently, the beginning of the tth trial the decision maker receives the
       [    ]
Agrawal 1995  suggested a number of simple sample-mean side information xt. Based on the value of xt and all infor-
based policies and showed that the resulting policies’ regret mation received by the decision maker at prior trials, the de-
after n decisions is O(log n). Since it is known that no al- cision maker must select an option It. Upon executing It,
location rule can achieve regret lower than Cp log n for an
                                                      the decision maker receives a reward rt = rIt,t(xt) and suf-
appropriate (problem dependent) constant Cp [Lai and Rob-
                                                      fers a delay δt = δIt,t(xt). That is, the next time point
bins, 1985], Agrawals’ policies are unimprovable apart from available when the decision maker can select an option is
constant factors. Lately, Auer et al. [2002] strengthened
                                                      τt+1 = τt + δIt,t(xt).
these results by suggesting policies that achieve logarithmic The goal of the decision maker is to ﬁnd a good allocation
regret uniformly over time, rather than just asymptotically. policy. Formally, an allocation policy maps possible histories
An added beneﬁt of their policies is that they are simple to to some index in the set A. The gain (average reward rate)
implement.                                            delivered by an allocation policy u is given by
  We base our algorithm on algorithm UCB1 from [Auer et                           
                                                                                E   n    u
al., 2002] (see also [Agrawal, 1995]). We assume a stationary        u            [t=1 rt ]
                                                                    λ  = lim sup    n    u ,
memoryless stochastic environment, where the side informa-                n→∞   E [ t=1 δt ]
tion is an i.i.d. process taking values in a ﬁnite set, the payoff-
                                                            { u}                        { u}
delay sequences are jointly distributed for any of the options where rt is the reward sequence and δt is the delay se-
and their distribution depends on the side information (the quence experienced when policy u is used. An optimal allo-
precise assumptions will be listed in the next section). Like cation policy is one that maximizes this gain. Note that the
UCB1, our algorithm decides which option to choose based problem as stated is a special case of semi-Markov decision
on sample-means corrected by upper conﬁdence bounds. In problems [Puterman, 1994]. The theory of semi-Markov de-
our case, however, separate statistics are kept for all option - cision problems furnishes us with the necessary tools to char-
side-information pairs. Our main result shows that the result- acterize optimal allocation policies: Let us deﬁne the optimal
                                                      gain by
ing policy achieves logarithmic regret uniformly in time and               ∗       u
                                                                          λ  =supλ  .
hence it is also unimprovable, apart from constant factors.                     u
  The paper is organized as follows: We deﬁne the problem                                      ∗     u
and the proposed algorithm in Section 2. Our main result, a A policy u is said to be optimal if it satisﬁes λ = λ .It
logarithmic regret bound on the algorithm’s performance is follows from the generic theory that there exist determinis-
presented in Section 3. Conclusions are drawn in Section 4. tic stationary policies that are optimal. An optimal action for
                                                      some x ∈Xcan be determined by ordering the options by
                                                      their relative values.Therelative value of option i upon ob-
2  The algorithm                                      serving x is the expected reward that can be collected minus
The problem of the previous section is formalized as fol- the expected reward that is not gained during the time it takes
lows: Let K denote the number of options available, and let to collect the reward:
X denote the set of possible values of the side information,          ∗                  ∗
                                                                     qi (x)=ri(x) − δi(x)λ .
which is assumed to be ﬁnite. Let x1,x2,...,xt bearan-
dom sequence of covariates representing the side information Intuitively it should be clear that a policy that always selects
available at the time of the t-th decision, generated indepen- options with best relative values should optimize the over-
dently from a distribution p supported on X . At each deci- all gain. In fact, it follows from the theory of semi-Markov
sion point the decision maker may select an option It from decision problems that this is indeed the case. A stationary
A    {       }
  =   1,...,K  and receives reward rt = rIt ,t(xt),where deterministic policy u : X→Ais optimal if and only if it
rit(xt) is the reward the decision maker would have received obeys the constraints
had it chosen option i. Unlike in classical bandit problems the              ∗                    ∗
                                                           ru(x)(x) − δu(x)(x)λ =max[ri(x) − δi(x)λ ] (1)
collection of the reward takes some random time. When op-                        i∈A
tion i is selected and the side information equals x, this time is
                                                      simultaneously for all x ∈X.
δit(x). We assume that for any ﬁxed x and i, (rit(x),δit(x))
                                                        The (total) regret of an allocation policy is deﬁned as the
is an i.i.d. sequence, independent of {xt}. We further as-
                                                      loss suffered due to not selecting an optimal option in each
sume that rit(x) ∈ [rmin,rmax], δit(x) ∈ [δmin,δmax] with
                                                      time step. Since we are interested in the expected regret only,
δmin > 0. (We expect that the boundedness assumptions can
                                                      our regret deﬁnition uses the optimal gain λ∗:
be relaxed to δit(x) ≥ 0 and appropriate moment conditions
                                                                              n      n
on δit(x) and rit(x).) Let                                                         
                                                                            ∗
                                                                     Rn = λ     δt −    rt.
                         E
                 ri(x)=   [ri1(x)]                                           t=1     t=1

                                                IJCAI-07
                                                   831The value of the ﬁrst term is the maximum reward that could Here ct,s is an appropriate deterministic sequence that is se-
                                                                                                      u
be collected during the time of the ﬁrst n decisions. The ex- lected such that simultaneously for all policies u ∈U, λt is
pected regret thus compares the expected value of the latter                 u
                                                      in the ct,Tu(t)-vicinity of λ with high probability. This se-
with the expected value of the actual total payoffs received. quence will be explicitly constructed during the proof where
It follows that an allocation policy that minimizes the regret we will also make sure that it depends on known quantities
will optimize the rate of return.                     only. In words, λn is the optimal gain that the decision maker
                     X
  When δit(x)=1,and    has a single element, the problem can guarantee itself with high probability given the data seen
reduces to the classical stochastic bandit problem. Since for so far.
the stochastic bandit problems the regret is lower bounded by                         UCB
                                                        Our proposed allocation policy, {ut }, selects the op-
       , we are seeking policies whose regret grows at most     UCB
O(log n)                                              tions It = ut (xt) by the rule
at a logarithmic rate.                                                                             
                                                          UCB                    −
  The idea underlying our algorithm is to develop upper  ut   (x) = argmax  rit(x) δit(x)λt +ˆct,Ti(x,t) ,
                      ∗                                              i∈A
estimates of the values qi (x) with appropriate conﬁdence
bounds. Just like in [Auer et al., 2002], the upper conﬁ- where, similarly to ct,s, cˆt,s is an appropriate deterministic
dence estimates are selected to ensure that for any given x sequence that will be chosen later.
with p(x) > 0 all options are ultimately selected inﬁnitely
often, but at the same time suboptimal options are selected 3Mainresult
increasingly rarely.                                  Our main result is the following bound on the expected regret:
  The algorithm is as follows: Let us consider the t-th deci- Theorem 1 Let the assumptions of the previous section hold
                                 ∗                                                                   UCB
sion. If we had a good estimate λt of λ , then for any given on rit,δit,xt.LetRn be the n-step regret of policy u .
x we could base our decision on the estimates of the relative      ≥
       ∗                                              Then, for all n 1,
                               it   −  it   t                                     
values qi (x) of the options given by r (x) δ (x)λ .Here                       2
rit(x) denotes the average of rewards during the ﬁrst n deci-     ∗          2π
                                                      E [Rn] ≤  L     2+             K|X | +2K|X | log(n)
sions for those time points when the side information is x and            3(|U| +1)2
option i was selected, and δit(x) is deﬁned analogously:                                        

                                                                                      |U|
                       t                                                      a log(n(    +1))
                   1                                               +                     2       ,
                           I                                                        Δi(x)
     rit(x)=                (Is = i, xs = x) rs,                     i:Δi>0 x∈X
                  i
                T (x, t) s=1                                 ∗        ∗
                                                      where L  = δmaxλ −  rmin,
                         t
                                                                      ∗       ∗
                   1                                      Δi(x)=maxqj   (x) − qi (x) ≥ 0,i=1,...,K,
     δit(x)=               I (Is = i, xs = x) δs,                 j∈A
                Ti(x, t) s=1
                                                      and the positive constant a is given by (7) in the proof of the
where Ti(x, t) denotes the number of times option i was theorem.
selected when side information x was present in trials The proof follows similar lines to that of Theorem 1 of [Auer
1, 2,...,t:         t                                et al., 2002], with the main difference being that now we have
                                                                                   ∗
           Ti(x, t)=   I (It = i, xt = x) .           to handle the estimation error of λ . We prove the theorem
                    j=1                               using a series of propositions.
                                                        The ﬁrst proposition bounds the expected regret in terms of
  The plan is to combine appropriate upper bounds on ri(x) the number of times when some suboptimal option is chosen:
and lower bounds on δi(x) based on the respective sample
                                                      Proposition 2 The following bound holds for the expected
averages rit(x), δit(x) and Ti(x, t), to obtain an upper es-
         ∗                                            regret of an arbitrary policy, u =(u1,u2,...):
timate of qi (x). However, in order to have a sample based                                        
                                                                                 n
estimate, we also need an appropriate lower estimate of λ∗.                    
                                                        E     ≤          ∗   E     I       ∈U∗
This estimate is deﬁned as follows:                       [Rn]      p(x)L (x)       (ut(x)     (x)) , (2)
  Let U denote the set of stationary policies: U = {u|u :       x∈X             t=1
                               u
X→A}.Pickanyu        ∈U.Letλt    denote the empirical where
                                                                 ∗             ∗           ∗
estimate of the gain of policy u:                              U  (x)={i ∈A|qi  (x)=maxqj   (x)}
                  t                                                                  j∈A
              u     s=1 I (Is = u(xs)) rs
            λt =  t                                  denotes the set of optimal options at x, and
                       I (Is = u(xs)) δs                           ∗                ∗
                    s=1                                           L (x)=max(δj(x)λ    − rj (x))
                                                                           j
and let Tu(t) denote the number of times when an option
                                                      is the loss for the worst choice at x.Further,byL∗(x) ≤ L∗,
‘compatible’ with policy u was selected:                                                          
                     t                                                        n
                                                       E       ≤    ∗        E     I       ∈U∗
             Tu(t)=     I (Is = u(xs)) .                 [Rn]      L     p(x)       (ut(x)     (x))
                     s=1                                             x∈X       t=1                   
                     ∗                                                     n
Then λt, the estimate of λ is deﬁned by                             ∗    E     I       ∈U∗
                         u                                     =   L             (ut(x)    (x),xt = x) .
              λt =max(λt  − ct,T (t)).                               x∈X    t=1
                   u∈U         u

                                                IJCAI-07
                                                   832                                            ∗
Proof.   Let us consider the t-th term, E [δtλ − rt], Proposition 3 Assume that the following conditions are sat-
                                        ∗
of the expected regret. We   have E [δtλ − rt]=      isﬁed:
           ∗                                                                   u
  i∈A E [(δtλ − rt)I (It = i)] . Using It = ut(xt) and that            u
                                                                     λ    ≥  λt − ct,T (t),           (3)
ut depends only on the past, i.e., if Ft is the sigma algebra of                     u
                                                                       ∗       ∗
 1  1  1     t  t t      t      Ft−1                                      ≤
x ,r ,δ ,...,x ,r ,δ then I = i is   measurable, we                  λ       λt + ct,Tu∗ (t).         (4)
get that
                                                      where the ﬁrst condition is meant to hold for all stationary
        ∗                                                      ∈U
  E [(δtλ − rt)I (It = i)]                            policies u  .Then
       E          ∗ −        I                                       ∗ ≥   ≥   ∗ −
    =    [(δi,t(xt)λ  ri,t(xt)) (It = i)]                           λ    λt   λ   2ct,Tu∗ (t).        (5)
                    ∗                                                                         u
    =  E [E [(δi,t(xt)λ − ri,t(xt))I (It = i) |Ft−1,xt]]                                       −
                                                      Proof.  Let u be the policy that maximizes λt ct,Tu(t).
                             ∗                                                              u
    =  E [I (It = i) E [(δi,t(xt)λ − ri,t(xt))|Ft−1,xt]]                
                                                      Since (3) holds for u , we get that λt = λt − ct,T  ≤
                           ∗                                                                       u (t)
    =  E [I (It = i) E [(δi(xt)λ − ri(xt))|Ft−1,xt]]   u    ∗
                                                      λ   ≤ λ , proving the upper bound for λt. On the other hand,
                         ∗                                                         ∗
    =  E [I (It = i)(δi(xt)λ − ri(xt))] .                                      ≥    −
                                                      because of the choice of u , λt λt ct,Tu∗(t) which can be
                                                                             ∗ −
Now, using again that ut does not depend on xt,weget  further lower bounded by λ 2ct,Tu∗(t) using (4), proving
                                                                                                        
        ∗                                             the lower bound for λt.
   E [δtλ − rt]
                                                       The following proposition shows that λt is indeed a lower
                                 ∗                              ∗
     =      E [I (ut(xt)=i)(δi(xt)λ − ri(xt))]        bound for λ with high probability.
         i∈A
                                                    Proposition 4 Let
                       ∗                                                 
     =   −        p(x)qi (x)E [I (ut(x)=i) |xt = x]                                
           i∈A x∈X                                                         2c1 log(t |U| +1)
                                                                 ct,s =
                       ∗                                                           s
     =   −        p(x)qi (x)E [I (ut(x)=i)] .
           i∈A x∈X                                    where                                        
                                                                                2   2              2
                                                                    (rmax − rmin)  rmax(δmax − δmin)
Then                                                    c1 =2max          2       ,       4           .
                                                                       δmin            δmin
E    ∗ −       −                ∗   E I
 [δtλ   rt]=         p(x)      qi (x) [ (ut(x)=i)]    Then
                 x∈X     i∈U ∗(x)                                                         
                                                          P        ∗ −           P   ∗      ≤  2
                                ∗                              λt <λ    2ct,Tu∗ (t) + λ  < λt     .
               −     p(x)      qi (x)E [I (ut(x)=i)] .                                           t
                 x∈X     i∈U ∗(x)                    Proof. According to Proposition 3, if (3) holds for all station-
                                                                                    ∗        ∗
                                                                                     ≥   t ≥   −  t,T ∗ (t)
                                 ∗                    ary policies u and if (4) holds then λ λ λ 2c u   .
Let wt(i|x)=E [I (ut(x)=i)] if i ∈U(x) and wt(i|x)=0                     ∗                   ∗
                                                      Hence, in order λt <λ − 2ct,T ∗ (t) or λt >λ to hold, we
                   |        |             |                                      u
otherwise, and let μt(i x)= wt(i x)/ j∈A wt(j x).Then must have that one of the conditions in Proposition 3 is vio-
   |   ≥     |                 |  ≤
μt(i x)  wt(i x) (since j∈A wt(j x)  1), the ﬁrst term lated. Using a union bound we get
                                                                                    
of the last expression can be upper bounded by                 ∗                 ∗
                                                      P  λt <λ  − 2ct,T ∗ (t) + P λ < λt
                                                                   u                              
                             ∗                                        u                      ∗
          vt = −     p(x)   qi (x)μt(i|x).              ≤     P   u      −           P   ∗
                                                                 λ  < λt   ct,Tu(t) +   λ <  λt + ct,Tu(t) .
                 x∈X      i                                 u

Since μt(i|x)=0if i is not optimal, μt deﬁnes an optimal Fix u. By the law of total probability,
(stochastic) policy and hence, Bellman’s equation gives t                  t                        
                                              v  =             u                       u
                                                      P   u     −              P   u     −
0. Therefore,                                            λ <  λt  ct,Tu(t) =      λ  < λt  cts,Tu(t)=s  .
                                                                          s=1
     ∗                            ∗
E [δtλ − rt] ≤−       p(x)       q (x)E [I (ut(x)=i)]
                                  i                   Deﬁne
                  x∈X     i∈U ∗(x)
                                                             t                        
                         ∗                              u         I                  u
             ≤      p(x)L (x)       E [I (ut(x)=i)]    rˆt =       (Is = u(xs)) rs,r=       p(x)ru(x)(x)
                x∈X           i∈U ∗(x)                        s=1                       x∈X
                                                               t
                         ∗                ∗                                             
             =      p(x)L (x)E [I (ut(x) ∈U (x))] .    u                            u
                                                       δˆt =      I (Is = u(xs)) δs,δ=      p(x)δu(x)(x).
                x∈X
                                                               s=1                       x∈X
Summing up this last expression over t gives the advertised Using elementary algebra, we get that
                                                                               
bound.                                                      u    u
                                                        P  λ  < λt − cts,Tu(t)=s
  The next statements are used to prove that with high prob-                u      u
                           ∗                              ≤   P (ctsδmin/2 ≤ rˆ /s − r ,Tu(t)=s)
ability λt is a good estimate of λ . Here and in what follows              t                        
                                                 ∗
 ∗                                         ∗    u                 P     2       ≤   u − ˆu
u denotes an arbitrary (ﬁxed) optimal policy and λt = λt .      +   ctsδmin/rmax   δ   δt /s, Tu(t)=s .

                                                IJCAI-07
                                                   833              u      u
Exploiting that rˆt and δˆt are martingale sequences and re- The expectations of the second two terms will be bounded
                                                                                                     
sorting to a slight variant of the Hoeffding-Azuma bound by Proposition 4. The ﬁrst term, multiplied by Zt(s, s ) is
(see, e.g. [Devroye et al., 1996]), we get the bound 2/(|U| + bounded by
  −2                                                          
1)t . Summing over s and u and by an analogous argument                         ∗
            ∗                                                 I         −                  
   P   ∗                                             Zt(s, s ) ri,t−1(x) δi,t−1(x)λ +ˆct−1,s
for   λ < λt + ct,Tu(t) , we get the desired bound.                                                    
                                                                      ∗        ∗       ∗
                                                                   > r   (x) − δ  (x)(λ − 2ct,s)+ˆct−1,s .
  Now we are ready to prove the main theorem. In the                  t−1      t−1
proof we put a superscript ‘∗’ to any quantity that refers to
                 ∗               ∗                    When this expression equals one then at least one of the fol-
the optimal policy u . For example, rt (x)=ru∗(x),t(x), lowing events hold:
 ∗                 ∗
δt (x)=δu∗(x),t(x), T (x, t)=Tu∗(x)(x, t),etc.
                                                      At,s,s =
                                         UCB
Proof of Theorem 1. Proposition 2 applied to u shows    ∗       ∗     ∗   ∗     ∗    ∗            
that it sufﬁces if for any ﬁxed x ∈Xand suboptimal choice {rt−1(x)−δt−1(x)λ ≤r (x)−δ (x)λ −ct−1,s,Zt(s, s )=1},
     ∗
i ∈U(x)  we derive an O(log n) upper bound on the ex-  t,s,s
                                               UCB    B     =
pected number of times choice i would be selected by u                  ∗             ∗             
                                                      {       −          ≥     −                       }
when the side information is x. That is, we need to show ri,t−1(x) δi,t−1(x)λ ri(x) δi(x)λ +ˆct−1,s ,Zt(s, s )=1 ,
                                                                 ∗       ∗    ∗               ∗
                                                       t,s,s {     −           i   −  i         t,s }
         n                                             C     =  r (x)   δ (x)λ <r(x)    δ (x)λ +2ˆc    .
                                                                        ∗
               UCB                                                     −
     E      I ut   (x)=i, xt = x  ≤  O(log n).  (6)   Here ct−1,s =ˆct−1,s 2δt ct−1,s. Now let us give the choices
        t=1                                           for the conﬁdence intervals. Deﬁne
                                                                        
                                                                                       
  Let qit(x)=rit(x) − δit(x)λt. Using the deﬁnition of             uts =  log t  |U| +1   /s.
 UCB      UCB
ut   ,ifut   (x)=i    holds then qit(x)+ˆct,Ti(x,t) >
 ∗                                                                                               √
qt (x)+ˆct,T ∗(x,t). Hence, for any integer A(n, x),  We have already deﬁned cts in Proposition 4: cts = 2c1uts,
                                                      where c1 was deﬁned there, too. We deﬁne cˆts implicitly,
n                                                                      
       UCB                                            through a deﬁnition of cts which is deﬁned so as to keep the
   I  ut  (x)=i   ≤ A(n, x)
                                                      probability of At,s,s small: Let
t=1                                                        
       n
                                                                             2  2                2
            UCB                                        a0 =  8max{(rmax  − rmin) ,rmax(δmax − δmin)/δmin},
    +    I ut   (x)=i, Ti(x, t − 1) ≥ A(n, x),xt = x
                                                                          
      t=1                                                                    2
                                                      cts = a0uts. and a1 = 2δmaxc1.Deﬁne
             n                                  
  ≤             I   UCB             −    ≥                                           2
    A(n, x)+      ut   (x)=i, Ti(x, t 1)   A(n, x) .                     a =(a0 + a1) ,               (7)
             t=1
                                                      and cˆts =(a0 +a1)uts. Using these deﬁnitions we bound the
We write the t-th term in the last sum as follows:    probabilities of the above three events. We start with At,s,s :
                              
   UCB                                                                     ∗      ∗          
I                   −   ≥                             P   t,s,s ≤ P     ≤      −        t
  ut   (x)=i, Ti(x, t 1)   A(n)                         (A    )    (cts/2 r (x)  rt (x),Z (s, s )=1)   
                                                                                ∗
                              ∗                                   P       ∗ ≤       −  ∗         
  =  I qi,t−1(x)+ˆct,T (x,t−1) >qt−1(x)+ˆct−1,T ∗(x,t−1),       +    cts/(2λ )  δt (x) δ (x),Zt(s, s )=1
                    i                                                                      
                                                                       2                2
                                                               ≤ exp  −cts s/(2(rmax − rmin) )
                  Ti(x, t − 1) ≥ A(n)                                                           
                                                                         2      ∗ 2           2
                                                                   −              max −  min
                             ∗                                 +exp    cts s/(2(λ ) (δ    δ   ) )
         I  i,t−1     t−1,s          t−1,s  t
  =        q    (x)+ˆc     >qt−1(x)+ˆc     Z  (s, s ),                            t
  (s,s)∈H(t)                                         Here    we    used   that     s=1 I (It = i, xt = x) rt,
                                                      t
                                                        s=1 I (It = i, xt = x) δt are martingales for any x, i,
where                                                 and the above-mentioned variant of the Hoeffding-Azuma
                                                                                          
                                                    inequality. Plugging in the deﬁnition of cts we get that the
   H(t)={(s, s    )|1 ≤ s ≤ t − 1,A(t) ≤ s ≤ t − 1},                                        −4        −2
                                                      probability of event At,s,s is bounded by 2t (|U| +1) .
            I       −         ∗    −
Zt(s, s )=    (Ti(x, t 1) = s ,T (x, t 1) = s) .      The probability of Bt,s,s can be bounded in the same way
                                                                                        
                                                     and by the same expression since cˆts >cts. Therefore
Fix any s, s ∈ H(t). Using the deﬁnition of qit(x),
                                                        n    
                      ∗
I  qi,t−1(x)+ˆct−1,s >qt−1(x)+ˆct−1,s                                [P (At,s,s )+P (Bt,s,s )]
       
                                                          t=1 (s,s)∈H(t)
   ≤  I ri,t−1(x) − δi,t−1(x)λt−1 +ˆct−1,s
                                                                n                             2
                                                                                 4           2π
                  ∗        ∗                                ≤                           ≤           .
            >    rt−1(x) − δt−1(x)λt−1 +ˆct−1,s,                             4 |U|    2     |U|    2
                                                               t=1 (s,s)∈H(t) t ( +1)   3(   +1)
         ∗           ∗
           ≥  t−1 ≥    −                                                                
        λ    λ      λ    2ct−1,Tu∗ (t−1)                                                               2
                                                    Moreover, deﬁne A(t, x)=a log(t( |U| +1))/Δi(x) .
                  ∗                     ∗
       I           −                 I                                                                 
      +   λt−1 <λ    2ct−1,Tu∗ (t−1) + λ  < λt−1 .    Now, if Ct,s,s holds then one must have Δi(x) > 2ˆct,s ,

                                                IJCAI-07
                                                   834