             Speaker-Invariant Features for Automatic Speech Recognition

                               S. Umesh, D. R. Sanand and G. Praveen
                                  Department of Electrical Engineering
                                     Indian Institute of Technology
                                        Kanpur 208 016, INDIA
                                {sumesh,drsanand,gpraveen}@iitk.ac.in


                                                          8
                                                        x 10
                    Abstract                            6
                                                                                            Male Speaker
    In this paper, we consider the generation of features          Smoothed Spectra for /aa/ Female Speaker

    for automatic speech recognition (ASR) that are ro- 5
    bust to speaker-variations. One of the major causes
    for the degradation in the performance of ASR sys-
    tems is due to inter-speaker variations. These vari- 4
    ations are commonly modeled by a pure scaling
    relation between spectra of speakers enunciating
    the same sound. Therefore, current state-of-the art 3
    ASR systems overcome this problem of speaker-

    variability by doing a brute-force search for the op- 2
    timal scaling parameter. This procedure known as
    vocal-tract length normalization (VTLN) is compu-
    tationally intensive. We have recently used Scale-  1
    Transform (a variation of Mellin transform) to gen-
    erate features which are robust to speaker variations
                                                        0
    without the need to search for the scaling param-   0      50      100    150     200     250    300
    eter. However, these features have poorer perfor-
    mance due to loss of phase information. In this      Figure 1: Spectra of 2 speakers for one “frame” of /aa/
    paper, we propose to use the magnitude of Scale-
    Transform and a pre-computed “phase”-vector for
    each phoneme to generate speaker-invariant fea-   there is a lot of interest in improving the performance of SI
    tures. We compare the performance of the proposed systems by accounting for inter-speaker variability.
    features with conventional VTLN on a phoneme        In general, two speakers enunciating the same sound have
    recognition task.                                 very different pressure waveforms and the resulting spectra
                                                      are very different. Fig. 1 shows the smoothed spectra (after
                                                      smoothing out pitch) for two speakers enunciating the vowel
1  Introduction                                       /aa/. As seen from the ﬁgure the two spectra are very differ-
Inter-speaker variation is a major cause for the degradation ent even though it is the same vowel spoken by two speak-
in the performance of Automatic Speech Recognition (ASR). ers. Since ASR systems use features extracted from the spec-
This is especially important in speaker-independent (SI) ASR tra, the features are themselves different for the same sound
systems which are typically built to handle speech from any enunciated by two speakers. Therefore for an automated sys-
arbitrary unknown speaker, e.g. in applications such as direc- tem to recognize these different features as belonging to the
tory assistance. As a rule of thumb speaker-dependent (SD) same sound is difﬁcult leading to a degradation in perfor-
ASR systems have half the error rates when compared to SI mance. Note that there will be a certain amount of variability
systems for the same task. This difference in performance for the same sound spoken by the same person, i.e. intra-
suggests that the performance of SI systems can be signif- speaker variability, which is handled by the statistical model.
icantly improved if we can account for inter-speaker varia- However, in general inter-speaker variability is substantially
tions. Note that in practice, we cannot directly build SD sys- larger than intra-speaker variability, leading to “coarse” sta-
tems for large vocabulary speech recognition tasks, since the tistical models and hence increased confusion between the
speech data required from that speaker will be too enormous sound classes.
even for a co-operative user. Instead, we build SI systems There is a lot of research that is being done in trying to
and then “adapt” the system for a particular speaker to make understand the mathematical relationship between spectra of
it speaker-dependent. Hence, irrespective of the application, two speakers enunciating the same sound. This has impor-

                                                IJCAI-07
                                                  1738tant implications in other areas apart from speech recogni- SpeechSignal
tion, such as in vowel perception, hearing-aid design and
speech pathology. One of the major causes for inter-speaker
variations is attributed to the physiological differences in the Preemphasis           Nonuniform
vocal-tracts of the speakers. Based on this idea, in ASR a
commonly used model to describe the relation between spec-    and                         DFT
tra of two speakers enunciating the same sound is given by Windowing                  (Log spaced)
                 SA(f)=SB(αf)                   (1)

where SA(f) and SB(f) are the spectra of speakers A and B
respectively; and α denotes the uniform (constant) frequency- Subframing
warp factor. We refer to the above model as linear-scaling                            Logmagnitude
model, since the two spectra are just scaled versions of each and
other. The motivation for linear scaling comes from the fact Hamming
that to a first-order approximation, the vocal tract shape can Windowing
be assumed to be a tube of uniform cross-section and for this                              IDFT
simplifying approximation, the resonant frequencies (which
characterize different sounds) are inversely proportional to
vocal-tract length ([Wakita, 1977]). Therefore, differences in Averaged                  Magnitude
vocal-tract lengths manifest in scaling of resonant frequencies Autocorrelation
with the scaling being inversely proportional to the vocal-tract
length.                                                   Estimation
  In practical ASR systems, we have access only to the                                    STCC
acoustic data from the speaker and therefore do not have any
idea of the speaker’s vocal-tract length. Therefore, we do a Figure 2: Block diagram for the computation of STCC
brute-force search for the optimal scaling-factor, α by trying
                   α
out different values of . The optimality criterion is based of frequency. For such a model, log-warping the frequency-
on maximizing the likelihood with respect to the SI Hid- axis of the spectra of speakers separates the linear scaling fac-
den Markov Model (HMM). Since in ASR we use features  tor as a translation factor in log-warped spectral domain, i.e.,
derived from the spectra, for each value of α we scale the
frequency-spectra appropriately and then compute the fea-       s (λ)=S   (f = eλ)=S   (αeλ)
                             α                                  a       A            B
tures. Therefore for each value of , we recompute the fea-                  λ+ln α
         ith         Xα                                              = SB(e      )=sb(λ +lnα).        (3)
ture of the utterance, i , for that scale-factor. We then
ﬁnd the optimal scale-factor by                       Hence, in the log-warped domain, λ, the speaker-dependent
             α              Xα|λ, W                                                              α
             ˆi =argmaxPr(    i     i)          (2)   scale-factor separates out as a translation factor ln for the
                      α                               linear scaling model of Eq. 1.
where α represents scale factor. λ is the HMM model and                                             s  λ
W                                                       Let the Fourier-transform of the log-warped spectra a( )
  i is the transcription of the utterance. Due to physiological   D   c           s λ              D   c
                                        α             be denoted as a( ) and that of b( ) be denoted as b( ),
constraints of the human vocal-tract apparatus, lies in the i.e.
range of 0.80 to 1.20. Therefore, in state-of-the-art ASR sys-
                                            α                     DFT                    DFT
tems, we compute the features for different values of from   sa(λ) ⇐⇒ Da(c)andsb(λ)      ⇐⇒  Db(c)    (4)
0.80 to 1.20 in steps of 0.02 and ﬁnd the optimal α using
Eq. 2. This is a computationally expensive process since for Using the property of the Fourier-transform that a translation
every frame the features have to be computed 21 times corre- in one domain appears only as linear-phase in the other do-
sponding to different α, before ﬁnding the optimal feature for main we have,
that frame after a maximum-likelihood (ML) search. Hence,            D   c   D   c e−jc ln α
a lot of research is being done to have alternate normalization        a( )=   b( )                   (5)
schemes that are computationally efﬁcient.            Therefore, the speaker-dependent term α appears only in the
  Recently, an alternate method for generating features that phase term. Hence the magnitudes of Da(c) and Db(c) are
are invariant to speaker-variations due to the linear scaling identical and independent of the speaker-variations i.e.
(see Eq. 1) was proposed [Umesh et al., 1999].Inthis
method, the features are insensitive to the scale-factor α and         |Da(c)| = |Db(c)|.             (6)
hence there is no need to search for the optimal α.Wede-
scribe the method in the next section.                We can therefore use the magnitude as speaker-invariant fea-
                                                      tures for ASR.
2  Review of Scale-Invariant Features using             We can show that the above procedure is equivalent to tak-
                                                      ing the Scale-Transform [Cohen, 1993] (a special case of
   Scale-Transform
                                                      Mellin transform) of SA(f) and SB(f) andthenusingthe
In the linear-scaling model of Eq. 1, the speaker-dependent magnitude of Scale-Transform as features in ASR. In this pa-
warp-factor, α is a multiplicative constant and is independent per, we refer to these features as Scale-Transform Cepstral

                                                IJCAI-07
                                                  1739 0.06                                                                      STCC     MFCC
                                           S (f)
                       Original Spectrum    A                                 .        .
                                           S (f)=S (α f)            Acc%    56 98    62 14
 0.04                                       B A AB

 0.02                                                 Table 1: Accuracy of the phoneme recognizer using STCC
                                                      and MFCC
  0
   0    10   20   30    40   50   60    70   80   90
                                                f
 0.06                                                 out skips over states. Two mixture Gaussians with full co-
                                         S (λ)
                      Log Warped Spectrum A
                                              λ+lnα   variance matrices per state were used. The features vec-
                                         S (λ)=S (e AB)
 0.04                                     B  A        tors are of 39 dimensions comprising normalized log-energy,
                                                      c1...c12(excluding c0) and their ﬁrst and second order deriva-
 0.02                                                 tives. Conventional MFCC features are computed as de-
                                                               [                ]
  0                                                   scribed in Lee and Rose, 1998 while the STCC features are
   0      50      100     150     200     250     300 computed as described in [Umesh et al., 1999] and a block
                                              λ=log(f)
10000                                                 diagram is shown in Fig. 2.
                        STCC Features          data1
                                               data2    The classiﬁcation performance between STCC and the
                                                      MFCC features are shown in Table 1. As seen from the Table,
5000                                                  STCC features have a lower classiﬁcation performance when
                                                      compared to MFCC features. Since STCC provides speaker-
  0                                                   invariant features, it should have provided improved normal-
   0      50      100     150     200     250     300
                                                      ization performance when compared to MFCC. The reason
                                                      for the degradation is the complete loss of phase-information.
Figure 3: Figure shows that linear-scaling of original spec-
                                                      We discuss about the loss of phase information in Section 3
tra, results in their being shifted versions in λ = log(f) do-
                                                      and propose new features to overcome this loss of information
main. Therefore, the magnitude of the subsequent Fourier-
                                                      by introducing an average phase vector which is discussed in
transform (STCC) are identical.
                                                      detail in Section 4.

                                                      3   Drawback of STCC – loss of phase
Coefﬁcients (STCC). The steps to compute STCC are shown   information
in Fig. 2.                                            Theoretically, STCC provides speaker-normalization by ex-
  Fig. 3 illustrates the idea of using Scale-Transform to ob- ploiting the fact that the magnitude of STCC are identical as
tain speaker-invariant features for the linear-scaling model of seen in Eq. 6. However, note that Da(c) and Db(c) are com-
Eq.1 using a synthetic example. Fig. 3(a) shows synthetic plex quantities with associated magnitude and phase. Let
spectra of two speakers enunciating the same sound that are
                                                                                  jφ(c)
exactly scaled versions of each other corresponding to the         Da(c)=|Da(c)|e        and          (7)
linear-scaling model of Eq.1. In Fig. 3(b) we show the same
                                                                                jφ(c) −j ln αAB c
spectra in the frequency-warped(i.e. log-warped) domain. As      Db(c)=|Da(c)|e     e        .        (8)
seen from the ﬁgure, the frequency-warped spectra are ex-
                                                      Therefore |Da(c)| = |Db(c)|. However, we have completely
actly shifted versions of each other as expected from Eq. 3. lost the phase information ejφ(c) which is also important for
Finally, we take the Fourier-transform of these frequency- discrimination of phonemes. This loss in phase information
warped spectra and then take the magnitude of the Fourier- degrades the performance of the STCC and even though it is
transform to get the STCC features. These are shown in insensitive to speaker-variations, the ﬁnal performance is in-
Fig. 3(c). As seen from the ﬁgure, the STCC features are ferior to the MFCC. In the next section, we propose a method
identical, and are therefore invariant to speaker variations that to overcome this problem and help improve the performance
are present in the original spectra as seen in Fig. 3(a). of the STCC.
  Since the linear scaling model of Eq. 1 is only a crude ap-
proximation, recently [Umesh et al., 2002] have proposed the
use of mel-scale for frequency-warping and not the log-scale. 4 Proposed Improvement over STCC features
With this mel-scale frequency-warping they claim better re- As seen in the previous section, the loss in phase information
sults [Sinha and Umesh, 2002]. Therefore, for the purposes ejφ(c) leads to degradation in performance when compared to
of this paper, we will use mel-scale warping to compute the MFCC features even though STCC features are invariant to
STCC features.                                        speakers as seen in Eq. 6. As seen in Eq. 8, Da(c),Db(c)
  We now compare the performance of conventional MFCC contain both the magnitude and phase information and the
features with STCC features. TIMIT database was used for speaker-speciﬁc factor, α, appears only in the phase term.
the experiments and we considered the classiﬁcation of 42 Hence retaining the complete phase will result in no normal-
monophones. The train and test set consist of 155015 and ization since the ln α will also be present. On the other hand,
56424 monophone occurrences for adult male and female taking only the magnitude will result in the loss of ejφ(c) in-
speakers respectively. The HMM models consisted of 3 emit- formation which provides additional information for discrim-
ting and 2 non-emitting states, with left-to-right and with- ination between vowels.

                                                IJCAI-07
                                                  1740  In this section, we propose a method to incorporate the 0.06
                ejφ(c)                         α                                                   No Shift
phase information    but not the speaker-speciﬁc ln in 0.04                                        Left Shift
the STCC features. The basic idea of this approach is to esti-                                     Right Shift
mate an “average” phase vector for each phoneme using train- 0.02
ing data from all speakers.                             0
  As previously discussed, for the linear-scaling model, the 0  50      100     150     200     250     300
                                                      0.06
speaker differences manifest themselves as speaker-speciﬁc                      Original Left Shift Vs Reconstructed 
shifts in the log-warped domain as seen in Fig. 3. The STCC 0.04

exploits this fact by considering the magnitude of the sub- 0.02
sequent Fourier-transform but in the process loses the phase
                                                        0
information completely. Our approach is to estimate the av- 0   50      100     150     200     250     300
erage phase for each phoneme from the training data and use 0.06
this same phase-vector for every occurrence of that phoneme                           Original Vs Reconstructed 
irrespective of the speaker. For example, for the phoneme 0.04
/ae/, our average-phase STCC features with acronym AP- 0.02
                                    ( )
STCC will be of the type |D (c)|ejφavg ae c . We will ﬁrst
                         a                              0
illustrate this idea through a synthetic example that is dis- 0 50      100     150     200     250     300
cussed below.                                         0.06
                                                                               Original Right Shift Vs Reconstructed 
  Consider that a particular phoneme is enunciated by 0.04
three different speakers and the corresponding spectra af-
ter frequency-warping are shifted versions of one another as 0.02
shown in Fig. 4(a). The three spectra in the Fourier-domain 0
will differ only in a linear-phase term and this can be seen 0  50      100     150     200     250     300
mathematically as:
                                                      Figure 4: Figure shows our proposed method of normaliza-
             DFT                    ( )
      s (λ)  ⇐⇒    D (c)=|D   (c)|ejφ c         (9)   tion using average phase. The top ﬁgure shows the shifted
       a            a        a                        spectra from different speakers in the frequency-warped do-
             DFT         −jτ1c         jφ(c) −jτ1c
 sa(λ − τ1)  ⇐⇒    Da(c)e     = |Da(c)|e   e          main. Using the average phase and the magnitude of the DFT

             DFT         +jτ2c         jφ(c) +jτ2c    we then reconstruct the spectra (shown in solid line).
 sa(λ + τ2)  ⇐⇒    Da(c)e     = |Da(c)|e   e
  If we now compute the average phase of all the three step, we will assume knowledge of appropriate phase vec-
phases, i.e.                                          tor and test the efﬁcacy of this method of normalization and
                                                      compare it with the conventional method of normalization.
                φ(c)+(φ(c) −  τ1c)+(φ(c)+τ2c)
    φavg (c)=                                  (10)
                               3                      5   Performance of the Proposed Normalization
                      (τ2 − τ1)c
            =   φ(c)+          ,                          Method
                          3
                                                      In this section, we will compare the performance of the pro-
                    φ    c
then the average-phase avg( ) is same as the phase of the posed normalization scheme with conventional normalization
phoneme, φ(c), with an additional linear-phase term that cor- scheme.
responds to the average of all the three shifts. Therefore, the Here we considered the classiﬁcation of 8 “most confus-
average phase φavg(c) preserves the phase of the phoneme able” vowels in contrast to the classiﬁcation of 42 mono-
with an additional term corresponding to the average shift of phones used for MFCC and STCC feature vectors discussed
the spectra of the training speakers. If we now use this av- in section.2. The main reason was to study the effect of
erage phase for all speakers along with the magnitude of the our proposed phase-estimation procedure in more detail. The
STCC feature, we get speaker-normalized features. This can vowels were extracted from TIMIT database, where the train
                                |D  c |
be easily seen by recalling the fact that a( ) is constant for and test set consisted of 22686 and 8265 utterances respec-
all speakers; and when it is multiplied by the same phase term tively. The HMM models consisted of 3 emitting and 2
     ( )
ejφavg c for all speakers, then the resulting features are same non-emitting states, with left-to-right and without skips over
for all speakers. This is shown in Fig. 4.            states. We used single mixture Gaussian with diagonal vari-
  In practice, since the processing is done in the discrete- ance for each state. The feature vectors are of 26 dimen-
domain, the phases are 2π wrapped, and hence we ﬁrst un- sions comprising normalized log-energy, c1...c12(excluding
wrap the phases before averaging. The other important practi- c0) and their ﬁrst order derivatives.
cal limitation at this point is the fact that we need to know the The conventional normalization scheme is based on Eq. 2,
phoneme aprioriso that we can add the appropriate average- which involves ML estimation of the warp-factor α.This
phase of that phone to the magnitude vector |Da(c)|.We method is also popularly known as Vocal-tract length normal-
are now working on practical algorithms to ﬁnd the appro- ization (VTLN). As discussed in the introduction, we do a
priate phase for a given frame without having the knowledge brute-force search for the optimal α by computing the MFCC
of which phoneme the frame came from. However, as a ﬁrst feature for each α by appropriately scaling the mel ﬁlter-bank

                                                IJCAI-07
                                                  1741    Condition  No Norm.      Norm.       Norm.        brute-force search for optimal features and are therefore
                Baseline  Recog. Trans. True Trans.   computationally very expensive. Recently a method has
  VTLN-MFCC      60.99       61.17       69.69        been proposed that uses a special transform called Scale-
    AP-STCC      60.42       36.58       99.36        Transform to obtain speaker-invariant features. Since the
                                                      Scale-Transform uses only the magnitude of the features to
Table 2: % of Accuracy for VTLN-MFCC and AP-STCC
                                                      obtain speaker-invariance, there is a loss of discriminabil-
speaker normalization methods for classiﬁcation of 8 vow-
                                                      ity between phonemes due to the loss of phase information.
els in TIMIT test set. First column is without normalization.
                                                      In our proposed method, we use the average-phase of the
In the second column we have used recognition output of ﬁrst
                                                      corresponding phoneme in place of the lost phase informa-
column as transcription for the normalization, while the third
                                                      tion of STCC. We are currently working on various distance
column shows the normalization performance when the true
                                                      measures to ﬁnd the appropriate average-phase for normal-
transcription is known.
                                                      ization. Using vowel classiﬁcation experiments and F-ratio
                                                      measures we show that if we have optimal average-phase in-
and choosing the feature that maximizes the likelihood with formation then the proposed method provides excellent nor-
respect to the statistical HMM model and the given transcrip- malization performance when compared to the conventional
tion Wi. Note that although the method needs transcription, VTLN method.
there is graceful degradation when there are errors in the tran-
scription.                                            7   Acknowledgments
  In our proposed method, we are still working on methods This work was supported in part by funding from the Dept.
to ﬁnd the optimal phase-vector to multiply a given frame of Science & Technology, Government of India under project
of speech. As seen from Table 2 if we are given exact tran- No. SR/S3/EECE/0008/2006-SERC-Engg.
scription, then we can multiply by the correct average-phase
vectors and the performance is exceptional. On the other
hand, even with exact transcription the VTLN method is far References
inferior to the proposed method of using AP-STCC features. [Cohen, 1993] L. Cohen. The Scale Representation. IEEE
But, if we use the recognition output of the baseline recog- Trans. Signal Processing, 41:3275–3292, Dec. 1993.
nizer (i.e. without normalization) as the transcription then the [Lee and Rose, 1998] L. Lee and R. Rose. Frequency Warp-
VTLN normalization method degrades gracefully while the  ing Approach to Speaker Normalization. IEEE Trans.
AP-STCC completely falls apart – it is worse than even base- Speech Audio Processing, 6:49–59, Jan. 1998.
line. Note that we do not need to know the transcription for
                                                      [                    ]
AP-STCC, but a method of ﬁnding “optimal” phase-average Sinha and Umesh, 2002 Rohit Sinha and S. Umesh. Non-
vector for each utterance. We are working on various distance Uniform Scaling Based Speaker Normalization. In Proc.
measures to obtain an “optimal” solution. However, it is im- of Int. Conf. on Acoustics, Speech, and Signal Processing,
portant to note that at least under ideal conditions (when true volume 1, pages 589–592, 2002.
transcription is known) our method shows that it is possible to [Umesh et al., 1999] S. Umesh, L. Cohen, N. Marinovic, and
remove almost all the speaker-variability which conventional D. Nelson. Scale Transform in Speech Analysis. IEEE
VTLN-MFCC is never able to achieve.                      Trans. Speech Audio Processing, Jan. 1999.
  Another approach to measure the normalization perfor- [Umesh et al., 2002] S. Umesh, L. Cohen, and D. Nelson.
mance is to measure the separability of the vowel models Frequency Warping and the Mel Scale. IEEE Signal Pro-
using the various normalization schemes. One good mea-   cessing Letters, 9(3):104–107, March 2002.
sure of separability is the F-ratio between models consid-
ered pair-wise. Tables 3,4,5,6 shows the F-ratio for the un- [Wakita, 1977] H. Wakita. Normalization of Vowels by Vo-
normalized features, the VTLN features and proposed AP-  cal Tract Length and its Application to Vowel Identiﬁ-
STCC features. These tables show F-ratio of models that are cation. IEEE Trans. Acoust., Speech, Signal Processing,
built using training data for which true transcription is always ASSP-25(2):183–192, Apr. 1977.
known. The F-ratio shows the separability between models
and the higher the number the better. Note that since MFCC
and STCC are computed slightly differently the features are
slightly different and hence there are small differences in per-
formance in the un-normalized case. From the Table, it can
be seen that the separability between the vowels models are
excellent for the AP-STCC features.

6  Conclusion & Discussion
In this paper, we have proposed a method of obtain-
ing speaker-invariant features for automatic speech recog-
nition. Our method is motivated by the fact that conven-
tional methods reduce inter-speaker variability by doing a

                                                IJCAI-07
                                                  1742