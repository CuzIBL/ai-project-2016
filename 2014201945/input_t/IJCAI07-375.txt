         Adaptive Genetic Algorithm with Mutation and Crossover Matrices

                                    Nga Lam Law and K.Y. Szeto
                           Hong Kong University of Science and Technology
                                         Department of Physics
                               Clear Water Bay, Hong Kong SAR, China
                                 Corresponding author: phszeto@ust.hk

                    Abstract                          formalism without crossover. Section 3 focuses on the imple-
                                                      mentation of the crossover matrix. A brief explanation of the
    A  matrix formulation for an adaptive genetic     test problem - Ising spin glass is given in section 4, and the
    algorithm is developed using mutation matrix      test results on Ising spin glass are discussed in section 5.
    and crossover matrix. Selection, mutation, and
    crossover are all parameter-free in the sense that
    the problem at a particular stage of evolution will 2 Mutation Matrix
    choose the parameters automatically. This time de-
    pendent selection process was ﬁrst developed in   Traditional genetic algorithm can be considered as a special
    MOGA (mutation only genetic algorithm) [Szeto     case of the mutation only genetic algorithm (MOGA). For
    and Zhang, 2005] and now is extended to include   a population of NP chromosomes, each encoded by L locus,
    crossover. The remaining parameters needed are    can be arranged into a population matrix A with NP rows and
    population size and chromosome length. The adap-  L columns. Each element of the matrix A has a probability p
    tive behavior is based on locus statistics and ﬁt- (0 <p<1)  to undergo mutation. These mutation probabili-
                                                             ( =1        ;  =1      )
    ness ranking of chromosomes. In crossover, two    ties Mij i   , ..., NP j , ..., L form a mutation matrix
    methods are introduced: Long Hamming Distance     M: Mij is the mutation probability of Aij . In matrix A, each
    Crossover (LHDC) and Short Hamming Distance       row is one chromosome. The rows are listed in decreasing
                                                                         ≥
    Crossover (SHDC). LHDC emphasizes exploration     order of the ﬁtness: fi fj if i<j.
    of solution space. SHDC emphasizes exploitation     In traditional genetic algorithm, the best N1 = c1 × NP
    of local search process. The one-dimensional ran- chromosomes survive (0 <c1 < 1). These survivors produce
    dom coupling Ising Spin Glass problem, which is   N2  = c2 × NP children to replace the less ﬁt ones through
    similar to a knapsack problem, is used as a bench- crossover and/or mutation (0 <c2 < 1 − c1). The remaining
    mark test for the comparison of various realiza-  N3 =(1−   c1 − c2)× NP unﬁt chromosomes are replaced by
    tions of the adaptive genetic algorithms. Our results the same number of randomly generated candidates in order
    show that LHDC is better than SHDC, but both are  to ensure population diversity. Thus the traditional genetic
    superior to MOGA, which has been shown to be      algorithm can be described by a mutation matrix which has
    better than many traditional methods.             Mij =0for the ﬁrst N1 rows; Mij = m(0 <m<1)  for the
                                                      next N2 rows; Mij =1for all remaining rows. In traditional
                                                      genetic algorithm, we must input three time independent pa-
1  Introduction                                       rameters: c1, c2 and m.
The notion of mutation matrix is used in the development of MOGA makes use of the information of the ﬁtness rank-
adaptive genetic algorithm so that the selection process does ing and loci statistics to determine Mij dynamically. It out-
not require any external input parameter. Numerical exam- performs traditional genetic algorithm in terms of both speed
ples on knapsack problems indicate that this new adaptive ge- and quality of solution. In this formalism, the mutation ma-
netic algorithm, called MOGA (mutation only genetic algo- trix is nearly the same as the population matrix A, but with
rithm) is superior to many traditional methods [Ma and Szeto, the loci arranged according to the standard deviation σ(j) for
2004; Szeto and Zhang, 2005; Zhang and Szeto, 2005].The the allele distribution. If there are V allele values possible for
idea behind MOGA is that mutation probability is a function the locus, (in ordinary binary string representation of genetic
of time, ﬁtness ranking of the chromosome, and locus. The algorithm, the possible value of an allele is either 0 or 1, so V
dependence is automatically determined by the statistics of = 2), then we deﬁne the standard deviation σ(j) for the allele
the locus and the ﬁtness ranking of the chromosome at the distribution as:
given time. The traditional genetic algorithm is a special case     
                                                                    
of MOGA in which all the mutation probabilities are ﬁxed.               NP            2
                                                                         =1(Aij − h(j)) × C(f(i))
In this paper, we extend the basic idea behind MOGA to in-    σ(j)=      i                           (1)
                                                                                NP   ( ( ))
clude the crossover process. Section 2 reviews the MOGA                         i=1 C f i

                                                IJCAI-07
                                                  2330where                                                 enough    : (   )=1−     1 ≈ 1,e.g.  (  )=095   for
                                                             NP  α NP         NP         α NP      .
                           NP
                        1                             NP  =20.
                h(j)=         A  .              (2)
                       N        ij                      Next, we employ the ﬁtness cumulative probability again
                         P i=1
                                                      to determine the number Nmg of loci to undergo mutation
Here, C(f(i)) is the ﬁtness cumulative probability of chromo- for the selected chromosomes, each with L loci. Stating the
some i, which, as well as ﬁtness, can be considered as an in- formula for Nmg,
formative measure of chromosome i. High ﬁtness cumulative
probability indicates high informative measure. The allele              Nmg  = α(i) × L.              (5)
standard deviation σ(j) for each loci can also be regarded as
an informative measure. A locus which has a smaller allele It is obvious that Nmg is also a function of the row index
standard deviation contains more useful information than the i, or in GA language, the ﬁtness. If a selected chromosome
other loci.                                           has a high ﬁtness, it has fewer loci to undergo mutation. If
  At the beginning of the algorithm, chromosomes are ran- a selected chromosome has a low ﬁtness, it has more loci to
domly generated. Alleles are randomly distributed. Thus the mutate. Finally, we mutate the Nmg leftmost loci located in
allele standard deviation is not so informative. After genera- the selected rows in the mutation matrix, i.e. the Nmg least
tions of evolution, chromosomes acquire higher ﬁtness, good informative loci of each chromosome. In this way, most of
chromosome structures emerge. Then more chromosomes   the informative loci remain and guide the evolution process
evolve to acquire the good structures. Therefore, a locus with to the next generation, while most of the less informative loci
high structural information has a small allele standard devia- mutate until they contain more information.
tion.
                              C(f(i))
  The presence of the factor of NP ( ( )) in the allele 3 Crossover Based on Hamming Distance
                               i=1 C f i
standard deviation is to take account of the informative use- With the implementation of the mutation matrix, the
fulness of each chromosome. Loci in chromosomes with high crossover is operated implicitly on the population. The im-
ﬁtness generally are more informative than those in chromo- plicit crossover deploys the chromosome ﬁtness ranking and
somes with low ﬁtness. With this factor, the allele standard the loci statistics in search of solution. Besides these two
deviation more appropriately reﬂect the amount of informa- kinds of information, the Hamming distance can also be
tion contained in a particular locus. Our cumulative probabil- used to search the solution. Through the introduction of a
ity C(f(i)) is deﬁned by                              crossover matrix, two different explicit crossover methods
                         1                           which employ the information of the Hamming distance are
              C(f(i)) =        N(g).            (3)   implemented and compared.
                        NP                                                               ×
                           g≤f                          The distance matrix H is a square NP NP matrix with
                                                      matrix element H  deﬁned by a distance measure between
N(g) is the number of chromosomes with ﬁtness value g. Note          ii              −→
                                                      the i-th chromosome, represented by R i which is the i-th row
that C(f) is a non-decreasing function of f where C(fmax)=
       1                                              vector of the population matrix A, and the i’-th chromosome
1 and (  ≤ C(f) ≤  1).                                −→            −→  −→
      NP                                                      =  (       )
  Therefore, the mutation matrix can be viewed as an ar- R i : Hii D R i, R i . In this paper, we have two differ-
                                                      ent measures for H. The ﬁrst one is called long Hamming dis-
rangement of genes based on the informative measures: the                                  L
                                                      tance crossover (LHDC) in which  =    =    ,where
ﬁtness cumulative probability C(f(i)) and the standard de-                        Hii    Hii   Dii
                                                         
viation σ(j) for the allele distribution. The higher the row Dii is equal to the number of different alleles in these two
                                                      rows. The second measure is called short Hamming distance
is, the more useful information the loci in this row contains.                     S
                                                      crossover (SHDC), with  =     =  −     . We see that
Similarly, the closer to the right side the locus is, the more            Hii    Hii   L   Dii
information it contains. Our mutation is to make use of these in LHDC, the smaller the distance between two chromosomes
two informative measures through the mutation matrix. i and i’ is, the more similar they are. While in SHDC, it is just
  In each generation, rows (i.e. chromosomes) in the muta- the opposite: smaller distance indicates low similarity. Note
tion matrix were ﬁrst selected for mutation based on the ﬁt- that the matrix H is a symmetric matrix. The diagonal entries
                                                      of H are zero in LHDC, but equal L in SHDC.
ness cumulative probability. The probability α(i) to choose
the ith row for mutation is deﬁned as,                  After deﬁning the matrix H, we have to select two chro-
                                                      mosomes for crossover. The probabilities to choose the two
                α(i)=1−   C(f(i)).              (4)   chromosomes are different, but they are both related to H.
                                                      One probability is just the ﬁtness cumulative probability, and
Therefore, a chromosome with higher ﬁtness has a lower                                     ( )
probability to participate in mutation, or in other words, it we call it the ﬁrst crossover probability PCI i .
has higher ability to survive. As the chromosomes are kept
                                                                       PCI(i)=C(f(i)).                (6)
in a decreasing order of the ﬁtness: fi ≥ fj if i<j,inthe
matrix A; α(i) is a non-decreasing function of the row index A chromosome, which we call the ﬁrst chromosome, is ﬁrst
(chromosome index) i. Thus, α(i) is higher for less ﬁt rows chosen with this ﬁrst crossover probability. As the row index
(chromosome). The ﬁttest chromosome, i.e. the ﬁrst row of in H is also the row index in A, chromosome represented by
the population matrix A, α(i)=0. It is, hence, never se- smaller H row index has higher probability to be selected for
lected for mutation. The worst chromosome, i.e. the last row crossover. Then, the second chromosome is selected with an-
of A, will mutate with a probability close to unity for large other probability called the second crossover probability. The

                                                IJCAI-07
                                                  2331second crossover probability depends on the chromosome al-   Nloci sample   MOGA     LHDC    SHDC
ready chosen. If chromosome i (i.e. chromosome with H row     10      1       20       6       6
index i), is the ﬁrst chromosome, then the second crossover                   (0)     (0)     (0)
               
probability PCII(i ) to choose chromosome i’ as the second            2       20       7       7
chromosome is:                                                                (0)     (0)     (0)
                                                              15      1      146      14       14
                           H  
               P   (i )=     ii   .            (7)                           (0)     (0)     (0)
                CII         NP
                            k=1 Hik                                   2      318      49       65
                                                                              (0)     (0)     (0)
  If the ﬁrst and the second chromosome are the same, the
                                                              20      1      2456     52       47
second one is chosen again until they are both different. Both
                                                                              (0)     (0)     (0)
LHDC and SHDC are single-point crossovers with the cross-
                                                                      2      1483     159     197
ing points chosen randomly once in a generation. In each
                                                                              (0)     (0)     (0)
generation, a total of N chromosomes are selected to partic-
                   P                                          25      1     41274     171     194
ipate in the crossover. There is no limitation on the number of
                                                                              (1)     (0)     (0)
times a chromosome can take part in crossover in one gener-
                                                                      2     31144     586     669
ation. Later, the ﬁttest N chromosomes are selected to form
                    P                                                         (0)     (0)     (0)
a new generation from the pool of N original chromosomes
                              P                               30      1     123621   1724    1956
and the NP children chromosomes produced by crossover.
  In each generation, we ﬁrst carry out crossover and then                    (0)     (0)     (0)
                                                                      2     123460   7610    11863
mutation. At the end of crossover, i.e. when the ﬁttest NP
chromosomes are selected, as well as at the end of the muta-                 (280)    (0)     (0)
tion, we update the ﬁtness and cumulative ﬁtness probability
                                                      Table 1: Average generation and number of failure in 400
for all the chromosomes in the population.
                                                      samples to reach solution using the MOGA, LHDC and
                                                      SHDC. The number of failure is written in the round brackets.
4  Application: One-Dimensional Ising Spin
   Glass                                              generation and number of successful searches. Moreover
The algorithm is applied to ﬁnd the minimum energy of a 1D LHDC gives the best performance.
periodic Ising model consisting of Nloci spins (Nloci = 10, 11, The two GAs with crossover is better due to the informa-
15, 20, 25 and 30). Each spin can be either up or down, rep- tion exchange between chromosomes. In LHDC, ﬁt chromo-
resented by ’1’, or ’-1’ respectively. The interaction energy some is selected and crossover with a very different chro-
between a pair of neighbouring Ising spin is randomly gener- mosome to generate two new chromosomes. This expands
ated in the range [-1,1]. The problem is to ﬁnd the minimum the exploration space and minimizes the chance of prema-
of the energy                                         ture convergence. In SHDC, ﬁt chromosome is selected and
                                                      crossover with a similar one. This exploits the local search
                   Nloci−1
                                                      process and speeds up convergence. The results suggest that
              E =        J  +1s s +1            (8)
                          j,j  j j                    a second crossover probability emphasizing the exploration
                     =1
                    j                                 of search space is better for the Ising-spin-glass-like problem
for a given set of interaction energy . For each value of (i.e. knapsack-type problem)[Kellerer et al., 2004].
Nloci, two different sets of interaction energy were gener-
ated. For each set of interaction energy, the exact minimum 6Conclusion
energy Emin was ﬁrst computed by exhaustive search over
2Nloci spin conﬁgurations. The program is then executed 400 Three different ways to implement the adaptive genetic al-
                                                      gorithm are tested and compared using the one-dimensional
times with a ﬁxed population size of NP =20chromosomes.
Each execution is regarded as one sample. An execution stops Ising model for spin glass as the testing problem. The re-
when an acceptable solution is found or the maximum al- sults show that LHDC is best and that all three methods
                                                      are parameter-free. Since MOGA  outperforms other al-
lowed generations NMA = 250000 is reached. An acceptable
solution is a candidate solution which is within 1% tolerance gorithms, including dynamic programming [Kellerer et al.,
                                                      2004; Simoes and Costa, 2001; 1999], for solving the knap-
of the exact minimum energy Emin. When no solution can be
                                                      sack problem, our test shows the superiority of LHDC in this
found within NMA generations in an execution, the execution
is considered as a failure sample.                    kind of search problem. This work also provides a matrix for-
                                                      mulation of parameter free adaptive genetic algorithm with
5  Results                                            both mutation and crossover.
We have compared the efﬁciency of three adaptive genetic al- Acknowledgments
gorithms (MOGA, LHDC and SHDC) on the spin glass prob-
lem. Table 1 summarizes the average generation to reach so- N.L.Law and K.Y. Szeto acknowledge interesting discussion
lution and the number of failure. Both GAs with crossover, from C.W. Ma. K.Y. Szeto acknowledges the support of RGC
LHDC and SHDC, outperform MOGA in terms of average    grant 603203.

                                                IJCAI-07
                                                  2332References
[Kellerer et al., 2004] Hans Kellerer, Ulrich Pferschy, and
  David Pisinger.  Knapsack Problems, pages 22–26.
  Springer, 2004.
[Ma and Szeto, 2004] Chun Wai Ma and Kwok Yip Szeto.
  Locus oriented adaptive genetic algorithm: Application
  to the zero/one knapsack problem. In Proceeding of The
  5th International Conference on Recent Advances in Soft
  Computing, RASC2004, pages 410–415, Nottingham, UK,
  December 2004.
[Simoes and Costa, 1999] A. B. Simoes and E. Costa. Trans-
  position versus crossover: An empirical study. In Proceed-
  ing of the Genetic and Evolutionary Computation Confer-
  ence. Morgan Kaufmann, 1999.
[Simoes and Costa, 2001] A. B. Simoes and E. Costa. An
  evolutionary approach to the zero/one knapsack problem:
  Testing ideas from biology. In Proceeding of Fifth Inter-
  national Conference on Aritical Neurakl Network and Ge-
  netic Algorithms, 2001.
[Szeto and Zhang, 2005] Kwok Yip Szeto and J. Zhang.
  Adaptive genetic algorithm and quasi-parallel genetic al-
  gorithm: Application to low-dimensional physics. In Lec-
  ture Notes in Computer Science 3743, pages 186–196, So-
  zopol, June 2005. Springer-Verlag.
[Zhang and Szeto, 2005] Jian Zhang and Kwok Yip Szeto.
  Mutation matrix in evolutionary computation: An appli-
  cation to resource allocation problem. In Lecture Notes
  in Computer Science 3612, pages 112–119, Changsha,
  China, August 2005. Springer-Verlag.


                                                IJCAI-07
                                                  2333