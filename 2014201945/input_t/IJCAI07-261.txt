               Using Ontologies and the Web to Learn Lexical Semantics

                       Aarti Gupta                                   Tim Oates
                     Amazon.com Inc.                  University of Maryland Baltimore County
                  1200 12th Avenue South                         1000 Hilltop Circle
                    Seattle, WA 98144                           Baltimore, MD 21784
                   aartig@amazon.com                             oates@cs.umbc.edu

                    Abstract                          components that most signiﬁcantly impact performance on
                                                      the task of statistics-based extension of symbolic knowledge
    A variety of text processing tasks require or bene- for language processing tasks.
    ﬁt from semantic resources such as ontologies and
                                                        Lexical acquisition methods use different sources of data
    lexicons. Creating these resources manually is te-
                                                      for learning. Historically, a static corpus has been used for
    dious, time consuming, and prone to error. We
                                                      training purposes (Widdows & Dorow 2002). Recently, the
    present a new algorithm for using the web to de-
                                                      web has become a popular alternative. We compare our al-
    termine the correct concept in an existing ontology
                                                      gorithm to that of Cilibrasi and Vitanyi (Cilibrasi & Vitanyi
    to lexicalize previously unknown words, such as
                                                      2004), which uses the web to extract the meaning of words
    might be discovered while processing texts. A de-
                                                      and phrases.
    tailed empirical comparison of our algorithm with
                                                        Our work is related to work on learning ontologies. Lin
    two existing algorithms (Cilibrasi & Vitanyi 2004,
                                                      (Lin 1998) constructs an ontology from scratch by identify-
    Maedche et al. 2002) is described, leading to in-
                                                      ing similar words in a parsed corpus. Agirre et al. (Agirre et
    sights into the sources of the algorithms’ strengths
                                                      al. 2000) use the web to enrich existing concepts in ontolo-
    and weaknesses.
                                                      gies. Each concept in WordNet (Fellbaum 1998) is associ-
                                                      ated with a topic signature (lists of topically related words).
1  Introduction                                       To identify the sense of a new word, they build a binary hi-
Semantic resources, such as ontologies and lexicons, are erarchical clustering of all possible senses, compare the new
widely used in Natural Language Processing (NLP). Auto- word’s context with the topic signatures for each branch and,
mated text processing and understanding, word sense dis- similar to a decision tree, choose the highest ranking branch
ambiguation, information extraction, speech recognition, and at each step. Maedche et al. (Maedche et al. 2002) deter-
machine translation all require lexical resources. Ontologies mine the class of a new word by employing tree-descending
are used in information retrieval for automatic query expan- and tree-ascending classiﬁcation algorithms. Our algorithm
sion, and the word sense disambiguation task can be made determines which concept in an existing ontology is the best
easier by domain-speciﬁc ontologies that reﬂect the distribu- candidate for the meaning of a new word via tree descending,
tion of word senses in a particular domain. Outside the realm and is compared to the algorithm of Maedche et al.
of NLP, ontologies are widely used in the Semantic Web to
allow computer programs to “understand” web content.  2   Methods
  Acquiring knowledge by hand is extremely costly and time
consuming. Construction of ontologies such as Word Net This section describes the proposed approach for learning lex-
(Fellbaum 1998), Cyc (Lenat 1995), and EDR (Yokoi 1995) ical semantics and summarizes two existing methods that are
required enormous investments in both time and money. In used for comparison.
addition, hand-crafted lexical resources are prone to human
errors, biases, and inconsistencies in judgment.      2.1  Proposed Approach
  We propose a novel method for learning lexical semantics. The OntoSem ontology consists of concepts and taxonomic
The ontological concept that best captures the lexical seman- relations between concepts. To enrich existing concepts in the
tics of a new word is determined using statistical analysis of ontology with new words, we need to ﬁnd in the hierarchy of
web pages containing the word. All experiments reported in concepts the concept that best approximates the meaning of
this paper use the OntoSem ontology (Nirenburg & Raskin a new word. This is accomplished using the notion of word
2004), a large ontology and knowledge representation system similarity, for which we use Latent Semantic Analysis (LSA)
for language understanding tasks developed by the Institute (Deerwester et al. 1990).
for Language and Information Technologies (ILIT). We per- To decide the most appropriate concept that a new word
form a detailed comparison of the proposed method with two should belong to, we need to determine the similarity of the
existing methods, and in the process identify the individual new word with words already lexicalized with the ontology.

                                                IJCAI-07
                                                  1618To do this, we obtain a training set T of words from the ontol- probability p(x, y) is the number of web pages returned by
ogy. For each word W ,whereW is in T or is the new word, Google, containing both the search term x and the search
document W.txt is created by tokenising the list of deﬁnitions term y divided by the overall number of web pages returned
retrieved after querying Google with “deﬁne:W ”. The result- by Google. The conditional probability p(x|y) is deﬁned as
ing document collection is then passed as input to the LSA p(x, y)/p(y).
algorithm. Using the World Wide Web to dynamically build We attempt to use the Google method to extend the On-
a document collection relevant to a given set of words allows toSem ontology. Once again, we use the tree descending al-
ad hoc word learning.                                 gorithm to assign a new word to a concept. For each child
  Words are stopped using Smart’s stop list, stemmed us- of the ontology node we currently are at, we randomly pick a
ing Porter’s stemming algorithm, and ﬁltered based on the representative set of words. We then compute the distance be-
logarithm of their term frequencies. A word i with term fre- tween the new word and every word extracted from the onto-
quency freqi is included in the term-document matrix only if logical subtrees by submitting queries to Google via the Java
1+log(freqi) ≥ t. Rather than represent raw term frequen- URL interface and scraping the page counts from the web
cies of the remaining content words in the term-document pages returned. The decision of which path to follow down
matrix, we use TFIDF.                                 the tree is made by choosing the child concept that subsumes
  We treat the OntoSem ontology similar to a decision tree. a word which is closest to the new word by the distance mea-
Starting at the root, we attempt to descend the ontology tree sure.
until we reach a leaf concept. At every tree node, the decision
of which path to follow is made by choosing the child concept 2.3 Category Based Tree Descending Algorithm
that is most similar to the new word. To compute similarity, Maedche et al. (Maedche et al. 2002) use a category-based
for each child of the ontology node we currently are at, we method to determine the child concept that is most similar to
randomly pick a representative set of words that belong to the a new word. For the ontology node that they are currently
subtree rooted at the child.                          at, they gather all hyponyms at most 3 levels below. Then
  We then obtain deﬁnitions for the words in the representa- then build a generalized class vector for each child concept
tive sets and the word we are trying to learn. For every word, by adding up all the vector representations of hyponyms gath-
we create a new document that contains a list of deﬁnitions ered that belong to the subtree rooted at the child and normal-
for the word. So, at the end, we have a document collection ize the resulting vector to unit length. Similarity between the
that contains a document for every word that we extracted unit vector of a new word and the class vectors is measured
from the various ontological sub-trees and a document for the by means of three different similarity metrics: Jaccard’s coef-
new word. We then invoke the LSA algorithm which gives us ﬁcient, L1 distance, and skew divergence. The child concept
word and document vectors in the reduced space. We com- whose class vector is most similar to the new word’s vector is
pute cosine similarity scores for the word vectors. Finally, the chosen as the next node in the path down the ontology tree.
1-nearest neighbor classiﬁer assigns the new word to the child The utility of using the category-based method is that be-
concept that subsumes the word that was most similar to the sides reducing computational cost, it summarizes the charac-
new word. The process is repeated until the search reaches a teristics of each class by combining multiple prevalent fea-
leaf node.                                            tures together, even if these features are not simultaneously
2.2  Automatic Meaning Discovery using Google         present in a single vector.
                                                        We use the Maedche method to extend the OntoSem ontol-
This subsection describes a method proposed by Cilibrasi and
                                                      ogy. At each ontology node, we gather all hyponyms at most
Vitanyi (Cilibrasi & Vitanyi 2004) to automatically learn the
                                                      three levels below, build a term-document matrix for the rel-
meaning of words and phrases. Intuitively, the approach is
                                                      evant document collection obtained from Google Deﬁne, ob-
as follows. Words that are semantically similar will co-occur
                                                      tain the normalized class vectors and use the L1 similarity
more than words that are semantically unrelated. Conditional
                                                      metric to choose the child concept that is closest to the new
probabilities can be used as a measure of the co-occurrence
                                                      word.
of terms. p(x|y) gives the probability of term x accompany-
ing term y while p(y|x) gives the probability of term y ac-
companying term x. These two probabilities are asymmetric. 3 Experiments
The semantic distance between terms x and y can be obtained This section describes the experiments performed in the pro-
by taking the negative logarithm of conditional probabilities cess of identifying the components that work best for the
   |         |
p(x y) and p(y x), selecting the one that is maximum and statistics-based reﬁnement of taxonomic relations. It also
normalizing it by the maximum of log 1/p(x), log 1/p(y). describes the experiments conducted to evaluate the perfor-
                           1        1
                 max{log p(x|y) , log p(y|x) }        mance of the proposed LSA-based tree descending method
                                         (1)
       D(x, y)=       {     1      1 }                against the Google method and the category-based tree de-
                  max  log p(x) , log p(y)            scending method.
On the Web, probabilities of term co-occurrence can be ex- Since document W.txt is created by obtaining context for
pressed by Google page counts. The probability of term x word W , similarity between two words W1 and W2 can
is given by the number of web pages returned when Google be measured by computing similarity between documents
is presented with search term x divided by the overall num- W1.txt and W2.txt. In all experiments, we compute simi-
ber of web pages M possibly returned by Google. The joint larity between document vectors and not word vectors be-

                                                IJCAI-07
                                                  1619cause we empirically found that using document vectors gave
marginally better results than word vectors.
  For all experiments, we performed leave-one-out cross-
validation, that is, one word was held back as the test word
and the remaining words constituted the training set. Per-
formance was measured in terms of the number of words cor-
rectly classiﬁed. A word is correctly classiﬁed if it is assigned
to the correct child concept of an ontology node.
  The value of t, that is, the threshold for ﬁltering out low
frequency words was ﬁxed at t=1.6, so words that occurred
fewer than two times in the corpus were ﬁltered out.

3.1  Impact of corpus on the learning task
We wanted to gauge the impact of the corpus on the LSA-
based tree descending algorithm. To do this, we randomly Figure 1: Performance of the LSA-based method using the
picked at most ﬁfty words from each of the subtrees rooted at WSJ corpus and Google Deﬁne corpus
the 18 child concepts of the SOCIAL-ROLE node in the On-
toSem ontology, giving a total of 269 words (after removing      Node       Words Classiﬁed Correctly
polysemous words).                                          SOCIAL-ROLE              2/254
  We used the Wall Street Journal corpus to obtain contexts.
For each word W , document W.txt was created by extract-      MAMMAL                 1/347
ing a window of width seven words centered on each of the      ANIMATE              103/208
ﬁrst 40 occurences of W in the corpus. We then replaced        OBJECT                17/282
the WSJ document collection with the Google Deﬁne docu-
ment collection, while keeping everything else the same. For Table 1: Results obtained for the category-based method us-
each word W , document W.txt was created by submitting to ing the L1 metric
Google the query “deﬁne:W ”.
  The results of using the Google Deﬁne corpus vs. the WSJ
corpus are shown in Figure 1. Accuracies for both corpora are show that it gives better results than Jaccard’s coefﬁcient and
calculated as the number of words correctly classiﬁed over skew divergence.
the total number of words N that could possibly be classiﬁed We carried out experiments at four nodes in the Ontology
correctly. For the WSJ corpus, N=202 and for the Google tree. For each node, we created a relevant document col-
deﬁne corpus, N=254. The discrepancy between N and the lection by picking all words at most three levels below the
number of words originally extracted from the ontology is node and Googling for the words’ deﬁnitions. We measured
due to the fact that not all words occur in the WSJ corpus or similarity between the normalized category-based vector rep-
have deﬁnitions on the Web, or if they do, they occur fewer resentations obtained by summing individual document vec-
times than the threshold frequency.                   tors. Table 1 shows the results obtained. Each cell in the table
  It can be seen from Figure 5 that over all values of k, shows the number of words classiﬁed correctly out of the total
the number of singular values retained when running LSA, number of words that could possibly be classiﬁed correctly.
the system performs signiﬁcantly better when fed with the Analysis of the results shows that in almost all cases, a
Google Deﬁne corpus than when it is fed with the WSJ cor- new word was found to be similar to the child concept that in
pus. The Google Deﬁne corpus has a best accuracy of 64.9% comparison to the other child concepts subsumed the fewest
for k=65 and k=85 while the WSJ corpus has a best accuracy number of words. For example, at the SOCIAL-ROLE node,
of 22.2% at k=65.                                     a vast majority of words were found to be similar to the
  Thus, the Google Deﬁne corpus dramatically improves the CELEBRITY concept which had just one word under it,
performance of the system because it contains fewer extra- while at the MAMMAL node, most words were found simi-
neous and irrelevant words that could potentially distract the lar to the concept FRESHWATER-MAMMAL which had just
LSA algorithm.                                        one word under it. This observation corroborates the ﬁndings
                                                      of Maedche et al. in which they observed that most words
3.2  Impact of Similarity metric                      were assigned to concepts that subsumed few words.
                                                        Given two words and their length n vectors, x and y,the
Maedche et al. make use of three different similarity met- L1 distance between them is given by:
rics: Jaccard’s coefﬁcient, L1 distance, and skew divergence
                                                                                n
in their category-based tree descending algorithm. We show                         |  −   |
that the performance of the category-based tree descending          Dist(x, y)=     xi  yi
algorithm can be signiﬁcantly improved by using the cosine                      i=0
similarity metric.                                    From the deﬁnition, we can see that the distance between x
  Initially, we implemented the category-based method using and y will be small if the difference between xi and yi is small
the L1 similarity metric because Maedche et al. empirically for all i. Thismeansthat,ifx was a document vector for a

                                                IJCAI-07
                                                  1620           Node       Words Classiﬁed Correctly            Node        Baseline  LSA     Google  Category
      SOCIAL-ROLE             177/254                      ALL          33.33    53.72   49.72     87.07
        MAMMAL                322/347                     OBJECT        20.00    59.74   48.48     78.72
        ANIMATE               186/208                    PHYS-OBJ       50.00    74.64   68.54     93.64
         OBJECT               222/282                    ANIMATE        33.33    85.19   80.42     88.30
                                                         ANIMAL         50.00    83.30   74.60     93.30
Table 2: Results obtained for the category-based method us- VERTEBRATE  16.66    81.71   61.22     92.38
ing the cosine similarity metric
                                                         MAMMAL         12.50    78.65   58.00     94.96
                                                         PRIMATE        50.00    93.68   94.76    100.00
new word and y was a generalized class document vector,
                                                          HUMAN        100.00   100.00   100.00   100.00
the distance between them would be small if two conditions
were satisﬁed. First, the documents for x and y contained SOCIAL-ROLE   5.55     64.16   37.50     69.60
the same words and second, the documents for y as a whole
contained few other words. Child concepts that subsume few Table 3: Average accuracy (%) of the LSA-based tree de-
words in the ontology tree are represented by few documents scending method, the Google method and the category-based
which means fewer words in y as a whole and thus for such tree descending method
concepts, by default, the second condition is satisﬁed.
  We replaced the L1 metric with the cosine similarity met- Google method
ric, and keeping everything else the same, carried out the ex- As with the LSA-based tree descending method, for every
periments at the four ontology nodes again. Table 2 shows node on the path, we randomly picked at most 50 words from
the results of using the cosine similarity metric. It can be each of the subtrees rooted at the child concepts of the node.
seen that the cosine similarity metric does signiﬁcantly better Thus, at each node we had a different set of words for which
than the L1 metric at all four nodes.                 we measured the performance of the Google method. The
                                                      experiments were carried out 10 times at each node, each time
3.3  Evaluation of the Three Methods                  with a different set of words.
This section describes the experiments carried out to evaluate Category-based tree descending method
the performance of the LSA-based tree descending algorithm,
                                                      We carried out the experiments for the category-based tree
the Google method and the category-based tree descending
                                                      descending method proposed by Maedche et al., except that
algorithm.
                                                      we replaced the metrics used by them with the cosine similar-
  We picked a path in the ontology from the root to   ity metric. For every node on the path, we gathered all words
a leaf node and carried out experiments at every node at most three levels below from each of the subtrees rooted
on this path. Thus, we carried out experiments at the at the child concepts of the node. Since there was no random
ALL, OBJECT, PHYSICAL-OBJECT, ANIMATE, ANI-           element involved, we carried out the experiments just once at
MAL, VERTEBRATE, MAMMAL, PRIMATE, HUMAN,              each node.
and SOCIAL-ROLE nodes. The goal was to evaluate the per-
formance of the three methods based on the number of words Results
that were classiﬁed correctly at each node on the path. A word To have a benchmark for the performance of the three meth-
is classiﬁed correctly if it is assigned to the correct child con- ods, a baseline was calculated, which was the number of
cept.                                                 words classiﬁed correctly when a new word was randomly
                                                      assigned to a child concept. Since the probability of getting a
LSA-based tree descending method                      word correct by chance depends on the number of child con-
For every ontology node on the path, we randomly picked at cepts of a given node, the baseline varies with the number of
most 50 words from each of the subtrees rooted at the child child concepts of a node.
concepts of the node. So, at the ALL node, we would have Table 3 shows the results for the three methods. Each cell
words picked from the OBJECT, EVENT and PROPERTY      shows the percentage accuracy for a given method, at a given
subtrees, at the OBJECT node we would have words picked node. For the LSA-based and Google methods, this value is
from the PHYSICAL-OBJECT, INTANGIBLE-OBJECT,          the average of the accuracies obtained for each of the 10 runs
MENTAL-OBJECT, SOCIAL-OBJECT and TEMPORAL-            of the experiment at a given node. Since Google’s estimates
OBJECT subtrees. Thus, each node on the path had a differ- of page counts can vary quite a bit for a given search term,
ent set of words on which the experiments were carried out. the results for the Google method are as per the page counts
For each set of words, we used cross-validation testing to de- returned at the time of performing the experiments.
termine the number of words that were classiﬁed correctly. From the table we can see that at all nodes, the category-
  To generalize the results over the random element involved based method outperforms the LSA-based and Google meth-
in picking words, at each node, we carried out the experi- ods. The LSA-based method does better than the Google
ments 10 times, each time picking a different set of words method at all nodes except at the PRIMATE node.
and performing cross-validation testing. We used values of k At every node, we performed a t-test on the accuracy distri-
when running LSA that were empirically good.          butions of the LSA-based and Google methods using the pub-

                                                IJCAI-07
                                                  1621                         y  y                         to maximize b and d. Maximizing b is not possible because
                     x   a  b                         b should be small to minimize the numerator. Thus, in either
                     x   c  d                         case, d needs to be maximized.
                                                        It is clear that the Google method will say that x and y
                                                      are similar if three conditions are satisﬁed. First, x and y co-
Table 4: Contingency table for the co-occurrence of x and y
                                                      occur in a certain number of web pages (cell 1), second, the
                                                      number of web pages in which either of x or y occurred with-
licly available Simple Interactive Statistical Analysis (SISA) out the other is minimal (cells 2 and 3) and, ﬁnally, there are
package. The t-test results show that the difference in perfor- inﬁnitely many web pages in which neither x nor y occurred
mance of the LSA-based and Google methods is statistically (cell 4).
signiﬁcant at all nodes other than at the ALL and PRIMATE The Web, by virtue of its unrestricted size satisﬁes the last
nodes.                                                condition. However, at the same time, the size of the Web
  Similarly, the z-scores for the category-based method show can be a problem too. Due to the sheer mass of web pages
that the difference in performance of the LSA-based and on almost every conceivable topic, it is likely that a word on
category-based methods is statistically signiﬁcant at all nodes the Web will be used in more than one sense. This means
other than at the ANIMATE and ANIMAL nodes.           that, even if two words x and y are semantically related, there
                                                      are likely to be a signiﬁcant number of web pages in which x
4  Analysis                                           is used in a sense not related to y and vice-versa. Thus, for
The experimental results show that the category-based tree almost all pairs of words, the second condition would not be
descending method outperforms the other two methods while satisﬁed. For example, the words monk and pope are both
the LSA-based tree descending method outperforms the Goo- RELIGIOUS-ROLES that a HUMAN plays. Since the two
gle method. In this section, we attempt to identify the reasons words are semantically related, they co-occur in a signiﬁcant
for the superior performance of the category-based method as number of pages. A Google query with the words “monk
well as the ﬂaws in the Google method.                and pope” retrieves 569,000 web pages. However, the query
                                                      “monk and not pope” retrieves 5,740,000 web pages while
4.1  Analysis of the Google method                    the query “pope and not monk” retrieves 17,600,000 pages.
In this subsection, we argue that the Google method performs
poorly because the NGD measure does not capture the true 4.2 Analysis of the Category-based method
semantic distance between polysemous terms.           Rather surprisingly, the category-based method, which com-
  Given two words x and y, the Normalized Google Distance putes word similarities in the original vector space performs
(NGD) between them is given by:                       better than the more sophisticated LSA-based method. To
                       {     1       1  }             identify the reasons for the category-based method perform-
                   max  log p(x|y , log p(y|x         ing better than the LSA-based method, we carried out a set of
      NGD(x, y)=             1       1    (1)
                    max{log p(x) , log p(y) }         controlled experiments.
                                                        We carried out experiments with the LSA-based method
Words x and y will be perceived to be semantically similar if and the category-based method at the PHYSICAL-OBJECT,
the distance between them is small. That is, the numerator of ANIMATE, ANIMAL, MAMMAL  and SOCIAL-ROLE
(1) should be small while the denominator should be large. nodes. The category-based method was modiﬁed slightly in
Minimizing the numerator means maximizing both p(x|y)
      |                                               that, at a given node, at most 50 words were randomly picked
and p(y x) while maximizing the denominator means mini- from each of the subtrees rooted at the child concepts. This
mizing either p(x) or p(y).                           modiﬁcation was made so that, other than the use of LSA in
  Table 4 shows a contingency table for the co-occurrence of the ﬁrst method and category-based vector representations in
terms x and y. From the table:                        the second method, the two methods were identical in all re-
                      p(x ∩ y)    a                   spects.
             p(x|y)=          =
                       p(y)     a + c                   For the experiments at each node, two control groups
                                                      were used. The ﬁrst “control group” is a method that com-
                      p(x ∩ y)    a
             p(y|x)=          =                       putes word similarity using document vector representations
                       p(x)     a + b                 in the original vector space, that is, it neither uses LSA
In order to maximize p(x|y) and p(y|x), c and b respectively nor category-based vector representations. The second “con-
should be minimized. Also, from the table,            trol group” is a method that computes word similarity using
                          a + b                       category-based vector representations in the reduced vector
                p(x)=                                 space, that is, it uses both LSA and category-based vector
                       a + b + c + d                  representations.
                          a + c
                p(y)=                                   At each node, we evaluated the performance of the four
                       a + b + c + d                  methods 10 times each, using a different set of words for each
To maximize the denominator, we minimize either p(x) or evaluation.
p(y). To minimize p(x), we need to maximize c and d.How- Tables 6 through 10 show the results obtained at each node.
ever, maximizing c is not possible because c needs to be mini- For each table, the ﬁrst cell shows the average percentage
mized to minimize the numerator. To minimize p(y), we need accuracy for the method that uses both LSA and category-

                                                IJCAI-07
                                                  1622