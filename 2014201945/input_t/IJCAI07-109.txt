Interactive    Clustering of Text Collections According to a User-Speciﬁed Criterion

   Ron Bekkerman              Hema Raghavan                James Allan                Koji Eguchi
     University of              University of              University of            National Institute
     Massachusetts              Massachusetts              Massachusetts              of Informatics
  Amherst MA, USA            Amherst MA, USA            Amherst MA, USA               Tokyo, Japan
  ronb@cs.umass.edu         hema@cs.umass.edu           allan@cs.umass.edu          eguchi@nii.ac.jp

                    Abstract                          are numerous non-topical criteria that could be considered—
                                                      e.g., clustering by sentiment [Turney, 2002], style, genre, au-
    Document clustering is traditionally tackled from thor’s mood, and so on. Other criteria may be esoteric or
    the perspective of grouping documents that are top- application-speciﬁc: e.g., clustering by the author’s age, by
    ically similar. However, many other criteria for  the age of the documents, by their credibility, expressiveness,
    clustering documents can be considered: for exam- readability, etc. It is unlikely that a simple BOW representa-
    ple, documents’ genre or the author’s mood. We    tion will be sufﬁcient for all of those purposes, meaning that
    propose an interactive scheme for clustering doc- most will require speciﬁc document representations. Intu-
    ument collections, based on any criterion of the  itively, some of these representations will be based primarily
    user’s preference. The user holds an active posi- on syntax, while others are likely to have a semantic nature.
    tion in the clustering process: ﬁrst, she chooses
                                                        This study proposes a uniﬁed framework for clustering
    the types of features suitable to the underlying
                                                      document collections according to nearly any criterion of the
    task, leading to a task-speciﬁc document represen-
                                                      users choice. (We restrict ourselves to hard clustering—i.e.,
    tation. She can then provide examples of features—
                                                      partitioning—of a document collection.) The user is ﬁrst
    if such examples are emerging, e.g., when cluster-
                                                      asked to choose types of features suitable for clustering by the
    ing by the author’s sentiment, words like ‘perfect’,
                                                      desired criterion. For example, genres may be represented by
    ‘mediocre’, ‘awful’ are intuitively good features.
                                                      sequences of Part-Of-Speech (POS) tags, by a particular fo-
    The algorithm proceeds iteratively, and the user can
                                                      cus on punctuation, stopwords, as well as by general words
    ﬁx errors made by the clustering system at the end
                                                      as captured in the standard BOW representation. The user is
    of each iteration. Such an interactive clustering
                                                      next asked to provide a few examples of features (seed fea-
    method demonstrates excellent results on clustering
                                                      tures) of the chosen types, if such examples are intuitive and
    by sentiment, substantially outperforming an SVM
                                                      can be obtained without much effort—e.g., when clustering
    trained on a large amount of labeled data. Even if
                                                      by authors mood, words like ‘angry’, ‘happy’, ‘upset’ might
    features are not provided because they are not in-
                                                      be easily suggested.
    tuitively obvious to the user—e.g., what would be
    good features for clustering by genre using part-   The clustering system then represents documents based on
    of-speech trigrams?—our multi-modal clustering    the users choice and applies a multi-modal clustering method
    method performs signiﬁcantly better than k-means  [Bekkerman and Sahami, 2006]. When seed features are pro-
    and Latent Dirichlet Allocation (LDA).            vided, the system iteratively clusters documents represented
                                                      over the chosen features and then enriches feature sets with
                                                      other useful features. The user can choose to intervene (or
1  Introduction                                       not) after each iteration, in order to ﬁx possible mistakes
The problem of data clustering is generally underspeciﬁed made by the system on the feature level (no document label-
unless criteria for clustering are explicitly provided. For ex- ing is required).
ample, given a set of objects of various colors and shapes, it We illustrate the effectiveness of our approach on two do-
is unclear whether clustering should be performed according mains: clustering by genre and clustering by author’s senti-
to the objects color, shape, or both. In the absence of labeled ment. Genre is a type of a domain where providing examples
instances, a clustering criterion might be expressed in terms of features is non-trivial: it is not intuitive, e.g., whether noun
of the data representation: e.g., if only shapes of objects are phrases are more effective than verbs. Sentiment classiﬁca-
known, there is no more doubt about the clustering criterion. tion is one where words like ‘brilliant’ and so on are easily
  When we talk about clustering text documents, we usually recognizable as useful, when using BOW features.
assume that the clustering will be by topic and we typically There has been work on interactive topical clustering
approach it using a Bag-Of-Words (BOW) representation that where the user corrects clustering errors on a document ba-
ignores word order [Willett, 1988]. However, there is no rea- sis [Basu et al., 2004], but that effort is more time consuming
son that text documents must be clustered in that way: there than feedback on features [Raghavan et al., 2005].Otherre-

                                                IJCAI-07
                                                   684cent work has had the user select important keywords for (su- ily constructed (e.g. it is non-trivial to come up with good
pervised) categorization, thereby leveraging the user’s prior feature examples for clustering by genre—see Section 5), the
knowledge [Dayanik et al., 2006; Raghavan et al., 2005]— user can skip this step and go to 4.
approaches that are more like that of our framework. Ragha- 4. Default clustering: If m feature types are chosen, but
van et al. [2005] further support this direction in the ﬁnding no seed features are provided by the user, documents are rep-
that users can identify useful features with reasonable accu- resented as m distributions, each of which is over the (entire)
racy as compared to an oracle. Liu et al. [2004] experiment feature sets of the corresponding type and then multi-modal
with labeling words instead of documents for text classiﬁca- distributional clustering [Bekkerman and Sahami, 2006] is
tion, providing the user with a list of candidate words from applied.
which to select potentially good seed words, based on which a 5. Interactive Clustering: For the cases when the user
training set is constructed from a set of unlabeled documents. has provided seed features for some of the feature types, we
A classiﬁer is then constructed given this training set. Liu et propose a new model for multi-modal clustering, which com-
al.’s document representation is the standard BOW, which has bines regular clustering of non-seeded variables with an in-
strong topical ﬂavor, and therefore cannot be used for clus- cremental, bootstrapping procedure for seeded variables:
tering by any criterion (for example, our preliminary exper- 1. Represent documents as distributions over the sets of
iments show that BOW is not appropriate for clustering by seed features. Ignore documents with zero probability
author’s mood). In addition, Liu et al.’s method involves the given the seed features. Cluster the remaining docu-
user only at the initial step (selecting seed words), limiting ments using the distributional clustering method.
the user’s control of the classiﬁcation process.
  In summary, we propose a new interactive learning frame- 2. Stop if most documents have been clustered (see Sec-
work for clustering by user-determined criteria (Section 2). tion 6 for details).
Our multi-modal clustering method based on combinatorial 3. Represent all features of the clustered documents as dis-
MRFs (Section 3) neatly incorporates multiple feature types tributions over the document clusters. Ignore features
as well as user prior knowledge into clustering presented as that have zero probability given the clustered documents.
a combinatorial optimization problem. We demonstrate the  Cluster the remaining features using the distributional
effectiveness of our system by testing it on genre clustering clustering method.
(Section 5) and multi-class clustering by author’s sentiment 4. Select feature clusters that contain the original seed
when seed features are provided (Section 6). To our knowl- words. Let the user revise the selected clusters: noisy
edge, this study is the ﬁrst in which clustering (as opposed features can be deleted; misplaced features can be relo-
to classiﬁcation) by genre is discussed and the ﬁrst to per- cated; new features can be added. The revised clusters
form multi-class clustering of documents by sentiment. We of features are the new sets of seed features. Go to 5.1.
show that our interactive clustering outperforms state-of-the-
art methods (SVM and LDA) on real-world data collections. 3 Combinatorial MRFs for clustering
                                                      A combinatorial Markov random ﬁeld (Comraf) [Bekkerman
2  Interactive  clustering scenario                   and Sahami, 2006] is a new framework for multi-modal learn-
We provide a step-by-step recipe for clustering documents by ing in general, and for multi-modal clustering in particular.
a particular criterion that the user has in mind:     Multi-modal (hard) clustering is a problem of simultaneously
  1. Specify the number of clusters: Learning the natural constructing m partitionings of m data modalities, e.g. of
number of clusters still remains an open problem. We do not documents, their words, authors, titles etc. While clustering
attempt to solve it in this paper, instead the user is asked to modalities simultaneously, one would overcome the statisti-
specify the desired number of clusters.               cal sparseness of the data representation, leading to a dense,
  2. Specify feature types: A list of various feature types smoothed joint distribution of the modalities that would result
is provided to the user. Examples of such types are: bag in (hypothetically) more accurate clusterings than the ones
of words or word n-grams, POS tags or POS tag n-grams, obtained when each modality is clustered separately. Bekker-
punctuation, parse subtrees and other types of syntactic and man et al. [2005] empirically justify this hypothesis.
semantic patterns that can be extracted from text. Such a list A Comraf model for multi-modal clustering is an undi-
                                                                                                  X    ≤
can hypothetically include a large variety of feature types that rected graphical model in which each data modality i :1
                                                      i ≤  m
would respond to everyone’s needs. From this list the user is corresponds to one discrete random variable (r.v.).
asked to choose one or more types that best serve the particu- This r.v. is deﬁned over all possible clusterings of Xi,which
lar clustering criterion.                             implies that the support of this r.v. is exponentially large in the
                                                            X                                       X
  3. Give examples of features: For each feature type size of i. We call such an r.v. a combinatorial r.v. Let i be
                                                                                         X
chosen, the user should attempt to construct (small) sets of an r.v. with an empirical distribution over i (e.g. over docu-
seed features that correspond to each category of documents. ments in the dataset); let X˜ij be an r.v. deﬁned over clusters
                                                                                  c
Sometimes this task is easy: e.g., if the clustering criterion in the j-th clustering of Xi;letX˜i be a combinatorial r.v. de-
is authors’ sentiments, then words such as ‘excellent’, ‘bril- ﬁned over all the possible clusterings of Xi. Edges eii in the
liant’ etc. would correspond to the category of positive docu- Comraf graph G correspond to interactions between modali-
ments, while ‘terrible’, ‘awful’ etc. would correspond to the ties (the graph is not necessarily fully connected). Examples
negative category. However, when such sets cannot be eas- of Comraf graphs are shown in Figure 1.

                                                IJCAI-07
                                                   685                                                         ~c          ~c          ~c              ~c
  The objective is to construct clusterings of modalities (or, D     D           D               D
in other words, to ﬁnd values of combinatorial r.v.’s) such that
                                                          c          ~c          ~ c          ~c     ~ c
the sum of pairwise Mutual Information between the cluster- S        S           W            S      W
ings of the interacting modalities is maximized:
                                                       (a)         (b)          (c)             (d)
           c∗
          x˜                 I X˜ij X˜ij .
              =argmaxc        (   ;   )         (1)
                    x˜j
                        eii ∈E                       Figure 1: Comraf graphs for: (a) 1-way document clustering
                                                      with POS unigrams as an observed r.v. (shaded node); (b) 2-
                                          G
This objective function naturally factorizes over ,sothat way clustering of documents and POS bigrams (same as for
an efﬁcient inference algorithm (such as Iterative Conditional POS 3-grams or 4-grams); (c) 2-way clustering with BOW;
Mode—ICM   [Besag, 1986]) can be directly applied. The (d) 3-way clustering with POS bigrams and BOW.
ICM algorithm iterates over each node in G,whichis opti-
mized with respect to the current values of its neighbors.
  In the Comraf case, the optimization of each node is a of categories in our dataset (see Section 4), we decide about
                                      xc
resource-consuming process. Each clustering ˜ij can be rep- feature types which would best match the task of clustering
                 c  ,c ,...,c         n
resented as a point ( j1 j2  jni ) in an i-dimensional by genre. Documents are labeled with genres on the basis
hypercube Hi of all the possible clusterings (where ni is the of external criteria such as intended audience, purpose and
number of elements of the i-th modality), meaning that ele- activity type [Lee, 2001]. The notion of genre can be de-
ment 1 belongs to cluster cj1,element2 belongs to cluster scribed in terms of the syntax/semantics duality of text: doc-
cj2 etc. We apply the simplest combinatorial optimization uments of different genres use different syntactic construc-
algorithm—hill climbing, where the procedure starts at some tions and/or different vocabulary. It is not obvious whether
point on Hi and greedily searches for a nearby point that sat- syntactic or semantic features play a major role in clustering
isﬁes Equation (1). Since the problem is non-convex, random documents by genre. We propose to take advantage of both.
restarts areusedtoovercomelocal optima.               We represent documents over two sets of features: words (that
  In this paper, we propose an interactive learning approach, correspond to documents’ vocabularies) and Part-Of-Speech
in which the user assists the clustering algorithm to avoid lo- (POS) n-grams (that correspond to the syntactic structure of
cal optima. First, by selecting seed features, the user speciﬁes text). POS n-grams are extracted from sentences in an incre-
a potentially good starting point on the hypercube Hi. Sec- mental manner: the ﬁrst n-gram starts with the POS tag of the
ond, by correcting the constructed clustering after each itera- ﬁrst word in the sentence, the second one starts with the tag
tion, the user causes a controlled jump from one region of Hi of the second word etc.
to another, in which potentially better clusterings are located. Intuitively, one cannot come up with particular features
                                                      that best capture documents’ genres (e.g. it is hard to say
4  Evaluation methodology                             whether a word ‘clouds’ is more often used in ﬁction, po-
In this paper we use clustering accuracy as a quality mea- etry or weather reports). To the contrary, document distrib-
sure of document clustering. Let T be the set of ground truth utions over the entire set of features would be different for
                                                      documents of different genres and are then the most appro-
categories. For each cluster d˜,letγT (d˜) be the maximal num-
                d˜                                    priate representation of documents for clustering by genre.
berofelementsof   that belong to one category. Then, the Thus, we apply the multi-modal clustering method described
        Prec d,˜ T   d˜              T
precision    (   ) of  with respect to ,isdeﬁnedas    in Section 3, without the interactive learning component.
Prec(d,˜ T )=γT (d˜)/|d˜|. The micro-averaged precision of Given a document collection, let D be a random variable
                                           ˜
                                        γT (d)                         W
                 d˜c  Prec  d˜c,T     d˜             over its documents, be a random variable over its words,
the entire clustering is:  (    )=       |d˜| ,which
                                        d˜            and S be a random variable over the POS n-grams of its
is the portion of documents appearing in the dominant cate- words. We apply a multi-modal Comraf model (Section 3)
gories. For all our experiments we ﬁx the number of clus- for constructing a clustering D˜ of documents, a clustering W˜
ters to be equal to the number of categories. In this case, of words and/or a clustering S˜ of POS n-grams, by maximiz-
Prec(d,˜ T ) equals clustering accuracy.              ing the objective from Equation (1). In this paper, we consider
  In our experiment with clustering by sentiment, we com- four Comraf models for clustering by genre:
pare Comraf clustering results with SVM classiﬁcation re- 1. POS unigrams: Since the number of POS tags in any
sults. Bekkerman and Sahami [2006] show that the clustering tagging system is relatively small, it makes no sense to clus-
accuracy can be directly compared with the (standard) classi- ter POS unigrams. Therefore, we apply a 1-way model for
ﬁcation accuracy if a constructed clustering is well-balanced, clustering documents using the Comraf graph shown in Fig-
meaning that each category prevails exactly in one cluster. ure 1(a). The objective function from Equation (1) in this
It appears that all our clusterings obtained using the Comraf simple case has the form of I(D˜; S).
model are well-balanced.                                2. POS n-grams, where  n > 1. The number of unique
                                                      POS  n-grams of order higher than 1 is exponential in n,so
5  Clustering by genre                                clustering them would be necessary. We perform a 2-way
According to the scenario proposed in Section 2, let us set up clustering with the Comraf graph from Figure 1(b) and the
an experiment of clustering documents by their genre. Af- objective I(D˜; S˜).
ter ﬁxing the number of clusters to be equal to the number 3. Bag-Of-Words: The number of unique words in our

                                                IJCAI-07
                                                   686 Doc representation k-means   LDA        Comraf             Comraf accuracy with POS ngrams Comraf accuracy with BOW
                           55.4 ± 0.1% 55.7 ± 0.2%       0.570                      0.570
 Bag-Of-Words      9.1%                                  0.545                      0.545
                           44.7 ± 0.2% 51.0 ± 0.2%
 POS bigrams      23.2%                                  0.520                      0.520
                                       58.5 ± 0.6%
                                                                                   accuracy
 BOW + POS bigr     n/a       n/a                       accuracy 0.495              0.495
                                                         0.470                      0.470
                                                              1   2    3   4            3 10 20       50
Table 1: Clustering by genre. Clustering accuracy on the          ngram size           threshold on low frequent words
BNC corpus, averaged over four independent runs. Standard
error of the mean is shown after the ± sign. Comraf results Figure 2: Clustering by genre. Comraf clustering accuracy
with other POS tuples, besides bigrams, are in Figure 2(left). as a function of: (left) size of POS n-gram (1-grams, 2-grams,
The BOW+POS hybrid setup is only applicable in Comrafs. 3-grams and 4-grams); (right) threshold on low frequency
                                                      words—a point i on the X axis means that in this experiment
                                                      words that appear in less than i documents are removed.
dataset is comparable with the number of POS trigrams, so
in analogy to the previous model, we perform a 2-way clus-
tering with the Comraf graph of Figure 1(c) and the objective
I D˜ W˜                                               fective than unigrams, and almost as effective as trigrams and
 ( ;   ).                                             fourgrams, while being much more efﬁcient.
  4. BOW+POS hybrid:    We combine contextual infor-
mation of BOW and stylistic information of POS n-grams
into a 3-way clustering model, where we simultaneously 6  Clustering by sentiment
cluster documents, words and bigrams of POS tags. Over In clustering by authors’ sentiment, data categories corre-
the Comraf graph of Figure 1(d), we maximize the sum  spond to different levels of the authors’ attitude to the dis-
I(D˜; S˜)+I(D˜; W˜ ).                                 cussed topic (e.g. liked/disliked, satisﬁed/unsatisﬁed etc.).
                                                      The categories can be ﬁner grained (strongly liked / some-
5.1  Dataset                                          what liked etc.)—as long as it is possible to distinguish be-
                                                      tween two adjacent categories.
We evaluate our models on the British National Corpus   Following the procedure described in Section 2, after
      [            ]
(BNC)  Burnard, 2000 . We employ David Lee’s ontology choosing the number of clusters and particular feature types,
             [        ]
of BNC genres Lee, 2001 with 46 genres covering most as- the user is asked to select a few seed features for each cate-
pects of modern literature such as ﬁction prose, biography, gory. For clustering by sentiment, as well as for close tasks of
technical report, news script and others. To perform fair eval- clustering by authors’ mood or by familiarity with the topic,
uation using clustering accuracy (Section 4), we choose 21 relevant feature types may be words or word n-grams (i.e.
largest categories, for each of which we uniformly at random semantic features). However, for other quite similar tasks,
choose 32 documents, so our resulting dataset consists of 672 e.g. clustering by authors’ age, not only semantics but also
documents. The BNC texts are represented in SGML. We  syntax can matter: children, for instance, use certain words
remove all markup, lowercase the text, and delete stopwords more often than adults do; children also tend to use primi-
and low frequency words. All words in the BNC corpus are tive (and sometimes erroneous) syntactic constructions (“me
semi-manually tagged using 91 POS tags, four of which re- going bye-bye” etc.). In this paper, for simplicity, we experi-
fer to punctuation. The resulting dataset has 63,634 unique ment with word features only.
words; and 5864 POS bigrams. Since the overall number of The task of selecting seed words has two issues. First, it
unique POS trigrams and fourgrams is prohibitively large, we is easier to come up with words that correspond to extreme
apply more aggressive term ﬁltering: we consider trigrams sentimental categories (‘spectacular’, ‘horrible’), but it is dif-
that appear in at least 10 documents (44,499 trigrams overall) ﬁcult to choose seed words for intermediate, mild categories.
and fourgrams that appear in between 10 and 99 documents Nevertheless, as we will see in Section 6.2 users usually suc-
(114,476 fourgrams).                                  ceed in accomplishing this task. Second, in our early exper-
                                                      iments, users consistently tended to choose words that were
5.2  Results                                          out of the vocabulary of a given dataset. Inspired by Liu et
We compare the results of our clustering model with the re- al. [2004], we decided to provide the users with a word list, to
sults of k-means (Weka implementation), as well as of Latent narrow her search only to the dataset vocabulary. Unlike Liu
Dirichlet Allocation (LDA)—a popular generative model for et al. [2004], whose task is topical clustering, we cannot auto-
unsupervised learning. We use Xuerui Wang’s LDA imple- matically predict which words would be relevant. Instead, we
mentation [McCallum et al., 2005] that performs Gibbs sam- employ Zipf’s law and provide the user with a list of words
pling with 10000 sampling iterations. Table 1 summarizes the from the interior of the frequency spectrum. We anticipate
results which appear to be surprisingly good for an unsuper- such a list to contain the most relevant seed words.
vised method, given that the result of a random assignment We then perform an iterative process of clustering that al-
of documents into 21 clusters would be about 5% accuracy. lows user’s involvement in between clustering iterations. We
As shown, the 3-way Comraf model signiﬁcantly outperforms apply a 2-way Comraf model (see Figure 1c): we ﬁrst clus-
other (1-way) models. Figure 2 shows results of stability tests ter documents that contain the selected seed words and then
of 2-way Comraf models: (left) the POS n-gram setup; (right) we cluster all words of these documents. In the latter step, our
the BOW setup. As shown on the left ﬁgure, the POS bigrams seed word groups are enriched with new words that have been
setup is preferable over the other POS tuples: it is more ef- clustered together with the original seed words. The user is

                                                IJCAI-07
                                                   687then asked to edit the new seed word groups, in order to cor- Doc repres. k-means LDA   Comraf      SVM
rect possible mistakes made by the system (word removal, BOW        28.2   37.0 ± 0.2 40.3 ± 0.8  39.1 ± 0.3
relocation and addition is allowed). By this, a clustering iter- Sentim. list 29.0 40.2 ± 0.5 43.0 ± 0.9 41.3 ± 0.6
ation is completed and the next iteration can be executed. Interactive clustering (Oracle) 47.1 ± 0.2 n/a
                                                                                            46.3 ± 0.1
  Since the seed word groups have been enlarged, we can ex- Simulated classiﬁcation (Oracle)
pect that a set of documents that contain these seed words is
now larger as well, so that the clustering process will cover Table 2: Clustering by sentiment. Clustering accuracy
                                                      vs. classiﬁcation accuracy. Standard error of the mean is
more and more documents from iteration to iteration. The           ±
process stops when no more documents are added to the pool. shown after the sign.
Documents that have never been covered (the ones that con-
tain no seed words from the largest seed word groups) are the idea behind interactive clustering and provided a brief de-
considered to be clustered incorrectly. An alternative ap- scription of the dataset. They were given a list of 563 words
proach to guarantee the algorithm’s convergence would be to that appeared in 50 ≤ n<500 documents in our dataset.
require enlargement of seed word groups such that at least The users proceeded as described in Section 2. Also, we con-
one document is added to the clustering at each iteration. The struct an oracle as follows: for each category t we select 25
algorithm would then stop when the entire dataset is covered. most frequent words that belong to a given list of sentimental
We choose the former approach because (a) we do not want words2 and their distribution over the categories has a peak at
to put additional constraints either on the user or on the Com- t. Unlike human users, the oracle does not provide feedback
raf clustering model; (b) in each real-world dataset there can between clustering iterations. To some extent, the oracle’s
be documents whose sentimental ﬂavor is hard to identify – it performance can be considered as an upper bound to results
would not be beneﬁcial to force such documents into any of obtained in practice, when a human user is involved.
the sentimental clusters.                               We perform  a simulated classiﬁcation (SC) experiment
                                                      analogous to the one of Liu et al. [2004] (see a description in
6.1  Experimental setup                               Section 1), where the seed words are provided by our oracle.
We evaluate our interactive clustering system on a dataset of We replace an ad-hoc kNN-like clustering in Liu et al.’s im-
movie reviews. Our dataset consists of 1613 reviews writ- plementation by our effective Comraf clustering, and a Naive
ten on “Harry Potter and the Goblet of Fire (2005)” that Bayes classiﬁer by an SVM.
                   IMDB.com              1
we downloaded from           in May 2006.  The data   6.2  Results
was preprocessed exactly as the BNC corpus. We ignore
reviews that do not have rating scores assigned by the user. Table 2 summarizes our observations. Surprisingly, with
The IMDB’s scoring system is from 1 (the worst) to 10 (the BOW features, our Comraf clustering method performs as
best). Based on our extensive experience with IMDB.com, well as an SVM trained on a large amount of data (Row 1). A
we translate these scores into four categories as follows: good performance of our unsupervised method (with BOW)
scores 1 to 4 are translated into the category strongly disliked indicates that the constructed topical clustering sheds some
(292 documents), scores 5 to 7 are translated into somewhat light on reviewers’ sentiments, which can occur when the re-
disliked (454 documents), scores 8 and 9 into somewhat liked viewers have a consensus on certain aspects of the movie,
(447 documents), and score 10 is translated into the category e.g. liked the actors but disliked the plot etc.
strongly liked (420 documents). We do not introduce a neu- After feature selection according to our list of sentimental
tral category because there are very few neutral reviews on words, the Comraf achieves a signiﬁcant boost in accuracy
IMDB.com.                                             surpassing the SVM (Row 2). Using an oracle in our interac-
  On the task of clustering by sentiment, we compare our tive clustering setup (Row 3) improves the performance even
method’s performance with that of an SVM classiﬁer trained further, while the SC result (Row 4) is only slightly (but sig-
on 22,476 movie reviews. The training data for the SVM con- niﬁcantly) inferior. These two results are close because the
sisted of reviews of 46 popular Hollywood movies released training set of SC is identical to the clustering constructed at
in 2005, of the same genre as Harry Potter. The reviews and the ﬁrst iteration of the Comraf algorithm. As its size appears
genre labels of movies are obtained from IMDB.com.Again, to be over 3/4 of the entire dataset, there is almost no room
we ignore reviews without user-assigned rating.       for the actual diversity in performance of the two methods.
  The system is evaluated on ﬁve users who are familiar with Figure 3 (left) shows the accuracy (micro-averaged over
the task of document clustering. The users were explained the classes) for each user and each iteration. For three of the
                                                      ﬁve users, selection of the initial seed words is sufﬁcient to
                                                      obtain signiﬁcantly higher accuracy than the best result of the
  1Bo Pang [Pang and Lee, 2005] maintains a popular dataset of
movie reviews that, unfortunately, does not fully correspond to our SVM. User 2 has signiﬁcantly lower accuracy than the base-
task because (a) we want to differentiate the problem of clustering line to begin with, but over the two correction steps is able to
by sentiment from the topical clustering—for this reason our dataset provide the necessary feedback so as to obtain an improve-
contains reviews written on one movie only, so that the topic of all ment in accuracy, equalling the baseline. We found that User
the reviews is potentially the same; (b) movie ratings in Bo Pang’s 2 was fairly conservative in her assessment of terms in the
dataset are extracted from the reviews’ text, which is an error-prone
procedure, whereas in our dataset the ratings are assigned by the 2Our list of 4295 sentimental words was obtained as described in
reviewers using an HTML form which leaves no room for errors. [Eguchi and Lavrenko, 2006].

                                                IJCAI-07
                                                   688