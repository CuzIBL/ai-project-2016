                        The Backbone of the Travelling Salesperson

            Philip Kilby                      John Slaney                       Toby Walsh
                ANU                        NICTA and ANU                    NICTA and UNSW
         Canberra, Australia              Canberra, Australia                Sydney, Australia
      Philip.Kilby@anu.edu.au           John.Slaney@anu.edu.au               tw@cse.unsw.edu.au

                    Abstract                          2   Backbones
                                                      In decision problems, the concept of the backbone has proven
    We study the backbone of the travelling salesper- to be very useful in understanding problem hardness. For ex-
    son optimization problem. We prove that it is in- ample, the backbone of a satisﬁability (SAT) problem is the
    tractable to approximate the backbone with any    set of literals which are true in every model [Monasson et al.,
    performance guarantee, assuming that P6=NP and    1998]. Backbone size appears closely correlated to problem
    there is a limit on the number of edges falsely re- hardness [Parkes, 1997; Monasson et al., 1998]. If a SAT
    turned. Nevertheless, in practice, it appears that problem has a large backbone, there are many opportunities
    much of the backbone is present in close to optimal to assign variables incorrectly. Such problems tend to be hard
    solutions. We can therefore often ﬁnd much of the therefore for systematic methods like Davis-Putnam. A large
    backbone using approximation methods based on     backbone also means that solutions are clustered. Such prob-
    good heuristics. We demonstrate that such back-   lems therefore can be hard to solve with local search methods
    bone information can be used to guide the search  like WalkSAT.
    for an optimal solution. However, the variance in   Backbones have been less well studied in the context of
    runtimes when using a backbone guided heuristic is optimization. In [Slaney and Walsh, 2001], the backbone of
    large. This suggests that we may need to combine  an optimization problem is deﬁned to be the frozen decisions:
    such heuristics with randomization and restarts. In those with ﬁxed outcomes for all optimal solutions. For ex-
    addition, though backbone guided heuristics are   ample, the backbone of a TSP problem is the set of edges
    useful for ﬁnding optimal solutions, they are less which occur in all tours of minimal cost. Such a concept
    help in proving optimality.                       again seems useful in understanding problem hardness. For
                                                      instance, the cost of ﬁnding optimal (or near optimal) solu-
                                                      tions is positively correlated with backbone size [Slaney and
                                                      Walsh, 2001].
1  Introduction
                                                      3   Computing the Backbone
In recent years, there has been much research into what makes
a search problem hard. In decision problems, we now have It is not difﬁcult to see that it is NP-hard to ﬁnd the backbone
a rich picture based upon rapid transitions in solubility and of a TSP problem. If the optimal tour is unique, then the
a corresponding thrashing in search algorithms for the crit- backbone is complete. A procedure to compute the backbone
ical constrained problems close to such “phase transitions” then trivially gives the optimal tour length. Complications
[Cheeseman et al., 1991; Mitchell et al., 1992]. The picture arise when the optimal tour is not unique and the backbone is
is much less clear for optimization. Optimization problems incomplete. For example, suppose there are two disjoint but
do not have a transition in solubility, since optimal solutions optimal tours. The backbone is then empty. Nevertheless, we
always exist. A strong candidate to replace the transition in can ﬁnd the optimal tour length using a polynomial number
solubility is a transition in the backbone size [Monasson et of calls to a procedure that determines backbone edges.
       ]
al., 1998 . In this paper, we study the complexity of comput- Theorem 1 The TSP BACKBONE problem (the problem of
ing and of approximating the backbone. We also look at us- deciding if an edge is in the backbone of a TSP problem) is
ing such backbone information to help ﬁnd optimal solutions, NP-hard.
and to prove optimality. Throughout this paper, we focus
on the symmetric travelling salesperson (TSP) optimization Proof: We rescale the TSP problem so that only one opti-
problem. This is the problem of computing the shortest tour mal tour remains. Without loss of generality, we assume that
which visits every city, where the inter-city distance matrix is inter-city distances are integers. First, for a problem with n
symmetric. However, most of our results easily generalise to cities, we multiply each inter-city distance by 2n(n+1). This
the asymmetric case.                                  gives n(n + 1) new bits in the least signiﬁcant digits of eachnumber. Note that this only requires polynomial space. We For example, the approximation procedure that returns all
divide these new bits into n sections, one for each city, of O(n2) possible edges returns all the backbone. We therefore
length n + 1 bits. For the inter-city distance between city i consider approximation procedures which limit the number
and city j, we set the jth bit in the ith section and the ith bit in of edges falsely assigned to the backbone. An approxima-
the jth section. Each tour length now encodes the list of edges tion procedure is a “majority-approximation” iff, when the
visited. In particular, the ith section of the tour length has two backbone is non-empty, more edges are returned that come
bits, say, j and k set. This represents the fact that the tour in- from the backbone than do not come from the backbone. If
cludes edges from j to i and then to k. Even if the old distance the backbone is empty, any number of edges can be falsely
matrix had several optimal tours, the new distance matrix returned.
only has one of them. Let ci be the two cities on a tour con- Theorem 2 If P 6= NP then no majority-approximation pro-
nected to the ith city. Then the optimal and unique tour of the cedure can be guaranteed to return a ﬁxed fraction α or
rescaled problem is the optimal tour of the unrescaled prob- greater of the backbone edges in polynomial time.
lem in which hmax(c1), min(c1), . . . , max(cn), min(cn)i is
lexicographically least. As the optimal tour is now unique, Proof: We show how such a procedure could determine if a
the backbone is complete. We can compute this by a polyno- graph (V, E) has a Hamiltonian path with a designated start-
mial number of calls to a procedure for deciding if an edge is ing and ending vertex in polynomial time, contradicting P 6=
in the backbone. 2                                    NP. We construct a TSP problem which can have two sorts of
  It remains open whether the TSP BACKBONE problem is shortest tours. For every Hamiltonian path between the start-
NP-complete or not. Completeness would seem to require a ing and ending vertex, there is a corresponding tour of length
short witness that a tour was optimal. We can, however, say it n − 1 where |V | = n. These tours have a non-empty set of
is both NP-hard and NP-easy. It is NP-easy as deciding if an backbone edges taken from some set S. On the other hand, if
edge is in the backbone can be reduced to a polynomial num- there is no Hamiltonian path between the starting and ending
ber of calls to a TSP decision procedure. Garey and Johnson vertex, the shortest tour is of length n, and has a non-empty
suggest that problems which are both NP-hard and NP-easy set of backbone edges (denoted T ) disjoint with S.
might be called NP-equivalent [Garey and Johnson, 1979]. Since the approximation procedure returns at least a ﬁxed
Although this class contains problems which do not belong to fraction of the backbone, it returns at least one correct back-
NP, the class has the property of NP-complete decision prob- bone edge. As it is a majority-approximation procedure, the
lems that: unless P=NP, no problem in the class can be solved number of edges in the backbone correctly returned is more
in polynomial time, and if P=NP then all problems in the class than the number incorrectly returned. By computing the ra-
can be solved in polynomial time. In other words, the TSP tio of the number of edges of the two types returned, we can
BACKBONE  problem is polynomial if and only if P=NP.  determine if the backbone is drawn from edges in S or from
                                                      those in T . That is, we can determine if there is a Hamiltonian
4  Approximating the Backbone                         path or not in polynomial time.
                                                        We need two special gadgets. The ﬁrst is a “backbone free”
Even though computing the backbone is intractable in gen- connector. This connects together two nodes without intro-
eral, might we be able to approximate it?             ducing any backbone edges. For example, to connect node i
4.1  Sound approximation                              to node j, we use the following small circuit which introduces
                                                      3 new intermediate nodes: k , k and k . All marked edges
Suppose that we have an approximation procedure that re-                      1  2      3
                                                      are of cost 0, and all unmarked edges are of cost n2:
turns some subset of the backbone edges. It is easy to see
that, assuming P 6= NP, no such procedure can be guaranteed                  k1
to return a ﬁxed fraction of the backbone in polynomial time:                @
                                                                              j@      $
First, we rescale the distance matrix as before so that the op-          i   k2   j
timal tour is unique and the backbone complete. The sound                  @     
                                                                          j@j      j
approximation procedure could be called to return at least one               k3
backbone edge - say (a, b). Create a new TSP by collapsing a
                    0           0                                             j       %
and b into a single node a . The cost c(a , x) for node x would A tour from i to j goes through some permutation of k1, k2,
be set to MIN (c(a, x), c(b, x)); all other costs as before. The k3. No edge is in all possible tours, and thus no edge ap-
procedure could be repeatedly called to construct the optimal pears in the backbone. Note that we can modify this (and the
tour edge by edge in polynomial time.                 other circuits) to have non-zero edges costs by increasing all
  A similar argument shows that no sound approximation other edge costs appropriately. We will draw this gadget as a
procedure can be guaranteed to return at least one backbone rectangular box between nodes i and j.
edge if the backbone is non-empty in polynomial time.   The second gadget is a “switch” circuit. This uses three
                                                      of the backbone free connector gadgets. Four edges go into
4.2  Unsound approximation                            this circuit, two horizontally and two vertically. The switch
Suppose that edges returned by an approximation procedure has two modes. In the ﬁrst mode, the tour enters and exits
are not guaranteed to be in the backbone. If we do not limit by the vertical edges. In the other mode, the tour enters and
the number of edges incorrectly returned, then there exists exits by the horizontal edges. The switch circuit contains 15
a polynomial time approximation that meets any approxima- nodes, all of which are visited in both modes. Note that no
tion ratio (that is, returns any given fraction of the backbone). backbone edges are common between the two modes. Thegadget is show below in a). As before, all marked edges are 5 Epsilon Backbone
                                      2
of cost 0, and all unmarked edges are of cost n . We give the Despite these negative complexity results, backbones may
two different modes of the circuit in b) and c). We will draw still be easy to compute in practice. We study here the pos-
this switch gadget as a square box containing an “x”. sibility of using approximation procedures. We deﬁne the -
                                                      backbone of a TSP problem as the set of edges which occur
                                                      in all tours within a factor (1 + ) of the optimal. In Figure 1,
                                                      we plot the fractional size of the -backbone (that is, the size
          Xf  f       Xf    f      f   f                                        
          XX           XX                         of the -backbone normalized by n) against 1 +  for random
          f     f      f     f      f     f           Euclidean TSP problems.
          f     f      f     f      f     f
           a)           b)           c)                                  Epsilon vs Size of Backbone
                                                                         100 trials, random Euclidean TSP, n = 100
  We use these gadget to construct a TSP problem with the       1
                                                                                     Trial data
required shortest tours. The problem has 16(n−1)+1 nodes,                            Regression line (degree 6)
n of which correspond to vertices in the graph (V, E) and the  0.8
rest are in n − 1 switch gadgets. If the graph (V, E) has an
edge between vertex i and j then the TSP problem has an edge   0.6
of cost 1 between node i and j. We shall assume that the
starting and ending vertices/nodes are 1 and n respectively.   0.4
To ﬁnish the tour, we put a zero cost edge between node 1
and n. There are also n − 1 switch circuits. The horizon-      Fraction  of nodes in epsilon-backbone 0.2
tal wires in the ith switch circuit are connected to node i and
                                                                0
                                                                 1   1.01  1.02 1.03  1.04 1.05 1.06
i + 1 (1 ≤ i ≤ n − 1). The vertical wires in the jth switch                  Approximation Ratio
circuit are connected to the j − 1th and j + 1th switch circuit
(1 < j < n − 1). The vertical wires in the 1st switch cir-
cuit are connected to node 1 and the 2nd switch circuit. The Figure 1: Fractional size of the -backbone (y-axis) plotted
vertical wires in the n − 1th switch circuit are connected to against approximation ratio (= 1 + ) for 100 random Eu-
the n − 2th switch circuit and to node n. All edge costs be- clidean TSP problems with 100 nodes. Degree 6 regression
tween switch circuits and nodes, and between switch circuits line ﬁtted.
are zero except for the edge between node 1 and the horizon-
                                                        We see that tours within 5% of optimal have approximately
tal input to the 1st switch circuit which has cost n. The cost
                                                      40% of the backbone of optimal tours. We observe similar
between any two unmarked nodes is n2 as before.
                                                      results with non-random instances taken from TSPLib. It ap-
  We give an example for n = 4 in which the graph (V, E)
                                                      pears that much of the backbone of a TSP problem is present
has edges between nodes 1 and 2, 3 and 4, and 2 and 4: when we are near to the optimal solution. As heuristics can
                                                      often ﬁnd near optimal solutions in a short time, it may be
                                1
                                  ZZ                  easy to compute a large part of the backbone in practice.
                 1               m    x
                                  
                m               2 Z                   6   Approximation Methods
            2        3             Z
              @@                 m    x               We now see if TSP heuristics can be used as the basis of an
             m        m           
                 4              3                     approximation method for computing the backbone. To com-
                                  ZZ                  pute if an edge is in the backbone or not, we can commit to
                 m               m    x               the edge and compute the optimal tour, and then throw the
                                  
                                4                     edge out and re-compute the optimal tour. The edge is in the
          The TSP Problem   Derivedm graph            backbone iff the ﬁrst tour is shorter in length than the sec-
                                                      ond. Suppose now that instead of computing the two optimal
  If there is a Hamiltonian path in the graph, we follow the tour lengths, we run some good heuristic method like [Lin
corresponding path in the TSP starting with node 1 and ending and Kernighan, 1973] for a polynomially bounded time. If it
at node n. We then exit from node n, and enter the vertical is true that the heuristic is good, it will frequently ﬁnd close
wire in the n − 1th switch, and continue through the vertical to optimal length tours (though we have no guarantee that it
wires to switch 1 where we exit and take the zero cost edge does). This yields a simple approximation method for com-
back to node 1. This completes a tour of length n − 1. On puting edges that are likely to be in the backbone.
the other hand, if there is not a Hamiltonian path, the shortest Tests were run on random Euclidean TSP problems of size
tour visits nodes 1 to n by alternating with the n − 1 switch 100, 250 and 500 nodes. Most problems have unique solu-
circuits. It then takes the zero cost edge from node n back to tions, and hence complete backbones. Exceptions were: 6
node 1. This completes a tour of length n. 2          problems of size 100, 16 of size 250 and 19 of size 500 had
  Similar arguments show that if P 6= NP then no majority- backbones of size < n. Table 1 shows the percentage of the
approximation procedure can be guaranteed to return at least backbone correctly identiﬁed using the heuristic proceedure,
one backbone edge when the backbone is non-empty in poly- as well as the percentage of “false positive” results. The Lin-
nomial time                                           Kernighan heuristic is very accurate at these problem sizes, so                      n = 100  n = 250  n = 500                           Mean            Median
    Ave Approx Ratio  1.0000   1.0002   1.0008             Method    Times/s  Nodes   Time/s  Nodes
    Correct     10%    99.0     69.4     28.8              bb           60.9   685.8     3.0     5.0
      (%)    Median    100.0    100.0    75.8              freqbb       98.5  1618.1     5.0   174.0
                90%    100.0    100.0    100.0             long         34.0   436.8     0.0     4.0
     False      10%     0.0      0.0      0.0              longish      21.8   332.0     0.0     5.0
    Positive Median     0.0      0.0      0.0              next         38.0   642.8     1.0    19.0
      (%)       90%     1.0      0.4      0.6              rand         58.1   980.5     2.0    40.0
                                                           short        61.1   909.8     7.0   113.0
Table 1: Estimation of backbone using the Lin-Kernighan
heuristic. 100 trials of random Euclidean TSP prob-   Table 2: Branching heuristics – ﬁnding the optimal solution
lems at each size.  Approx Ratio is mean (heuristic-
solution)/(optimal-solution) of original problem.                    Mean            Median        limit
                                                       Method    Time/s  Nodes   Time/s  Nodes   exceeded
approximation ratios were small. The heuristic is very good bb    199.1  2559.8    35.0   244.0     12
at identifying backbone edges and gives few false-positives. freqbb 206.5 3284.9   28.5   422.0     14
Its main weakness is missing backbone edges.           long       360.3  4816.7    51.6   755.0     26
                                                       longish     87.7  1946.1    12.5   130.0     4
7  Backbone Guided Heuristics                          next        93.7  1775.3    10.1   108.0     4
                                                       rand       179.1  2859.3    24.4   348.0     10
One motivation for identifying backbone is to try to reduce short 199.8  2921.9    35.8   413.0     11
search. For example, Climer and Zhang use backbone vari-
ables in asymmetric TSP problems to preprocess and simplify
[Climer and Zhang, 2002]. As a second example, Dubois and Table 3: Branching heuristics – proving optimality
Dequen use a backbone guided heuristic to solve 700 variable
                         [                     ]
hard random 3 SAT problems Dubois and Dequen, 2001 . In of each edge is updated. Edges with higher frequency are
the following section, we show that whilst backbone guided more likely to be backbone edges, so the branch choice is the
heuristics can be helpful in ﬁnding optimal solutions, we must edge with the greatest frequency.
use them with care. Median runtime are good, but there are
                                                        next  The next unused edge in the tour is chosen
a few long runs (suggesting that a randomization and restarts
strategy may be useful). In addition, we show both exper- long The longest unused edge in the tour is chosen
imentally and theoretically, that backbone guided heuristics short The shortest unused edge in the tour is chosen
can be very poor when it comes to proving optimality. To longish A method suggested by the results of this test.
make the demonstration very direct, we use a method for Use long for the ﬁrst n branches, then use next.
solving TSP problems that relies heavily on the branching rand A  random edge is chosen.  As this is non-
heuristic. The method does not use the “cuts” that charac- deterministic, the heuristic was run ﬁve times on each prob-
terise state-of-the-art solvers. However, it lets us compare lem.
directly the efﬁcacy of various branching decisions.    A branch-and-bound solver has two phases: ﬁnding the op-
  We use a branch and bound solver with lower bounds pro- timal solution, and proving optimality. Different heuristics do
vided by Lagrangean relation with 1-trees [Reinelt, 1994]. better in different phases, so we report the results separately
Upper bounding is by a single, deterministic run of Or-opt in Table 2, and Table 3. We used 50 node random Euclidean
[Or, 1976] testing all forward and reverse moves of blocks of TSP problems with a time limit of 1000 seconds on each run.
size n/2 down to 1. The MST bound is enhanced by elim- Table 3 shows the number of times this limit was exceeded.
inating all other edges into a node if two incident edges are The time and node results include problems for which the
already forced into the solution. Depth-ﬁrst search is used, time limit was exceeded.
with the node having the least lower bound explored. The These results show a “heavy-tailed” distribution [Gomes et
algorithm branches on an edge currently in the tour repre- al., 2000] with many problems being solved quickly (hence
senting the upper bound. The edge can be chosen in one of a low median) but a few taking a very long time (hence
several ways:                                         a high average). Randomised algorithms and restarts have
  bb  We exclude in turn every possible edge from the solu- been shown to reduce the average time in NP-hard problems
tion. A comparison is then made between the new objective [Meisels and Kaplansky, 2004; Selman et al., 1994]. How-
and the current upper bound. If the new objective is reduced, ever, we wished to look at the “pure” algorithms for compar-
then an improvement to the upper bound has been found. If ison.
such an improving move is found, the edge giving the largest These results show that the bb heuristic is quite effective at
improvement is chosen. If no such improvement is found, ﬁnding the optimal in terms of the median number of nodes
then the edge giving the largest increase in objective is most visited. However, the average is quite large, indicating the
likely to be in the backbone, so that edge is chosen. method could beneﬁt from multiple restarts. This reduced
  freqbb  A frequency table is kept during search. As each number of nodes comes at a price in terms of runtime. Other
new upper-bound tour is created, the frequency of appearence methods, such as long outperform it in runtime. In termsof proving optimality, the bb heuristic performed relatively       4                        4
                                                             20                        20
poorly, being little better than random.                         20 20                     20
                                                          5    2       3                 2      3
                                                                  20               5        20
8  Pathological Example                                 20           30
                                                              10 30                     10
                                                          21
As a further caution to using backbone guided branching 6              10        6              10
                                                               10  1         10         10  1         10
                                                SP          21
heuristics, we present a pathological instance of the T 20     10                        10
problem. If we branch on a non-backbone edge, this instance 21  10                        10
                                                                           20                        20
can be solved in a single branch. However, if we branch   7                        7
                                                           20
ﬁrst on backbone edges, we visit an exponential number of     8    20   9               8         9
branches before proving optimality. Since a search space
                                                                  (a)                      (b)
is deﬁned by a particular algorithm, we must deﬁne a solu-
                                                       TSP problem. Optimal tour The  Minimum   Spanning
tion method. The following algorithm is sensible, but simple
                                                       is 1,2,...,10 – cost 200  Tree – cost 130
enough that we can predict its behavior theoretically. It lacks
the reﬁned LP-based cuts that allow modern TSP codes to
solve huge problems, but is a fairly standard basic algorithm Figure 2: The TSP problem, and its MST
([Reinelt, 1994]). It has the following features:
                                                                   4                        4
 1. Uses branch and bound.                                   20                        20
                                                                 20                        20
 2. Stops when the lower bound equals the upper bound.         2       3                 2      3
                                                          5       20               5        20
                                                        20                        20
 3. Upper bound provided by current best tour.                  30                      10
                                                        6              10        6              10
 4. Lower bound provided by 1-tree relaxation. A 1-tree is         1         10             1         10
    simply a minimum spanning tree (MST) with one extra 20                               10
                                                                10                        10
    edge added from a leaf to another node already in the 7                20      7                20
    tree. The 1-tree contains exactly one cycle, which may 20
    be a complete tour.                                       8    20   9               8         9
 5. If two edges are forced to be incident to a single node,      (a)                      (b)
    then the lower-bounding procedure does not consider Force (1,2) – MST cost 180, Force (5,6) – MST cost 120,
    any other edges incident to that node.             1-tree bound 200          1-tree bound 140
 6. Preprocessing identiﬁes nodes which have only two       Figure 3: Forcing edges. Forced edges in bold.
    “useful” incident edges. These two are forced into the
    solution. A “useful” edge here is deﬁned to be an edge
    with cost less than any available upper bound (an edge of forcing edgec (5,6) into the solution. Forcing (5,6) gives
    with cost greater than an estimate for the entire tour will an MST with cost 140. The 1-tree is connected using an edge
    never be used in an optimal solution). An initial upper like (6,7) or (3,4) with cost 20, giving a lower bound of 160.
    bound can be calculated using the tour (1, 2, 3, ...). If there were other clones of 6, when they were forced into
                                                      the solution, then the lower bound would not approach the
  The TSP problem is given in Figure 2(a). Node 6 can be upper bound until edges between all clones have been forced.
“cloned” arbitrarily many times, and has been cloned at nodes Forcing out (5,6) leaves a problem that looks very similar to
7 and 8. The clones connect to each neighbour at cost 20, the original. It still has backbone edges round the perimeter,
node 1 at cost 10, and all other clones at cost 21. Arcs not but an MST/1-tree which uses the “spoke”-type edges around
shown have cost 1000 + 20n, and hence will never be used in node 1 and hence gives an unachievable lower bound.
an optimal solution. The upper bound for this graph is given On backtracking, forcing out (5,6) gives the same lower
by the tour 1, 2, .. 10, 1 with cost 200. The minimum span- bound as the parent problem. A new upper bound can be
ning tree is shown in Figure 2(b) and has cost 130. The bold found using the cost 21 edges. As the lower-bound is still less
edges (10,1) and (10,9) are ﬁxed in due to the preprocessing. than the upper, we can not stop. A recursive argument shows
The one-tree adds, for example (3,4) at cost 20. The lower that the right branch must be completely explored before it
bound is therefore 150.                               can be excluded. It is not until all backbone edges are forced
  Forcing the non-backbone edge (1,2) excludes the “spoke” in, or forced out, and the algorithm moves on to branch on
edges at nodes 1, as node 1 now has 2 edges incident. The edge (1,2) or (1,3) that the optimal is deterimend. Hence the
MST (shown in Figure 3(a)) now has cost 180, and the 1-tree backbone heursitic would need to visit an exponential number
bound (using edge (8,9) or (3,4) to connect) is 200. As the of branches to solve the problem.
lower bound is the same as the upper bound, the branch and
bound algorithm completes. So if (1,2) (or, by a symmetric
argument (1,3)) were chosen as the branch node, the proce- 9 Related Work
dure terminates immediately. On the other hand, branching Slaney and Walsh demonstrated that the cost of ﬁnding op-
on backbone edges would force in edges like (5,6). If 6 were timal (or near optimal) solutions is postively correlated with
cloned as described above, there could be arbitrarily many of backbone size [Slaney and Walsh, 2001]. However, they also
these edges. Figure 3(b) shows the effect on the lower bound showed that the cost of proving optimality is negatively cor-