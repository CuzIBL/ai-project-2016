               Parametric Distance Metric Learning with Label Information* 

                              Zhihua Zhang, James T. Kwok and Dit-Yan Yeung 
                                Hong Kong University of Science and Technology 
                                      Clear Water Bay, Kowloon, Hong Kong 
                                      {zhzhang, jamesk, dyyeung}@cs.ust.hk 


                        Abstract                               of our method are summarized as follows. Using class la•
                                                              bel information, we define a similarity measure (and hence 
     Distance-based methods in pattern recognition and 
                                                               also the corresponding dissimilarity measure) between pat•
     machine learning have to rely on a similarity or 
                                                              terns in the input space. The dissimilarity measure implicitly 
     dissimilarity measure between patterns in the in•
                                                               induces a metric space for embedding the original patterns. 
     put space. For many applications, Euclidean dis•
                                                              To explicitly represent the mapping from the input space to 
     tance in the input space is not a good choice and 
                                                              the feature space, we then approximate the mapping by a re•
     hence more complicated distance metrics have to 
                                                              gression model to embed the original patterns in an Euclidean 
     be used. In this paper, we propose a parametric 
                                                               space. The regression parameters are estimated from data 
     method for metric learning based on class label in•
                                                              with the objective that the dissimilarity between patterns in 
     formation. We first define a dissimilarity measure 
                                                              the input space is approximated by the Euclidean distance be•
     that can be proved to be metric. It has the favorable 
                                                              tween points in the feature space. Once the regression model 
     property that between-class dissimilarity is always 
                                                              has been found, any new pattern can be mapped to its corre•
     larger than within-class dissimilarity. We then per•
                                                              sponding location in the feature space. Distance-based meth•
     form parametric learning to find a regression map•
                                                              ods, such as k-means clustering, nearest neighbor classifiers 
     ping from the input space to a feature space, such 
                                                              and support vector machines, can then be applied in the fea•
     that the dissimilarity between patterns in the in•
                                                              ture space for clustering or classification applications. 
     put space is approximated by the Euclidean dis•
     tance between points in the feature space. Para•            The rest of this paper is organized as follows. A modified 
     metric learning is performed using the iterative ma-     metric incorporating class label information is proposed in 
     jorization algorithm. Experimental results on real-      Section 2. Section 3 outlines our regression model for metric 
     world benchmark data sets show that this approach        learning and the corresponding optimization method. Exper•
     is promising.                                            imental results are presented in Section 4, and the last section 
                                                              gives some concluding remarks. 

1 Introduction                                                2 Modified Metric with Label Information 

The notion of similarity or dissimilarity plays a fundamental Denote the input space by Rq and the set of all C possible 
role in pattern recognition and machine learning. A promis•   class (target) labels by T. A training set has 
ing direction to pursue is to learn good (dis)similarity mea• n patterns where U = r if pattern i 
sures from data. Recently, learning distance metrics from     belongs to class r. Here, each pattern is assumed to belong to 
data has aroused a great deal of interest from machine learn• only one class. In general, a number of similarity measures 
ing researchers. One typically wants to embed patterns in     can be defined on these patterns [Gower and Legendre, 1986]. 
a (possibly non-metric) input space into a feature space, in  In this paper, we utilize also the label information in defining 
which the Euclidean distance between points accurately re•    the similarity sij between patterns xi and Xj: 
flects the dissimilarity between the corresponding patterns. 
Therefore the (linear or nonlinear) mapping from the input 
space to the feature space corresponds to feature extraction.                                                         (1) 
Alternatively, the feature space may be a low-dimensional 
space for data visualization. 
                                                              where denotes the Euclidean norm and B > 0 is a width 
  In this paper, we propose a parametric distance metric      parameter. The corresponding dissimilarity Sij is then: 
learning method in the supervised setting. The main ideas 
   This research has been partially supported by the Re•
search Grants Council of the Hong Kong Special Administra•
tive Region under grants DAG01/02.EG28, HKUST2033/00E and                                                             (2) 
HKUST6195/02E. 


1450                                                                                                  POSTER PAPERS    As illustrated in Figure 1, this (dis)similarity measure en• (tl). Moreover, the distance metric dtJ, like the associated 
joys some nice properties for pattern discrimination. For ex• enjoys those nice properties useful for pattern discrimination. 
 ample, the (dis)similarity between any two patterns in the 
 same class is always larger (smaller) than that between any  3 Metric Learning with Regression Model 
 two patterns belonging to different classes. Moreover, the 
 larger the Euclidean distance between the patterns is, the   As mentioned in Section 2, an approximate solution for 
 smaller is the within-class similarity while the larger is the             can be obtained by using MDS. However, this 
 between-class similarity.                                    may be intractable for large data sets. Moreover, for new pat•
                                                              terns with unknown labels, the problem then is on how to 
                                                              determine sl3 in the first place. Following [Koontz and Fuku-
                                                              naga, 1972; Cox and Ferry, 1993; Webb, 1995], we attempt 
                                                              to find a mapping from x, in the original input space to 
                                                                 in the embedded Euclidean space One possibility is to 
                                                              first obtain a MDS configuration, and then construct a regres•
                                                              sion model from xt to iCox and Ferry, 1993]. However, 
                                                              this mapping is not determined as part of the MDS procedure 
                                                              [Webb, 1995]. In the following, we will follow the approach 
                                                              of [Webb, 19951. 
                                                                 Denote the mapping from the original input space to the 
                                                              embedded Euclidean space by f — (f1,..., fi)'. Assume 
                                                              that each fi is a linear combination of p basis functions: 

                                                                                                                     (3) 

     Figure 1: Similarity and dissimilarity in (1) and (2).   where W = contains the free parameters, and the 
                                                                      are basis functions that can be linear or nonlinear. 
   In recent years, finite metric spaces and their embeddings The regression mapping (3) can be written in matrix form as 
have received much attention [Indyk, 2001; Linial et al, 
 1995]. Among embedding into normed spaces, embedding 
into an Euclidean space is the most popular.                  where Let X be the target con•
   Given the dissimilarity matrix we are in•                  figuration, with where is defined in (2). 
terested in the question of whether and how the dissimilarity Using the iterative majorization algorithm, we then minimize 
matrix can be embedded. In other words, for the origi•        the squared error 
nal points we attempt to find a configuration 
of points in some Euclidean space such that 
the squared distances between these points will be equal to                                                          (4) 
the so-defined dissimilarities  
   The following theorem confirms that can be embedded. 
Theorem 1 Define d ij ~ with as in (2). The matrix 
D — is metric. In other words, satisfies the following 
properties:                                                   4 Experiments 
                                                              In this Section, we perform experiments on six benchmark 
                                                              data sets (Table 1) from the UCI repository [Murphy and Aha, 
                                                              1994]. The distance metric is learned using a small subset of 
                                                              the labeled patterns, with / = p = q, = x and the width 
                                                                in (1) set to the average distance of the labeled patterns to 
The proof of this theorem can be found in IZhang et al,       the class means. The remaining patterns are then used for 
2003]. The subsequent task is then to find the embedding,     testing. 
i.e., points such that the inter-point dis•                     Table 2 shows the classification results by the nearest mean 
tance is equal to dij In general, obtaining an exact solution and nearest neighbor classifiers, with both the Euclidean and 
for these xi's is difficult. Nevertheless, because D is met•  learned metrics. As can be seen, the learned metric almost 
ric, an approximate solution can be easily obtained by using  always outperforms the original metric. 
principal coordinate analysis or other multidimensional scal•   Next, we perform clustering experiments using the A;-
ing (MDS) methods [Cox and Cox, 2000]. We will return to      means clustering algorithm, with the value of k set to the true 
this problem in Section 3.                                    number of clusters in each data set. The clustered patterns 
   Notice that the resultant Euclidean embedding will still in• are assigned labels and the clustering accuracy is measured 
corporate information from both the input space representa•   by comparing these labels with the true labels (as in classi•
tion (Xi) of the patterns and their corresponding class labels fication problems). As these cluster labels can be permuted 


POSTER PAPERS                                                                                                      1451    Table 1: The six UCI data sets used in the experiments.    Table 3: Clustering accuracies on the UCI data sets (Numbers 
                                                              in bold indicate the better results). 
       data set       total # of patterns  # patterns for 
                      class      size     metric learning             data set    Euclidean metric   learned metric 
    Pima Indians        1        500            80                    diabetes        459/638           480/638 
  diabetes (diabetes)   2        268            50                    soybean          37/37             37/37 
       soybean          1        10              2                     wine           85/118            117/118 
                        2        10              2                     WBC            412/469           446/469 
                        3        10             2                   ionosphere        168/251           221/251 
                        4        17             4                       iris          107/120           110/120 
        wine            1        59             20 
                        2        71             20 
                        3        48             20            References 
  Wisconsin breast      1        212            50 
                                                              [Cox and Cox, 2000] T.F. Cox and M.A.A. Cox. Multidi•
    cancer (WBC)        2        357            50 
                                                                 mensional Scaling. Chapman & Hall/CRC, second edition, 
     ionosphere         1        126            50 
                                                                 2000. 
                        2        225            50 
         iris           1        50             10            [Cox and Ferry, 1993] T.F. Cox and G. Ferry. Discriminant 
                        2        50             10               analysis using non-metric multidimensional scaling. Pat-
                        3        50             10               tern Recognition, 26(1): 145-153, 1993. 
                                                              [Gower and Legendre, 1986] J.C. Gower and P. Legendre. 
                                                                 Metric and Euclidean properties of dissimilarities coeffi•
without changing the clustering solution, results reported here  cients. Journal of Classification, 3:5-48, 1986. 
arc based on the labeling with the highest clustering accuracy. [Indyk, 2001] P. Indyk. Algorithmic applications of low-
As can be seen from Table 3, the learned metric outperforms      distortion geometric embeddings. In Proceedings of the 
that with the original metric on all data sets.                  42nd Annual IEEE Symposium on Foundations of Com•
                                                                 puter Science, pages 10-33, 2001. 

Table 2: Classification accuracies on the UCI data sets (Num• [Koontzand Fukunaga, 1972] VV.L.G. Koontz and K. Fuku-
bers in bold indicate the better results).                       naga. A nonlinear feature extraction algorithm using dis•
                                                                 tance information. IEEE Transactions on Computers, 
                                                                 21(l):56-63, 1972. 
   data set  1 nearest mean          | nearest neighbor 
               Euclidean    learned    Euclidean   learned    [Lini-dl etal, 1995] N. Linial, E. London, and Y. Rabi-
             1 metric       metric   1 metric       metric       novich. The geometry of graphs and some of its algorith•
                                                                 mic applications. Combinatorica, 15(2):215-245, 1995. 
   diabetes 1 1 463/638    475/638 1   432/638     425/638 
   soybean       36/37       37/37       35/37      37/37     [Murphy and Aha, 1994] P.M. Murphy and D.W. Aha. 
    wine        86/118      115/118     77/118     117/118       UCI repository of machine learning databases. 
    WBC         430/469    451/469     420/469     453/469       http://www.ics.uci.edu/~mlearn/MLRepository.html, 
 ionosphere     159/251    201/251     212/251     225/251       1994. 
     iris       108/120    110/120     114/120     114/120    [Scholkopf, 2002] B. Scholkopf and A.J. Smola. Learning 
                                                                 with Kernels. MIT Press, 2002. 
                                                              [Webb, 1995] A.R. Webb. Multidimensional scaling by it•
                                                                 erative majorization using radial basis functions. Pattern 
5 Concluding Remarks                                             Recognition, 28(5):753-759, 1995. 
In this paper, we proposed a new parametric method for dis•   [Zhang et al, 2003] Z. Zhang, J.T. Kwok and D.Y. Yc-
tance metric learning based on class label information. Ex•      ung. Parametric distance metric with label information. 
periments on UCI data sets show promising results.               ftp://ftp.cs.ust.hk/pub/techreport/03/tr03-02.ps.gz, 2003. 
  The current work can be extended in several directions. 
First, nonlinear basis functions can be used to improve the 
approximation power of the regression mapping. Second, al•
though Theorem 1 states that the dissimilarity measure in•
duces a metric, it is not clear whether the matrix is also Eu•
clidean. If this is the case, a new kernel can then be defined 
on the joint space of the input space and class label space 
[Scholkopf, 2002]. Third, in addition to using label infor•
mation, we will also incorporate manifold structure between 
neighboring patterns into our metric learning process. 


1452                                                                                                  POSTER PAPERS 