               Improving Word Sense Disambiguation in Lexical Chaining 

                                Michel Galley and Kathleen McKeown 
                                           Columbia University 
                                    Department of Computer Science 
                                       New York, NY 10027, USA 
                               {galley,kathy}@cs.Columbia.edu 

                     Abstract                            In this paper, we further investigate the automatic identifi•
                                                       cation of lexical chains for subsequent use as an intermediate 
    Previous algorithms to compute lexical chains suf• representation of text. In the next section, we propose a new 
    fer either from a lack of accuracy in word sense   algorithm that runs in linear time and adopts the assumption 
    disambiguation (WSD) or from computational in•     of one sense per discourse [Gale et al., 19921. We suggest 
    efficiency. In this paper, we present a new linear- that separating WSD from the actual chaining of words can 
    time algorithm for lexical chaining that adopts the increase the quality of chains. In the last section, we present 
    assumption of one sense per discourse. Our results an evaluation of the lexical chaining algorithm proposed in 
    show an improvement over previous algorithms       this paper, and compare it against [Barzilay and Elhadad, 
    when evaluated on a WSD task.                      1997; Silber and McCoy, 2003] for the task of WSD. This 
                                                       evaluation shows that our algorithm performs significantly 
1 Introduction                                         better than the other two. 
Passages from spoken or written text have a quality of unity 
that arises in part from the surface properties of the text; 2 Lexical Chaining with a Word Sense 
syntactic and lexical devices can be used to create a sense of Disambiguation Methodology 
connectedness between sentences, a phenomenon known as 
textual cohesion [Halliday and Hasan, 1976]. Of all cohesion Our algorithm uses WordNet [Miller, 1990] as a knowledge 
devices, lexical cohesion is probably the most amenable source to build chains of candidate words (nouns) that are 
to automatic identification [Hoey, 1991]. Lexical cohesion related semantically. We assign a weight to each semantic 
arises when words are related semantically, for example relation; in our work semantic relations are restricted to 
in reiteration relations between a term and a synonym or hypernyms/hyponyms (e.g. between cat and feline) and 
superordinate.                                         siblings (hyponyms of hypernyms, e.g. dog and wolf). 
  Lexical chaining is the process of connecting semantically Distance factors for each type of semantic relation prevent 
related words, creating a set of chains that represent different the linkage of words that are too far apart in the text; these 
threads of cohesion through the text. This intermediate factors are summarized in Table 1. 
representation of text has been used in many natural language The algorithm can be decomposed into three steps: build•
processing applications, including automatic summarization ing a representation of all possible interpretations of the text, 
[Barzilay and Elhadad, 1997; Silber and McCoy, 2003 ], infor• disambiguating all words, and finally building the lexical 
mation retrieval [Al-Halimi and Kazman, 1998], intelligent chains. First, similarly to [Silber and McCoy, 2003], we 
spell checking [Hirst and St-Onge, 1998], topic segmentation process the whole text and collect all semantic relations 
[Kan et al, 1998], and hypertext construction [Green, 1998]. between candidate words under any of their respective senses. 
  A first computational model of lexical chains was in• No disambiguation is done at this point; the only purpose is to 
troduced by Hirst and St-Onge [1998]. This linear-time build a representation used in the next stages of the algorithm. 
algorithm, however, suffers from inaccurate WSD, since their Note that this is the only stage where the text is read; all 
greedy strategy immediately disambiguates a word as it is later stages work on this implicit representation of possible 
first encountered. Later research [Barzilay and Elhadad, interpretations, called a disambiguation graph (Figure 1). 
1997] significantly alleviated this problem at the cost of a In this kind of graph, nodes represent word instances and 
worse running time (quadratic); computational inefficiency weighted edges represent semantic relations. Since WordNet 
is due to their processing of many possible combinations of doesn't relate words but senses, each node is divided into as 
word senses in the text in order to decide which assignment is many senses as the noun has, and each edge connects exactly 
the most likely. More recently, Silber and McCoy [2003] pre• two noun senses. 
sented an efficient linear-time algorithm to compute lexical This representation can be built in linear time by first build•
chains, which models Barzilay's approach, but nonetheless ing an array indexed by senses of WordNet and processing a 
has inaccuracies in WSD.                               text sequentially, inserting a copy of each candidate word into 


1486                                                                                      POSTER PAPERS         Semantic relation    1 sent.   3 sent.   1 par.  other           discourse. We perform the disambiguation of every word, 
                 synonym        1 
                                          1       0.5     0.5            instead of disambiguating word occurrences as in e.g. [Hirst 
      hypernym/hyponym          1        0.5      0.3     0.3 
                                                                         and St-Onge, 1998; Silber and McCoy, 2003]. We process 
                   sibling      1        0.3      0.2      0 
                                                                         all occurrences (nodes) of one word at a time, and sum the 
                                                                         weight of all edges leaving these nodes under their different 
Table 1: Weights of relations depending on the kind of 
                                                                         senses. The one sense of the word that is assigned the highest 
semantic relationship and distance (in number of sentences 
or paragraphs) between two words.                                        score (sum of weights) is considered the most probable sense. 
                                                                        For example in Figure 1, the sense of bank that has the 
                                                                        highest score is financial institution. That sense is assigned 
                                                                        to all occurrences of the word; in other words, we impose the 
                                                                        constraint of one sense per discourse. In case of a tie between 
                                                                        two or more senses, we select the one sense that comes first 
                                                                        in WordNet, since WordNet orders the senses of a word by 
                                                                        decreasing order of frequency. 
                                                                           The final step is to build the actual lexical chains by 
                                                                        processing the entire disambiguation graph. At this point, we 
                                                                        have already assigned a sense to each word, so the last step is 
                                                                        to remove from the disambiguation graph all semantic links 
                                                                        that connect words taken under their (assumed) wrong senses. 
                                                                        Once all such edges have been removed, we are left with the 
                                                                        semantic links corresponding to a unique interpretation of the 
                                                                        text, and the edges that remain in the graph are the actual 
Figure 1: A disambiguation graph, an implicit representation 
                                                                        lexical chains of our algorithm.
of word-sense combinations (in this example, all weights are                                               l 
                                                                           The separation of WSD and lexical chaining into two 
equal to 1.) 
                                                                        different sub-tasks is important. All semantic relations, 
                                                                        whether correct or incorrect, can be investigated in WSD 
all entries that are valid senses of this word (for example, in         without necessarily creating incorrect semantic relations in 
Figure 2, the instances car and auto have been inserted under           the chaining process. Words are disambiguated by summing 
the same sense in the array). Then, wc check whether the                weights of semantic relations, but mistakenly counting edges 
noun instance that was just inserted is semantically related            relating words under wrong senses (as in Figure 2 between 
to other nouns already present in the array. We do so by                fall and bank) doesn't necessarily have the undesirable effect 
looking at hypernyms, hyponyms, and siblings, and if any                of linking the two words in the same chain. Our assumption 
of these related senses have non-empty entries in the array,            is that summing edge weights generally helps in selecting the 
we create the appropriate links in the disambiguation graph.            right senses, e.g. bank is disambiguated as a financial institu•
For example, in Figure 2, the algorithm finds an hyponymy               tion, 'and fall and bank are thus prevented from appearing in 
relation between the noun taxi (under its unique sense in               the same chain. 
the array) and another entry in the array containing car and 
auto, so semantic links are added to the disambiguation graph           3 Evaluation 
between these two words and taxi (shown here attached to 
the array). Processing all nouns this way, we can create                The evaluation of lexical chains is generally difficult. Even 
all semantic links in the disambiguation graph in time 0{n)             if they can be effectively used in many practical applica•
(where n is the number of candidate words.)                             tions like automatic summarization, topic segmentation, and 
   In the second step, we use the disambiguation graph to               others, lexical chains are seldom desirable outputs in a real-
perform WSD, enforcing the constraint of one sense per                  world application, and it is unclear how to assess their quality 
                                                                        independently of the underlying application in which they are 
                                                                        used. For example, in summarization, it is hard to determine 
                                                                        whether a good or bad performance comes from the efficiency 
                                                                        of the lexical chaining algorithm or from the appropriateness 
                                                                        of using lexical chains in that kind of application. In 
                                                                        this section, we evaluate lexical chaining algorithms on the 
                                                                        basis of WSD. This arguably is independent of any targeted 

                                                                            'Our algorithm has some similarities with Silber and McCoy's 
                                                                        algorithm, but it is actually quite different. First, they process 
                                                                        each noun instance separately; thus, nothing prevents a noun from 
                                                                        having different senses in the same discourse. Second, they process 
                                                                        the entire text twice instead of once. In the second pass of their 
                                                                        algorithm, they perform WSD and the actual chaining at the same 
Figure 2: First pass of the algorithm: using an array, we can           time, whereas we postpone the chaining process until each word has 
build the disambiguation graph in linear time.                          been fully disambiguated. 


POSTER PAPERS                                                                                                                         1487                             Algorithm      Accuracy 
               Barzilay and Elhadad          56.56% 
                   Silber and McCoy          54.48% 
               Galley and McKeown            62.09% 

    Table 2: Accuracy of noun disambiguation on semcor. 


application, since any lexical chaining algorithm has to deal 
with the problem of WSD. We do not attempt to further 
evaluate other aspects of chains. 
   We tested three lexical chaining algorithms on the semantic 
concordance corpus (semcor), a corpus that was extracted 
from the Brown Corpus and semantically tagged with Word-
Net senses. We limited our evaluation to a set of 74 
documents of that corpus; this represents about 35,000 nouns. 
WSD was evaluated on nouns, since all three algorithms that 
                                                                           Figure 3: Accuracy by polysemy of the three algorithms. 
were tested (Barzilay and Elhadad; Silber and McCoy, and 
ours) build lexical chains with nouns only. We used the 
original implementation of Barzilay and Elhadad's algorithm,             References 
but had to implement Silber and McCoy's algorithm since                  FAl-Halimi and Kazman, 1998] R. Al-Halimi and R. Kaz-
we didn't have access to their source code. We tested the                   man. Temporal indexing of video through lexical chaining. 
accuracy of WSD on the set of 35,000 nouns and obtained                     In WordNet: An electronic lexical database. MIT Press, 
the results presented in Table 2;2 accuracy by polysemy                     1998. 
is displayed in Figure 3. We can see that our algorithm 
                                                                         IBarzilay and Elhadad, 1997] R. Barzilay and M. Elhadad. 
outperforms Barzilay and Elhadad's, and a one-sided t-test3 
                                                                            Using lexical chains for text summarization. In Proc. 
of the null hypothesis of equal means shows significance at 
                                                                            of the Intelligent Scalable Text Summarization Workshop 
the .001 level (p = 4.52 • 10 -4). Barzilay and Elhadad in turn 
                                                                            (ISTS'97),ACL, 1997. 
outperform Silber and McCoy, but this result is not significant 
at the basic .05 level (p = 0.0537).                                     IBarzilay, 1997] R. Barzilay. Lexical chains for summariza•
                                                                            tion. Master's thesis, Ben-Gurion University, 1997. 
                                                                         [Gal et al., 1992] W. Gale, K. Church, and D. Yarowsky. 
4 Conclusions                                                               One sense per discourse. In Proc. of the DARPA Speech 
                                                                            and Natural Language Workshop, 1992. 
In this paper, we presented an efficient linear-time algo•
rithm to build lexical chains, showing that one sense per                [Green, 1998] S. Green. Automated link generation: Can 
discourse can improve performance. We explained how                         we do better than term repetition? In Proc. of the 7th 
the separation of WSD from the construction of the chains                   International World-Wide Web Conference, 1998. 
enables a simplification of the task and improves running                [Halliday and Hasan, 1976] M. Halliday and R. Hasan. Co•
time. The evaluation of our algorithm against two known                     hesion in English. Longman, London, 1976. 
lexical chaining algorithms shows that our algorithm is more 
                                                                         [Hirst and St-Onge, 1998] G. Hirst and D. St-Onge. Lexical 
accurate when it chooses the senses of nouns to include 
                                                                            chains as representations of context for the detection and 
in lexical chains. The implementation of our algorithm 
                                                                            correction of malapropisms. In WordNet: An electronic-
is freely available for educational or research purposes at 
                                                                            lexical database. MIT Press, 1998. 
http://www.cs.columbia.edurgalley/research.html. 
                                                                         [Hoey, 1991] M. Hoey. Patterns of lexis in text. Oxford 
                                                                            University Press, 1991. 
Acknowledgments                                                          (Kan et al, 1998] M.-Y. Kan, J. Klavans, and K. McKeown. 
                                                                            Linear segmentation and segment significance. In Proc. 
We thank Regina Barzilay, the three anonymous reviewers, 
                                                                            of the 6th Workshop on Very Large Corpora (WVLC-98), 
and the Columbia natural language group members for help•
                                                                            1998. 
ful advice and comments. 
                                                                         [Miller, 1990] G. Miller. WordNet: An on-line lexi•
                                                                            cal database. International Journal of Lexicography, 
   2In Barzilay and Elhadad's algorithm, a word can sometimes 
belong to two different chains. In order to map each word to                3(4):235-312,1990. 
one single sense, we applied the strong chain sense disambiguation       [Silber and McCoy, 2003] G. Silber and K. McCoy. Ef•
strategy described in [Barzilay, 1997] (i.e. picking the word sense         ficiently computed lexical chains as an intermediate 
used in the strongest chain).                                               representation for automatic text summarization. Compu•
   3The samples in the t-test arc the WSD accuracies on each                tational Linguistics, 29(1), 2003. 
individual documents. 


1488                                                                                                                   POSTER PAPERS 