                            Repairing Concavities in ROC Curves

                      Peter A. Flach                                Shaomin Wu
             Department of Computer Science             School of Construction Management
                   University of Bristol               and Engineering, University of Reading
                 Bristol, United Kingdom                      Reading, United Kingdom
           Peter.Flach@bristol.ac.uk                     Shaomin.Wu@reading.ac.uk

                    Abstract                          curve, which aggregates its behaviour for all possible deci-
                                                      sion thresholds. The quality of a probabilistic classiﬁer can be
    In this paper we investigate methods to detect and measured by the Area Under the ROC Curve (AUC), which
    repair concavities in ROC curves by manipulating  measures how well the classiﬁer separates the two classes
    model predictions. The basic idea is that, if a point without reference to a decision threshold. A good classiﬁer
    or a set of points lies below the line spanned by two should have a large AUC, and AUC=1 means that there is
    other points in ROC space, we can use this informa- a decision threshold such that the corresponding categorical
    tion to repair the concavity. This effectively builds classiﬁer has 100% accuracy.
    a hybrid model combining the two better models      We use the term model repair to denote approaches that
    with an inversion of the poorer models; in the case modify given models in order to obtain better models. In
    of ranking classiﬁers, it means that certain inter- contrast, ensemble methods produce hybrid models that leave
    vals of the scores are identiﬁed as unreliable and the original models intact. An approach to model construc-
    candidates for inversion. We report very encour-  tion using ROC space is given in [Blockeel and Struyf, 2002],
    aging results on 23 UCI data sets, particularly for where the authors identify and assemble parts of a decision
    naive Bayes where the use of two validation folds tree that perform well in different areas of ROC space. Our
    yielded signiﬁcant improvements on more than half approach in this paper is to identify ‘bad’ areas, or concav-
    of them, with only one loss.                      ities, in a ROC curve and repair them by manipulating the
                                                      corresponding low-quality predictions. The approach is ex-
                                                      perimentally validated using both naive Bayes and decision
1  Introduction                                       tree, but the approach has much wider scope as it can be ap-
There is an increasing amount of work on model selection and plied to any classiﬁer that computes class scores.
model combination in the machine learning and data mining To illustrate the approach, we describe in Section 2 the Re-
literature: for instance, model selection based on ROC space pairPoint algorithm that combines three models based on dif-
[Provost and Fawcett, 2001], model combination by means ferent thresholds of the same probabilistic classiﬁer, and cre-
of bagging [Breiman, 1996], boosting [Freund and Schapire, ates a new model which theoretically should improve upon
1996], arcing [Breiman, 1998], the mixture of experts method the worst of the three models. In Section 3 we introduce the
[Jacobs et al., 1991], to name just a few. A review on en- main algorithm RepairSection, that mirrors an entire con-
sembles of learning machines can be found in [Valentini and cave region (a region of the curve that is below its convex
Masulli, 2002].                                       hull). In Section 4 we present experimental results on 23
  Typically, these methods assume a set of given models with data sets from the UCI repository. Section 5 reviews some
ﬁxed performance, and the issue is how best to combine these related work on model ensembles, gives the main conclusion
models to obtain a better ensemble model. There is no at- and suggests further work.
tempt to analyse the performance of the given models to de-
termine a region where performance is sub-standard. This pa- 2 Basics of repairing classiﬁers in ROC space
per investigates methods to improve given models using ROC
analysis.                                             Assume that the confusion matrix of a classiﬁer evaluated on
  ROC (Receiver Operating Characteristic) analysis is usu- a test set is as in Table 1. Then the true positive rate of the
ally associated with classiﬁer selection when both class and classiﬁer is a/(a + b) and the false positive rate of the classi-
misclassiﬁcation cost distribution are unknown at training ﬁer is c/(c + d). The point (c/(c + d),a/(a + b)) in the XY
time. However, ROC analysis has a much broader scope  plane (i.e., ROC space) will be used to represent the perfor-
that is not limited to cost-sensitive classiﬁcation. A categor- mance of this classiﬁer.
ical classiﬁer is mapped to a point in ROC space by means If a model is under the ascending diagonal in ROC space,
of its false positive rate on the X-axis and its true positive this means that it performs worse than random. Models
rate on the Y-axis. A probabilistic classiﬁer results in a ROC A and B in Figure 1 are such worse-than-random models.                 predicted positive predicted negative  Given three models Model 1, Model 2 and Model 3, output
    actual positive    a               b                Model 4 that operates as follows:
    actual negative    c               d                1.  If both Model 1 and Model 2 predict negative, then pre-
                                                            dict negative;
             Table 1: A confusion matrix.               2.  If both Model 1 and Model 2 predict positive, then predict
                                                            positive;
                                                        3.  If Model 1 predicts negative and Model 2 predicts posi-
However, there is a very useful trick to obtain better-than- tive, then predict the opposite of what Model 3 predicts;
random models: simply invert all predictions of the origi- 4. Otherwise, predict what Model 3 predicts.
nal model. This corresponds to exchanging the columns in
the contingency table, leading to a new true positive rate of Table 2: Algorithm RepairPoint. The last clause does not
b/(a + b) = 1 − a/(a + b), i.e. one minus the original true apply under the inclusion constraints and is only added for
positive rate; similarly we obtain a new false positive rate of completeness.
d/(c + d) = 1 − c/(c + d). Geometrically, this corresponds
to mirroring the original ROC point through the midpoint on
the ascending diagonal.                               siﬁed negative by Model 1 and positive by Models 3 and 2
                                                      ((TP3 − TP1) ∪ (FP3 − FP1)); those classiﬁed negative by
                                                      Models 1 and 3 and positive by Model 2 ((TP2 − TP3) ∪
                                                      (FP2 − FP3)); and those classiﬁed negative by all three mod-
                                                      els ((POS − TP2) ∪ (NEG − FP2), where POS and NEG are
                                                      the sets of all positive and all negative examples, respec-
                                                      tively). By construction, Model 4 classiﬁes the ﬁrst group as
                                                      positive, the second group as negative, the third group as pos-
                                                      itive, and the fourth group as negative. The true positives of
                                                      Model 4 are thus TP1 ∪(TP2 −TP3); because of the inclusion
                                                      constraints the result follows (analogous for false positives).


Figure 1: By inverting their predictions, worse-than-random
models A and B below the diagonal can be transformed into
better-than-random models -A and -B above the diagonal.

  Notice that the ascending diagonal really connects two
classiﬁers: the classiﬁer which always predicts negative in
(0,0), and the classiﬁer which always predicts positive in
(1,1). This suggests that the above repair procedure can be
generalised to line segments connecting arbitrary classiﬁers.
For instance, consider Figure 2. Denote the sets of true and
false positives of Model i by TPi and FPi, then we can con- Figure 2: Model 3 is mirrored to Model 4 with the help of
struct Model 4 under the condition that TP1 ⊆ TP3 ⊆ TP2 Models 1 and 2.
and FP1 ⊆ FP3 ⊆ FP2. In particular, these inclusion con-
straints are satisﬁed if Models 1, 2 and 3 are obtained by An equivalent construction is the following. Remove from
setting thresholds on the same probabilistic model, which is the test set all instances classiﬁed positive by Model 1, and all
what we assume throughout the paper.                  instances classiﬁed negative by Model 2. We can imagine this
  Model 4 operates as indicated in Table 2. The inclusion as a smaller nested ROC space in which Model 1 represents
constraints guarantee that the geometric conﬁguration of Fig- (0,0) and Model 2 represents (1,1); because of the inclusion
ure 2 holds. This is formally stated in the following theorem. constraints the position of Model 3 remains unchanged. Note
                                                      that Model 3 performs worse than a random model in this
Theorem 1 Assuming that TP1 ⊆ TP3 ⊆ TP2  and FP1 ⊆    nested ROC space. We then construct Model 4 as in Figure 1,
FP3 ⊆ FP2, the model produced by Algorithm RepairPoint by inverting the predictions of Model 3 on the remaining test
has true and false positive rates TPr4 = TPr1 +TPr2 −TPr3 examples.
and FPr4 = FPr1 + FPr2 − FPr3, where TPri and FPri de-  We end this section by noting that the true and false posi-
note true and false positive rates of Model i.        tive rates derived for Model 4 only hold for the same test set
Proof. Under the inclusion constraints expressed in the the- on which Models 1, 2 and 3 were evaluated. On a second, in-
orem, there are four disjoint groups of examples: those clas- dependent test set the ROC locations of the 4 classiﬁers may
siﬁed positive by Models 1, 3 and 2 (TP1 ∪ FP1); those clas- be different – in particular, Model 3 may not be below theline connecting Models 1 and 2, in which case Model 4 will score calculated by the probabilistic model in this interval,
be evaluated worse than Model 3. In our experiments, we and output a constant score (e.g., the mid-point of the inter-
therefore use validation sets to decide whether concavities are val). Assuming that ties are broken by assigning a random
stable across different samples. This will be further discussed rank, this would replace the concave region of the ROC curve
in Section 4.                                         with the line segment cd. It is interesting to note that if this
                                                      procedure is followed for all concavities, this corresponds to
3  Identifying and repairing concavities in a         constructing the convex hull by discretising the probability
   ROC curve                                          scores.
                                                        However, in theory we should be able to do better than that:
The previous section outlined the main ideas underlying re-
                                                      we can invert the ranking of the instances in the probability
pairing concavities in ROC curves. However, preliminary ex-
                                                      interval, which can be seen as applying Algorithm Repair-
periments indicated that the three-point approach is too crude
                                                      Point to all thresholds in this interval. For instance, by ap-
to work well in practice. In this section we introduce our
                                                      plying this algorithm to the model corresponding to threshold
main algorithm, which manipulates a whole section of a ROC
                                                      c2, this point would be point-mirrored through the midpoint
curve. A ROC curve is obtained by evaluating a probabilistic
                                                      on line segment cd to the other side of the convex hull. The
classiﬁer on a test set and varying the decision threshold, re-
                                                      same can be done for the other thresholds. The resulting ROC
sulting in a step curve [Hand and Till, 2001]. An efﬁcient
                                                      curve is shown in Figure 4. We can note that the area C under
way of constructing this curve is by ranking the instances
                                                      the curve has been replaced by an equally large area C0 above
corresponding to their predicted probability of being positive
                                                      the curve. The AUC of the repaired curve is therefore larger
[Fawcett, 2003]. Figure 3 shows both the ROC curve for a
                                                      than both the AUC of the original curve and the AUC of its
probabilistic model evaluated on a small test set1 and its con-
                                                      convex hull.
vex hull [Provost and Fawcett, 2001].


  Figure 3: A probabilistic ROC curve and its convex hull. Figure 4: Mirroring a concave part of the ROC curve.

  Four points of the ROC curve (point a, b, c and d) are lo-
cated on the convex hull. Each of these points corresponds The algorithm to produce the model with the repaired ROC
to a probability threshold, and thus each segment of the con- curve is given in Table 3. The procedure works for any model
vex hull corresponds to a probability interval. For instance, that calculates a score; a probabilistic model is a special case.
the convex hull in Figure 3 has three segments corresponding
to three disjoint probability intervals. Three out of these ﬁve
                                                        Given a scoring model M and two thresholds T1 > T2, construct
segments delineate concave regions of the ROC curve, indi- a scoring model M0 predicting scores as follows. Let S be the
cated as A, B and C. For example, area A is delineated by line score predicted by M:
ab and the ROC curve between point a and point b. In gen- 1. If S > T1, then predict S;
eral, a concave area means that the ranking obtained from the 2. If S < T2, then predict S;
probabilistic model in this probability interval is worse than 3. Otherwise, predict T1 + T2 − S.
random. For instance, consider area C which is the largest
concavity. One way to repair this concavity is by ignoring the Table 3: Algorithm RepairSection. The algorithm effec-
                                                      tively inverts the ranking of all instances whose score as pre-
  1
   For illustration purposes, the test set contains only a few ex- dicted by M falls in the interval T1 < S < T2.
amples and the resolution of the ROC curve is low. In practical
circumstances a step curve with much higher resolution is obtained.4  Experimental evaluation                              1.  Train a naive Bayes or decision tree model M on the train-
                                                            ing data; construct a ROC curve C and its convex hull H
We describe a number of experiments to evaluate our ap-     on the training data.
proach. We used 23 two-class data sets from the UCI repos- 2. Find adjacent points on H such that in this interval the
itory [Blake and Merz, 1998]. Table 4 shows their numbers   area between C and H is largest. Let T1 and T2 be the
of attributes, numbers of examples, and relative size of the corresponding score thresholds.
majority class.                                         3.  Produce a new probabilistic model M0 by calling Repair-
                                                            Section(T1, T2).
       Dataset       #Attrs #Exs  %MajClass             4.  Evaluate M and M0 on the validation set, construct their
       Australia     14     690   55.51                     ROC curves and calculate their AUCs. If AUC(M0) ≤
       Sonar         60     208   51.92                     AUC(M) then go to 6.
       Glass         9      214   67.29                 5.  Evaluate M and M0 on the test set, construct their ROC
       German        20     1000  69.40                     curves and calculate their AUCs.
       Car           6      1728  69.68                 6.  Go to 1. until each fold has been used as a test set.
       Anneal        38     798   74.44
       Monk1         6      566   50.00                         Table 5: Experiment RepairSection.
       Monk2         6      601   65.72
       Monk3         6      554   55.41
       Hepatitis     19     155   78.71                   Dataset       AUC         AUC         Better
       House         16     435   62.07                                 (Original)  (Repaired)  ?
       Tic-tac-toe   9      958   64.20                   Australia     90.9 ± 0.88 90.6 ± 0.94 ×
       Heart         13     270   55.56                   Sonar         77.5 ± 1.30 76.8 ± 1.31
       Ionosphere    34     351   64.10                   Glass         76.6 ± 1.43 80.6 ± 1.36 ∨
       Breast Cancer 9      286   70.28                   German        79.3 ± 0.88 78.9 ± 0.88 ×
       Lymphography  17     148   56.81                   Car           99.0 ± 0.079 99.0 ± 0.08
       Primary Tumor 17     339   55.75                   Anneal        86.7 ± 0.48 90.2 ± 0.47 ∨
       Soybean-large 35     683   55.51                   Monk1         75.4 ± 0.70 77.4 ± 0.70 ∨
       Solar-Flare   12     323   56.35                   Monk2         64.6 ± 0.71 66.1 ± 0.92 ∨
       Hayes-Roth    4      133   60.91                   Monk3         96.2 ± 0.29 97.7 ± 0.24 ∨
       Credit        15     690   55.51                   Hepatitis     86.3 ± 1.94 85.1 ± 1.22
       Balance       4      625   53.92                   House         96.3 ± 0.35 96.3 ± 0.35
       Bridges       12     108   66.67                   Tic-Tac-Toe   74.1 ± 0.61 75.6 ± 0.58 ∨
                                                          Heart         91.0 ± 1.04 90.2 ± 1.08 ×
     Table 4: UCI data sets used in our experiments.      Ionosphere    93.2 ± 0.69 93.1 ± 0.690
                                                          Breast Cancer 76.1 ± 1.63 77.7 ± 1.57 ∨
  The experimental procedure for the ﬁrst experiment was  Lymphography  90.3 ± 1.39 91.0 ± 1.49 ∨
as follows. We split a data set into ten folds and use eight Primary Tumor 79.6 ± 1.02 80.6 ± 1.03 ∨
of them for training, one for validation and one for testing. Soybean-Large 91.9 ± 0.46 91.9 ± 0.46
We trained a naive Bayes model and a decision tree model2 Solar-Flare   91.1 ± 0.71 91.2 ± 0.74
on the training data, and chose two thresholds that delineate Hayes-Roth 89.4 ± 1.53 91.3 ± 1.58 ∨
a concavity. We then produced a new model by repairing    Credit        90.8 ± 0.68 90.7 ± 0.65
the probabilities between the thresholds; we only use the new Balance   98.4 ± 0.27 98.4 ± 0.29
model if it improves AUC on the validation set. The detailed Bridges*   92.7 ± 1.66 92.9 ± 1.89
procedure is given in Table 5.                            Average       86.41       87.1
  We ran Experiment RepairSection ten times and obtained
m pairs of AUC values. Since we use a validation set, we are Table 6: Results of Experiment RepairSection with naive
able to decide whether or not repair resulted in a better model Bayes. *For the Bridges data set we used 5-fold cross-
– if not, we discard it and use the original, unrepaired model. validation.
For this reason, we only report results on the non-discarded
models, so m may be smaller than 100. The average AUCs of therefore conducted an experiment with two validation folds:
the unrepaired curve and the repaired curve for each data set we would only use the repaired model if its AUC was higher
are given in Tables 6 and 7. We also performed a paired t-test on both validation folds. The results for naive Bayes are
with m − 1 degrees of freedom and level of conﬁdence 0.1 shown in Table 8. We now obtain 12 signiﬁcant wins and
to test the signiﬁcance of the average difference in AUC. The only 1 signiﬁcant loss, and the average increase in accuracy
results are favourable: the signiﬁcance tests yield 10 wins and is more than a percentage-point. Interestingly, two validation
3 losses for repaired naive Bayes models and 11 wins and 5 folds didn’t work well for decision trees.
losses for repaired decision tree models.
                                                        Finally, we conducted an experiment with naive Bayes
  Experiments without using a validation set yielded worse
                                                      whereby we selected all concavities, and repaired those that
results, so the use of a validation set appears crucial. We
                                                      occurred on two validation sets. The results were similar to
  2Notice that decision trees can be viewed as scoring classiﬁers the results in the ﬁrst experiment (11 signiﬁcant wins and
[Ferri et al., 2002; Provost and Domingos, 2003].     three losses), with some of the wins and losses occurring on   Dataset       AUC          AUC          Better     5   Discussion and conclusions
                 (Original)   (Repaired)   ?
   Australia     82.27 ±0.50  82.28 ±0.49             The work reported in this paper bears some similarity with en-
   Sonar         87.35 ±1.20  88.92 ±1.01  ∨          semble methods. Bagging and boosting are two well-known
   Glass         76.09 ±1.34  80.69 ±1.15  ∨          ensemble approaches. Both approaches are implemented by
   German        80.78 ±0.55  81.01 ±0.54  ∨          re-sampling methods. In bagging [Breiman, 1996], the en-
   Car           99.05 ±0.074 99.07 ±0.081            semble is formed by making bootstrap replicates of the train-
   Anneal        86.86 ±0.41  90.64 ±0.37  ∨          ing data sets and then multiple generated hypotheses are used
   Monk1         75.82 ±0.74  78.05 ±0.704 ∨                                                      [
                      ±           ±        ∨          to get an aggregated predictor. Boosting algorithms Freund
   Monk2         66.46 0.66   67.95 0.662                              ]
   Monk3         96.30 ±0.32  98.14 ±98.14 ∨          and Schapire, 1996 assign different weights to training in-
   Hepatitis     91.92 ±2.84  91.92 ±2.84             stances depending on whether they are correctly classiﬁed.
   House         96.18 ±0.31  96.66 ±0.29  ∨          The approaches presented in this paper do not make use of
   Tic-Tac-Toe   74.81 ±0.54  75.97 ±0.56  ∨          re-sampling techniques.
   Heart         91.72 ±1.34  90.77 ±1.21  ×            Another relevant ensemble method is majority voting
   Ionosphere    96.57 ±0.91  95.88 ±0.62             [Kimura and Shridar, 1991; Lam and Sue, 1997], in which the
   Breast Cancer 74.82 ±1.71  75.75 ±1.51             class predicted by the ensemble is the most predicted class
   Lymphography  87.71 ±1.86  87.88 ±1.89  ∨          among the base classiﬁers. Algorithm RepairPoint in this
   Primary Tumor 76.63 ±1.23  75.51 ±1.36  ×          paper uses a kind of voting: when both Model 1 and Model
   Soybean-Large 90.93 ±0.50  91.10 ±0.52  ×          2 (see Figure 2) agree on the classiﬁcation of an instance,
   Solar-Flare   86.61 ±1.59  85.65 ±1.65  ×          then we choose that class. Otherwise, we choose the class
   Hayes-Roth    86.87 ±2.24  88.33 ±2.08  ∨          not predicted by Model 3. In majority voting, on the other
   Credit        91.09 ±0.65  90.92 ±0.64             hand, we would choose the class predicted by Model 3. The
                      ±           ±
   Balance       99.37 0.18   99.42 0.189             difference is that majority voting does not take the quality of
   Bridges       85.57 ±4.36  81.71 ±3.35  ×
   Average       86.16        86.71                   the different models into account, whereas our repair scheme
                                                      knows that Model 3 is sub-optimal and therefore corrects it
Table 7: Results of Experiment RepairSection with decision predictions in the relevant region.
trees.                                                  ROC curves contain a wealth of information about the per-
                                                      formance of one or more classiﬁers, which can be utilised to
                                                      construct better models. They have been used to ﬁnd optimal
   Dataset       AUC          AUC         Better                            [             ]
                 (Original)   (Swapped)   ?           labelling of decision trees Ferri et al., 2002 and to ﬁnd good
                                                                                             [
   Australia     87.17±2.28   86.86±2.33  ×           decision thresholds for probabilistic classiﬁers Lachiche and
   Sonar         81.34±2.46   84.61±2.66  ∨           Flach, 2003]. In this paper we have proposed a novel ap-
   Glass         74.67±1.61   74.75±1.42  ∨           proach to construct new models by repairing concavities in
   German        82.33±0.62   82.72±0.60  ∨           a ROC curve. The ﬁrst method, RepairPoint, works on a
   Car           99.25±0.091  99.29±0.098             probabilistic classiﬁer with three probability thresholds, and
   Anneal        87.06±0.53   90.32±0.48  ∨           tries to improve the poorest model with help of the other two.
   Monk1         75.68±1.12   78.65±0.92  ∨           Preliminary experimental results (not reported) showed that
   Monk2         65.74±0.89   67.24±0.95  ∨           this didn’t work too well, but this may be due to the fact that
   Monk3         96.75±0.47   98.54±0.25  ∨           the selection of the threshold for Model 3 is not easy. The
   Hepatitis     92.67±3.79   93.19±3.37              threshold is a point chosen based on the ROC curve of the
   House         96.76±0.37   96.76±3.44  ∨           training data set; this point (see point c3 in Figure 3) has the
   Tic-Tac-Toe   75.62±0.69   76.92±0.71  ∨           farthest distance to the convex hull. If this threshold is not
   Heart         92.91±1.29   92.98±1.42              optimal on the test data (for instance, the position c3 in the
   Ionosphere    97.07±0.77   97.53±0.57              ROC curve on the test data set unfortunately is located in the
   Breast Cancer 78.20±2.43   79.16±2.12              position c2), the AUC after repair becomes worse. Still, we
   Lymphography  86.94±2.41   90.49±1.94  ∨           believe that the idea of mirroring models around lines in ROC
   Primary Tumor 78.14±1.31   80.19±1.27  ∨
                                                      space will prove to be very useful. An interesting investiga-
   Soybean-Large 91.36±0.45   91.63±0.44
   Solar-Flare   89.69±1.90   89.94±1.88              tion for future work is whether a similar method can be made
   Hayes-Roth    84.89±3.56   88.33±3.09  ∨           to work if the models are not obtained from a single scoring
   Credit        89.03±1.56   88.67 ±1.70             model (this would invalidate Theorem 1, i.e., the position of
   Balance       99.53±0.027  99.10±0.078             Model 4 may be different from the point obtained by point-
   Average       86.49        87.63                   mirroring).
                                                        The second method, RepairSection, locates and repairs
Table 8: Results with naive Bayes using two validation folds. an entire concave region of a ROC curve. Experimental re-
                                                      sults were very encouraging for both naive Bayes and de-
                                                      cision trees. For naive Bayes we were able to improve re-
different data sets. It appears that repairing only the largest sults even further by using two validation folds, but this didn’t
concavity and using two validation sets is the best strategy (at work for decision trees. We are currently investigating why
least for naive Bayes).                               this is so. One possible explanation is that ROC curves ob-