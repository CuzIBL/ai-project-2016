            Graph-Based Semi-Supervised Learning as a Generative Model 

                      Jingrui He               Jaime Carbonell               Yan Liu 
                                   Carnegie Mellon University 
                                  School of Computer Science 
                              5000 Forbes Avenue, Pittsburgh 15213 
                 jingruih@cs.cmu.edu           jgc@cs.cmu.edu          yanliu@cs.cmu.edu  

                   Abstract                      with the labeled data.  Finally, the classification labels are 
                                                 obtained by comparing the function value and a pre-specified 
    This paper proposes and develops a new 
                                                 threshold.  For example, in the Gaussian random fields and 
    graph-based semi-supervised learning method.  
                                                 harmonic function method, the learning problem is formu-
    Different from previous graph-based methods that 
                                                 lated in terms of a Gaussian random field on the graph, and 
    are based on discriminative models, our method is 
                                                 the mean of the field serves as the function [Zhu et al., 2003].  
    essentially a generative model in that the class 
                                                 Another example is the local and global consistency method, 
    conditional probabilities are estimated by graph 
                                                 in which the function at each point is iteratively determined 
    propagation and the class priors are estimated by 
                                                 by both the information propagated from its neighbors and its 
    linear regression.  Experimental results on various 
                                                 initial label [Zhou et al., 2004].  Yet another example is the 
    datasets show that the proposed method is superior 
                                                 graph mincut method whose function corresponds to parti-
    to existing graph-based semi-supervised learning 
                                                 tioning the graph in a way that roughly minimizes the number 
    methods, especially when the labeled subset alone 
                                                 of similar pairs of examples that are given different labels 
    proves insufficient to estimate meaningful class 
                                                 [Blum and Chawla, 2001].  In the mincut method, the func-
    priors. 
                                                 tion can only take binary values. 
                                                   Up till now, graph-based semi-supervised learning meth-
 1 Introduction                                  ods are generally approached from the discriminative per-
 In many real world classification tasks, the number of labeled spective [Zhu, 2005] in that the function on the graph cor-
 instances is very few due to the prohibitive cost of manually responds to posterior probabilities in one way or another.  In 
 labeling every single data point, while the number of unla- the discriminative setting, however, the use of unlabeled data 
 beled data can be very large since they are easy to obtain.  does not necessarily guarantee better decision boundaries.  In 
 Traditional classification algorithms, known as supervised addition, there is no clear explanation why the function on 
 learning, only make use of the labeled data, therefore prove the graph should correspond to posterior probabilities from 
 insufficient in these situations.  To address this problem, statistics point of view. 
semi-supervised learning has been developed, which makes  In this paper, we propose a new graph-based 
use of unlabeled data to boost the performance of supervised semi-supervised learning method from the generative model 
learning.  In particular, graph-based semi-supervised learn- perspective.  Specifically, the class conditional probabilities 
 ing algorithms have proved to be effective in many applica- and the class priors are estimated from the weighted graph.  
 tions, such as hand-written digit classification [Zhu et al., The potential advantages involve several aspects: first, it can 
 2003; Zhu et al., 2005], medical image segmentation [Grady be theoretically justified that in the ideal cases where the two 
and Funka-Lea, 2004], word sense disambiguation [Niu, Ji classes are separable, the output functions in terms of certain 
and Tan, 2005], image retrieval [He et al., 2004], etc. eigenvectors of the graph converge to the class conditional 
  Compared with other semi-supervised learning methods, probabilities as the number of training data goes to infinity.  
such as TSVM [Joachims, 1999], which finds the hyperplane In non-ideal cases, our functions still provide a good estimate 
that separates both the labeled and unlabeled data with the of the class conditional probabilities.  Finally, the estimated 
maximum margin, graph-based semi-supervised learning class priors make use of both the labeled and unlabeled data, 
methods make better use of the data distribution revealed by which compensate for the lack of label information in many 
unlabeled data.  In graph-based semi-supervised learning, a practical situations.  Experimental results show that our ap-
weighted graph is first constructed in which both the labeled proach leads to better performance than other existing 
and unlabeled data are represented as vertices.  Then many of graph-based methods on a variety of datasets.  Hence we can 
these methods can be viewed as estimating a function on the claim both stronger theoretical justification and better em-
graph [Zhu, 2005].  Based on the assumption that nearby pirical results. 
points in the feature space are likely to have the same label,   The rest of the paper is organized as follows.  In Section 2 
the function is defined to be locally smooth and consistent and Section 3, we introduce how to estimate the class condi-

                                            IJCAI-07
                                             2492 tional probabilities and the class priors respectively.  Section   W   0
                                                                W    1                     (3)
 4 deals with the out-of-sample problem, followed by an out-         0 W
 line of the algorithm in Section 5.  Then the experimental              0
                                                      W     W
 results are shown in Section 6.  Finally, we give conclusion where 1  and 0  represent the sub-matrices corresponding 
 and hint on future work in Section 7.           to the positive and negative samples respectively, and 0 
                                                 represents zero matrix.  If the total number of positive 

 2  Estimating Class Conditional Probabilities   (negative) samples in the training set is n1  ( n0 ), W1  (W0 ) is 

                                                 an nn11 ( nn00) square matrix.  Let D1  and D0  be  two 
 2.1 Notation                                    diagonal matrices, the diagonal elements of which are the 

 In a binary classification problem, suppose that we are given row sums of W1  and W0 .  Then S  can be written as 
                              d
                                                                   1/2  1/2
 a set of n  training samples: xx1, n .  The first nl  sam- S 0  DWD           0
                                                      S   1       111                      (4)
ples are labeled, including n  positive ( yi1, 1, , n) and                   1/2  1/2
                     l1        il1                        0 S0       0     DWD000
 nnn              yin          n
  lll01 negative  ( ill0,1  1, , ) samples.  The   The following theorem connects the class conditional 

remaining nnnul samples are unlabeled.  Our goal is to probabilities with the diagonal elements of D . 

 predict the class labels of these nu  points by computing the Theorem 1. If xij, xxxV i j n n, where n  and 

posterior probability  Pyxii.                    Vn  are positive parameters, and the function  satisfies 
  By Bayes rule, we have                         the following conditions: u  0  ,    udu  1 , 
                   px(|) y Py ()
         Py|  x       ii     i                                      d
            ii                             (1)   sup u    , limuu       0 , limV 0 , lim nV , as 
                     px(| y ') Py (')                               i 1 i     n        n
                       ii      i                  u         u              n       n
                 yi '0,1        ,
                                                 the number of examples n  goes to infinity, Dn  con-
                                                                                      ii yi
 yi  is predicted to be 1 iff Pyii1| x 0.5.  In our genera-

                                                 verges to pxyii. 
tive model, in order to calculate Pyii| x , we need to esti-
                                                   The proof of the theorem is straightforward and therefore 
         pxy       py
mate both   ii and     .  In this section, we focus on we put it in the appendix.  Notice that this theorem is similar 
                                                 to a result in kernel density estimation.  The difference is that 
estimating the class conditional probability pxyii, and the 
                                                 in kernel density estimation, we only have labeled data from 
 estimation of py will be discussed in the next section. a single class; while in our situation, we have both labeled 
  We first form an affinity matrix W   nn  with  and unlabeled data, and we could estimate the class condi-
 Wxx,      , where  x , x  is a non-negative function tional distributions of the two classes at the same time. 
  ij   i j          ij                             Suppose that the labeled data are noise-free.  According to 
measuring the direct similarity between xi  and x j .  Then 
                                                 Theorem 1, we can use Dii  to approximate the class condi-
define  D    as the diagonal matrix, where 
                                                 tional probability of xi  given the observed label yi .  How-
      n
 Dwin,1,         , and SDWD1/2 1/2 .  Finally define 
  iij  1 ij                                      ever, for the unlabeled points, we do not know if Dii  corre-
 f     f
    and   as two n -dimensional vectors.  The element of sponds to pxyii 1  or pxyii 0 .  To address this prob-
 f  ( f ) is set to 1 iff the corresponding point is a positive lem, we can make use of the eigenvectors of S . 
 (negative) labeled one. 
                                                   It is easy to show that the largest eigenvalue of S1  and S0  
                                                 is 1, and if W  and W  form a connected graph respectively, 
 2.2  The Ideal Case                                       1     0
                                                                                  vD1/2
 To start with, let us first consider the ideal case where the two the corresponding eigenvectors would be 1 1  and 
 classes are far apart.  In this case, we have the following 1/2
                                                 vD0    1  [Chung, 1997].  Based on v  and v , we can 
 equation: 
                                                 construct two eigenvectors of S  with eigenvalue 1: 
     px  Py11   pxy     Py   0 pxy  0                               TT
                                           (2)                  TT
                                                            vv100,    v  0  v              (5)
          Pyxx pxy
                                                 where 0  is a zero vector.  Notice that if we square v1  and v0  
                                                                2     2
 where yx  is the observed class label of data point x . 
                                                 by elements to get v1  and v0 , and then add them up, we get 
   Based on this assumption, if xi  and x j  are from two dif- 22                          (6)
                                                            vvD10  1111 D 0 D 

 ferent classes, the corresponding Wij 0 .  Therefore if we  2      2
                                                  Obviously, v1  and v0  correspond  to pxyii 1  and 
knew the labels of all the samples and put together the sam-
                                                 pxy
ples from the same class, the affinity matrix W , and thus the ii 0  respectively, and their non-zero elements are 

 symmetric matrix S  would be block-diagonal.  To be spe- equal to Dii . 
 cific, let 


                                            IJCAI-07
                                             2493 To get v   and v  , we perform fSf and         sponds to eigenvalue 1.  If we still iterate fSf and 
                                                fSf until  convergence, both f  and f  will  con-
fSf until convergence.  Since the initial value of f  
                                                verge to the same eigenvector.  On the other hand, the op-
                 v                f     v
is not orthogonal to 1  (the  elements  of  and 1  are eration of f Sf and fSf can be seen as the 

non-negative), f  will converge to v1 .  Similarly, f  will labeled data gradually spreading their information to nearby 
                                      2     2   points.  If the iteration steps are unlimited, every data point 
         v                          f    f
converge to 0 .  Therefore, upon convergence, i  ( i ) will be equally influenced by the positive and negative la-
is in proportion to the class conditional probability of the beled data, leading to the same value of f  and f . 
                                     2     2      To solve this problem, in our algorithm, we have designed 
positive (negative) class.  After normalizing f  ( f ) 
                                    i    i      a stopping criterion, and the iteration process is stopped once 
so that it sums to 1, we have an empirical estimation of the criterion is satisfied.  To be more specific, when esti-

pxyii 1  ( pxyii 0 ), which converges to its true value mating the class conditional probabilities of the positive class, 
as n  goes to infinity.                         we could get an estimate of pxyii 1  in each iteration step 
                                                                2
  Figure 1 gives an example of density estimation in the (by normalizing f  so that it sums to 1).  By summing up 
ideal case.  Figure 1(a) shows the training data, where the two i
moons represent two classes, and each class has one labeled this probability for negative labeled samples, we have the 
example marked as star.  Figure 1(b) and 1(c) show the es- average likelihood of these samples in the positive class: 
                                                      nl
timated class conditional distributions of the two classes. Lpxynii 1 .  We stop the iteration when the 
             6                                        in 1

             4                                  second derivative of L  with respect to the iteration steps 
             2                                  crosses 0.  This criterion can be justified as follows: in the 
             0                                  initial iteration steps, only a few negative data get positive 
            -2                                  score from their nearby positive labeled points, so the rate at 
            -4                                  which L  increases is very low; as the iteration proceeds, 
              -5   0     5    10                those negative data have accumulated high scores and 
                     (a)                        propagate to the majority of negative points, so the rate 
                                                gradually increases; finally, as f  begins to converge, its 
                                                value at each data point becomes stable, so the rate decreases 
                                                until it reaches 0.  If we plot the curve of L  with respect to 
                                                the number of iteration steps, the shape would be convex first, 
                                                and then concave until convergence (Figure 2(b)).  Notice 
                                                that in the initial iteration steps, the positive points, which are 
                                                far away from the positive labeled points but connected to 
                                                them via some kind of manifold, cannot get positive scores.  
          (b)                   (c) 
                                                If the algorithm stops at this stage, it may not fully explore 
Figure 1. Density Estimation in the Ideal Case. (a): training data; (b) 
                                                the data distribution and cause misclassification on certain 
and (c) class conditional distributions 
                                                clusters of data.  Therefore we choose the transition point 
2.3  The General Case                           between convex and concave as the stopping point in order to 
                                                trade off between prematurity and excessive propagation.  
In the general cases, the two classes are not far apart, and we 
                                                The stopping criterion for the negative class can be derived 
have the following theorem. 
                                                                nl1
                                                similarly, i.e. L pxyii0    n l1 .  A key point in our 
Theorem 2. If xij, x  satisfies the conditions in Theorem 1,    i 1
                                                algorithm is that the estimation of the class conditional 
as the number of samples n  goes to infinity, Dnii  con-
                                                probabilities of the two classes is independent, i.e. the 
verges to pxy11 Py     pxy   0 Py  0 
          ii             ii                     numbers of iteration steps when the two stopping criterions 
 The proof to this theorem is quite similar to Theorem 1.  So are satisfied are not necessarily the same. 
we omit the details here.  It can be seen easily that Theorem 1   Figure 2 gives an example of density estimation in the 
is a special case of Theorem 2 when the two classes are far general case showing the effectiveness of our criterion.  This 
apart, i.e.                                     example is quite similar to the one shown in Figure 1 except 

      limDnii pxy i i 1 Py 1 , if y i 1         that the two classes are not far apart.  Figure 2(b) shows the 
      n                                   (7)   value of L  (the upper curve) and L  (the lower curve) in 
     limDnii  pxy i i 0 Py 0 , if y i 0  
     n                                          each iteration step.  The arrows point to the positions in the 
                                                curves where the two criterions are satisfied.  Figure 2(c) and 
  Equation (7), together with the fact that limnn1 Py 1 , 
leads to Theorem 1.            n                2(d) show the estimated class conditional distributions of the 
  In the general cases, W  tends to form one connected graph two classes.  Although there are small gaps in the middle of 
instead of two, and S  only has one eigenvector that corre- the distributions, the moon structure is recovered fairly well. 


                                           IJCAI-07
                                            2494                         x 10-3
                         3
  8                                              4  Prediction of New Testing Data 
                        2.5
  6                                                                    d
                            L+                   To classify a data point x  that is not present during the 
                         2
  4                         L-                   training stage, we first calculate its class conditional prob-
                        1.5
  2                                              abilities via kernel regression: 
                                                                   n
  0                      1                                            x, xpxy
                                                                   i 1   ii                (10)
                        0.5                                pxy        n          
  2                                                                       xx,
                                                                      i 1   i
  4                      0
   -5    0    5     10   0   200 400 600  800      Using these class conditional probabilities and the class 
           (a)                               
                                 (b)             priors obtained during the training stage, we can calculate the 
                                                 posterior probability and make a prediction. 

                                                 5 The Algorithm 

                                                 The procedures for estimating pxyii and Py are  sum-
                                                 marized in Table 1 and Table 2 respectively. 
                                                                                   nn
                                                 1.  Form the affinity matrix W       , where 
                                 (d) 
           (c)                                       Wxx,      .  Calculate D  and S . 
 Figure 2. Density Estimation in the Generation Case. (a): training ij i j
 data; (b): L  and L  in each iteration; (c) and (d): class conditional 2. Initialize f  and f .  The element of f  ( f ) is set 
 distributions.                                      to 1 if the corresponding point is a positive (negative) 
   Note that the stopping criterion discussed above is based labeled one, and 0 otherwise. 
 on simple heuristics.  Currently we are trying to design a 3. Update fSf, fSf. 
 stopping criterion in a more principled manner. 
                                                                        2               2
                                                 4.  Assign pxyii1   f i , pxyii0()   f i , and 

                                                                               n
 3 Estimating Class Priors                                                      pxy    11
                                                     normalize so that         i 1 ii        , 
 In this section, we focus on estimating the class prior Py.  n
                                                         pxy   01. 
 Existing graph-based semi-supervised learning methods only i 1 ii
 use the labeled set to estimate the class priors, either explic- 5. Calculate the average likelihood of negative (positive) 
 itly [Zhu et al., 2003] or implicitly [Zhou et al., 2004].  Ob- labeled points in the positive (negative) class: 
                                                           n                    n
 viously, in real applications, the proportion of positive and L l pxy n  Lpxynl1
                                                           in   ii1     l0 (    i   ii0    l1 ).
 negative labeled data is often far from the true class priors. l1 1             1
   In our algorithm, we use both the labeled and unlabeled Go to step 4 unless one of the following conditions is 
 data to estimate the class priors.  According to Theorem 2, satisfied: 
 once we have estimated the class conditional probability a. L  ( L ) remains at 0, and f  ( f ) has converged; 
                                                       
 pxyii, we can feed them into the following equations and b. L  ( L ) does not remain at 0, and the second de-
 form a linear regression problem, the solution of which is rivative of L  ( L ) with respect to the iteration steps 
 equal to the least squares estimator of Py 1 .        crosses 0. 
                                                    
                                           (8)   6.  Output pxyii 1  and pxyii 0 . 
   Dii n pxy i i101,1,, pˆˆ pxy i i p i n 
   However, when the number of labeled data is small, the 
                                                       Table 1. Description of Estimation for pxyii 
 estimated class conditional probabilities may not be very  
 accurate, and thus pˆ  is not very reliable.  To solve this 1. Solve the following linear regression problem for the 
problem, we use a beta distribution as the prior distribution least squares estimator pˆ  of Py 1 : 
for Py  1 , the parameters of which are pˆ  and 1 pˆ .  Then    ppxyˆˆ11 p pxy 0 D ni ,1,, n 
the estimate of Py 1  based on the labeled set:           ii             ii      ii
                                                    
              pnˆ                                2.  Calculate the class priors as the smoothed proportion of 
       Py1,011l1    Py       Py            (9)       the positive and negative samples in the labeled set 
               1 n
                  l                                              pnˆ
 which is equivalent to smoothing the proportion of the posi- Py1,011l1 Py      Py     
                                                                 1 n
 tive and negative samples in the labeled set.  When the            l
 number of labeled data is small, unlabeled data can be fully Table 2. Description of Estimation for Py 
 exploited to compensate for the proportion in the labeled set 
 that is not the same as the class priors; when the number of 6 Experimental Results 
 labeled data is large, labeled data will dominate the estima-
                                                 In this section, we present the comparative experimental 
 tion of the class priors. 
                                                 results on two datasets: Cedar Buffalo binary digits database 
                                                 [Hull, 1994], and a document genre-classification dataset 
                                                 [Liu et al., 2003].  Our algorithm is compared with two other 


                                            IJCAI-07
                                             2495 graph-based semi-supervised learning methods: Gaussian case, while the performance of our algorithm is comparable 
 random fields [Zhu et al., 2003] and the local and global to the balanced case.  This is because the class mass nor-
 consistency method [Zhou et al., 2004].  We did not compare malization procedure adopted in Gaussian random fields 
 with supervised learning methods, such as one nearest depends on the labeled set only to estimate the class priors; 
 neighbor, since they have been proved to be less effective while our algorithm makes use of both the labeled and the 
 than Gaussian random fields based on experimental results unlabeled set to estimate the class priors.  Therefore, it is 
 [Zhu et al., 2003].                             more robust against the perturbation in the proportion of the 
   We have designed two kinds of experiments: balanced and positive and negative data in the labeled set. 
 unbalanced.  In the balanced case, the ratio of labeled points 
 from each class is always the same as the class priors; in the 6.2 Genre Dataset 
 unbalanced case, if not explained otherwise, we fix the total Genre classification is to classify the documents based on its 
 number nl  of labeled points, and perturb the number of writing styles, such as political articles and movie reviews.  
                        n
positive labeled points around l 2  with a Gaussian distri- The genre dataset that we use consists of documents from 10 
                                  n 10
bution of mean 0 and standard deviation l .  In each genres, including biographies (b), interview scripts (is), 
 experiment, we gradually increase the number of labeled data, movie reviews (mr), product reviews (pr), product press 
 perform 20 trials for each labeled data volume, and average releases (ppr), product descriptions on store websites (pd), 
 the accuracy at each volume point. 
                                                 political articles on newspapers (pa), editorial papers on 
 6.1  Cedar Buffalo Binary Digits Database       politics (ep), news (n), and search results from multiple 
                                                 search engines using 10 queries (sr).  We randomly select 
 We first perform experiments on Cedar Buffalo binary digits 380 documents from each category to compose the whole 
 database [Hull, 1994] including two classification tasks: dataset of 3800 documents.  Each document is processed into 
 classifying digits “1” vs “2”, with 1100 images in each class; a “tf.idf” vector, which is generated based on the top 10,000 
 and odd vs even digits, with 2000 images in each class (400 most frequent words in this dataset after stemming, with the 
 images for each digit).  The data we use are the same as those header and stop words removed.  Here 
 used in [Zhu et al., 2003].  Here 
              d                                    xx,exp1      x  x   xx   0.03, which is bor-
            222          2                          ij           i j   i j
   xxij,2       exp x i x j 2, where     is the 
                                                 rowed from [Zhu et al., 2003] and roughly measures the 
 average distance between each data point and its 10 nearest similarity between documents.  The only difference is that we 
 neighbors. 
  1                     0.85                     keep all the edges instead of keeping edges for only 10 

                        0.8                      nearest neighbors.  Next we perform experiments to compare 
 0.9
                        0.75                     the three algorithms.  The results are provided in Figure 5 and 
                                                 Figure 6 respectively. 
 0.8                    0.7
       Our Algorithm                                                    0.65
                                                     Our Algorithm         Our Algorithm
                                                  0.7
 0.7   Gaussian Random Fields 0.65 Our Algorithm     Gaussian Random Fields Gaussian Random Fields

 accuracy               accuracy                     Local and Global Consistency Local and Global Consistency
       Local and Global Consistency 0.6 Gaussian Random Fields
                                                  0.65                   0.6
 0.6                           Local and Global Consistency
                        0.55

 0.5                    0.5                       0.6
   0  20  40   60  80  1   20   40   60  80   1
                                                                        accuracy
         labeled set size       labeled set size accuracy               0.55
           (a)                   (b)              0.55
   Figure 3. Balanced Classification. (a): 1 vs 2; (b) odd vs even 
  1                     0.85                      0.5                    0.5
                                  Our Algorithm      20   40  60   80       20  40   60   80
                                                          labeled set size      labeled set size
 0.9                     0.8      Gaussian Random Fields                
                                  Local and Global Consistency
                        0.75                               (a)                    (b) 
 0.8                                             Figure 5. Classification between Random Partitions. (a): balanced; 
         Our Algorithm   0.7
 0.7     Gaussian Random Fields                  (b): unbalanced 
                        0.65                       1                     0.9


 accuracy Local and Global Consistency accuracy      Our Algorithm         Our Algorithm
 0.6
                         0.6                      0.9 Gaussian Random Fields 0.85 Gaussian Random Fields
                                                     Local and Global Consistency Local and Global Consistency
 0.5                    0.55                      0.8                    0.8
                                                  0.7                   0.75
 0.4                     0.5
  0   20  40   60  80  1    20  40   60   80  1
         labeled set size        labeled set size 0.6                    0.7
                                                  0.5                   0.65
           (a)                    (b)             accuracy              accuracy
  Figure 4. Unbalanced Classification. (a): 1 vs 2; (b) odd vs even 0.4  0.6
   Figure 3(a) and 3(b) show the results of the two classifi- 0.3       0.55
                                                  0.2                    0.5
 cation tasks in the balanced case.  The performance of our 50 100 150       50   100   150
                                                          labeled set size       labeled set size
 algorithm is comparable with Gaussian random fields, and  (a)                    (b) 
 both of them are much better than the local and global con-
                                                 Figure 6. Unbalanced Classification. (a): pa vs other; (b) b vs other 
 sistency method.  Figure 4(a) and 4(b) show the results in the 
                                                   For Figure 5, we randomly partition the 10 categories into 
 unbalanced case.  In this situation, the performance of 
                                                 two classes, i.e. pa, pr, sr, b, and is, vs mr, ppr, pd, ep and n.  
 Gaussian random fields is much worse than in the balanced 
                                                 Figure 5(a) and 5(b) correspond to the balanced and unbal-


                                            IJCAI-07
                                             2496