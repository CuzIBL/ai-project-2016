                                     The Power of Suggestion* 

                                 Barry Smyth and Lorraine McGinty 
                        Smart Media Institute, Department of Computer Science, 
                          University College Dublin, Belfield, Dublin 4, Ireland. 
                             Email: {barry.smyth, lorraine.mcginty}@ucd.ie 


                     Abstract                          me more like Holly's Bistro but less formal''), to good effect 
                                                       [Burke, 20001. However, if these properties are not present 
    User feedback is vital in many recommender sys•    then other strategies are needed. For example, if item de•
    tems to help guide the search for good recommen•   scriptions are not available, collaborative filtering can be used 
    dations. Preference-based feedback (e.g. "Show     to recommend items that have been rated positively by sim•
    me more like item A ") is an inherently ambiguous  ilar users [Burke, 2000; Shardanand and Maes, 1995]. The 
    form of feedback with a limited ability to guide the whiskey recommendation scenario above is different. The 
    recommendation process, and for this reason it is  availability of item descriptions suggests case-based recom•
    usually avoided. Nevertheless we believe that cer• mendation, but the lack of user expertise limits the use of 
    tain domains demand the use of preference-based    feedback strategies such as value elicitation or critiquing. 
    feedback. As such, we describe and evaluate a flex•
                                                         Indeed the whiskey domain is not unique in this respect. In 
    ible recommendation strategy that has the potential 
                                                       many domains users are able to express a preference without 
    to improve the performance of case-based recom-
                                                       necessarily understanding individual item properties. This is 
    menders that rely on preference-based feedback. 
                                                       especially true in domains where a specialised v :abulary al•
                                                       ready exists to describe items (e.g. fashion, jewellery, art, mu•
1 Introduction                                         sic, etc.). Consider a recommender system designed to help 
                                                       a bride-to-be to choose her ideal dress, individual dresses are 
At last year's European Case-Based Reasoning conference described using specific features (e.g. coul-back, scoop-cut, 
(ECCBR 2002) in Scotland, delegates were treated to a  georgian-style, etc.) but most of these terms are meaningless 
whiskey tasting. A range of different whiskeys were sam• to the average bride, and yet the typical bride-to-be is capable 
pled, each accompanied by a feature-based description (e.g. of recognising what she likes when she sees it. This problem 
distillery, age, alcohol content, cask, peatiness, sweetness, is more than the user being unaware of the correct vocabulary 
palette, etc.). However, the value of these descriptions was to use during recommendation. The point is that even if they 
limited as most of us had little or no understanding of the were presented with the appropriate vocabulary terms, they 
features, nor could we reliably reconcile individual features would not be in a position to use or appreciate these terms. 
with the taste of a particular whiskey. Nevertheless virtually 
                                                         One form of feedback that could be used is preference-
everyone could chose a favourite whiskey by the end of the 
                                                       based feedback; the user simply expresses a preference for 
session (after a lot of sampling it has to be said!). This exam•
                                                       an item (e.g. "Show me more like dress 2"). This has 
ple corresponds to a challenging problem for developers of 
                                                       been largely avoided by researchers - it is assumed to be 
recommender systems, for reasons that will become clear. 
                                                       too limited to efficiently guide the recommendation process. 
  Normally the success of case-based recommenders de•  Nevertheless in our research we arc interested in looking at 
pends on two important domain properties: (1) the items how to make this a practical form of feedback in these chal•
need to be described using well-defined features; and (2) lenging domains. Incidentally, unlike other feedback types, 
users must have some understanding of these features and preference-based feedback also benefits from minimal inter•
how they relate to their requirements. For example, restau• face needs, and so is particularly relevant for recommenders 
rants are readily described using features such as cuisine, lo• designed for use with the current generation of mobile de•
cation and price, and most people have a reasonable under• vices, such as WAP phones and PDAs. 
standing of their needs in relation to these features. When the 
above characteristics are present content-based recommenda• In this work we describe a way to improve the perfor•
tion strategies can be combined with different forms of user mance of recommenders that use preference-based feedback. 
feedback, such as value elicitation (e.g. "I want an expensive, Our novel adaptive selection method mirrors how real-life 
Asian restaurant") [Bridge, 20011, or critiquing (e.g. "Show sales assistants adapt their recommendation strategies in re•
                                                       sponse to user feedback by balancing the importance of sim•
   "The support of the Informatics Research Initiative of Enterprise ilarity and diversity during each recommendation cycle. Dra•
Ireland is gratefully acknowledged                     matic improvements in recommendation performance have 


CASE-BASED REASONING                                                                                   127  been observed (e.g. reductions in dialog length of up to 80%). 

 2 Comparison-Based Recommendation 
Comparison-based recommendation is a content-based 
(or case-based) recommendation strategy [Burke, 2000; 
 McGinty and Smyth, 2002], as opposed to a collaborative 
recommendation strategy [Burke, 2000; Konstan etal., 1997; 
 Shardanand and Maes, 1995]. The notion of comparison-
 based recommendation was introduced by [McGinty and 
Smyth, 2002] to emphasise the role of feedback in content-
based recommenders, and to allow for an analysis of 
preference-based feedback in particular. The focus was         Figure 1: Profiling the similarity of the preference case to the 
on how query modification methods might be used with           target case in a typical recommendation session 
preference-based feedback to produce efficient recommenda•
tion dialogs without richer forms of feedback like value elic- 2.3 Query Modification Strategies 
itation or critiquing. We briefly review this earlier work, de•
scribing the basic comparison-based recommendation frame-      Query modification techniques can relieve the false-lead 
work and some successful query modification strategies.        problem, and have been shown to improve recommenda•
                                                               tion efficiency, reducing MLT dialogs by 30% on average 
2.1 The Basic Algorithm                                        [McGinty and Smyth, 2002]. The central idea is to identify 
Comparison-based recommendation (Figure 1) is an iterative     features in the preference case that are likely to be the rea•
recommendation algorithm that presents the user with a se•     son for the user's choice, and those features that are likely 
lection of k items as recommendations during each of a se•     to be irrelevant. Only relevant features are transferred to the 
quence of n recommendation cycles. During each cycle the       new query. Returning to the whiskey example above, if the 
user expresses a preference for one of the suggestions and this rejected cases are also very sweet then sweetness may irrel•
feedback is used to revise the query for the next cycle; in the evant to the user. Hence, one query modification strategy, 
vocabulary of Shimazu, it is a type of navigation by propos•   partial MLT (pMLT) only includes preference features in the 
ing [Shimazu, 2001]. The recommendation dialog terminates      query that are not found in any of the rejected cases. 
when the user is presented with an acceptable suggestion.        [McGinty and Smyth, 2002] also propose more sophis•
                                                               ticated query modification strategies that weight preference 
2.2 Simple Selection: More Like This 
                                                               features in the query according to how likely they are to be 
The simplest approach to comparison-based recommendation       reliable. One strategy computes a weight for a preference 
is to recommend the k most similar items to the current query, feature based on the proportion of rejected items that also 
and use the newly preferred item as the query for the rec•     contain that feature. If 3 cases are recommended to the user, 
ommendation next cycle. This more like this (MLT) method       and if the preference and just one of the rejected items are 
is a facility often provided by Web search engines; although   oak-aged and then this feature is transferred to the new query 
it should be noted that search engines rarely rely upon this   with a weight of 0.5 (50% of the rejected items are oak-aged). 
method, primarily focusing instead on query elicitation and 
term-based search. The MLT method is flawed by a ten-
dancy to follow so-called false-leads as it overfits to the cur• 3 Adaptive Case Selection 
rent preference, and this results in protracted recommenda•
tion dialogs. Using each preferred item as the next query is   So far we have assumed the use of similarity-based selection 
problematic if some of its features are not relevant to the user. during each cycle. However, in the real-world two selection 
For example, a user may indicate a preference for a whiskey    strategies can be observed in the dialogs that take place be•
that is oak-aged, sweet, and peaty. But if the preference was  tween customers and sales assistants. If a customer is unsure 
largely a result of the peatiness and independent of, or even at of their exact needs the sales assistant will tend to present a 
odds with, the sweetness or the aging, then blindly focusing   diverse range of alternatives, based on a preliminary set of 
the next selection on sweet and oak-aged whiskeys, as well     requirements, to focus in on a promising region of a product 
as peaty ones, may result in poor recommendations.             space. A good sales assistant can recognise when the cus•
  Figure 1 shows the scope of this problem by graphing the     tomer sees something that they genuinely like and uses this 
similarity of the preference to the target in a sample recom•  as a cue to switch their strategy to one that tries to refine sub•
mendation dialog. Instead of an increasing similarity pro•     sequent recommendations in the region of the preference by 
file, we find sustained decreases in similarity as MLT follows selecting similar items. 
false-leads. Between cycle 3 and 11, target similarity falls     In this paper, instead of using query modification methods 
from 0.46 to as low as 0.26 as the user is forced to accept    to improve recommendation efficiency, we propose an alter•
poor recommendations, and again between cycle 11 and 40        native called adaptive selection. It adapts the way that new 
there is a noticeable similarity trough. Indeed it is only after items are selected for recommendation and is motivated by 
cycle 38 that MLT finally begins to focus on the target region the above observation that different selection strategies are 
and similarity rises.                                          appropriate at different times in the recommendation process. 


128                                                                                           CASE-BASED REASONING 3.1 Similarity vs Diversity                            balances similarity and diversity for improved recommenda•
Our observation about real-life recommendation scenarios is tion coverage. 
partly echoed by recent research that has questioned the ap• In theory carrying the preference may lead to new ineffi•
parent over-emphasis on similarity in content-based recom- ciencies because the carried preference takes up a valuable 
menders, with diversity forwarded as an important additional slot in each cycle, thus limiting recommendation coverage. 
selection constraint. A number of strategies for improving However, this is easily compensated for because carrying the 
recommendation diversity have been suggested. [Shimazu, preference helps protect against against false-leads. If none 
2001] proposes the recommendation of three items or cases, of the k: - 1 new cases are relevant then by reselecting the 
                                                       carried preference the user is at least maintaining the previ•
i1, in and 73, per recommendation cycle: i1 is the most simi•
lar case to the query; i  is maximally dissimilar from i ; and ous best recommendation rather than being forced to accept a 
                   2                          1        lower quality false-lead. In practice, this on its own can offer 
i13 is maximally dissimilar from i1 and i2. By design these 
items are maximally diverse, but similarity to the query may a substantial improvement to recommendation efficiency. 
be compromised as there are no guarantees that in and i3 will 3.3 Refine & Refocus 
be very similar to the query. 
                                                       As mentioned above, the refine strategy makes use of a stan•
  Other researchers have proposed alternative diversity en•
                                                       dard similarity-based selection method, picking k — 1 new 
hancing mechanisms with a more manageable balance be•
                                                       items that are maximally similar to the current query. The 
tween item diversity and query similarity [Bridge, 2001; 
                                                       refocus strategy uses the bounded-greedy diversity technique 
McSherry, 2002; Smyth and McClave, 20011. Although all 
                                                       proposed by [Smyth and McClave, 2001]; see Figure 2. 
of these techniques introduce diversity into selection in a uni•
                                                         Very briefly, the bounded-greedy technique involves two 
form way, they do not fully reflect the strategy switching seen 
                                                       basic phases. First, the bk most similar items to the query 
in real-world scenarios. Moreover, while increasing diversity 
                                                       are selected (where b is typically an integer between 2 and 5). 
may improve the efficiency of of a recommender by allowing During the second phase, the set (R) of selected items is built 
it to cover more of the item-space per recommendation cy• incrementally. During each step of this build the remainder 
cle, it may also lead to new types of inefficiencies. Diversity of the bk items are ordered according to their quality and the 
enhancing methods pass over items that might otherwise be highest quality item added to R. The quality of a item i is pro•
selected and the target item may be one of these.      portional to the similarity between i and the current query q, 
  A more flexible mechanism for introducing diversity must and to the diversity of i relative to those items so far selected, 
be adopted based on whether or not the recommender has fo•
                                                       R = {r1, ...,rm}; see Equations 1 & 2. 
cussed on the correct region of the item space. If there is evi•
dence that it is correctly focused then a pure similarity-based Quali1y(q,i,R) = a* Sim(q,i) + (1 - a)* Div(i, R) (1) 
selection method can be used to refine the recommendations 
and safe-guard against passing over the target item. If the 
recommender is not correctly focused, then diversity is intro• Div(i,R) = 1 if R = {}; 
duced in order to maximise the coverage of the item space 
in the next cycle, and so help to refocus the recommender by        — — otherwise (2) 
offering the user contrasting recommendations.                                    m 
                                                       The first case to be selected is always the most similar to the 
3.2 Preference Carrying                                query and in subsequent iterations, the chosen case has the 
                                                       highest quality relative to the query and diversity and cases 
Unfortunately it is not immediately obvious how to judge 
                                                       selected so far. Note, we set b = 3 and a = 0.5 to balance 
whether the recommender is correctly focused; the target item 
                                                       similarity and diversity during refocusing. 
is unknown and user requirements are typically vague. Adap•
tive selection solves this by evaluating whether the recom• 4 Evaluation 
mendations made in the iih cycle are an improvement on 
those in the i ~ Ith cycle before choosing a selection strat• In this section we evaluate the performance of adaptive se•
egy for the i + \th cycle.                             lection focusing on the average number of cycles and unique 
  This is achieved by carrying the preference item from the items that must be presented to a user in a typical recommen•
previous cycle to the current cycle. If the user selects the car• dation dialog. The shorter the dialog, the more successful the 
ried preference in the current cycle it must mean that they are recommender is likely to be, and we demonstrate that adap•
unhappy with the k - I new alternatives in that cycle. These tive selection delivers dramatic reductions in dialog length. 
new items must have failed to improve upon the recommen• 4.1 Setup 
dations made during the previous cycle, and so the recom•
mender must not be correctly focussed. If the user ignores the A case-base of 585 unique Scotish whiskey cases (Figure 3) 
carried preference and selects one of the newly recommended is used to compare two comparison-based recommenders us•
items then the recommender must be correctly focused. Thus, ing preference-based feedback: MLT uses the standard MLT 
by carrying the preference item, and monitoring whether or strategy; MLT+AS implements adaptive selection. We use 
not the user reselects it, we can implement a switching mech• a leave-one-out methodology for testing using 200 randomly 
anism between two alternative selection strategies: a refine selected cases as test cases. Each test case (base) is removed 
strategy that emphasises similarity; and a refocus strategy that from the case-base and used in two ways. 


CASE-BASED REASONING                                                                                   129 130                                                                                           CASE-BASED REASONING   First it serves as the basis for a set of queries made by tak• mimics the situation where users are likely to make prefer•
ing random subsets of features. Second, it is used to identify ence mistakes more frequently if there is little difference be•
a maximally similar (target) case. Thus, the base is the ideal tween the target similarities of the recommended items. 
query for a user, the generated query is the initial test query Figure 4(d&e) graph the mean number of cycles and items 
given to the recommender, and the target is the best case for presented to the user versus the noise limit for moderate 
the user given their ideal. During each recommendation cy• queries. As expected, introducing preference noise has a neg•
cle 3 cases are recommended to the user, and the most sim• ative impact on the ability of each recommender to locate the 
ilar to the target is chosen as a preference, thus simulating a target item. MLT dialogs increase from 24 cycles at 0% noise 
user that is capable of correctly selecting the best preference to 39 cycles at 40% noise, and MLT-AS dialogs increase from 
item in each cycle; we relax this assumption in Section 4.3. 11 cycles to 37 cycles. The number of items required is also 
A query is satisfied when the target is returned in a cycle, seen to increase in a similar manner. However, the benefits of 
thereby simulating the situation where the user is seeking a MLT-AS remain across all levels of noise (see Figure 4(f)) al•
specific case; once again, we relax this condition in Section though the magnitude of these benefits is seen to fall as noise 
4.4. In total 1250 queries are generated and divided into three increases. For example, the MLT-AS benefit, in terms of 
groups based on their difficulty; where difficulty is based on unique items, falls from 68% at the 0% noise level to 36% at 
the number of recommendation cycles required by MLT.   the 40% level. Nevertheless, adaptive selection once again of•
                                                       fers significant improvements in recommendation efficiency 
4.2 Recommendation Efficiency                          even when users make imperfect preference selections. While 
Basic recommendation efficiency can be measured in terms of the results presented here focus only on moderate queries, for 
the average number of cycles (or unique cases or items) that reasons of brevity, it is worth noting that qualitatively simi•
a user must work through before being presented with their lar results are found for simple and difficult queries. Once 
ideal target. The leave-one-out method outlined above is used again the MLT-AS benefits increase with query difficulty; for 
by the two recommenders for each of the 1250 queries and example, in terms of unique items, at the 40% noise level, a 
and the mean number of cycles and unique items presented 27% MLT-AS benefit is observed for simple queries, and a 
to the user are measured. The results are shown in Figure 43% MLT-AS benefits for difficult queries. 
4(a-b) as graphs of mean cycles and items for each algorithm 
and query group. The benefits of MLT+AS should be clear 4.4 Target Noise 
for both measures. For example, we see that MLT requires Our second key assumption is that users are interested in a 
17.5 cycles (and 52.5 items) for simple queries, but MLT-AS single target item during recommendation, and that the dia•
needs only 9.5 cycles (and 20 items), a relative reduction of log only terminates when this item is returned. It is perhaps 
45% in terms of cycles, and 62% in terms of items.     more realistic to assume that the user will be satisfied by any 
  We see similar results for the other query sets. In fact, the one of a group of items that are similar to the optimal target. 
relative reductions enjoyed by MLT-AS increase with query We re-evaluate our recommenders under this more relaxed 
difficulty. The recommendation dialogs associated with more termination condition by repeating the basic efficiency exper•
difficult queries include more false-leads than the dialogs iment except that we terminate each dialog once an item has 
for simpler queries, and so offer adaptive selection an even been recommended that is within some pre-defined similarity 
greater opportunity for dialog reduction. This trend is clearly of the target. A similarity threshold of 70% means that the 
seen in Figure 4(c), which graphs the percentage reduction in dialog terminates when an item that is at least 70% similar to 
cycles and items due to MLT-AS, relative to MLT. For cycles, the targer has been recommended. 
the relative reduction grows from 45% (simple) to 68% (diffi• Figure 4(g-i) presents the results in a number of ways. First 
cult) and for unique items it grows from 62% to 78%. These Figures 4(g&h) graph the mean number of cycles and items 
results clearly show a dramatic benefit for adaptive selection. presented to the user versus the similarity threshold for mod•
Indeed this benefit is so great that MLT-AS is capable of satis• erate queries. As expected, relaxing the termination condi•
fying the most difficult queries far more efficiently than MLT tion results in shorter recommendation dialogs for MLT and 
takes to satisfy even the simplest queries.            MLT-AS. For example, MLT dialogs reduce from just under 
                                                       26 cycles at the 100% similarity threshold (where the optimal 
4.3 Preference Noise                                   item must be recommended) to just over 17 cycles at 60% 
In this experiment we re-evaluate our recommenders by re• similarity. MLT-AS dialogs reduce from 11 cycles to just un•
laxing the assumption that the user always prefers the most der 9 cycles across the same similarity range. The number 
similar item to the target to test whether benefits are found of items required is also seen to reduce in a similar manner. 
with sub-optimal preferences. The above experiment is re• However, positive MLT-AS benefits are maintained across all 
peated except that noise is introduced into the preference se• similarity thresholds (see Figure 4(i)). In fact, as the threshold 
lection by perturbing the similarities between each recom• increases, and the termination condition becomes more rigid, 
mended item and the target by some random amount within a we find increasing benefits for MLT-AS in terms of cycles and 
set noise limit. A 10% noise means that each similarity value items. For example, the MLT-AS benefit, in terms of unique 
can change by up to +/-10% of its actual value. This will po• items, grows from 54%; at the 60% similarity threshold to just 
tentially change the internal ordering of recommended items under 70% at the 100% threshold. Once again, adaptive se•
during each cycle resulting in the selection of a preference lection delivers significant improvements in recommendation 
that may not be the most similar to the target. This approach efficiency across a variety of termination conditions. 


CASE-BASED REASONING                                                                                   131 