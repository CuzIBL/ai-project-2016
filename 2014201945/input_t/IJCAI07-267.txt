               Cluster-Based Selection of Statistical Answering Strategies

                               Lucian Vlad Lita    and  Jaime Carbonell
                                      Carnegie Mellon University
                                        {llita, jgc}@cs.cmu.edu


                    Abstract                          system. In this paper we investigate the beneﬁts of a prin-
                                                      cipled strategy selection method when applied to the main
    Question answering (QA) is a highly complex task  components of a QA system: document retrieval, answer ex-
    that brings together classiﬁcation, clustering, re- traction, and answer merging (i.e. overall QA performance).
    trieval, and extraction. Question answering sys-  We present experiments which show that by carefully select-
    tems include various statistical and rule-based com- ing less than 10% of the available answering strategies, no
    ponents that combine and form multiple strategies signiﬁcant performance degradation is observed. Moreover,
    for ﬁnding answers. However, in real-life scenarios we integrate a cluster-based conﬁdence scoring method with
    efﬁciency constraints make it infeasible to simul- an answer merging component and observe signiﬁcant ques-
    taneously use all available strategies in a QA sys- tion answering performance improvements.
    tem. To address this issue, we present an approach
                                                        Experiments [Collins-Thompson et al., 2004] using the
    for carefully selecting answering strategies that are
                                                      CMU Javelin [Nyberg et al., 2003] and Waterloo’s MultiText
    likely to beneﬁt individual questions, without sig-
                                                      [Clarke et al., 2002] question answering systems corrobo-
    niﬁcantly reducing performance. We evaluate the
                                                      rate the expected correlation between improved document re-
    impact of strategy selection on question answer-
                                                      trieval performance and QA accuracy across systems. Results
    ing performance at several important QA stages:
                                                      suggest that retrieval methods adapted for question answering
    document retrieval, answer extraction, and answer
                                                      which include question analysis performed better than ad-hoc
    merging. We present strategy selection experiments
                                                      IR methods, supporting previous ﬁndings [Monz, 2003].
    using a statistical question answering system, and
    we show signiﬁcant efﬁciency improvements. By       Several practical approaches have been developed to deal
    selecting 10% of the available answering strategies, with the complexity of the question answering process. The
                                                                 [                   ]
    we obtained similar performance when compared     SMU system  Harabagiu et al., 2000 and later the LCC sys-
                                                          [                  ]
    to using all of the strategies combined.          tem  Moldovan et al., 2002 incorporate feedback loops be-
                                                      tween components of their question answering system. The
                                                      CMU system  treats the QA process as planning problem,
1  Introduction and Related Work                      formalizing the notion of feedback. Several other QA sys-
In the past few years, an increasing number of question tems using statistical components [Chu-Carroll et al., 2003;
answering systems have started employing multi-strategy Nyberg et al., 2003; Lita and Carbonell, 2004] introduced
approaches that attempt to complement one another when multiple answering strategies that can be used simultaneously
searching for answers to questions. These approaches of- and their results can be combined. Furthermore, when an-
ten include multiple question classiﬁcations, several retrieval swering complex questions, [Harabagiu and Lacatusu, 2004]
approaches, multiple answer extractors, and different data argue for a multi-strategy approach for question processing,
sources. Question answering performance is often presented extraction, and selection.
within the context of ofﬁcial evaluations where systems are The strategy selection problem is closely related to active
processing batches of questions with no time constraints. learning, which explores the trade-off between performance
However, in real-life scenarios, only a limited number of and cost. While active learning algorithms suggest data for
these strategies (component combinations, parameter set- labeling by minimizing the expected error [Roy and McCal-
tings, etc) can be fully explored. In these cases, the trade- lum, 2001], in the problem of strategy selection, the goal is to
off between performance and problem complexity (and in- reduce QA complexity by limiting the number of answering
directly response time) require careful selection of answering strategies while not increasing the error of the QA process.
strategies such that performance is optimized according to re-
alistic constraints.                                  2   Answering Strategies
  We present an answering strategy selection approach that
directly addresses the performance-complexity trade-off and Most question answering systems are implemented as a
we apply it to a statistical, instance-based question answering pipeline where different stages successively process data.

                                                IJCAI-07
                                                  1653However, for each stage in the QA pipeline there is a vari- into account the answer type, disregarding question structure,
ety of methods that can be employed. Each method typically or domain knowledge.
has different parameters, needs different resources, and may An approach that is similar to using ontologies is ques-
produce answers with different conﬁdences, which may not tion clustering [Lita and Carbonell, 2004], in which training
be comparable across methods. We will refer to a complete questions are clustered according to different similarity crite-
combination of components at each stage in the pipeline as ria such as shared number of n-grams (contiguous sequences
an answering strategy. In most of today’s QA systems, an of words), semantic similarity, and same answer type. Com-
answering strategy consists of the following components: pared to ﬁxed ontologies, this approach is adaptive to training
 1. question analysis – produces an expected answer type, data, is language and domain independent, and allows over-
    extracts question keywords, and analyzes the question. lapping types (clusters) that do not have a hierarchical rela-
    Part of speech tagging, parsing, semantic analysis and tionship. Figure 1 shows the relationship between an ontol-
    additional processing are often used in question analysis. ogy and clustering as it is used in question analysis (stage 1)
 2. retrieval – speciﬁes what query types and what query of a QA process. If clustering is performed at different gran-
    content yield high expected performance. Very often ularities, each cluster corresponds to an ontology node. Thus,
    QA systems pre-specify the query type and additional individual answering strategies are built for different clusters,
    content according to the question and answer types iden- rather than different ontology nodes.
                                                                              1
    tiﬁed earlier in the strategy.                      This clustering approach allows each component in an
 3. answer extraction – speciﬁes how answers are identi- answering strategy to be learned only from i) training ques-
    ﬁed from relevant documents. Answer extraction meth- tions and ii) their known correct answers. Therefore strategies
    ods range from rule and pattern-based extractors to hid- are learned for individual clusters, using corresponding ques-
    den markov models (HMM), maximum    entropy, and  tions as training data. The retrieval component learns which
    support vector machine-based extractors.          queries and query types have high performance when run on
                                                      in-cluster training questions. The answer extraction com-
 Stage          Strategy SA     Strategy SB           ponent is trained on correct answers for all in-cluster ques-
 1) answer type temporal        year                  tions. Finally, the answer merging component considers clus-
 2) queries     when mozart die mozart die biography  ter statistics, retrieval performance, extraction performance
                                mozart died death     and merges answer sets produced by answering strategies.
 3) extraction  rule-based      HMM                     If there is sufﬁcient data for learning (i.e. sufﬁcient num-
                                                      ber of questions), the more clusters of training questions a
Table 1: Answering strategies SA and SB use different answer QA system generates, the more answering strategies will be
types, different queries, and extraction methods.     applied to new questions. However, while QA performance
                                                      may increase with additional answering strategies, so will the
  When applied to a new question, an answering strategy noise (e.g. from irrelevant clusters) and the time it takes to
processes the question text, retrieves documents and extracts actually run the strategies. Our goal is to allow the exis-
a set of possible answers. In the case when multiple strate- tence of multiple cluster-based strategies, but only select a set
gies are simultaneously applied to a new question, an answer of clusters associated to the strategies most likely to lead to
merging component is employed to combine answers and  high performance. For document retrieval, high performance
conﬁdences into a ﬁnal answer set. Table 1 shows two sim- translates into high recall of relevant documents. For answer
plistic strategies for the question “When did Mozart die?”. In extraction high performance corresponds to a large number
realistic scenarios the question analysis component produces of correct answers being extracted.
more information than just an expected answer type, several Queries learned by different strategies often lead to some of
queries are generated according to pre-speciﬁed types, and the same relevant documents – e.g. the queries “the ﬁrst aria
various processing is performed before answer extraction. composed Mozart”vs.“aria Mozart” may lead to an overlap
                                                      in their retrieved document sets. If a strategy already leads
2.1  Cluster-Based Strategies                         to the retrieval of a document di, subsequent strategies will
As the ﬁrst stage in answering strategies, most question an- not beneﬁt if they retrieve di again. Therefore, each strategy
swering systems employ question ontologies. These on- selection depends on the n-1 previously selected strategies
tologies combine expected answer types (date, location etc)
and question types (birthday(X), nickname(X), construc- 3 Experiments & Results
tion date(X) etc). Consider again the question “When did For our experiments we have chosen to use a statistical ques-
Mozart die?”. Depending on the desired answer type gran- tion answering system for several reasons. Statistical QA
ularity, this question can be classiﬁed as a temporal question, systems are usually faster to implement than rule-based sys-
a temporal::year question, or more speciﬁcally as a tempo- tems, they require less knowledge resources and limited man-
ral::year::death year question. Each classiﬁcation may lead ual input, and their results are easier to replicate on standard
to an entirely different answering strategy. Existing systems datasets. In this paper we have implemented an instance-
consider answer types ranging from simple answer type sets based question answering (IBQA) system [Lita and Car-
and QA speciﬁc ontologies to semantic networks such as
WordNet which provide better coverage and more speciﬁcity. 1for more details about this data-driven framework, refer to the
However, these ontologies are very restrictive and only take [Lita and Carbonell, 2004] publication

                                                IJCAI-07
                                                  1654Figure 1: (a) Classiﬁcation according to a question ontology versus classiﬁcation according to a set of clusters in the training data (b)
Answering a new question using multiple strategies learned from training questions in each cluster (or ontology node). The answer sets
produced by individual cluster-based strategy are merged into a single ranked answer set.

bonell, 2004]. The question analysis component relies on be able to extract multiple instances of a correct answer and
clustering training questions at different levels of granular- at the same time assign them higher conﬁdence scores com-
ity and then classifying new questions into these clusters. For pared to incorrect answers. Towards this end, we evaluate
each cluster, an answering strategy is learned which equiva- question answering performance using several metrics:
lent to a set of retrieval, extraction, and answer type models. • Mean reciprocal rank (MRR) is computed as the inverse
In the remainder of the paper we shall refer to these strategies of the rank of the ﬁrst correct answer returned by a sys-
as cluster-based since a single answering strategy is learned tem. The higher the ﬁrst correct answer can be found in
from the training questions of an individual cluster.     the ranked answer list, the higher the MRR score.
  When new questions are presented to a QA system, they • The second metric tests the existence of at least one cor-
are classiﬁed according to the training clusters and the cor- rect answer within the top ﬁve answers (Top5)andis
responding answering strategies are activated, each generat- less strict compared to MRR.
ing a set of answers. The strategy-speciﬁc answers are then • Towards the goal of redundancy, for document retrieval
merged into a ﬁnal ranked answer list. Details on learning and we evaluate the number of relevant documents obtained
implementation of the answer type, retrieval, and extraction (i.e. documents containing at least a correct answer) and
models can be found in [Lita and Carbonell, 2004].        for answer extraction we evaluate the number of correct
  The question datasets used in the experiments presented in answers extracted.
this paper consist of all temporal questions from previous trec
evaluations TREC 8-13 [Voorhees, 1999 to 2004]. Temporal                 questions      strategies
questions have the advantage of having a relatively high den-           MRR    Top5   MRR    Top5
sity necessary in training statistical QA components. They   all       0.431   0.656  0.256  0.461
are also distributed such that they cover simple questions such extracted 0.576 0.879 0.267  0.480
as “When did Beethoven die?”, but also structurally more
complex questions such as “What year did General Mont- Table 2: Statistical QA system results on temporal questions for
gomery lead the Allies to a victory over the Axis troops in i) all questions and ii) questions for which at least an answer was
North Africa?”. Each question in this collection has a set of extracted. We show average score over all questions, but also the
corresponding answer keys in the form of regular expressions. average performance over all strategies. However, for question per-
  The questions were processed with a part of speech tagger formance, only a small number of strategies need to be successful.
[Brill, 1994], a morphological analyzer [Minnen et al., 2001],
and a sentence splitter. Synonyms and hypernyms were ex- For comparison purposes, we have evaluated the instance-
tracted and used to enhance keyword-based queries in doc- based QA system using MRR and Top5. Table 2 shows the
ument retrieval. Several queries are generated for each an- MRR and the Top5 scores for the question dataset used in
swering strategy and at most one hundred documents were this paper. We present the QA system performance over all
retrieved for each of these query through the Google API questions (all) in the dataset as well as the performance the
(www.google.com/api). Documents containing any reference questions with at least one proposed answer (extracted). Intu-
to TREC or the actual question, and other problematic con- itively, the latter measure can be viewed as precision and the
tent were ﬁltered out.                                former measure as recall. At the question level, which is what
  Two desired qualities in question answering systems are TREC evaluations [Voorhees, 1999 to 2004] use, we show
1) correctness – correct answers should have higher rank the performance by averaging over all questions – for each
than incorrect answers, and 2) redundancy – correct answers question we combine results from all strategies. At the strat-
should occur more than once in documents. More speciﬁ- egy (cluster) level, we compute the performance by averaging
cally, the retrieval component should produce multiple rel- over all strategies – for each strategy, we compute the perfor-
evant documents; the answer extraction component should mance over all questions it can be applied to. Note that not

                                                IJCAI-07
                                                  1655all answering strategies are successful, hence the lower MRR speciﬁc answering strategies. We measure the retrieval conﬁ-
average. At the same time, questions beneﬁt from multiple dence conf(AIR(Cj )|q) of an answering strategy A derived
strategies and therefore their average MRR is higher. Per- from cluster Cj given a new test question q:
formance in the above experiments was computed through
leave-one-out cross-validation.                                              +
                                                       conf(AIR(Cj )|q)=P  (d |AIR(Cj )) · P (Cj |q) · s(j) (1)
  In these experiments, each iteration corresponds to an an-
                                                      where P (d + |AIR(Cj)) is the probability of retrieving a rel-
swering strategy being selected. The newly selected answer-          +
ing strategy includes speciﬁc query types that are known to evant document d using strategy AIR(Cj) and is measured
perform well on training questions in the their respective by testing its effectiveness on a held-out set of questions from
clusters. In addition, a cluster-speciﬁc SVM extractor ﬁnds the cluster. P (Cj|q) is the probability of a cluster containing
and scores potential answers. In some of these experiments, questions similar to q and is given by the average similarity
the computation of a greedy-optimal (oracle) cluster selection between q and qj (i ∈ Cj ) normalized over all clusters. s(j)
method is tractable. This is not to be confused with a global is a minimal cardinality condition for clusters.
optimal classiﬁer that ﬁnds the absolute best strategy selec- Figure 2 shows the effect of using conﬁdence-based selec-
tion sequence – when tractable, we present its performance. tion in order to iteratively add appropriate answering strate-
                                                      gies (i.e. beneﬁcial query content). The more strategies are
3.1  Selection for Document Retrieval                 employed to create queries and retrieve new documents, the
We assume a document to be relevant in the context of ques- less time will be available for answer extraction and answer
tion answering if it contains a correct answer in a correct con- merging. The iterative process offers a good trade-off be-
text. Since it is very difﬁcult to automatically evaluate the tween performance and number of strategies used, as well as
correctness of context, the notion of relevance is sometimes a good basis for user-deﬁned utility functions. In our exper-
                                                                                       10%
relaxed to whether a document contains the correct answer, iments, if the QA system selects only of the available
                                                                                                   80%
regardless of context. Through cluster-speciﬁc data, the re- strategies, the retrieval performance is approximately of
trieval component of the QA system learns n-grams and fea- the maximum achievable using the existing current strategies.
tures that improve retrieval when added to queries. The im- 3.2 Selection for Answer Extraction
provement is measured when these queries are used to re-
trieve documents for the questions in the same cluster. The For a particular cluster, the SVM answer extractor is trained
learned features become part of the cluster-based answering on the documents obtained by running the retrieval compo-
strategy which can be applied to new similar questions. nent of the answering strategy (i.e. using the learned queries
                                                      to retrieve documents). The basic features include proximity
                                                      features, sentence statistics, and patterns (n-grams and para-
             Strategy Selection for Document Retrieval
       140                                            phrases) that discriminate best between sentences that contain
                                                      correct answers and sentences that do not. For the classiﬁer,
       120                                            the value of these features is given by information gain.
                                                        The answer extractor used in these experiments consists of
       100                                            a support vector machine (SVM) classiﬁer [Joachims, 2002]
                                                      producing probabilities, and whose task is to decide whether
        80                                            or not sentences contain at least one correct answer. The
                                                      SVM   was trained on features extracted from one-sentence
        60                                            passages containing at least one keyword from the original
                                                      question. The features consist of: distance between keywords
        40
                                                      and potential answers, keyword density in a passage, simple
      #  of relevant docs retrieved
        20                 confidence selection       statistics such as document and sentence length, query type,
                           random selection           lexical n-grams (up to six words in length), and paraphrases.
         0                                              Under the cluster-based approach, it is not sufﬁcient to
         0      20     40    60     80     100        train an answer extractor for each answering strategy. These
                   # iterations (strategies)
                                                      strategies are trained on different number of questions (i.e.
                                                      cluster size), they are sensitive to the notion of cluster rele-
Figure 2: Smart selection based on strategy conﬁdence allows the
                                                      vance, and they are based on different query types and dif-
QA system to employ only 10% of its available strategies to retrieve
                                                      ferent relevant document distributions. Therefore, extractor
80% of the accessible relevant documents.
                                                      conﬁdence has to be taken within the context of its history –
  When trying to answer the question “When did Mozart i.e. the rest of the answering strategy. We measure the an-
                                                      swer extraction conﬁdence conf(AAE(Cj )|q) of a strategy
die?” it may be beneﬁcial to create queries that contain fea- A          C                        q
tures such as “biography”, “cemetery”, “spent his life”, “sac- derived from cluster j given a new test question :
riﬁced himself”, etc. In many QA systems the retrieval com-
                                                                             +
ponent contains rules for building better queries for speciﬁc conf(AAE(Cj )|q)=P (a |AAE (Cj)) · conf(AIR(Cj )|q)
types of questions – in our example: time of death.Inthe                                              (2)
                                                               +
cluster-based approach, these features are learned from other where P (a |AAE (Cj) is the probability of extracting a cor-
                                                                  +
similar questions in the training data, and get added to cluster- rect answer a using the answering strategy AAE(Cj ) –

                                                IJCAI-07
                                                  1656                                    Strategy Selection for Answer Extraction (w/o Merging)
                                                                          0.35
    1                                 0.25
                                                                           0.3
  0.8
                                       0.2                                0.25

  0.6                                 0.15                                 0.2

                                     MRR                                 Top5 0.15
  0.4                                  0.1
                                                                           0.1
                                                        greedy oracle
  0.2                                 0.05              confidence selection
 fraction  of extracted answers                         random selection  0.05
                                                        cluster size selection
    0                                   0                                   0
     0    20   40    60   80   100       0    20   40    60   80   100       0    20    40   60    80   100
           # iterations (strategies)           # iterations (strategies)            # iterations (strategies)

Figure 3: Selection based on conﬁdence yields the best performance after carefully selecting a limited number of strategies (cluster, queries,
and extractor) to answer a question. However, for better redundancy it is better use additional answering strategies if further correct answers
are required. No answer merging method has been used here – answers preserve their individual strategy-generated scores.

more speciﬁcally, using the cluster-trained SVM extractor. answer extraction stage in a QA system.
   +
P (a |AAE(Cj ) is measured by testing its effectiveness on
a held-out set of training questions from the cluster. 3.3 Selection for Answer Merging
                                                      At this stage in the QA pipeline, all answering strategies that
  In Figure 3 we evaluate the effectiveness of our selection
                                                      were activated produce a strategy-speciﬁc answer set. The
method (conﬁdence selection) according to MRR, Top5, and
                                                      task of the answer merging component is to make use of re-
the fraction of correct answers extracted out of the total num-
                                                      dundancy and re-score the answers such that the correct an-
ber of correct answers that would be extracted if all strate-
                                                      swers are ranked higher than incorrect answers. The answer-
gies were used. The random selection consists of random
                                                      ing merging method we implemented consists of a weighted
sampling from the available strategies and using them to ex-
                                                      sum of the individual answer conﬁdences for all answer in-
tract more answers. The cluster size selection is an intuitive
                                                      stances with the same surface form. The answer conﬁdence
baseline which gives priority to more speciﬁc, focused strate-
                                                      conf(ak|q) of an answer ak at the end of the question an-
gies that correspond to clusters with higher similarity to the
                                                      swering process is aggregated across all clusters Cj and is
test question: P (Cj|q). However, it does not perform well,
                                                      given by:
since cluster similarity is a necessary property, but it is not
sufﬁcient in the selection process. Finally, the greedy oracle       
optimizes at each iteration the strategy that yields the most conf(ak|q)= P (ak|AAE(Cj )) · conf(AAE(Cj)|q)
additional correct answers. In many cases, our conﬁdence se-       ak Cj
lection method performs virtually indistinguishable from the                                          (3)
oracle sequence.                                      where P (ak|AAE(Cj ) is the probability of extracting a cor-
                                                                a                          A   (C )
  While there is a beneﬁt in using this selection method to rect answer k using the answering strategy AE j .
quickly obtain a larger number of correct answers, if high an- Both in terms of MRR and Top5 scores, as seen from Fig-
swer redundancy is required, then further strategies must be ure 4 and Figure 3, the weighted answer merging method
                                                      gains approximately 0.15 MRR points (60%)andalso0.15
used. However, in terms of MRR and Top5 scores, a very            43%
small number of carefully selected strategies can be as ef- Top5 points ( ) in performance. The gap trade-off be-
fective as utilizing all of the available answering strategies. tween using the conﬁdence selection scores and using all
A very important observation is that performance does not strategies also improved. As in the case of answer extrac-
degrade with subsequent iterations and increased number of tion, it is encouraging that the conﬁdence selection approach
strategies. This can be explained by the fact that the best closely follows the greedy optimal selection.
strategies provide the highest conﬁdence values and corre-
sponding answer scores, while unsuccessful strategies do not 4 Conclusions & Future Work
introduce additional noise. In these experiments no answer An increasing number of question answering systems are re-
merging method was used. Each instance of an answer was lying on multi-strategy approaches in order to ﬁnd answers
treated separately, with its original conﬁdence score given by to questions. They rely on multiple question classiﬁcations,
a speciﬁc strategy. This approach does not provide any boost answer extractors, multiple retrieval methods using several
in conﬁdence if the same answer has been seen more than data sources, and different web-based services. While QA
once. However, it provides a measure of relative answering performance is often presented on batch processing of ques-
strategy noise and is informative to the performance of the tions with no time constraints, in real-life scenarios, only a

                                                IJCAI-07
                                                  1657