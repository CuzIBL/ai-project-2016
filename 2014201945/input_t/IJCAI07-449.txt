                Collapsed Variational Dirichlet Process Mixture Models∗

         Kenichi Kurihara                    Max Welling                      Yee Whye Teh
     Dept. of Computer Science         Dept. of Computer Science        Dept. of Computer Science
Tokyo Institute of Technology, Japan        UC Irvine, USA            National University of Singapore
     kurihara@mi.cs.titech.ac.jp          welling@ics.uci.edu            tehyw@comp.nus.edu.sg


                    Abstract                          be orders of magnitude faster than sampling, especially when
                                                      special data structures such as KD trees are used to cache cer-
    Nonparametric Bayesian mixture models, in partic- tain sufﬁcient statistics [Moore, 1998; Verbeek et al., 2003;
    ular Dirichlet process (DP) mixture models, have  Kurihara et al., 2006].
    shown great promise for density estimation and      [Blei and Jordan, 2005] recently applied the framework of
    data clustering. Given the size of today’s datasets, variational Bayesian (VB) inference to Dirichlet process (DP)
    computational efﬁciency becomes an essential in-  mixture models and demonstrated signiﬁcant computational
    gredient in the applicability of these techniques to gains. Their model was formulated entirely in the truncated
    real world data. We study and experimentally com- stick-breaking representation. The choice of this representa-
    pare a number of variational Bayesian (VB) ap-    tion has both advantages and disadvantages. For instance,
    proximations to the DP mixture model. In partic-  it is very easy to generalize beyond the DP prior and use
    ular we consider the standard VB approximation    much more ﬂexible priors in this representation. On the ﬂip
    where parameters are assumed to be independent    side, the model is formulated in the space of explicit, non-
    from cluster assignment variables, and a novel col- exchangeable cluster labels (instead of partitions). In other
    lapsed VB approximation where mixture weights     words, randomly permuting the labels changes the probabil-
    are marginalized out. For both VB approximations  ity of the data. This then requires samplers to mix over cluster
    we consider two different ways to approximate the labels to avoid bias [Porteous et al., 2006].
    DP, by truncating the stick-breaking construction,
                                                        In this paper we propose and study alternative approaches
    and by using a ﬁnite mixture model with a sym-
                                                      to VB inference in DP mixture models beyond that proposed
    metric Dirichlet prior.
                                                      in [Blei and Jordan, 2005]. There are three distinct contri-
                                                      butions in this paper: in proposing an improved VB algo-
1  Introduction                                       rithm based on integrating out mixture weights, in comparing
                                                      the stick-breaking representation against the ﬁnite symmet-
Mixture modeling remains one of the most useful tools in ric Dirichlet approximation to the DP, and in the maintain-
statistics, machine learning and data mining for applications ing optimal ordering of cluster labels in the stick-breaking
involving density estimation or clustering. One of the most VB algorithms. These lead to a total of six different algo-
prominent recent developments in this ﬁeld is the application rithms, including the one proposed in [Blei and Jordan, 2005].
of nonparametric Bayesian techniques to mixture modeling, We experimentally evaluate these six algorithms and compare
which allow for the automatic determination of an appropriate against Gibbs sampling.
number of mixture components. Current inference algorithms In Section 2.1 we explore both the truncated stick-breaking
for such models are mostly based on Gibbs sampling, which approximation and the ﬁnite symmetric Dirichlet prior as ﬁ-
suffer from a number of drawbacks. Most importantly, Gibbs nite dimensional approximations to the DP. As opposed to the
sampling is not efﬁcient enough to scale up to the large scale truncated stick-breaking approximation, the ﬁnite symmetric
problems we face in modern-day data mining. Secondly, sam- Dirichlet model is exchangeable over cluster labels. Theoret-
pling requires careful monitoring of the convergence of the ically this has important consequences, for example a Gibbs
Markov chain, both to decide on the number of samples to sampler is not required to mix over cluster labels if we are
be ignored for burn-in and to decide how many samples are computing averages over quantities invariant to cluster label
needed to reduce the variance in the estimates. These con- permutations (as is typically the case).
siderations have lead researchers to develop deterministic al-
                                                        In Section 2.2 we explore the idea of integrating out the
ternatives which trade off variance with bias and are easily
                                                      mixture weights π, hence collapsing the model to a lower di-
monitored in terms of their convergence. Moreover, they can
                                                      mensional space. This idea has been shown to work well for
  ∗This material is based in part upon work supported by the LDA models [Teh et al., 2006] where strong dependencies ex-
National Science Foundation under Grant Number D/HS-0535278. ist between model parameters and assignment variables. Such
YWT thanks the Lee Kuan Yew Endowment Fund for funding. dependencies exist between mixture weights and assignment

                                                IJCAI-07
                                                  2796variables in our mixture model context as well, thus collaps- after T terms,
ing could also be important here. This intuition is reﬂected in
                                                           v ∼B   v  ,α                i   , ..., T −
the observation that the variational bound on the log evidence i ( i;1  )               =1         1  (1)
is guaranteed to improve.                                  vT =1                                      (2)
                                                                  
  In Section 3 we derive the VB update equations corre-    πi = vi  (1 − vj)              i =1, ..., T (3)
sponding to the approximations in Section 2. We also con-         j<i
sider optimally reordering cluster labels in the stick-breaking
                                                           πi =0                               i>T    (4)
VB algorithms. As mentioned, the ordering of the cluster la-
bels is important for models formulated in the stick-breaking B v ,α                          v
                                                      where  ( ;1  ) is a beta density for variable with para-
representation. In the paper [Blei and Jordan, 2005] this issue                            T
                                                      meters 1 and α, and one can verify that i=1 πi =1.In-
was ignored. Here we also study the effect of cluster reorder- corporating this into a joint probability over data items X =
ing on relevant performance measures such as the predictive
                                                      {xn},n=1, ..., N, cluster assignments z = {zn},n=
log evidence.
                                                      1, ..., N, stick-breaking weights v = {vi},i=1, ..., T and
  The above considerations lead us to six VB inference meth- cluster parameters η = {ηi},i=1, ..., T we ﬁnd
ods, which we evaluate in Section 4. The methods are: 1)
                                                        P (X, z, v, η)=
the truncated stick-breaking representation with standard VB                                    
(TSB), 2) the truncated stick-breaking representation with N                      T
                                                             p x |η   p z |π v        p η B v   ,α
collapsed VB (CTSB), 3) the ﬁnite symmetric Dirichlet rep-    ( n  zn ) ( n ( ))       ( i) ( i;1 )   (5)
resentation with standard VB (FSD), 4) the ﬁnite symmetric n=1                     i=1
Dirichlet presentation with collapsed VB (CFSD), and 5) and
                                                      where π(v) are the mixture weights as deﬁned in (3). In this
6) being TSB and CTSB with optimal reordering (O-TSB and
                                                      representation the cluster labels are not interchangeable, i.e.
O-CTSB respectively).
                                                      changing labels will change the probability value in (5). Note
                                                      also that as T →∞the approximation becomes exact.
                                                        A second approach to approximate the DP is by assuming
2  Four Approximations to the DP                      a ﬁnite (but large) number of clusters, K, and using a sym-
                                                      metric Dirichlet prior D on π [Ishwaran and Zarepour, 2002],
                                                                       π ∼Dπ    α , ..., α
We describe four approximations to the DP in this section.                   ( ; K    K )             (6)
These four approximations are obtained by a combination of This results in the joint model,
truncated stick-breaking/ﬁnite symmetric Dirichlet approxi-
mations and whether the mixture weights are marginalized P (X, z, π, η)=
                                                                                   
out or not. Based on these approximations we describe the N                   K
six VB inference algorithms in the next section.            p x  |η  p z |π       p η   D π  α , ..., α
                                                             ( n  zn ) ( n )       ( i)   ( ; K    K ) (7)
  The most natural representation of DPs is using the Chi- n=1                 i=1
nese restaurant process, which is formulated in the space of
                                                      The essential difference with the stick-breaking representa-
partitions. Partitions are groupings of the data independent
                                                      tion is that the cluster labels remain interchangeable under
of cluster labels, where each data-point is assigned to exactly
                                                      this representation, i.e. changing cluster labels does not
1 group. This space of partitions turns out to be problem-
                                                      change the probability [Porteous et al., 2006]. The limit
atic for VB inference, where we wish to use fully factorized
                                                      K  →∞is somewhat tricky because in the transition K →
variational distributions on the assignment variables, Q(z)=
                                                     ∞  we switch to the space of partitions, where states that
   q(zn). Since the assignments z1 =1,z2 =1,z3 =2rep-
  n                                                   result from cluster relabelings are mapped to the same par-
resent the same partition (1, 2)(3) as z1 =3,z2 =3,z3 =2,
                                                      tition. For example, both z1   ,z2     ,z3      and
there are intricate dependencies between the assignment vari-                    =1      =1      =2
                                                      z1 =3,z2   =3,z3  =2are mapped to the same partition
ables and it does not make sense to use the factorization
                                                        ,    .
above. We can circumvent this by using ﬁnite dimensional (1 2)(3)
approximations for the DP, which are formulated in the space In ﬁgure 1 we show the prior average cluster sizes under
                                                      the truncated stick-breaking (TSB) representation (left) and
of cluster labels (not partitions) and which are known to
closely approximate the DP prior as the number of explic- under the ﬁnite symmetric Dirichlet (FSD) prior (middle) for
itly maintained clusters grows [Ishwaran and James, 2001; two values of the truncation level and number of clusters re-
                                                      spectively. From this ﬁgure it is apparent that the cluster
Ishwaran and Zarepour, 2002]. These ﬁnite approximations
are what will we discuss next.                        labels in the TSB prior are not interchangeable (the proba-
                                                      bilities are ordered in decreasing size), while they are inter-
                                                      changeable for the FSD prior. As we increase T and K these
2.1  TSB and FSD Approximations                       priors approximate the DP prior with increasing accuracy.
                                                        One should note however, that they live in different spaces.
                                                      The DP itself is most naturally deﬁned in the space of parti-
In the ﬁrst approximation we use the stick-breaking represen- tions, while both TSB and FSD are deﬁned in the space over
tation for the DP [Ishwaran and James, 2001] and truncate it cluster labels. However, TSB and FSD also live in different

                                                IJCAI-07
                                                  2797        Truncated Stick−Breaking Representation Finite Symmetric Dirichlet Prior Truncated Stick−Breaking Representation 2
  0.4                               0.18                               0.5

                                                                      0.45
  0.35                              0.16
                                                         K=6           0.4
                                    0.14
  0.3
                                                                      0.35
                                    0.12                       K=11
  0.25                                                                 0.3
                                     0.1
  0.2                                                                 0.25
                                    0.08
                   T=6                                                 0.2
  0.15
                                    0.06
                                                                      0.15
                                                                      expected  cluster size
 expected  cluster size 0.1         expected  cluster size                              T=6
                             T=11   0.04                               0.1
                                                                                                  T=11
  0.05                              0.02                              0.05

   0                                 0                                  0
      1 2  3 4  5 6  7 8 9  10 11       1  2 3  4 5 6  7 8  9 10 11        1 2 3  4 5  6 7  8 9 10 11
               cluster label                      cluster label                     cluster label

Figure 1: Average cluster size for three ﬁnite approximations to the DP prior. Left: Truncated stick-breaking prior (TSB) as given in (3).
Middle: Finite Symmetric Dirichlet prior (FSD). Right: Stick-breaking representation corresponding to the FSD prior. In each ﬁgure we
show results for two truncation levels: T/K =6(left bars) and T/K =11(right bars).

spaces! More precisely, one can transform a sample from with
the FSD prior into the stick-breaking representation by per-    N                    N
forming a size-biased permutation of the mixture weights π N       I z   i     N         I z >i
                         D  π                               i =     ( n = )      >i =     ( n   )    (11)
(i.e. after every sample from ( ) we sample an ordering        n=1                    n=1
according to π without replacement). As it turns out, for ﬁ-
nite K this does not exactly recover the left hand ﬁgure in and N≥i = Ni + N>i. For FSD we ﬁnd instead,
1, but rather samples from a prior very closely related to it                  
                                                                            α    K    N     α
shown in the right pane of ﬁgure 1. This prior is given by      p    z    Γ( )   k=1 Γ( k + K )
                                                                  FSD( )=               α K          (12)
a stick-breaking construction as in eqn.(3) with stick-lengths              Γ(N  + α)Γ( K )
sampled from,
                           α      iα                  3   Variational Bayesian Inference
             vi ∼B(vi;1+     ,α−    )           (8)
                          K       K                   The variational Bayesian inference algorithm [Attias, 2000;
Conversely, we can obtain samples from the FSD prior by Ghahramani and Beal, 2000] lower bounds the log marginal
applying a random, uniformly distributed permutation on the likelihood by assuming that parameters and hidden variables
cluster weights obtained from eqn.(8). Although these two are independent. The lower bound is given by,
                                                                          
stick-breaking constructions are slightly different, for large                            P X, z, θ
enough K, T they are very similar and we do not expect any L X ≥ B X          Q  z Q θ      (      )
                                                         (  )    ( )=           ( ) ( )log Q z Q  θ  (13)
difference in terms of performance between the two.                     z   dθ              ( )  ( )
2.2  Marginalizing out the Mixture Weights            where θ is either {η, v}, {η, π} or {η} in the various DP
The variational Bayesian approximations discussed in the approximations discussed in the previous section. Approxi-
next section assume a factorized form for the posterior dis- mate inference is then achieved by alternating optimization
tribution. This means that we assume that parameters are in- of this bound over Q(z) and Q(θ). In the following we will
dependent of assignment variables. This is clearly a very bad spell out the details of VB inference for the proposed four
assumption because changes in π will have a considerable methods. For the TSB prior we use,
         z                                                                                   
impact on . Ideally, we would integrate out all the parame-               N        T
ters, but this is too computationally expensive. There is how- Q z, η, v     q z       q η  q v
                                        π                   TSB(     )=       ( n)      ( i) ( i)    (14)
ever a middle ground: we can marginalize out from both                     n        i=1
methods without computational penalty if we make another
approximation which will be discussed in section 3.3. For where q(v) is not used in the TSB model with v marginalized
both TSB and FSD representations the joint collapsed model out. For the FSD prior we use,
over X, z, η is given by,                                                                
                                                                     N         K
                 N                  ∞
                                                        QFSD(z, η, π)=    q(zn)      q(ηk)  q(π)   (15)
   P  X, z, η       p x |η    p z      p η
     (      )=       ( n  zn ) ( )      ( i)    (9)                       n         k=1
                n=1                 i=1
                                                              q π
with different distributions over cluster labels p(z) in both As well, ( ) is left out for the collapsed version.
cases. For the TSB representation we have,
                                                     3.1  Bounds on the Evidence
                      Γ(1 + Ni)Γ(α + N>i)
         pTSB(z)=                              (10)   Given the variational posteriors we can construct bounds on
                              α   N≥i
                  i<T   Γ(1 +   +     )               the log marginal likelihood by inserting Q into eqn.(13). Af-

                                                IJCAI-07
                                                  2798            ter some algebra we ﬁnd the following general form,   where the conditional p(zn|z¬n) is different for the FSD and
                                                                  TSB priors. For the TSB prior we use (10), giving the condi-
                       N    
                                                                  tional
               B X                 q zn q ηz    p xn|ηz
                 ( )=               (  ) ( n )log (    n )                                 ¬n             ¬n
                               dηzn                                                  1+N             α + N>i
                       n=1 zn                                       p z    i|z             i
                                                                    ( n =   ¬n)=            ¬n              ¬n  (22)
                                       N                                        1+α   + N≥i     1+α   + N≥i
                                 p(ηi)                                                          j<k
               +         q(ηi)log     −       q(zn)logq(zn)
                       η         q(ηi)                                  N ¬n   N −  I z   i  N ¬n  N    − I z >i
                   i  d i               n=1 zn                    where  i  =   i   ( n =  ), >i =   >i   ( n    ) are
                                                                                            z
               + Extra Term                                (16)   the corresponding counts with n removed. In contrast, for
                                                                  the FSD prior we have,
            where the “extra term” depends on the particular method. For                         ¬n   α
                                                                                               N   +
            the TSB prior we have,                                             p z    k|z       k     K
                                                                                ( n =    ¬n)=  N ¬n   α          (23)
                                                                                                 +
                        N  T           zn
              TermTSB =         q(zn)       q(vi) log p(zn|v)     3.3  Gaussian Approximation
                                      dv
                        n=1 zn=1         i=1                      The expectation required to compute the update (21) seems
                          T                                     intractable due to the exponentially large space of all assign-
                                         p(vi)                             z
                        +        q(vi)log                  (17)   ments for . It can in fact be computed in polynomial time
                                         q vi
                          i=1  dvi        (  )                    using convolutions, however this solution still tended to be
                                                                  too slow to be practical. A much more efﬁcient approximate
            On the other hand for the FSD prior we ﬁnd,
                                                                  solution is to observe that both random variables Ni and N>i
                                  
                              K                                 are sums over Bernoulli variables: Ni = n I(zn = i) and
                                                                  N         I z  >i
                 TermFSD =            q(zn)q(π)logp(zn|π)           >i =   n ( n    ). Using the central limit theorem these
                            n z =1  dπ                            sums are expected to be closely approximated by Gaussian
                              n
                                        p π                       distributions with means and variances given by,
                                q π      (  )
                           +      ( )log q π               (18)                  N
                              dπ         (  )
                                                                         E[Ni]=     q(zn = i)                    (24)
            For both collapsed versions these expressions are replaced by,       n=1
                                              
                                      N                                        N
                   TermCTSB/CFSD =        q(zn) log p(z)   (19)          V[Ni]=     q(zn = i)(1 − q(zn = i))     (25)
                                   z   n=1                                       n=1
                                                                                  N  
            3.2  VB Update Equations
                                                                         E[N>i]=         q(zn = j)               (26)
            Given these bounds it is now not hard to derive update equa-          n=1 j>i
            tions for the various methods. Due to space constraints we
                                                                                  N             
            will refer to the papers [Blei and Jordan, 2005; Ghahramani  V N             q z    j    q z    k
            and Beal, 2000; Penny, 2001; Yu et al., 2005] for more details [ >i]=         ( n =  )    ( n =  )   (27)
                                                                                  n=1
            on the update equations for the un-collapsed methods and fo-              j>i         k≤i
            cus on the novel collapsed update equations.          To apply this approximation to the computation of the average
              Below we will provide the general form of the update in (21), we use the following second order Taylor expansion,
            equations where we do not assume anything about the par-
                                 p η                                                          1  
            ticular form of the prior ( i). The equations become par-     E[f(m)] ≈ f(E(m)] +   f (E[m])V[m]     (28)
            ticularly simple when we choose this prior in the conju-                          2
            gate exponential family. Explicit update equations for q(ηi) This approximation has been observed to work extremely
            can be found in the papers [Ghahramani and Beal, 2000; well in practice, even for small values of m.
            Blei and Jordan, 2005; Penny, 2001; Yu et al., 2005].
              For q(ηi) we ﬁnd the same update for both methods,  3.4  Optimal Cluster Label Reordering
                                                      

                                                                 As discussed in section 2.1 the stick-breaking prior assumes a
               q(ηi) ∝ p(ηi)exp    q(zn = i)logp(xn|ηi)    (20)   certain ordering of the clusters (more precisely, a size-biased
                                 n                                ordering). Since a permutation of the cluster labels changes
                                                                  the probability of the data, we should choose the optimal per-
                    q z
            while for ( n) we ﬁnd the update                      mutation resulting in the highest probability for the data. The
                           ⎛                         ⎞
                                                                optimal relabelling of the clusters is given by the one that or-
                           ⎝                         ⎠            ders the cluster sizes in decreasing order (this is true since
                q(zn) ∝ exp         q(zm)logp(zn|z¬n)
                                                                  the average prior cluster sizes are also ordered). In our ex-
                             z¬n m=n
                                                    
            periments we assess the effect of reordering by introducing
                                                                  algorithms O-TSB and O-CTSB which always maintain this
                       ×            q η     p x  |η
                         exp         ( zn )log ( n zn )    (21)   optimal labelling of the clusters. Note that optimal ordering
                                η
                                d zn                              was not maintained in [Blei and Jordan, 2005].

                                                            IJCAI-07
                                                              2799Figure 2: Average log probability per data-point for test data as a Figure 4: Average log probability per data-point for test data as a
function of N.                                        function of T (for TSB methods) or K (for FSD methods).


Figure 3: Relative average log probability per data-point for test Figure 5: Relative average log probability per data-point for test
data as a function of N.                              data as a function of T (for TSB methods) or K (for FSD methods).

4  Experiments                                        coefﬁcient1 c =2. We studied the accuracy of each algorithm
                                                      as a function of the number of data cases and the truncation
In the following experiments we compared the six algorithms
                                                      level of the approximation. In ﬁgures 2 and 3 we show the
discussed in the main text in terms of their log-probability on
                                                      results as we vary N (keeping T and K ﬁxed at 30) while in
held out test data. The probability for a test point, xt,isthen
                                                      ﬁgures 4 and 5 we plot the results as we vary T and K (keep-
given by,
                                                     ing N ﬁxed at 200). We plot both the absolute value of the log
                                                     probability of test data and the value relative to a Gibbs sam-
   p xt           p xt|ηz q ηz E p zt|z   q(z  )
    ( )=           (    t ) ( t ) [ ( train)] train   pler (GS). We 50 iterations for burn-in, and run another 200
               dηz
           zt    t                                    iterations for inference. Error bars are computed on the rela-
                  E p z |z                            tive values in order to subtract variance caused by the differ-
where the expectation [ ( t train)]q(ztrain) is computed using
the techniques introduced in section 3.3. All experiments ent splits (i.e. we measure variance on paired experiments).
were conducted using Gaussian mixtures with vague priors 1Following [Dasgupta, 1999], a Gaussian mixture is c-separated
                                                                                                     2
on the parameters.                                    if for each pair (i, j) of components we have ||mi − mj || ≥
                                                       2       max  max        max
  In the ﬁrst experiment we generated synthetic data from a c D max(λi ,λj ) ,whereλ denotes the maximum eigen-
mixture of 10 Gaussians in 16 dimensions with a separation value of their covariance.

                                                IJCAI-07
                                                  2800