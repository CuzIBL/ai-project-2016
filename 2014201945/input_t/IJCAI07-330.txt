                   Using Learned Policies in Heuristic-Search Planning

          SungWook Yoon                        Alan Fern                       Robert Givan
    Computer Science & Engineering     Computer Science Department     Electrical & Computer Engineering
        Arizona State University          Oregon State University             Purdue University
           Tempe, AZ 85281                  Corvallis, OR 97331            West Lafayette, IN 47907
        Sungwook.Yoon@asu.edu                afern@cs.orst.edu                givan@purdue.edu


                     Abstract                          limit the size of the training data, as occurs in planning
                                                       competitions. Nevertheless such policies still capture use-
  Many current state-of-the-art planners rely on forward heuris- ful, though imperfect, constraints on good courses of action.
  tic search. The success of such search typically depends
  on heuristic distance-to-the-goal estimates derived from the The main goal of this paper is to develop and evaluate an ap-
  plangraph. Such estimates are effective in guiding search for proach for combining such imperfect policies and heuristics
  many domains, but there remain many other domains where in order to improve over the performance of either alone.
  current heuristics are inadequate to guide forward search ef- There are techniques that can improve on imperfect poli-
  fectively. In some of these domains, it is possible to learn cies. Policy rollout (Bertsekas & Tsitsiklis 1996) and lim-
  reactive policies from example plans that solve many prob- ited discrepancy search (LDS) (Harvey & Ginsberg 1995)
  lems. However, due to the inductive nature of these learning are representative of such techniques. Policy rollout uses
  techniques, the policies are often faulty, and fail to achieve online simulation to determine for each encountered state,
  high success rates. In this work, we consider how to ef- as it is encountered, which action performs best if we take
  fectively integrate imperfect learned policies with imperfect
  heuristics in order to improve over each alone. We propose it and then follow the learned base policy to some horizon.
  a simple approach that uses the policy to augment the states Policy rollout performs very poorly if the base policy is too
  expanded during each search step. In particular, during each ﬂawed to ﬁnd any reward, as all actions look equally attrac-
  search node expansion, we add not only its neighbors, but all tive. This occurs frequently in goal-based domains, where
  the nodes along the trajectory followed by the policy from policy rollout cannot improve on a zero-success-ratio policy
  the node until some horizon. Empirical results show that our at all.
  proposed approach beneﬁts both of the leveraged automated Discrepancy search determines a variable search horizon
  techniques, learning and heuristic search, outperforming the by counting the number of discrepancies from the base pol-
  state-of-the-art in most benchmark planning domains. icy along the searched path, so that paths that agree with the
                                                       policy are searched more deeply. The search cost is expo-
                                                       nential in the number of discrepancies tolerated, and as a
                  Introduction                         result the search is prohibitively expensive unless the base
                                                       policy makes mostly acceptable/effective choices.
Heuristic search has been the most successful and dominant Due to limitations on the quantity of training data and the
approach for suboptimal AI Planning (Hoffmann & Nebel  lack of any guarantee of an appropriate hypothesis space,
2001; Alfonso Gerevini & Serina 2003; Vidal 2004). The machine learning might produce very low quality policies.
success of this approach is largely due to the development of Such policies are often impossible to improve effectively
intelligent automated heuristic calculation techniques based with either policy rollout or LDS. Here, we suggest using
on relaxed plans (RPs), plans that ignore the action effect such learned policies during node expansions in heuristic
delete lists. RP-based heuristics provide an effective gra- search. Speciﬁcally, we propose adding not only the neigh-
dient for search in many AI planning domains. However, bors of the node being expanded, but also all nodes that oc-
there are some domains where this gradient provides inade- cur along the trajectory given by the learned policy from the
quate guidance, and heuristic search planners fail to scale up current node. Until the policy makes a bad decision, the
in such domains.                                       nodes added to the search are useful nodes.
  For some of those domains, e.g. Blocksworld, there are
                                                         In contrast to discrepancy search, this approach leverages
automated techniques (Martin & Geffner 2000; Fern, Yoon,
                                                       the heuristic function heavily. But in contrast to ordinary
& Givan 2004) that can ﬁnd good policies through machine
                                                       heuristic search, this approach can ignore the heuristic for
learning, enabling planners to scale up well. However, in-
                                                       long trajectories suggested by the learned policy. This can
duced policies lack deductive guarantees and are in practice
                                                       help the planner critically in escaping severe local minima
prone to be faulty—even more so when resource constraints
                                                       and large plateaus in the heuristic function. Policy evalua-
Copyright c 2006                                      tion is typically cheaper than heuristic function calculation
All rights reserved.                                   and node expansion, so even where the heuristic is working

                                                IJCAI-07
                                                  2047well, this approach can be faster.                     that each oi is in set speciﬁed by the corresponding Cij —
  We tested our proposed technique over all of the     upon ﬁring the rule suggests action y(o1,...,om). The ear-
STRIPS/ADL domains of International Planning Competi-  liest rule in the list that can be ﬁred in the current state will
tions (IPC) 3 and 4, except for those domains where the be ﬁred. If more than one action is suggested by that rule,
automated heuristic calculation is so effective as to pro- the tie is broken lexicographically based on an arbitrary un-
duce essentially the real distance. We used some compe- changing ordering of the domain objects.
tition problems for learning policies and then used the rest The syntax for the concepts is the following.
of the problems for testing our approach. Note that this ap-   =             |  ∩   |¬   |
proach is domain-independent. Empirical results show that   C      any-object C   C    C
                                                                   (              ∗                )
our approach performed better than using policies alone and         pC1  ...Ci−1    Ci+1 ...Carity(p)
in most domains performed better than state-of-the-art plan-
ners.                                                  Here, p is a predicate symbol, from P or as speciﬁed below.
                                                       The predicate symbol p is applied to smaller class expres-
                    Planning                           sions, but with one argument omitted, indicated by a “*”.
                                                       Such applications denote the class of objects that make the
A deterministic planning domain D deﬁnes a set of possible predicate true when ﬁlled in for “*”, given that the other ar-
      A                 S
actions and a set of states in terms of a set of predicate guments of p (if any) are provided to match the class expres-
symbols P , action types Y , and objects C. Each symbol sions given. The semantics of the other constructs as classes
σ in P or Y has a deﬁned number of arguments it expects, of objects is as expected—please refer to (Yoon, Fern, &
denoted by arity(σ).Astates is a set of state facts, where a Givan 2002; 2006; McAllester & Givan 1993) for the de-
state fact is an application of a predicate symbol p to arity(p) tailed speciﬁcation of the syntax and semantics. Note that
objects from C. An action a is an application of an action this syntax automatically derives from predicate symbols of
type y to arity(y) objects from C.                     the target planning domain.
  Each action a ∈Ais associated with three sets of state The evaluation of the semantics of each class C is rela-
facts, Pre(a),Add(a),andDel(a) representing the precondi- tive to the relational database D constructed from the cur-
tion, add, and delete effects respectively. As usual, an action rent state, the goal information and other information that is
a is applicable to a state s iff Pre(a) ⊆ s, and the applica- deduced from the current state and goal information. In our
tion of an (applicable) action a to s, results in the new state case this includes the relaxed plan from the current state to a
a(s)=(s \ Del(a)) ∪ Add(a).                            goal state, a form of reasoning to construct features (Geffner
  Given a planning domain, a planning problem is a tuple 2004) and the reﬂexive transitive closure p∗ for each binary
(s, A, g),whereA ⊆Ais a set of actions, s ∈Sis the initial predicate p. Predicate symbols other than those in P are al-
state, and g is a set of state facts representing the goal. A lowed in order to represent goal information and features of
solution plan for a planning problem is a sequence of actions the relaxed plan, as described in detail in (Yoon, Fern, &
(a1,...,al) from A, where the sequential application of the Givan 2002; 2006).
sequence starting in state s leads to a goal state s where
                                                        As an example decision list policy, consider a simple
g ⊆ s .                                                Blocksworld problem where the goal is clearing off a block
                                                       A. The following decision-list policy is an optimal policy.
               Learning Policies                       {putdown(x1):x1   ∈ holding(∗), unstack(x1):x1 ∈
                                                           (∗) ∩ ( ∗ ∗   )}
A reactive policy π for a planning domain D is a function clear  on     A   . The ﬁrst rule says “putdown any
that maps each state s to an action a that is applicable to the block that is being held” and the second rule says “unstack
state s. We desire a policy π for which iterative application any block that is above the block A and clear”.
of π to the initial state s of a problem p in D will ﬁnd a
                                                       Learning Reactive Policies
goal, so that g ⊆ τπ(τπ( ...τπ(s))), writing τπ(s) for the
next state under π, i.e., (π(s)) (s). Good reactive decision- For our evaluation, we learn reactive policies from solved
list policies have been represented and learned for many AI small problems. For this purpose we deploy a learner similar
planning domains (Khardon 1999; Martin & Geffner 2000; to that presented in (Yoon, Fern, & Givan 2002). Given a set
Yoon, Fern, & Givan 2002; 2005).                       of training problems along with solutions we create a train-
                                                       ing set to learn a classiﬁer that will be taken to be the learned
Taxonomic Decision List Policies                       policy. The classiﬁer training set contains states labeled by
                                                       positive and negative actions. The positive actions are those
We represent a reactive policy as an ordered list of rules
                                                       that are contained in the solution plan for each state, and all
(Rivest 1987), each of which speciﬁes constraints on the ar-
                                                       other actions in a state are taken to be negative. The learn-
guments of an action:
                                                       ing algorithm conducts a beam search through the candidate
         DL  = {rule1,...,rulen}                       class expressions, greedily seeking constraints on each ar-
        rulei = y : x1 ∈ Ci1,...,xm ∈ Cim            gument of each action type to match the positive actions and
                                                       not match the negative actions. Note that the training set
Here, m is arity(y) for action type y and each Cij is a con- is noisy in the sense that not all actions labeled as negative
cept expression specifying a set of objects, as described be- are actually bad. That is, there can be many optimal/good
low. A rule can ﬁre on any tuple of objects o1,...,om such actions from a single state, though only one is included in

                                                IJCAI-07
                                                  2048the training solutions. This noise is one reason that learned the resulting state. While policy rollout can improve a pol-
policies can be imperfect.                             icy that is mostly optimal, it can perform poorly when the
                                                       policy commits many errors, leading to inaccurate action
          Using Non-Optimal Policies                   costs. Multi-level policy rollout, e.g. as used in (Xiang Yan
                                                       & Van Roy 2004), is one way to improve over the above
Typically learned policies will not be perfect due to the bias procedure by recursively applying rollout, which takes time
of the policy language and variance of the learning proce- exponential in the number of recursive applications. This
dure. In some domains, the imperfections are not catas- approach can work well when the policy errors are typically
trophic and the policies still obtain high success rates. In restricted to the initial steps of trajectories. However, for
other domains, the ﬂaws lead to extremely poor success policies learned in planning domains the distribution of er-
rates. Nevertheless, in many states, even learned policies rors does not typically have this form; thus, even multi-step
with poor success rates suggest good actions and we would rollout is often ineffective at improving weak policies.
like methods that can exploit this information effectively.
First, we describe two existing techniques, rollout and dis-
crepancy search that utilize search to improve upon an im- Discrepancy Search
perfect policy. Our experiments will show that these tech-
niques do not work well in many planning domains, typi- A discrepancy in a search is a search step that is not selected
cally where the quality of the learned policy is low. These by the input policy or heuristic function. Limited discrep-
failures led us to propose a new approach to improving poli- ancy search (LDS) (Harvey & Ginsberg 1995) bounds the
cies with search, which is discussed at the end of this section. search depth to some given number of discrepancies. Dis-
                                                       crepancy search limited to n disrepancies will consider ev-
Policy Rollout                                         ery search path that deviates from the policy at most n times.

                        S
                           Select MIN                                          0

                  a1    ai   an
                                                                  0                         1
             p
                                                             0          1              2          1


             Follow Policy                                0     1    2221           3               1
                                                                                                    G


                 S1     Si     Sn
           p          p         p

          Q a1 = Length Q ai = Length Q an = Length
                  1         i          n               Figure 2: An Example of Discrepancy Search. Thin edges
          + Heuristic(S1 ) + Heuristic(Si ) + Heuristic(Sn )
                                                       in the ﬁgure represent discrepancies. Nodes are labeled with
                                                       their discrepancy depth.
Figure 1: Policy Rollout. At a state s, for each action a, roll-
out the input policy π from state a(s) until some horizon.
For deterministic domains, the length of the rollout trajec- Figure 2 shows an example of discrepancy search. The
tory plus the heuristic at the end can be used as the Q-value thick lines are choices favored by the heuristic function and
for that action. Select the best action.               the thin lines are discrepancies. Each node is shown as a
                                                       rectangle labeled by the number of discrepancies needed to
  Policy rollout (Bertsekas & Tsitsiklis 1996) is a technique reach that node. Consider the depth of the goal in the search
for improving the performance of a non-optimal “base” pol- tree and the number of discrepancies needed to reach the
icy using simulation. Policy rollout sequentially selects the goal node from the root node. In ﬁgure 2, the goal node is at
action that is the best according to the one-step lookahead depth three and discrepancy depth one from the root node.
policy evaluations of the base policy. Figure 1 shows the Plain DFS or BFS search will search more nodes than limit
one-step lookahead action selection. For each action, this one disrepancy search. DFS and BFS need to visit 14 nodes
procedure simulates the action from the current state and before reaching the goal, while discrepancy search with limit
then simulates the execution of the base policy from the re- one will ﬁnd the goal after a 9-node search. Greedy heuristic
sulting state for some horizon or until the goal is found. Each search on this tree needs to backtrack many times before it
action is assigned a cost equal to the length of the trajectory reaches the goal node. The search space of LDS has size
following it plus the value of a heuristic applied at the ﬁnal exponential in the discrepancy bound, and so, like policy
state (which is zero for goal states). For stochastic domains rollout, LDS also is only effective with policies or heuristic
the rollout should be tried several times and the average of functions with high quality. Heuristics can be incorporated
the rollout trials is used as a Q-value estimate. Policy roll- by applying the heuristic at the leaves of the discrepancy
out then executes the action that achieved the smallest cost search tree and then selecting the action that lead to the best
from the current state and repeats the entire process from heuristic value.

                                                IJCAI-07
                                                  2049Incorporating Policies into Heuristic Search           we report the number of solved problems, the average solu-
In this work, we consider an alternative approach for in- tion time for solved problems and the average length of the
corporating imperfect policies into search. We attempt to solved problems in separate columns of the results.
take advantage of both approaches, automated heuristic cal- We used the ﬁrst 15 problems as training data in each do-
culation and automated policy learning. The main idea is main and the remaining problems for testing. We considered
to use policies during node expansions in best-ﬁrst heuristic a problem to be unsolved if it was not solved within 30 min-
search, as described by the node expansion function shown utes. For PR, D and PH systems, we used a horizon of 1000.
in Figure 3. At each node expansion of the best-ﬁrst search, All the experiments were run on Linux machines with a 2.8
we add to the search queue the successors of the current best Xeon Processor and 2GB of RAM.
node as usual, but also add the states encountered by follow-
ing the policy from the current best node for some horizon. Blocksworld
Our approach is similar to Marvin (Coles & Smith 2004), Figure 4 shows the results on Blocksworld from Track 1 of
MacroFF (Botea et al. 2004), and YAHSP (Vidal 2004).   IPC 2. In this domain, LEN, PH, D solved all the prob-
However, unlike these approaches, we do not just add the ﬁ- lems but FF and PR failed to solve some problems. We
nal state encountered by the policy or macro, but rather add used 100 cpu secs in learning the policy. The learned policy,
all states along the trajectory.                       πBlocksworld solved 13 problems. Thus, all of the policy im-
  Embedding the policy into node expansion during heuris- provement approaches, PR, D, and PH improved the input
tic search yields two primary advantages over pure heuris- policy. The PH system solved all of the problems and im-
tic search. First, when the input policy correlates well proved the solution time over LEN, PR and D, showing the
with the heuristic values, the embedding can reduce the beneﬁt of our approach for speeding-up planning, though
search time. Typically, the direct policy calculation is much PH produced slightly longer plans than FF and D.
faster than greedy heuristic search because greedy heuristic
search needs to compute the heuristic value of every neigh-               Blocksworld (IPC2)
                                                                               ↑      ↓       ↓
bor, while direct policy execution considers only the current    Systems Solved (20) Time Length
state in selecting actions. Second, like Blocksworld, where       FF       16       0.64   38.1
heuristic calculation frequently underestimates the true dis-     LEN      20      11.74   116.3
                                                                  PR       19       7.89   37.8
tance, our node expansion can lead the heuristic search out        D       20       2.09    38
of local minima.                                                  PH       20       0.06    44

  Node-Expansion-with-Policy (s, π, H)
  // problem s, a policy π, a horizon H                            Figure 4: Blocksworld Results

  N  ←  Neighbors(s)
                                                      IPC3
  s  ← s
  for i =0until i == H                                 Figure 5 summarizes the results on the IPC3 domains. The
      s ← π(s)                                       domains are signiﬁcantly more difﬁcult to learn policies for
      N  ← N  ∪{s}                                    and we used 8h, 6h and 6h of cpu time for learning the poli-
  Return N                                             cies in Depots, Driverlog, and FreeCell respectively. Al-
                                                       though the learning times are substantial, this is a one-time
                                                       cost: such learned policies can be resused for any problem
Figure 3: Node expansion for a heuristic search with a pol- instance from the domain involved. In these domains, the
icy. Add nodes that occur along the trajectory of the input learned policies cannot solve any of the tested problems by
policy as well as the neighbors                        themselves.
                                                         We see that all of PR, D and PH are able to improve on
                                                       the input policies. PH solved more problems overall than
                                                       FF, though FF has a slight advantage in solution length in
                  Experiments                          the Depots domain. The Freecell domain has deadlock states
We evaluated the above approaches on the STRIPS domains and our policy performs poorly, probably by leading too of-
from the recent international planning competitions IPC3 ten to deadlock states. Still the experiments show that our
and IPC4, and on Blocksworld. We show the performance  approach attained better solution times, demonstrating again
of each planning system in each row of the following ﬁg- that using policies in search can reduce the search time by
ures. To test the effectiveness of our approach, we tested quickly ﬁnding low-heuristic states.
our base system, LEN, as well as our proposed technique, PH outperforms LEN decisively in Depots and Driver-
PH. The LEN system uses best-ﬁrst search with the relaxed- log, showing a clear beneﬁt from using imperfect policies
plan–length (RPL) heuristic. We also compare against the in heuristic search for these domains.
planner FF, and, for each IPC4 domain, against the best
planner in the competition on the particular domain. Finally, IPC4
we compare against policy rollout (PR) and limited discrep- Figure 6 shows the results for the IPC4 domains. We used
ancy search (D) techniques. For each system and domain, 18, 35, 1, 3 and 4 hours of CPU time for learning poli-

                                                IJCAI-07
                                                  2050                       IPC3                                                   IPC4
      Domain  Systems Solved(5) ↑ Time ↓ Length ↓            Domain   Systems Solved (35) ↑ Time ↓ Length ↓
                FF      5       4.32   54.2                            FF       21      71.20   48.2
               LEN      1       0.28   29.0                            LEN      15      71.38   48.2
      Depots   PR       2      70.86    32                  Pipesworld  B       35       4.94   74.6
                D       3      34.23   36.5                            PR       18      472.79  59.5
               PH       5       3.73   63.2                            D        14      215.51  49.3
                FF      1       1105   167.0                           PH       29      129.21  76.7
               LEN      1       1623   167.0                           FF        4      532.20  62.0
      Driverlog PR      0        -      -                              LEN       4      333.34  62.0
                D       0        -      -                   Pipesworld  B       28      221.96  165.6
               PH       4      75.91   177.3                 Tankage   PR        7      366.11  49.7
                FF      5      574.84  108.4                           D         5      428.71  66.4
               LEN      5      442.94  109.0                           PH       21      124.07  86.3
      Freecell PR       3      545.11  111.3                           FF       17      638.42  104.2
                D       3      323.23  85.6                   PSR      LEN      22      646.59  107.2
               PH       4      217.49  108.3                 (middle    B       18      124.93  106.8
                                                            complied)  PR        6      691.04  72.0
               Figure 5: IPC3 results                                  D         4      225.56  55.5
                                                                       PH       17      659.37  107.0
                                                                       FF        0        -      -
                                                                       LEN       0        -      -
cies for Pipesworld, Pipesworld-Tankage, PSR, Philosopher  Philosophers B       14       0.20   258.5
and Optical Telegraph respectively. Here we evaluated one              PR       33       3.89   363.0
more system, B, which in each domain denotes the best per-             D        33       2.59   363.0
former from that domain from IPC4. The numbers for B                   PH       33       2.59   363.0
were downloaded from the IPC web site and cannot be di-                FF        0        -      -
rectly compared to our systems results. Still, the numbers             LEN       0        -      -
give some idea of the performance comparison. Note that      Optical    B       10      721.13  387.0
the learned policies alone can solve all the problems from  Telegraph  PR       33      23.77   594.0
the Philosopher and Optical Telegraph domains.                         D        33      19.67   594.0
  PH generally outperforms FF in solved problems, render-              PH       33      19.67   594.0
ing solution length measures incomparable, except in PSR
                                                                       Figure 6: IPC4 results
where the two approaches essentially tie.
  For Pipesworld and Pipesworld-with-Tankage domains,
the best performers of those domains in the competition per-
formed much better than our system PH. However, PH still In Figures 10 and 9, we show the heuristic traces for LEN
performed better than LEN, FF, PR and D, showing the ben- and PH on a Freecell problem where PH failed. Here PH is
eﬁts of our approach. Marvin, Macroff and YAHSP partic- unable to escape from the plateau, making only small jumps
ipated the competition. Each of these macro action based in heuristic value over many node expansions. Interestingly,
planners solved 60 or so problems. The best planners for the trace of LEN system shows a plateau at a higher heuris-
each domain combined solved 105 problems. PH solved 133 tic value than that for PH followed by a rapid decrease in
problems, showing a clear advantage for our technique. heuristic value upon which the problem is solved. We are
  Overall, the results show that typically our system is able currently investigating the reasons for this behavior. At this
to effectively combine the learned imperfect policies with point, we speculate that the learned policy manages to lead
the relaxed-plan heuristic in order to improve over each the planner away from a useful exit.
alone. This is a novel result. Neither policy rollout nor dis-
crepancy search are effective in this regard and we are not               Conclusion
aware of any prior work that has successfully integrated im-
perfect policies into heuristic search planners.       We embedded learned policies in heuristic search by follow-
                                                       ing the policy during node expansion to generate a trajectory
Heuristic Value Trace in Search                        of new child nodes. Our empirical study indicates advan-
                                                       tages to this technique, which we conjecture to have three
In order to get a view of how incorporating policies can im- sources. When the policy correlates well with the heuris-
prove the heuristic search, we plotted a trace of the heuristic tic function, the embedding can speed up the search. Sec-
value for each node expansion during the search for problem ondly, the embedding can help escape local minima during
20 of Depots. Figure 7 shows a long plateau of heuristic val- the search. Finally, the heuristic search can repair faulty ac-
ues by the LEN system, while Figure 8 shows large jumps in tion choices in the input policy. Our approach is easy to
heuristic value for the PH system, using far fewer node ex- implement and effective, and to our knowledge represents
pansions, indicating that the policies are effectively helping the ﬁrst demonstration of effective incorporation of imper-
to escape plateaus in the search space.                fect policies into heuristic search planning.

                                                IJCAI-07
                                                  2051