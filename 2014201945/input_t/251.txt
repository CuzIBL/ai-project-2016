           Inducing criteria for lexicalization parts of speech using the Cyc KB 

                           Tom O'Hara Michael Witbrock, Bjorn Aldag, Stefano Bertolo, 
                 Computer Science Department Nancy Salay, Jon Curtis and Kathy Panton 
                  New Mexico State University Cycorp, Inc. 
                      Las Cruces, NM 88001 Austin, TX 78731 
                      tomohara@cs.nmsu.edu {witbrock,bertolo,aldag,nancy, jone,panton}@cyc.com 


                        Abstract                               denotational assertion maps a phrase, specified via a lexical 
                                                               concept with optional string modifiers, into a concept, usu•
     We present an approach for learning part-of-speech        ally a collection. The part of speech is specified by Cyc's 
     distinctions by induction over the lexicon of the         SpeechPart constants. The simplest type of denotational map•
     Cyc knowledge base. This produces good results            ping uses the denotation predicate. For example, (denotation 
     (74.6%) using a decision tree that incorporates both      Device-Word CountNoun 0PhysicalDevice) indicates that sense 0 
     semantic features and syntactic features. Accurate 
                                                               of the count noun 'device1 refers to PhysicalDevice (via the 
     results (90.5%) are achieved for the special case         wordforms "device" and "devices"). Three additional predi•
     of deciding whether lexical mappings should use           cates account for phrasal mappings: compoundString, head-
     count noun or mass noun headwords. Comparable             MedialString, and multiWordString are used for phrases with 
     results are also obtained using OpenCyc, the pub•         the headword at the beginning, the middle, and the end, re•
     licly available version of Cyc.                           spectively. 
                                                                 These denotational assertions, excluding lexical mappings 
1 Introduction                                                 for technical, informal and slang terms, form the training data 
                                                               for a lexicalization speech part classifier. 
In semantic lexicons, a lexical mapping describes the re•
lationship between a concept and phrases used to refer to      2 Inference of lexicalization part of speech 
it. This mapping includes syntactic information for deriv•
                                                               Our method of inferring the part of speech for lexicalizations 
ing variant phrases, including the part of speech of the head•
                                                               is to apply decision tree learning over existing lexical map•
word. Selecting the part of speech for the lexical mapping is 
                                                               pings. For each target denotatum term, the corresponding def•
required so that proper inflectional variations can be recog•
                                                               initional information (e.g., isa), asserted or inferable via tran•
nized and generated for the term. Although often a straight-
                                                               sitivity, is extracted from the ontology. For simplicity, these 
forward task, there are special cases that can pose problems, 
                                                               definitional types are referred to as ancestor terms. Associa•
especially when fine-grained speech part categories are used. 
                                                               tions between the lexicalization parts of speech and common 
   To reduce the need for linguistic expertise in producing 
                                                               ancestor terms underlie the lexicalization speech part clas•
these lexicalizations for a large knowledge base like Cyc 
                                                               sifier and its special case, the mass-count classifier (distin•
[Lenat, 1995], linguistic criteria can be inferred from deci•
                                                               guishing, e.g., "much sand" from "many books"). To reduce 
sions that have been made by lexical knowledge engineers in 
                                                               the size of the training feature vector, only the most frequent 
lexicalizing preexisting terms. 
                                                               256 atomic terms from the thousands of possible ancestor 
   The Cyc knowledge base, containing 120,000 concepts and 
                                                               terms are selected, after excluding certain internal bookkeep•
over one million axioms,  divides roughly into three layers. 
                        1                                      ing constants. 
The upper ontology formalizes fundamental distinctions (e.g., 
                                                                 Given a training instance, such as a denotation from a lex•
tangibility versus intangibility). The lower ontology collects 
                                                               eme into a specific Cyc concept using a particular SpeechPart 
specific facts, often related to concrete applications, and the 
                                                               (e.g., MassNoun or a CountNoun), the feature specification 
middle ontology encodes commonsense knowledge about the 
                                                               is derived by determining all the ancestor terms of the de•
world. The KB also includes a broad-coverage English lexi•
                                                               notatum term and converting this into a vector of occurrence 
con mapping words and phrases to terms throughout the KB. 
                                                               indicators, one indicator per reference term. Then the head•
   Natural language lexicons are integrated directly into the 
                                                               word is checked for the occurrence of a set of commonly used 
Cyc KB [Burns and Davis, 1999]. Binary predicates, as in 
                                                               suffixes. If found, the suffix itself is added to the vector (in a 
(nameString HEBCompany "HEB'), map names to terms. A 
                                                               position set aside for suffixes of the same length). The part of 
   'These figures and the results discussed later are based on Cyc speech serves as the classification variable. 
KB v576 and system vl.2577. See www.cyc.com for detailed doc•    We use decision trees for this classification. Part of the 
umentation on the KB and [O'Hara et al., 2003] for more technical motivation is that the result is readily interpretable and can 
details related to this work.                                  be incorporated directly by knowledge-based applications. A 


1496                                                                                                   POSTER PAPERS                                OpenCyc         Cyc                       (via an ontology). Our rules are induced from the knowledge 
                 Instances          2607    30676                       base, which alleviates the need for rule maintenance as well 
                 Classes                2         2                     as rule construction. 
                 Entropy            0.76      0.90                         This paper shows that an accurate decision procedure 
                 Baseline           78.3      68.2                      (90.5%) for determining the mass-count distinction in lexi-
                 Accuracy           87.5      90.5                      calizations can be induced from the lexical mappings in the 
                                                                        Cyc KB. The performance (74.6%) in the general case is also 
Table I: Mass-count classification over Cyc lexical map•                promising, given that it is a much harder task with over 30 
pings, using Cyc reference terms and headword suffixes as               part of speech categories to choose from. The features in•
features. Instances is size of the training data. Classes indi•         corporate semantic information, in particular Cyc's ontolog•
cates number of choices. Baseline selects most frequent case.           ical types, in addition to syntactic information (e.g., head•
Accuracy is average in the 10-fold cross validation.                    word morphology). Although the main approach incorporates 
                                                                        Cyc's conceptual distinctions, it can be extended to non-Cyc 
                               OpenCyc         Cyc                      applications via the WordNet mapping [OTlara et al., 2003]. 
                 Instances         3721     43089                          This work is just a small initial step in applying machine 
                 Classes               16       33                      learning techniques to the massive amount of data in Cyc. 
                 Entropy            1.95      2.11                      The recent release of OpenCyc enables wider investigation 
                 Baseline           54.9      49.0                      and exploitation of the information in the Cyc knowledge 
                 Accuracy           71.9      74.6                      base for intelligent applications. 
                                                                        Acknowledgements 
    Table 2: General speech part classification using Cyc. 
                                                                        The lexicon work at Cycorp has been supported by many staff mem•
                                                                        bers, and, in part by grants from NIST, DARPA, and ARDA. The 
simple fragment from the resulting decision tree shows how              first author is currently supported by a GAANN fellowship from the 
ontological features interact with morphological ones:                  Department of Education. The work utilized NMSU resources ob•
                                                                        tained through Mil Grants E1A-9810732 and E1A-0220590. 
      if (isa AbstractInformationalThing) then 
        if (suffix - "-cr") then                                        References 
           if (not isa SomethingExisting) then AgentiveNoun 
           if (isa SomethingExisting) then MassNoun                     [Brill, 1995] Eric Brill. Transformation-based error-driven 
        if (suffix = "-cd") then MassNoun                                   learning and natural language processing: A case study 
        if (suffix "-al") then Adjective                                    in part of speech tagging. Computational Linguistics, 
        if (suffix = "-or") then AgentiveNoun                               21(4):543-565,1995. 
                                                                         [Burns and Davis, 1999] Kathy J. Burns and Anthony B. 
                                                                            Davis. Building and maintaining a semantically adequate 
   Table 1 shows the results of 10-fold cross validation for the 
                                                                            lexicon using Cyc. In Evelyn Viegas, editor, The Breadth 
mass-count classification. This was produced using the J48 
                                                                            and Depth of Semantic Lexicons, pages 121-143. Kluwer, 
algorithm in the Weka machine learning package [Witten and 
Frank, 1999], which is an implementation of Quilan's C4.5                   1999. 
[Quinlan, 1993] decision tree learner. This shows that the sys•         [Lenat, 1995] D. B. Lenat. Cyc: A large-scale investment in 
tem achieves an accuracy of 90.5%, an improvement of 22.3                   knowledge infrastructure. Communications of the ACM, 
percentage points over a baseline of always selecting the most              38(11), 1995. 
frequent case. The OpenCyc version of the classifier also per•
                                                                        [O,Hara et al.,2003] Tom O'Hara, Nancy Salay, Michael 
forms well. This suggests that sufficient data is already avail•            Witbrock, Dave Schneider, Bjoern Aldag, Stefano Bertolo, 
able in OpenCyc (available online at www.opencyc.org) to                    Kathy Panton, Fritz Lehmann, Matt Smith, David Baxter, 
allow for good approximations for such classifications.                     Jon Curtis, and Peter Wagner. Inducing criteria for mass 
   The mass/count noun distinction can be viewed as a special               noun lexical mappings using the Cyc KB, and its exten•
case of speech part classification. Running the same classi•                sion to WordNet. In Proc. Fifth International Workshop 
fier using the full set of speech part classes yields the results          on Computational Semantics (IWCS-5), 2003. 
shown in Table 2. Here the overall result is not as high, but 
                                                                        [Quinlan, 1993] J. Ross Quinlan. C4.5: Programs for Ma-
there is a similar improvement over the baseline. 
                                                                            chine Learning. Morgan Kaufmann, San Mateo, Califor•
                                                                           nia, 1993. 
3 Discussion 
                                                                        [Witten and Frank, 1999] Ian H. Witten and Eibe Frank. 
Contextual part of speech tagging [Brill, 1995] has received               Data Mining: Practical Machine Learning Tools and 
substantial attention in the literature, but there has been rela•           Techniques with Java Implementations. Morgan Kauf•
tively little written on automatically determining default lex-            mann, 1999. 
icalization parts of speech. Woods [Woods, 2000] describes 
                                                                        [Woods, 2000] W. Woods. Aggressive morphology for ro•
an approach to this problem using manually-constructed rules 
                                                                           bust lexical coverage. In Proc. ANLP-00, 2000. 
incorporating syntactic, morphological, and semantic tests 


POSTER PAPERS                                                                                                                         1497 