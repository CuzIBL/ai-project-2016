         A SIMPLIFIED HEURISTIC VERSION OF RAVIV'S ALGORITHM FOR USING CONTEXT IN TEXT RECOGNITIONS 

         R. Shinghal                                      D. Rosenberg                                   G.T. Toussaint 
School of Computer Science                 Department of Electrical Engineering                    School of Computer Science 
      McGill University                                 McGill University                                McGill University 

                           Abstract 

      Word-position-independent and word-position                      P (C,IC ) is the probability of occurrence of 
dependent n-gram probabilities were estimated 
                                                                       character C. in position (m+1) of a word 
from a large English language corpus. A text-
recognition problem was simulated, and using the                       given that character C occurs in position m. 
estimated n-gram probabilities, four experiments 
were conducted by the following methods of classi•                            Since the corpus was large, it was assumed 
fication: without contextual information, Raviv's                      that PCC^) , P(C1 |Cj), VV' ?m(CilCj) e8tl" 
recursive Bayes algorithm, the modified Viterbi                        mated from the corpus, reflect closely the true 
algorithm, and a proposed heuristic approximation                      n-gram probabilities of the English language for 
to Raviv's algorithm. Based on the estimates of                        the 27-character set, tf and the letters A to Z. 
the probabilities of misclassifixation observed                        Each experiment described in Section 4 was re•
in the four experiments, the above methods are                         peated twice: once, by using the word-position-
compared. The heuristic approximation of Raviv's                       independent (WPI) n-gram probabilities and a 
algorithm performed just as well as Raviv's and                        second time, by using word-position-dependent 
required far less computation. 
                                                                        (WPD) n-gram probabilities. 
                      1. Introduction 
                                                                        3• Simulation of the Text-Recognition Problem 
      Some of the contextual text-recognition al•
                                                                              Images on 24 x 24 arrays of mixed-font 
gorithms invoke the assumption that English lan•                       machine printed letters were taken from Ryan's* 
guage is a Markov source of order r>l[l]. Having                        data set. The images were size-normalized and a 
made these assumptions these algorithms use either                      36-dimensional feature-vector X= (x.. ,x_ ,. . . ,x ) 
sequential or nonsequential decision theory. 
                          This paper describes an ex•                  was extracted from each image [4]. 
perimental investigation of text-recognition by                               The total number of patterns available was 
four methods: without contextual information,                           13,337. These were divided into two sets: a 
Raviv's recursive Bayes algorithm [2], the modi•                        training set of 6,651 patterns and a testing set 
fied Viterbi algorithm 15 ], and a proposed                             of 6,686 patterns. The classifier was trained, 
heuristic approximation to Raviv's algorithm.                           and P(X|C ) —the probability distribution of X 
The objective of the experiments was to compare 
the above methods.                                                      conditioned on C,—was estimated, 
                                                                                              k 
      It was assumed that English language Is a                               A passage, called the Old Passage, was com•
Markov source of order r=l . To conduct the                             piled from ten segments, such that each segment 
experiments, it was necessary that unigram (sin•                       was arbitrarily chosen from one of the ten sub•
gle-letter) and first-order transition probabili•                       ject matters in the corpus (described in Section 
ties be estimated from the English language. The                        2). Another passage, called the New Passage, 
probabilities are collectively referred to as the                      was similarly compiled but from sources outside 
n-gram probabilities.                                                   the corpus [4], The Old Passage consisted of 
                                                                        2,521 words and the New Passage of 2,256 words. 
        2. Estimation of N-gram Probabilities                          These two passages were used as text to be recog•
               From the English Language                               nized in the experiments described in Section 4. 

      A multi-authored English language corpus                                   4. The Text-Recognition Problem 
written on ten different subject matters [4]                                      and the Experiments Conducted 
comprising over half-a-million words was compiled. 
Probability estimates were obtained from this                                 Patterns of the passages were presented 
corpus for the following:                                               sequentially to the classifier. It was assumed 
                                                                        that all blanks were perfectly recognizable. 
       (i) Word-position-independent unigram pro•
                                                                       Let 
             babilities P(C1) . 
                                                                                x * x    , x   x    x           , x
     (ii) Word-position-independent transition                                         Q    ±9    2            n    n+1 
             probabilities PCcJc.) .                                   be the pattern-sequence input to the classifier 
    (iii) Word-position-dependent unigram proba•
             bilities P (C.) for position m                            t This research was supported by the National 
                           m I                                         Research Council of Canada under grant number 
             (l<m<9) .                                                 NRC-A9293. 
     (iv) Word-position-dependent transition pro•
             babilities P (C,|C.) for 0<m<9 .                          * Pattern Recognition Data Base No. 1.1.1 A, 
                              m i j                                    IEEE Computer Society 

                                             Natural Lan^tia^e •0: ScMnphal 
                                                                 179 