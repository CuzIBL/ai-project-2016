                         Shallow    Semantics     for Relation    Extraction

                   Sanda   Harabagiu,  Cosmin   Adrian  Bejan   and  Paul Morar˘ escu
                            Human   Language  Technology   Research Institute
                                      University of Texas at Dallas
                                   Richardson, TX  75083-0688,   USA
                             sanda,ady,paul@hlt.utdallas.edu

                    Abstract                          able from the WordNet database [2]. The average precision
                                                      of relation extraction using dependency trees was reported in
    This paper presents a new method for extracting   [1] to be 67.5%. Since meaningful relations between relevant
    meaningful relations from unstructured natural lan- entities are of semantic nature, we argue that additional se-
    guage sources. The method is based on informa-    mantic resources should be used for extracting relations from
    tion made available by shallow semantic parsers.  texts. In this work, we were interested in investigating the
    Semantic information was used (1) to enhance a    contribution of two shallow semantic parsing techniques to
    dependency tree kernel; and (2) to build semantic the quality of relation extraction.
    dependency structures used for enhanced relation
    extraction for several semantic classiﬁers. In our  We   explored two main  resources:  PropBank  and
    experiments the quality of the extracted relations FrameNet. Proposition Bank or PropBank is a one mil-
    surpassed the results of kernel-based models em-  lion word corpus annotated with predicate-argument struc-
    ploying only semantic class information.          tures. The corpus consists of the Penn Treebank 2 Wall
                                                      Street Journal texts (www.cis.upenn.edu/∼treebank). The
                                                      PropBank  annotations were performed at University of
1  Introduction                                       Pennsylvania (www.cis.upenn.edu/∼ace). To date Prop-
With the advent of the Internet, more and more information Bank has addressed only predicates lexicalized by verbs,
is available electronically. Most of the time, information on proceeding from the most to the least common verbs
the Internet is unstructured, generated in textual form. One while annotating verb predicates in the corpus. The
way of automatically identifying information of interest from FrameNet project (www.icsi.berkeley.edu/∼framenet) pro-
the vast Internet resources is by recognizing relevant entities duced a lexico-semantic resource encoding a set of frames,
and meaningful relations they share. Examples of entities are which represent schematic representations of situations char-
PERSONS, ORGANIZATIONS   and LOCATIONS. Examples of   acterized by a set of target words, or lexicalized predicates,
relations are ROLE, NEAR and SOCIAL.                  which can be verbs, nouns or adjectives. In each frame, vari-
  In the 90s, the Message Understanding Conferences   ous participants and conceptual roles are related by case-roles
(MUCs) and the TIPSTER programs gave great impetus to or theta-roles which are called frame elements or FEs. FEs
research in information extraction (IE). The systems that par- are local to each frame, some are quite general while oth-
ticipated in the MUCs have been quite successful at recog- ers are speciﬁc to a small family of lexical items. FrameNet
nizing relevant entities, reaching near-human precision with annotations were performed on a corpus of over three mil-
over 90% accuracy. More recently, the Automatic Content lion words. Recently, semantic parsers using PropBank and
Extraction (ACE) program focused on identifying not only FrameNet have started to become available. In each sentence,
relevant entities, but also meaningful relations between them. verbal or nominal predicates are discovered in relation to their
If success in recognizing entities with high precision was at- arguments or FEs. Our investigation shows that predicate-
tributed to the usage of ﬁnite-state transducers, in the last arguments structures and semantic frames discovered by shal-
three years the dominant successful technique for extracting low semantic parsers play an important role in discovering
relations was based on kernel methods.                extraction relations. This is due to the fact that arguments
  Kernel methods are non-parametric density estimation of extracted relations belong to arguments of predicates or to
techniques that compute a kernel function to measure the sim- FEs.
ilarity between data instances. [8] introduced the formaliza- To investigate the role of semantic information for relation
tion of relation extraction in terms of tree kernels, which are extraction we have used two shallow semantic parsers, one
kernels that take advantage of syntactic trees. [1] extended the trained on PropBank and one on FrameNet. We used the
work by using dependency trees that describe the grammati- semantic information identiﬁed by the parsers in two ways.
cal relations between the words of a sentence. Furthermore, First it was used to enhance the features of dependency ker-
each word of a dependency tree is augmented with lexico- nels. Second, it was used to generate a new representation,
semantic information, including semantic information avail- called semantic dependency structure. The results of the ex-periments indicate that not only the precision of kernel meth- − PHRASE TYPE (pt): This feature indicates the syntactic
ods is improved, but also relation extraction based on shallow type of the phrase labeled as a frame element, e.g.
semantics outperforms kernel-based methods.               NP for Employer in Figure 1.
  The remainder of this paper is organized as follows. Sec- − PARSE TREE PATH (path): This feature contains the path
                                                          in the parse tree between the predicate phrase and the 
tion 2 describes the semantic parsers. Section 3 details the target word, expressed as a sequence of nonterminal
kernel methods and their enhancement with semantic infor- labels linked by direction symbols (up or down), e.g.
mation. In Section 4 we show how relation extraction based NP    S    VP    VP for Employer in Figure 1.
on semantic information is produced. Section 5 details the − POSITION (pos) − Indicates if the constituent appears  
experimental results and Section 6 summarizes our conclu- before or after the predicate in the sentence.
sions.                                                    − VOICE (voice) − This feature distinguishes between
                                                          active or passive voice for the predicate phrase.
2  Shallow   Semantic  Parsing                            − HEAD WORD (hw) − This feature contains the head word
                                                          of the evaluated phrase. Case and morphological information
Shallow semantic information represented by predicates and are preserved.
their arguments, or frames and their FEs, can be identiﬁed − GOVERNING CATEGORY (gov) − This feature applies to
in text sentences by semantic parsers. The idea of automat- noun phrases only, and it indicates if the NP is dominated
ically identifying and labeling shallow semantic information by a sentence phrase (typical for subject arguments with
                                                          active−voice predicates), or by a verb phrase (typical 
was pioneered by [3]. Semantic parsers operate on the output for object arguments).
of a syntactic parser. When using the PropBank information, − TARGET WORD − In our implementation this feature
the semantic parser (1) identiﬁes each verbal predicate and (2) consists of two components: (1) WORD: the word itself with
labels its arguments. The expected arguments of a predicate the case and morphological information preserved; and 
are numbered sequentially from Arg0 to Arg5. Additionally, (2) LEMMA which represents the target normalized to lower 
the arguments may include functional tags from Treebank,  case and infinitive form for the verbs or singular for nouns. 
e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates           Figure 2: Feature Set 1 (FS1).
a locative and ArgM-TMP stands for a temporal. When us-
ing the FrameNet information, the semantic parser (1) iden-
                                                          − CONTENT WORD (cw) − Lexicalized feature that selects an informative 
tiﬁes the target words; (2) disambiguates the semantic frame word from the constituent, different from the head word.
for each target word; and (3) labels the FEs that relate to the
                                                          PART OF SPEECH OF HEAD WORD (hPos) − The part of speech tag of
target word based on the frame deﬁnition. For example, in the head word.
FrameNet the EMPLOYMENT  frame has the FEs: Employee,
                                                          PART OF SPEECH OF CONTENT WORD (cPos) −The part of speech 
Employer, Purpose, Compensation, Manner, Place, Position, tag of the content word.
Time, Field, Duration and Task.
                                                          NAMED ENTITY CLASS OF CONTENT WORD (cNE) − The class of 
                   S                                      the named entity that includes the content word

                NP    VP                                  BOOLEAN NAMED ENTITY FLAGS − A feature set comprising: 
                                                          − neOrganization: set to 1 if an organization is recognized in the phrase
                        VP    NP     PP                   − neLocation: set to 1 a location is recognized in the phrase
                                         NP               − nePerson: set to 1 if a person name is recognized in the phrase
                                                          − neMoney: set to 1 if a currency expression is recognized in the phrase
                                                          − nePercent: set to 1 if a percentage expression is recognized in the phrase
           Syntactic  Parse Tree
              The Chippewas hired Eckstein as a lobbyist  − neTime: set to 1 if a time of day expression is recognized in the phrase
                                                          − neDate: set to 1 if a date temporal expression is recognized in the phrase 
   Semantic Task 1:[The Chippewas] hired [Eckstein] [as a lobbyist]
   Parser
   (PropBank) Task 2: Arg0 PREDICATE Arg1 ArgM                      Figure 3: Feature Set 2 (FS2).

   Semantic Task 1: [The Chippewas] hired [Eckstein] [as a lobbyist]
   Parser
   (FrameNet) Task 2: Employer TARGET Employee Position Several classiﬁers were previously tried for this problem:
       Figure 1: Sentence with labeled semantic roles. decision trees [7], and Support Vector Machines (SVMs) [6].
                                                      Based on the results of the CoNLL and Senseval-3 evalua-
                                                      tions1, we selected an implementation based on SVMs, us-
  Figure 1 illustrates the output of both semantic parsers ing the SVMlight package 2. For the semantic parser based
when processing a sentence. The parsing consists of two on PropBank, we combined feature sets: FS1 (illustrated in
tasks: (1) identifying the parse tree constituents correspond- Figure 2) introduced in [3], FS2 (illustrated in Figure 3) in-
ing to arguments of each predicate or FEs of each frame; and troduced in [7], FS3 (illustrated in Figure 4) introduced in
(2) recognizing the role corresponding to each argument or [6]. For the semantic parser based on FrameNet we have also
FE. Each task is cast as a separate classiﬁer. For example, added the feature set FS4 (illustrated in Figure 5). The se-
task 1 identiﬁes the two NPs and the PP as arguments and
FEs respectively. The second classiﬁer assigns the speciﬁc 1The Conference on Natural Language Learning (CoNLL)
roles: Arg0, Arg1 and ArgM given the predicate “hired” and (http://cnts.uia.ac.be/signll/shared.html) has focused in 2004 on the
FEs Employer, Employee and Position given the target word problem of semantic role labeling, using PropBank data. Senseval-
“hired”. In the case of semantic parsing parsed on FrameNet, 3 (www.senseval.org/senseval3) had a semantic role labeling task,
a third task of ﬁnding the frame corresponding to the sentence using FrameNet data.
is cast as a third classiﬁcation problem.                2http://svmlight.joachims.org/ mantic   parser   using    FrameNet      also  required    the  disambigua-                 PARSE TREE PATH WITH UNIQUE DELIMITER −  This feature removes
 tion  of  the  frame.     To  disambiguate       the  frame,    we   used   the             the direction in the path, e.g. VBN−VP−ADVP

BayesNet      algorithm     implemented       in the  Weka    learning    pack-              PARTIAL PATH − This feature uses only the path from the constituent to
      3
age    . The   features   that  were   used   are  (1)  the  target  word;    (2)            the lowest common ancestor of the predicate and the constituent

the  part-of-speech      of the  target  word;    (3) the  phrase   type   of all            FIRST WORD − First word covered by constituent
the  FEs    (e.g.  NP,  VP,   PP);  and   (4)  the  grammatical       function.              FIRST POS − POS of first word covered by constituent
The    grammatical       function    is  deﬁned     in Figure    4.   To   learn
the   grammatical      function,    we   again    used   the  SVM     with   the             LAST WORD − Last word covered by the constituent
                                                                                             LAST POS − POS of last word covered by the constituent
features    FS1,   FS2,   the  features    Human      and   Target-Type
from    FS3   and   only  the  feature   PP-PREP        from   FS4.                          LEFT CONSTITUENT − left sibling constituent label
                                                                                             LEFT HEAD − Left sibling head word

      HUMAN: This feature indicates whether the syntactic phrase is either                   LEFT POS HEAD − Left sibling POS of head word
       (1) a personal pronoun or
                                                                                             RIGHT CONSTITUENT − right sibling constituent label
       (2) a hyponym of sense 1 of PERSON in WordNet
                                                                                             RIGHT HEAD − Right sibling head word
      SUPPORT_VERBS that are recognized for adjective or noun target words
      have the role of predicate for the FEs. For example, if the target = clever,           RIGHT POS HEAD − Right sibling POS of head word
      in the sentence "Smith is very clever, but he’s no Einstein", the 
      FE = Smith is an argument of the support verb ’is’ rather than of the                  PP PREP − If constituent is labeled PP get first word in PP
      target word. The values of this feature are either (1) The POS of the head
      of the VP containing the target word or (2) NULL if the target word does               DISTANCE − Distance in the tree from constituent to the target word
      not belong to a VP
                                                                                                             Figure    5:  Feature  Set  4 (FS4).
      TARGET−TYPE: the lexical class of the target word, e.g. VERB, NOUN
      or ADJECTIVE

      LIST_CONSTITUENT (FEs): This feature represents a list of the syntactic
      consituents covering each FE of the frame recognized in a sentence.               x  and   y, K(x,    y)   =      i φi(x)φi(y)      =    φ(x)   · φ(y),   where
      For the example illustrated in Figure 1, the list is: [NP, NP, PP]               φi(x)    is some     featurePfunction      over   the  instance    x.  The    in-
      Grammatical Function: This feature indicates whether the FE is:                  stances    can   be  represented     in  several   ways.     First,  each   sen-
       − an External Argument (Ext)                                                    tence    where    a relation   of  interest   occurs    can  be   viewed    as  a
       − an Object (Obj)
       − a Complement (Comp)                                                            list of words.    Thus,   the  similarity    between    two   instances    rep-
       − a Modifier (Mod)                                                               resented    in this  way    is computed      as  the  number     of  common
       − Head noun modified by attributive adjective (Head)
       − Genitive determiner (Gen)                                                     words     between     the  two   instance    sentences.     All   words    from
       − Appositive (Appos)                                                             instances    x  and   y  are   indexed    and   φi(x)    is  the  number      of
                                                                                        times   instance    x  contains    the  word    referenced     by  i.  Such    a
      LIST_Grammatical_Function: This feature represents a list of the 
      grammatical functions of the FEs recognized in the sentence.                      kernel   is known     as bag-of-words       kernel.   When     sentences    are
                                                                                        represented    as  strings  of  words,   string   kernels,   count   the num-
      NUMBER_FEs: This feature indicates how many FEs were recognized 
      in each sentence.                                                                 ber  of  common      subsequences        in the  two    strings  and   weight
                                                                                        their  matches     by  their  length.    Thus    φi(x)    is the  number      of
      FRAME_NAME: This feature indicates the name of the semantic frame 
      for which FEs are labeled                                                         times   string  x   contains    the  subsequence       referenced     by  i.  If
                                                                                        the  instances    are  represented      by  syntactic    trees,   more    com-
      COVERAGE: This feature indicates whether there is a syntactic structure
      in the parse tree that perfectly covers the FE                                    plex  kernels    are  needed.     A  class   of  kernels,   called   convolu-
                                                                                        tion  kernels,   was   proposed     to  handle   such   instance    represen-
      CORE: This feature indicates whether the FE is one that instantiates
      a conceptually necessary participant of a frame. For example, in the              tations.   Convolution      kernels    measure     the  similarity   between
      REVENGE frame, Punishment is a core element. The values of this feature           two   structured    instances    by   summing      the   similarity   of  their
      are: (1) core; (2) peripheral and (3) extrathemathic. FEs that mark notions       substructures.      Thus,    given   all  possible    substructures      in  in-
      such as Time, Place, Manner and Degree are peripheral. Extratematic
      FEs situate an event against a backdrop of another event, by evoking              stances   x  and   y,  φi(x)   counts    not  only   the  number     of  times
      a larger frame for which the target event fills a role.                           the  substructure     referenced     by   i is matched      into  x,  but  also
      SUB_CORPUS: In FrameNet, sentences are annotated with the name                    how   many    times   it is matched     into  any  of  its substructures.
      of the subcorpus they belong to. For example, for a verb target word,
      V−swh represents a subcorpus in which the trget word is a predicate                  Given    a training   set  T ={x1,   . . . xN },  kernel   methods     com-
      to a FE included in a relative clause headed by a wh−word.
                                                                                        pute  the  Gram    matrix    G  such   that  Gij  =K(xi,     xj ). G   enables
                      Figure    4:  Feature  Set  3 (FS3).                              a classiﬁer    to ﬁnd   a  hyperplane     which    separates     instances    of
                                                                                        different   classes.   G  enables    classiﬁers   to  ﬁnd  a  separating    hy-
                                                                                        perplane    that separates    positive   and   negative   examples.     When
                                                                                        a new   instance    y needs    to be  classiﬁed,    y is projected    into  the
3     Dependency            Tree     Kernels                                            feature   space   deﬁned     by   the  kernel   function.     Classiﬁcation
 In [1]  the  relation    extraction    problem     was   cast   as  a classiﬁ-         consists   of  determining      on   which    side  of  the  separating     hy-
 cation   problem     based   on  kernels    that  operate   on   dependency            perplane    y  lies.  Support    Vector    Machines(SVMs)           formulate
trees.   Kernels    measure     the   similarity   between     two   instances          the task  of  ﬁnding    the  separating    hyperplane      as a solution    to a
of   a relation.     If X   is the  instance     space,   a  kernel   function          quadratic    programming       problem.Therefore,         following     the so-
 is a mapping     K:X×X→[0,          ∞)    such   that given   two   instances          lution  proposed     by   [1], they   are  used   for  classifying    relation
                                                                                        instances   in  texts.
     3http://www.cs.waikato.ac.nz/∼ml                                                      To   measure     the   similarity    between     two    instances    of  the (a)             S
                                                               (d)       demanding
          NP             VP
                                                                   trooper apology attorneys
 DT   NNP    NN  NN  VBZ    VP
                                                                                REL−A
 A  Masachusetts state trooper is VBG NP PP                                    Arg0
                                                                               Employee
                       demanding DT
                                  NN   IN    NP                                      represented
                              an apology
                                      from                                          Predicate/Target
                                           CD    PP                          yesterday       Woodward
                                           one                                               REL−B
                                               IN    NP                                      Arg1
                                               of                                            Employer
                                                  NPB        SBAR
 (b)             S (demanding)
                                               DT  NN    WHNP       S
         NP (trooper)
                         VP (demanding)
                                               the attorneys
                                                         WP     PP           VP
 DT  NNP    NN   NN VBZ     VP (demanding)
 A Masachusetts state trooper is                         who  IN  NP     VBD        NP
                        VBG     NP(apology) PP (attorneys)
                                                             until     represented
                      demanding DT NN                                           NPB FW   NN  NNP   NNP
                                      IN    NP (attorneys)        NPB
                             an apology
                                      from                       yesterday     British au pair Louise Woodward
                                          CD    PP (attorneys)
 (c)     demanding
                                          one
                                              IN    NP (attorneys)
    trooper apology attorneys                 of
                                                 NPB (attorneys) SBAR (represented)

                     represented               DT  NN    WHNP       S (represented)

                                               the attorneys
              yesterday       Woodward                   WP    PP (yesterday) VP (represented)

                                                         who IN   NP    VBD        NP (Woodward)

                                                            until      represented
                                                                 NPB           NPB  FW  NN  NNP    NNP
                                                                yesterday     British au pair Louise Woodward

           Figure 6: (a) Syntactic parse; (b) Head word propagation; (c) Dependency tree; (d) Semantic dependency tree.

same extraction relation both [1] and [8] relied on kernels using FrameNet; (4) the target word for the frame; (5) the
that operate on trees expressing syntactic information. To grammatical function; (6) the Frame; (7) the WordNet do-
build such a tree we have used the Collins syntactic parser. main; (8) the WordNet concept that expressed the type of re-
When using this parser, for each constituent in the parse tree, lation in which the word may belong within the domain; (9) a
we also have access to a dependency model that enables the set of other related WordNet concepts (e.g. direct hypernym);
mapping of parse trees into sets of binary relations between (10) the ProbBank concepts that most frequently occur as ar-
the head-word of each component and its sibling words. For guments for the predicate; (11) the FrameNet concepts that
example, Figure 6(a) describes the parse tree of a sentence. most frequently occur in FEs for the frame. The features and
For each possible constituent in the parse tree, rules ﬁrst de- their values for the node “attorneys” are listed in Figure 7.
scribed in [4] identify the head-child and propagate the head
word to the parent. Figure 6(b) illustrates the propagation Feature Set F1 Example Feature Set F2 Example
for the parse tree illustrated in Figure 6(a). When the prop- 1 Word attorneys  1 Predicate−argument number Arg0
                                                       2 Part−of−speech NN   2 Predicate Target represent
agation is over, head-modiﬁer relations are extracted, gener- 3 General−POS noun  3 FE      Employee
ating a dependency structure. Figure 6(c) illustrates the de- 4 Syntactic chunk tag NP  4 Target word represent
                                                       5 Entity−type PERSON  5 Gramatical function Ext
pendency structure of the sentence that was analyzed syntac- 6 Entity−level nominal  6 Frame Employment
                                                       7 Relation−argument REL−A  7 WordNet Domain jurisprudence
tically in Figure 6(a). The nodes of this dependency structure 8 WordNet hypernym Individual  8 WordNet Relation Concept lawyer−client
were augmented with features, to enable the calculation of                   9 WordNet Semantic Concepts professional,paralegal
                                                                            10 PropBank(WN) Concepts banker, lawyer, share
the kernel. Figure 7 lists the features that were assigned to               11 FrameNet(WN) Concepts relation, men
each node in the dependency tree. Two sets of features were
                              [ ]                     Figure 7: Sets of features assigned to each node in the dependency
used: F1, the features proposed in 1 and F2, a new set of tree.
features that we have added. Feature set F1 contains: (1) the
word; (2) the part-of-speech (POS) (24 values); (3) a gener-
                                                        The features are used by a tree kernel function K(T1, T2)
alized POS (5 values); (4) the tag of the nonterminal from the that returns a similarity score in the range (0, 1). We pre-
parse tree having the feature word as a head; (5) the entity ferred the more general version of the kernel introduced in
type, as deﬁned by the ACE guidelines; (6) the entity level [1] to the kernel described by [8]. This kernel is based
(e.g. name); (7) the relation argument (e.g. REL-A); and (8) on two functions deﬁned on the features of tree nodes: a
a WordNet hypernym. The new set of features F2 contains: matching function m(t , t ) ∈ {0, 1} and a similarity func-
(1) the predicate argument number provided by the semantic                i j
                                                      tion s(ti, tj ) ∈ (0, ∞). The feature vector of a tree node
parser when using PropBank; (2) the predicate target for that φ(t ) = {v , . . . v } consists of two possibly overlapping
argument; (3) the FE provided by the semantic parser when i      1    d
                                                      subsets φm(ti) ⊆ φ(ti) and φs(ti) ⊆ φ(ti). As in [1], φm(ti)are used by the matching function and φs(ti) are used by the SDT (R1) [“attorneys”→“represented”←“W oodward”]
similarity function. The two functions are deﬁned by: and SDT  (R2)[“intern”→“dismissed”←“lawyer”].   We
                     1  if φ (t ) = φ (t )            called such trees semantic dependency trees since they are
        m(t , t ) =        m  i     m  j
            i j      0  otherwise                     characterized by semantic features present in the nodes of de-
                                                     pendency trees. Semantic dependency trees (SDTs) are bi-
and                                                   nary trees containing three nodes: a verbal predicate that is
         s(ti, tj ) =            C(vq, vr)            the root of the tree; and two children nodes, each an argu-
                  vq ∈Xφs(ti) vr ∈Xφs(tj )            ment of the predicate. To measure the similarity of two SDTs
                                                      we built a very simple kernel:
where C(vq, vr) is some compatibility function between the
                                                                    0                  if m(c(T1), c(T2))=0
two feature values. For example, in the simplest case where K(T1, T2) =
                                                                   S(r(T1), r(T2)) + Sp(T1, T2)  otherwise
                         1  if v = v
            C(v , v ) =        q    r                 where  the matching  function is performed only  on
               q  r      0  otherwise
                                                     the children.  The  matching features that were used
                                                                 1
s(ti, tj ) returns the number of feature values in common be- comprised φm={General-P OS, Entity-T ype, Relation-
                                                                   2              3
tween the feature vectors φs(ti) and φs(tj ). For two depen- argument}, φm={F E} and φm={P redicate-argument
dency trees T1 and T2 with root nodes r1 and r2 the tree ker- number}. m(c(T1), c(T2))=1 if there is a combination of
nel is deﬁned as:                                     the pair of arguments that has the same matching features.
                                                      For example, in the case of SDT (R1) and SDT (R2), the
              0                        if m(r1, r2)=0
K(T1, T2) =                                           combination is {(“attorneys”, “lawyer”) and (“intern”,
             s(r1, r2) + Kc(r1[c], r2[c]) otherwise                             1   2
                                                      “W oodward”)}  when using φm∪φm,  since both attorney
where Kc is a kernel function over the children of the nodes and lawyer are nouns, Persons, REL-A and they are both cov-
r1 and r2. Let a and b be sequences of indices such that a ered by the FE Employee in their respective frames. To mea-
is a sequence a1 ≤ a2 ≤ . . . ≤ an and likewise for b. Let sure the similarity between the verbal predicates, the func-
d(a) = an − a1 + 1 and l(a) be the length of a. Then for tion S(r(T1), r(T2)) measures the semantic compatibility of
every ti ∈ T1 and tj ∈ T2, we have                    the predicates. The features used for measuring similarity of
                                                                   P
                            d(a) d(b)                 predicates are φS ={F rame, P redicate T arget, W ordNet
   Kc(ti[c], tj [c]) =     λ    λ   K(ti[a], tj [b])  Domain}.  The compatibility measures assigned to each fea-
                  a,b,lX(a)=l(b)                      ture are: 1 if the same predicate target, 0.9 if both predi-
                                                      cates are targets of the same frame, 0.7 if both predicates
where λ∈(0, 1) represents a decay factor that penalizes
                                                      are targets of frames from the same event structure, 0.5 if
matching subsequences that are spread out within the child
                                                      both predicates belong to the same WordNet domain and 0.3
sequences. The deﬁnition of K , the kernel function over
                           c                          if the predicates are not covered in FrameNet, but have the
children, assumes that the matching function used in the def-
                                                      same arguments in PropBank as other predicates from the
inition of the tree kernel K(t [a], t [b]) operates not only on
                        i    j                        same frame. The predicate similarity brings forward semantic
single nodes, but also on node sequences t [a] or t [b]. If all
                                   i      j           frames that characterize a type of extraction relation. For ex-
the nodes in the sequence are matched, m(t [a], t [b]) = 1.
                                     i    j           ample, in the case of Role Client relation, such frames were
For each matching pair of nodes (a , b ) is a matching sub-
                              i j                     (a) Employment end and its subframes from the event struc-
sequence, we accumulate the result of the similarity func-
                                                      ture; Employment Start; and (b) Commerce Sell or (c) Com-
tion s(a , b ) and then recursively search for matching sub-
       i j                                            merce Buy.
sequences of their children.
                                                        The S  T , T  similarity focuses on the combination of
  As in [1], we implemented two types of tree kernels: a con- p( 1  2)
                                                      FEs or predicate-arguments that are identiﬁed for the children
tiguous kernel and a sparse kernel. A contiguous kernel only
                                                      of the SDTs. The similarity features that are used are FE,
matches children subsequences that are uninterrupted by non-                                         {
                                                      Predicate-argument number, Grammatical Function, Word-
matching nodes. Therefore d(a) = l(a). A sparse tree ker-
                                                      Net Domain, WordNet  Relation Concept . For example,
nel, by contrast, allows non-matching nodes within matching                               }
                                                      when the similarity S SDT R  , SDT R    is computed,
subsequences.                                                           p(     ( 1)     (  2))
                                                      since we have an {Employer Employee} relation between
                                                      the FEs in both SDTs, the conﬁdence assigned based on iden-
4  Relation  Extraction                               tical FE-FE relation is 1. If we have identical Predicate-
When  analyzing the dependency kernels, we noticed that argument numbers, the conﬁdence is 0.6. For identical Word-
only few nodes bear semantic information derived by the se- Net domains, 0.4 and for the same WordNet relation concept,
mantic parsers. We also noticed that these nodes are clus- we assign the conﬁdence 0.7.
tered together in the dependency tree. For example, Fig- One limitation of the SDTs stems from the fact that this for-
ure 6(d) illustrates the cluster of nodes from the dependency malism cannot capture extraction relations that are expressed
tree that contains semantic information. Instead of using in the same noun phrase, e.g. “our customers”, “his urolo-
the entire dependency tree to compute similarities, we se- gist” or “George’s high school”. To recognize such relations
lected sub-trees that contain nodes having values for the fea- we consider that they relate to some arguments of “unspec-
tures from set F2 (illustrated in Figure 7). Typically these iﬁed” predicates. In the case when NPs contain pronouns,
nodes correspond to target predicates and their arguments they are resolved by a successful coreference resolution algo-
or FEs.  This allowed us to compare trees of the form rithm [5], and the pronoun is substituted by a pair (referent,