  Generalization Bounds for Weighted Binary Classiﬁcation with Applications to
                                      Statistical Veriﬁcation
                                        Vu Ha      Tariq Samad
                                            Honeywell Labs
                                          3660 Technology Dr.
                                        Minneapolis, MN 55418
                                   vu.ha,tariq.samad  @honeywell.com
                                 {                   }
                    Abstract
    We describe an approach to statistically verify-
    ing complex controllers. This approach is based
    on deriving practical Vapnik-Chervonenkis-style
    (VC) generalization bounds for binary classiﬁers
    with weighted loss. An important case is deriv-
    ing bounds on the probability of false positive.
    We show how existing methods to derive bounds
    on classiﬁcation error can be extended to derive
    similar bounds on the probability of false posi-
    tive, as well as bounds in a decision-theoretic set-
    ting that allows tradeoffs between false negatives Figure 1: The Organic Air Vehicle (left), and the complexity of an
    and false positives. We describe experiments us-  iterative equilibrium angle-of-attack computation as determined by
                                                                                                 qS¯
    ing these bounds in statistically verifying compu- two factors: The ﬂight path angle γ and the net lift force mg (right).
    tational properties of an iterative controller for an The computational complexity here is equated with the number of
    Organic Air Vehicle (OAV).                        iterations. The safe operating envelope consists of those inputs that
                                                      require at most two iterations. The decision boundary is empirically
                                                      drawn using 1000 random samples.
1  Introduction
The computational requirements for high-performance com-
plex control algorithms can vary considerably. The varia- may have drastic consequences such as loss of the vehicle.
tion arises because the computation depends on a number of Thus we would like to obtain an SOE that has some statisti-
factors such as the sensed/estimated state of the system un- cal guarantee that it is indeed safe, i.e., a classiﬁer with prov-
der control, environmental disturbances, and the operational ably small probability of false positive. This however is not
mode of the system. Furthermore, the variation in general the only criterion, for otherwise the trivial classiﬁer that clas-
can not be determined analytically. In hard real-time systems siﬁes everything as negative, namely the current state of the
where the computation must complete in time for a command art, would be the top candidate. Instead, the goal is to push
to be issued to the actuator at the next sample instant, these the boundaries of the SOE as far as possible while keeping a
uncertainties pose a signiﬁcant challenge. This is the rea- cap on the probability of false positive.
son that PID (Proportional-Integral-Derivative) controllers, As an example, let us consider the computational property
with their deterministic execution time, are still the preferred of a high-performance controller for an OAV (Figure 1). The
choice in many applications despite their lesser performance. OAV has a ducted fan propulsion unit, with control provided
  In order to bring practical acceptance to high-performance by movable vanes in the propwash. The vanes are situated
complex control algorithms, we propose a compromise that in the propulsion airﬂow and consequently the interactions
makes use of both types of controller algorithms. High- between the propulsion and the control surfaces are highly
performance algorithms will be used within a safe opera- non-linear. The trim calculation for the OAV is an iterative
tional envelope (SOE) where they are guaranteed to complete algorithm whose computational time depends on several fac-
within the allocated time. Outside of the SOE, lower perfor- tors [Elgersma and Morton, 2000]. We are interested in con-
mance and computationally simpler algorithms will be used. ditions under which this calculation can be reliably used.
The SOE is determined based on simulation data, and hence Our approach is based on statistical learning theory (SLT).
is only safe with some statistical guarantees.        Speciﬁcally, we derive statistical guarantees for SOEs using
  The problem of identifying the SOE is a binary classiﬁca- Vapnik-Chervonenkis-style generalization bounds for classi-
tion problem where a false negative merely means a conserv- ﬁcation problems with weighted loss, of which the classiﬁca-
ative use of a low performance controller and a false positive tion problem with false positive loss is a special case. We areinterested practical bounds–bounds that are asymptotically A X, deﬁne   A =   C   A  : C     . The growth
competitive and have small pre-constants. While the SLT lit- function⊆ of is deﬁnedC ∩ as { ∩ ∈ C}
erature contains a vast collection of VC-style bounds, the ma-  C
                                                               ∆k( ) = max     A : A  X, A  = k .
jority of these bounds are stated and proved only for the prob-   C       {|C ∩ |   ⊆    | |   }
                                                                       k
ability of misclassiﬁcation and only a few are directly applica- Clearly, ∆k( ) 2 . The VC Dimension of is deﬁned as
                                                                 C ≤                       C
ble to our problem. Furthermore, SLT bounds are often de-                                  k
                                                                  d( ) = sup k Z : ∆k( ) = 2 .
rived with little emphasis on obtaining optimal pre-constants.     C       { ∈       C      }
In addition to the emphasis on ﬁnding small pre-constants, Let d = d(H). We assume that d < .
                                                                                    ∞
our analysis has two unique aspects. First, we assume that Theorem 2.1. [Vapnik, 2000, Equations 3.26]
it is possible to achieve small empirical loss (which is true
for the case of false positive loss). Second, our analysis is                 1      2en     4
upper-tail-oriented, as we are only interested in deriving up- l(h) <δ ln(h) +   d ln    + ln  .     (2.1)
                                                                            s n       d      δ
per bounds for the expected loss.                                                            
                                                        The bound (2.1) is obtained by applying a very general re-
2  Preliminaries                                      sult of Vapnik to the special setting of learning binary clas-
                                                      siﬁers with a speciﬁc loss function l(ρ). It is thus natural to
Notations: We use the symbols P, E, and V to denote the ask if this bound can be improved. While many SLT results
probability, expectation, and variance, respectively. σ = have been obtained that address this issue, the majority of are
    n
 σi i=1 denotes a Rademacher sequence–a sequence of in- formulated and proved for l = l(1) only. One of the goals
dependent,{ } symmetric 1/1-valued random variables.
                   −                                  of this paper is to examine if these results can be extended to
  Let X and Y be non-empty sets and Z = X    Y . A                    (ρ)
                                           ×          the loss functions l , ρ [0, 1). It turns out that these re-
pair of (x, y) X    Y is denoted as z. Let (Z, µ) be a sults fall into two categories:∈ those that are applicable for all
ﬁxed probability∈ measure.× A training set is a ﬁnite sample
         n              n      n                      ρ   [0, 1], and those that are applicable for ρ = 0, 1 only.
 n =   zi    =  (xi, yi)     Z   drawn independently    ∈
S    {  }i=1   {      }i=1 ∈                            We conclude the preliminaries with the statement of the
according to µ. The probability and expectation with respect often-used Sauer’s lemma.
to n are written as Pn and En. A hypothesis space is a                                           d
  S                                                   Sauer’s Lemma.  [Sauer, 1972] ∆n(H)  (en/d) .
set H of functions from X to Y . A loss function is a func-                              ≤
tion l : Y  Y     R. The loss of a hypothesis h H
on z is l(h,× z) =→l(h(x), y). The expected loss of∈h is 3 VC Dimension-Based Bounds
l(h) = El(h, z). The empirical loss of h on the training set Suppose that h is a hypothesis with “small” empirical loss:
                       1   n
   is l (h) = l(h, ) =        l(h, z ). We assume that ln(h)   ǫ . We would like to bound the probability that
 n   n           n     n   i=1    i                         ≤  1
allS loss functions haveS range [0, 1]. When Y is ﬁnite, we have l(h) is “large”: l(h) > ǫ for some ǫ > ǫ1. This amounts to
                                                              P
a classiﬁcation problem. WhenP Y = 2, the classiﬁcation is bounding n(Q) where Q is deﬁned as
                           | |
binary, and let Y = 1, 1 . Here we only deal with binary        Q  =   n : h : ln(h) ǫ1, l(h) > ǫ .
classiﬁcation.   {−    }                                             {S  ∃        ≤          }
  If we assume that correct classiﬁcations incur zero loss, i.e. There are two major approaches to do this: The classical ap-
l( 1, 1) = l(1, 1) = 0, then what emerges is a loss function proach of Vapnik and Chervonenkis [1971], and the approach
that− we− refer to as weighted classiﬁcation error, deﬁned as: based on abstract concentration inequalities developed by Ta-
                                                      lagrand and others.
                 0  if y = y′
    (ρ)
    l (y, y′) =  ρ  if y = 1, y′ = 1 (false negative) 3.1  The Classical Approach
                        −
                 1  if y′ = 1, y = 1 (false positive).
                        −                            The classical VC analysis begins with the observation that
                                                      Pn(Q)    P(R)/P(R  Q) for any R that satisﬁes P(R Q) >
Here ρ is a number between 0 and 1. The idea is that false
                                                      0. We then≤ deﬁne R based| on n and an additional indepen-|
positives (bad errors) are more costly than false negatives dent sample whose size may orS may not be equal to n. In this
(good errors). When ρ = 1, i.e. when no difference is made                                  ′       2n
                                                      analysis we take the former approach: Let n = zi i=n+1
between the two types of errors, the loss is called the misclas- be an independent sample of size n, commonlyS referred{ } to
                         (0)                                                   ′          ′
siﬁcation error. When ρ = 0, l (h) is the probability of the as the ghost sample and let ln(h) = l( n, h), the empiri-
                                                                                         S         ′
classiﬁer h making a false positive error.            cal loss on the ghost sample. Denote n = ( n, ). Let
                                                                                      S2      S  Sn
  Given a training set n, a hypothesis h H, and a loss 0  ǫ1 < ǫ2  ǫ. Deﬁne R as
function l, how can weS bound the expected∈ loss l(h)? Sta- ≤    ≤
                                                               R =   2n : h : ln(h) ǫ1, l′ (h) > ǫ2 . (3.1)
tistical learning theory (SLT) provides a probabilistic answer     {S    ∃       ≤    n       }
to this question. A typical SLT result of the form Pn(l(h) >
                                                      Upper bounding Pn(Q)  now  reduces to upper bounding
ǫ) < δ or, succinctly, l(h) <δ ǫ provides an upper bound ǫ
                                                      P n(R) (the covering step) and lower bounding P n(R Q)
on l(h) with conﬁdence at least 1 δ, where ǫ and δ are pos- 2                                    2    |
itive, reasonably small numbers. The− upper bound ǫ typically (the symmetrization step).
                                                        The Covering Step: Upper Bounding P2n(R). Intuitively,
depends on δ, the sample size n, the empirical loss ln(h), and P
a complexity measure of the hypothesis space H. The most 2n(R) is small because if h has small empirical loss on a
important complexity measure in SLT is VC dimension. Let sample, its empirical loss on a ghost sample should also be
                                                      small. We can change the deﬁnition of R in (3.1) as
    2X  be a set of subsets of X. Note that H, as a set of
binaryC ⊆ classiﬁers on X, is an example of such sets. For any R = 2n : h : l′ (h) ln(h) > η, where η = ǫ2 ǫ1 .
                                                              S   ∃    n   −                     −
                                                                                                    The next step uses the so-called permutation technique. Lemma 3.4. [Blumer et al., 1989]

 P2n(R) = E2n  h : ln′ (h) ln(h) > η                                                    d
              ∃       −                                  P                         2en
                     n                                    2n ln(h) = 0, ln′ (h) > ǫ2     exp ( nǫ2 ln 2) .
                                                                              ≤    d        −
       = E2nEσ    h :  σi(l(h, zi+n) l(h, zi)) > nη .                                
                ∃                 −                                           
                    i=1                        !        The Symmetrization Step: Lower Bounding P n(R Q). To
                    X                                                                         2   |
                                                      lower bound P2n(R Q), we can ﬁx n, ignore the condition
Next, we ﬁx 2n and bound the inner expectation. Let                    |            S
          S                                           ln(h)  ǫ1, and bound the following conditional probability:
                                              2n
  H2n =  h2n = (h(x1), . . . , h(x2n)) h H 1, 1 .          ≤
        {                     |  ∈  } ⊆ {−   }                      P ′
                                                                      n    n′ : h : ln′ (h) > ǫ2 h : l(h) > ǫ ,
The mapping h    h2n is many-to-one. Denote li(h2n) =                S   {S  ∃            }|∃
                                                        or, equivalently, P
l(h, zi), 1 i 7→2n. The inner expectation can be written as          n (   h : ln(h) > ǫ2 h : l(h) > ǫ) . 
        ≤   ≤                                                          {∃            }|∃
                   n                                  Let h be a hypothesis such that l(h) > ǫ. It sufﬁces to lower
   Eσ   h2n  H2n :    σi (li+n(h2n) li(h2n)) > nη     bound P              Intuitively, this quantity is large be-
       ∃    ∈                    −                           n(ln(h) > ǫ2).
                   i=1                        !
                  X                                   cause the empirical loss should be large (> ǫ2) wherever the
which, by the union bound, is bounded by              expected loss is large (> ǫ). Since ǫ2 < ǫ, we have
               n
                                                       Pn(ln(h)  ǫ2)  P(l(h)  ln(h) > ǫ ǫ2)
                                                               ≤    ≤       −         −
          Eσ     σi (li+n(h2n) li(h2n)) > nη . (3.2)                                  2
                            −                                                6n(ǫ  ǫ2)
   h2n H2n    i=1                        !                                                           (3.3)
      ∈                                                               exp        −       := ι(n, ǫ, ǫ2),
     X        X                                                     ≤      − 4(ǫ ǫ2) + 3
                                                                                      
The cardinality of H2n is at most ∆2n(H), which is at most                     −
       d                                              by Bernstein’s left-tail inequality. Coupled this with
(2en/d) by Sauer’s lemma. For a ﬁx h2n H2n, the sum-
mand in (3.2) can be written as     ∈                 Lemma 3.2, we obtain the following result.
              n                                       Corollary 3.5.
         P
          σ     σi (li+n(h2n) li(h2n)) > nη             P
                           −                             n (ln(h) ǫ1, l(h) > ǫ)
             i=1                        !                       ≤
             X                                                                      d               2
                                                                            1  2en          n(ǫ2  ǫ1)
which, by Hoeffding’s¨ right-tail inequality, is bounded by 1 (1  ι(n, ǫ, ǫ2))−      exp  −     −      .
                                                        ≤   ∨   −               d          2(ǫ2 + ǫ1)
                    2 2                    2                                                      
                  2n η                   nη                                 
   exp          −             2    exp        .
        4   (li+n(h2n) li(h2n))  ≤     −  2             In this inequality, the parameter ǫ2 is unspeciﬁed, and we
           i        −                    
                                                      can minimize the bound over ǫ2 (ǫ1, ǫ).
Thus we haveP arrived at the following result.                                   ∈   (ρ)
                                                        When  ρ = 0, 1, the loss function l is binary, and nln(h)
Lemma 3.1. [Vapnik and Chervonenkis, 1971]            is a binomial random variable with parameters n and l(h).
                               d                      We can thus use several lower bounds on the right-tails of the
                           2en                  2
P2n ln(h)  ǫ1, l′ (h) > ǫ2       exp  .5n(ǫ2  ǫ1) .   binomial to obtain
         ≤    n        ≤    d        −      −
                             
                                                        nǫ > 1  Pn ( ln(h) > ǫ/4 h : l(h) > ǫ) > 1/4 (3.4)
  When ǫ1   ǫ2, it is possible to improve upon Lemma 3.1.       ⇒    {          }|∃
The idea is,≪ instead of bounding the probability that the ab- nǫ > 2 Pn ( ln(h) > ǫ/2 h : l(h) > ǫ) > 1/2 (3.5)
                ′                                               ⇒    {          }|∃
solute discrepancy ln(h) ln(h) is large, we bound the prob-
                               ′                      For the case     , we can set      and combine (3.4)
                    −         ln(h)−ln(h)                        ǫ1 > 0           ǫ2 =  ǫ
ability that the relative discrepancy ′  is large. We with Lemma 3.2 to obtain the following result.
                             √ln(h)+ln(h)
“weaken” the deﬁnition of R in (3.1) as               Corollary 3.6. [Vapnik and Chervonenkis, 1971] For l =
                                                       (ρ)
    R =    2n : h : l′ (h) ln(h) > η′ ,               l  , ρ = 0, 1,
         S    ∃   n   −
                 ln′ (h) + ln(h)   ln′ (h) + ln(h)                           4     2en     4
    η′ = (ǫ  ǫ )             = η             .                 l(h) <δ 2ln(h) +  d ln   + ln   .     (3.6)
          2   1                                                               n      d      δ
           −   r    ǫ2 + ǫ1     r    ǫ2 + ǫ1                                                
Now, proceed identically as before, except that η is now re-
           ′                                            For the case ǫ1 = 0, we can set ǫ2 = ǫ/2 and combine (3.5)
placed with η , we arrive at the following results.   with Lemma 3.4 to obtain the following result.
Lemma 3.2. [Vapnik and Chervonenkis, 1971]
                                                      Corollary 3.7. [Blumer et al., 1989] For l = l(ρ), ρ = 0, 1,
                               d               2
                           2en         n(ǫ2  ǫ1)
P2n ln(h)  ǫ1, ln′ (h) > ǫ2      exp  −    −      .                           2       2en     2
         ≤             ≤    d          2(ǫ2 + ǫ1)         ln(h) = 0  l(h) <δ      d ln   + ln   .    (3.7)
                                                               ⇒        n ln 2     d      δ
Corollary  3.3. [Vapnik and Chervonenkis, 1971]                                              
                                                        Shawe-Taylor et al. [1993] further improve (3.7), using an
                                  d                                                  ′
                              2en                     argument that uses a second sample of size k, and ǫ2 = rǫ,
   P2n ln(h) = 0, l′ (h) > ǫ2      exp ( .5nǫ2) .                                    k
                 n        ≤    d       −                                            S
                                                    where r = 1    2/(ǫk), k = n (ern/d 1).
  By considering  the relative discrepancy, we have man-        −                      −
                                                      Theorem 3.8. [Shawe-Taylorp et al., 1993] For l = l(ρ), ρ =
aged to insert the term ǫ2 + ǫ1, resulting in a tighter bound 0, 1,
in Lemma (3.2). Further tightening is possible when ǫ1 = 0.
Instead of using Hoeffding’s inequality, Blumer et al. [1989] nǫ > 4d Pn (ln(h) = 0, l(h) > ǫ)
                                                                ⇒
use a combinatorial argument that leads to the following im-                                  en
                                                                  2 exp 2√2d   ǫn + d ln ǫ + 2d ln . (3.8)
provement of Corollary 3.3.                                     ≤            −                d
                                                                                                 Compared with (3.7), the sample complexity derived  Simple algebra shows that for all h H, l(h)    (1 +
                               4                                                       ∈          ≤
from (3.8) is smaller by a factor of (1 √ǫ)  5.7 for  L˜ )l (h) + L˜2 . Consequently, an upper bound on L˜ can be
                               ln 2 −     ≈             n n       n                               n
typical values ǫ (say, < 0.05).                       translated into an upper bound on l(h). Unlike the previous
  We point out that Corollary 3.6, 3.7, and Theorem 3.8 were analysis, it appears that we can not use McDiarmid’s bounded
previously stated and proved for the loss function l = l(1) difference inequality, as the introduction of the term l(h)
only. Our analysis extends them to the case l = l(0) (and renders the “difference” unbounded. The solution to this
shows that they do not hold when 0 < ρ < 1). The cov- problem originates from the work of Talagrand on abstractp
ering argument remains the same, while the symmetrization concentration inequalities and their applications to bounding
                                     (ρ)
argument uses the simple observation that nln (h) is a bino- the suprema of empirical processes [Talagrand, 1994, 1996].
mial random variable with parameters n and l(h), regardless Talagrand’s inequalities were later improved using the so-
of whether ρ = 0 or ρ = 1.                            called entropy method. The following version provides the
                                                      best known bound.
3.2  Talagrand’s Method                               Lemma 3.12.  [Bousquet, 2002] Let be a countable set of
                                                                        R            F         E
Observe that l(h)  ln(h) + suph∈H (l(h) ln(h)). The   functions from Z to . Let b = supf∈F (sup( (f) f)),
                ≤                     −                           V                               E−
supremum is a random function of n, where changing a sin- v = supf∈F (f) and Bn = B( n) =  supf∈F ( (f)
                             S                            n                         S                  −
gle element zi results in a change of at most 1/n, and thus 1
                                                      n   i=1 f(zi)). Then for any α > 0,
                                             1   1
is <δ-bounded by En (sup ∈ (l(h) ln(h))) +     ln
                       h H     −             2n  δ      P                    2v ln(1/δ)   1  1   b ln(1/δ)
by McDiarmid’s inequality [McDiarmid, 1997]. The next Bn <δ (1 + α)En(Bn) +           +    +            .
                                          q                                      n        3  α      n
quantity to bound is the expectation of the supremum. This                 r                  
is accomplished using the concept of Rademacher aver-   We now apply Lemma 3.12 with f(z) = l(h, z))/ l(h),
age. Let G  be a class of functions from Z to the reals b = v = 1, to obtain
R. The  Rademacher average of G is deﬁned as RnG =                                                 p
                      1   n
R(G,  n, σ) = sup ∈          σig(zi) . In this analy-                         2 ln(1/δ)  1   1   ln(1/δ)
                 g G  n   i=1                          L˜n <δ (1 + α)En(L˜n) +        +    +           .
sis, theS role of is played by the loss class associated with                    n       3   α     n
             G                                                              r                 
H:  G  =  l  =   z     lP(h, z) : h  H . The sym-
          H                                                                   E   ˜
metrization inequality{ (e.g.7→ [Bartlett et al.∈, 2005])} states that We then proceed to bound n(Ln) with a technique from
                                                      Massart [2000] that is referred to as peeling and the concept
En (sup ∈ (l(h)  ln(h)))   2ERnlH  . Thus it remains
       h H     −        ≤                             of sub-root functions. A function ψ : (0, ) (0, ) is
to bound the Rademacher average. The technique is well-                                   ∞   →     ∞
established and based on the concepts of covering number. called sub-root if ψ is non-decreasing and ψ(r)/√r is non-
                      n                               increasing. Any sub-root function ψ(r) is known to have a
Denote by N(u, lH , L2(µ )) the u-covering number of lH              ∗        ∗     ∗
                          n                           unique ﬁx-point r (i.e. ψ(r ) = r ) [Bartlett et al., 2005].
with respect to the metric L2(µ ).
                                                      Now, suppose that ψ is a sub-root function with ﬁxed point
Lemma 3.9. [Dudley, 1999]                             r∗ such that
                    1
               12                   n                    E
     EσRnlH            ln N(u, lH , L2(µ ))du. (3.9)       n (sup l(h)  ln(h) : l(h) r )  ψ(r), r > 0.
            ≤  √n                                               {|   −      |     ≤   } ≤      ∀
                  Z0
                     p                                Then we can show that α > 0, L˜n is <δ-bounded by
  The  last piece of the puzzle reveals a bound on                        ∀
N(u, l , L (µn))), deﬁned based on the VC dimension d.
     H   2                                                            e       1         2 ln 1 (3 + α) ln 1
                                                      (1+α)  √r   1 +  (1 + ln( ))  +      δ +         δ .
Lemma 3.10. [Haussler, 1995] For all n:                         ∗     2       r       s   n       3αn
                                 S                                           ∗ 
                                       d                                       ∗
                     n             2e                 The ﬁnal step is to bound r using d. This can be done
          N(u, lH , L2(µ )) e(d + 1)    .     (3.10)
                        ≤          u2                 [Koltchinskii and Panchenko, 2000] by setting ψ(r) to be
                                    
                                                      Dudley’s entropy integral. What we end up with is a <
  Combining (3.9) and (3.10), with some algebra we obtain                                               δ
                                                      bound on l(h) that is O(d ln n/n), asymptotically compa-
the following result.
                                                      rable to those obtained using the classical approach such as
Theorem 3.11. (Dudley-Haussler)                       Theorem 3.8, albeit with worse constants.
                         d     ln(1/δ)                3.3  Summary of VC Dimension-Based Bounds
      l(h) <δ ln(h) + 30   +          .      (3.11)
                         n        2n                  Given a sample  n of size n and a hypothesis h that has
                       r     r                                       S
  The bound (3.11) is based on an analysis of the absolute empirical loss ln(h) on n, what can we say about the ex-
                                                      pected loss l(h) of h?S The results in this section provide
discrepancy l(h) ln(h). Its deviation term is O( d/n),
which should not− come as a surprise. It is natural to ask if several answers to this question. They all have the general
this approach can be used to analyze some form ofp relative form of “If ln(h) is small, then with high probability, l(h) is
discrepancy between the expected loss l(h) and the empirical small”. The common assumption is that h is selected from a
loss ln(h). Consider                                  hypothesis space H with ﬁnite VC dimension d. In the gen-
                                                      eral case when l is only assumed to have range [0, 1], Corol-
                                                                                          (ρ)
              l(h)  ln(h)                             lary 3.5 seems most useful. When l = l , ρ = 0, 1, we
    L˜n = sup     −      : h  H, l(h) > 0 .   (3.12)
             (     l(h)     ∈          )              can exploit several binomial tail inequalities to obtain much
                 psimpler bounds. When ln(h) > 0, Corollary 3.6 should be the hyper-rectangle contains a sampled point for which the
used. When ln(h) = 0, Theorem 3.8 provides the best-known algorithm does not converge (an unsafe point), then we elim-
bound. These results are all based on the idea of uniform inate that hyper-rectangle (since the guarantees are based on
convergence of relative discrepancies (UCRD). Even Corol- Theorem 3.8 and require zero false positives). Otherwise, we
lary 3.7 and Theorem 3.8 can be viewed as based on degener- count the number of safe points that lie outside the hyper-
ate cases of UCRD, with special-purpose combinatorial argu- rectangle (false negatives), and choose the hyper-rectangle
ments replacing the general-purpose Hoeffding’s bound. Re- that has the fewest number of false negatives. After look-
sults that are based on uniform convergence of absolute dis- ing at 10,000 random hyper-rectangles, we are able to come
crepancies such as Lemmas 3.1 and Theorem 3.11 are not up with one that contains 18,616 safe points and no unsafe
as useful for our purpose, as they have to cover situations that points. This hypothesis thus has 32,114 - 18,616 = 13,498
Vapnik and Chervonenkis [1971] refer to as pessimistic cases. false negatives. Note that the number of false negatives is still
Simply put, pessimistic bounds are loose since they need to quite large. This is because we use hyper-rectangles which
account for hypotheses with expected losses close to 0.5. In constitute a simple hypothesis space that does not approxi-
contrast, we only need to concern ourselves with hypotheses mate the decision surface very well (this SOE is nevertheless
with zero or small empirical losses.                  a big improvement over the trivial, empty SOE that is the
  The statistical/computational learning theory literature current state of the art). The advantage to this is that the VC
contains a vast collection of generalization bounds in the gen- dimension is low, and thus only a small number of samples
eral case of function learning and in the special case of learn- are required to obtain the statistical guarantee, which reads
ing binary classiﬁers. To our knowledge no work has explic- “The found hyper-rectangle has probability of false positive
itly derived generalization bounds for binary classiﬁers with bounded by 0.0035, and we have at least 95% conﬁdence in
weighted error penalties as deﬁned in this paper. The bounds this statement.”
in Section 3.1 originate from the seminal work of Vapnik and There are a number of alternatives to the above proce-
Chervonenkis [1971]. Our contribution here is the extensions dure. For example, we can use Corollary 3.6 instead of The-
to the case ρ = 0, and Corollary 3.5.                 orem 3.8, if the requirement of zero empirical loss is too
  Talagrand’s approach provides a completely different way restrictive. In the OAV example with 34,000 samples, this
to arrive at generalization bounds for all loss function leads to a hypothesis with 111 false positives but only 3272
l(ρ), ρ [0, 1] that are asymptotically equivalent with clas- false negatives (a reduction of 75%!) while still maintaining
sical bounds.∈ This approach analyzes the mean (or me- 99% conﬁdence that the probability of false positive is less
dian [Panchenko, 2002]) of the supremum of the (sometimes than 0.01. Furthermore, we can replace the criterion “as few
weighted) discrepancies between the expected and empiri- false negatives as possible” with other criteria, for example
cal losses using Talagrand’s various concentration inequali- one that prefers hypotheses with large volumes. Finally, if
ties, completed invariably with the symmetrization inequal- we are willing to make a decision-theoretic tradeoff between
ity, Dudley’s entropy integral bound, and Haussler’s packing false negatives and false positives (e.g. one false positive is as
bound. The resulting bounds often have much larger con- costly as one thousand false negatives), we can set ρ = .001
stants and are not as useful for our non-asymptotic purpose. and apply Corollary 3.5.

4  Experiments                                        5   Summary and Related Work
We now describe the applications of the bounds derived in It has been said that the divide between SLT and practice is
Section 3 to our OAV experiment as described in the introduc- of Grand Canyon proportions, perhaps because VC bounds
tion. We identify four factors that affect the computational are often too loose to be useful in practice. This paper of-
time of the iterative algorithm, and choose 4-dimensional fers a counterargument in the form of an SLT-based approach
axis-parallel hyper-rectangles as our hypothesis space. The to verifying complex controllers. We demonstrated this ap-
VC dimension of this hypothesis space is 8, as it is known that proach on a problem of signiﬁcant industrial and military
the VC dimension of axis-parallel hyper-rectangles in Rm is interest: Deriving a safe operating envelope for a complex
2m. Thus for δ = .05, ǫ = .05, we need 34,000 samples us- control algorithm. This approach offers control engineers
ing Theorem 2.1. But using Theorem 3.8, we need only 1810 a principled way to increasingly replace low-performance,
samples. With 34,000 samples, if we set δ = .05, ǫ can be as simple control algorithms with high-performance, complex
small as 0.0035. The improvement in generalization bound ones while still maintaining a statistically high conﬁdence in
(ǫ) is about 14-fold and in samples complexity (n) is about safety. A key to making this offer attractive lies in deriv-
18-fold.                                              ing practical VC-style generalization bounds for weighted bi-
  The search for the best hyper-rectangle in this experiment nary classiﬁcation (a problem that hitherto has not been given
is rather simple. For each sampled input, we determine if much attention). Our VC analysis, which builds upon stan-
the iterative algorithm converges. It turns out that in 32,114 dard VC analysis of unweighted binary classiﬁcation, shows
instances (roughly 94%), the algorithm converges. Despite that such bounds are indeed possible. They are signiﬁcantly
this high (empirical) rate of success, in current practice it better than a general bound by Vapnik. Our analysis precisely
still loses out to a PID-like controller with ﬁxed deterministic pointed to the place where the false negative penalty had an
computation time. Next, we randomly choose an axis-parallel effect, namely the symmetrization argument. We have suc-
4-dim hyper-rectangle in the input ranges as a hypothesis. If cessfully applied this veriﬁcation framework to several other