                        Topic and Role Discovery in Social Networks

                   Andrew McCallum, Andres´      Corrada-Emmanuel, Xuerui Wang
                                    Department of Computer Science
                                  University of Massachusetts Amherst
                                       Amherst, MA 01003 USA
                              {mccallum, corrada, xuerui}@cs.umass.edu

                    Abstract                          the network. Furthermore, using these properties we can as-
                                                      sign “roles” to certain nodes, e.g. [Lorrain & White, 1971;
    Previous work in social network analysis (SNA)    Wolfe & Jensen, 2003]. However, it is clear that network
    has modeled the existence of links from one en-   properties are not enough to discover all the roles in a social
    tity to another, but not the language content or  network. Consider email messages in a corporate setting, and
    topics on those links. We present the Author-     imagine a situation where a tightly knit group of users trade
    Recipient-Topic (ART) model for social network    email messages with each other in a roughly symmetric fash-
    analysis, which learns topic distributions based on ion. Thus, at the network level they appear to fulﬁll the same
    the direction-sensitive messages sent between en- role. But perhaps, one of the users is in fact a manager for the
    tities. The model builds on Latent Dirichlet Al-  whole group—a role that becomes obvious only when one
    location (LDA) and the Author-Topic (AT) model,   accounts for the language content of the email messages.
    adding the key attribute that distribution over top-
    ics is conditioned distinctly on both the sender and Outside of the social network analysis literature, there has
    recipient—steering the discovery of topics accord- been a stream of new research in machine learning and natural
    ing to the relationships between people. We give  language models for clustering words in order to discover the
    results on both the Enron email corpus and a re-  few underlying topics that are combined to form documents
                                                                                       [             ]
    searcher’s email archive, providing evidence not  in a corpus. Latent Dirichlet Allocation Blei et al., 2003 ro-
    only that clearly relevant topics are discovered, but bustly discovers multinomial word distributions of these top-
                                                                                      [             ]
    that the ART model better predicts people’s roles. ics. Hierarchical Dirichlet Processes Teh et al., 2004 can
                                                      determine an appropriate number of topics for a corpus. The
                                                      Author-Topic Model [Steyvers et al., 2004] learns topics con-
1  Introduction and Related Work                      ditioned on the mixture of authors that composed a document.
Social network analysis (SNA) is the study of mathemati- However, none of these models are appropriate for SNA, in
cal models for interactions among people, organizations and which we aim to capture the directed interactions and rela-
groups. With the recent availability of large datasets of hu- tionships between people.
man interactions [Shetty & Adibi, 2004; Wu et al., 2003], the The paper presents the Author-Recipient-Topic (ART)
popularity of services like Friendster.com and LinkedIn.com, model, a directed graphical model of words in a message gen-
and the salience of the connections among the 9/11 hijackers, erated given their author and a set of recipients. The model is
there has been growing interest in social network analysis. similar to the Author-Topic (AT) model, but with the crucial
  Historically, research in the ﬁeld has been led by social enhancement that it conditions the per-message topic distri-
scientists and physicists [Lorrain & White, 1971; Albert & bution jointly on both the author and individual recipients,
Barabasi,´ 2002; Watts, 2003; Wasserman & Faust, 1994], rather than on individual authors. Thus the discovery of top-
and previous work has emphasized binary interaction data, ics in the ART model is inﬂuenced by the social structure in
with directed and/or weighted edges. There has not, how- which messages are sent and received. Each topic consists of
ever, previously been signiﬁcant work by researchers with a multinomial distribution over words. Each author-recipient
backgrounds in statistical natural language processing, nor pair has a distribution over topics. We can also easily calcu-
analysis that captures the richness of the language contents late marginal distributions over topics conditioned solely on
of the interactions—the words, the topics, and other high- an author, or solely on a recipient, in order to ﬁnd the topics
dimensional speciﬁcs of the interactions between people. on which each person is most likely to send or receive.
  Using pure network connectivity properties, SNA often Most importantly, we can also effectively use these person-
aims to discover various categories of nodes in a network. conditioned topic distributions to measure similarity between
For example, in addition to determining that a node-degree people, and thus discover people’s roles by clustering using
distribution is heavy-tailed, we can also ﬁnd those particular this similarity. For example, people who receive messages
nodes with an inordinately high number of connections, or containing requests for photocopying, travel bookings, and
with connections to a particularly well-connected subset of meeting room arrangements can all be said to have the role“administrative assistant,” and can be discovered as such be-
cause in the ART model they will all have these topics with Latent Dirichlet Allocation  Author Model
high probability in their receiving distribution. Note that we      (LDA)           (Multi-label Mixture Model)
can discover that two people have similar roles even if in the [Blei, Ng, Jordan, 2003]  [McCallum 1999]
graph they are connected to very different sets of people.
                                                                      α                       a
  We demonstrate this model on the Enron email corpus                                         d
comprising 147 people and 23k messages, and also on about
10 months of incoming and outgoing mail of the ﬁrst author,           θ                       z
comprising 825 people and 23k messages. We show not only
that ART discovers extremely salient topics, but also gives
evidence that ART predicts people’s roles better than AT and          z
SNA. Furthermore we show that the similarity matrix pro-
duced by ART is different from both the SNA matrix and the
AT matrix in several appropriate ways.                 β     φ       w         β     φ        w
                                                               T                       A
  We also describe an extension of the ART model that ex-              Nd                       Nd
plicitly captures roles of people, by generating role associa-
tions for the authors and recipients of a message, and con-              D                       D
ditioning the topic distributions on role assignments. The
model, which we term Role-Author-Recipient-Topic (RART),      Author-Topic Model   Author-Recipient-Topic Model
naturally represents that one person can have more than one          (AT)                    (ART)
                                                        [Rosen-Zvi, Griffiths, Steyvers, Smyth 2004] [This paper]
role. We describe several possible RART variants, and de-
scribe preliminary experiments with one of these variants.
                                                                      ad                      ad   rd
2  Author-Recipient-Topic Models
                                                                      x                            x
Before describing the ART model, we ﬁrst describe three
related models. Latent Dirichlet Allocation (LDA) is a
Bayesian network that generates a document using a mixture α θ        z        α     θ          z
of topics [Blei et al., 2003]. In its generative process, for each A                 A,A
document d, a multinomial distribution θ over topics is ran-
domly sampled from a Dirichlet with parameter α, and then β  φ        w        β     φ         w
                                                               T                       T
to generate each word, a topic z is chosen from this topic              Nd                         Nd
distribution, and a word, w, is generated by randomly sam-
                                                                         D                           D
pling from a topic-speciﬁc multinomial distribution φz. The
robustness of the model is greatly enhanced by integrating out
uncertainty about the per-document topic distribution θ. Figure 1: Three related models, and the ART model. In
  The Author model (also termed a Multi-label Mixture all models, each observed word, w, is generated from a
Model) [McCallum, 1999], is a Bayesian network that simul- multinomial word distribution, φz, speciﬁc to a particular
taneously models document content and its authors’ interests topic/author, z, however topics are selected differently in each
with a 1-1 correspondence between topics and authors. For of the models.
each document d, a set of authors ad is observed. To generate
each word, an author, z, is sampled uniformly from the set,
and then a word, w, is generated by sampling from an author- versa, but the nature of the requests and language used may
                                                      be quite different. Even more dramatically, consider the large
speciﬁc multinomial distribution φz. The Author-Topic (AT)
model is a similar Bayesian network, in which each authors’ quantity of junk email that we receive; modeling the topics of
interests are modeled with a mixture of topics [Steyvers et al., these messages as undistinguished from the topics we write
2004]. In its generative process for each document d, a set of about as authors would be extremely confounding and unde-
                                                      sirable since they do not reﬂect our expertise or roles.
authors, ad, is observed. To generate each word, an author x
is chosen uniformly from this set, then a topic z is selected Alternatively we could still employ the AT model by ig-
from a topic distribution θx that is speciﬁc to the author, and noring the recipient information of email and treating each
then a word w is generated from a topic-speciﬁc multinomial email document as if it only has one author. However, in this
distribution φz. However, as described previously, none of case (which is similar to the LDA model) we are losing all in-
these models is suitable for modeling message data.   formation about the recipients, and the connections between
  An email message has one sender and in general more than people implied by sender-recipient relationships.
one recipients. We could treat both the sender and the recip- Thus, we propose an Author-Recipient-Topic (ART) model
ients as “authors” of the message, and then employ the AT for message data. The ART model captures topics and the di-
model, but this does not distinguish the author and the recipi- rected social network of senders and recipients by condition-
ents of the message, which is undesirable in many real-world ing the multinomial distribution over topics distinctly on both
situations. A manager may send email to a secretary and vice the author and one recipient of a message. Unlike the AT, theART model takes into consideration both author and recipi-  Topic 5           Topic 17           Topic 27
ents distinctly, in addition to modeling the email content as a “Legal Contracts” “Doc. Review” “Time Scheduling”
mixture of topics.                                     section    0.0299  attached  0.0742  day         0.0419
  The ART model is a Bayesian network that simultaneously party   0.0265  agreement 0.0493  friday      0.0418
models message content, as well as the directed social net- language 0.0226 review  0.0340  morning     0.0369
                                                       contract   0.0203  questions 0.0257  monday      0.0282
work in which the messages are sent. In its generative pro- date  0.0155  draft     0.0245  ofﬁce       0.0282
cess, for each message d, an author, ad, and a set of recip- enron 0.0151 letter    0.0239  wednesday   0.0267
ients, rd, are observed. To generate each word, a recipient, parties 0.0149 comments 0.0207 tuesday     0.0261
x, is chosen uniformly from rd, and then a topic z is chosen notice 0.0126 copy     0.0165  time        0.0218

from a multinomial topic distribution θad,x, where the distri- days 0.0112 revised  0.0161  good        0.0214
bution is speciﬁc to the author-recipient pair (ad, x). Finally, include 0.0111 document 0.0156 thursday 0.0191
the word w is generated by sampling from a topic-speciﬁc M.Hain   0.0549  G.Nemec   0.0737  J.Dasovich  0.0340
multinomial distribution φz. The result is that the discovery J.Steffes   B.Tycholiz        R.Shapiro
of topics is guided by the social network in which the collec- J.Dasovich 0.0377 G.Nemec 0.0551 J.Dasovich 0.0289
tion of message text was generated.                    R.Shapiro          M.Whitt           J.Steffes
  The Bayesian network for all models is shown in Figure 1. D.Hyvl 0.0362 B.Tycholiz 0.0325 C.Clair     0.0175
  In the ART model, given the hyperparameters α and β, an K.Ward          G.Nemec           M.Taylor
author a, and a set of recipients r, the joint distribution of the Topic 34   Topic 37           Topic 41
topic mixtures θ, the word mixtures φ, a set of recipients x, a “Operations” “Power Market”   “Gov. Relations”
set of topics z and a set of words w in the corpus is given by: operations 0.0321 market 0.0567 state   0.0404
                                                       team       0.0234  power     0.0563  california  0.0367
 p(θ, φ, x, z, w|α, β, a, r)                           ofﬁce      0.0173  price     0.0280  power       0.0337
              D  N                                     list       0.0144  system    0.0206  energy      0.0239
             Y   Yd
=p(θ|α)p(φ|β)       p(x  |r )p(z |θ     )p(w  |φ  ).   bob        0.0129  prices    0.0182  electricity 0.0203
                       dn d    dn  ad,xdn   dn  zdn    open       0.0126  high      0.0124  davis       0.0183
             d=1 n=1                                   meeting    0.0107  based     0.0120  utilities   0.0158
  Integrating over θ and φ, and summing over x and z, we gas      0.0107  buy       0.0117  commission  0.0136
get the marginal distribution of a corpus:             business   0.0106  customers 0.0110  governor    0.0132
                                                       houston    0.0099  costs     0.0106  prices      0.0089
                              D  N
               ZZ            Y  Yd X  X                S.Beck     0.2158  J.Dasovich 0.1231 J.Dasovich  0.3338
 p(w|α, β, a, r) = p(θ|α)p(φ|β)           p(x |r )
                                            dn  d      L.Kitchen          J.Steffes         R.Shapiro
                             d=1 n=1 xdn zdn           S.Beck     0.0826  J.Dasovich 0.1133 J.Dasovich  0.2440

               · p(zdn|θad,xdn )p(wdn|φzdn )dφdθ.      J.Lavorato         R.Shapiro         J.Steffes
                                                       S.Beck     0.0530  M.Taylor  0.0218  J.Dasovich  0.1394
                                                       S.White            E.Sager           R.Sanders
3  Experimental Results
We present results with the Enron email corpus and the per- Table 1: An illustration of several topics from a 50-topic run
sonal email of the ﬁrst author of this paper (McCallum). for the Enron Email Dataset. Each topic is shown with the top
After preprocessing, the Enron corpus we use contains 147 10 words and their corresponding conditional probabilities.
users and 23,488 emails, and the McCallum dataset consists The quoted titles are our own summary for the topics. Below
of 23,488 messages written by 825 authors, sent or received are prominent author-recipient pairs for each topic.
by McCallum during Jan.-Oct., 2004. Gibbs sampling is em-
ployed to conduct all experiements (as detailed in [McCallum
et al., 2004]).                                       President of Regulatory Affairs, and Steffes, who was Vice
                                                      President of Government Affairs. Results on the McCallum
3.1  Topics and Prominent Relations from ART          email dataset are reported in Table 2.
Table 1 shows the highest probability words from six topics in
                                                      3.2  Stochastic Blockstructures and Roles
an ART model trained on the 147 Enron users with 50 topics.
(The quoted titles are our own interpretation of a summary The stochastic equivalence hypothesis from SNA states that
for the topics.) The clarity and speciﬁcity of these topics are nodes in a network that behave stochastically equivalently
typical of the topics discovered by the model.        must have similar roles. In the case of an email network con-
  Beneath the word distribution for each topic are the three sisting of message counts, the natural way to measure equiva-
author-recipient pairs with highest probability of discussing lence is to examine the probability that a node communicated
that topic—each pair separated by a horizontal line, with the with other nodes. If two nodes have similar probability distri-
author above the recipient. For example, Hain, the top author bution over their communication partners, we should consider
of messages in the “Legal Contracts” topic, was an in-house them role-equivalent. We can measure a similarity symmetri-
lawyer at Enron. In the “Operations” topic, it is satisfying cally by calculating the Jensen-Shannon (JS) divergence, and
to see Beck, who was the Chief Operating Ofﬁcer at Enron. inverting it.
In the “Government Relations” topic, we see Dasovich, who Standard recursive graph-cutting algorithms on this matrix
was a Government Relation Executive, Shapiro who was Vice can be used to cluster users, rearranging the rows/columns to    Topic 5   Topic 31     Topic 38     Topic 41                16 : teb.lokey
                                                             15 : steven.harris
    “Grant   “Meeting       “ML         “Friendly          14 : kimberly.watson
  Proposals”  Setup”       Models”     Discourse”             13 : paul.y’barbo
                                                                12 : bill.rapp
  proposal   today     model           great                   11 : kevin.hyatt
  data       tomorrow  models          good                    10 : drew.fossum
                                                             9 : tracy.geaccone
  budget     time      inference       don                    8 : danny.mccarty
  work       ll        conditional     sounds                7 : shelley.corman
                                                               6 : rod.hayslett
  year       meeting   methods         work                  5 : stanley.horton
  glenn      week      number          wishes                   4 : lynn.blair
                                                               3 : paul.thomas
  nsf        talk      sequence        talk                  2 : larry.campbell
  project    meet      learning        interesting         1 : joe.stepenovitch
  sets       morning   graphical       time                                   1 2 3 4 5 6 7 8 910111213141516

  support    monday    random          hear              16                     16
                                                         15                     15
  smyth      ronb      casutton        mccallum          14                     14
  mccallum   mccallum  mccallum        culotta           13                     13
                                                         12                     12
  mccallum   wellner   icml04-webadmin mccallum          11                     11
                                                         10                     10
  stowell    mccallum  icml04-chairs   casutton          9                      9
  mccallum   casutton  mccallum        mccallum          8                      8
                                                         7                      7
  lafferty   mccallum  casutton        ronb              6                      6
                                                         5                      5
  mccallum   mccallum  nips04workﬂow   mccallum          4                      4
  smyth      casutton  mccallum        saunders          3                      3
                                                         2                      2
  pereira    mccallum  weinman         mccallum          1                      1
  lafferty   wellner   mccallum        pereira             1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Table 2: The four topics most prominent in McCallum’s email Figure 2: Top: SNA Inverse JS Network. Left Bottom: ART
exchange with smyth (Padhraic Smyth), from a 50-topic run Inverse JS Network. Right Bottom: AT Inverse JS Network.
of ART on 10 months of McCallum’s email. The topics pro- Darker shades indicate higher similarity.
vide an extremely salient summary of McCallum and Smyth’s
relationship during this time period: they wrote a grant pro- President—rather different roles—and, thus output of ART
posal together; they set up many meetings; they discussed and AT is more appropriate. Thus, SNA analysis shows that
machine learning models; they were friendly with each other. they wrote email to similar sets of people, but the ART anal-
Below are prominent author-recipient pairs for each topic. ysis illustrates that they used very different language.
The people other than smyth also appear in very sensible as- Here ART and AT provide similar role distance, but they
sociations: stowell is McCallum’s proposal budget adminis- show their differences elsewhere. For example, AT indicates
trator; McCallum also wrote a proposal with lafferty (John a very strong role similarity between Geaconne and Hayslett
Lafferty) and pereira (Fernando Pereira); McCallum also sets (user 6), who was her boss (and CFO & Vice President in
up meetings, discusses machine learning and has friendly dis- the Division); on the other hand, ART more correctly desig-
course with his graduate student advisees: ronb, wellner, ca- nates a low role similarity for this pair—in fact, ART assigns
sutton, and culotta; he does not, however, discuss the details low similarity between Geaconne and all others in the ma-
of proposal-writing with them.                        trix, which is appropriate because she is the only executive
                                                      assistant in this small sample of Enron employees.
form approximately block-diagonal structures. This is the fa- Another interesting pair is Blair (user 4) and Watson (user
miliar process of ‘blockstructuring’ used in SNA. We perform 14). ART predicts them to be role-similar, while the SNA and
such an analysis on two datasets: a small subset of the Enron AT models do not. ART’s prediction seems more appropriate
users consisting mostly of people associated with the Tran- since Blair worked on “gas pipeline logistics” and Watson
swestern Pipeline Division within Enron, and the entirety of worked on “pipeline facility planning”, two very similar jobs.
McCallum’s email.                                       Based on the above examples, and other similar examples,
  Beginning with the Enron data, Figure 2 shows the results we would claim that the ART model is clearly better than the
from traditional SNA (in this case, JS divergence of distri- SNA model in predicting role-equivalence between users, and
butions on recipients from each sender), ART (JS divergence somewhat better than the AT model in this capacity.
of recipient-marginalized topic distributions for each sender) We also carried out this analysis with the personal email
and AT (using the topics distributions from AT instead of for McCallum to further validate the difference between the
our ART). Darker shading indicates higher similarity between ART and SNA predictions. There are 825 users in this email
people.                                               corpus. Table 3 shows the closest pairs, as calculated by the
  Consider Enron employee Geaccone (user 9 in all the ma- ART model and SNA model. The difference in quality be-
trices in Figure 2). According to the traditional SNA role tween the ART and SNA halves of the table is striking.
measurement, Geaccone and McCarty (user 8) have very sim- Almost all the pairs predicted by the ART model look rea-
ilar roles, however, both the AT and ART models indicate no sonable while many of those predicted by SNA are not at
special similarity. Inspection of the data reveals that Gea- all. For example, ART matches mike and mikem are actu-
conne was an Executive Assistant, while McCarty was a Vice- ally two different email addresses for the same person. (Most         Pairs considered most alike by ART
 User Pair        Description                              Role-Author-Recipient-Topic Role-Author-Recipient-Topic
                                                                  Model 1                   Model 2
 editor reviews   Both journal review management                  (RART1)                  (RART2)
 mike mikem       Same person! (manual coref error)

 aepshtey smucker Both students in McCallum’s class                ad   rd                   ad  rd
 coe laurie       Both UMass admin assistants
 mcollins mitchell Both ML researchers on SRI project
                                                                        x        γ   ψ       gd  hd
          Pairs considered most alike by SNA                                           A
 User Pair        Description                          γ   ψ       g    h                        h
 aepshtey rasmith Both students in McCallum’s class          A
 donna editor     Spouse is unrel. to journal editor
                                                       α    θ        z          α    θ         z
 donna krishna    Spouse is unrel. to conf. organizer       R,R                      R,R
 donna ramshaw    Spouse is unrel. to researcher at BBN
 donna reviews    Spouse is unrel. to journal editor   β    φ        w          β    φ         w
                                                             T                         T
                                                                        Nd                        Nd
Table 3: Pairs considered most alike by ART and SNA on                   D                         D
McCallum  email. All pairs produced by the ART model
are accurately quite similar. This is not so for the top SNA Figure 3: Two possible variants for the Role-Author-
pairs. Many users are considered similar by SNA merely be- Recipient-Topic (RART) model.
cause they appear in the corpus mostly sending email only to
McCallum. However, this causes people with very different
                                                      lated topics gather together clusters that indicate roles. Each
roles to be incorrectly declared similar—such as McCallum’s
                                                      sender-role and recipient-role pair has a multinomial distribu-
spouse (donna) and the JMLR editor.
                                                      tion over topics, and each topic has a multinomial distribution
                                                      over words.
other correferent email addresses were pre-collapsed by hand As shown in Figure 3, different strategies can be employed
during preprocessing; here ART has pointed out out a mis- to incorporate the “role” latent variables. First in RART1,
taken omission, indicating the potential for ART to be used role assignments can be made separately for each word in a
as a helpful component of an automated coreference system.) document. This model represents that a person can change
Users coe and laurie are both UMass CS Department admin- role during the course of the email message. In RART2, on
istrative assistants; they rarely send email to the same people, the other hand, a person chooses one role for the duration of
but they write about similar things. On the other hand, the the message. Here each recipient of the message selects a role
pairs declared most similar by the SNA model are mostly ex- assignment, and then for each word, a recipient (with corre-
tremely poor. Most of the pairs include donna, and indicate sponding role) is selected on which to condition the selection
pairs of people who are similar only because in this corpus of topic. Some other variants are possible, for example (not
they appeared mostly sending email only to McCallum, and shown in Figure 3), the recipients together result in the selec-
not others. User donna is McCallum’s spouse.          tion of a common, shared role, which is used to condition the
                                                      selection of every word in the message. This last model may
4  Role-Author-Recipient-Topic Models                 help capture the fact that a person’s role may depend on the
                                                      other recipients of the message, but also restricts all recipients
To better explore the roles of authors, an additional level of to a single role.
latent variables can be introduced to explicitly model roles. The generative process of RART models is similar to that
Of particular interest is capturing the notion that a person can for ART, as described in Section 2. The Gibbs sampling for-
have multiple roles simultaneously—for example, a person mulae for RART models can be derived in the same way as
can be both a professor and a mountain climber. Each role is for ART, but in a more complex form.
associated with a set of topics, and these topics may overlap.
For example, professors’ topics may prominently feature re-
search, meeting times, grant proposals, and friendly relations; 5 Experimental Results with RART
climbers’ topics may prominently feature mountains, climb- Preliminary experiments have been conducted with the
ing equipment, and also meeting times and friendly relations. RART1 model. Because we introduce two sets of additional
  We incorporate into the ART model a new set of vari- latent variables (author role and recipient role), the sampling
ables that take on values indicating role, and we term this procedure at each iteration is signiﬁcantly more complex. To
augmented model the Role-Author-Recipient-Topic (RART) make inference more efﬁcient, we can instead perform it in
model. In RART, authors, roles and message-contents are two distinct parts. One strategy we have found useful is to
modeled simultaneously. Each author has a multinomial dis- ﬁrst train an ART model, and use a sample to obtain topic
tribution over roles. Authors and recipients are mapped to assignments and recipient assignments for each word token.
some role assignments, and a topic is selected based on these Then, in the next stage, we treat topics and recipients as ob-
roles. Thus we have a clustering model, in which appear- served (locked). Although such a strategy may not be rec-
ances of topics are the underlying data, and sets of corre- ommended for arbitrary graphical models, we feel this is rea-