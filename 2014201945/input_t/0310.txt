              Measuring Semantic Similarity by Latent Relational Analysis 

                                            Peter D. Turney 
                                  Institute for Information Technology 
                                   National Research Council Canada 
                       M-50 Montreal Road, Ottawa, Ontario, Canada, K1A 0R6 
                                      peter.turney@nrc-cnrc.gc.ca 

                    Abstract                          For example, the word pair mason:stone is analogous to the 
                                                      pair carpenter:wood; the relation between mason and stone 
    This  paper  introduces  Latent  Relational  Analysis is highly similar to the relation between carpenter and wood.  
    (LRA),  a  method  for  measuring  semantic  similar-
                                                        Past  work  on  semantic  similarity  measures  has  mainly 
    ity. LRA measures similarity in the semantic rela-
                                                      been  concerned  with  attributional  similarity.  For  instance, 
    tions between two pairs of words. When two pairs  Latent Semantic Analysis (LSA) can measure the degree of 
    have a high degree of relational similarity, they are 
                                                      similarity between two words, but not between two relations 
    analogous.  For  example,  the  pair  cat:meow  is 
                                                      [Landauer and Dumais, 1997].  
    analogous  to  the  pair  dog:bark.  There  is  evidence Recently the Vector Space Model (VSM) of information 
    from  cognitive  science  that  relational  similarity  is 
                                                      retrieval has been adapted to the task of measuring relational 
    fundamental to many cognitive and linguistic tasks 
                                                      similarity, achieving a score of 47% on a collection of 374 
    (e.g.,  analogical  reasoning).  In  the  Vector  Space college-level multiple-choice word analogy questions [Tur-
    Model  (VSM)  approach   to  measuring  relational 
                                                      ney and Littman, 2005]. The VSM approach represents the 
    similarity,  the similarity between two pairs is cal-
                                                      relation between a pair of words by a vector of frequencies 
    culated by the cosine of the angle between the vec- of predefined patterns in a large corpus.  
    tors  that  represent  the  two  pairs.  The  elements  in 
                                                        LRA  extends  the  VSM  approach  in  three  ways:  (1)  the 
    the vectors are based on the frequencies of manu-
                                                      patterns are derived automatically from the corpus (they are 
    ally constructed patterns in a large corpus. LRA ex- not  predefined),  (2)  the  Singular  Value  Decomposition 
    tends the VSM approach in three ways: (1) patterns 
                                                      (SVD) is used to smooth the frequency data (it is also used 
    are derived automatically from the corpus, (2) Sin-
                                                      this  way  in  LSA),  and  (3)  automatically  generated  syno-
    gular  Value  Decomposition  is  used  to  smooth  the nyms are used to explore reformulations of the word pairs. 
    frequency  data,  and  (3)  synonyms  are  used  to  re-
                                                      LRA  achieves  56%  on  the  374  analogy  questions,  statisti-
    formulate  word  pairs.  This  paper  describes  the 
                                                      cally equivalent to the average human score of 57%. On the 
    LRA algorithm and experimentally compares LRA     related problem of classifying noun-modifier relations, LRA 
    to VSM on two tasks, answering college-level mul-
                                                      achieves  similar  gains  over  the  VSM.  For  both  problems, 
    tiple-choice  word  analogy  questions  and  classify-
                                                      LRA’s performance is state-of-the-art. 
    ing  semantic  relations  in  noun-modifier  expres- To motivate this research, Section 2 briefly outlines some  
    sions. LRA achieves state-of-the-art results, reach-
                                                      possible applications for a measure of relational similarity. 
    ing human-level performance on the analogy ques-
                                                      Related work with the VSM approach to relational similar-
    tions  and  significantly  exceeding  VSM  perform- ity  is  described  in  Section  3.  The  LRA  algorithm  is  pre-
    ance on both tasks.  
                                                      sented  in  Section  4.  LRA  and  VSM  are  experimentally 
                                                      evaluated by their performance on  word analogy questions 
1  Introduction                                       in Section 5 and on classifying semantic relations in noun-
This paper introduces  Latent  Relational  Analysis (LRA), a modifier expressions in Section 6. We discuss the interpreta-
method for measuring relational similarity. LRA has poten- tion  of  the  results,  limitations  of  LRA,  and  future  work  in 
tial  applications  in  many  areas,  including  information  ex- Section 7. The paper concludes in Section 8.  
traction,  word  sense  disambiguation,  machine  translation, 
and information retrieval.                            2  Applications of Relational Similarity 
  Relational similarity  is correspondence between relations, 
                                                      Many  problems  in  text  processing  would  be  solved  (or  at 
in contrast with attributional similarity , which is correspon- least  greatly  simplified)  if  we  had  a  black  box  that  could 
dence  between  attributes  [Medin et  al. ,  1990].  When  two 
                                                      take  as  input  two  chunks  of  text  and  produce  as  output  a 
words have a high degree of attributional similarity, we say 
                                                      measure of the degree of similarity in the  meanings of the 
they are synonymous . When two pairs  of words have a high two chunks. We could use it for information retrieval, ques-
degree  of  relational  similarity,  we  say  they  are analogous . tion  answering,  machine  translation  using  parallel corpora,             n
                                                                               r ⋅r
information  extraction,  word  sense  disambiguation, text                 ƒ   ,1 i ,2 i    r ⋅r
summarization, measuring lexical cohesion, identifying sen- cosine( θ ) =    i=1          =   1 2  . 
                                                                         n         n         r ⋅ r
timent  and  affect  in  text,  and  many  other  tasks  in natural       ( )2  ⋅    ( )2    1   2
                                                                        ƒr  ,1 i  ƒ   r ,2 i
language  processing.  This  is  the  vision  that  motivates  re-      i=1i       = 1
search in paraphrasing [Barzilay and McKeown, 2001] and We  make  a  vector, r,  to  characterize  the  relationship  be-
textual entailment [Dagan and Glickman, 2004], two topics tween two  words, X and Y, by counting the frequencies of 
that have lately attracted much interest.             various short phrases containing X and Y. Turney and Litt-
  In the absence of such a black box, current approaches to man  [2005]  use  a  list  of  64  joining  terms,  such  as “of”, 
these problems typically use measures of attributional simi- “for”, and  “to”, to form 128 phrases that contain X and Y, 
larity. For example, the standard bag-of-words approach to such as “X  of Y”, “Y  of X”, “X  for Y”, “Y  for X”, “X  to Y”,  
information  retrieval  is  based  on  attributional  similarity and “Y  to X”.  These phrases are then used as queries for a 
[Salton and McGill, 1983]. Given a query, a search engine search engine and the number of hits (matching documents) 
produces  a  ranked  list  of  documents,  where  the  rank  of  a is recorded for each query. This process  yields a vector of 
document  depends  on  the  attributional  similarity  of  the 128 numbers. If the number of hits for a query is x, then the 
document  to  the  query.  The  attributes  are  based  on word corresponding element in the vector r is log( x + )1 .  
frequencies; relations between words are ignored.       Turney and Littman [2005] evaluated the VSM approach 
  Although  attributional  similarity  measures  are  very  use- by  its  performance  on  374  college-level  multiple-choice 
ful, we believe that they are limited and should be supple- SAT analogy questions, achieving a score of 47%. A SAT 
mented  by  relational  similarity  measures.  Cognitive  psy- analogy  question  consists  of  a  target  word  pair,  called  the 
chologists  have  also  argued  that  human  similarity  judge- stem ,  and  five choice   word  pairs.  To  answer  an  analogy 
ments  involve  both  attributional  and  relational  similarity question,  vectors  are  created  for  the  stem  pair  and  each 
[Medin et al.,  1990].                                choice  pair,  and  then  cosines  are  calculated  for  the  angles 
  Consider word sense disambiguation for example. In iso- between  the  stem  vector  and  each  choice  vector.  The  best 
lation, the word “plant” could refer to an industrial plant or guess is the choice pair with the highest cosine. We use the 
a  living  organism.  Suppose  the  word  “plant”  appears  in same set of analogy questions to evaluate LRA in Section 5.  
some text near the word “food”. A typical approach to dis- The best previous performance on the SAT questions was 
ambiguating  “plant”  would  compare  the  attributional  simi- achieved by combining thirteen separate modules [Turney et 
larity  of  “food”  and  “industrial  plant”  to  the  attributional al. , 2003]. The performance of LRA significantly surpasses 
similarity  of  “food”  and  “living  organism”  [Lesk,  1986; this combined system, but there is no real contest between 
Banerjee and Pedersen, 2003]. In this case, the decision may these  approaches,  because  we  can  simply  add  LRA  to the 
not be clear, since industrial plants often produce food and combination, as a fourteenth module. Since the VSM mod-
living organisms often serve as food. It would be very help- ule had the best performance of the thirteen modules [Tur-
ful to know the relation between “food” and “plant” in this ney et al. , 2003], the following experiments focus on com-
example.  In  the  text  “food for   the  plant”,  the  relation  be- paring VSM and LRA. 
tween  food  and  plant  strongly  suggests  that  the  plant  is  a The VSM was also evaluated by its performance as a dis-
living organism, since industrial plants do not need food. In tance  measure  in  a  supervised  nearest  neighbour  classifier 
the  text  “food at   the  plant”,  the  relation  strongly  suggests for  noun-modifier  semantic  relations  [Turney  and  Littman, 
that  the  plant  is  an  industrial  plant,  since  living  organisms 2005]. The problem is to classify a noun-modifier pair, such 
are not usually considered as locations.              as  “laser  printer”,  according  to  the  semantic  relation  be-
  A  measure  of  relational  similarity  could  potentially  im- tween the head noun (printer) and the modifier (laser). The 
prove  the  performance  of  any  text  processing  application evaluation  used  600  noun-modifier  pairs  that  have  been 
that currently uses a measure of attributional similarity. We manually labeled with 30 classes of semantic relations [Nas-
believe  relational  similarity  is  the  next  step,  after  attribu- tase and Szpakowicz, 2003]. For example, “laser printer” is 
tional similarity, towards the black box envisioned above. classified as instrument ; the printer uses the laser as an in-
                                                      strument for printing. A testing pair is classified by search-
3  Related Work                                       ing  for  its  single  nearest  neighbour  in  the  labeled  training 
Let R  be the semantic relation between a pair of words, A data. The best guess is the label for the training pair with the 
     1                                                highest cosine; that is, the training pair that is most analo-
and B,  and let R2 be the semantic relation between another 
pair, C and  D . We wish to measure the relational similarity gous  to the testing pair, according to VSM. LRA is evalu-
                                                      ated with the same set of noun-modifier pairs in Section 6.  
between R1 and R2. The relations R1 and R2 are not given to 
us;  our  task  is  to  infer  these  hidden  (latent)  relations  and 
then compare them.                                    4  Latent Relational Analysis 
  In the VSM approach of Turney and Littman [2005], we LRA takes as input a set of word pairs and produces as out-
create vectors, r1 and r2, that represent features of R1 and R2, put a  measure of the relational similarity between any two 
and measure the similarity of R1 and R2 by the cosine of the 

                                                      of  the  input  pairs.  LRA  relies  on  three  resources, (1)  a 
                =    >            =    >
angle  between r1 r 1,1 , , r ,1 n  and r2 r 1,2 , r ,2 n : search engine with a very large corpus of text, (2) a broad-
                                                      coverage  thesaurus  of  synonyms,  and  (3)  an  efficient  im-plementation of SVD. LRA does not use labeled data, struc- column j is the frequency of the j-th pattern (see step 6) in 
tured data, or supervised learning.                   phrases that contain the i-th word pair (see step 5). 
  LRA proceeds as follows:                            8.  Calculate  entropy:   Apply  log  and  entropy  transforma-
1. Find alternates:  For each word pair A:B  in the input set, tions  to  the  sparse  matrix  [Landauer  and  Dumais,  1997]. 
look in the thesaurus for the top num_sim  words (in the fol- Each  cell  is  replaced  with  its  logarithm,  multiplied  by  a 
lowing experiments, num_sim  = 10) that are most similar to weight based on the negative entropy of the corresponding 
A. For each A′  that is similar to A, make a new word pair column vector in the matrix. This gives more weight to pat-
A′:B .  Likewise,  look  for  the  top num_sim   words  that  are terns that vary substantially in frequency for each pair. 
most similar to B, and for each B′ , make a new word pair 9. Apply SVD:  After log and entropy transformations, ap-
A: B′ . A:B   is  called  the original   pair  and  each A′:B   or ply SVD to X . SVD decomposes X  into a product of three 
A: B′  is an alternate  pair. The intent is for alternates to have matrices UΣV T , where U  and V  are in column orthonor-
almost the same semantic relations as the original.   mal  form  (i.e.,  the  columns  are  orthogonal  and  have  unit 
2.  Filter  alternates:   For  each  original  pair A:B ,  filter  the length)  and Σ   is  a  diagonal  matrix  of singular  values  
2 × num_sim   alternates  as  follows.  For  each  alternate  pair, (hence SVD) [Golub and Van Loan, 1996]. If X  is of rank 
                                                              Σ                     Σ          <
send a query to the search engine, to find the frequency of r , then  is also of rank r . Let k , where k r , be the 
phrases that begin with one member of the pair and end with diagonal matrix formed from the top k  singular values, and 

the  other.  The  phrases  cannot  have  more  than max_phrase  let U k   and Vk   be  the  matrices  produced  by  selecting  the 
words (we use max_phrase  = 5). Sort the alternate pairs by corresponding  columns  from U   and V .  The  matrix 
                                                         Σ   T
the frequency of their phrases. Select the top num_filter  most U k k Vk  is the matrix of rank k  that best approximates the 
frequent  alternates  and  discard  the  remainder  (we  use original  matrix X ,  in  the  sense  that  it  minimizes  the  ap-
num_filter  = 3, so 17 alternates are dropped). This step tends proximation  errors  [Golub  and  Van  Loan,  1996].  We  may 
                                                                           Σ   T
to eliminate alternates that have no clear semantic relation. think  of  this  matrix U k k Vk   as  a  “smoothed”  or  “com-
3.  Find  phrases:   For  each  pair  (originals  and  alternates), pressed” version of the original matrix. SVD is used to re-
make  a  list  of  phrases  in  the  corpus  that  contain  the  pair. duce noise and compensate for sparseness. 
                                                                                Σ
Query the search engine for all phrases that begin with one 10. Projection:  Calculate U k k  (we use k  = 300, as rec-
member of the pair and end with the other, with a minimum ommended  by  Landauer  and  Dumais  [1997]).  This  matrix 
of min_inter  intervening words and a maximum of max_inter  has  the  same  number  of  rows  as X ,  but  only k   columns 
intervening  words  (we  use min_inter  =  1, max_inter  =  3  = (instead  of  2  × num_patterns   columns;  in  our  experiments, 
max_phrase  –  2).  We  ignore  suffixes  when  searching  for that  is  300  columns  instead  of  8,000).  We  do  not  use V , 
phrases  that  match  a  given  pair.  These  phrases  reflect  the because we want to calculate the cosines between row vec-
semantic relations between the words in each pair.    tors, and it can be proven that the cosine between any two 
                                                                      Σ
4.  Find  patterns:   For  each  phrase  found  in  the  previous row vectors in U k k  is the same as the cosine between the 
                                                                                     Σ   T
step, build patterns from the intervening words. A pattern is corresponding two row vectors in U k k Vk . 
constructed by replacing any or all or none of the interven- 11. Evaluate alternates:  Let A:B  and C:D  be any two word 
ing  words with wild cards (one wild card can only replace pairs in the input set. From step 2, we have (num_filter  + 1) 
one  word).  For  each  pattern,  count  the  number  of  pairs versions of A:B , the original and num_filter  alternates. Like-
(originals and alternates) with phrases that match the pattern wise,  we  have (num_filter  +  1)  versions  of C:D .  Therefore 
(a  wild  card  must  match  exactly  one  word).  Keep  the  top we have (num_filter  + 1) 2 ways to compare a version of A:B 
                                                                                                      Σ
num_patterns  most frequent patterns and discard the rest (we with a version of C:D . Look for the row vectors in U k k  
use num_patterns  = 4,000). Typically there will be millions that  correspond  to  the  versions  of A:B   and  the  versions  of 
of patterns, so it is not feasible to keep them all.  C:D   and  calculate  the (num_filter  +  1) 2  cosines  (in  our  ex-
5. Map pairs to rows:  In preparation for building a matrix periments, there are 16 cosines).  
X , suitable for SVD, create a mapping of word pairs to row 12. Calculate relational similarity:  The relational similar-
numbers.  For  each  pair A:B , create  a  row  for A:B   and  an- ity  between A:B   and C:D   is  the  average  of  the  cosines, 
other row for B:A . This will make the matrix more symmet- among  the (num_filter  +  1) 2  cosines  from  step  11,  that  are 
rical, reflecting our knowledge that the relational similarity greater than or equal to the cosine of the original pairs, A:B  
between A:B  and C:D  should be the same as the relational and C:D .  The  requirement  that  the  cosine  must  be  greater 
similarity between B:A  and D:C . (Mason is to stone as car- than or equal to the original cosine is a way of filtering out 
penter is to wood. Stone is to mason as wood is to carpen- poor analogies, which may be introduced in step 1 and may 
ter.) The intent is to assist SVD by enforcing this symmetry have slipped through  the filtering in step 2.  Averaging  the 
in the matrix.                                        cosines, as opposed to taking their maximum, is intended to 
6. Map patterns to columns:  Create a mapping of the top provide some resistance to noise. 
num_patterns  patterns to column numbers. For each pattern In  our  experiments,  the  input  set  contains  from  600  to 
P, create a column for “ word 1 P word 2” and another column 2,244 word pairs. Steps 11 and 12 can be repeated for each 
for  “ word 2 P  word 1”.  Thus  there  will  be  2  × num_patterns  two input pairs that are to be compared.  
columns in X .                                          In the following experiments, we use a local copy of the 
7.  Generate  a  sparse  matrix: Generate  a  matrix X   in Waterloo MultiText System (WMTS) as the search engine, 
sparse  matrix  format.  The  value  for  the  cell  in  row i  and with a corpus of about 5 × 10 10  English words. The corpus was gathered by a web crawler from US academic web sites ence  between  the  average  human  score  and  the  score of 
[Clarke et  al. ,  1998].  The  WMTS  is  a  distributed  (multi- LRA is not statistically significant. 
processor)  search  engine,  designed  primarily  for  passage With  374  questions  and  6  word  pairs  per  question  (one 
retrieval (although document retrieval is possible, as a spe- stem and five choices), there are 2,244 pairs in the input set. 
cial  case  of  passage  retrieval).  Our  local  copy  runs  on  a In step 2, introducing alternate pairs multiplies the number 
16-CPU Beowulf Cluster.                               of pairs by four, resulting in 8,976 pairs. In step 5, for each 
  The WMTS is well suited to LRA, because it scales well pair  A:B,  we add B:A,  yielding 17,952 pairs. However, some 
to  large  corpora  (one  terabyte,  in  our  case),  it  gives  exact pairs  are  dropped  because  they  correspond  to  zero  vectors 
frequency  counts  (unlike  most  web  search  engines), it  is (they do not appear together in a window of five words in 
designed  for  passage  retrieval  (rather  than  document  re- the  WMTS  corpus).  Also,  a  few  words  do  not  appear  in 
trieval), and it has a powerful query syntax.         Lin’s  thesaurus,  and  some  word  pairs  appear  twice  in  the 
  As a source of synonyms, we use Lin’s [1998] automati- SAT questions (e.g., lion:cat). The sparse matrix (step 7) has 
cally generated thesaurus. Lin’s thesaurus was generated by 17,232  rows  (word  pairs)  and  8,000  columns  (patterns), 
parsing a corpus of about 5 × 10 7 English words, consisting with a density of 5.8% (percentage of nonzero values). 
of text from the Wall Street Journal, San Jose Mercury, and Table  2  compares  LRA  to  VSM  with  the  374  analogy 
AP  Newswire  [Lin,  1998].  The  parser  was  used  to  extract questions.  VSM-AV  refers  to  the  VSM  using  AltaVista’s 
pairs of words and their grammatical relations. Words were database as a corpus. The VSM-AV results are taken from 
then clustered into synonym sets, based on the similarity of Turney  and  Littman  [2005].  We  estimate  the  AltaVista 
their  grammatical  relations.  Two  words  were  judged to  be search index contained about 5 × 10 11  English words at the 
highly similar when they tended to have the same kinds of time  the  VSM-AV  experiments  took  place.  Turney  and 
grammatical relations with the same sets of words.    Littman [2005] gave an estimate of 1 × 10 11  English words, 
  Given a word and its part of speech, Lin’s thesaurus pro- but  we  believe  this  estimate  was  slightly  conservative. 
vides a list of words, sorted in order of decreasing attribu- VSM-WMTS  refers  to  the  VSM  using  the  WMTS,  which 
tional similarity. This sorting is convenient for LRA, since it contains  about  5  ×  10 10   English  words.  We  generated  the 
makes it possible to focus on words with higher attributional VSM-WMTS results by adapting the VSM to the WMTS.  
similarity and ignore the rest.  
  We use Rohde’s SVDLIBC implementation of the Singu-    Table 2. LRA versus VSM with 374 SAT analogy questions. 
lar  Value  Decomposition,  which  is  based  on  SVDPACKC            VSM-AV  VSM-WMTS       LRA 
[Berry, 1992].                                               Correct     176        144       210 
                                                             Incorrect   193        196       160 
5  Experiments with Word Analogy Questions                   Skipped      5         34         4 
Table 1 shows one of the 374 SAT analogy questions, along    Total       374        374       374 
with  the  relational  similarities  between  the  stem  and  each Score  47.3%    40.3%     56.4% 
choice, as calculated by LRA. The choice with the highest All three pairwise differences in the three scores in Table 
relational similarity is also the correct answer for this ques- 2 are statistically significant with 95% confidence, using the 
tion (quart is to volume as mile is to distance).     Fisher Exact Test. Using the same corpus as the VSM, LRA 
Table 1. Relation similarity measures for a sample SAT question. achieves a score of 56% whereas the VSM achieves a score 
                                                      of  40%,  an  absolute  difference  of  16%  and  a  relative  im-
    Stem:        quart:volume  Relational similarity  provement of 40%. When VSM has a corpus ten times lar-
    Choices:  (a)  day:night       0.373725           ger than LRA’s corpus, LRA is still ahead, with an absolute 
             (b)  mile:distance    0.677258           difference of 9% and a relative improvement of 19%. 
             (c)  decade:century   0.388504             Comparing  VSM-AV  to  VSM-WMTS,  the  smaller  cor-
             (d)  friction:heat    0.427860           pus has reduced the score of the VSM, but much of the drop 
             (e)  part:whole       0.370172           is due to the larger number of questions that were skipped 
                                                      (34  for  VSM-WMTS  versus  5  for  VSM-AV).  With  the 
  LRA  correctly  answered  210  of  the  374  analogy  ques- smaller corpus,  many  more of the input  word pairs simply 
tions  and  incorrectly  answered  160  questions.  Four ques- do not appear together in short phrases in the corpus. LRA 
tions were skipped, because the stem pair and its alternates is able to answer as many questions as VSM-AV, although 
did not appear together in any phrases in the corpus, so all it  uses  the  same  corpus  as  VSM-WMTS,  because  Lin’s 
choices  had  a  relational  similarity  of  zero.  Since there  are [1998]  thesaurus  allows  LRA  to  substitute  synonyms for 
five choices for each question, we would expect to answer words that are not in the corpus. 
20% of the questions correctly by random guessing. There- VSM-AV  required  17  days  to  process  the  374  analogy 
fore we score the performance by giving one point for each questions [Turney and Littman, 2005], compared to 9 days 
correct  answer  and  0.2  points  for  each  skipped  question. for  LRA.  As  a  courtesy  to  AltaVista,  Turney  and  Littman 
LRA attained a score of 56.4% on the 374 SAT questions.  [2005]  inserted  a  five  second  delay  between  each  query. 
  The  average  performance  of  college-bound  senior  high Since  the  WMTS  is  running  locally,  there  is  no  need  for 
school  students  on  verbal  SAT  questions  corresponds  to  a delays. VSM-WMTS processed the questions in one day. 
score of about 57% [Turney and Littman, 2005]. The differ-6  Experiments with Noun-Modifier Relations           the two VSM accuracies and F measures are not significant. 
                                                      Using the same corpus as the VSM, LRA’s accuracy is 14% 
This section describes experiments with 600 noun-modifier higher in absolute terms and 32% higher in relative terms. 
pairs,  hand-labeled  with  30  classes  of  semantic  relations 
[Nastase and Szpakowicz, 2003]. We experiment with both Table 3. Comparison of LRA and VSM on the 30-class problem. 
a 30-class problem and a 5-class problem. The 30 classes of            VSM-AV  VSM-WMTS      LRA 
semantic  relations  include cause   (e.g.,  in  “flu  virus”,  the 
                                                             Correct     167        148       239 
head noun “virus” is the cause  of the modifier “flu”), loca-
                                                             Incorrect   433        452       361 
tion  (e.g., in “home town”, the head noun “town” is the lo-
                                                             Total       600        600       600 
cation  of the modifier “home”), part  (e.g., in “printer tray”, 
                                                             Accuracy   27.8%      24.7%     39.8% 
the head noun “tray” is part  of the modifier “printer”), and 
                                                             Precision  27.9%      24.0%     41.0% 
topic   (e.g.,  in  “weather  report”,  the  head  noun  “report”  is 
                                                             Recall     26.8%      20.9%     35.9% 
about the topic  “weather”). For a full list of classes, see Nas-
                                                             F          26.5%      20.3%     36.6% 
tase and Szpakowicz [2003] or Turney and Littman [2005]. 
The  30  classes  belong  to  5  general  groups  of  relations, 
causal   relations, temporal   relations, spatial   relations, par- Table 4. Comparison of LRA and VSM on the 5-class problem. 
ticipatory  relations (e.g., in “student protest”, the “student”       VSM-AV  VSM-WMTS      LRA 
is the agent  who performs the “protest”; agent  is a partici- Correct   274        264       348 
patory  relation), and qualitative  relations (e.g., in “oak tree”, Incorrect  326  336       252 
“oak” is a type  of “tree”; type  is a qualitative  relation). Total     600        600       600 
  The  following  experiments  use  single  nearest  neighbour Accuracy  45.7%     44.0%     58.0% 
classification  with  leave-one-out  cross-validation.   For Precision  43.4%      40.2%     55.9% 
leave-one-out  cross-validation,  the  testing  set  consists  of  a Recall  43.1%  41.4%    53.6% 
single noun-modifier pair and the training set consists of the F        43.2%      40.6%     54.6% 
599  remaining  noun-modifiers.  The  data  set  is  split  600 
times, so that each noun-modifier gets a turn as the testing 
word pair. The predicted class of the testing pair is the class 7  Discussion 
of  the  single  nearest  neighbour  in  the  training  set.  As  the The  experimental  results  in  Sections  5  and  6  demonstrate 
measure of nearness, we use LRA to calculate the relational that LRA performs significantly better than the VSM, but it 
similarity between the testing pair and the training pairs. is also clear that there is room for improvement. The accu-
  Following  Turney  and  Littman  [2005],  we  evaluate  the racy  might  not  yet  be  adequate  for  practical  applications, 
performance by accuracy and also by the macroaveraged F although past work has shown that it is possible to adjust the 
measure  [Lewis,  1991].  The  F  measure  is  the  harmonic tradeoff  of  precision  versus  recall  [Turney  and  Littman, 
mean of precision and recall. Macroaveraging calculates the 2005].  For  some  of  the  applications,  such  as  information 
precision,  recall,  and  F  for  each  class  separately,  and  then extraction, LRA might be suitable if it is adjusted for high 
calculates the average across all classes.            precision, at the expense of low recall.  
  There are 600 word pairs in the input set for LRA. In step Another limitation is speed; it took almost nine days for 
2, introducing alternate pairs multiplies the number of pairs LRA to answer 374 analogy questions. However, with pro-
by four, resulting in 2,400 pairs. In step 5, for each pair  A:B,  gress  in  computer  hardware,  speed  will  gradually  become 
we  add B:A,   yielding  4,800 pairs.  Some  pairs  are  dropped less of a concern. Also, the software has not been optimized 
because they correspond to zero vectors and a few words do for  speed;  there  are  several  places  where  the  efficiency 
not appear in Lin’s thesaurus. The sparse matrix (step 7) has could be increased and many operations are parallelizable. It 
4,748 rows and 8,000 columns, with a density of 8.4%. may also be possible to precompute much of the information 
  Table 3 shows the performance of LRA and VSM on the for LRA, although this would require substantial changes to 
30-class problem. VSM-AV is VSM with the AltaVista cor- the algorithm. 
pus and VSM-WMTS is VSM with the WMTS corpus. The       The  difference  in  performance  between  VSM-AV  and 
results  for  VSM-AV  are  taken  from  Turney  and  Littman VSM-WMTS shows that VSM is sensitive to the size of the 
[2005]. All three pairwise differences in the three F  meas- corpus.  Although  LRA  is  able  to  surpass  VSM-AV  when 
ures are statistically significant at the 95% level, according the WMTS corpus is only about one tenth the size of the AV 
to the Paired T-Test. The accuracy of LRA is significantly corpus, it seems likely that LRA would perform better with 
higher  than  the  accuracies  of  VSM-AV  and  VSM-WMTS, a larger corpus. The WMTS corpus requires one terabyte of 
according  to  the  Fisher  Exact  Test,  but  the  difference  be- hard disk space, but progress in hardware will likely make 
tween the two VSM accuracies is not significant. Using the ten  or  even  one  hundred  terabytes  affordable  in  the  rela-
same corpus as the VSM, LRA’s accuracy is 15% higher in tively near future. 
absolute terms and 61% higher in relative terms.        For  noun-modifier  classification,  more  labeled  data 
  Table 4 compares the performance of LRA and VSM on  should  yield  performance  improvements.  With  600  noun-
the  5-class  problem.  The  accuracy  and  F  measure  of LRA modifier pairs and 30 classes, the average class has only 20 
are significantly higher than the accuracies and F measures examples. We expect that the accuracy would improve sub-
of VSM-AV and VSM-WMTS, but the differences between 