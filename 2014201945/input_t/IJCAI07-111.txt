            Unsupervised Discretization Using Kernel Density Estimation

    Marenglen Biba, Floriana Esposito, Stefano Ferilli, Nicola Di Mauro, Teresa M.A Basile 
                       Department of Computer Science, University of Bari 
                               Via Orabona 4, 70125 Bari, Italy 
                          {biba,esposito,ferilli,ndm,basile}@di.uniba.it 


                  Abstract                      vantages of discrete values over continuous ones, the point 
                                                is that many learning algorithms can only handle discrete 
   Discretization, defined as a set of cuts over do-
                                                attributes, thus good discretization methods are a key issue 
   mains of attributes, represents an important pre- for them since they can significantly affect the learning out-
   processing task for numeric data analysis. Some come. There are different axes by which discretization 
   Machine Learning algorithms require a discrete 
                                                methods can be classified, according to the different direc-
   feature space but in real-world applications con- tions followed by the implementation of discretization tech-
   tinuous attributes must be handled. To deal with niques due to different needs: global vs. local, splitting (top-
   this problem many supervised discretization meth-
                                                down) vs. merging (bottom-up), direct vs. incremental and 
   ods have been proposed but little has been done to supervised vs. unsupervised. 
   synthesize unsupervised discretization methods to Local methods, as exemplified by C4.5, discretize in a lo-
   be used in domains where no class information is 
                                                calized region of the instance space. (i.e. a subset of in-
   available. Furthermore, existing methods such as stances). On the other side, global methods use the entire 
   (equal-width or equal-frequency) binning, are not instance space [Chmielevski and Grzymala-Busse, 1994].  
   well-principled, raising therefore the need for 
                                                  Splitting methods start with an empty list of cutpoints 
   more sophisticated methods for the unsupervised and, while splitting the intervals in a top-down fashion, pro-
   discretization of continuous features. This paper duce progressively the cut-points that make up the discreti-
   presents a novel unsupervised discretization 
                                                zation. On the contrary, merging methods start with all the 
   method that uses non-parametric density es-  possible cutpoints and, at each step of the discretization re-
   timators to automatically adapt sub-interval di- finement, eliminate cut-points by merging intervals. 
   mensions to the data. The proposed algorithm 
                                                  Direct methods divide the initial interval in n sub-
   searches for the next two sub-intervals to produce, intervals simultaneously (i.e., equal-width and equal-
   evaluating the best cut-point on the basis of the frequency), thus they need as a further input from the user 
   density induced in the sub-intervals by the current 
                                                the number of intervals to produce. Incremental methods 
   cut and the density given by a kernel density esti- [Cerquides and Mantaras, 1997] start with a simple discreti-
   mator for each sub-interval. It uses cross-validated zation step and progressively improve the discretization, 
   log-likelihood to select the maximal number of in-
                                                hence needing an additional criterion to stop the process. 
   tervals. The new proposed method is compared to Supervised discretization considers class information 
   equal-width and equal-frequency discretization while unsupervised discretization does not. Equal-width and 
   methods through experiments on well known ben-
                                                equal-frequency binning are simple techniques that perform 
   chmarking data.                              unsupervised discretization without exploiting any class 
                                                information. In these methods, continuous intervals are split 
1  Introduction                                 into sub-intervals and it is up to the user specifying the 
Data format is an important issue in Machine Learning width (range of values to include in a sub-interval) or fre-
(ML) because different types of data make relevant differ- quency (number of instances in each sub-interval). These 
ence in learning tasks. While there can be infinitely many simple methods may not lead to good results when the con-
values for a continuous attribute, the number of discrete tinuous values are not compliant with the uniform distribu-
values is often small or finite. When learning, e.g., classifi- tion. Additionally, since outliers are not handled, they can 
cation trees/rules, the data type has an important impact on produce results with low accuracy in the presence of skew 
the decision tree induction. As reported in [Dougherty et data. Usually, to deal with these problems, class information 
al.,1995], discretization makes learning more accurate and has been used in supervised methods, but when no such 
faster. In general, the decision trees and rules learned using information is available the only option is exploiting unsu-
discrete features are more compact and more accurate than pervised methods. While there exist many supervised meth-
those induced using continuous ones. In addition to the ad- ods in literature, not much work has been done for synthe-


                                           IJCAI-07
                                             696 sizing unsupervised methods. This could be due to the fact value in x, every other point in the same bin, contributes 
 that discretization has been commonly associated with the equally to the density in x, no matter how close or far away 
 classification task. Therefore, work on supervised methods from x these points are. 
 is strongly motivated in those learning tasks where no class 
 information is available. In particular, in many domains, 
 learning algorithms deal only with discrete values. Among 
 these learning settings, in many cases no class information 
 can be exploited and unsupervised discretization methods 
 such as simple binning are used.  
  The work presented in this paper proposes a top-down, 
 global, direct and unsupervised method for discretization. It 
 exploits density estimation methods to select the cut-points 
 during the discretization process. The number of cutpoints is 
 computed by cross-validating the log-likelihood. We con-
sider as candidate cutpoints those that fall between two in-
stances of the attribute to be discretized. The space of all the 
possible cut-points to evaluate could grow for large datasets 
that have continuous attributes with many instances with 
different values among them. For this reason we developed 
and implemented an efficient algorithm of complexity  Figure 1. Simple binning places a block in every 
                                                      sub-interval for every instance x that falls in it 
Nlog(N) where N is number of instances. 
  The paper is organized as follows. In Section 2 we de-
 scribe non-parametric density estimators, a special case of This is rather restricting because it does not give a real mir-
                                                 ror of the data. In principle, points closer to x should be 
 which is the kernel density estimator. In Section 3 we pre-
 sent the discretization algorithm, while in Section 4 we re- weighted more than other points that are far from it. The 
 port experiments carried out on classical datasets of the UCI first step in doing this is eliminating the dependence on bin 
                                                 origins fixed a-priori and place the bin origins centered at 
 repository. Section 5 concludes the paper and outlines future 
 work.                                           every point x. Thus the following pseudo-formula: 
                                                        1
                                                             # {instances  that fall in a bin containing  x}
 2  Non-parametric density estimation               n  binwidth
 Since data may be available under various distributions, it is 
 not always straightforward to construct density functions 
 from some given data. In parametric density estimation, an should be transformed in the following one:  
 important assumption is made: available data has a density 
                                                         1
 function that belongs to a known family of distributions,    #{instances  that fall in a bin around  x}
 such as the normal distribution or the Gaussian one, having n binwidth
 their own parameters for mean and variance. What a para-
 metric method does is finding the values of these parameters The subtle but important difference in constructing binning 
 that best fit the data. However, data may be complex and density with the second formula, permits to place the bin 
 assumptions about the distributions that are forced upon the around x and the calculation of the density is performed not 
 data may lead to models that do not fit well the data. In the- in a bin containing x and depending from the origin C, but 
 se cases, where making assumptions is difficult, non- in a bin whose center is upon x. The bin center on x, allows 
 parametric density functions are preferred.     successively to assign different weights to the other points 
  Simple binning (histograms) is one of the most well- in the same bin in terms of impact upon the density in x de-
 known non-parametric density methods. It consists in as- pending on the distance from x. If we consider intervals of 
 signing the same value of the density function f to every width h centered on x, then the density function in x is given 
 instance that falls in the interval [C – h/2, C + h/2), where C by the formula: 
 is the origin of the bin and h is the binwidth. The value of 
 such a function is defined as follows (symbol # stands for 1
 ‘number of’):                                      f =      #{instances that fallin [x h, x h]}
                                                         2hn
         1                      h    h
  f =      #{instances  that fall in C ,C }      In this case, when constructing the density function, a box 
        n h                     2    2           of width h is placed for every point that falls in the interval 
                                                 centered in x. These boxes (the dashed ones in Figure 2) are 
 Once fixed the origin C of a bin, for every instance that falls then added up, yielding the density function of Figure 2.  
 in the interval centered in C and of width h, a block of size 1 This provides a way for giving a more accurate view of 
 by the bin width is placed over the interval (Figure 1). Here, what the density of the data is, called box kernel density 
 it is important to note that, if one wants to get the density 


                                            IJCAI-07
                                              697 estimate. However, the weights of the points that fall in the represent instances of the data, we decided to cut in the 
 same bin as x have not been changed yet.        middle points between instance values. The advantage is 
                                                 that this cutting strategy avoids the need of deciding 
                                                 whether the point at which the cut is performed is to be in-
                                                 cluded in the left or in the right sub-interval. 
                                                   The second question is which (sub-)interval should be 
                                                 cut/split next among those produced at a given step of the 
                                                 discretization process. Such a choice must be driven by the 
                                                 objective of capturing the significant changes of density in 
                                                 different separated bins. Our proposal is to evaluate all the 
                                                 possible cut-points in all the sub-intervals, by assigning to 
                                                 each of them a score according to a method whose meaning 
                                                 is as follows. Given a single interval to split, any of its cut-
                                                 points produces two bins and thus induces upon the initial 
                                                 interval two densities, computed using the simple binning 
                                                 density estimation formula. Such a formula, as shown in the 
                                                 previous section, assigns the same density value of the func-
                                                 tion f to every instance in the bin and ignores the distance 
                                                 from x of the other instances of the bin when computing the 
                                                 density in x. Every sub-interval produced has an averaged 
                                                 binned density (the binned density in each point) that is dif-
                                                 ferent from the density estimated with the kernel function. 
                                                 The less this difference is, the more the sub-interval fits the 
  Figure 2. Placing a box for every instance in the interval data well, i.e. the better this binning is, and hence there is no 
  around x and adding them up.                   reason to split it. On the contrary, the idea underlying our 
                                                 discretization algorithm is that, when splitting, one must 
 In order to do this, the kernel density function is introduced: search for the next two worst sub-intervals to produce, 
                                                 where “worst” means that the density shown by each of the 
                     n                           sub-intervals is much different than it would be if the dis-
                  1        x X i
                    p = K                        tances among points in the intervals and a weighting func-
                  nh i 1    h                    tion were considered. The identified worst sub-intervals are 
                                                 just those to be split to produce other intervals, because they 
where K is a weighting function. What this function does is do not fit the data well. In this way intervals whose density 
providing a smart way of estimating the density in x, by differs much from the real data situation are eliminated, and 
counting the frequency of other points Xi in the same bin as replaced by other sub-intervals. In order to achieve the den-
x and weighting them differently depending on their dis- sity computed by the kernel density function we should re-
tance from x. Contributions to the density value of f in x produce a splitting of the main interval such as that in Fig-
 from points Xi vary, since those that are closer to x are ure 2.  
weighted more than points that are further away. This prop- An obvious question that arises is: when a given sub-
erty is fulfilled by many functions, that are called kernel interval is not to be cut anymore? Indeed, searching for the 
functions. A kernel function K is usually a probability den- worst sub-intervals, there are always good candidates to be 
sity functions that integrates to 1 and takes positive values split. This is true, but on the other hand at each step of the 
in its domain. What is important for the density estimation algorithms we can split only one sub-intervals in other two. 
does not reside in the kernel function itself (Gaussian, Thus if there are more than one sub-interval (this is the case 
Epanechnikov or quadratic could be used) but in the band- after the first split) to be split, the scoring function of the 
width selection [Silverman 1986]. We will motivate our cut-points allows to choose the sub-interval to split. 
choice for the bandwidth (the value h in the case of kernel 
functions) selection problem in the next section where we 3.1 The scoring function for the cutpoints 
introduce the problem of cutting intervals based on the den- At each step of the discretization process, we must choose 
sity induced by the cut and the density given by the above  from different sub-intervals to split. In every sub-interval we 
kernel density estimation.                       identify as candidate cut-points all the middle points be-
                                                 tween the instances. For each of the candidate cut-points T
 3  Where and what to cut                        we compute a score as follows: 
 The aim of discretization is always to produce sub-intervals 
                                                            k                 n
 whose induced density over the instances best fits the avail- (p(x ) f (x ) )  (p(x ) f (x ))
 able data. The first problem to be solved is where to cut. Score(T)  =  i i        i     i
                                                           i 1               i k 1
 While most supervised top-down discretization method cut 
 exactly at the points in the main interval to discretize that 


                                            IJCAI-07
                                              698 where i= 1,..,k refers to the instances that fall into the left Discretize(Interval) 
sub-interval and i= k +1,..,n to the instances that fall into the Begin 
right bin. The density functions p and f are respectively the PotentialCutpoints = ComputeCutPoints(Interval); 
kernel density function and the simple binning density func-  PriorityQueueIntervals.Add(Interval); 
tion. These functions are computed as follows:     While stopping criteria is not met do 
                                                   If PriorityQueueCPs is empty  
                          m                             Foreach cutpoint CP in PotentialCutpoints do 
           f(xi)      =                                 scoreCP = ComputeScoringFunction(CP,Interval); 
                         w  N                          PriorityQueueCPs.Add(CP,scoreCP);     
                                                        End for 
                                                   Else  
where m is the number of instances that fall in the (left or         BestCP = PriorityQueue.GetBest(); 
right) bin, w is the binwidth and N is the number of in-         CurrentInterval = PriorityQueueIntervals.GetBest(); 
 stances in the interval that is being split. The kernel density      NewIntervals = Split(CurrentInterval,BestCP); 
 estimator is given by the formula:                LeftInterval = NewIntervals.GetLeftInterval(); 
                                                   RightInterval = NewIntervals.GetRightInterval();   
                       N
                    1       xi  X j                  PotentialLeftCPs  = ComputeCutPoints(LeftInterval); 
            p(xi) =      K                           PotentialRightCPs =ComputeCutPoints(RightInterval); 
                   hN  j 1     h
                                                       Foreach cutpoint CP in PotentialLeftCPs 
                                                     scoreCP = ComputeScoringFunction(CP,LeftInterval); 
 where h is the bandwidth and K is a kernel function. In this PriorityQueueCPs.Add(CP,scoreCP); 
 framework for discretization, it still remains to be clarified PriorityQueueIntervals.Add(LeftInterval,scoreCP); 
 how the bandwidth of the kernel density estimator is chosen.   End For  
 Although there are several ways to do it, as reported in        // the same foreach cycle for PotentialRightCPs  
 [Silverman 1986], in fact in this context we are not inter- End while   
 ested in the density computed by a classic kernel density End
 estimator that considers globally the entire set of available 
 instances. The classic way a kernel density estimation works Figure 3. The discretization algorithm in pseudo language 
 considers N as the total number of instances in the initial 
 interval and chooses h as the smoothing parameter. The       10 15 20 25 30
 choice of h is not easy and various techniques have been 
 investigated to find an optimal h. Our proposal, in this con-

 text, is to adapt the classic kernel density estimator by tak- 10 15 17,5 17,5 20 25 30
 ing h equal to the binwidth w, specified as follows. Indeed, 
 as can be seen from the formula of p(xi), instances that are Figure 4. The first cut 
more distant than h from xi, contribute with weight equal to The candidate cut-points are placed in the middle of adja-
zero to the density of xi. Hence, if a sub-interval (bin) under cent instances: 12.5, 17.5, 22.5, 27.5; the sub-intervals pro-
 consideration has binwidth h, only the instances that fall in duced by cut-point 12.5 are [10 , 12.5] and [12.5 , 30], and 
 it will contribute, depending on their distance from xi, to the similarly for all the other cut-points. Now, suppose that, 
 density in xi. As we are interested in knowing how the cur- computing the scoring function for each cut-point, the great-
 rent binned density (induced by the candidate cut-point and est value (indicating the cut-point that produces the next two 
 computed by f with binwidth w) differs from the density in worst sub-intervals) is reached by the cut-point 17.5. Then 
 the same bin but computed weighting the contributions of Xj the sub-intervals are: [10 , 17.5] and [17.5 , 30] and the list 
 to the density in xi on the basis of the distance xi – Xj, it is of candidate cut-points becomes <12.5, 16.25, 18.75, 22.5, 
useless to consider, for the function p, a bandwidth greater 27.5>. Suppose the scoring function evaluates as follows: 
than w.                                          Score(12.5) = 40, Score(16.25) = 22, Score(18.75) = 11, 
                                                 Score(22.5) = 51, Score(27.5) = 28. The algorithm selects 
 3.2 The discretization algorithm                22.5 as the best cut-point and splits the corresponding inter-
 Once a scoring function has been synthesized, we explain val as shown in Figure 5. 
 how the discretization algorithm works. Figure 3 shows the 
 algorithm in pseudo language. It starts with an empty list of 10 15 20 25 30
 cut-points (that can be implemented as a priority queue in 
 order to maintain, at each step, the cut-points ordered after 
 their value according to the scoring function) and another 10 15 17,5 17,5 20 25 30
 priority queue that contains the sub-intervals generated thus 
 far. Let us see it through an example. Suppose the initial 
                                                                17,5   22,5      22,5 25 30
 interval to be discretized is the one in Figure 4 (frequencies     20
 of the instances are not shown).                           Figure 5. The second cut 


                                            IJCAI-07
                                              699This second cut produces two new sub-intervals and hence where nj-train is the number of training instances in bin j, nj-test
the current discretization is made up of three sub-intervals: is the number of test instances that fall in bin j, N is the total 
[10 , 17.5], [17.5 , 22.5], [22.5 , 30], with candidate cut- number of instances and w is the width of bin j.
points <12.5, 18.75, 23.75, 27,75>. Suppose values of the As regards the kernel density estimator complexity, from 
scoring function are as follows: Score(12.5) = 40, the formula of p, it can be deduced that the complexity for 
Score(18.75) = 20, Score(23.75) = 35, Score(27,5) = 48. evaluating the kernel density in N points is N2. For univari-
The best cut-point 27.5 suggests the third cut and the discre- ate data, the complexity problem has been solved by the 
tization becomes [10 , 17.5], [17.5 , 22.5], [22.5 , 27.5], algorithms proposed in [GreenGard and Strain, 1991] and  
[27.5 , 30]. Thus, the algorithm refines those sub-intervals [Yang et al 2003] which compute the kernel density esti-
that show worst fit to the data. A note is worth: in some ca- mate in O(N+N) instead of O(N2). In our context we deal 
ses it might happen that a split is performed even if one of only with univariate data because only single continuous 
the two sub-intervals (which could be the left or the right attributes have to processed, and thus for N instances, the 
one) it produces shows such a good fit, compared to the theoretical complexity of the algorithm is O(NlogN).
other sub-intervals, that it is not split in the future. This is 
not strange, since the scoring function evaluates the overall 4   Experiments 
fit of the two sub-intervals. This is the case of the first cut in 
the present example: the cut-point 17.5 has been chosen, In order to assess the validity and performance of the pro-
where the left sub-interval [10 , 17.5] shows good fit to the posed discretization algorithm, we have performed  experi-
data in terms of density while the right one [17.5 , 30] ments on several datasets taken from the UCI repository and 
shows bad fit. In this case the interval [10 , 17.5] will not be classically used in the literature to evaluate discretization 
cut before the interval [17.5 , 30] and perhaps will remain algorithms in the past. Specifically, the dataset used are: 
untouched till the end of the discretization algorithm. The autos, bupa, wine, ionosphere, ecoli, sonar, glass, heart, 
algorithm will stop cutting when the stopping criterion (the hepatitis, arrhythmia, anneal, cylinder, and auto-mpg. These 
maximal number of cut-points, computed by a procedure datasets contain a large set of numeric attributes of various 
explained in the next paragraph) is met.        types, from which 200 continuous attributes were extracted 
                                                at random and used to test the discretization algorithm. 
3.3 Stopping criteria and complexity              In order to evaluate the discretization carried out by the 
                                                proposed algorithm with respect to other algorithms in the 
The definition of a stopping criterion is fundamental, to pre-
                                                literature, we compared it to three other methods: equal-
vent the algorithm from continuing to cut until each bin con- width with fixed number of bins (we use 10 for the experi-
tains a single instance. Even without reaching such an ex- ments), equal-frequency with fixed number of bins (we use 
treme situation, the risk of running into overfitting the 
                                                10 for the experiments), equal-width cross-validated for the 
model is real, because, as usual in the literature, we use log- number of bins. The comparison was made along the log-
likelihood to evaluate the density estimators, the simple likelihood on the test data using a 10-fold cross-validation 
binning and the kernel density estimate. As a solution, in-
                                                methodology. The results on the test folds were compared 
stead of requiring a specific number of intervals (that could through a paired t-test as regards cross-validated log-
be too rigid and not based on valid assumptions), we pro- likelihood. Table 1 presents the results of the t-test based on 
pose the use of cross-validation to provide an unbiased es-
                                                cross-validated log-likelihood with a risk level  = 0.05. It 
timation of how the model fits the real distribution. For the shows the number of continuous attributes whose discretiza-
experiments performed the 10-fold cross-validation was tion through our method was significantly better, equal or 
used. For each fold the algorithm computes the stopping 
                                                significantly worst compared to the other methods. 
criterion as follows: Supposing there are N – 1 candidate 
cut-points, for each of them the cross-validated log-
likelihood is computed. In order to optimize performance, at 
                                                  Our method                    Our method 
each step a structure maintains the sub-intervals in the cur-
                                                            significanlty Equal significanlty 
rent discretization and the corresponding splitting values, so 
                                                            more accurate       less accurate 
that only the new values for the interval to be split have to 
                                                 EqualWidth 
be computed at each step. Thus the algorithm that computes 
                                                 10 bins      71 (35,5%)   126    3  (1,5%) 
the log-likelihood for the N – 1 cut-points is performed 10 
times overall. The number of cut-points that shows the EqualFreq 
maximum value of the averaged log-likelihood on the test 10 bins  79 (39,5%)  119  2  (1,0%) 
folds is chosen as the best. The log-likelihood on the test EqualWidth 
data is given by the following formula:          Cross-       54 (27,0%)   136    10  (5,0%) 
                                                 Validated
                     n         n
                                j train           Table 1. Results of paired t-test based on cross-validated 
    Log-likelihood  =  n j test log
                    j 1        w  N               log-likelihood on 10 folds. 

                                                  It is clear that, even if in the majority of cases the new al-
                                                gorithm shows no difference in performance with respect to 


                                           IJCAI-07
                                             700