                Approximate Policy Iteration using Large-Margin Classifiers 

                                   Michail G. Lagoudakis and Ronald Parr 
                                                   Duke University 
                                                 Durham, NC 27708 
                                          {mgl,parr}@cs.duke.edu 


                        Abstract                              A is a finite set of actions; P is a Markovian transition model, 
                                                               where is the probability of making a transition to 
     We present an approximate policy iteration algo•
                                                              state sf when taking action a in state s; R is a reward (or cost) 
     rithm that uses rollouts to estimate the value of each   function, such that R(s, a) is the expected reward for taking 
     action under a given policy in a subset of states and    action a in state s: [0,1) is the discount factor for future 
     a classifier to generalize and learn the improved        rewards; and, D is the initial state distribution from which 
     policy over the entire state space. Using a multi-       states are drawn when the process is initialized. 
     class support vector machine as the classifier, we          In reinforcement learning, it is assumed that the learner can 
     obtained successful results on the inverted pendu•       observe the state of the process and the immediate reward at 
     lum and the bicycle balancing and riding domains.        every step, however P and R are completely unknown. In 
                                                              this paper, we also make the assumption that our learning al•
1 Introduction                                                gorithm has access to a generative model of the process which 
                                                              is a black box that takes a state s and an action a as inputs and 
Reinforcement learning provides an intuitively appealing      outputs a next state s' drawn from P and a reward r. Note that 
framework for addressing a wide variety of planning and con•  this is not the same as having the model (P and R) itself. 
trol problems. There has been significant success in tackling    A policy for an MDP is a mapping : S A from 
large-scale problems through the use of value function and/or states to actions, where is the action the agent takes at 
policy approximation. The success of these methods, how•      state s. The value of a state s under a policy n is the 
ever, is contingent upon extensive and careful feature engi•  expected, total, discounted reward when the process begins 
neering, a common problem in most machine learning meth•      in state s and all decisions at all steps are made according to 
ods.                                                          policy 7r: 
  Modern classification methods have mitigated the feature 
engineering problems through the use of kernel methods, but 
very little has been done to exploit these recent advances for 
the purposes of reinforcement learning. Of course, we are not 
                                                              The goal of the decision maker is to compute or learn an op•
the first to note the potential benefits of modern classifica•
                                                              timal policy that maximizes the expected total discounted 
tion methods to reinforcement learning. For example, Yoon et 
                                                              reward from the initial state distribution: 
al. [2002] use inductive learning techniques, including boost•
ing, to generalize across similar problems. Dietterich and 
Wang [2001] also use a kernel-based approximation method 
                                                              It is known that for every MDP, there exists at least one opti•
to generalize across similar problems. The novelty in our ap•
                                                              mal deterministic policy. 
proach is the application of modern learning methods within 
a single, noisy problem at the inner loop of a policy iteration 3 (Approximate) Policy Iteration 
algorithm.1 By using rollouts, we avoid the sometimes prob•
lematic step of value function approximation. Thus, our tech• Policy iteration (PI) is a method of discovering such a policy 
nique aims to address the critiques of value function methods by iterating through a sequence of monotonically improving 
raised by the proponents of direct policy search, while avoid• policies. Improvement at each iteration i is typically achieved 
ing the confines of a parameterized policy space.             by computing or learning the state-action value function 
                                                              of the current policy 7rt, defined as 
2 Definitions and Assumptions 
A Markov decision process (MDP) is defined as a 6-tuple 
(5, A, P, R, r, D) where: S is the state space of the process; and then the improved policy as 

   !Fcrn, Yoon, and Givan are also pursuing a similar approach. 


1432                                                                                                  POSTER PAPERS In practice, policy iteration terminates in a surprisingly small 
number of steps, however, it relies on exact representation of 
value functions and policies. 
   Approximate methods are frequently used when the state 
space of the underlying MDP is extremely large and exact 
(solution or learning) methods fail. A general framework of 
using approximation within policy iteration is known as ap•
proximate policy iteration (API). In its most general form, 
API assumes approximate representations of value functions 
(Q) and policies As a result, API cannot guarantee mono-
tonic improvement and convergence to the optimal policy. A 
performance bound on the outcome of API can be constructed 
in terms of bounds on the approximation errors [Bert-
sekas and Tsitsiklis, 1996]. In practice, API often finds very         Figure 1: API with rollouts and classification 
good policies in a few iterations, since it normally makes big 
steps in the space of possible policies. This is in contrast to 
policy gradient methods which, despite acceleration methods,  ing a full policy iteration algorithm, and terminating with the 
are often forced to take very small steps.                    optimal policy. However, for large-scale problems, choosing 

                                                              Sf) and dealing with imperfect classifiers poses challenges. 
4 Practical API without Value Functions                          We consider a number of alternative choices for the dis•
The dependence of typical API algorithms on approximate       tribution p. A uniform distribution over the state space is 
value functions places continuous function approximation      a good choice for a low-dimensional state space, but it will 
methods at their inner loop. These methods typically mini•    result in poor coverage in high-dimensional spaces. A bet•
                                                              ter choice would be a distribution that favors certain impor•
mize L2 error, which is a poor match with the bounds 
                                                              tant parts of the state space over others. In classification, it 
for API. This problem is not just theoretical. Efforts to im•
                                                              is widely assumed that the classifier is trained on examples 
prove performance, such as adding new features, can some•
                                                              that are drawn from the same distribution it will be tested on. 
times lead to surprisingly worse performance, making feature 
                                                              Therefore, representative states for learning policy in it•
engineering a somewhat tedious and counterintuitive task. 
                                                              eration i should be drawn from the future state 
   An important observation, also noted by Fern, Yoon and 
                                                              distribution [Jaakkola et al, 19951 of the yet unknown policy 
Givan 120031, is that a Monte-Carlo technique, called roll-
                                                              7T   . Even though this policy is yet unknown, it is possible to 
outs, can be used within API to avoid the problematic            i+1
                                                              closely approximate this distribution . Starting in some state 
value function approximation step entirely. Rollouts estimate                                    2
                                                              .s  drawn from Z), a trajectory under can be simulated 
Q   (s,a) from a generative model by executing action a in      0
  nt                                                          by using rollouts repeatedly in each visited state to determine 
state s and following policy 7r, thereafter, while recording the 
                                                              the actions of If the trajectory terminates with proba•
total discounted reward obtained during the entire trajectory. 
                                                              bility then states visited along such trajectories can 
This simulation is repeated several times and the results are 
                                                              be viewed as samples from the desired distribution. 
averaged over a large number of trajectories to obtain an accu•
rate estimate Rollouts were first used                           The main contribution of this paper is a particular em•
by Tesauro and Galperin [1997] for online improvement of a    bodiment of the algorithm described in the previous section. 
backgammon player. For our purposes, we can choose a rep•     For rollouts, we used basic statistical hypothesis testing (two-
                                                              sample f.-test) to determine statistically significant differences 
resentative set of states Sp (with distribution over the state 
space) and perform enough rollouts to determine which ac•     between the rollout estimates of for different actions. If 
tion maximizes for the current policy. Rather than            action a is determined to be a clear winner for state s, then 
fitting a function approximator to the values obtained by the we treat (s, a) as a positive training example for state s, and 
                                                              all (s, a'), as negative examples for s. Even if there 
rollouts, we instead train a classifier on Spy where each state 
is labelled with the maximizing action for the state. The al• is no clear winner action in some state s, negative examples 
gorithm is summarized in Figure 1.                            can be still extracted for the clearly bad actions in s, allow•
                                                              ing the classifier to choose freely any of the remaining (good) 
   LEARN is a supervised learning algorithm that trains a clas• actions as the winner. The only case where no examples are 
sifier given a set of labeled training data. The termination  generated is when all actions seem to be equally good. 
condition is left somewhat open ended. It can be when the 
                                                                 The most significant contribution of effort is the use of sup•
performance of the current policy does not exceed that of the 
                                                              port vector machines (SVMs) for LEARN. In our implemen•
previous one, when two subsequent policies are similar (the 
                                                              tation we used the SVMTorch package [Collobert and Ben-
notion of similarity will depend upon the learner used), or 
                                                              gio, 2001] as the multi-class classifier. With the kernel trick, 
when a cycle of policies is detected (also learner dependent). 
                                                              SVMs are able to implicitly and automatically consider clas•
If we assume a fortuitous choice of Sp and a sufficiently pow•
                                                              sifiers with very complex feature spaces. 
erful learner that can correctly generalize from Sp to the en•
tire state space, the algorithm at each iteration learns the im•
proved policy over the previous one, effectively implement-       Special thanks to Alan Fern for sharing this observation. 


POSTER PAPERS                                                                                                      1433   Figure 2: Positive(+), negativc(x) examples; support vcctors(o).                 Figure 3: Successful trajectories of the bicycle. 

5 Experimental Results                                                   (1000,0) (right side). The input to the SVM was simply the 
                                                                         six-dimensional state description and the value of C was 1. 
In the inverted pendulum problem the goal is to balance a pen•           Performance was sensitive to the SVM parameters. We are 
dulum by applying forces to the left (LF), or to the right (RF),         currently doing further experimentation with larger numbers 
or no force (NF) at all. All actions are noisy. The state space          of rollout states and different kernel parameters. 
is continuous and consists of the vertical angle and the angu•
                                                                         This research was supported in part by NSF grant 0209088. 
lar velocity of the pendulum. Transitions are governed by the 
nonlinear dynamics of the system and the time step is 0.01               References 
seconds. The reward is -1 when the angle exceeds in 
absolute value (end of the episode) and 0 otherwise. The dis•            [Bcrtsekas and Tsitsiklis, 1996] D. Bertsekas and J. Tsitsiklis. 
count factor is 0.95. Using about 200 states from p to perform              Neuro-Dynamic Programming. Athena Scientific, Belmont, Mas•
rollouts, the algorithm consistently learns balancing policies              sachusetts, 1996. 
in one or two iterations, starting from the random policy. The           [Collobert and Bengio, 2001] Ronan Collobert and Samy Bcngio. 
choice of p or the kernel (polynomial/Gaussian) did not affect              SVMTorch: Support vector machines for large-scale regres•
the results significantly. Figure 2 shows the training data for             sion problems. Journal of Machine Learning Research (JMLR), 
                                                                            1:143 160, 2001. 
the LF action with p being the uniform distribution. The num•
ber of support vectors was normally smaller than the number              [Dietterich and Wang, 2001] T. G. Dietterich and X. Wang. Batch 
of rollout states. The constant C, the trade-off between train•             value funtion approximation via support vectors. In Advances in 
ing error and margin, was set to 1.                                         Neural Information Processing Systems 14, 2001. MIT Press. 
   In the bicycle balancing and riding problem [Randlov and              I Fern et al, 2003] A. Fern, S. Yoon, and R. Givan. Approxi•
Alstrom, 1998] the goal is to learn to balance and ride a bi•               mately policy iteration with a policy language bias: Learning 
cycle to a target position located 1 km away from the starting              control policies in relational planning domains. In Submitted to 
                                                                            The Nineteenth International Conference on Machine Learning 
location. The state description is a six-dimensional vector 
                                                                            (ICML-2003), July 2003. 
                    where is the angle of the handlebar, is 
the vertical angle of the bicycle, and is the angle of the               [Jaakkola et al., 1995] Tommi Jaakkola, Satinder P. Singh, and 
                                                                            Michael I. Jordan. Reinforcement learning algorithm for partially 
bicycle to the goal. The actions are the torque r applied to 
                                                                            observable Markov decision problems. In G. Tesauro, D. Touret-
the handlebar {-2,0, +2} and the displacement of the rider 
                                                                            zky, and T. Leen, editors, Advances in Neural Information Pro•
v {-0.02,0,4-0.02}. Actions are restricted so that either                   cessing Systems 7, Cambridge, Massachusetts, 1995. MIT Press. 
r = 0 or v = 0 giving a total of 5 actions. The noise is 
                                                                         [Ng et al, 1999] Andrew Y. Ng, Daishi Harada, and Stuart Russell. 
a uniformly distributed term in [-0.02, + 0.02] added to the 
                                                                            Policy invariance under reward transformations: theory and ap•
displacement. The dynamics of the bicycle are based on the 
                                                                            plication to reward shaping. In Proc. 16th International Conf 
model of Randlov and Alstrom [1998] and the time step is set                on Machine Learning, pages 278-287. Morgan Kaufmann, San 
to 0.02 seconds. As is typical with this problem, we used a                 Francisco, CA, 1999. 
shaping reward [Ng et al., 1999]. The reward r       given at each 
                                                    t                    [Randl0V and Alstram, 1998] J. Randlav and P. Alstrom. Learn•
time step was rt — where dt is the distance                                 ing to drive a bicycle using reinforcement learning and shaping. 
of the back wheel of the bicycle to the goal position at time t             In The Fifteenth International Conference on Machine Learning, 
and 7 is the discount factor which is set to 0.95.                          1998. Morgan Kaufmann. 
   Using a polynomial kernel of degree 5, we were able to                [Tesauro and Galperin, 1997] G. Tesauro and G. R. Galperin. On•
solve the problem with uniform sampling of about 5000 roll•                 line policy improvement using monte-carlo search. In Advances 
out states. Sampling from the distribution of                               in Neural Information Processing Systems (NIPS) 9, 1997. 
the target policy, the problem is solved with as few as 1000             [Yoon et al, 2002] S. W. Yoon, A. Fern, and B. Givan. Inductive 
rollout states, as shown in Figure 3 which shows ten sam•                   policy selection for first-order MDPs. In Proceedings of the Eigh-
ple trajectories of the bicycle in the two-dimensional plane                teenth Conference on Uncertainty in Artificial Intelligence, 2002. 
from the initial position (0,0) (left side) to the goal position            Morgan Kaufmann. 


1434                                                                                                                   POSTER PAPERS 