                                 One Class per Named Entity:
                Exploiting Unlabeled Text         for Named Entity Recognition
                                 Yingchuan Wong     and  Hwee Tou Ng
                                    Department of Computer Science
                                    National University of Singapore
                                  3 Science Drive 2, Singapore 117543
                  yingchuan.wong@gmail.com, nght@comp.nus.edu.sg

                    Abstract                          wick, 1999]. Each name class N is divided into 4 sub-classes:
                                                      N     , N       , N  ,andN        . Since the CoNLL
    In this paper, we present a simple yet novel method begin  continue  end      unique
                                                      2003 English NER shared task data has 4 name classes (per-
    of exploiting unlabeled text to further improve
                                                      son, organization, location, and miscellaneous), there is a to-
    the accuracy of a high-performance state-of-the-
                                                      tal of 17 classes (4 name classes × 4 sub-classes + 1 not-a-
    art named entity recognition (NER) system. The
                                                      name class) that a word can possibly be assigned to.
    method utilizes the empirical property that many
    named entities occur in one name class only. Us-  2.1  Maximum Entropy
    ing only unlabeled text as the additional resource,
                                                      In the maximum entropy framework, the best model is the one
    our improved NER system achieves an F1 score of
                                                      that has the highest entropy while satisfying the constraints
    87.13%, an improvement of 1.17% in F1 score and
                                                      imposed. These constraints are derived from training data,
    a 8.3% error reduction on the CoNLL 2003 En-
                                                      expressing some relationship between features and class. It
    glish NER ofﬁcial test set. This accuracy places
                                                      is unique, agrees with the maximum-likelihood distribution,
    our NER system among the top 3 systems in the
                                                      and has the exponential form [Pietra et al., 1997]:
    CoNLL 2003 English shared task.
                                                                                 k
                                                                              1        (  )
                                                                    p(c|h)=         αfj h,c ,
1  Introduction                                                             Z(h)     j
                                                                                 j=1
The named entity recognition (NER) task involves identify-
ing and classifying noun phrases into one of many semantic where c refers to the class, h the history (or context), and
classes such as persons, organizations, locations, etc. NER Z(h) is a normalization function. The features used in the
systems can be built by supervised learning from a labeled maximum entropy framework are binary-valued functions
data set. However, labeled data sets are expensive to prepare which pair a class with various elements of the context. An
                                                      example of a feature function is
as it involves manual annotation efforts by humans. As unla-       
beled data is easier and cheaper to obtain than labeled data,        1  if c = person  ,word=JOHN
                                                         f (h, c)=                 begin
exploiting unlabeled data to improve NER performance is an j         0  otherwise
important research goal.
  An empirical property of named entities is that many Generalized Iterative Scaling (GIS) is used to estimate the
named entities occur in one name class only. For example, parameters αj [Darroch and Ratcliff, 1972].
in the CoNLL 2003 English NER [Sang and Meulder, 2003]
training set, more than 98% of named entity types have ex- 2.2 Testing
actly one name class. This paper presents a novel yet sim- It is possible that the classiﬁer produces a sequence of invalid
ple method of exploiting this empirical property of one class classes when assigning classes to the words in a test sentence
per named entity to further improve the NER accuracy of a during testing (e.g., personbegin followed by locationend).
high-performance state-of-the-art NER system. Using only To eliminate such sequences, we deﬁne a transition probabil-
unlabeled data as the additional resource, we achieved an F1 ity between classes, P (ci|cj), to be equal to 1 if the transition
score of 87.13%, which is an improvement of 1.17% in F1 is valid, and 0 otherwise. The probability of assigning the
score and a 8.3% error reduction on the CoNLL 2003 English classes c1,...,cn to the words in a sentence s in a document
NER ofﬁcial test set. This accuracy places our NER system D is deﬁned as follows:
among the top 3 systems in the CoNLL 2003 English shared                     n
task.                                                     P (c1,...,cn|s, D)=   P (ci|s, D) ∗ P (ci|ci−1),
                                                                             i=1
2  System Description
                                                      where P (ci|s, D) is determined by the maximum entropy
The NER system we implemented is based on the maximum classiﬁer. The sequence of classes with the highest proba-
entropy framework similar to the MENE system of [Borth- bility is then selected using the Viterbi algorithm.

                                                IJCAI-07
                                                  17633  Feature Representation                             different shared tasks. It can be seen that this empirical prop-
Our feature representation is adapted from [Chieu and Ng, erty occurs in other languages as well.
    ]
2003 ,butwithout using any external name lists. We now 4.2 Motivation
give a general overview of the features used.
                                                      Consider the following sentence taken from a news article
3.1  Local Features                                   reporting soccer results in the CoNLL 2003 English shared
Local features of a token w are those that are derived from task ofﬁcial test set:
only the sentence containing w. The main local features are: AC Milan ( 9 ) v Udinese ( 11 ) 1330
  Zoning Each document is segmented using simple rules
                                                        This sentence contains two named entities: AC Milan and
into headline, author, dateline,andtext zones. The zone
                                                      Udinese are organization names. If these named entities have
is used in combination with other information like capitaliza-
                                                      not been seen in the training data, it might be difﬁcult to
tion and whether the token is the ﬁrst word of the sentence.
                                                      predict their name class due to poor contextual information.
  Lexical Information Strings of the current, previous, and
                                                      Consider the following two sentences found in the English
next words are used in combination with their case informa-
                                                      Gigaword Corpus:
tion. Words not found in a vocabulary list generated from
training data are marked as rare.                         AC Milan, who beat Udinese 2-1 at the San Siro
  Orthographic Information Such information includes      stadium, and Lazio, who won 1-0 at Cagliari, also
case, digits, and the occurrence of special symbols like $ have a maximum six points from two games.
within a word.                                            Milan needed a 10th minute own goal headed in
  Word Sufﬁxes For each named entity in the training data, by Udinese defender Raffaele Sergio to get off the
3-letter word sufﬁxes that have high correlation score with a mark, and were pegged back on the hour by Paolo
particular name class are collected.                      Poggi.
  Class Sufﬁxes A list is compiled from training data for to-
kens that frequently terminate a particular name class. For It is easier to predict the named entities in the above sen-
example, the token association often terminates the organiza- tences as tokens like beat and defender offer useful hints to
tion class.                                           the classiﬁer.
                                                        In a data set, different sentences contain differing amounts
3.2  Global Features                                  of useful contextual information. This results in a classiﬁer
Global features are derived from other sentences in the same predicting a mix of correct and incorrect labels for the same
                                                      named entity. If we have a reasonably accurate classiﬁer,
document containing w. The global features include:
  N-grams Unigrams and bigrams of another occurrence of there will be more correct labels assigned than incorrect la-
the token.                                            bels in general.
  Class Sufﬁxes Class sufﬁx of another occurrence of the We propose a method of exploiting this empirical property
token.                                                of one class per named entity by using the most frequently
  Capitalization The case information of the ﬁrst occurrence occurring class of a named entity found in a machine-tagged
of the token in an unambiguous position (non-ﬁrst words in a data set. We call this most frequently occurring class of a
text zone).                                           named entity its majority tag.
  Acronyms  Words made up of all capitalized letters are 4.3 Baseline Method
matched with sequences of initial capitalized words in the
same document to identify acronyms.                   The CoNLL baseline system [Sang and Meulder, 2003] ﬁrst
  Sequence of InitCaps For every sequence of initial cap- determines the majority class C of a named entity N found
italized words, its longest substring that occurs in the same in the training data. The baseline system then labels all oc-
document is identiﬁed.                                currences of N found in the test data with class C, regardless
  Name Class of Previous Occurrences  The predicted   of context. Any named entities in the test data which do not
name classes of previous tokens are used as features. appear in the training data are not assigned any name class.
4  Approach                                                                   Test A  Test B
                                                                   All NEs     71.18   59.61
4.1  One Class per Named Entity                                    Seen NEs    83.44   80.46
In the CoNLL 2003 English NER training data, we have ob-
served that around 98% of the named entity types and 91% of Table 2: F1 score of the baseline method for all named entities
the named entity tokens have exactly one class. Incidentally, and for only named entities seen in the training data
within natural language processing, there are similar obser-
vations proposed previously regarding the number of word Table 2 summarizes the F1 score of the baseline method
senses of a collocation (one sense per collocation [Yarowsky, for all (seen and unseen) named entities and all seen named
1993]) and the number of word senses of occurrences of a entities. In all tables in this paper, Test A refers to the CoNLL
word in a document (one sense per discourse [Gale et al., 2003 development test set and Test B refers to the CoNLL
1992]). Table 1 shows the percentage of named entities that 2003 ofﬁcial test set. A named entity is seen if it exists in
have exactly one class from the training datasets of several the training data. The baseline F1 score of the ofﬁcial test

                                                IJCAI-07
                                                  1764                            Language             Shared Task   Types  Tokens
                            English              CoNLL 2003    98%     91%
                            German               CoNLL 2003    99%     96%
                            Dutch                CoNLL 2002    99%     98%
                            Spanish              CoNLL 2002    96%     76%
                            English (Biomedical) BioNLP 2004   98%     85%

         Table 1: Percentage of named entities that have exactly one class for several shared task training datasets.

                                                      machine-tagged data set U .InL, the case of named entities
T ← training dataset                                  is retained (i.e., whether a named entity appears in upper or
U ←  unlabeled dataset                                lower case), and named entities that occur only once in U  are
E ←  test dataset                                     pruned away from L.
                                                        Lastly, a ﬁnal-stage classiﬁer h2 is trained with labeled data
1. Train classiﬁer h1 on T                            T and uses the majority tag list L to generate the new feature
    
2. U ← label U using h1                               described as follows. During training or testing, when the h2
    ←        (            )           
3. L   extract NE,MajTag   pairs from U               classiﬁer encounters a sequence of tokens w1...wn such that
4. Train classiﬁer h2 on T using L                    (w1...wn,nc) ∈ L, a feature MJTAG-nc will be turned on for
     ←
5. E   label E with h2 using L                        each of the tokens w1, ..., wn in the sequence. If there is more
                                                      than one matching sequence of different lengths, preference
                                                      will be given to the longest matching sequence. For example,
    Figure 1: Pseudocode for the majority tag method  consider the following sentence:
                                                          Udinese midﬁelder Fabio Rossitto has the ﬂu.
set is 59.61. However on closer examination, the F1 score  (              )  (                    )
is 80.46 when we only consider the named entities seen in If Udinese, ORG ,   F abio Rossitto, P ER ,and
                                                      (           )
the training data. Unseen NEs lower the F1 score since the Fabio,LOC are present in the majority tag list, the fea-
baseline method does not assign any name class to named tures in the ﬁrst column below will be turned on. Notice
entities in the test set that are not found in the training data. the feature that is turned on for Fabio is MJTAG-PER and
  This shows that the poor baseline performance is primar- not MJTAG-LOC, because the longest matching sequence is
ily attributed to unseen named entities. In addition, reason- Fabio Rossitto.
able (though not great) performance can be obtained using   Feature     Token
this baseline method if all the named entities are seen, even MJTAG-ORG Udinese
though contextual information is not used. This suggests that  -        midﬁelder
the majority tag is valuable and can be exploited further if MJTAG-PER  Fabio
more unseen named entities in the test data can be discovered MJTAG-PER Rossitto
beforehand. One way to do this is to gather named entities in  -        has
machine-tagged unlabeled text so as to increase the likelihood -        the
that a named entity found in the test set is seen beforehand.  -        ﬂu
Even though the NER system that labels the unlabeled text is
not perfect, useful information could still be gathered.
4.4  The Majority Tag as a Feature                    5   Experiments
There are some words like Jordan that can refer to a person Our unlabeled data set consists of 600 million words from the
or a location depending on the context in which they are used. LDC English Gigaword Corpus. For all experiments, features
Hence, if the majority tag is incorporated as a feature, a clas- that occur only once in the training data are not used and the
siﬁer can be trained to take into account the context in which GIS algorithm is run for 1,000 iterations. Unlabeled data is
a named entity is used, as well as exploit the majority tag. randomly selected from the English Gigaword Corpus. The
  Figure 1 gives the high-level pseudocode of our method reported scores are the averages of ten runs, and similar trends
which is based on the maximum entropy framework and in- are observed over all runs.
volves two stages. First, an initial-stage supervised classiﬁer
h1 is trained with labeled data T . The classiﬁer h1 then labels 5.1 Results
a large unlabeled dataset U to produce a machine-tagged data Table 3 shows the F1 scores of two systems trained with
set U .                                              5k and 204k labeled examples and exploiting increasing
  Next, the majority tag list L is produced by extracting the amounts of unlabeled data. The entire CoNLL 2003 labeled
list of named entities with their associated majority tags from training set is used for the 204k runs. Performance peaks at
this machine-tagged data set U . L thus contains a list of 300 million words of unlabeled data on the CoNLL 2003 ofﬁ-
(NE,MajTag)    pairs, where MajTag is the name class  cial test set (Test B) and drops slightly after that. The best F1
that occurs most frequently for the named entity NE in the score for the ofﬁcial CoNLL English test set is 87.13% using

                                                IJCAI-07
                                                  1765300 million words of unlabeled data. This is an improve- 5.2 Comparison with Previous Best Results
ment of 1.17% in F1 score and an error reduction of 8.3%. Figure 4 shows the performance comparison of our system
The +4.02% improvement in the 5k system shows that the against the systems in CoNLL 2003. The top system [Florian
method is even more effective on a small labeled dataset. Im- et al., 2003] uses an externally trained NER system as part
provements on small labeled data sets are important for other of a combined system. The second best system [Chieu and
resource-poor languages where labeled data is scarce. Ng, 2003] uses external name lists and has a score of 88.31,
  Figure 2 and Figure 3 show the accuracy improvements but when not using any external name lists, its score drops to
with increasing amounts of unlabeled data, using 204k and 5k 86.84.
labeled examples respectively. Accuracy generally increases Note that the NER system reported in this paper does not
at a decreasing rate with additional unlabeled data for both utilize any external name lists. Rather, the majority tag list
systems.                                              was generated automatically from machine-tagged unlabeled
                                                      data, without manual labeling of name class for names found
   87.20                                              in this list. In addition, the names found in our majority tag
                                                      list may contain errors in general, since they are automatically
   87.00                                              determined. Despite this, we have achieved performance at
                                                      the third place. Our results are encouraging given that we use
                                                      unlabeled data as the only additional resource.
   86.80
                                                      6   Analysis
   86.60
                                                      Table 4 shows the number of named entities in Test B dis-
   86.40                                              covered due to adding 300 million words of unlabeled data.
                                                      In the rightmost column of Table 4, named entities are con-
                                                      sidered seen if they are found in either the training data or
   86.20
                                                      the machine-tagged unlabeled data. The unlabeled data is la-
                                                      beled with a classiﬁer trained with 204k labeled examples. It
   86.00                                              can be seen that adding unlabeled data has successfully led
                                                      to a large proportion of named entities previous unseen to be
   85.80                                              discovered.
        0         200        400        600
                                                                                0M     300M
Figure 2: Test B F1 score of our system trained with the full      Seen NEs     3148   5164
204k labeled training dataset from CoNLL and exploiting in-        Unseen NEs   2500    484
creasing amounts of unlabeled data.
                                                      Table 4: Seen and unseen counts of named entities in Test B
                                                      before and after using 300 million words of unlabeled data.
                                                      In this table, named entities are seen if they exist in the train-
   56.50
                                                      ing data or machine-tagged unlabeled data. The classiﬁer is
   56.00                                              trained with 204k labeled examples.
   55.50
                                                        Table 5 shows the Test B F1 score of the two systems
   55.00                                              before and after using 300 million words of unlabeled text.
   54.50
                                                                (a) 204k Labeled Training Examples
   54.00
                                                                            0M    300 M   Change
   53.50                                                      All          85.96   87.13   +1.17
   53.00                                                      Seen NEs     91.77   92.12   +0.35
                                                              Unseen NEs   75.23   77.29   +2.06
   52.50
   52.00                                                         (b) 5k Labeled Training Examples
                                                                            0M    300 M   Change
   51.50
                                                              All          52.14   56.16   +4.02
        0         200        400        600
                                                              Seen NEs     71.91   72.52   +0.61
                                                              Unseen NEs   44.53   49.14   +4.61
Figure 3: Test B F1 score of our system trained with 5k la-
beled examples from the CoNLL training dataset and exploit-
                                                      Table 5: Breakdown of improvement in F1 score for seen and
ing increasing amounts of unlabeled data.
                                                      unseen named entities in Test B using 300 million words of
                                                      unlabeled text.

                                                IJCAI-07
                                                  1766                                                 204k (All)         5k
                            MWords             Test A  Test B  Test A  Test B
                            0                  90.80   85.96   57.09   52.14
                            10                 91.16   86.25   58.84   53.45
                            20                 91.20   86.48   58.96   53.44
                            50                 91.29   86.58   59.80   54.38
                            100                91.36   86.86   60.40   55.04
                            200                91.47   87.06   60.97   55.27
                            300                91.50   87.13   61.21   56.16
                            400                91.55   87.11   61.37   56.00
                            500                91.59   87.08   61.13   56.10
                            600                91.64   87.01   61.15   56.14
                            Max Improvement    +0.84   +1.17   +4.28   +4.02

Table 3: F1 scores for different amounts of labeled and unlabeled data. Unlabeled data are randomly selected from the English
Gigaword Corpus. Reported F1 scores are obtained using an average of 10 runs.


     95.00
            88.76
     90.00       88.31 87.13
                            86.07 85.50
                                      85.00 84.89 84.67 84.30 84.04 83.92
     85.00                                                           82.50 81.70
                                                                               79.78
                                                                                    78.20
     80.00                                                                               76.97
     75.00
     70.00
     65.00
                                                                                              60.15
     60.00
     55.00

Figure 4: Comparison of F1 scores on the ofﬁcial test set against the systems in the CoNLL 2003 shared task. Our system’s
performance is represented by the black shaded column at the third place.


                                                IJCAI-07
                                                  1767