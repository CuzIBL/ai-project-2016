                 Opinion Sentence Search Engine on Open-domain Blog 

               Osamu Furuse, Nobuaki Hiroshima, Setsuo Yamada, Ryoji Kataoka 
                               NTT Cyber Solutions Laboratories,
                                       NTT Corporation 
                     1-1 Hikarinooka Yokosuka-Shi Kanagawa, 239-0847 Japan 
                                   furuse.osamu@lab.ntt.co.jp

                   Abstract                      do not explicitly express sentiments, can also be informative 
                                                 for a user who wants to know others’ opinions about a 
    We have introduced a search engine that can extract 
                                                 product.  
    opinion sentences relevant to an open-domain query The sentence-level subjectivity classification approaches 
    from Japanese blog pages. The engine identifies [Cardie et al., 2003; Riloff and Wiebe, 2003; Wiebe and 
    opinions based not only on positive or negative 
                                                 Riloff, 2005] try to identify subjective information that is 
    measurements but also on neutral opinions, requests, broader than sentiments and suggest a way of searching for 
    advice, and thoughts. To retrieve a number of opinion sentences in open-domain topics. In these efforts, the 
    opinion sentences that a user could reasonably be 
                                                 subjectivity/objectivity of a current sentence is judged based 
    expected to read, we attempted to extract only ex- on the existence of subjective/objective clues in both the 
    plicitly stated writer's opinions at the sentence-level sentence itself and the neighboring sentences. The subjective 
    and to exclude quoted or implicational opinions. In 
                                                 clues, such as adjectives, nouns, verb phrases, and other 
    our search engine, opinion sentences are identified collocations, are learned from corpora [Wiebe, 2000; Wiebe 
    based on features such as opinion clue expressions, et al., 2001].  
    and then, the relevance to the query of each identi-
                                                   Opinion sentence searches using sentence-level subjectiv-
    fied opinion sentence is checked. The experimental ity classification often collect too many sentences for a user 
    results for various topics, obtained by comparing to read. According to a previous study [Wiebe et al., 2001], 
    the output of the proposed opinion search engine 
                                                 70% of sentences in opinion-expressing articles like editori-
    with that of human judgments as to whether the als and 44% of sentences in non-opinion expressing articles 
    sentences were opinions, showed that the proposed like news reports were judged to be subjective. Sen-
    engine has promise as a practical application. 
                                                 tence-level subjectivity can be used to analyze subjectivity in 
                                                 inputted documents [Wilson et al., 2003; Wilson et al., 2005]. 
 1 Introduction                                  However, in searching opinion sentences from web pages, it 
 An enormous number of blog pages are freely written and is necessary to limit the number of retrieved sentences so that 
 frequently updated as private articles about various topics, a user can survey them without undue effort.  
including very timely ones. As numbers of blog writers and We introduce opinion clue expressions, which are more 
readers rapidly increase, blog pages as a consumer-generated restrictive than sentence-level subjectivity in conventional 
medium (CGM) become increasingly important information methods, as a criterion for judging opinion sentences. We 
sources about people's personal ideas, beliefs, feelings, and also propose a method for searching opinion sentences from 
sentiments (positive or negative measurement). Such sub- web pages using these clue expressions. Using the proposed 
 jective information in blog pages can often be useful for method, we created a prototype opinion sentence search 
 finding out what people think about various topics in making system in blogspace. The search engine extracts opinion 
 a decision.                                     sentences relevant to a user’s query phrase about 
  Studies on automatically extracting and analyzing reviews open-domain topics on products, persons, events, and social 
 about a specific subject on the web [Dave et al., 2003; Mo- phenomena. The search engine identifies opinion sentences 
 rinaga et al., 2002; Turney, 2002; Nasukawa and Yi, 2003] based on sentiments, neutral opinions, requests, advice, and 
have been conducted. An attempt has also been made to thoughts. To retrieve a number of opinion sentences that is 
develop a system to analyze sentiments about open-domain reasonable and that a user will want to read, we attempted to 
queries in blogspace [Nanno et al., 2004]. These efforts have extract only explicitly stated writer's opinions at the sen-
focused on positive or negative measurement.     tence-level and to exclude quoted or implicational opinions. 
  Sentiments and different kinds of subjective information Section 2 describes what sentences should be searched as 
 such as neutral opinions, requests, and judgments provide opinions. Section 3 gives an overview of our prototype 
 useful information. For instance, opinion sentences like “In opinion search system for Japanese blog pages. Sections 4 
 my opinion this product should be priced around $15,” which and 5 explain the two major modules, opinion search ex-


                                            IJCAI-07
                                             2760 traction and query-relevant sentence extraction, and Section Thus, depending on the type of opinion clue, it is necessary to 
 6 evaluates the opinion sentence search method of our pro- consider where the expression occurs in the sentence to judge 
 totype system.                                  whether the sentence is an opinion. 

 2  Opinion Sentences to be Searched             3  Architecture of Opinion Sentence Search  
   We judge a sentence to be an opinion if it explicitly de- Figure 1 shows the configuration of our prototype opinion 
 clares the writer’s idea or belief at a sentence level. We de- sentence search system in blogspace.  
 fine as an “opinion clue”, the part of a sentence that contrib-
utes to explicitly conveying the writer’s idea or belief in the Blog data server Opinion sentence search engine 
opinion sentence [Hiroshima et al., 2006]. For example, “I 
                                                       Blog          Opinion sentence
 am glad” in the sentence “I am glad to see you” can convey             extraction 
 the writer’s pleasure to a reader, so we regard the sentence as pages
 an “opinion sentence” and “I am glad” as an “opinion clue”.                          Off-line
 Another example of an opinion clue is the exclamation mark Blog      Index table of 
 in the sentence “We got a contract!” It conveys the writer’s crawler opinion sentences 
 emotion about the event to a reader. 
  The existence of word-level or phrase-level subjective           Query-relevant sentence
 parts does not assure that the sentence is an opinion. Some            extraction 
word-level or phrase-level subjective parts can make the Web                           On-line
sentence an opinion depending on where they occur in the 
sentence. Consider the following two sentences.                 Query     User
                                                                         interface 
  (1) This house is beautiful.                               Search results 
  (2) We purchased a beautiful house. 
                                                      Figure 1: Configuration of opinion sentence search 
  Both (1) and (2) contain the word-level subjective part 
“beautiful”. Our criterion would lead us to say that sentence The blog data server collects blog pages by periodically 
(1) is an opinion, because “beautiful” is placed in the predi- crawling the web. Our opinion sentence search engine, which 
cate part and (1) is considered to declare the writer’s evalua- receives blog pages from the blog data server, consists of two 
tion of the house to a reader. This is why “beautiful” in (1) main modules: opinion sentence extraction and 
contributes to make the sentence an opinion. By contrast, query-relevant sentence extraction.  
sentence (2) is not judged to be an opinion, because “beau- The opinion sentence extraction module checks whether 
tiful” is placed in the object of the verb “purchase” and (2) is each sentence in the crawled blog pages can be considered an 
considered to report the event of the house purchase rather opinion. Opinion sentences are extracted and indexed as 
objectively to a reader. Sentence (2) contains subjective off-line processing, which, for a practical real-time search, 
information about the beauty of the house; however this should be as high a proportion of the entire processing as 
information is unlikely to be what a writer wants to empha- possible. The index table in the blog data server can ac-
size. Thus, “beautiful” in (2) does not contribute to making commodate more than 1,262,000 updated blog pages every 
the sentence an opinion.                         month. 
  These two sentences illustrate the fact that the presence of The query-relevant sentence extraction module retrieves 
a subjective word (“beautiful”) does not unconditionally opinion sentences relevant to the user’s query phrases from 
assure that the sentence is an opinion. Additionally, these the index table of opinion sentences in the blog page server. 
examples do suggest that whether a sentence is an opinion Since users’ queries cannot be predicted, query-relevant 
can be judged depending on where such word-level or sentence extraction has to include on-line processing. 
phrase-level subjective parts as evaluative adjectives are 
placed in the predicate part.                       Query
  Some word-level or phrase-level subjective parts such as 
subjective sentential adverbs contribute to making the sen- Page title
tence an opinion depending on where they occur in the sen-
tence. Sentence (3) is judged to be an opinion because its 
main clause contains a subjective sentential adverb “amaz- Opinion
ingly”, which expresses the writer’s feeling about the event. sentences 
  (3) Amazingly, few people came to my party. 
  The presence of idiomatic collocations in the main clause 
also affects our judgment as to what constitutes an opinion 
sentence. For example, sentence (4) can be judged as an 
opinion because it includes “my wish is”. 
  (4) My wish is to go abroad.                           Figure 2: User interface by open-domain query 


                                            IJCAI-07
                                             2761   Figure 2 shows the user interfaces we provide now. A user  2,514 clues appearing in the predicate part 
inputs open-domain keyword phrases in the query box and Thought:   I think this book is his.  
then clicks the search button. The opinion sentences resulting Intensifier:  They played extremely well. 
from the search are presented in a blog page unit. The result Impression:  This terminology is confusing. 
pages can be ranked according to the number of opinion Emotion:  I am glad to see you. 
sentences, the ratio of opinion sentences to total sentences, or Positive/negative judgment:   
total strength of the opinion sentences.                    Your audio system is terrific. 
                                                   Modality about propositional attitude:  
 4  Opinion Sentence Extraction                             You should go to the movie. 
  It is difficult to enumerate the opinion-judgment rules Value judgment: This sentence makes no sense. 
describing diversified features under some conditions in a Utterance-specific sentence form: 
rule-based method. To avoid the poor performance caused by          However, it's literally just a dream now. 
data sparseness and the daunting task of writing rules, we Symbol:  We got a contract! 
adopted a learning method that binarily classifies sentences Uncertainty:  I am wondering what I should eat for lunch. 
using opinion clues and their positions in sentences [Hi- Imperative:  Don’t do that. 
roshima et al., 2006] as feature parameters of a Support  422 clues not appearing in the predicate part 
Vector Machine (SVM). An SVM can efficiently learn the  Declarative adverb: 
model for classifying sentences as opinion and non-opinion,            I will possibly go to Europe next year. 
based on the combinations of multiple feature parameters.   Interjection:   Oh, wonderful. 
Following are the feature parameters of our method.  
                                                     Idiomatic collocation:  It's hard to say. 
    
    2,936 opinion clue expressions                 The opinion clues in the Japanese examples are placed in 
    2,715 semantic categories 
                                                 the last part of sentences in the first group. This reflects the 
    150 frequent words                           heuristic rule that Japanese predicates are in principle placed 
    
    13 parts of speech                           in the last part of a sentence.  
  Opinion clue expressions and semantic categories are 
crucial feature parameters. The semantic categories we 4.2  Augmentation by Semantic Categories 
adopted have a hierarchical structure and are from a Japanese Opinion clue expressions can be augmented by the semantic 
thesaurus [Ikehara et al., 1997].                categories of the words in the expressions. The feature pa-
                                                 rameters for a semantic category have two roles: one is to 
 4.1  Clue Expression Collection                 compensate for the insufficient amount of opinion clue ex-
  Whether expressions have opinion clues is a basic criterion pressions, and the other is to consider the relations between 
for judging whether a sentence expresses an opinion. To clue expressions and co-occurring words in the opinion sen-
collect opinion clue expressions for an open-domain opinion tences. Consider the following two sentence patterns: 
sentence search, we extracted opinion sentences from the top (5) X is beautiful. 
twenty Japanese web pages retrieved with forty queries on (6) X is pretty. 
various kinds of topics. The queries correspond to possible 
situations in which a user wants to retrieve opinions from The words “beautiful” and “pretty” are adjectives in the 
web pages about a particular topic. The retrieved pages were common semantic category, “appearance”, and the degree of 
unrestricted to blog pages because we target the opinion sentence-level subjectivity of these sentences is almost the 
sentence search engine applicable not only to blog pages but same regardless of what X is. Therefore, even if “beautiful” 
also to other CGM pages or all web pages, and we hypothe- is learned as an opinion clue but “pretty” is not, the semantic 
size that opinion clues do not differ between blog pages and category “appearance” to which the learned word “beautiful” 
other web pages.                                 belongs, enables (6) to be judged an opinion as well as (5). 
  Out of 75,575 sentences in the total 800 retrieved pages, Many of the opinion clue expressions have co-occurring 
the 13,363 sentences judged unanimously by three evaluators words in the opinion sentence. Consider the following two 
to be opinions were extracted. Then, of these 13,363 sen- sentences. 
tences considered very likely to be opinions, 8,425 were used (7) The sky is high. 
to extract opinion clues by the human analysts, while the (8) The quality of this product is high. 
remaining 4,938 were reserved for future assessment for 
                                                   Both (7) and (8) contain the word “high” in the predicate 
other CGM pages or general web pages.            part. Sentence (7) is considered to be less of an opinion than 
  The total number of opinion clues obtained was 2,936. (8) because (7) might be judged to be the objective truth, 
These clue expressions were classified into two groups, as 
                                                 while (8) is likely to be judged an opinion. The adjective 
shown in the example sentences below. The underlined ex- “high” in the predicate part can be validated as an opinion 
pressions in example sentences are extracted as opinion clues. clue depending on co-occurring words. However, providing 
There were 2,514 clues and 422 clues in each group. The 
                                                 all possible co-occurring words with each opinion clue ex-
example sentences are translations of Japanese opinion sen- pression is not a realistic option. The co-occurrence infor-
tences extracted by human analysts.  


                                            IJCAI-07
                                             2762 mation about each opinion clue expression can be general- evaluator judged to be an opinion, 2,544 were judged to be 
 ized using semantic categories.                 relevant to the queries by at least one evaluator, and 7,297 
                                                 were judged by all three evaluators to be unrelated to the 
 4.3  Training and Test Set                      queries. The high percent of the latter figure, 7,297, which is 
  The training set was chosen from blog pages different 74.1% of the 9,841 opinion sentences, shows that it is inap-
from the web pages used for opinion clue collection. This propriate to accept as search results all opinion sentences in 
was done in order to conduct training specific to blog the pages retrieved by a user’s query.  
searches and in order to conduct training and testing inde-
pendently of opinion clue collection.            5.1  Permissible Scope of Query Relevance 
  We used the same procedure as we did to collect opinion Not all of the retrieved opinion sentences are closely related 
clues, to prepare training and test sets that are both specific to to the query because some of the pages describe miscella-
 blog search. We first retrieved Japanese blog pages with neous topics. The permissible scope between individual users 
 ninety queries covering a wide range of topics:  for query relevance of a sentence differs. The following are 
  Culture: movies, books, music                  opinion sentences from the retrieved pages queried with 
  Entertainment:  sports, TV drama, games        “product name of a game console”. The number of evaluators 
  Facilities: museums, zoos, amusement parks     who judged the sentence to be query-relevant is shown in 
  Food:  beer, noodles, ice cream                parentheses. 
  Health: medicine, syndromes                      (9) I was impressed with the compactness. (all three)  
  Local information: restaurants, hotels, hot springs (10) An adult also can enjoy this. (two of the three) 
  Person: comedians, idols                         (11) The manufacturer is marvelous. (one of the three)  
  Phenomena: lifestyle, environment                (12) Technological advancement is very rapid. (none) 
  Politics, Economy: elections, gasoline prices    The above sentences show that individual judgments differ 
  Products:   cell phones, cars, beer, cosmetics, software when a sentence tends to be indirectly or weakly relevant to 
  Opinion sentences were extracted from the top ten re- the query. We take a stand on accepting weak query rele-
trieved blog pages for each query, leaving 900 pages and vance because it is more advantageous in a real-time search 
29,486 sentences in total. Three evaluators judged whether to pursue possible query relevance heuristically or elimina-
each sentence was an opinion or not. Out of 29,486 sentences, tively rather than to verify query relevance precisely. Thus, 
2,868 were judged to be opinions by all three evaluators, we considered the sentences judged by at least one evaluator 
3,725 by two evaluators, 3,248 by one evaluator, and 19,645 query-relevant to be a correct answer. 
were judged to be non-opinions by all three evaluators. 
                                                 5.2  Strategies about Query Relevance 
            Number Training set Test set         Query-relevant sentence extraction in the prototype system 
            Query 72                    18       has the following two heuristic and simple strategy options. 
         Total sentences      23,800   5,686       (a) A sentence is relevant to the query only when a 
  Sentences all three judged opinions  2,416   452     query phrase occurs in the sentence or within some 
  Sentences the  two judged opinions  3,003   722      number of sentences before the sentence. 
   Sentences the one judged opinions  2,631   617  (b) A sentence is relevant to the query only when a 
    Sentences none judged opinions  15,750 3,895       query phrase occurs in the sentence or within the 
             Table 1: Training and test set            chunk that the sentence belongs to and only opinion 
                                                       sentences consecutively appear in. 
  Eighteen queries, one-fifth of the total, were randomly 
 selected, and the sentences for the queries were used for The strategy adopted affects the number of opinion sen-
 testing. The sentences of the other seventy-two queries were tences an index table can accommodate. From this viewpoint, 
 used for training. The breakdown of training and test sets is Strategy (b) is better because an index table needs informa-
 shown in Table 1.                               tion only about opinion sentences. In contrast, with Strategy 
  We set the sentences with at least one judged opinion as a (a), an index table has to accommodate non-opinion sen-
 cut-off point. Thus, 8,050 were then used to learn positive tences immediately before opinion sentences in addition to 
 examples in the SVM, and 1,791 were used to assess the the opinion sentences themselves.  
 performance of the opinion sentence search system (Section 
 6). 15,750, non-opinion sentences were used to learn nega- 6 Experiments  
 tive examples, and 3,895 were used for assessment. We conducted experiments in the prototype opinion sentence 
                                                 search system in blogspace to assess opinion sentence ex-
 5  Query-relevant Sentence Extraction           traction, query-relevant sentence extraction, and a combina-
 The three evaluators also judged whether each opinion sen- tion of the two. All experiments used the Japanese sentences 
 tence in a training and test set in Section 4.3 was described in Sections 4.3. The numbers of sentences used for 
 query-relevant. Of the 9,841 sentences that at least one training and testing are shown in Table 1.  


                                            IJCAI-07
                                             2763 6.1  Evaluation of Opinion Sentence Extraction  assigning the individual position condition to each opinion 
                                                 clue or locating the predicate part more precisely signifi-
 The experiments on opinion sentence extraction were de-
                                                 cantly improves performance. 
 signed to ascertain the effect of feature parameters on opinion 
                                                   The ratios of sentences the system judged opinion were, 
 sentence learning and the effect of position information on 74.3% to the opinion sentences three evaluators judged to be 
 opinion clues. Answers where at least one of the three opinions, 62.0% to those two judged to be opinions, 44.4% to 
 evaluators judged the sentence to be an opinion were defined those one judged to be opinions, and 11.4% to those three 
 as correct, and answers where no evaluator judged the sen- judged to be non-opinions. Even though all sentences judged 
 tence to be an opinion were defined as wrong.   by at least one evaluator to be opinions were equally trained 
 Method           Precision Recall Accuracy      as correct answers, the higher the number of evaluators 
                                                 judging a sentence to be an opinion, the more likely our 
 Baseline          67.5%    40.3%    75.1%       method was to judge it an opinion. This result shows that our 
 Proposed (without 
 semantic categories) 75.0% 47.6%    78.5%       method is congruent with human judgment. 
 Proposed (with 
 semantic categories) 72.5% 54.8%    79.2%       6.2  Evaluation of Query Relevance 
            Table 2: Comparison with baseline      We investigated the performance of the query-relevant 
                                                 sentence extraction strategies described in Section 5.2, using 
  The main feature parameters for the SVM learner are clue all 1,791 opinion sentences in a test set in Table 1. The per-
expressions and semantic categories, as explained in Section formance values were computed based on the correct an-
4. We prepared a baseline method that regards a sentence as swers being the 429 sentences that at least one of the three 
an opinion if it contains a number of opinion clues that does evaluators had judged to be query-relevant and the wrong 
not dip below a certain threshold. The best threshold was set answers being 1,362 sentences that all three evaluators had 
through trial and error at four occurrences. The experimental judged to be query-irrelevant. We modified Strategy (a) in 
results in Table 2 show that our method performs better than Section 5.2, as follows. 
the baseline method. Precision is defined as the correctness 
ratio of the sentences extracted as opinions. Recall is defined (a)’  A sentence is relevant to the query only when a 
as the ratio of opinion sentences correctly extracted over the query phrase exists in the sentence or in those right 
total number of test opinion sentences. Accuracy is defined before the sentence. 
as the correct judgment ratio of all the test (both opinion and Strategy (b) was not modified for the evaluation. We 
non-opinion) sentences. The two bottom rows show the re- prepared a baseline method that regards a sentence as query 
sults of our opinion sentence extraction method. The second relevant if it contains a query phrase. 
bottom row concerns methods that do not use semantic 
categories, and the bottom row concerns those that do. The Method Precision  Recall Accuracy 
results in these two cases show that clue expressions are Baseline 74.0%     16.6%     78.6%
effective and that semantic categories improve performance.  Strategy (a)’  65.0% 33.3% 79.7%
  We also evaluated the effect of position information of Strategy (b)  53.2% 41.3%    77.2%
2,936 opinion clues based on the heuristic rule that a Japa- Table 4: Evaluation of query relevance strategy 
nese predicate part almost always appears in the last ten Table 4 shows the experimental results of query-relevance 
words in a sentence. Instead of more precisely identifying extraction from 2,868 opinion sentences in the baseline, 
predicate position from parsing information, we employed Strategy (a)’, and Strategy (b). These results show that our 
this heuristic rule as a feature parameter in the SVM learner strategies performed with much better recall and slightly 
and classifier for practical reasons.            worse precision than the baseline method. Although the 
        Position Precision Recall Accuracy       above results show that our strategies need improvement, 
                                                 Strategy (a)’ and Strategy (b) seem to amount to a practical 
        All words     71.2%    48.6%   77.6%     solution at present. Strategy (b), which our system is cur-
    Quasi predicate part 72.5% 54.8%   79.2%     rently using is advantageous from the viewpoint of the 
      Table 3: Effect of opinion-clue position restriction amount of opinion sentences in an index table but is some-
  Table 3 lists the experimental results for position restric- what inferior to Strategy (a)’ in precision. 
tion of opinion clues. “All words” indicates that all feature 
parameters are permitted at any position in the sentence. 6.3  Evaluation of Total Performance 
“Quasi predicate part” indicates that all feature parameters The total performance of the opinion sentence search is ob-
are permitted only if they occur within the last ten words in tained by multiplying performance of the two modules, 
the sentence. Although we narrowed the scope to consider opinion sentence extraction, and query-relevant sentence 
the feature parameters and adopted an expedient method to extraction. The performance values were computed based on 
locate the predicate part, feature parameters within the last the correct answers being the 429 sentences that were judged 
ten words perform better in all evaluations than those without by at least one of the three evaluators to be query-relevant 
position restriction. The fact that the equal position restric- opinions out of all 5,686 test sentences in Table 1. The ratio 
tion on all opinion clues improved performance suggests that of opinion query-relevant sentences in test sentences, 7.5%, 


                                            IJCAI-07
                                             2764