               Hierarchical Multi-channel Hidden Semi Markov Models                      ∗

                             Pradeep Natarajan     and  Ramakant Nevatia
                             Institute for Robotics and Intelligent Systems,
                                   University of Southern California,
                                     Los Angeles, CA 90089-0273
                                      {pnataraj, nevatia}@usc.edu

                    Abstract                          as limb joints) and the high level semantic concepts (such
                                                      as gestures) that need to be recognized and to handle the
    Many interesting human actions involve multiple   large variations in the duration and styles of these activi-
    interacting agents and also have typical durations. ties when performed by different people or even the same
    Further, there is an inherent hierarchical organiza- person at different times. Due to uncertainty inherent in
    tion of these activities. In order to model these we sensory observations, probabilistic reasoning offers a natu-
    introduce a new family of hidden Markov models    ral approach. While several probabilistic models have been
    (HMMs) that provide compositional state represen- proposed over the years in various communities for activity
    tations in both space and time and also a recursive recognition, hidden Markov models (HMMs) and their ex-
    hierarchical structure for inference at higher levels tensions have by far been the most widely used ones. They
    of abstraction. In particular, we focus on two possi- offer advantages such as clear Bayesian semantics, efﬁcient
    ble 2-layer structures - the Hierarchical-Semi Par- learning and inferencing algorithms, and a natural way to in-
    allel Hidden Markov Model (HSPaHMM) and the       troduce domain knowledge into the model structure. [Bui et
    Hierarchical Parallel Hidden Semi-Markov Model    al., 2004] presented an extension of the hierarchical hidden
    (HPaHSMM). The lower layer of HSPaHMM con-        Markov model (HHMM) for inferring activities at different
    sists of multiple HMMs for each agent while the   levels of abstraction. [Hongeng and Nevatia, 2003] used the
    top layer consists of a single HSMM. HPaHSMM      hidden semi-Markov model (HSMM) for modeling activity
    on the other hand has multiple HSMMs at the lower durations, while [Duong et al., 2005] presented the switch-
    layer and a Markov chain at the top layer. We     ing hidden semi-Markov model (S-HSMM) to exploit both
    present efﬁcient learning and decoding algorithms the hierarchical structure as well as typical durations for ac-
    for these models and then demonstrate them ﬁrst on tivity recognition. [Vogler and Metaxas, 1999] introduced
    synthetic time series data and then in an application the parallel hidden Markov model (PaHMM) for recognizing
    for sign language recognition.                    complex 2-handed gestures in ASL while [Brand et al., 1997]
                                                      introduced the coupled hidden Markov model (CHMM) and
1  Introduction                                       applied it to recognize tai chi gestures as well as multi-agent
                                                      activities.
Our goal is to develop methods for inferring activities given We believe that in most real applications we need models
observations from visual and other sensory data. Activities that combine the features of all of these models and intro-
have a natural hierarchical structure where combinations of duce a new class of models, namely the hierarchical multi-
simpler activities form higher level, more complex activities. channel hidden semi-Markov models. In particular we focus
In general, multiple agents may be involved who take par- on two model structures - the hierarchical semi parallel hid-
allel actions but whose temporal (and spatial) relations are den Markov model (HSPaHMM) and the hierarchical paral-
important for inference of higher level activities. We present lel hidden semi-Markov model (HPaHSMM) and present efﬁ-
a mathematical formalism for recognizing such multi-agent cient decoding and learning algorithms for them. We validate
activities that can take advantage of both their inherent hi- the utility of these models by testing on simulated data as well
erarchical organization and typical durations. Such models as for the real task of continuous sign language recognition.
can have a wide range of applications such as surveillance, The rest of the paper is organized as follows - In the
assistive technologies like sign language recognition and in- next section we deﬁne the parameters of HSPaHMM and
telligent environments.                               HPaHSMM formally. Then in the sections 3 and 4 we present
  A key challenge faced by researchers in activity recog- efﬁcient decoding and learning algorithms for them. Finally
nition is to bridge the gap between the observations (such in section 5 we present the experimental results.
  ∗This research was partially funded by Advanced Research and
Development Activity of the U.S. Government under contract MDA-
904-03-C-1786.

                                                IJCAI-07
                                                  25622  Model Deﬁnition and Parameters                      HPaHSMM contains multiple HSMMs at the lower layer and
                                       λ              a single Markov chain at the upper layer and hence has the
We begin by deﬁning a standard HMM model by the tuple following set of parameters-
(Q, O, A, B, π) where, Q is the set of possible states, O is the
                                                       HPaHSMM       C     C      C     C     C     C
set of observation symbols, A is the state transition probabil- λlower =(Qlower,Olower,Alower,Blower,Dlower,πlower)
ity matrix (aij = P (qt+1 = j|qt = i)), B is the observation HPaHSMM
                                                            λupper     =(Qupper,Oupper,Aupper,Bupper,πupper)
probability distribution (bj(k)=P (ot = k|qt = j)) and π is
the initial state distribution. It is straightforward to generalize Figure 1 illustrates the various HMM extensions dis-
this model to continuous (like gaussian) output models.
  This can be extended to a hierarchical hidden Markov
model (HHMM) by including a hierarchy of hidden states.
A HHMM    can be formally speciﬁed by the tuples- λ =
(Qd,Od,Ad,Bd,πd)  where d ∈  1..H indicates the hierar-
chy index.
  In traditional HMMs, the ﬁrst order Markov assumption
implies that the duration probability of a state decays expo-
nentially. The hidden semi-Markov models (HSMM) were
proposed to alleviate this problem by introducing explicit
state duration models. Thus a HSMM model can be speci-
ﬁed by the tuple- λ =(Q, O, A, B, D, π) where, D contains
a set of parameters of the form P (di = k), i.e. the probability
of state i having a duration k.
  In the basic HMM, a single variable represents the state
of the system at any instant. However, many interest-
ing activities have multiple interacting processes and sev-
eral multi-channel HMMs have been proposed to model
these. These extensions basically generalize the HMM state
                                S   =  S1,,SC
to be a collection of state variables ( t t t ). In   Figure 1:  Structure of a)HMM   b)HSMM    c)PaHMM
their most general form, such extensions can be represented d)PaHSMM e)HSPaHMM f)HPaHSMM
as- λ =(QC ,OC ,AC ,BC ,πC ) where Qc and Oc are
the possible states and observations at channel c respec- cussed. The key difference between HSPaHMM  and
tively and πc represents the initial probability of channel c’s HPaHSMM is that HSPaHMM models the duration for the
states. AC contains the transition probabilities over the com- entire low-level PaHMM with the top-level HSMM state
                1      C    1    C          C         while HPaHSMM   models the duration of each state in each
posite states (P ([qt+1, .., qt+1]|[qt , .., qt ])), and B con-
tains the observation probabilities over the composite states low-level HSMM. Thus HSPaHMM requires fewer parame-
    1    C    1   C                                   ters, while HPaHSMM is a richer structure for modeling real
(P ([ot , .., ot ]|[qt , .., qt ])). In this form, the learning as well
as inferencing algorithms are exponential in C, and also re- events.
sult in poor performance due to over-ﬁtting and large number
of parameters to learn. The various multi-channel extensions 3 Decoding Algorithm
typically introduce simplifying assumptions that help in fac-
torizing the transition and observation probabilities. Parallel In this section, we use the following notations - Let C be the
hidden Markov models (PaHMM) factor the HMM into mul- number of channels in lower layer, T the number of frames
tiple independent chains and hence allow factorizing both AC or observations in each channel, N the number of states in
                                                                                1
and BC . Coupled hidden Markov models (CHMM) factor the each channel of the lower layer and W the number of lower
HMM into multiple chains where the current state of a chain level HMMs. Then, the top-level HMM will have one state
depends on the previous state of all the chains.      for every lower level HMM as well as a state for every possi-
  The hierarchical multi-channel hidden semi-Markov mod- ble transition between lower level HMMs. This is because in
els that we propose try to combine all the above characteris- applications like sign language recognition where each word
tics into a single model structure. In the most general form, is modeled by a multi-channel HMM/HSMM each transition
they can be described by a set of parameters of the form- between the words in the lexicon is distinct. Each of the W
λ =(Qc ,Oc,Ac ,Bc,Dc,πc)      d ∈ 1..H
         d  d  d   d   d  d where,         is the hi- HMMs can transition to any of the W HMMs giving a total of
erarchy index and c is the number of channels at level d, and W 2 transition states. Thus the top-level HMM has a total of
the parameters have interpretations similar to before. Each W ∗ (W +1)
channel at a higher level can be formed by a combination          states.
of channels at the lower level. Also, the duration models at
each level is optional. Further, the channels at each level in 3.1 HSPaHMM Decoding
the hierarchy maybe factorized using any of the methods dis- The decoding algorithm for HSPaHMM works by travers-
cussed above (PaHMM, CHMM etc). It can be seen that λ
                                                ing the top-level HSMM and at each time segment ﬁnding
presents a synthesis of λ , λ and λ . HSPaHMM has 2 lay- the maximum likelihood path through the low-level PaHMM
ers with multiple HMMs at the lower layer and HSMM at the
upper layer and has the following set of parameters-     1We have assumed that all the lower level HMMs have the same
       λHSPaHMM  =(QC     ,OC   ,AC   ,BC    ,πC   )  number of states for simplicity of notation. It is straightforward to
        lower         lower lower lower  lower lower  extend the algorithms presented to cases where the low-level HMMs
 HSPaHMM
λupper    =(Qupper,Oupper,Aupper,Bupper,Dupper,πupper) have varying number of states.

                                                IJCAI-07
                                                  2563corresponding to the upper HSMM’s state. Traversing the Algorithm 1 HSPaHMM Decoding
                              2       2  2
top-level HSMM states takes O(W (W +1) T  ) time and   1: Top-level HSMM has W*(W+1) states
                                     2
each call to the lower PaHMM takes O(CN T ) time giving 2: We use the following indices for states-
a total complexity of O(W 2(W +1)2N 2T 3). We can re-     - Top-level states 0..W-1 have low-level PaHMMs correspond-
duce this complexity if the duration models for the top-level ing to them.
HSMM   are assumed to be uniform or normal. In this case, - Top-level state W + w1W + w2 is the transition state for the
                                                          w →  w              w ,w  ∈ [0..W − 1]
we can deﬁne parameters M and Σ for each state in the top- 1    2 transition where 1 2
                                                       3: Beam(t)=Top K states at time t.
level HSMM such that we only need to consider state dura-     i        ∈ [M(i) − Σ(i),M(i)+Σ(i)]
               [M  − Σ,M  +Σ]                          4: State ’s duration
tions in the range             . The values of M and      δi                                 i      t
Σ                                                      5:  t = probability of max-likelihood path to state at time .
  can be obtained from the lower and upper thresholds in 6: P t(m|l) = Probability of transitioning to state m from state l.
                                                           o
the case of uniform distributions or from the mean and vari- 7: P (Ot1..Ot2|m) = Probability of observing Ot1..Ot2 in state
ance in normal distributions. In this case since we need to m.
check only 2Σ durations at each instant and each low-level 8: P d(d|m) = Probability of spending duration d in state m.
PaHMM   has M observations on average, traversing the top- 9: maxP ath(m, t1,t2) = Function to calculate probability of
level HSMM takes O(W 2(W +1)2T Σ) and each lower level    max-likelihood path through low-level PaHMM of state m from
PaHMM takes O(CN2M)    giving a total inference complex-  time t1 to t2. See [Vogler and Metaxas, 1999] for details.
ity of O(CW2(W +1)2N  2TMΣ). Still, even for small val- 10: for i =1to T do
                                                      11:   for j =1to K do
ues of W the inference complexity become prohibitively large  l ← jth     Beam(i)
as it varies with W 2(W +1)2. We can get around this by 12:         state in
                                                      13:     if l<Wthen
storing only the top K states of the upper-HSMM at each in-        m =(l +1)∗ W    (l +1)∗ W + W
                                        O(W )         14:       for              to              do
stant. Then, since each state can only transfer to states 15:     for n = M(m) − Σ(m) to M(m)+Σ(m)  do
(transition states can transfer only to the destination states       m          l     t          d
                                                      16:           δi+n   ←    δi ∗ P (m|l) ∗ P  (n|m) ∗
                                                                      o
while states corresponding to lower-level PaHMMs can trans-         P (Oi+1..Oi+n|m)
                                                                       m
fer to W transition states), the overall inference complexity 17:   if δi+n in top K paths at time i + n then
                  2
becomes O(CWKN     TMΣ). Figure 2 illustrates the decod- 18:          add m to Beam(i + n)
ing algorithm and Algorithm 1 presents the pseudocode. 19:          end if
                                                      20:         end for
                                                      21:       end for
                                                      22:     else
                                                      23:       m ←  Remainder(l/W)
                                                      24:       for n = M(m) − Σ(m) to M(m)+Σ(m)  do
                                                                   m     l   d
                                                      25:         δi+n ← δi ∗ P (n|m) ∗ maxP ath(m, i +1,i+ n)
                                                                    m
                                                      26:         if δi+n in top K paths at time i + n then
                                                      27:           add m to Beam(i + n)
                                                      28:         end if
                                                      29:       end for
                                                      30:     end if
                                                      31:   end for
                                                      32: end for
                                                      33: return δ of maximum probability state in Beam(T)
            Figure 2: Decoding HSPaHMM

                                                                 2        2  3
3.2  HPaHSMM Decoding                                 takes O(CW  (W  + N) T  ). If the duration models are nor-
                                                      mal or uniform we only need to consider state durations in the
For decoding HPaHSMMs, we have to traverse the top-level range [m−σ, m +σ] where the values of m and σ are deﬁned
Markov chain and at each state call the corresponding low- for each state of every PaHSMM. The time complexity then
level PaHSMM. This procedure can be simpliﬁed by string- becomes O(CW2(W + N)2Tmσ). Further by storing only
ing together the low-level PaHSMMs and also the transition the top k states in the compound PaHSMM at each instant,
states into one large compound PaHSMM and then traversing we can reduce the complexity to O(CkW(W + N)Tmσ).
it. Since the W low-level PaHSMMs have N states each and Figure 3 illustrates the decoding algorithm and Algorithm 2
        W 2
there are  transition states in the top level, the compound presents the pseudocode for HPaHSMM.
PaHSMM has  W  ∗ (W + N) states. Note that the transition
states can have their own duration and output models. Since 4 Embedded Viterbi Learning
each channel of the compound PaHSMM is a HSMM taking
O(W 2(W  + N)2T 3) 2 time, the entire decoding algorithm In many applications like sign language recognition and event
                                                      recognition the training samples typically contain a sequence
  2[Murphy, 2002] points out that HSMM inference complexity of words/events which are not segmented. In such cases, we
can be reduced to O(W 2(W +1)2T 2) by pre-calculating output string together the individual word/event HMMs and then re-
probabilities for all states and intervals. However, since in beam estimate the parameters of the combined HMM using the tra-
search we only store only a small subset of the states at each time ditional Baum-Welch algorithm. Thus the individual events
interval, this procedure actually increases run time in our case. are segmented automatically during the re-estimation pro-

                                                IJCAI-07
                                                  2564                                                      Algorithm 2 HPaHSMM Decoding
                                                       1: Beam(c,t)=Top k states at time t in channel c.
                                                       2: State i’s duration ∈ [m(i) − σ(i),m(i)+σ(i)]
                                                       3: minT h =min(m(i) − σ(i)); maxT h =max(m(i)+σ(i))
                                                           c,i
                                                       4: δt = Probability of max-likelihood path to state i in channel c
                                                          at time t.
                                                           t
                                                       5: Pc (m|l) = Probability of transitioning to state m from state l
                                                          in channel c.
                                                           o
                                                       6: P (Ot1..Ot2|c, l) = Probability of observing Ot1..Ot2 in state
                                                          l in channel c.
                                                       7: P d(d|c, l) = Probability of spending duration d in state l in
                                                          channel c.
            Figure 3: Decoding HPaHSMM                 8: Beam(c,1) set using initialization probabilities of states ∀ c.
                                                       9: for i =2to T do
cess. In constrained HMM structures like the the left-right 10: for j = minT h to maxT h do
HMM (where each state can transition only to itself or to one 11: for l =1to C do
state higher), the re-estimation procedure can be simpliﬁed 12: for m =1to W ∗ (W + N) do
by calculating the Viterbi path through the HMM at each it- 13:   for n =1to k do
eration and re-estimating the parameters by a histogram anal- 14:   p ← nth state in Beam(l, i − 1)
                                                                     l,m    l,p    t        d
ysis of the state occupancies. Since in our applications we 15:     δi+j = δi−1 ∗ Pc (m|p) ∗ P (j +1|l, m) ∗
                                                                      o
are primarily interested in left-right HMMs, we adopted a           P (Oi..Oi+j |l, m)
                                                                       l,m
Viterbi-like embedded training algorithm for HSPaHMM and 16:        if δi+j in top k paths at time i + j then
HPaHSMM. Algorithm   3 presents the pseudocode for the 17:            add m to Beam(l, i + j)
learning algorithm where at each iteration we call the corre- 18:   end if
sponding decoding algorithm and then re-estimate the param- 19:   end for
eters using a simple histogram analysis of state occupancies. 20: end for
                                                      21:     end for
                                                      22:   end for
                                                      23: end for
5  Experiments                                                   C
                                                      24: return Σc=1 δ of maximum probability state in Beam(c,T)
To evaluate learning and decoding in HSPaHMM   and
HPaHSMM   we conducted two experiments: one with syn-
thetic data, other with real data for a sign language (ASL)
recognition task. In both cases, we compare our results with sequences producing in total 50-60 sequences. We then ran-
PaHMM without any duration models and beam search on the domly chose a set of training sequences such that each word
states. All run time results are on a 2GHz Pentium 4 Windows occurred at least 10 times in the training set and used the rest
platform with 2GB RAM, running Java programs.         as test sequence. Thus the training set contained 10-20 se-
                                                      quences and the test set contained 40-50 sequences. The data
5.1  Benchmarks on Synthetic Data                     generators were then discarded and then the following models
In this experiment, we used a discrete event simulator to gen- were trained - 1)HPaHSMM with randomly initialized output
erate synthetic observation sequences. Each event can have models, left-right low-level PaHSMMs and low-level dura-
C=2 channels/agents, and each channel can be in one of N=5 tion models (parameters m, σ - see section 3.2) set accurately
states at any time. State transitions were restricted to be left- using the corresponding simulator parameters. The beam-
to-right so that each state i can transition only to state (i+1). size k was set manually. 2) HSPaHMM with randomly ini-
The states had 3-dimensional Gaussian observation models tialized output models, left-right low-level PaHMMs and top-
with the means uniformly distributed in [0,1] and the co- level duration models whose means (M) were set by summing
variances were set to I/4. Further, each state had gaussian over the means of the corresponding low-level PaHSMMs in
duration models with means in the range [10,15] and vari- the simulator and set the parameters K and Σ (see section
ances set to DPARAM=10. We then built a top-level event 3.1) manually. 3) PaHMM with the output models initialized
transition graph with uniform inter-event transition probabil- randomly and decoding performed by a Viterbi Beam Search.
ities. Continuous observation sequences were then generated Each model was trained by re-estimating the output models
by random walks through the event transition graph and the using Embedded Viterbi learning described in Section 4 un-
corresponding low-level event models. Random noise was til the slope of the log-likelihood curve fell below 0.01 or
added in between the event sequences as well as at the be- when 100 training iterations were completed. We then ran
ginning and end of the observation sequence. Figure 4 illus- the learned models on the test sequences and obtained accu-
trates this procedure fora2eventtop-level transition graph. racy measures using the metric (N − D − S − I)/N where
As can be seen, this setup corresponds to the HPaHSMM N=number of events in test set, D=no. of deletion errors,
model structure. Observation sequences were generated us- S=no. of substitution errors, I=no. of insertion errors.
ing the setup described above using a 5-event top-level tran- Since the accuracy as well as complexity of the decod-
sition graph such that each sequence had 2-5 individual events ing algorithms depend on manually set parameters (K,Σ for
and each event occurred at least 30 times in the entire set of HSPaHMM and k for HPaHSMM) we ﬁrst investigated their

                                                IJCAI-07
                                                  2565Algorithm 3 Embedded Viterbi Learning
 1: numTrain = number of training samples
     t
 2: Pc (j|i) = Probability of transitioning from state i to state j in
   channel c.
     o
 3: Pc (oi|j) = Probability of observing symbol oi in state j in
   channel c.
     d
 4: Pc (d|i) = Probability of spending a duration d in state i in chan-
   nel c.
 5: for i =1to numT rain do
 6:  jointHMM ← String together HMMs of words/events form-
     ing training sequence i.
 7:  repeat
 8:    maxPath ← State sequence of maximum probability path
       through jointHMM obtained using decoding algorithm.
         c
 9:    ni ← No. of times channel c is in state i in maxPath.
         c
10:    ni,j ← No. of times channel c transitions from state i to
       state j in maxPath.
         c,oj                                                  Figure 4: Structure of Event Simulator
11:    ni   ← No. of times oj is observed in state i channel c
       in maxPath.
         c,d
12:    ni  ←  No. of times state i in channel c spends duration
       d in maxPath.
13:    Re-estimate parameters using the following equations
         t       c   c
14:    Pc (j|i) ← ni,j /ni
         o        c,oi c
15:    Pc (oi|j) ← nj /nj
         d        c,d c
16:    Pc (d|i) ← ni /ni
17:  until convergence
18:  Split HMMs and update corresponding word/event HMMs.
19: end for

effects. To do this, we varied the parameters and ran 50 iter- Figure 5: Variation of HSPaHMM Speed(frames/sec or fps)
ations of the train-test setup described above for HSPaHMM and Accuracy(%) with a)Sigma, Beam Size=11 b)Beam Size,
and HPaHSMM for each parameter value. Figures 5-6 show Sigma=1
these variations.
  As can be seen, while increasing Σ in HSPaHMM pro-
                                                      ties; Both hands go through a complex sequence of states si-
duces a signiﬁcant drop in frame rate, it does not affect the
                                                      multaneously, each sign has distinct durations, and there is hi-
accuracy. On the other hand, increasing the beam size(K) pro-
                                                      erarchical structure at the phoneme, word and sentence level.
duces a signiﬁcant increase in accuracy at the cost of slower
                                                        We experimented with a set of 50 test sentences from a
speed. For HPaHSMM, increasing the beam size does not
                                                      larger dataset used in [Vogler and Metaxas, 1999](provided
improve accuracy. Based on these observations we ran a set
                                                      by  Dr.Vogler); the sequences were collected using a
of 50 tests comparing HSPaHMM (with Σ=1,  K  =11),               TM
                                                      MotionStar     system at 60 frames per second. Vocabu-
HPaHSMM (with  k =10) and PaHMM. Table 1 summarizes
                                                      lary is limited to 10-words; each sentence is 2-5 words long
the average accuracies and speeds. Thus, HSPaHMM pro-
                                                      for a total of 126 signs. The input contains the (x, y, z) loca-
                                                      tion of the hands at each time instant; from these we calculate
  Model         Accuracy                    Speed
  HPaHSMM       83.1%                                 the instantaneous velocities which are used as the observation
                     (N=124, D=17, S=1, I=3) 12.0     vector for each time instant.
  HSPaHMM       63.7%(N=124, D=3, S=36, I=6) 40.2
  PaHMM         4.8%(N=124, D=38, S=53, I=27) 39.1      We   model each  word  as  2-channel PaHMM    (for
                                                      HSPaHMM) or PaHSMM      (for HPaHSMM) based on the
      Table 1: Model Accuracy(%) and Speed(fps)       Movement-Hold(MH)  model  [Liddell and Johnson, 1989]
                                                      which breaks down each sign into a sequence of ”moves”
                                                      and ”holds”. During a ”move” some aspect of the hand is
duces a huge jump in performance when compared to plain changed while during a ”hold” all aspects are held constant.
PaHMM   without affecting the speed. While HSPaHMM’s  The MH model also identiﬁes several aspects of hand con-
accuracy is still lower than HPaHSMM, it is 3 times faster ﬁguration like location (chest, chin, etc), distance from body,
and thus serves as a good mean between HPaHSMM and    hand shape, kind of movement (straight, curved, round) etc.
PaHMM.                                                With these deﬁnitions, we can encode the signs for various
                                                      words in terms of constituent phonemes. For example, in
5.2  Application to Continuous Sign Language          the word ”I”, a right-handed signer would start some dis-
     Recognition                                      tance from his chest with all but his index ﬁnger closed and
Sign language recognition, besides being useful in itself, pro- end at the chest. This can be encoded in the MH model
vides a good domain to test hierarchical multi-agent activi- as (H(p0CH)M(strT oward)H(CH)), where p0CH indi-

                                                IJCAI-07
                                                  2566