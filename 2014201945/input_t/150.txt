                 Gaussian Process Models of Spatial Aggregation Algorithms 

                      Naren Ramakrishnan                                  Chris Bailey-Kellogg 
                Department of Computer Science                     Department of Computer Sciences 
                 Virginia Tech, VA 24061, USA                      Purdue University, IN 47907, USA 
                         naren@cs.vt.edu                                   cbk@cs.purdue.edu 

                        Abstract 

     Multi-level spatial aggregates are important for 
     data mining in a variety of scientific and engineer•
     ing applications, from analysis of weather data (ag•
     gregating temperature and pressure data into ridges 
     and fronts) to performance analysis of wireless sys•
     tems (aggregating simulation results into config•
     uration space regions exhibiting particular perfor•
     mance characteristics). In many of these applica•
     tions, data collection is expensive and time con•
     suming, so effort must be focused on gathering 

     samples at locations that will be most important                                        SNRl,dB 
     for the analysis. This requires that we be able to 
     functionally model a data mining algorithm in or•         Figure 1: Mining configuration spaces in wireless system 
     der to assess the impact of potential samples on          configurations. The shaded region denotes the largest por•
     the mining of suitable spatial aggregates. This pa•       tion of the configuration space where we can claim, with 
     per describes a novel Gaussian process approach           confidence at least 99%, that the average bit error probability 
     to modeling multi-layer spatial aggregation algo•         (BEP) is acceptable for voice-based system usage. Each cell 
     rithms, and demonstrates the ability of the resulting     in the plot is the result of the spatial and temporal aggregation 
     models to capture the essential underlying qualita•       of hundreds of time-consuming wireless system simulations. 
     tive behaviors of the algorithms. By helping cast 
     classical spatial aggregation algorithms in a rigor•
     ous quantitative framework, the Gaussian process          the base station uses two transmitter antennas separated by a 
     models support diverse uses such as directed sam•         small distance. If the signal from one of the antennas is weak, 
     pling, characterizing the sensitivity of a mining al•     the signal from another is likely to be high, and the over•
     gorithm to particular parameters, and understand•         all performance is expected to improve. In this application, 
     ing how variations in input data fields percolate up      it is important to assess how the power-imbalance between 
     through a spatial aggregation hierarchy.                  the two branches impacts the BEP of the simulated system, 
                                                               across a range of SNRs (see Fig. 1; [ Verstak et al, 2002]). 
1 Introduction                                                   Characterizing the performance of WCDMA systems re•
                                                               quires the identification of multi-level spatial aggregates in 
Many important tasks in data mining, scientific computing,     the high-dimensional configuration spaces of wireless sys•
and qualitative modeling involve the successive and system•    tems. The lowest level (input) contains individual Monte 
atic spatial aggregation and redescription of data into higher- Carlo simulation runs providing unbiased estimates of BEPs. 
level objects. For instance, consider the characterization of  This space is high-dimensional (e.g. 10), owing to the mul•
WCDMA (wideband code-division multiple access) wireless        titude of wireless system parameters (e.g. channel models, 
system configurations for a given indoor environment. In       fading characteristics, coding configurations, and hardware 
noisy channels, the performance goal is to quantitatively as•  controls). Wireless design engineers prefer to work in at most 
sess the relationship between the signal-to-noise ratio (SNR)  two or three dimensions (e.g. to study the effect of power im•
and the bit error rate (BER) or bit error probability (BEP)    balance on system performance) for ease of tunability and de•
of the realized configuration. To improve performance in of•   ployment. The next level of spatial aggregation thus contains 
fice environments (characterized by doorways, walls, cubi•     buckets which aggregate data in terms of two dimensions, us•
cles), a common trick used is to incorporate space-time trans• ing various consistency constraints and design specifications. 
mit diversity (STTD). Instead of a single transmitter antenna, Finally, the third level aggregates buckets into regions of con-


QUALITATIVE REASONING                                                                                               1045 strained shape; the shape of the regions illustrates the nature grate the two approaches in order to achieve our goal of prob•
of joint influence of the two selected configuration parame•  abilistically modeling spatial data mining algorithms. We il•
ters on performance. Specific region attributes, such as width, lustrate this ability within the context of identifying pockets 
provide estimates for the thresholds of sensitivity of configu• underlying the gradient in a field — an application that cap•
rations to variations in parameter values.                    tures many of the interesting characteristics of more complex 
  The results of such mining are important for both qualita•  studies like the wireless application. 
tive and quantitative analysis. For instance, when the average 
SNRs of the two branches are equal, the BEP is minimal and    2 Spatial Aggregation 
the width of the mined region in Fig. 1 depicts the largest ac•
ceptable power imbalance (in this case, approximately 12dB).  The Spatial Aggregation Language (SAL) [Bailey-Kcllogg et 
However, the width is not uniform and the region is narrower  al, 1996; Yip and Zhao, 1996J, provides a set of operators 
for smaller values of the SNRs. The qualitative result is that and data types, parameterized by domain-specific knowledge, 
system designs situated in the lower left corner of the con•  for uncovering and manipulating multi-layer geometric and 
figuration space are more sensitive to power imbalance in the topological structures in spatially distributed data. SAL ap•
two branches.                                                 plications construct increasingly abstract descriptions of the 
  Each input data point captures the results of a wireless sys• input data by utilizing knowledge of physical properties such 
tem simulation which takes hours or even days (the simula•    as continuity and locality, expressed with the vocabulary of 
tions in Fig. 1 were conducted on a 200-node Beowulf cluster  metrics, adjacency relations, and equivalence predicates. To 
of workstations). Thus it is imperative that we focus data col• understand the SAL approach (see Fig. 2), consider a SAL 
lection in only those regions that are most important to sup• program for analyzing flows in a vector field (e.g. wind ve•
port our data mining objective, viz. to qualitatively assess the locity or temperature gradient). 
performance in configuration spaces. This requires that we       In the first level, the goal is to group input vectors (a) into 
model the functioning of the data mining algorithm, in order  paths so that each sample point has at most one predeces•
to optimize sample selection for utility of anticipated results. sor and at most one successor. SAL breaks the process into 
Modeling data mining algorithms in this manner is useful for  two key steps, one capturing locality in the domain space (i.e. 
closing the loop, characterizing the effects of the data mining sample location), and the other capturing similarity in the 
algorithm's parameters, and improving our understanding of    feature space (i.e. vector direction). A neighborhood graph 
how variations in data fields percolate up through the layers. aggregates objects with a specified adjacency predicate ex•
A particularly interesting application is to use such modeled pressing the notion of locality appropriate for a given domain. 
structures to design information-theoretic measures for eval• As shown (b), a sample point's neighbors include all other 
uating experimental designs [MacKay, 1992] and for active     points within some specified radius r. Feature comparison 
data selection [Cohn et al, 1996; Denzler and Brown, 2002J.   then must consider only neighbors in this graph, thereby ex•
  In order to address these goals, this paper develops a novel ploiting physical knowledge to gain computational efficiency 
Gaussian process approach to modeling algorithms that mine    while maintaining correctness. Here we break feature com•
spatial aggregates. We first overview the Spatial Aggregation parison into a sequence of predicates and graph operations. 
mechanism for spatial data mining and the Gaussian process    In particular, we first filter the graph (c), applying a predicate 
approach to Bayesian modeling. We then show how to inte•      that keeps only those edges whose direction is similar enough 


1046                                                                                         QUALITATIVE REASONING  (within some angle tolerance 0) to the directions of the vec• based technique, since the estimated parameters only describe 
tors at the endpoints. The remaining graph has some "junc•     the underlying covariance function of a stochastic process. 
 tion" points where vector direction suggests multiple possible Thus, predictions of the response variable for new sample 
 neighbors, and the most appropriate path extension from the   points are conditionally dependent on the measured values 
point must be chosen. A similarity metric sums the distance    and their sample points; by unrolling the effect of the pa•
 between the junction and a neighbor, weighted by a constant   rameters of the random process, we can directly express this 
d, and the difference in vector direction at the junction and  dependency. 
the neighbor. The most similar neighbor for the junction is      Kriging is often motivated as a local modeling technique, 
selected ((d) and (e), for successor and predecessor junctions, capable of approximating or interpolating functions with mul•
respectively).                                                 tiple local extrema, and generalizes well to applications ex•
   The remaining graph edges are collected and redescribed     hibiting anisotropics and trends. The stochastic prior is also 
as more abstract streamline curve objects (f), for the second  viewed as a mathematically elegant mechanism to impart any 
 level of analysis. Again, computation is localized so that    available domain knowledge to the modeling technique. In 
only neighboring streamlines are compared. The neighbor•       1989, Sacks et al. [Sacks et al, 1989] showed how krig•
 hood graph here (not shown) uses an adjacency predicate that  ing can actually be used to model processes with determin•
declares streamlines neighbors if their constituent points were istic outcomes, especially in the context of computer exper•
 in the first level. It is then straightforward to identify conver• iments. The justification for modeling a deterministic code 
gent flows (g) with an equivalence predicate that tests when   as a stochastic process is often that even though the re•
constituent points form a junction in the graph in (c). If de• sponse variable is deterministic, it may 'resemble the sam•
sired, these flow bundles can be abstracted and analyzed at an ple path of a suitably chosen stochastic process' [Sacks et 
even higher level.                                             al., 1989]. Alternatively, using a stochastic process prior can 
   SAL's uniform spatial reasoning mechanism, instantiated     be viewed as a Bayesian approach to data analysis [Sivia, 
with appropriate domain knowledge, has proved success•         19961, and this is the idea emphasized by most recent com•
ful in applications ranging from decentralized control de•     puter science research in Gaussian processes [Gibbs, 1997; 
sign iBailey-Kellogg and Zhao, 1999; 2001 J, to weather data   Rasmussen, 1996]. The stochastic process can be suitably 
analysis [Huang and Zhao, 1999], to analysis of diffusion-     formulated to ensure that the model reproduces the same re•
reaction morphogenesis [Ordonez and Zhao, 2000]. Recent        sponse value for repeated invocations of a given sample input 
work has focused on optimizing sample selection for applica•   (i.e., absence of random error). For instance, the Gaussian 
tions where data collection is expensive, including identifying prior can be chosen so that the diagonal entries of the covari•
flows in multi-dimensional gradient fields [Bailey-Kellogg     ance matrix are 1, meaning that the model should interpolate 
and Ramakrishnan, 2001 ] and analyzing matrix properties via   the data points. 
perturbation sampling [Ramakrishnan and Bailey-Kellogg,          In the recent past, Gaussian processes have become popu•
2002]. This paper provides the mathematical foundations        lar in the statistical pattern recognition community [MacKay, 
necessary for the modeling of such SAL programs to support     1997] and graphical models literature [Jordan (ed.), 1998]. 
the meta-level reasoning tasks outlined in the introduction.   Neal established the connection between Gaussian processes 
                                                               and neural networks with an infinite number of hidden 
3 Gaussian Processes                                           units [Neal, 1996]. Such relationships allow us to take tradi•
                                                               tional learning techniques and re-express them as imposing a 
Gaussian processes have become popular in the last few         particular covariance structure on the joint distribution of in•
years, especially as a unifying framework for studying mul•    puts. For instance, we can take a trained neural network and 
tivariate regression [Rasmussen, 1996], pattern classifica•    mine the covariance structure implied by the weights (given 
tion [Williams and Barber, 1998], and hierarchical model•      mild assumptions such as a Gaussian prior over the weight 
ing [Menzefricke, 2000]. The underlying idea can be traced     space). Williams motivates the usefulness of such studies and 
back to the geostatistics technique called kriging [Journel    describes common covariance functions [Williams, 1998]. 
and Huijbregts, 1992], named after the South African miner       Williams and Barber [Williams and Barber, 1998] describe 
Danie Krige. In kriging, the unknown function to be modeled    how the Gaussian process framework can be extended to clas•
(e.g., ozone concentration) over a (typically) 2D spatial field sification, where the modeled variable is categorical. Essen•
is expressed as the realization of a stochastic process. A prior tially, the idea is to (i) use a logistic function to conduct tradi•
is placed over the function space represented by this stochas• tional Gaussian regression modeling, and (ii) adopt a softmax 
tic process, by suitably selecting a covariance function. Given function to bin the logistic output into a given set of classes. 
measured function values at sample locations, kriging then     This means that the logistic function uses a "latent variable" 
proceeds to estimate the parameters of the covariance func•    as input in its computation, since its values are not provided 
tion (and any others pertaining to the random process). Using  by the dataset. 
such values a prediction of the response variable can then be 
made for a new sample point, typically using MAP or ML in•     4 Gaussian Processes for Spatial Aggregation 
ference. This basic approach is still popular in many tasks of 
spatial data analysis.                                         SAL programs construct multi-layer spatial aggregates based 
   Even though parameters are estimated in this approach, it   on specified local adjacency relations, similarity metrics, and 
is important to note that kriging is fundamentally a memory-   consistency checks. We describe here how to capture the 


QUALITATIVE REASONING                                                                                               1047                                                               determine the extent and stringency of this neighborhood re•
                                                              lation — one of the defining parameters of a SAL program. 
                                                              Specifically, we posit a process such as: 

                                                                                                                      (1) 
                                                              The idea then is to estimate a model of the same 
                                                              form as /, on the basis of a given set of observations 
                                                                                                A typical choice for Z in 
                                                                 is a random process with zero mean and covariance 
                                                              where scalar is the estimated variance and is a matrix 
                                                              that captures the correlation between the inputs (i.e., the given 
                                                              locations). Notice that even though the input is one dimen•
                                                              sion, the size of R depends on the number of locations for 
                                                              which gradient measurements are available. The above model 
                                                              for also includes the constant term this can be estimated 
                                                              based on the k observations, or we can substitute more com•
                                                              plex terms (e.g. linear), or even omit it altogether. 
Figure 3: Modeling the reversal of gradients in a ID field us•   The functional form of R (including its parameterization) 
ing Gaussian processes, (top) Original field, (bottom) Given  in effect defines the stochastic process and must be carefully 
measured values of gradient vector angles at specific data    chosen to reflect the underlying data's fidelity or any domain-
points (blue), the model posits that the conditional distribu• specific assumptions about local variation. The parameters of 
tion of the angle at unseen data points is a Gaussian (shown  the process are then estimated using multidimensional opti•
in red).                                                      mization involving a suitable objective function. For instance, 
                                                              given the following form for /?: 

qualitative behaviors of such aggregates using Gaussian pro•                                                          (2) 
cesses. The essence of a Gaussian process is its covariance 
structure, so we focus on determining covariance structures   the problem reduces to estimating from the given data. 
in a SAL program. For example, in the two-layer SAL pro•      Notice that this formulation for R implicitly enforces that 
gram of Sec. 2, the parameters (r, 0, d) impose a covariance  the model exactly interpolate the given data points, since 
structure by specifying the reach of the neighborhood graph,                  A common objective function for estimating 
enforcing the similarity of angles in the vector field, and pe• is to minimize the mean squared error (MSE), 
nalizing for the distance at decisions involving junctions.   between and . The p that minimizes MSE is given by the 
                                                              solution to the optimization problem: 
4.1 Covariance Structure 
We now describe how to model the covariance structure of a                                                            (3) 
given SAL program. We give the mathematical framework 
for the case of mining a 1D field to determine if there is a re• where R is the symmetric correlation matrix formed from 
versal of gradient as we move along the spatial dimension, but For a new sample point a prediction for the regressed 
essentially the same machinery applies to two and higher di•  variable is given by: 
mensional spaces. The basic problem is one of classifying ID 
points to determine the qualitative structure of same-direction                                                       (4) 
flows. Fig. 3 (top) depicts the given input field along the x di•
                                                              where r is the correlation vector between the response at 
mension. As is shown, the field consists of unit vectors with 
                                                              and all the other points (derived from /?), is the identity 
different orientation. The Gaussian process approach is first 
                                                              vector of dimension and « is the estimate of a given by: 
to model an underlying regressed variable and then to use a 
logistic or softmax function to bin the output into classes. In                                                      (5) 
our application, the regressed variable represents the gradient 
and can be simply summarized as the angle of unit vector ori• The variance in the estimate is given by: 
entation y in Fig. 3 (top). In other applications, the regressed 
variable could be an unobserved 'latent' variable. In either                                                          (6) 
case, it is modeled as a function / of the input x. 
  First, assume / to be a Gaussian process on x, meaning that In this case, the optimization is one-dimensional due to the 
the conditional probability distribution of y given a value of x presence of the single parameter With a different param•
is a Gaussian. For instance, Fig. 3 (bottom) depicts measured eterization, we will employ multi-dimensional optimization 
values of y superposed with distributions of y at two unseen  over the entire set of hyperparameters. When dimensional•
points. A covariance structure among the y values could, for  ity is large, the hyperparameters are estimated using MCMC 
example, capture the intuition that adjacent values of y should methods. Once such a modeling is complete, as discussed in 
agree more than distant values. The goal of modeling is to    the previous section, we can relate a categorical class variable 


1048                                                                                         QUALITATIVE REASONING to y using softmax functions. For instance, the reversal of the 
gradient in Fig. 3 can be captured by first using the Gaussian 
process model to make predictions of the gradient at untested 
points and then determining if (and where) a zero crossing 
occurs. 
   The above equations extend naturally to a 2D case such as 
that described in Sec. 2. The covariance prior has to be suit•
ably parameterized and we also have the option of taking into 
account any interactions between the two dimensions (both 
 linear and nonlinear). 
                                                                             Figure 4: A 2D pocket function. 
4.2 Modeling Many Layers 
When SAL programs consist of many layers, we need to de•       neighborhood calculations), the underlying vector fields must 
velop a sequence of Gaussian process models, each with a       also be similar in direction. Expressing the covariance in 
suitable covariance function, which can then be superposed     terms of position alone can cause the resulting estimated hy-
to yield a composite covariance function. Recall that while    perparameters to be misleading or difficult to interpret, as 
one could simply assess the covariance of the output field for their effect is confounded with the underlying vector field. 
sample values of the parameters and a given input field, the   One solution is to artificially inflate the dimensionality, so 
real purpose of a Gaussian process model is to express the co- that position and direction together describe the data. Besides 
variance of the output as a function of the characteristics of increasing the dimensionality, this approach spells trouble for 
the input. This is the key property that allows reasoning about estimation using MCMC methods since significant portions 
closing the loop and selecting optimal samples. In addition,   of the sample space will remain unsampled and it would be 
Gaussian process models help capture the randomness inher•     difficult to assess their effects on the minimized functional. 
ent in some of SAL's computations, e.g. non-determinism in     An alternative solution is to use the fact that the vector field 
labeling, and variations due to how ties are broken for aggre• is itself a surrogate and add a term to the covariance outside 
gation purposes. Refer again to Sec. 2 for an example of the   the above structure, capturing the contribution due to similar•
types of operations that the covariance model must capture.    ity in the vector field. We place a Gamma prior on this term 
   At the very bottom of the hierarchy is the input data field. with a shape parameter that ensures that its role is secondary 
For applications characterized by expensive data collection    to the covariance structure on position (directional similar•
(as in the introduction), it can be advantageous to start with a ity alone is not enough for high covariance at the output; the 
sparse set of sample data. The Gaussian modeling approach      sample locations also must be spatially proximate). This is 
to regression is ideal for creating surrogate representations of recognized in the statistics community as a hierarchical prior 
data fields from such a sparse dataset. That is, given a sparse and described in detail in [Neal, 1997]. 
set of samples, interpolate a dense field satisfying those val•
ues and incorporating any appropriate domain knowledge, as     5 Experimental Results 
discussed above regarding kriging. Such surrogate functions 
can then been used as the starting points for qualitative anal• In order to test our approach, we studied de Boor's pocket 
ysis [Bailey-Kellogg and Ramakrishnan, 2001].                  function (see Fig. 4): 
   The operators in a SAL level deal with both locality (which 
object locations are close to which other ones, as encapsu•
lated in a neighborhood graph) and similarity (which object 
features are close to which other ones, as encapsulated in met•
rics and predicates). For instance, in the example of Sec. 2, 
two points are assigned to the same pocket if they are spa•
tially proximate and their flows converge. Here the Gaussian   where X is the n-dimensional point at which 
process is classification (or more generally, density estima•  the pocket function is evaluated, I is the identity n-vector, 
tion). A popular covariance structure for an n-dimensional     and is the L2 norm. This function exploits the fact that 
input field captures locality:                                 the volume of a high dimensional cube is concentrated in its 
                                                               corners and p is designed so that it has a "dip" in each corner. 
                                                        (7)    It embodies many aspects of datasets like those encountered 
                                                               in the wireless simulation study, including multiple local ex-
                                                               trema, non-systematic variation in the location of the pockets, 
where the expression relates the function values at positions  and regional variation. The pocket function is also important 
     and then the covariance function will                     as a benchmark for high-dimensional data exploration, where 
be positive definite, satisfying the normalization constraints the goal is to identify the most interesting regions of the de•
of a posteriori inference.                                     sign space without necessarily conducting a (costly) global 
   To see how to capture similarity, consider when two sam•    optimization over the entire design space. Data mining pro•
ple locations are classified into the same trajectory in Sec. 2. grams are hence required to identify the most promising re•
In addition to being spatially proximate (as inferred by SAL's gions using as few function evaluations as possible. 


QUALITATIVE REASONING                                                                                               1049 