                  Using Linear Programming for Bayesian Exploration in
                                    Markov Decision Processes
                               Pablo Samuel Castro and Doina Precup
                                           McGill University
                                      School of Computer Science
                                     {pcastr,dprecup}@cs.mcgill.ca

                    Abstract                          which are based on heuristics that help the agent get data
                                                      while trying to protect it from “danger”, e.g. [Thrun, 1992;
    A key problem in reinforcement learning is ﬁnd-   Meuleau and Bourgine, 1999].  Recently, several algo-
    ing a good balance between the need to explore the rithms have been proposed which carry guarantees in terms
    environment and the need to gain rewards by ex-   of the number of samples necessary for the agent to at-
    ploiting existing knowledge. Much research has    tain almost optimal performance [Kearns and Singh, 1998;
    been devoted to this topic, and many of the pro-  Brafman and Tennenholtz, 2001; Strehl and Littman, 2005;
    posed methods are aimed simply at ensuring that   Strehl et al., 2006].
    enough samples are gathered to estimate well the    In this paper we explore a Bayesian approach to explo-
                              [
    value function. In contrast, Bellman and Kal-     ration. The initial idea was due to Bellman [1959] who sug-
            ]
    aba, 1959 proposed constructing a representation  gested keeping information about the agent’s current state of
    in which the states of the original system are paired knowledge, in addition to the model being learned by the
    with knowledge about the current model. Hence,    agent. This method is Bayes-optimal from the point of view
    knowledge about the possible Markov models of     of decision making, but its complexity grows exponentially
    the environment is represented and maintained ex- with the horizon considered. This makes it inapplicable ex-
    plicitly. Unfortunately, this approach is intractable cept for the case of bandit problems, where it gives rise to
    except for bandit problems (where it gives rise to Gittins indices. Recently, a body of work on Bayesian rein-
    Gittins indices, an optimal exploration method). In forcement learning has developed method for approximating
    this paper, we explore ideas for making this method the Bayes-optimal procedure for the case of general MDPs
    computationally tractable. We maintain a model of [Dearden et al., 1999; Duff, 2002; Wang et al., 2005].In
    the environment as a Markov Decision Process. We  this paper, we pursue a similar idea, but with two important
    sample ﬁnite-length trajectories from the inﬁnite differences. First, we propose to use linear programming in
    tree using ideas based on sparse sampling. Find-  order to approximate the value function during this proce-
    ing the values of the nodes of this sparse subtree dure. Second, we use a sampling approximation based on the
    can then be expressed as an optimization problem, value function to expand the horizon of the decision making
    which we solve using Linear Programming. We il-   process.
    lustrate this approach on a few domains and com-    The paper is organized as follows. In Section 2 we present
    pare it with other exploration algorithms.        the necessary background. In Section 3 we present the lin-
                                                      ear programming approach to this problem. In Section 4 we
1  Introduction                                       present empirical results in three domains.
A key problem in reinforcement learning is posed by the
need to explore the environment sufﬁciently in order to dis- 2 Background
cover the sources of reward, while at the same time exploit- A ﬁnite Markov Decision Process (MDP) is a tuple
ing the knowledge that the agent has already, by taking ac- S, A, T , R,whereS is the ﬁnite state space, A is the ﬁnite
tions that yield high return. The most popular techniques action space, T : S×A×S →deﬁnes the transition prob-
in the current literature (e.g.,-greedy or Boltzmann explo- abilities, and R : S×A→is the reward function, bounded
ration) are not aimed at efﬁcient exploration; instead, they by RMAX. If the agent is in state s ∈Sand performs action
ensure that action choices are randomized, which enables a ∈A, T (s, a, ·) is the distribution over next possible states
the agent to try out all actions in all states. This approach and R(s, a) is the expected immediate reward. A determinis-
guarantees the convergence of reinforcement learning algo- tic stationary policy π : S→Ais a function that determines
rithms, but is not efﬁcient from the point of view of the what action to take depending on the current state. One of
number of samples gathered, and may cause the agent to re- the goals of reinforcement learning is to ﬁnd the policy that
peatedly try harmful actions. An extensive line of research maximizes the expected discounted return received, given by
                                                        ∞    t−1
has been devoted to directed exploration methods, most of t=1 γ rt,wherert is the reward received at time step t

                                                IJCAI-07
                                                  2437and γ ∈ (0, 1) is a discount factor. The value of a state V π(s) i; all parameters for other states do not change. Assuming
is given by the expected discounted return received when ex- that there are a ﬁxed number of rewards R that can be ob-
ecuting policy π starting from state s:               served, each hyperstate in the HMDP has at most |A|×|S|×R
                                                    successors. Moreover, each successor hyperstate is uniquely
                          ∞
               π               t−1                    indexed. Hence, the HMDP is an inﬁnite tree. Note also that
              V  (s)=Eπ      γ   rt             (1)
                                                      the HMDP takes into account all possible transitions. Hence,
                          t=1
                                                      it is fully known, even though the MDP itself is not.
Note that the upper bound on the value of any policy from One can express the Bellman optimality equations for an
                                                      HMDP as:
any state is RMAX/(1 − γ)                                            8                                9
                                                                     <       X                        =
                                                              ¯        ¯ a       a         a      a ¯
                                                       V (i, M, R)=max Ri + γ   p¯ij (M)V (j, Dij (M),Dij (R))
2.1  Hyperstate MDPs                                              a∈A :                               ;
                                                                              j
In this paper we consider MDPs in which the transition prob-
abilities T and rewards R are unknown. In this situation, Solving this set of equations exactly would yield an optimal
the agent maintains an estimate of the MDP model based policy [Bellman and Kalaba, 1959; Martin, 1967].However,
on prior knowledge and its observations. For the transition it is clear that the number of states in the HMDP is inﬁnite, so
probabilities, it is convenient to use a distribution that is an exact solution is intractable. It is also clear that the num-
closed under updates performed after observing a state tran- ber of hyperstates increases exponentially with the depth of
sition. For MDPs with ﬁnite state and action sets, the es- the tree, so an exact solution of a ﬁnite-horizon subset of the
timate of the transition probabilities can be maintained in a tree would be limited to very shallow depths. The focus of
matrix P of size |S| × |A| × |S|.Asshownin[Martin,    this paper is to present an algorithm for computing an em-
1967], the Matrix Beta distribution is closed under updates pirically good learning policy from a ﬁnite subset of the tree.
and is a natural choice of distribution for this case. We de- Theoretical guarantees will be left for future work and we
               a
note by M =  mij  the current indexing parameters of the will focus on comparing the performance of our algorithm to
                   a
distribution, where mij is simply the number of observed other well-known exploration techniques.
transitions from state i to state j under action a.Theex-
                                                      2.2  Related Work
pected transition probabilities based on this model are given
   E  pa  =¯pa  =  ma /   |S| ma                      Optimal learning has seen an increase in interest in recent
by    ij    ij     ij   k=1  ik. Letting the matrix
       k                                              years. Much of the recent work has been spurred by [Kearns
N =   n   denote an additional number of observed transi-                                       3
       ij                                             and Singh, 1998], where the authors introduce E , an algo-
tions from state i to state j under action a,in[Martin, 1967]
                                  P                  rithm that is guaranteed to converge to an optimal learning
it is shown that the posterior distribution with parameters policy in polynomial time. The drawbacks with this algo-
M  = M  +  N is also a Matrix Beta distribution. In what
                       Da (M)                         rithm are its difﬁculty of implementation, and the intuition
follows, we will denote by ij the new distribution ob- that it does not necessarily scale well to large state spaces. A
tained from M, through this operation, after observing on
                 i       j           a                practical model-based algorithm with similar guarantees was
transition from state to state under action .                 [                           ]
                                                 a    given in Brafman and Tennenholtz, 2001 . More advanced
  The estimated reward received when performing action
                                  a       |S| a a     model-based algorithms with PAC guarantees, based on real-
from state i can then be estimated by r¯i = j=1 p¯ij r˜ij , time dynamic programming, have been recently proposed in
      a
where r˜ij is the average reward obtained when going for state [Strehl and Littman, 2005; Strehl et al., 2006].Thesemeth-
i to state j under action a. These rewards can be summarized ods explore by assigning a reward for exploratory actions.
          ¯
in a matrix R.                                        The drawback of both [Kearns and Singh, 1998] and [Strehl
  The Matrix Beta distribution can be viewed as a summary et al., 2006] is that their strategies are myopic - they do not
of the information that the agent has accumulated so far. This consider the long term effects their actions may have on the
information, together with the original MDP states and ac- total reward.
tions, will form a new, Hyperstate-MDP (HMDP). The ac-  Using hyperstates as the model for exploration overcomes
tions in the HMDP are the same as in the original MDP. The this myopic behavior. The concept of hyperstates was ﬁrst
states of the HMDP consist of pairings of states from S with introduced in [Bellman and Kalaba, 1959], which refers to
possible information states M. A hyperstate (i, M, R¯) con- this model as an adaptive control process. Although math-
tains an MDP state i, all the counts summarizing the agent’s ematically rich, the paper presents no algorithmic approach
experience so far M, and all the estimates of the expected for this idea. In [Duff, 2002], various heuristic methods are
rewards R¯. The counts deﬁne a distribution over all MDP presented for approximating an exact solution to the adaptive
models consistent with the data observed so far.      control process. These produce empirically good results, but
  At any given time step when action a is taken in the orig- with no general theoretical guarantees.
inal MDP from state i, precisely one transition is observed, Wang et al [2005] propose sampling from the inﬁnite hy-
to some state j. So, from any given hyperstate (i, M, R¯), pertree to produce a small, more manageable tree. They solve
we can consider taking all possible actions a ∈A.Ifsuch exactly the MDPs along the path they are sampling, which
an action results in a state j, the new hyperstate will be gives them estimates for the values of different actions. Then,
    a       a  ¯
(j, Dij (M),Dij (R)), where we have updated the innforma- they use Thompson sampling to expand the tree locally under
tion model as described above to reﬂect the new transition. promising actions. The authors estimate unsampled regions
We note that this update only affects the parameters for state by effectively ‘copying’ the last sampled node down to the

                                                IJCAI-07
                                                  2438current planning horizon. Finally, sparse sampling [Kearns et corresponding MDP state). If such a value has not been com-
al., 1999] is used to decide what action is optimal. A correc- puted, the action-value function for the MDP state is used
tion procedure is used when the desired horizon has not been instead. Note that any unsampled regions constitute a sub-
reached. Empirical results demonstrate that the algorithm tree of our hyperstate MDP. These unsampled subtrees are
performs well in comparison to other algorithms. Further- estimated by setting the value of the hyperstate at the root
more, the sparse sampling technique enables the algorithm to of the subtree, (i, M, R) to the value of the correspondinng
be able to handle inﬁnite state and action spaces.    MDP state i. With this method, we can effectively choose
                                                      how many variables and constraints we want to use in the lin-
3  Solving the hyperstate MDP                         ear program.
                                                        Once the sampled hyper tree is built, we can solve it using
In this paper we take an approach similar to [Wang et al., linear programming. Then, in order to gather more samples,
2005] but with two key differences. First, action-value esti- we will choose action greedily based on the values computed
mates are maintained for the state-action pairs of the origi- by the linear program. If we enter an unsampled region, the
nal MDP, in addition to hyperstate values. The action-values action choice becomes greedy with respect to the value esti-
are updated from the obtained samples using standard Q- mates attached to the original MDP states, until we decide to
learning [Sutton and Barto, 1998]. Hence, we always have construct a new hyper tree and compute new values.
an easy estimate of how good different actions are. Second,
we compute the value of the hyperstates using Linear Pro-             levels, maxSamples, numSteps, maxEpochs
gramming. Linear Programming (LP) is a technique that has Algorithm 1 Explore(                          )
been around for a long time, is well understood and has an el- 1: Initialize the matrix of counts to all 1s (uniform distribu-
           [             ]                                tion)
egant theory Puterman, 1994 . Furthermore, recent work in      epochs ≤ maxEpochs
                            [                          2: while                    do
approximate linear programming de Farias and Roy, 2003;                                       levels
                              ]                        3:   Construct a hyper-tree of a depth of    using
2004; Hauskrecht and Kveton, 2004 ) suggests that this ap-  maxSamples
proach can be used to work with continuous states using lin-            sampled trajectories
ear function approximation. We are hopeful that such tech- 4: Solve the LP of the sample hypertree
niques could eventually be used to generalize our approach to 5: Choose action greedily based on hyperstate values
continuous MDPs. However, for now we limit ourselves to 6:  Observe transition and update hyper parameter for ob-
                                                            served transition
discrete MDPs.                                                 i =1   numSteps
  In a manner similar to [Schweitzer and Seidmann, 1985], 7: for    to          do
the optimality equations for the hyperstate MDP can be for- 8: Choose action greedily based either on current hy-
mulated as a linear program as follows:                       perstate value or current estimate of state values
                                                      9:     Observe transition and update hyper parameters
            minimize        V (i, M, R¯)                      based on the observed transition
                          i                           10:   end for
                                                      11:   epochs ← epochs + numSteps1
such that                                             12: end while
           "                                  #
                  X
       ¯    ¯ a       a          a       a ¯
V (i, M, R)− Ri + γ  p¯ij (M)V (j, Dij (M),Dij (R)) ≥ 0 Algorithm 1 presents an outline for our approach. The
                   j                                  maxEpochs   parameter deﬁnes how many total samples will
                                                      be gathered from the environment. The levels parameter de-
for all states i ∈S, all actions a ∈Aand all information
                                                      scribes the depth to which we will expand the sampled hyper-
states M. We will refer to this formulation as the exact LP.
                                                      tree. The maxSamples  parameter controls how many tra-
However, at a depth  d, the number of hyperstates is at least
            d                                         jectories are sampled (i.e., the width of the tree). Together,
O  (|S| × |A|) (more if we consider several possible re-
                                                      these two parameters control how large each linear program
ward levels). This means that  for depth d the linear program
                      d                               is. The numSteps parameter deﬁnes how many steps in the
has at least O (|S| × |A|) variables and constraints, a pro-
                                                      environment are taken before the LP solution is recomputed.
hibitive amount.
                                                      This allows us to trade off the precision of the computation
  In [Kearns et al., 1999] the authors overcome a similar
                                                      against time. Obviously, if we re-compute the LP after every
obstacle by constructing a sparse sample tree. We apply a
                                                      sample, we always generate samples based on the action that
similar idea here and construct a sampling tree incrementally.
                                                      is truly believed to be best. If we wait longer, we may start
More precisely, we sample from the hyperstate MDP using
                                                      selecting actions based on imperfect estimates of the values
our current estimates of the action values to decide what to
                                                      of the underlying MDP. However, this speeds up the compu-
do. At each node, we use the corresponding estimate of the
                                                      tation signiﬁcantly.
dynamics of the system to sample the next node in the trajec-
tory. Each trajectory is completed to a desired horizon. We
continue constructing trajectories in this way until a maxi- 4 Empirical results
mum desired number of samples has been reached.       We tested our algorithm on three different problems. For all
  If we encounter a hyperstate for which a value was com- domains, results are averaged over 30 independent runs. We
puted already using an LP, we use the LP-based estimate compared our approach with a Q-learning agent (with α =
(which, in the limit, approaches the true optimal value of the 0.1)usingan-greedy approach (with varying values of ),

                                                IJCAI-07
                                                  2439Figure 1: Two-state, three-action MDP: dynamics (left, comparison with the other algorithm (center) and effect of parameters
(right)

and with the algorithm described in [Wang et al., 2005].We left panel for action 1 (solid), action 2 (dashed) and action
also experimented with a myopic agent (acting only based 3 (solid grey). The leftmost state corresponds to not winning,
on immediate reward), and against an agent which chooses the middle state corresponds to winning under machine 1 and
actions randomly. These last two algorithms did far worse the rightmost state corresponds to winning under machine 2.
than all the others, and so are omitted here for clarity. For our There is no cost associated with gambling.
algorithm, we experimented with different parameter settings, For the comparison between algorithms, we use parameters
as explained in more detail below. Because the primal LP has levels =6,numSteps =5,maxSamples =45. For this
more constraints than variables, the dual was solved instead. problem [Wang et al., 2005]’s algorithm was able to handle
  We also considered two different possible uses of the up to 45 samples as well. Our algorithm outperformed all
model that is constructed by our algorithm. In the ﬁrst ver- the others. In Figure 4 we can see that even with only 15
sion, once the desired number of epochs has been completed, samples, our algorithm does almost as well as the best Q-
we perform dynamic programming using the acquired model, learning algorithm.
to determine a value function for the original MDP. In the
second setting, We just use the values determined by the LP
as value estimates. Obviously, this latter approach is faster, Grid World
since no extra computation is required, but it is also more im- The third problem is a 2x4 gridWorld with a reward of +1
precise.                                              in the top right state and 0 everywhere else. There are four
                                                      actions (north, south, west and east) with a 0.1 probability
Small MDP                                             of remaining in the same state. If the agent tries to move
The ﬁrst problem was a two-state, three-action MDP, de- into a wall it will deterministically stay in the same state.
scribed in the left panel of Figure 3. Intuitively, the best pol- The parameter values used are levels =9,numSteps =
icy is to perform action 1 until state 2 is reached, then do 8,maxSamples =15.
action 3 and keep in the same state.                    This is a larger problem than the ﬁrst two, and all the other
  The center graph compares our algorithm, with parameters algorithms are ‘stuck’ with low reward, while our algorithm
levels =7,numSteps   =6,maxSamples    =35, with Q-    is able to ﬁnd a good policy. In ﬁgure ?? it is clear that
learning using three different values of  (0.01, 0.1 and 0.5) using Dynamic Programming to obtain state value estimates
and the approach by Wang et al. For all algorithms we plot is advantageous for this task.
average return per episode. Note that the implementation of It is interesting to see that in the ﬁrst two problems us-
the algorithm described in [Wang et al., 2005] was not able to ing the values returned by the LP to estimate the state val-
handle more than 35 samples, which is why we performed the
                                             =0.1    ues yields better returns initially. This is probably because of
comparison at this level. Although Q-Learning with    the ‘lookahead’ nature of HMDP. However, for the gridWorld
outperformed all the other algorithms in this graph, we can problem the performance is much better when using the DP
see that if we allow our algorithm 50 samples to construct the solution to estimate the state values. This behavior is most
hypertree, it performs better than the rest. Interestingly, in probably due to the size of the hypertree created. As the state
this case, using the values from the LP also gives better per- and action space get bigger, more samples are needed to ob-
formance than doing one last step of dynamic programming. tain state value estimates that are closer to the true values. It
                                                      should be noted that even when the DP was not solved ex-
Bandit problem                                        actly, in all cases the performance when using HMDPs was
The second problem is similar to a bandit problem with two superior to the other algorithms. We note that we also ex-
slot machines (bandits) and three actions. This problem was perimented with Boltzmann exploration algorithms (omitted
modeled using three states and three actions, with dynam- from the plots for clarity), but in all cases their performance
ics as given in ﬁgure 4. The dynamics are shown in the was similar to the Q-learning algorithms.

                                                IJCAI-07
                                                  2440   Figure 2: Bandit problem: dynamics (left), comparison of different algorithms (center) and parameter inﬂuence (right)


           Figure 3: Comparison of algorithms (left) and inﬂuence of parameters (right) in the gridworld task

Running time                                          5   Conclusions and future work
The results demonstrate that our approach has an advantage The aim of this paper was to demonstrate the performance of
compared to the other methods in terms of accuracy. How- Bayesian learning using sparse sampling and linear program-
ever, in terms of computation time, using hyperstates is obvi- ming. It was observed that in general using hyperstates does
ously much slower than Q-learning. Table 1 plots the average lead to greater average returns when exploring an unknown
running time per episode when solving the DP to estimate the environment than if we used simpler exploration techniques
state values versus using the values returned by the LP for the or even state-of-the-art techniques such as the algorithm pre-
different domains, as well as the running time of [Wang et sented in [Wang et al., 2005]. The drawback of this technique
al., 2005]’s algorithm. It is worth observing the difference in is that it may leave certain areas of the environment unex-
performance/running time depending on how the state values plored, but this could be the desired behavior. If you con-
are estimated (solving with DP or using the values returned sider a physical agent (such as a robot) placed with the task
by the LP). Although our algorithm is considerably slower of exploring a new environment, you would want the agent to
when solving with DP, it is faster than [Wang et al., 2005]’s avoid areas which could cause harm to its mechanical parts
algorithm when using the values returned by the LP solution. or even to other people around it, yet still explore sufﬁciently
However, for the gridWorld problem, although solving with to try to reach states with high reward. A situation like that
HMDP is slower than [Wang et al., 2005]’s algorithm, the would justify the increase in computation. Furthermore, the
savings when using [Wang et al., 2005]’s algorithm are in- method of sparse sampling could allow the hypertree to have
signiﬁcant when compared against the superior performance a greater depth, and thus, the agent could see states that other
demonstrated in ﬁgure 3.                              myopic methods may not be able to. This ability to value ac-
                                                      tions not only on their immediate but long-term effect makes
                     HMDP              Wang           Bayesian learning a good candidate for exploratory agents.
              Using DP    No DP                         The next step is to obtain theoretical guarantees for our
 Small MDP     0.0065    2.8408e-04  5.3850e-04       algorithm. We would also like to implement the algorithms
   Bandit      0.0094    3.3234e-04  3.4113e-04       presented in [Strehl et al., 2006] and possibly [Duff, 2002] to
 Grid world    0.0548     0.0083      0.0025          empirically compare our algorithm against more of the latest
                                                      advances in Bayesian decision theory. It would also be inter-
         Table 1: Comparison of running times         esting to try our algorithm against problems with continuous
                                                      state and/or action spaces. Placing a Gaussian over sampled

                                                IJCAI-07
                                                  2441