                          Locality Sensitive Discriminant Analysis∗
              Deng Cai                         Xiaofei He                        Kun Zhou
      Department of Computer Science        Yahoo! Research Labs             Microsoft Research Asia
  University of Illinois at Urbana Champaign hex@yahoo-inc.com        kunzhou@microsoft.com
    dengcai2@cs.uiuc.edu

                        Jiawei Han                                   Hujun Bao
                 Department of Computer Science                 College of Computer Science
              University of Illinois at Urbana Champaign           Zhejiang University
                  hanj@cs.uiuc.edu                           bao@cad.zju.edu.cn

                    Abstract                          techniques for this purpose are Principal Component Analy-
                                                      sis (PCA) and Linear Discriminant Analysis (LDA) [Duda et
    Linear Discriminant Analysis (LDA) is a popular   al., 2000].
    data-analytic tool for studying the class relation- PCA is an unsupervised method. It aims to project the data
    ship between data points. A major disadvantage of along the direction of maximal variance. LDA is supervised.
    LDA is that it fails to discover the local geometri- It searches for the project axes on which the data points of
    cal structure of the data manifold. In this paper, we different classes are far from each other while requiring data
    introduce a novel linear algorithm for discriminant points of the same class to be close to each other. Both of
    analysis, called Locality Sensitive Discriminant  them are spectral methods, i.e., methods based on eigenvalue
    Analysis (LSDA). When there is no sufﬁcient train- decomposition of either the covariance matrix for PCA or
    ing samples, local structure is generally more im- the scatter matrices (within-class scatter matrix and between-
    portant than global structure for discriminant analy- class scatter matrix) for LDA. Intrinsically, these methods
    sis. By discovering the local manifold structure, try to estimate the global statistics, i.e. mean and covari-
    LSDA ﬁnds a projection which maximizes the mar-   ance. They may fail when there is no sufﬁcient number of
    gin between data points from different classes at samples. Moreover, both PCA and LDA effectively see only
    each local area. Speciﬁcally, the data points are the Euclidean structure. They fail to discover the underlying
    mapped into a subspace in which the nearby points structure, if the data lives on or close to a submanifold of the
    with the same label are close to each other while the ambient space.
    nearby points with different labels are far apart. Ex-
    periments carried out on several standard face data- Recently there has been a lot of interest in geometri-
    bases show a clear improvement over the results of cally motivated approaches to data analysis in high dimen-
    LDA-based recognition.                            sional spaces. Examples include ISOAMP [Tenenbaum et
                                                      al., 2000], Laplacian Eigenmap [Belkin and Niyogi, 2001],
                                                      Locally Linear Embedding [Roweis and Saul, 2000].These
1  Introduction                                       methods have been shown to be effective in discovering the
                                                      geometrical structure of the underlying manifold. However,
Practical algorithms in supervised machine learning degrade they are unsupervised in nature and fail to discover the dis-
in performance (prediction accuracy) when faced with many criminant structure in the data. In the meantime, manifold
features that are not necessary for predicting the desired out- based semi-supervised learning has attracted considerable at-
put. An important question in the ﬁelds of machine learning, tention [Zhou et al., 2003], [Belkin et al., 2004].Thesemeth-
knowledge discovery, computer vision and pattern recogni- ods make use of both labeled and unlabeled samples. The la-
tion is how to extract a small number of good features. A beled samples are used to discover the discriminant structure,
common way to attempt to resolve this problem is to use di- while the unlabeled samples are used to discover the geomet-
mensionality reduction techniques. Two of the most popular rical structure. When there is a large amount of unlabeled

  ∗                                                   samples available, these methods may outperform traditional
    The work was supported in part by the U.S. National Science supervised learning algorithms such as Support Vector Ma-
Foundation NSF IIS-03-08215/IIS-05-13678, Specialized Research chines and regression [Belkin et al., 2004].However,insome
Fund for the Doctoral Program of Higher Education of China (No. applications such as face recognition, the unlabeled samples
20030335083) and National Natural Science Foundation of China may not be available, thus these semi-supervised learning
(No. 60633070). Any opinions, ﬁndings, and conclusions or recom-
mendations expressed in this paper are those of the authors and do methods can not be applied.
not necessarily reﬂect the views of the funding agencies. In this paper, we introduce a novel supervised dimension-

                                                IJCAI-07
                                                   708ality reduction algorithm, called Locality Sensitive Discrim- Besides LDA, there is recently a lot of interests in graph
inant Analysis, that exploits the geometry of the data mani- based linear dimensionality reduction. The typical algo-
fold. We ﬁrst construct a nearest neighbor graph to model the rithms includes Locality Preserving Projections (LPP, [He
local geometrical structure of the underlying manifold. This and Niyogi, 2003]), Local Discriminant Embedding (LDE,
graph is then split into within-class graph and between-class [Chen et al., 2005]), Marginal Fisher Analysis (MFA, [Yan et
graph by using the class labels. In this way, the geometri- al., 2005]), etc. LPP uses one graph to model the geometri-
cal and discriminant structure of the data manifold can be cal structure in the data. LDE and MFA are essentially the
accurately characterized by these two graphs. Using the no- same. Both of them uses two graphs to model the discrim-
tion of graph Laplacian [Chung, 1997], we can ﬁnd a linear inant structure in the data. However, these two algorithms
transformation matrix which maps the data points to a sub- implicitly consider that the within-class and between-class re-
space. This linear transformation optimally preserves the lo- lations are equally important. This reduces the ﬂexibility of
cal neighborhood information, as well as discriminant infor- the algorithms.
mation. Speciﬁcally, at each local neighborhood, the margin
between data points from different classes is maximized. 3 Locality Sensitive Discriminant Analysis
  The paper is structured as follows: in Section 2, we provide
                                                      In this section, we introduce our Locality Sensitive Discrimi-
a brief review of Linear Discriminant Analysis. The Locality
                                                      nant Analysis algorithm which respects both discriminant and
Sensitive Discriminant Analysis (LSDA) algorithm is intro-
                                                      geometrical structure in the data. We begin with a description
duced in Section 3. In Section 4, we describe how to perform
                                                      of the locality sensitive discriminant objective function.
LSDA in Reproducing Kernel Hilbert Space (RKHS) which
gives rise to kernel LSDA. The experimental results are pre- 3.1 The Locality Sensitive Discriminant Objective
sented in Section 5. Finally, we provide some concluding   Function for Dimensionality Reduction
remarks in Section 6.
                                                      As we described previously, naturally occurring data may be
                                                      generated by structured systems with possibly much fewer
2  Related Works                                      degrees of freedom than the ambient dimension would sug-
The generic problem of linear dimensionality reduction is the
                                    n                 gest. Thus we consider the case when the data lives on or
following. Given a set x1, x2, ··· , xm in R , ﬁnd a transfor-
                               n×d                    close to a submanifold of the ambient space. One hopes
mation matrix A =(a1, ··· , ad) ∈ R that maps these m
                                     d                then to estimate geometrical and discriminant properties of
points to a set of points y1, y2, ··· , ym in R (d  n),such
                               T                      the submanifold from random points lying on this unknown
that yi “represents” xi,whereyi = A xi.               submanifold. In this paper, we consider the particular ques-
  Linear Discriminant Analysis (LDA) seeks directions that tion of maximizing local margin between different classes.
                                                                                                n
are efﬁcient for discrimination. Suppose these data points be- Given m data points {x1, x2, ··· , xm}⊂R sampled
long to c classes and each point is associated with a label from the underlying submanifold M, one can build a near-
l(xi) ∈{1, 2, ··· ,c}. The objective function of LDA is as est neighbor graph G to model the local geometrical struc-
follows:                                              ture of M. For each data point xi, we ﬁnd its k nearest
                              T S
                             a   ba                   neighbors and put an edge between xi and its neighbors. Let
               aopt =argmax   T                 (1)              1      k
                          a  a Swa                    N(xi)={xi   , ··· , xi } be the set of its k nearest neighbors.
                 c                                   Thus, the weight matrix of G can be deﬁned as follows:
                        i       i    T
            Sb =    mi(μ  − μ)(μ − μ)           (2)               
                                                                     1,  if xi ∈ N(xj) or xj ∈ N(xi)
                 i=1                                        Wij =                                     (5)
                 ⎛                      ⎞                            0,  otherwise.
              c   mi
                 ⎝      i    i  i    i T ⎠
         Sw =         (xj − μ )(xj − μ )        (3)   The nearest neighbor graph G with weight matrix W char-
              i=1  j=1                                acterizes the local geometry of the data manifold. It has
                                                      been frequently used in manifold based learning techniques,
where μ is the total sample mean vector, mi is the number of
                       i                              such as [Belkin and Niyogi, 2001], [Tenenbaum et al., 2000],
samples in the i-th class, μ is the average vector of the i-th
          i                                           [Roweis and Saul, 2000], [He and Niyogi, 2003].However,
class, and x is the j-thsampleinthei-th class. We call Sw
          j                                           this graph fails to discover the discriminant structure in the
the within-class scatter matrix and SB the between-class scat-
                                                      data.
ter matrix. The basis functions of LDA are the eigenvectors
                                                        In order to discover both geometrical and discriminant
of the following generalized eigen-problem associated with
                                                      structure of the data manifold, we construct two graphs, i.e.
the largest eigenvalues:
                                                      within-class graph Gw and between-class graph Gb.Let
                    Sba = λSwa                  (4)   l(xi) be the class label of xi. For each data point xi,the
                                                         N  x                                  N  x
Clearly, LDA aims to preserve the global class relationship set ( i) can be naturally split into two subsets, b( i) and
                                                      Nw(xj ). Nw(xi) contains the neighbors sharing the same
between data points, while it fails to discover the intrinsic  x        N   x
local geometrical structure of the data manifold. In many label with i, while b( i) contains the neighbors having
real world applications such as face recognition, there may different labels. Speciﬁcally,
                                                                          j   j
not be sufﬁcient training samples. In this case, it may not be Nw(xi)={xi |l(xi )=l(xi), 1 ≤ j ≤ k}
able to accurately estimate the global structure and the local
                                                                         j   j
structure becomes more important.                             Nb(xi)={xi  |l(xi ) = l(xi), 1 ≤ j ≤ k}

                                                IJCAI-07
                                                   709              (a)                   (b)                   (c)                        (d)

Figure 1: (a) The center point has ﬁve neighbors. The points with the same color and shape belong to the same class. (b)
The within-class graph connects nearby points with the same label. (c) The between-class graph connects nearby points with
different labels. (d) After Locality Sensitive Discriminant Analysis, the margin between different classes is maximized.

Clearly, Nb(xi) ∩ Nw(xi)=∅   and Nb(xi) ∪ Nw(xi)=     By simple algebra formulation, the objective function (8) can
N(xi).LetWw   and Wb be the weight matrices of Gw and be reduced to
G                                                                  
 b, respectively. We deﬁne:                                      1           2
                                                                    (yi − yj) Ww,ij
              ,    x ∈ N   x    x  ∈ N  x                        2
   W         1   if i    b( j) or j    b( i)                       ij
     b,ij =   ,                                 (6)                              
             0   otherwise.                                      1      T      T   2
                                                             =         a xi − a xj  Ww,ij
                                                                2
             1,  if xi ∈ Nw(xj ) or xj ∈ Nw(xi)                    ij
   Ww,ij =                                      (7)                               
             0,  otherwise.                                          T        T         T         T
                                                             =      a xiDw,iixi a −    a xiWw,ijxj a
It is clear to see W = Wb + Ww and the nearest neighbor           i                 ij
     G                                                            T       T     T       T
graph  can be thought of as a combination of within-class    =   a XDwX    a − a XWwX     a
graph Gw and between-class graph Gb.
                                                            D
  Now consider the problem of mapping the within-class where w is a diagonal matrix; its entries are column
 (or row,
                                                           W                      W    D           W
graph and between-class graph to a line so that connected since w is symmetric) sum of w, w,ii =  j  w,ij.
points of Gw stay as close together as possible while con- Similarly, the objective function (9) can be reduced to
               G                              y                         
nected points of b stay as distant as possible. Let =                 1            2
 y ,y , ··· ,y T                                                           (yi − yj) Wb,ij
( 1 2      m)   be such a map. A reasonable criterion for             2
choosing a “good” map is to optimize the following two ob-               ij
                                                                                       
jective functions:                                                    1       T     T    2
                                                                 =         a  xi − a xj  Wb,ij
                             2                                        2  ij
               min    (yi − yj) Ww,ij           (8)
                                                                       T              T
                   ij                                             =   a  X(Db − Wb)X   a
                                                                      T       T
                              2                                    =  a  XLbX   a
               max    (yi − yj) Wb,ij           (9)
                    ij                                where Db is a diagonal matrix; its entries are
 column (or row,
                                                      since Wb is symmetric) sum of Wb, Db,ii = j Wb,ij . Lb =
under appropriate constraints. The objective function (8) D − W                     G
on within-class graph incurs a heavy penalty if neighboring b b is the Laplacian matrix of b.
                                                        Note that, the matrix Dw provides a natural measure on the
points xi and xj are mapped far apart while they are actu-
                                                      data points. If Dw,ii is large, then it implies that the class con-
ally in the same class. Likewise, the objective function (9) x                      x
on between-class graph incurs a heavy penalty if neighboring taining i has a high density around i. Therefore, the bigger
                                                      the value of Dw,ii is, the more “important” is xi. Therefore,
points xi and xj are mapped close together while they actu-
ally belong to different classes. Therefore, minimizing (8) is we impose a constraint as follows:
                                                                  T             T       T
an attempt to ensure that if xi and xj are close and sharing the y Dwy  =1⇒    a XDwX    a =1
same label then yi and yj are close as well. Also, maximiz-
ing (9) is an attempt to ensure that if xi and xj are close but Thus, the objective function (8) becomes the following:
                     y     y                                                   T       T
have different labels then i and j are far apart. The learning       min  1 − a XWwX    a            (10)
procedure is illustrated in Figure 1.                                  a
                                                      or equivalently,
3.2  Optimal Linear Embedding                                                T       T
                                                                       max  a XWwX     a             (11)
In this subsection, we describe our Locality Sensitive Dis-             a
criminant Analysis algorithm which solves the objective And the objective function (9) can be rewritten as follows:
                           a
functions (8) and (9). Suppose is a projection vector, that                   T      T
  yT    aT X      X     x , ··· , x    n × m                            max a  XLbX   a              (12)
is,  =      ,where   =(  1      m) is a     matrix.                      a

                                                IJCAI-07
                                                   710Finally, the optimization problem reduces to ﬁnding:    Let Φ denote the data matrix in RKHS:
                                     
                 T                      T
    arg max     a X  αLb +(1−   α)Ww  X  a     (13)               Φ=[φ(x1),φ(x2),  ··· ,φ(xm)]
          a
   T      T
  a XDw X  a =1                                       Now, the eigenvector problem in RKHS can be written as fol-
                                                      lows:
     α                       0 ≤  α ≤ 1                                         
where  is a suitable constant and      . The projec-            αL      − α W     T v   λ D    T v
tion vector a that minimizes (13) is given by the maximum    Φ     b +(1    ) w  Φ   =   Φ  wΦ       (15)
eigenvalue solution to the generalized eigenvalue problem: Because the eigenvector of (15) are linear combinations
                       
                          T             T             of φ(x1),φ(x2), ··· ,φ(xm), there exist coefﬁcients αi,i =
     X  αLb +(1−  α)Ww  X   a = λXDwX    a     (14)
                                                      1, 2, ··· ,msuch that
                    a  , a , ··· , a
Let the column vector 1  2      d be the solutions of                     m
                                              λ1 >
equation (14), ordered according to their eigenvalues,                v =    αiφ(xi)=Φα
···>λ
      d. Thus, the embedding is as follows:                               i=1
                  x →  y    AT x                                               T    m
                   i    i =     i                     where α =(α1,α2, ··· ,αm)  ∈ R  .
                A =(a1, a2, ··· , ad)                   Following some algebraic formulations, we get:
                                                                                 
                                                                                    T           T
where yi is a d-dimensional vector, and A is a n × d matrix.   Φ αLb +(1−   α)Ww  Φ  v = λΦDwΦ    v
                                                                                 
  Note that, if the number of samples (m)islessthan                                 T             T
                                                           ⇒   Φ αLb +(1−   α)Ww  Φ  Φα  = λΦDwΦ   Φα
the number of features (n), then rank(X) ≤ m. Conse-                                 
                   T                                       ⇒      T   αL      − α W     T  α
quently, rank  (XDwX ) ≤ m  and rank X(αLb  +(1−                 Φ Φ     b +(1    ) w  Φ Φ
α W   XT   ≤  m              XD   XT      X  αL                     T      T
 )  w)          . The fact that  w    and   (  b +             = λΦ  ΦDwΦ    Φα
            T                                                                     
(1 − α)Ww)X   are n × n matrices implies that both of them
                                                           ⇒   K  αLb +(1−  α)Ww   Kαα = λKDwKαα     (16)
are singular. In this case, one may ﬁrst apply Principal Com-
ponent Analysis to remove the components corresponding to where K is the kernel matrix, Kij = K(xi, xj).Letthe
zero eigenvalues.                                     column vectors α1,αα2, ··· ,ααm be the solutions of equation
                                                      (16). For a test point x, we compute projections onto the
4  Kernel LSDA                                        eigenvectors vk according to
LSDA is a linear algorithm. It may fail to discover the in-          m                  m
                                                           k             k                    k
trinsic geometry when the data manifold is highly nonlinear. (v · φ(x)) = αi (φ(x) · φ(xi)) = αi K(x, xi)
In this section, we discussion how to perform LSDA in Re-            i=1                 i=1
producing Kernel Hilbert Space (RKHS), which gives rise to  αk     ith                  αk
kernel LSDA.                                          where  i is the element of the vector . For the original
                                                      training points, the map can be obtained by y = Kαα,where
  Suppose X = {x1, x2, ··· , xm}∈Xis the training sam-    th
                                          F           the i element of y is the one-dimensional representation of
ple set. We consider the problem in a feature space induced x
by some nonlinear mapping                              i.
                    φ : X→F                           5   Experimental Results
For a proper chosen φ, an inner product ,  can be deﬁned In this Section, we investigate the use of LSDA on face recog-
on F which makes for a so-called reproducing kernel Hilbert nition. We compare our proposed algorithm with Eigenface
space (RKHS). More speciﬁcally,                       (PCA, [Turk and Pentland, 1991]), Fisherface (LDA, [Bel-
                                                      humeur et al., 1997]) and Marginal Fisher Analysis (MFA,
               φ(x),φ(y) = K(x, y)
                                                      [Yan et al., 2005]). We begin with a brief discussion about
holds where K(., .) is a positive semi-deﬁnite kernel func- data preparation.
tion. Several popular kernel functions are: Gaussian ker-
nel K(x, y)=exp(−x    −  y2/σ2); polynomial kernel  5.1  Data Preparation
K(x, y)=(1+x,      y)d; Sigmoid kernel K(x, y)=     Two face databases were tested. The ﬁrst one is the Yale data-
tanh(x, y + α).                                     base1, and the second one is the ORL database2. In all the
  Given a set of vectors {vi ∈F|i =1, 2, ··· ,d} which are experiments, preprocessing to locate the faces was applied.
orthonormal (vi, vj = δi,j ), the projection of φ(xi) ∈F Original images were normalized (in scale and orientation)
to these v1, ··· , vd leads to a mapping from X to Euclidean such that the two eyes were aligned at the same position.
space Rd through                                      Then, the facial areas were cropped into the ﬁnal image for
                                            T        matching. The size of each cropped image in all the experi-
    yi =  v1,φ(xi), v2,φ(xi), ··· , vd,φ(xi)    ments is 32 × 32 pixels, with 256 gray levels per pixel. Thus,
We look for such {vi ∈F|i   =1,  2, ··· ,d} that helps   1http://cvc.yale.edu/projects/yalefaces/
{yi|i =1, ··· ,m} preserve local geometrical and discrim- yalefaces.html
inant structure of the data manifold. A typical scenario is 2http://www.cl.cam.ac.uk/Research/DTG/
X = Rn, F = Rθ with d<<n<θ.                           attarchive/facesataglance.html

                                                IJCAI-07
                                                   711Figure 2: Sample face images from the Yale database. For each subject, there are 11 face images under different lighting
conditions with facial expression.

                     Table 1: Recognition accuracy of different algorithms on the Yale database
                      Method       2Train        3Train       4Train       5Train
                      Baseline   43.4%(1024)  49.4%(1024)  52.6%(1024)   56.2%(1024)
                     Eigenfaces   43.4%(29)    49.4%(44)    52.6%(58)     56.2%(74)
                     Fisherfaces  47.2%(10)    64.9%(14)    72.9%(14)     78.8%(14)
                       MFA        47.7%(10)    65.7%(14)    74.1%(14)     78.9%(14)
                       LSDA       56.5%(14)    68.5%(14)    74.4%(14)     79.0%(14)

each image can be represented by a 1024-dimensional vec- reaches the best performance almost always at c − 1 dimen-
tor in image space. No further preprocessing is done. Dif- sions. This property shows that LSDA does not suffer from
ferent pattern classiﬁers have been applied for face recogni- the problem of dimensionality estimation which is a crucial
tion, including nearest neighbor [Turk and Pentland, 1991], problem for most of the subspace learning based face recog-
Bayesian [Moghaddam, 2002], and Support Vector Machines nition methods.
[Phillips, 1998], etc. In this paper, we apply nearest neighbor
classiﬁer for its simplicity. In our experiments, the number 5.3 Face Recognition on ORL Database
of nearest neighbors (k) is taken to be 5. The parameter α is The ORL (Olivetti Research Laboratory) face database is
estimated by leave one out cross validation.          used in this test. It consists of a total of 400 face images,
  In short, the recognition process has three steps. First, we of a total of 40 people (10 samples per person). The images
calculate the face subspace from the training set of face im- were captured at different times and have different variations
ages; then the new face image to be identiﬁed is projected including expressions (open or closed eyes, smiling or non-
into d-dimensional subspace; ﬁnally, the new face image is smiling) and facial details (glasses or no glasses). The images
identiﬁed by nearest neighbor classiﬁer.              were taken with a tolerance for some tilting and rotation of the
                                                      face up to 20 degrees. 10 sample images of one individual are
5.2  Face Recognition on Yale Database                displayed in Figure 3. For each individual, l(= 2, 3, 4, 5) im-
The Yale face database is constructed at the Yale Center for ages are randomly selected for training and the rest are used
Computational Vision and Control. It contains 165 grayscale for testing.
images of 15 individuals. The images demonstrate variations The experimental design is the same as before. For each
                                                           l
in lighting condition (left-light, center-light, right-light), fa- given , we average the results over 20 random splits. The best
cial expression (normal, happy, sad, sleepy, surprised, and result obtained in the optimal subspace and the corresponding
wink), and with/without glasses. Figure 2 shows some sam- dimensionality for each method are shown in Table 2.
ple images of one individual.                           As can be seen, our LSDA algorithm performed the best
  For each individual, l(= 2, 3, 4, 5) images were randomly for all the cases. The Fisherface method performed com-
selected as training samples, and the rest were used for test- paratively to LSDA as the size of the training set increases.
ing. The training set was used to learn a face subspace us- Moreover, the optimal dimensionality obtained by LSDA and
ing the LSDA, Eigenface, and Fisherface methods. Recog- Fisherface is much lower than that obtained by Eigenface.
nition was then performed in the subspaces. We repeated 5.4 Discussion
this process 20 times and calculate the average recognition
                                                      Several experiments on two standard face databases have
rate. In general, the recognition rates varies with the dimen-
                                                      been systematically performed. These experiments have re-
sion of the face subspace. The best performance obtained by
                                                      vealed a number of interesting points:
these algorithms as well as the corresponding dimensionality
of the optimal subspace are shown in Table 1. For the baseline 1. All the three algorithms (LSDA, MFA, and Fisherface)
method, we simply performed face recognition in the original performed better in the optimal face subspace than in the
1024-dimensional image space. Note that, the upper bound  original image space. This indicates that dimensionality
of the dimensionality of Fisherface is c − 1 where c is the reduction can discover the intrinsic structure of the face
number of individuals [Duda et al., 2000].                manifold and hence improve the recognition rate.
  As can be seen, our algorithm outperformed all other three 2. In all the experiments, our LSDA algorithm consistently
methods. The Eigenface method performs the worst in all   outperformed the Eigenface, Fisherface and MFA meth-
cases. It does not obtain any improvement over the baseline ods. Especially when the size of the training set is small,
method. It would be interesting to note that, when there are LSDA signiﬁcantly outperformed Fisherface. This is
only two training samples for each individual, the best perfor- probably due to the fact that Fisherface fails to accu-
mance of Fisherface is no longer obtained in a c − 1(= 14) rately estimate the within-class scatter matrix from only
dimensional subspace, but a 10-dimensional subspace. LSDA a small number of training samples.

                                                IJCAI-07
                                                   712