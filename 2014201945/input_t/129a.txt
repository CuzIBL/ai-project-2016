                                    TRAINING AND TRACKING IN ROBOTICS* 


                                                            Oliver G. Selfridge and Richard S. Sutton 
                                                                 GTE Laba, Waltham, MA 02254 

                                                                         Andrew G. Barto 
                                                       Department of Computer and Information Science 
                                                       University of Massachusetts, Amherst, MA 01003 

                               ABSTRACT                                              does not lead to increased learning speed through training. Although 
                                                                                     an obvious role of learning is to construct a knowledge base through 
      We explore the use of learning schemes in training and adapting 
                                                                                     experience in domains where there is little a priori knowledge, another 
performance on simple coordination tasks. The tasks are 1-D pole bal•
                                                                                     role that receives less attention is to track a moving optimum, either 
ancing. Several programs incorporating learning have already achieved 
                                                                                     incrementally or non-incrementally, as unforeseen changes take place. 
this (1, S, 8]: the problem is to move a cart along a short piece of track 
to at to keep a pole balanced on its end; the pole is hinged to the cart                   Here we show how both training and tracking can be done with 
at its bottom, and the cart is moved either to the left or to the right               a learning system that was described by Barto, Sutton, and Ander•
by a force of constant magnitude. The form of the task considered                    son [1]. Our domain is the classic problem of balancing a pole in one 
here, after (3), involves a genuinely difficult credit-assignment prob•               dimension. This problem has the advantage that it has been consid•
lem. We use a learning scheme previously developed and analysed                       ered by several researchers [3, 8], and it can be made quite difficult by 
(1, 7] to achieve performance through reinforcement, and extend it to                 adopting certain assumptions. Specifically: 
include changing and new requirements. For example, the length or 
mast of the pole can change, the bias of the force, its strength, and so 
on; and the system can be tasked to avoid certain regions altogether. 
                                                                                           A rigid pole is hinged to a cart, which is free to move within 
In this way we explore the learning system's ability to adapt to changes 
                                                                                           the limits of a 1-D track. The learning system attempts 
and to profit from a selected training sequence, both of which are of 
                                                                                           to keep the pole balanced and the cart within iti limits by 
obvious utility in practical robotics applications. 
                                                                                           applying a force of fixed magnitude to the cart, either to the 
     The results described here were obtained using a computer sim•                        left or to the right. 
ulation of the pole-balancing problem. A movie will be shown of the 
performance of the system under the various requirements and tasks. 
                                                                                      This task is susceptible to a number of different learning schemes de•
                                                                                      pending on the quality of the evaluative feedback provided to the learn•
                           I INTRODUCTION                                             ing system. Here we consider a version of the task similar to the one 
                                                                                      studied by Michie and Chambers [3] in which the only evaluative feed-
      The importance of good training experience it well recognised in 
                                                                                      back occurs on failure—hitting the stops with the cart, or having the 
pattern classification and inductive inference, where careful choice of 
                                                                                      pole fall over. This sparsity of evaluative feedback create! a genuinely 
rule exemplars and counter-exemplars clearly affects learning progress 
                                                                                      difficult credit-assignment problem. Since a failure usually occurs only 
 (e.g., ref. [9]). It is also important for learning in symbolic problem 
                                                                                      after a long sequence of individual control decisions, it is difficult to 
solving as illustrated by the problem generation component of LEX [5]. 
                                                                                      determine which decisions were responsible for it. To solve this prob•
 Here we show that similar pedagogic care can be significant in non-
                                                                                      lem, the system identifies certain regions as being in some sense "close" 
symbolk problem solving of the kind that is important in robotics; 
                                                                                      to failure, and will try to avoid them as well. The back-propagation 
namely, problems of learning to control physical dynamical systems. 
                                                                                      method for generating this internal evaluation is like Samuel's method 
      It is sometimes faster for a system to Warn to solve a different                (6), but refined and improved by Sutton [7]. 
problem from the one assigned and then to adapt that solution once 
                                                                                           The learning system usually starts with an empty experiential 
it it learned. And it it usually far fatter for a system to adapt to 
                                                                                      knowledge base, and it takes some time to fill it with enough data 
new requirements by modifying old solutions than for it to start again 
                                                                                      for the system to perform well. We first show that to learn a new 
from scratch. In the case of learning to control a physical system, it 
                                                                                      task—for example, one with very different parameters, like a vastly 
may be much easier, for example, to first learn to control a related 
                                                                                      heavier pole—can be easier when the system adjusts its knowledge base 
system with simpler dynamics, and then to continue to learn at that 
                                                                                      from a successful state than from scratch. While it is obvious that a 
system is deformed, continuously or by a sequence of steps, into the 
                                                                                      suitably primed knowledge base ought to facilitate learning compared 
system required. This it noi an approach that lends itself to orthodox 
                                                                                      to "tabula rata" learning, our point is that the initial knowledge can be 
control theory due to the difficulty of adjusting to nnforeseen changes 
                                                                                      that acquired from learning to solve an easier task, which may itself 
in dynamics and task requirements. 
                                                                                      have been learned at a result of facing an even easier task. Other 
      A related issue is that the ability of a system to adjust to unfore-            results show this explicitly. 
seen changes in circumstances and requirements it important even if it 
                                                                                           We also consider tasks that add a constraint, such as avoiding 
                                                                                      particular pole positions. In these tasks, the learning system must 
     * This research was supported by the Air Force Office of Scientific              adapt to the constraint without explicit instruction as to how to do it. 
        Research and the Avionks Laboratory (Air force Wright Aero-                   Systems capable of discovering "how" to accomplish a goal, specified 
        nautical Laboratories) through contract F33615-83-1078.                       only in terms of "what", are more powerful than those that require                                                                                                                                      0. Selfridge et al. 671 


 more explicit specifications. Such "operationalization* capabilities are            was increased one order of magnitude (to 1kg). This made the prob•
 ihown by systems like ours that modify behavior without performing                  lem much harder, and performance dropped accordingly. However, 
 adaptive model reference system identification, as is usual in adaptive             criterial performance was soon regained: initial training took an aver•
 :ontrol theoretic techniques. That is, what is used here is a "learn•               age of 68 failures; subsequent adaptation to the heavier pole took only 
 ing by discovery" method that owes more to AI than to conventional                  20. On the other hand, without pre-training, learning to balance the 
 adaptive control.                                                                   heavier pole took an average of 86 failures. It is clearly more efficient 
                                                                                     to use a previous solution as the starting place for this new task than 
II TECHNICAL APPROACH AND EXPERIMENTAL RESULTS                                       to start from scratch. 

     Although the state space of the cart-pole system is continuous—                      The system finds it harder to learn to handle shorter poles. If we 
 -hat is, the state variables are continuous—we divide the four of them              started training with a pole reduced in length and mass to two-thirds 
                                                                                     of the original lm/.lkg pole, it took 119 failures on average to reach 
 discretely, as in [3]: 
                                                                                     criterion, compared to 67 with the full-sise pole. When the system 
                                                                                     was first trained to criterion on the full-sise pole and then switched 
                                                                                     to the short pole, however, only 6 additional failures were incurred 
                                                                                     on average after the switch, for a total of 73 failures overall. This 
                                                                                     result shows not only adaptability to changing requirements, but also 
 which yield together 162 regions corresponding to the combinations.                 improvement in learning rate with "directed* training. 
 The system is always in just one region. The job of the learning 
 system, then, is to assign the proper action to each region, so that                     Directed training resulted in a larger advantage in switching to 
 the system will act correctly. The learning algorithm is given in the               a shorter track. With the track length 2 meters (instead of 4.8m), 
 appendix and is discussed in detail in refs. [1] and [7].                           learning took more than 250 failures on the average. But training first 
                                                                                     at 4.8m, then at 4m, 3m, and finally at 2m (each to criterion) took 
     The cart-pole system was simulated on a VAX-11/780, using the                   merely 64 + 4 + 9 + 5- 82 failures. 
 equations of motion given in [2] and the following initial parameter 
 values:                                                                                  The final task required learning to avoid some region of state 
                                                                                     space; namely, the region in which the pole is near vertical (±1°). 
                                                                                     We added a penalty, equal to 1/10 the penalty for failure, for being 
                                                                                     within this region. Despite the fact that remaining in this state is the 
                                                                                     "natural" way to balance the pole, the learning system reduced the 
                                                                                     proportion of time spent there from 22% to 8%. Increasing the penalty 

                                                                                     reduced the fraction still further, but then hitting the stops became 
                                                                                     more attractive relatively, and balancing failures occurred more often. 
There are two small coefficients of friction, one for the cart and the 
 other for the pole. In the initial state the pole is stationary and                                   Ill ANALYSIS AND DISCUSSION 
 upright, and the cart is stationary in the middle of the track. Since 
 there is always a force, the pole will not long stay upright. The time                   The training illustrated in these results consists of selecting a 
 from initial position to failure—the cart or pole hitting a stop—is                 sequence of control tasks that ends with the required task and that 
 considered one learning trial. Time was discretised to a fiftieth of a              presents a graded series of difficulties. Since this kind of sequence 
 second. The system was judged to have succeeded in balancing the                    consists of entire problem-solving tasks, rather than exemplars and 
 pole when it achieved a trial that continued without failure for 10,000             counter-exemplars of a pattern class, it is quite different from the 
 time steps, corresponding to some 3.3 minutes real time. We used                    usual training sequence in'supervised learning pattern classification or 
 the number of failures before reaching this criteria of success as some             concept formation. The potential utility of this type of training seems 
 measure of the power and efficiency of learning. A run using the                    clear; these results show that the learning system considered here is 
 standard parameters listed above typically might have 70 trials before              in fact able to benefit from it. The learning system treated here is, 
 a criterion trial is achieved.                                                      moreover, a simple one; it does not deal with hierarchical control at 
                                                                                     all, nor does it take advantage of a specific accessible knowledge base. 
      In the first set of tasks the system was required to adapt to 
 changes in the parameters of the cart-pole system. One task required                     Part of the utility of the training in our examples is due to the 
 adaptation to bias in the force, which changed from +10 and —10                     fact the external evaluative feedback is in the form of a binary signal— 
 newtons to +12 and -8, and to +8 and —12. This can be consid•                       failure or not. Such systems are always subject to what has been 
 ered an (inaccurate) approximation to tilting the track right and left.             termed the "mesa phenomenon" [4]. What is needed is a more contin•
 That problem was solvable: the control surface learned was specific to              uous feedback so that the system can become better and better, rather 
the direction of the tilt, and a system trained for one tilt did not work            than merely satisfy some set of binary constraints. One observes this 
well for another. However, after several switches of the direction of                constantly in people performing physical tasks; it would obviously be 
 tilt, with training to criterion for each, the system was eventually able           desirable in robotics as well. One mechanism used by the learning 
 to balance the pole each way without failure. That is, the system had               system to provide a more continuous evaluation is the part of the 
 generalised its solution to satisfy all the problems simultaneously.                algorithm that constructs an internal evaluation signal. Training pro•
                                                                                     vides another way of reducing the effect of the mesa phenomenon by 
      The second task required adaptation to an increase in the mass 
                                                                                     effectively leading the system to better regions of the solution space. 
 of the pole. The initial training used the standard .1kg pole to the 
                                                                                     Significantly, this can be done by manipulating aspects of the task 
 criterion of 10,000 steps without failure. Then the mass of the pole 
                                                                                     without using detailed knowledge of what the performance surface is  672 0. Selfrldge et al. 


like. 

      Another way to ease the problems caused by the mesa phe•
nomenon is to provide our robotics systems not with just one purpose, 
but with a structure of purposes: the first purpose to be satisfied is 
to fulfill the constraints and avoid outright failure—that is, to keep 
the pole upright and to avoid allowing the cart to crash into the ends                                               REFERENCES 
of the track. Next the system might try to keep the pole upright, by 
reducing the average magnitude of 6 ; after that, it might try to min•
                                                                                           [1] Barto, A. G., Sutton, R. S., k Anderson, C. W. Neuronlike 
imise the angular velocity, so as to keep the pole still. Beyond that, 
                                                                                              elements that can solve difficult learning control problems. 
we can imagine a universal purpose of wanting to do all the above                              IEEE Trans, on Systems, Man, and Cybernetics, vol. SMC-
with minimum work, just as people's physical efforts become more                               14, 1984, 834-846. 
and more efficient with practice. 
                                                                                           [2] Cannon, R. H. Dynamics of Physical Systems. New York: 
      Related to the issue of training, and equally important, is the                         McGraw-Hill, 1967. 
ability of the learning system to track a changing optimum for the 
control parameters. In robotics, there are practical reasons for this:                     [3] Michie, D., k Chambers, R. A. BOXES: An experiment in 
the dynamics change with time, sises and viscosities change with tem•                         adaptive control. In Dale, E., k Michie, D. (Eds.), Machine 
perature, bearings and surfaces suffer wear, and so on. Such problems                          Intelligence S. Edinburgh: Oliver and Boyd, 1968, 137-152. 
are clearly not feasible to solve analytically, and in any case, the point 
is to provide systems whose behavior is robust enough to handle un•                        [4] Minsky, M. L. k Selfridge, O. G. Learning in random nets. In 
expected as well as expected situations.                                                      C. Cherry (Ed.), Information Theory: Fourth London Sympo•
                                                                                               sium. London: Butterworths, 1961. 

                        ACKNOWLEDGEMENTS                                                   [5] Mitchell, T. M. Learning and problem solving. Proc. IJCAI 

                                                                                               8S} 1983, 1139-1151. 
      The authors thank Charles W. Anderson for assistance with the 
pole-balancing experiments.                                                                [6] Samuel, A. L. Some studies in machine learning using the 
                                                                                               game of checkers. IBM Journal of Research and Development, 
                                                                                               1959, 3, 210-229. 

                                                                                           [7] Sutton, R. S. Temporal aspects of credit assignment in rein•
                                                                                              forcement learning. Univ. of Massachusetts Technical Report 
                                                                                               84-2, 1984. 

                                                                                           [8] Widrow, B., k Smith, F. W. Pattern-recognising control sys•
                                                                                              tems. In Tow, J. T., k Wilcox, R. H., Computer and Infor•
                                                                                               mation Sciences. Washington D. C: Spartan Books, 1964, 
                                                                                               288-317. 

                                                                                           [9] Winston, P. H. Learning structural descriptions from exam•
                                                                                              ples. In Winston, P. H. (Ed.) The Psychology of Computer 
                                                                                               Vision. New York: McGraw-Hill, 1975, 157-209. 