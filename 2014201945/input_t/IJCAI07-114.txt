          Exploiting Known Taxonomies in Learning Overlapping Concepts

                                  Lijuan Cai   and Thomas Hofmann
                                    Department of Computer Science
                                Brown University, Providence, RI, USA
                                        {ljcai, th}@cs.brown.edu


                    Abstract                          tween categories into the Perceptron learning and SVM clas-
                                                      siÔ¨Åcation architecture. The rest of the paper is organized as
    Many real-world classiÔ¨Åcation problems involve    follows. Section 2 introduces two ways of representing tax-
    large numbers of overlapping categories that are ar- onomy knowledge. First, derive pairwise similarities between
    ranged in a hierarchy or taxonomy. We propose to  categories based on their relative locations in the taxonomy
    incorporate prior knowledge on category taxonomy  in order to tie learning across categories. Second, adapt the
    directly into the learning architecture. We present standard 0/1 loss function to weigh misclassiÔ¨Åcation errors in
    two concrete multi-label classiÔ¨Åcation methods, a accordance with the taxonomy structure. In Section 3 we for-
    generalized version of Perceptron and a hierarchi- mulate the hierarchical learning problem in terms of a joint
    cal multi-label SVM learning. Our method works    large margin problem, for which we derive an efÔ¨Åcient train-
    with arbitrary, not necessarily singly connected tax- ing algorithm. In Section 4 we propose a hierarchical Percep-
    onomies, and can be applied more generally in     tron algorithm that exploits taxonomies in a similar fashion.
    settings where categories are characterized by at- Section 5 examines related work. Section 6 presents exper-
    tributes and relations that are not necessarily in- imental results which show the two new hierarchical algo-
    duced by a taxonomy.  Experimental results on     rithms bring signiÔ¨Åcant improvements on all metrics. Con-
    WIPO-alpha collection show that our hierarchical  clusions and future work are discussed in Section 7.
    methods bring signiÔ¨Åcant performance improve-
    ment.
                                                      2   Utilizing Known Taxonomies
1  Introduction                                       2.1  Problem Setting
Many real-world classiÔ¨Åcation tasks involve large numbers We assume the patterns such as documents are represented
of overlapping categories. Prominent examples include the as vectors, x ‚ààX ‚äÜRd, which can be mapped to a higher
International Patent ClassiÔ¨Åcation scheme (approx. 69,000 dimensional feature space as œÜ(x). We denote the set of cat-
patent groups), the Open Directory project (approx. 590,000 egories by Y = {1,...,q}, a category by y ‚ààY,anda
categories for Web pages), and the Gene Ontology (ap- label set by Y ‚àà P(Y) where P(Y) is the power set of Y.
prox. 17,000 terms to describe gene products). In most cases, A taxonomy is a directed acyclic graph (V,E) with nodes
instances are assigned to more than one category, since cat- V‚äáYsuch that the set of terminal nodes equals Y, formally
egories are rarely mutually exclusive. This leads to large Y = {y ‚ààV: v ‚ààV,  (y,v) ‚àà E}. Note that we do
scale multi-label classiÔ¨Åcation problems. The categories are not assume that a taxonomy is singly connected (tree or for-
typically organized in hierarchies or taxonomies, most com- est), but allow for converging nodes. In some cases one wants
monly by introducing superordinate concepts and by relating to express that items belong to a super-category, but to none
categories via ‚Äòis-a‚Äô relationships. Multiply connected tax- of the terminal categories in Y, we suggest to model this by
onomies are not uncommon in this context.             formally adding one terminal node to each inner node, repre-
  We believe that taxonomies encode valuable domain   senting a ‚Äúmiscellaneous‚Äù category; this avoids the problem
knowledge which learning methods should be able to capital- of partial paths.
ize on, in particular since the number of training examples for In multi-label learning, we aim at Ô¨Ånding a mapping f :
individual classes may be very small when dealing with tens X‚ÜíP(Y), based on a sample of training pairs {(xi,Yi),i=
of thousands or more classes. The potential loss of valuable 1,...,n}‚äÜX√óP(Y). A popular approach as suggested,
information by ignoring class hierarchies has been pointed for instance by [Schapire and Singer, 2000], is to actually
out before and has led to a number of approaches that employ learn a ranking function over the categories for each pattern,
different ways to exploit hierarchies [McCallum et al., 1998; g : X‚ÜíSq,whereSq is the set of permutations of ranks 1
Wang et al., 1999; Dumais and Chen, 2000].            to q. In order to get a unique subset of labels, one then needs
  In this paper, we present an approach for systematically address the additional question on how to select the number
incorporating domain knowledge about the relationships be- of categories a pattern should be assigned to.

                                                IJCAI-07
                                                   714  It is common to deÔ¨Åne the ranking function g implic- and c+ for assigning an irrelevant item. For any label set Y ,
itly via a scoring function F : X√óY ‚ÜíR, such that     deÔ¨Åne anc(Y ) ‚â°{v ‚ààV:   ‚àÉy ‚àà Y,v ‚àà  anc(y)}.Nowwe
g(x)(y) <g(x)(y) whenever F (x,y) >F(x,y), i.e. cate- can quantify the loss in the following manner:
gories with higher F -values appear earlier in the ranking (for                              
                                                             ÀÜ    ‚àí         +      ‚àí   ‚àí    +
ease of presentation we ignore ties). Notation g(y) is used (Y,Y )=c    sv + c    sv  (c  + c )   sv  (2)
when clear from context.                                           v‚ààanc(Y ) v‚ààanc(YÀÜ )       v‚ààanc(Y )
                                                                                               ‚à©anc(YÀÜ )
2.2  Class Attributes                                 Note that only nodes in the symmetric difference anc(Y ) 
Following [Cai and Hofmann, 2004] we suggest to use scor- anc(YÀÜ ) contribute to the loss. In the following we will sim-
                                                                                        ‚àí    +
ing functions F that are linear in some joint feature repre- plify the presentation by assuming that c = c =1. Then,
sentation Œ¶ of inputs and categories, namely F (x,y; w) ‚â° by further setting sv =1(‚àÄv ‚ààV) one gets (Y,YÀÜ )=
w Œ¶  x           w
  ,  ( ,y) ,where     is a weight vector. Following   |anc(Y )  anc(YÀÜ )|. Intuitively, this means that one colors all
                                             Œ¶
[Tsochantardis et al., 2004; Cai and Hofmann, 2004] will nodes that are on a path to a node in Y with one color, say
be chosen to be of the form Œõ(y) ‚äó œÜ(x),whereŒõ(y)=                                     ÀÜ
                T    s                                blue, and all nodes on paths to nodes in Y with another color,
(Œª1(y),...,Œªs(y)) ‚àà R refers to an attribute vector repre-
                   ‚äó                                  say yellow. Nodes that have both colors (blue+yellow=green)
senting categories and is the Kronecker product. One can are correct, blue nodes are the ones that have been missed and
interpret w in terms of a stacked vector of individual weight
          w    wT      wT  T                          yellow nodes are the ones that have been incorrectly selected;
vectors, i.e. =( 1 ,...,s ) , leading to the additive de- both types of mistakes contribute to the loss proportional to
           F (x,y; w)=   s  Œª (y)w , x
composition              r=1 r     r   . The general  their volume.
idea is that the notion of class attributes will allow general- During training, this loss function is difÔ¨Åcult to deal with
ization to take place across (similar) categories and not just directly, since it involves sets of labels. Rather, we would like
across training examples belonging to the same category. In to work with pairwise contributions, e.g. involving terms
the absence of such information, one can set s = q and deÔ¨Åne
                                                                          |             |
Œªr(y)=Œ¥ry  which leads to F (x,y)=wy, x.                          (y,y )=  anc(y)  anc(y ) .        (3)
  How are we going to translate the taxonomy information In singly connected taxonomies Eq. (3) is equivalent to the
into attributes for categories? The idea is to treat the nodes in length of the (undirected) shortest path connecting the nodes
the taxonomy as properties. Formally, we deÔ¨Åne        y    y            [               ]
                                                       and  , suggested by Wang et al., 1999 . In order to relate
                      t , if v ‚àà anc(y)               the two, we state the following proposition.
             Œª (y)=    v                        (1)
              v       0,  otherwise ,                 Proposition 1. For any Y,YÀÜ ‚äÜYsatisfying Y ‚äÜ YÀÜ and
                                                      YÀÜ ‚äÜ Y ,
where tv ‚â• 0 is the attribute value for node v. In the simplest                 
case, tv can be set to a constant, like 1. We denote by anc(y) |anc(Y )  anc(YÀÜ )|‚â§ |anc(y)  anc(ÀÜy)|

the set of ancestor nodes of y in the taxonomy including y                     y‚ààY ‚àíYÀÜ
itself (for notational convenience). This leads to an intuitive                yÀÜ‚ààYÀÜ ‚àíY
decomposition of the scoring function F into contributions For learning with ranking functions g, we translate this into
from all nodes along the paths from a root node to a speciÔ¨Åc               
terminal node.                                                (Y,g)=            |anc(y)  anc(ÀÜy)| . (4)
                                                                        y‚ààY,yÀÜ‚ààY‚àíY
2.3  Loss Functions                                                      g(y)>g(ÀÜy)
A standard loss function for the multi-label case is to use We look at every pair of categories where an incorrect cate-
the symmetric difference between the predicted and the ac- gory comes before a correct category in the order deÔ¨Åned by g
tual label set, i.e. to count the number of correct categories and count the symmetric difference of the respective ancestor
missed plus the number of incorrect categories that have been sets as the corresponding loss.
assigned, 0(Y,YÀÜ ) ‚â°|Y  YÀÜ |.
  Yet, in many applications, the actual loss of a predicted la- 3 Hierarchical Support Vector Machines
bel set relative to the true set of category labels will depend 3.1 Multi-label ClassiÔ¨Åcation
on the relationship between the categories. As a motivation We generalize the multiclass SVM formulation of [Crammer
we consider the generic setting of routing items based on their and Singer, 2001] to a multi-label formulation similar to that
membership at nodes in the taxonomy. For instance, in a news in [Elisseff and Weston, 2001]. For a given set of correct
routing setting, readers may sign-up for speciÔ¨Åc topics by se-
                                                      categories Yi we denote the complement by Y¬Øi = Y‚àíYi.
lecting an appropriate node, which can either be a terminal Then following [Elisseff and Weston, 2001] we approximate
node in the taxonomy (e.g. the category ‚ÄúSoccer‚Äù) or an in- the separation margin of w with respect to the i-th example
ner node (e.g. the super-category ‚ÄúSports‚Äù). Note that while as
we assume that all items can be assigned to terminal nodes of
the taxonomy only, customers may prefer to sign-up for many  Œ≥i(w) ‚â°   min  Œ¶(xi,y) ‚àí Œ¶(xi, y¬Ø), w . (5)
                                                                     ‚àà   ¬Ø‚àà ¬Ø
categories en bloc by selecting an appropriate super-category.       y Yi,y Yi
  We assume that there is some relative sign-up volume sv ‚â• Our formulation aims at maximizing the margin over the
                           ‚àí
0 for each node as well as costs c of missing a relevant item whole training set, i.e. maxw:w=1 mini Œ≥i(w).Thisis

                                                IJCAI-07
                                                   715equivalent to minimizing the norm of the weight vector w Algorithm 1 Hierarchical Multilabel SVM
while constraining all (functional) margins to be greater than             {x    }n            ‚â•
                                                       1: inputs: training data i,Yi i=1, tolerance  0
or equal to 1. A generalized soft-margin SVM formulation 2: initialize Si = ‚àÖ, Œ±iyy¬Ø =0, ‚àÄ i, y ‚àà Yi, y¬Ø ‚àà Y¬Øi.
can be obtained by introducing slack variables Œæi‚Äôs. The 3: repeat
penalty is scaled proportional to the loss associated with       ÀÜ         n
                                                       4:   select i =argmaxi=1 œài
the violation of the respective category ordering, a mech-          ÀÜ                 ¬Ø
                                                       5:   select (ÀÜy,y¬Ø) = argmaxy‚ààYÀÜ,y¬Ø‚ààYÀÜ GÀÜ ¬Ø
anism suggested before (cf. [Tsochantardis et al., 2004;                          i   i  iyy
                                                                             SÀÜ = SÀÜ ‚à™{(ÀÜy,y¬ØÀÜ)}
Cai and Hofmann, 2004]). Putting these ideas together yields 6: expand working set: i i
                                                                                {Œ±ÀÜ  :(y,y¬Ø) ‚àà SÀÜ}
the convex quadratic program (QP)                      7:   solve QP over subspace iyy¬Ø        i
                                                       8:   reduce working set: SÀÜ = SÀÜ ‚àí{(y,y¬Ø):Œ±ÀÜ =0}
               n                                                             i    i           iyy¬Ø
     1    2                                            9: until œàÀÜ ‚â§ 
min   w  + C     Œæi                           (6)             i
 w,Œæ 2
               i=1
                          Œæi                            Note that to minimize (an upper bound on) the loss in
 s.t. w,Œ¥Œ¶i(y,y¬Ø)‚â•1 ‚àí       ,  (‚àÄi, y ‚àà Yi, y¬Ø ‚àà Y¬Øi)
                       (y,y¬Ø)                        Eq. (4), we could simply assign one slack variable Œæiyy¬Ø for
                                                      every triplet of instance, positive label, and negative label.
    Œæi ‚â• 0,  (‚àÄi) .
                                                      This leads to a dual program similar to Eq. (7) except the sec-
     Œ¥Œ¶  (y,y¬Ø) ‚â° Œ¶(x ,y) ‚àí Œ¶(x , y¬Ø)                                            Œ±iyy¬Ø ‚â§  ‚àÄ    ‚àà     ‚àà ¬Ø
where   i           i        i   .                    ond set of constraints become (y,y¬Ø) C i, y Y,y¬Ø Y .
  This formulation is similar to the one used in [Schapire and We have yet to explore this direction.
Singer, 2000] and generalizes the ranking-based multi-label
SVM  formulation of [Elisseff and Weston, 2001]. Follow- 3.2 Optimization Algorithm
ing [Crammer and Singer, 2001] we have not included bias The derived QP can be very large. We therefore employ an
terms for categories. The efforts are put into correctly or- efÔ¨Åcient optimization algorithm that is inspired by the SMO
dering each pair of positive/negative labels. We can use a algorithm [Platt, 1999] and that performs a sequence of sub-
size prediction mechanism such as the one in [Elisseff and space ascents on the dual, using the smallest possible subsets
Weston, 2001] to convert the category ranking into an actual of variables that are not coupled with the remaining variables
multi-label classiÔ¨Åcation.                            through constraints. Our algorithm successively optimizes
  The dual QP of (6) becomes                          over subspaces spanned by {Œ±iyy¬Ø : y ‚àà Yi, y¬Ø ‚àà Y¬Øi} for some
            n                                       selected instance i. Moreover an additional variable selec-
max  Œò(Œ±)=          Œ±iyy¬Ø                       (7)   tion is performed within each subspace. This strategy is also
 Œ±                                                                             [                ]
             i=1 y‚ààYi                                 known as column generation Demiriz et al., 2002 .DeÔ¨Åne
                  ¬Ø
                y¬Ø‚ààYi                                          n    
                  
           1                                            w(Œ±) ‚â°             Œ±iyy¬ØŒ¥Œ¶i(y,y¬Ø)            (11)
         ‚àí              Œ±iyy¬ØŒ±jrr¬ØŒ¥Œ¶i(y,y¬Ø),Œ¥Œ¶j (r, r¬Ø),
                                                                =1       ¬Ø
           2                                                   i   y‚ààYi,y¬Ø‚ààYi
             i,j y‚ààYi r‚ààYj
                y¬Ø‚ààY¬Ø ¬Ø
                   i r¬Ø‚ààYj                                   ‚â°         ‚àí          w  Œ± 
                                                       Giyy¬Ø    (y,y¬Ø)(1  Œ¥Œ¶i(y,y¬Ø), ( ) )          (12)
                                   Œ±iyy¬Ø                                                 
s.t. Œ±iyy¬Ø ‚â• 0(‚àÄi, y ‚àà Yi, y¬Ø ‚àà Y¬Øi) ,   ‚â§ C (‚àÄi) .
                                  (y,y¬Ø)                  li ‚â° max     max   {Giyy¬Ø} ,  0           (13)
                             y‚ààYi                                            ¬Ø
                               ¬Ø                                     i,y‚ààYi,y¬Ø‚ààYi
                             y¬Ø‚ààYi                             ‚éß
                                                               ‚é™            ¬Ø
                                                               ‚é®min  i,y‚ààYi ,y¬Ø‚ààYi: Giyy¬Ø      if Œ∂i =0
Note that                                                                  0
                                                             ‚â°       Œ±iyy¬Ø>               
                                                           ui  ‚é™
       Œ¥Œ¶i(y,y¬Ø),Œ¥Œ¶j(r, r¬Ø)                                  ‚é©                 ¬Ø
                                                                 min  min i,y‚ààYi ,y¬Ø‚ààYi: Giyy¬Ø , 0 if Œ∂i > 0 ,
                                                                                0
     =Œõ(y) ‚àí  Œõ(¬Øy), Œõ(r) ‚àí Œõ(¬Ør)œÜ(x ),œÜ(x ) . (8)                      Œ±iyy¬Ø>
                                    i     j                                                          (14)
                                                                     
Herein one can simply replace the inner products by corre-                       Œ±iyy¬Ø
                                                      where Œ∂i = C ‚àí    ‚àà   ¬Ø‚àà ¬Ø     .DeÔ¨Åneœài   ‚â° li ‚àí ui.
sponding kernel functions. It is straightforward to observe           y Yi,y Yi (y,y¬Ø)
that 1   Œæ yields an upper bound on the training loss of By derivation similar to that in [Cai and Hofmann, 2004],it
    n   i i                                                                  ‚àÄ
the resulting classiÔ¨Åer measured by  in the following sense. can be shown that œài =0( i) is the necessary and sufÔ¨Åcient
DeÔ¨Åne the maximum loss as                             condition for a feasible solution to be optimal. Hence the
                                                      score œài is used for selecting subspaces. Giyy¬Ø is also used to
        x(Y,g)  ‚â°      max       (y,y¬Ø) .     (9)   select new variables to expand the active set of each subspace.
                    ‚àà  ¬Ø‚àà ¬Ø : ( )‚â• (¬Ø)
                   y Y,y Y g y g y                    The resulting algorithm is depicted in Algorithm 1.
The maximal loss over a set of examples is deÔ¨Åned as    More details on convergence and sparseness of a more gen-
                                                                                       [
                         n                           eral class of algorithms can be found in Tsochantardis et al.,
                 n     1                              2004].
      (F, {x ,Y } ) ‚â°        (Y ,g(x ; F ))  (10)
      x     i  i i=1   n      x  i    i
                         i=1                          4   Hierarchical Perceptron
                        w ŒæÀÜ
Proposition 2. Denote by ( ÀÜ , ) a feasible solution of the Although SVM is competitive in generating high-quality
              1   n  ÀÜ
QP in (6).Thenn   i=1 Œæi is an upper bound on the empiri- classiÔ¨Åers, it can be computationally expensive. The Percep-
                    w   {x    }n
cal maximal loss x(F (; ÀÜ ), i,Yi i=1).               tron algorithm [Rosenblatt, 1958] is known for its simplicity

                                                IJCAI-07
                                                   716Algorithm 2 Hierarchical Minover Perceptron algorithm misclassiÔ¨Åcations are penalized along all ancestors that miss
                     {x   }n                          relevant patterns or include irrelevant ones. In H-loss, how-
 1: inputs: training data i,Yi i=1, desired margin c.
 2: Initialize Œ±iyy¬Ø =0, ‚àÄ i, y ‚àà Yi, y¬Ø ‚àà Y¬Øi        ever, if punishment already occurs to a node, its descendents
 3: repeat                                            are not penalized again. In addition, our loss function works
                                                      with arbitrary taxonomy, not just trees.
 4:  (ÀÜi, y,ÀÜ y¬ØÀÜ) = argmin ‚àà ¬Ø‚àà ¬Ø w(Œ±),Œ¥Œ¶i(y,y¬Ø)
                     i,y Yi,y Yi                        [Rousu et al., 2005] applies the Maximum-Margin Markov
 5:  if w(Œ±),Œ¥Œ¶ÀÜ(ÀÜy,y¬ØÀÜ) >c then
                i                                     Networks [Taskar et al., 2003] to hierarchical classiÔ¨Åcation
 6:    terminate with a satisfactory solution
                                                      where the taxonomy is regarded as Markov Networks. They
 7:  else
                                                      propose a simpliÔ¨Åed version of H-loss that decomposes into
       Œ±ÀÜ   ‚Üê Œ±ÀÜ   + (ÀÜy,y¬ØÀÜ)
 8:      iyÀÜy¬ØÀÜ iyÀÜy¬ØÀÜ                                contributions of edges so as to marginalize the exponential-
 9:  end if                                           sized problem into a polynomial one. In our methods, learn-
10: until maximal number of iterations are performed  ing occurs on taxonomy nodes instead of edges. We view the
                                                      taxonomy as a dependency graph of ‚Äúis-a‚Äù relation.
                                                        [Cai and Hofmann, 2004] proposes a hierarchical SVM
and speed. In this section, we propose a hierarchical Per- that decomposes discriminant functions into contributions
ceptron algorithm 2 using the minimum-overlap (Minover) from different levels of the hierarchy, the same way as this
learning rule [Krauth and M¬¥ezard, 1987].             work. Compared to [Cai and Hofmann, 2004], which was re-
  The Minover Perceptron uses the instance that violates the stricted to multiclass classiÔ¨Åcation, however, we deal with the
desired margin the worst to update the separating hyperplane. additional challenge posed by overlapping categories, i.e. the
We can also deal with the instances sequentially, as in a truly multi-label problem, for which we employ the category rank-
online fashion. Using the minimum overlap selection rule ing approach proposed in [Schapire and Singer, 2000].
effectively speeds up the convergence and yields sparser so- In summary, our major contributions are: 1) Formulate
lutions.                                              multilabel classiÔ¨Åcation as a global joint learning problem
  If using taxonomy-based class attribute scheme in Eq. (1) that can take taxonomy information into account. 2) Exploit
with tv =1, the simple update rule in step 8 of Algorithm 2 taxonomy by directly encoding structure in the scoring func-
can be decomposed into                                tion used to rank categories 3) Propose a novel taxonomy-
                                                      based loss function between overlapping categories that is
  wv ‚Üê  wv + (y,y¬Ø)œÜ(xi)  ‚àÄv : v ‚àà anc(y) ‚àí anc(¬Øy)
  w  ‚Üê  w  ‚àí         x    ‚àÄ    ‚àà       ‚àí             motivated by real applications. 4) Derive a sparse optimiza-
    v     v    (y,y¬Ø)œÜ( i)  v : v anc(¬Øy) anc(y)      tion algorithm to efÔ¨Åciently solve the joint SVM formula-
  Only the weight vectors of those nodes that are predeces- tion. Compared to multiclass classiÔ¨Åcation, sparseness is
sors of y or y¬Ø but not both will be updated. Other nodes are even more important now that there are more constraints and
left intact. This strategy is also used in [Dekel et al., 2004] for hence more dual variables. 5) Present a hierarchical Percep-
online multiclass classiÔ¨Åcation. The more severe the loss is tron algorithm that takes advantage of the proposed methods
incurred, the more dramatic the update will be. Moreover step of encoding known taxonomies.
8 not only updates the scoring functions of the two classes in
question, but also spread the impact to other classes sharing 6 Experiments
affected ancestors with them.                         6.1  Experimental Setup
                                                      In this section we compare our hierarchical approaches
5  Related Work                                       against their Ô¨Çat counterparts on WIPO-alpha, a data set com-
Many approaches for hierarchical classiÔ¨Åcation use a decision prising patent documents.
tree like architecture, associating with each inner node of the Taxonomy-derived attributes are employed in the hierar-
taxonomy a classiÔ¨Åer that learns to discriminate between the chical approaches.‚àö For comparison purpose, tv in Eq. (1) is
children [Dumais and Chen, 2000; Cesa-Bianchi et al., 2005]. set to 1/ depth where depth ‚â° maxy |{anc(y)}|,sothat
While this offers advantages in terms of modularity, the local maxy Œõ(y) =1for either the Ô¨Çat or the hierarchical mod-
optimization of (partial) classiÔ¨Åers at every inner node is un- els. In the experiments, hierarchical loss equals half the value
able to reÔ¨Çect a more global objective.               in Eq. (3) for historical reason. The hierarchical learning em-
  [Cesa-Bianchi et al., 2005] introduces a loss function, ploys this hierarchical loss while the Ô¨Çat one employs the 0‚àí1
called the H-loss, speciÔ¨Åcally designed to deal with the loss. We used a linear kernel and set C =1. Each instance is
case of partial and overlapping paths in tree-structured tax- normalized to have 2-norm of 1. In most experiments, the test
onomies. [Cesa-Bianchi et al., 2006] has proposed B-SVM, performance is evaluated by cross-validation and then macro-
which also uses the H-loss and uses a decoding scheme averaging across folds.
that explicitly computes the Bayes-optimal label assignment The measures we used include one-accuracy, average pre-
based on the H-loss and certain conditional independence as- cision, ranking loss, maximal loss,andparent one-accuracy.
sumptions about label paths. The loss function we proposed The Ô¨Årst three are standard metrics for multilabel classiÔ¨Åca-
in Eq. (2) exploits the taxonomy in a different way from H- tion problem [Schapire and Singer, 2000; Elisseff and We-
loss, partly because we always convert partial path categories ston, 2001]. One-accuracy (acc) measures the empirical
to complete path ones. Our loss function is inspired by real probability of the top-ranked label being relevant to the doc-
applications like routing and subscription to a taxonomy. So ument. Average precision (prec) measures the quality of la-

                                                IJCAI-07
                                                   717           section #cat   #doc  #cat    acc (%)    prec (%)    x-loss    rloss (%)   pacc (%)
                                /doc  Ô¨Çat   hier  Ô¨Çat   hier Ô¨Çat    hier Ô¨Çat   hier Ô¨Çat    hier
               A    846  15662  1.44  53.8  54.2  56.0  57.3 1.66  1.34  9.09  3.85 70.3  73.5
               B   1514  17626  1.48  37.3  37.8  40.8  42.5 2.08  1.76  12.6  5.38 59.9  65.0
               C   1152  14841  2.11  45.3  45.4  45.5  45.9 2.10  1.68  10.1  4.95 67.8  73.3
               D    237   2194  1.53  47.3  48.6  52.7  55.0 1.82  1.45  11.7  7.35 67.7  71.6
               E    267   3586  1.34  38.7  38.7  44.9  46.5 1.99  1.63  12.7  7.44 63.7  66.2
               F    862   8011  1.45  36.6  37.6  41.3  43.4 2.07  1.69  11.6  5.13 59.7  65.0
               G    565  12944  1.29  47.2  47.2  52.3  52.8 1.73  1.50  10.5  5.46 64.9  67.0
               H    462  13178  1.35  48.7  49.2  55.1  56.2 1.63  1.34  8.25  4.15 66.5  69.6

Table 1: SVM experiments on WIPO-alpha corpus. Each row is on categories under the speciÔ¨Åed top level node (i.e. section). The results
are from random 3-fold cross-validation. Better performance is marked in bold face. ‚Äú#cat/doc‚Äù refers to the average number of categories
per document, ‚ÄúÔ¨Çat‚Äù the Ô¨Çat SVM and ‚Äúhier‚Äù the hierarchical SVM.


                                                         70
                                                                                  50

                                                         60                       45

                                                                                  40

                                                         50
                                                                                  35              flat prec

                                                                                  30
                                                         40                                       hier prec
                                                                                  25              flat rloss

                                                         30              flat acc 20              hier rloss

                                                                         hier acc 15
                                                        performance  (%) 20 flat pacc performance  (%)
                                                                                  10
                                                                         hier pacc
                                                         10                       5
                                                           1 2 3 4 5 6 7 8 9 10     1 2 3 4 5 6 7  8 9 10
Figure 1: The four columns, from left to right, depict one accuracy sample number per category sample number per category
for Ô¨Çat and hierarchical Perceptron, and average precision for Ô¨Çat
and hierarchical Perceptron.                          Figure 2: Flat and hierarchical SVM on section D data, with vary-
                                                     ing training set size. A small number of documents are sampled
   section  acc (%)  prec (%)   x-loss  pacc (%)      from each category for training purpose. The learned classiÔ¨Åers are
            Ô¨Çat hier Ô¨Çat hier Ô¨Çat hier  Ô¨Çat hier      tested on all remaining documents. This is repeated 10 times for
       A    14.8 16.3 16.8 20.5 2.71 2.28 39.3 47.3   each sampling number. The bars depict sample standard deviation.
       B    12.4 14.0 14.0 17.9 2.78 2.39 40.1 48.6
       C    14.8 16.3 15.6 18.2 2.79 2.23 46.6 58.1   ument is labeled with one primary category as well as any
       D    19.4 21.3 24.0 27.3 2.58 2.06 49.3 56.3   number of secondary categories. Both types of categories are
       E    14.4 15.2 19.9 22.4 2.66 2.19 45.2 50.4   used to form a multi-label corpus. We have performed in-
        F   13.4 14.9 16.6 20.2 2.74 2.25 41.5 51.1   dependent experiments on taxonomies under the 8 top-level
       G    11.4 12.4 14.9 18.4 2.75 2.40 35.1 41.9   sections.
       H    15.1 16.2 19.2 22.6 2.67 2.12 40.2 48.4
                                                        Document parsing was performed with the Lemur toolkit 2.
Table 2: SVM experiments on WIPO-alpha corpus with subsam- Stop words are removed. Stemming is not performed. Word
pling. Three documents or less are sampled for each category. counts from title and claim Ô¨Åelds are used as document fea-
                                                      tures. Table 1 summarizes the SVM performance across the
bel rankings. Precision is calculated at each position where a sections. The hierarchical SVM signiÔ¨Åcantly outperforms the
positive label occurred, as if all the labels ranked higher than Ô¨Çat SVM in terms of x-loss, ranking loss and parent ac-
it including itself are predicted as relevant. These precision curacy in each individual setting. This can be attributed not
values are then averaged to obtain average precision. Rank- only to the fact that the hierarchical approach explicitly op-
ing loss (rloss) measures the average fraction of positive label timizes an upper bound on the x-loss, but also to the spe-
and negative label pairs that are misordered. These metrics ciÔ¨Åc hierarchical form of the discriminant function. More-
are described in details in [Schapire and Singer, 2000]. over, hierarchical SVM often produces higher classiÔ¨Åcation
  Maximal loss, denoted by x, was introduced in Eq. (10). accuracy and average precision with gains being more mod-
It is measured by the hierarchical loss function. We also eval- erate. To see if the improvement is statistically signiÔ¨Åcant,
uate parent one-accuracy (pacc), which measures the one- we conducted 10-fold cross-validation on section E and then
accuracy at the category‚Äôs parent nodes level.        paired permutation test. The achieved level of signiÔ¨Åcance is
                                                      less than 0.08 for one accuracy, and less than 0.005 for the
6.2  Experiments on WIPO-alpha Collection             other four measures.
WIPO-alpha collection comprises patent documents released Figure 1 depicts the performance of Perceptron algorithm
by the World Intellectual Property Organization (WIPO) 1. with the same setting. We allow Perceptron to run until
which are classiÔ¨Åed into IPC categories. IPC is a 4-level hier- convergence. It takes signiÔ¨Åcantly less time than SVM but
archy consisting of sections, classes, subclasses, and groups. reaches lower performance. We observe the hierarchical Per-
The categories in our experiments refer to main groups which ceptron performs better in all cases.
are all leaves at the same depth in the hierarchy. Each doc- In addition we randomly sampled 3 documents from each

  1www.wipo.int/ibis/datasets                            2www.lemurproject.org

                                                IJCAI-07
                                                   718