     A Scalable Kernel-Based Algorithm for Semi-Supervised Metric Learning∗

                               Dit-Yan Yeung, Hong Chang, Guang Dai
                           Department of Computer Science and Engineering
                           Hong Kong University of Science and Technology
                                Clear Water Bay, Kowloon, Hong Kong
                          {dyyeung,hongch,daiguang}@cse.ust.hk

                    Abstract                          formulation and the approach to solve the problem. The semi-
                                                      supervised learning literature is too large to do a comprehen-
    In recent years, metric learning in the semi-     sive review here. Interested readers are referred to some good
    supervised setting has aroused a lot of research in- surveys, e.g., [Zhu, 2006].
    terests. One type of semi-supervised metric learn-  One way to categorize many, though not all, semi-
    ing utilizes supervisory information in the form  supervised learning methods is to consider the type of su-
    of pairwise similarity or dissimilarity constraints. pervisory information available for learning. Unlike unsu-
    However, most methods proposed so far are either  pervised learning tasks, supervisory information is available
    limited to linear metric learning or unable to scale in semi-supervised learning tasks. However, the information
    up well with the data set size. In this paper, we is in a form that is weaker than that available in typical su-
    propose a nonlinear metric learning method based  pervised learning tasks. One type of (weak) supervisory in-
    on the kernel approach. By applying low-rank ap-  formation assumes that only part (usually a limited part) of
    proximation to the kernel matrix, our method can  the training data are labeled. This scenario is commonly en-
    handle signiﬁcantly larger data sets. Moreover, our countered in many real-world applications. One example is
    low-rank approximation scheme can naturally lead  the automatic classiﬁcation of web pages into semantic cate-
    to out-of-sample generalization. Experiments per- gories. Since labeling web pages is very labor-intensive and
    formed on both artiﬁcial and real-world data show hence costly, unlabeled web pages are far more plentiful on
    very promising results.                           the web. It would be desirable if a classiﬁcation algorithm
                                                      can take advantage of the unlabeled data in increasing the
                                                      classiﬁcation accuracy. Many semi-supervised classiﬁcation
1  Introduction                                       methods [Zhu, 2006] belong to this category.
1.1  Semi-Supervised Learning                           Another type of supervisory information is even weaker
In supervised learning, we are given a training sample in the in that it only assumes the existence of some pairwise con-
form of input-output pairs. The learning task is to ﬁnd a func- straints indicating similarity or dissimilarity relationships be-
                                                      tween training examples. In video indexing applications, for
tional relationship that maps any input to an output such that
disagreement with future input-output observations is mini- example, temporal continuity in the data can be naturally used
mized. Classiﬁcation and regression problems are the most to impose pairwise similarity constraints between successive
                                                      frames in video sequences. Another example is in proteomic
common supervised learning problems for discrete-valued
and continuous-valued outputs, respectively. In unsupervised analysis, where protein-protein interactions can naturally be
learning, we are given a training sample of objects with no represented as pairwise constraints for the study of proteins
                                                      encoded by the genes (e.g., Database of Interacting Pro-
output values, with the aim of extracting some structure from
them to obtain a concise representation or to gain some un- teins (DIP), http://dip.doe-mbi.ucla.edu/). Yet
derstanding of the process that generated the data. Clustering, another example is the anti-spam problem. Recent studies
                                                      show that more than half of the e-mail in the Internet today
density estimation, and novelty detection problems are com-
mon unsupervised learning problems.                   is spam, or unsolicited commercial e-mail. One recent ap-
                                                      proach to spam detection is based on the trustworthiness of
  Over the past decade or so, there has been growing inter-
                                                      social networks [Boykin and Roychowdhury, 2005].Such
est in exploring new learning problems between the super-
                                                      social networks can naturally be represented as graphs with
vised and unsupervised learning extremes. These methods
                                                      edges between nodes representing pairwise relationships in
are generally referred to as semi-supervised learning meth-
                                                      the social networks.
ods, although there exist large variations in both the problem
                                                        While supervisory information in the form of limited la-
  ∗This research has been supported by Competitive Earmarked beled data can be transformed into pairwise similarity and
Research Grant 617404 from the Research Grants Council of the dissimilarity constraints, inverse transformation is in general
Hong Kong Special Administrative Region, China.       not possible except for the special case of two-class problems.

                                                IJCAI-07
                                                  1138In this sense, the second type of supervisory information is of 1.3 Organization of Paper
a weaker form and hence the corresponding learning problem In Section 2, we propose a simple and efﬁcient kernel-based
is more difﬁcult to solve. The focus of our paper is on this metric learning method based on pairwise similarity con-
category of semi-supervised learning problems.        straints. Like many kernel methods, a limitation of this
                                                      method is that it does not scale up well with the sample size.
                                                      In Section 3, we address the scalability issue by applying
1.2  Semi-Supervised Metric Learning Based on         low-rank approximation to the kernel matrix. This extended
     Pairwise Constraints                             method can also naturally give rise to out-of-sample gener-
                                                      alization, which is addressed in Section 4. We present some
While many semi-supervised learning methods assume the experimental results in Section 5 to demonstrate the effec-
existence of limited labeled data [Zhu, 2006], there are much tiveness of our metric learning algorithm. Finally, Section 6
fewer methods that can work with pairwise constraints only. concludes the paper.
We survey the most representative methods in this subsection.
  Very often, the pairwise constraints simply state whether 2 Kernel-Based Metric Learning
two examples belong to the same class or different classes. 2.1 Problem Setup
[Wagstaff and Cardie, 2000] ﬁrst used such pairwise infor-
                                                         {x }n            n                           X
mation for semi-supervised clustering tasks by modifying the Let i i=1 be a set of data points in some input space .
standard k-means clustering algorithm to take into account Suppose we have a Mercer kernel kˆ which induces a nonlin-
the pairwise similarity and dissimilarity constraints. Exten- ear feature map φˆ from X to some reproducing kernel Hilbert
sions have also been made to model-based clustering based space H [Sch¨olkopf and Smola, 2002]. The corresponding set
on the expectation-maximization (EM) algorithm for Gaus-                          n
                                                      of feature vectors in H is {φˆ(xi)}i=1 and the kernel matrix is
sian mixture models [Shental et al., 2004; Lu and Leen,
                                                      Kˆ =[kˆ(xi, xj)]n×n =[φˆ(xi), φˆ(xj )]n×n. Choices for kˆ
2005]. However, these methods do not explicitly learn a dis-
                                                      include the Gaussian RBF kernel and the polynomial kernel.
tance function but seek to satisfy the constraints, typically for
                                                      We apply a centering transform such that the feature vectors
clustering tasks. Hence, they are sometimes referred to as
                                                      in H have zero mean. The resulting kernel matrix K can be
constraint-based clustering methods.
                                                      computed as K =[k(xi, xj )]n×n =[φ(xi),φ(xj )]n×n =
  As a different category, some methods have been proposed HKHˆ , where the centering matrix H = I − (1/n)11T with
to learn a Mahalanobis metric or some other distance function I being the n × n identity matrix and 1 being the n × 1 vector
based on pairwise constraints. [Xing et al., 2003] formulated of ones.
a convex optimization problem with inequality constraints to We consider a type of semi-supervised metric learning in
learn a Mahalanobis metric and demonstrated performance which the supervisory information is given in the form of
improvement in a subsequent clustering task. Solving a simi- pairwise similarity constraints.1 Speciﬁcally, we are given
lar problem to learn a Mahalanobis metric, the relevant com- a set of point pairs, which is a subset of X×X,asS =
ponent analysis (RCA) algorithm [Bar-Hillel et al., 2003;
                                                      {(xi, xj) | xi and xj belong to the same class}. Our goal is to
    ]
2005 was proposed as a simpler and more efﬁcient algorithm make use of S to learn a better metric through modifying the
          [             ]
than that of Xing et al., 2003 . However, RCA can make use kernel so that the performance of some subsequent task (e.g.,
                          [              ]
of similarity constraints only. Hertz et al., 2004 proposed clustering, classiﬁcation) based on the metric is improved af-
a distance function learning method called DistBoost. How- ter kernel learning.
ever, there is no guarantee that the distance function learned
is a metric. [Bilenko et al., 2004] explored the possibility of 2.2 Kernel Learning
integrating constraint-based clustering and semi-supervised
                                                      Since the kernel matrix K is symmetric and positive semi-
clustering based on distance function learning. The distance                               p        T
                                                      deﬁnite, we can express it as K =    r=1 λrvrvr  =
function learning methods reviewed above either learn a Ma- p
                                                        r=1 λrKr, where λ1 ≥  ··· ≥ λp >  0 are the p ≤ n
halanobis metric that corresponds to linear transformation                K  v ,...,v
only, or learn a distance function that is not a metric. In our positive eigenvalues of , 1 p are the corresponding
                                                                               K    v  vT
previous work [Chang and Yeung, 2004], we proposed a met- normalized eigenvectors, and r = r r .
ric learning method that corresponds to nonlinear transfor- We consider a restricted form of kernel matrix learning
mation. While this method is more powerful than the linear by modifying K through changing the λr’s while keep-
methods, the optimization problem is non-convex and hence ing all Kr’s ﬁxed. To ensure that the eigenvalues are
                                                      nonnegative, we rewrite K as Kβ =[kβ(xi, xj)]n×n =
is more complicated to solve.                                                   p    2
                                                      [φβ(xi),φβ(xj)]n×n =        βr Kr, which represents
  In this paper, we focus on the distance function learn-                       r=1
                                                      a  family of kernel matrices parameterized by β  =
ing approach because distance functions are central to many      T
                                                      (β1,...,βp) .
learning models and algorithms. This also makes it easier to
achieve out-of-sample generalization. Moreover, we focus on 1As we will see later in the formulation of the optimization prob-
learning metrics because this allows us to formulate the met- lem for kernel learning, these constraints are “soft” constraints rather
ric learning problem based on the kernel approach [Sch¨olkopf than “hard” constraints in that they are only preferred, not enforced.
and Smola, 2002], which provides a disciplined, computa- This makes it easy to handle noisy constraints, i.e., erroneous super-
tionally appealing approach to nonlinear metric learning. visory information, if we so wish.

                                                IJCAI-07
                                                  1139  We perform kernel learning such that the mean squared Eu- 3.1 Low-Rank Approximation
clidean distance induced by Kβ between feature vectors in H We apply low-rank approximation to approximate K by an-
                          S
corresponding to point pairs in is reduced. Thus the crite- other n × n symmetric and positive semi-deﬁnite matrix K :
rion function for optimization is
                                                                                      T
                                                                       K  K = WLW     ,             (5)
  JS (β)                                                    W  ∈ Rn×m       n × m           L  ∈ Rm×m
                                                     where            is an       matrix and          is
         1          K       K     −   K               an m × m symmetric and positive semi-deﬁnite matrix, with
    =   |S|       [(  β)ii +( β)jj  2(  β)ij ]        m  
 n
           (x ,x )∈S                                        .
             i ⎡j                              ⎤        There are different ways to construct L for low-rank ap-
        p                                           proximation. We consider one way which constructs L us-
           β2 ⎣ 1          b − b  T K  b −  b  ⎦                      n
    =       r  |S|        ( i   j)   r( i    j)       ing a subset of the data points. We refer to these points as
        r=1       (xi,xj )∈S                          landmarks [de Silva and Tenenbaum, 2003; Weinberger et al.,
                                                                          ]
        βT D β,                                       2005; Silva et al., 2006 . Without loss of generality, we as-
    =       S                                   (1)   sume that the n points are ordered in such a way that the ﬁrst
                                                      m                             {x }m
                                              2          points form the set of landmarks i i=1. We should en-
where bi is the ith column of the p × p identity matrix and
                                                      sure that all points involved in S are chosen as landmarks.
DS is a p × p diagonal matrix with diagonal entries
                                                      Other landmarks are randomly sampled from all the data
                                                     points. Similar to K in the previous section, L is obtained
    D           1          b −  b  T K b  − b
   (  S )rr =   |S|       ( i    j)   r( i   j)       here by applying the centering transform to Lˆ,asL = HLHˆ ,
                   (x ,x )∈S
                    i j                               where Lˆ   kˆ xi, xj m×m    φˆ xi , φˆ xj  m×m is the
                                                             =[(       )]     =[   (  )  (  ) ]
                1                  T   2                       m × m             Kˆ
            =             [(bi − bj) vr] ≥ 0.   (2)   upper-left     submatrix of .
                |S|                                     We apply eigendecomposition on L and express it as
                   (xi,xj )∈S
                                                                      q
                                                                 L       μ α  αT   V   D  VT ,
  To prevent β from degenerating to the zero vector 0 and           =     r  r r =   α  μ  α          (6)
to eliminate the scaling factor, we minimize the convex func-         r=1
                                    T
tion JS (β) subject to the linear constraint 1 β = c for some where μ1 ≥···≥μq > 0 are the q ≤ m positive eigenvalues
constant c>0. This is a simple convex optimization prob- of L, α1,...,αq are the corresponding normalized eigenvec-
                                                          D          μ ,...,μ      V     α  ,...,α
lem with a quadratic objective function and a linear equality tors, μ = diag( 1 q),and α =[ 1    q]. Sub-
                                       ρ                                                      q     
constraint. We introduce a Lagrange multiplier to minimize stituting (6) into (5), we can rewrite K as K = r=1 μrKr,
the following Lagrangian:                                                     T
                                                      where Kr =(Wαr)(Wαr)      .
                                   T
           JS (β,ρ)=JS (β)+ρ(c  − 1  β).        (3)   3.2  Kernel Learning
                                                      We apply low-rank approximation to devise a scalable kernel
  The optimization problem can be solved easily to give the learning algorithm which can be seen as an extension of the
optimal value of β as the following closed-form solution:
                                                      algorithm described in Section 2. We use K to approximate
                                                      K
                        cD−11                            and deﬁne the following parameterized family of kernel
                   β =     S   .                (4)            K      q  β2K .                 β
                       1T D−11                        matrices: β =    r=1 r  r Note, however, that is now
                           S                          a q × 1 vector rather than a p × 1 vector.
          −1                                            The optimal value of βhas the same form as (4), except
Note that D  exists as long as all the diagonal entries of                    q   √
          S                                           that the constant c is set to μr and DS is now a q × q
                                3                                             r=1
DS are positive, which is usually true. We set the constant
   p    √                                            diagonal matrix with diagonal entries
c =   r=1  λr.                                                          
                                                          D        1          b  − b  T Wv    2 ≥  .
                                                         (  S )rr = |S|      [( i   j) (    r)]   0   (7)
3  Scalable Kernel Learning                                           (xi,xj )∈S
                                                      3.3  Computing the Embedding Weights
The kernel learning method described above requires per-                                              W
forming eigendecomposition on K. In case n is very large A question that remains to be answered is how to obtain
and hence K is a large matrix, operations such as eigende- for low-rank approximation. We use a method that is similar
composition on K are computationally demanding. In this to locally linear embedding (LLE) [Roweis and Saul, 2000;
section, we apply low-rank approximation to extend the ker- Saul and Roweis, 2003], with two differences. First, we only
nel learning method so that it scales up well with n. use the ﬁrst part of LLE to obtain the weights for locally linear
                                                      ﬁtting. Second, we perform locally linear ﬁtting in the kernel-
                                                      induced feature space H rather than the input space X .
  2This indicator variable will be “overloaded” later to refer to a                            T
                                                        Let W  =[wij ]n×m and wi =(wi1,...,wim) .Ifxi is a
column of any identity matrix whose size is clear from the context.
  3                                                   landmark, i.e., 1 ≤ i ≤ m,then
   In case DS is really singular (though a rare case), a common             
way to make it invertible is to add a term I to DS where  is a             1  i = j
                                                                     wij =                            (8)
small positive constant.                                                     0  otherwise.

                                                IJCAI-07
                                                  1140                                                                                            n
If xi is not a landmark, then we minimize the following func- Y of H, so that the non-landmark points {xi}i=m+1 and any
tion to obtain wi:                                    out-of-sample point x can be embedded into Y in the same
                
                       

                
                      
2            way.
        E(wi)=
φ(xi)  −         wij φ(xj )
 ,   (9)     Let {u1,...,uq} be an orthonormal basis with each ur ∈
                                                      H                                          r
                        φ(xj )∈Ni                        being a unit vector along the direction of the th prin-
                                                      cipal component. We deﬁne  U  =[u1,...,uq].Then
where Ni is the set of K nearest landmarks of φ(xi) in H, each landmark xi (i =1,...,m) can be embedded into
                                       T                                                    T
                               wij   1  wi            Y           q                 y      U  φ x  .
subject to the constraints φ(xj )∈Ni =     =1and         to give a-dimensional vector i =     ( i) Since
                                                              1    m
w            φ x  ∈N                E w              ur     √        αjrφ xj ,               yi    yi
 ij =0for all ( j )   i. We can rewrite ( i) as           =   μr   j=1     (  ) we can express   as    =
                                                       −1/2  T        1/2  T
                                             T        Dμ    VαLbi  =  Dμ  Vαbi. Let Ym  =[y1,...,ym].So
  E(wi)=                 wij wik(φ(xi) − φ(xj )) ·
                                                      Y     D1/2VT  .
             φ(x ),φ(x )∈N                              m =   μ    α
                j   k   i                                                       {x }n
                                                        For the non-landmark points i i=m+1, from (9), we use
                         (φ(xi) − φ(xk))              
                                                      φ xi              wij φ xj              φ xi     yi
               T                                       (  )=    φ(xj )∈Ni   (  ) to approximate ( ) and
          =  wi Giwi,                          (10)                           
                                                      to denote the embedding of φ(xi) in Y. Thus we have
where                                                           T            T       1/2  T  T
                                                         yi = U φ(xi)=YmW      bi = Dμ  VαW    bi.  (13)
Gi =[k(xi, xi)+k(xj, xk) − k(xi, xj) − k(xi, xk)]K×K    Similarly, for any out-of-sample example x,weuse
                                                             
                                               (11)   φ x              wjφ  xj               φ x     y
                                                       ( )=     φ(xj )∈Ni  (  ) to approximate ( ) and to
is the local Gram matrix of xi in H.                                       
                                                      denote the embedding of φ(x) in Y. The embedding weights
  To prevent wi from degenerating to 0, we minimize E(wi)             T
                                       T              w  =(w1,...,wm)    can be determined as in Section 3.3.
subject to the constraints     wij = 1  wi =1and
                        φ(xj )∈Ni                                                    {x }n
w            φ x   ∈N                                Similar to the non-landmark points i i=m+1, we can ob-
 ij =0for all  ( j )   i. As above for the kernel learn- tain
ing problem, we solve a convex optimization problem with a                    1/2  T
                                                                        y = Dμ  Vαw.                (14)
quadratic objective function and a linear equality constraint.
                                                        Based on (13) and (14), the squared Euclidean distance be-
The Lagrangian with a Lagrange multiplier α is as follows:
                                                      tween yi and y in Y before kernel learning can be expressed
                     T              T
         L(wi,α)=wi   Giwi +  α(1 − 1 wi).     (12)   as
                                                         2                   2
                                                        d (xi, x)=yi   − y
The closed-form solution for this optimization problem is               T       T         T    T
               −1      T  −1      −1      4                       =(bi   W  − w  )VαDμVα(W      bi − w)
given by wi =(Gi 1)/(1  Gi  1) if Gi exists.
                                                                        T       T      T
  Instead of performing matrix inversion, a more efﬁcient         =(bi   W  − w  )L(W   bi − w).     (15)
way of ﬁnding the solution is to solve the linear system of The squared Euclidean distance after kernel learning is
equations Giwˆ i = 1 for wˆ i and then compute wi as wi = 2         T       T         T    T
     T                                     T           dβ(xi, x)=(bi  W  − w  )VαDβVα(W     bi − w), (16)
wˆ i/(1 wˆ i) to ensure that the equality constraint 1 wi =1
                                                                       2      2
is satisﬁed.                                          where Dβ =diag(β1 ,...,βq ).
  We assume above that the neighborhood relationships be-
tween points in H and the local Gram matrices remain ﬁxed 5 Experimental Results
during the kernel learning process. A simple extension of this In this section, we present some experiments we have per-
basic algorithm is to repeat the above procedure iteratively formed based on both artiﬁcial and real-world data.
using the learned kernel at each iteration. Thus the basic al-
gorithm is just a special case of this iterative extension when 5.1 Experimental Setup
the number of iterations is equal to one.             We compare two versions of our kernel-based metric learning
                                                      method described in Sections 2 and 3 with RCA [Bar-Hillel et
4  Out-of-Sample Generalization                       al., 2003; 2005], which is a promising linear metric learning
The exact form of out-of-sample generalization depends on method that usually performs equally well as other computa-
                                                      tionally more demanding methods. For baseline comparison,
the operation we want to perform. For example, the n given
                                                      we also include two metrics without metric learning. They
points are ﬁrst clustered into C ≥ 2 classes after kernel learn-
ing, and then a new data point x is classiﬁed into one of the are the Euclidean metric in the input space and the Euclidean
                                                      metric in the feature space induced by a Gaussian RBF ker-
C classes. We are interested in the case where both the clus-
           n                                          nel.
tering of the points and the classiﬁcation of new points are                                       S
based on the same Euclidean metric in H.                For each data set, we randomly generate 10 different sets.
                                                      For small data sets, we can learn Kβ without low-rank ap-
  The key idea of our out-of-sample generalization scheme
                                                      proximation. For large data sets, in addition to the data points
rests on the observation that kernel principal component anal-
                                                      involved in S, we also randomly select some other points as
ysis (KPCA) [Sch¨olkopf et al., 1998] can be performed on                  
    n                                                                     Kβ
{xi}i=1 to obtain an embedding in a q-dimensional subspace landmarks for learning . We use the iterative extension
                                                      of the scalable kernel learning algorithm with the number of
  4
   Similar to DS above, we may add I to make sure that Gi is iterations equal to 3. We also measure the change in metric
invertible.                                           learning performance as the number of landmarks increases.

                                                IJCAI-07
                                                  1141                   2                       2


                                                                   2
                   1                       1

                                                                   1


                   0                       0                       0


                                                                   −1

                   −1                      −1
                                                                   −2


                   −2                      −2
                   −2  −1   0    1    2    −2   −1   0   1    2    −2   −1   0   1    2
                           (a)                     (b)                     (c)


                  0.08                     0.2                     0.6


                  0.04                     0.1                     0.3


                   0                       0                       0


                  −0.04                   −0.1                    −0.3


                  −0.08                   −0.2                    −0.6
                   −0.08 −0.04 0 0.04 0.08 −0.08 −0.04 0 0.04 0.08   −0.2 −0.1 0 0.1 0.2
                           (d)                     (e)                     (f)

Figure 1: XOR illustration. (a) input data and similarity constraints; (b) new data points; (c) RCA; (d) RBF; (e) Kβ;
   
(f) Kβ (m = 40).

                                                                       
                                                            db    /nb         xi − xj
5.2  An Illustrative Example                          where   =(1     )  yi=yj        is the mean between-
                                                                       n
Figure 1 uses the XOR data set to compare the performance class distance with b being the number of point pairs with
                                                                            dw     /nw        xi − xj
of different metrics, some with metric learning. Figure 1(a) different class labels, and =(1 ) yi=yj   is
shows 360 data points in the input space. The points with the mean within-class distance with nw being the number of
the same color and mark belong to the same class. The ran- point pairs with the same class label. Note that J is closely re-
domly generated point pairs corresponding to the similarity lated to the optimization criterion JS in (1), except that JS is
constraints in S are shown in solid lines. Figure 1(b) shows deﬁned for the labeled data (i.e., data involved in the pairwise
40 new data points in the input space. The results obtained by constraints) only while J is for all data assuming the exis-
RCA, RBF kernel and two versions of our method are shown tence of true class labels. For kernel methods, we use φ(xi)
                                                         
in Figure 1(c)–(f). RCA performs metric learning directly in or φ(xi) in place of xi and apply the kernel trick to compute
the input space, which can be generalized to new data using the mean distances and hence J. A larger value of J corre-
the learned linear transformation. For the kernel methods, sponds to a better metric due to its higher class separability.
we apply KPCA using the (learned) kernel matrix to embed We ﬁrst perform some experiments on a much larger XOR
the data points to a 2-dimensional space, as shown in Fig- data set with 8,000 data points. We randomly select 50 sim-
ure 1(d)–(f). New data points can be embedded using the ilarity constraints to form S and measure the metric learning
generalization method described in Section 4. As expected, performance in terms of the J value for an increasing number
RCA does not perform satisfactorily for the XOR data set of landmarks. Table 1 shows the results for different metrics.
since it can only perform linear transformation. On the other For the metric learning methods (i.e., RCA and our method),
hand, our kernel-based metric learning method can group the we show for each trial the mean (upper) and standard devia-
points according to their class membership. The result of the tion (lower) over 10 random runs corresponding to different
scalable kernel learning method with low-rank approximation S sets. From the results, we can see that our method outper-
based on 40 landmarks is almost as good as that of the basic forms the other methods signiﬁcantly. Moreover, using more
algorithm based on all 400 data points. Moreover, the result landmarks generally gives better results.
on embedding of new data points veriﬁes the effectiveness of We further perform some experiments on real-world data
our out-of-sample generalization method.              sets. One of them is the Isolet data set from the UCI Machine
                                                      Learning Repository which contains 7,797 isolated spoken
5.3  Quantitative Performance Comparison              English letters belonging to 26 classes, with each letter repre-
       n
Let {yi}i=1 be the set of true class labels of the n data points. sented as a 617-dimensional vector. Other data sets are hand-
We deﬁne the following performance measure:           written digits from the MNIST database.5 The digits in the
                                                      database have been size-normalized and centered to 28×28
                          db
                     J =    ,                  (17)      5
                          dw                             http://yann.lecun.com/exdb/mnist/

                                                IJCAI-07
                                                  1142