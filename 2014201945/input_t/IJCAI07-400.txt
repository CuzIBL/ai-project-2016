             Revisiting Output Coding for Sequential Supervised Learning
                                     Guohua Hao     and  Alan Fern
                        School of Electrical Engineering and Computer Science
                                        Oregon State University
                                       Corvallis, OR 97331 USA
                                  {haog, afern}@eecs.oregonstate.edu
                    Abstract                          and Moore, 2005]. Both of these approaches have demon-
                                                      strated good performance on a number of sequence-labeling
    Markov models are commonly used for joint in-     problems that satisfy the assumptions of the models.
    ference of label sequences. Unfortunately, infer-
    ence scales quadratically in the number of labels,  A third recent approach, which is the focus of this paper, is
    which is problematic for training methods where   sequential error-correcting output coding (SECOC) [Cohn et
    inference is repeatedly preformed and is the pri- al., 2005] motivated by the success of error-correcting output
    mary computational bottleneck for large label sets. coding (ECOC) for non-sequential problems [Dietterich and
    Recent work has used output coding to address this Bakiri, 1995]. The idea is to use an output code to “reduce” a
    issue by converting a problem with many labels to multi-label sequence-learning problem to a set of binary-label
    a set of problems with binary labels. Models were problems. SECOC solves each binary problem independently
    independently trained for each binary problem, at and then combines the learned models to make predictions
    a much reduced computational cost, and then com-  over the original large label set. The computational cost of
    bined for joint inference over the original labels. learning and inference scales linearly in the number of binary
    Here we revisit this idea and show through exper- models and is independent of the number of labels. Experi-
    iments on synthetic and benchmark data sets that  ments on several data sets showed that SECOC signiﬁcantly
    the approach can perform poorly when it is critical reduced training and labeling time with little loss in accuracy.
    to explicitly capture the Markovian transition struc- While the initial SECOC results were encouraging, the
    ture of the large-label problem. We then describe a study did not address SECOC’s general applicability and its
    simple cascade-training approach and show that it potential limitations. For non-sequential learning, ECOC has
    can improve performance on such problems with     been shown to be a formal reduction from multi-label to bi-
    negligible computational overhead.                nary label classiﬁcation [Langford and Beygelzimer, 2005].
                                                      One contribution of this paper is to show that this result does
1  Introduction                                       not hold for sequential learning. That is, there are sequence
                                                      labeling problems, such that for any output code, SECOC per-
Sequence labeling is the problem of assigning a class label forms poorly compared to directly learning on the large label
to each element of an input sequence. We study supervised set, even assuming optimal learning for the binary problems.
learning for sequence-labeling problems where the number
of labels is large. Markov models are widely used for se- Given the theoretical limitations of SECOC, and the prior
quence labeling, however, the time complexity of standard empirical success, the main goals of this paper are: 1) to bet-
inference algorithms, such as Viterbi and forward-backward, ter understand the types of problems for which SECOC is
scales quadratically with the number of labels. When this suited, 2) to suggest a simple approach to overcoming the
number is large, this can be problematic when predictions limitations of SECOC and to evaluate its performance. We
must be made quickly. Even more problematic is the fact present experiments on synthetic and benchmark data sets.
that a number of recent approaches for training Markov mod- The results show that the originally introduced SECOC per-
els, e.g. [Lafferty et al., 2001; Tsochantaridis et al., 2004; forms poorly for problems where the Markovian transition
Collins, 2002], repeatedly perform inference during learning. structure is important, but where the transition information is
As the number of labels grows, such approaches quickly be- not captured well by the input features. These results suggest
come computationally impractical.                     that when it is critical to explicitly capture Markovian transi-
  This problem has led to recent work that considers various tion structure SECOC may not be a good choice.
approaches to reducing computation while maintaining high In response we introduce a simple extension to SECOC
accuracy. One approach has been to integrate approximate- called cascaded SECOC where the predictions of previously
inference techniques such as beam search into the learning learned binary models are used as features for later mod-
process [Collins and Roark, 2004; Pal et al., 2006]. A sec- els. Cascading allows for richer transition structure to be
ond approach has been to consider learning subclasses of encoded in the learned model with little computational over-
Markov models that facilitate sub-quadratic inference—e.g. head. Our results show that cascading can signiﬁcantly im-
using linear embedding models [Felzenszwalb et al., 2003], prove on SECOC for problems where capturing the Markov-
or bounding the number of distinct transition costs [Siddiqi ian structure is critical.

                                                IJCAI-07
                                                  24862  Conditional Random Fields                            This idea has recently been applied to sequence labeling
We study the sequence-labeling problem, where the goal problems under the name sequential error-correcting output
is to learn a classiﬁer that when given an observation se- coding (SECOC) [Cohn et al., 2005], with the motivation of
quence X =(x1, x2,...,xT ) returns a label sequence Y = reducing computation time for large label sets. In SECOC,
(y1,y2,...,yT ) that assigns the correct class label to each instead of training a multi-class CRF, we train a binary-class
observation. In this work, we will use conditional random CRF hk for each column bk of a code matrix M. More specif-
ﬁelds (CRFs) for sequence labeling. A (sequential) CRF is ically the training data for CRF hk is given by {(Xi,bk(Yi))},
a Markov random ﬁeld [Geman and Geman, 1984] over the where bk(Y )=(bk(y1),bk(y2),...,bk(yT )) is a binary la-
label sequence Y globally conditioned on the observation se- bel sequence. Given a set of binary classiﬁers for a code ma-
quence X. The Hammersley-Clifford theorem states that the
                        |X                            trix and an observation sequence X we compute the multi-
conditional distribution P (Y ) has the following form: label output sequence as follows. We ﬁrst use the forward-
                          
                    1                                 backward algorithm on each hk to compute the probabil-
        P (Y |X)=      exp    Ψ(yt−1,yt, X),
                  Z(X)                                ity that each sequence position should be labeled 1. For
                            t
                                                      each sequence element xt we then form a probability vector
                                                         x
where Z(X) is a normalization constant, and Ψ(yt−1,yt, X) H( t)=(p1, ..., pn) where pk is the probability computed
                                                              x                       x
is the potential function deﬁned on the clique (yt−1,yt).For by hk for t. We then let the label for t be the class label yt
simplicity, we only consider the pair-wise cliques (yt−1,yt), whose codeword is closest to H(xt) based on L1 distance.
i. e. chain-structured CRFs, and assume that the potential The complexity of this inference process is just O(n · T )
function does not depend on position t.Asin[Lafferty et where n is the codeword length, which is typically much
al., 2001], the potential function is usually represented as a smaller than the number of labels squared. Thus, SECOC
linear combination of binary features:                can signiﬁcantly speed inference time for large label sets.
                                                                [             ]
 Ψ(       X)=          (  X)+         (       X)        Prior work Cohn et al., 2005 has demonstrated on several
   yt−1,yt,        λαfα yt,       γβgβ yt,yt−1, .     problems that SECOC can signiﬁcantly speed training time
                α               β                     without signiﬁcantly hurting accuracy. However, this work
The functions f capture the relationship between the label yt did not address the potential limitations of the SECOC ap-
and the input sequence X; the boolean functions g capture proach. A key characteristic of SECOC is that each binary
the transitions between yt−1 and yt (conditioned on X). CRF is trained completely independently of one another and
  Many CRF training algorithms have been proposed for each binary CRF only sees a very coarse view of the multi-
training the parameters λ and γ, however, most all of these class label sequences. Intuitively, it appears difﬁcult to repre-
methods involve the repeated computation of the partition sent complex multi-class transition models between yt−1 and
function Z(X) and/or maximizing over label sequences, yt using such independent chains. This raises the fundamen-
which is usually done using the forward-backward and  tal question of the representational capacity of the SECOC
Viterbi algorithms. The time complexity of these algorithms model. The following counter example gives a partial answer
is O(T ·Lk+1),whereL is the number of class labels, k is the to this question, showing that the SECOC model is unable to
order of Markov model, and T is the sequence length. So even represent relatively simple multi-label transition models.
for ﬁrst order CRFs, training and inference scale quadratically SECOC Counter Example. Consider a simple Markov
                                                      model with three states Y = {1, 2, 3} and deterministic tran-
in the number of class labels, which becomes computationally  →      →         →
demanding for large label sets. Below we describe the use of sitions 1 2, 2 3 and 3 1. A 3-by-3 diagonal code
                                                      matrix is sufﬁcient for capturing all non-trivial codewords for
output coding for combating this computational burden. this label set—i. e. all non-trivial binary partitions of Y.Be-
                                                      low we show an example label sequence and the correspond-
3  ECOC for Sequence Labeling                         ing binary code sequences.

                                                                       y1 y2  y3  y4 y5  y6  y7
For non-sequential supervised classiﬁcation, the idea of          Y    1231231
Error-Correcting Output Coding (ECOC) has been success-          b1(Y )1001001
                                     [                           b2(Y )0100100
fully used to solve multi-class problems Dietterich and          b3(Y )0010010
Bakiri, 1995]. A multi-class learning problem with train-
ing examples {(xi,yi)} is reduced to a number of binary- Given a label sequence Y = {1, 2, 3, 1, 2, 3, 1,...},aﬁrst-
class learning problems by assigning each class label j abi- order Markov model learned for b1(Y ) will converge to
nary vector, or code word, Cj of length n. A code matrix P (yt =1|yt−1 =0)=P (yt =0|yt−1 =0)=0.5. It can
M  is built by taking the code words as rows. Each column be shown that as the sequence length grows such a model will
bk in M can be regarded as a binary partition of the origi- make i.i.d. predictions according to the stationary distribution
nal label set, bk(y) ∈{0, 1}. We can then learn a binary that predicts 1 with probability 1/3. The same is true for b2
classiﬁer hk using training examples {(xi,bk(yi))}.Topre- and b3. Since the i.i.d. predictions between the binary CRFs
dict the label of a new instance x, we concatenate the pre- are independent, using these models to make predictions via
dictions of each binary classiﬁer to get a vector H(x)= SECOC will yield a substantial error rate, even though the
(h1(x),h2(x),...,hn(x). The predicted label yˆ is then given sequence is perfectly predictable. Independent ﬁrst-order bi-
by yˆ =argminj Δ(H(x),Cj  ),whereΔ  is some distance  nary transition models are simply unable to capture the tran-
measure, such as Hamming distance. In some implementa- sition structure in this problem. Our experimental results will
tions H(x) stores probabilities rather than binary labels. show that this deﬁciency is also exhibited in real data.

                                                IJCAI-07
                                                  24874  Cascaded SECOC Training of CRFs                    transition model. This model allows us to assess the accuracy
Each binary CRF in SECOC has very limited knowledge   that can be attained by looking at only a window of observa-
about the Markovian transition structure. One possibility to tion features. Second, we denote by “i-SECOC” the use of
improve this situation is to provide limited coupling between i-SECOC to train ﬁrst-order binary CRFs. Third, we denote
the binary CRFs. One way to do this is to include observation by “c-SECOC(h)” the use of c-SECOC to train ﬁrst-order bi-
features in each binary CRF that record the binary predictions nary CRFs using a cascade history of length h.
of previous CRFs. We call this approach cascaded SECOC  Summary of Results. The results presented below justify
(c-SECOC), as opposed to independent SECOC (i-SECOC). ﬁve main claims: 1) i-SECOC can fail to capture signiﬁcant
                            { X     }     (j)         transition structures, leading to poor accuracy. Such obser-
  Given training examples S = ( i,Yi) ,letYi be the   vations were not made in the original evaluation of i-SECOC
prediction of the binary CRF learned with the j-th binary par-
            (j)                     (j)               [Cohn et al., 2005]. 2) c-SECOC can signiﬁcantly improve
tition bj and yit be the t-th element of Yi . To train the on i-SECOC through the use of cascade features. 3) The per-
binary CRF hk based on binary partition bk, each training ex-
                                                    formance of c-SECOC can depend strongly on the base CRF
ample (Xi,Yi) is extended to (X ,bk(Yi)), where each x is
                           i                   it     algorithm. In particular, it appears critical that the algorithm
the union of the observation features xit and the predictions
of the previous h binary CRFs at sequence positions from t−l be able to capture complex (non-linear) interactions in the ob-
to t + r (l = r =1in our experiments) except position t: servation and cascade features. 4) c-SECOC can improve on
                                                      models trained using beam search when GTB is used as the
   x =(x    (k−1)     (k−1) (k−1)    (k−1)
    it    it,yi,t−l ,...,yi,t−1 ,yi,t+1 ,...,yi,t+r , base CRF algorithm. 5) When using weaker base learning
            (k−h)     (k−h) (k−h)    (k−h)            methods such as VP, beam search can outperform c-SECOC.
        ...,yi,t−l ,...,yi,t−1 ,yi,t+1 ,...,yi,t+r ).
We refer to h as the cascade history length. We do not use the 5.1 Nettalk Data Set
                                          x
previous binary predictions at position t as part of it,since The Nettalk task [Sejnowski and Rosenberg, 1987] is to as-
such features have signiﬁcant autocorrelation which can eas- sign a phoneme and stress symbol to each letter of a word so
ily lead to overﬁtting. To predict Y for a given observation se-
      X                                               that the word is pronounced correctly. Here the observations
quence  , we make predictions from the ﬁrst binary CRF to correspond to letters yielding a total of 26 binary observa-
the last, feeding predictions into later binary CRFs as appro- tion features at each sequence position. Labels correspond to
priate, and then use the same decoding process as i-SECOC. phoneme-stress pairs yielding a total of 134 labels. We use
  Via the use of previous binary predictions, c-SECOC has the standard 1000 training and 1000 test sequences.
the potential to capture Markovian transition structure that i- Comparing to i-SECOC. Figures 1a and 1b show the re-
SECOC cannot. Our experiments show that this is impor- sults for training our various models using GTB with window
tant for problems where the transition structure is critical sizes 1 and 3. For window size 1, we see that i-SECOC is
to sequence labeling, but is not reﬂected in the observation able to signiﬁcantly improve over iid, which indicates that
features. The computational overhead of c-SECOC over i- i-SECOC is able to capture useful transition structure to im-
SECOC is to increase the number of observation features, prove accuracy. However, we see that by increasing the cas-
which typically has negligible impact on the overall training cade history length c-SECOC is able to substantially improve
time. As the cascade history grows, however, there is the po- over i-SECOC. Even with h =1the accuracy improves by
tential for c-SECOC to overﬁt with the additional features. over 10 percentage points. This indicates that the indepen-
We will discuss this issue further in the next section. dent CRF training strategy of i-SECOC is unable to capture
                                                      important transition structure in this problem. c-SECOC is
5  Experimental Results                               able to exploit the cascade history features in order to cap-
We compare CRFs trained using i-SECOC, c-SECOC, and   ture this transition information, which is particularly critical
beam search over the full label set. We consider two existing in this problem where apparently just using the information in
base CRF algorithms: gradient-tree boosting (GTB) [Diet- the observation window of length 1 is not sufﬁcient to make
terich et al., 2004] and voted perceptron (VP) [Collins, 2002]. accurate predictions (as indicated by the iid results).
GTB is able to construct complex features from the primitive Results for window size 3 are similar. However, the im-
observations and labels, whereas VP is only able to combine provement of i-SECOC over iid and of c-SECOC over i-
the observations and labels linearly. Thus, GTB has more SECOC are smaller. This is expected since the larger window
expressive power, but can require substantially more compu- size spans multiple sequence positions, allowing the model to
tational effort. In all cases we use the forward-backward al- capture transition information using the observations alone,
gorithm to make label-sequence predictions and measure ac- making the need for an explicit transition model less impor-
curacy according to the fraction of correctly labeled sequence tant. Nevertheless, both SECOC methods can capture useful
elements. We used random code matrices constrained so that transition structure that iid cannot, with c-SECOC beneﬁting
no columns are identical or complementary, and no class la- from the use of cascade features. For both window sizes, we
bels have the same code word.                         see that c-SECOC performs best for a particular cascade his-
  First, we consider a non-sequential baseline model denoted tory length, and increasing beyond that length decreases ac-
as “iid” which treats all sequence elements as independent curacy by a small amount. This indicates that c-SECOC can
examples, effectively using non-sequential ECOC at the se- suffer from overﬁtting as the cascade history grows.
quence element level. In particular, we train iid using i- Figures 1c and 1d show corresponding results for VP. We
SECOC with zeroth-order binary CRF, i. e. CRFs with no still observe that including cascade features allows c-SECOC

                                                IJCAI-07
                                                  2488                   80                                     77

                   70                                     76

                   60                                     75

                   50                                     74

                   40                                     73                      iid
                                                                                i-SECOC
                                                                               c-SECOC(1)
                   30                                     72                   c-SECOC(2)


                 prediction  accuracy (%)               prediction  accuracy (%) c-SECOC(3)
                                                                               c-SECOC(9)
                   20                       iid           71
                                         i-SECOC
                                        c-SECOC(1)
                   10                   c-SECOC(2)        70
                                        c-SECOC(5)
                                        c-SECOC(11)
                   0                                      69
                    0  20  40  60  80  100  120  140  160  180  200  0  20  40  60  80  100  120  140  160  180  200
                              length of code words                   length of code words
                      (a) window size 1 with GTB             (b) window size 3 with GTB

                   60                                     56

                                                          54
                   50
                                                          52
                   40
                                                          50

                   30                                     48

                                                          46

                 prediction  accuracy (%)  20           prediction  accuracy (%) iid
                                            iid
                                                          44         i-SECOC
                                         i-SECOC                    c-SECOC(9)
                                        c-SECOC(5)                  c-SECOC(19)
                   10                   c-SECOC(11)
                                                          42        c-SECOC(50)
                                        c-SECOC(19)                 c-SECOC(all)
                                        c-SECOC(50)
                                        c-SECOC(all)
                   0                                      40
                    0  20  40  60  80  100  120  140  160  180  200  0  20  40  60  80  100  120  140  160  180  200
                              length of code words                   length of code words
                       (c) window size 1 with VP              (d) window size 3 with VP
                    Figure 1: Nettalk data set with window sizes 1 and 3 trained by GTB and VP.

to improve upon i-SECOC, though compared to GTB longer believe this is because GTB requires forward-backward infer-
cascade histories are required to achieve improvement. No ence during training whereas VP does not, and this is more
overﬁtting was observed up to cascade history length 50. adversely affected by beam search. Rather, c-SECOC using
However, we were able to observe overﬁtting after code GTB performs signiﬁcantly better than VP with beam search.
length 160 by including all possible cascade history bits, de-
noted by c-SECOC(all). All of our overﬁtting results suggest 5.2 Noun Phrase Chunking
that in practice a limited validation process should be used to We consider the CoNLL 2000 Noun Phrase Chunking (NPC)
select the cascade history for c-SECOC. For large label sets, shared task that involves labeling the words of sentences.
trying a small number of cascade history lengths is a much This was one of the problems used in the original evaluation
more practical alternative than training on the full label set. of i-SECOC. There are 121 class labels, which are combi-
  GTB versus VP. c-SECOC using GTB performs signiﬁ-   nations of Part-of-speech tagging labels and NPC labels, and
cantly better than using VP. We explain this by noting that 21,589 observation features for each word in the sequences.
the critical feature of c-SECOC is that the binary CRFs are There are 8,936 sequences in the training set and 2,012 se-
able exploit the cascade features in order to better capture the quences in the test set. Due to the large number of observa-
transition structure. VP considers only linear combinations tion features, we can not get good results for GTB using our
of these features, whereas GTB is able to capture non-linear current implementation and only present results for VP.
relationships by inducing complex combinations of these fea- Comparing to i-SECOC. As shown in Figure 3a, for win-
tures and hence capturing richer transition structure. This in- dow size 1, i-SECOC outperforms iid and incorporating cas-
dicates that when using c-SECOC it can be important to use cade features allows c-SECOC to outperform i-SECOC by
training methods such as GTB that are able to capture rich a small margin. Again we see overﬁtting for c-SECOC for
patterns in the observations and hence of the cascade features. larger numbers of cascade features. Moving to window size
  Comparing to Beam Search. Here we compare the per-  3 changes the story. Here a large amount of observation in-
formance of c-SECOC with multi-label CRF models trained formation is included from the current and adjacent sentence
using beam search in place of full-Viterbi and forward- positions, and as a result the iid performs as well as any of the
backward inference, which is a common approach to achiev- SECOC approaches. The large amount of observation infor-
ing practical inference with large label sets. For beam search, mation at each sequence position appears to capture enough
we tried various beam-sizes within reasonable computational transition information so that SECOC gets little beneﬁt from
limits. Figure 2 shows the accuracy/training-time trade-offs learning an explicit transition model. This suggests that the
for the best beam-search results and the best results achieved performance of the SECOC methods in this domain are pri-
for SECOC with 200 code-word bits and various cascade his- marily a reﬂection of the ability of non-sequential ECOC.
tories. The graph shows the test set performance versus the This is interesting given the i-SECOC results in [Cohn et al.,
amount of training time in CPU seconds. First notice that 2005], where all domains involved a large amount of local
GTB with beam search is signiﬁcantly worse than for VP. We observation information. IID results were not reported there.

                                                IJCAI-07
                                                  2489                   80                                     77

                   70                                     75

                   60                                     73

                   50                                     71

                   40                                     69

                   30                                     67
                 prediction  accuracy (%)               prediction  accuracy (%)
                   20                                     65

                                       i-SECOC(GTB)                           i-SECOC(GTB)
                   10                  c-SECOC(GTB)       63                  c-SECOC(GTB)
                                        Beam(GTB)                              Beam(GTB)
                                         Beam(VP)                               Beam(VP)
                   0                                      61
                    0      5000   10000  15000  20000      0      5000   10000  15000  20000
                               CPU Seconds                             CPU Seconds
                          (a) window size 1                      (b) window size 3
                         Figure 2: Nettalk: comparison between SECOC and beam search.

                   82                                     88

                   81                                     87

                   80                                     86

                   79                                     85


                 prediction  accuracy (%)  78           prediction  accuracy (%)  84

                                            iid
                                         i-SECOC                                   iid
                   77                   c-SECOC(1)        83                    i-SECOC
                                        c-SECOC(3)                             c-SECOC(1)
                                        c-SECOC(7)                             c-SECOC(3)
                                        c-SECOC(19)                            c-SECOC(19)
                   76                                     82
                    0  20  40  60  80  100  120  140  160  180  200  0  20  40  60  80  100  120  140  160  180  200
                              length of code words                   length of code words
                          (a) window size 1                      (b) window size 3
                         Figure 3: NPC data set with window sizes 1 and 3 trained by VP.

  Comparing to Beam Search.  We compare the accuracy  increases the transition model importance.
versus training-time for models trained with SECOC and with “Transition” Data Set. Here we use po =0.2, ko =8,
beam search, using the already described experimental setup. pl =0.6, kl =2, so that the transition structure is very im-
Figure 4 shows that beam search performs much better than portant and observation features are quite uninformative. Fig-
the c-SECOC methods within the same training time using ure 5a shows the results for GTB training using window size
VP as the base learning algorithm. We believe the poor per- 1. We see that i-SECOC is signiﬁcantly better than iid indicat-
formance of c-SECOC compared to beam search is that us- ing that it is able to exploit transition structure not available
ing VP does not allow for rich patterns to be captured in the to iid. However, including cascade features allows c-SECOC
observations which include the cascade features. We hypoth- to further improve performance, showing that i-SECOC was
esize, as observed in Nettalk, that c-SECOC would perform unable to fully exploit information in the transition model. As
as well or better than beam search given a base CRF method in our previous experiments we observe that c-SECOC does
that can capture complex patterns in the observations. overﬁt for the largest number of cascade features. For win-
                                                      dow size 3 (graph not shown) the results are similar except
5.3  Synthetic Data Sets                              that the relative improvements are less since more observa-
                                                      tion information is available, making the transition model less
Our results suggest that i-SECOC does poorly when critical
                                                      critical. These results mirror those for the Nettalk data set.
transition structure is not captured by the observation features
                                                        “Both” Data Set. Here we use o   , o    , l     ,
and that c-SECOC can improve on i-SECOC in such cases.                            p  =0.6 k  =2  p =0.6
                                                       l     so that both the transition structure and observation
Here we further evaluate these hypotheses.            k  =2
                                                      features are very informative. Figure 5b shows results for
  We generated data using a hidden Markov model (HMM)
                                                      GTB with window size 3. The observation features provide a
with 40 labels/states {l1,...,l40} and 40 possible observa-
                                                      large amount of information and performance of iid is similar
tions {o1,...,o40}. To specify the observation distribution,
                                                      to that of the SECOC variants. Also we see that c-SECOC
for each label li we randomly draw a set Oi of ko observations
                                                      is unable to improve on i-SECOC in this case. This suggests
not including oi. If the current state is li, then the HMM gen-
                                                      that the observation information captures the bulk of the tran-
erates the observation oi with probability po and otherwise
                                                      sition information, and the performance of the SECOC meth-
generates a randomly selected observation from Oi. The ob-
                                                      ods is a reﬂection of non-sequential ECOC, rather than of
servation model is made more important by increasing po and
                                                      their ability to explicitly capture transition structure.
decreasing ko. The transition model is deﬁned in a similar
way. For each label li we randomly draw a set Li of kl labels
not including li. If the current state is li then the HMM sets 6 Summary
the next state to li with probability pl and otherwise generates We uncovered empirical and theoretical shortcomings of in-
a random next state from Li. Increasing pl and decreasing kl dependently trained SECOC. Independent training of binary

                                                IJCAI-07
                                                  2490