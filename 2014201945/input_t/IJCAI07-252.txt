              Using Focal Point Learning to Improve Tactic Coordination
                                in Human-Machine Interactions
                  Inon Zuckerman1     and  Sarit Kraus1  and  Jeffrey S. Rosenschein2
                 1Computer Science Department, Bar-Ilan University, Ramat Gan, Israel
                                      {zukermi,sarit}@cs.biu.ac.il
        2School of Engineering and Computer Science,    The Hebrew University, Jerusalem, Israel
                                            jeff@cs.huji.ac.il

                    Abstract                          points [Janssen, 1998]. In the second approach, there were
                                                      assumed to be two automated agents, and both agents ran the
    We consider an automated agent that needs to coor-
                                                      same focal point ﬁnder algorithm [Kraus et al., 2000].How-
    dinate with a human partner when communication
                                                      ever, the use of focal points when an automated agent needs
    between them is not possible or is undesirable (tac-
                                                      to coordinate with an arbitrary human partner has yet to be
    tic coordination games). Speciﬁcally, we exam-
                                                      explored, and it raises new challenges.
    ine situations where an agent and human attempt to
    coordinate their choices among several alternatives The main motivation for such research comes from the in-
    with equivalent utilities. We use machine learning creasing interest in task teams that contain both humans and
    algorithms to help the agent predict human choices automated agents. Human-Agent collaboration can take the
    in these tactic coordination domains.             form of robots working with human partners. Another sce-
                                                      nario is the development of user interfaces that diverge from
    Learning to classify general human choices, how-  a master-slave relationship with the user, adopting a collab-
    ever, is very difﬁcult. Nevertheless, humans      orative, task-sharing approach in which the computer explic-
    are often able to coordinate with one another in  itly considers its user’s plans and goals, and is thus able to
    communication-free games, by using focal points,  coordinate various tasks [Grosz, 2004].
    “prominent” solutions to coordination problems.
                                                        One important type of natural human-machine interaction
    We integrate focal points into the machine learn- is the anticipation of movement, without the need for prior
    ing process, by transforming raw domain data into explicit coordination. This movement can be physical, such
    a new hypothesis space. This results in classiﬁers as the movement of a robotic arm that is assisting a human
    with an improved classiﬁcation rate and shorter   in a construction task (e.g., a machine helping a human weld
    training time. Integration of focal points into learn- pipes). As humans naturally anticipate their partners’ choices
    ing algorithms also results in agents that are more in certain situations, we would like automated agents to also
    robust to changes in the environment.             act naturally in their interactions with humans. Coordinated
                                                      anticipation can also take place in virtual environments, in-
1  Introduction                                       cluding online games, where humans and automated agents
Agents often need to coordinate their actions in a coherent can inhabit shared worlds and carry out shared activities.
manner. Sometimes, achieving coherent behavior is the result There are several constraints implicit in the above scenar-
of explicit communication and negotiation. However, com- ios. First, the human partner with whom our automated agent
munication is not always possible, for reasons as varied as is trying to coordinate may not always be known ahead of
high communication costs, the need to avoid detection, dam- time, and we want coordination strategies suitable for novel
aged communication devices, or language incompatibility. partners. Second, the environment itself is not fully speciﬁed
  Schelling [1963] called coordination-without-communi- ahead of time, and may be conﬁgured somewhat randomly
cation scenarios tactic coordination games, and named these (although the overall domain is known, i.e., the domain ele-
games’ “prominent solutions” focal points. A classic exam- ments are a given, but not their speciﬁc arrangement). Third,
ple is the solution most people choose when asked to divide there is no option to “hard-wire” arbitrary coordination rules
$100 into two piles, of any sizes; they should attempt only into all participants, since we are not dealing with coordina-
to match the expected choice of some other, unseen player. tion between two centrally-designed agents.
Usually, people create two piles of $50 each, and that is what We speciﬁcally consider environments in which a human
Schelling dubbed a focal point.                       and automated agent aspire to communication-free coordina-
  The use of focal points to solve coordination games tion, and the utilities associated with coordinated choices are
has previously been studied, both theoretically and exper- equal. Clearly, if utilities for various choices differed, the
imentally, from two different perspectives. In the ﬁrst, agent and human could employ game theoretic forms of anal-
there were assumed to be two human coordinators, and re- ysis, which might specify certain strategies.
search explored a formal theoretical framework for focal Our integration of machine learning algorithms and focal

                                                IJCAI-07
                                                  1563point techniques (which we call Focal Point Learning [FPL]) • Singularity — give prominence to choices that are
is done via a semi-automatic data preprocessing technique. unique or distinguishable relative to other choices in the
This preprocessing transforms the raw domain data into a  same set. This uniqueness can be, for example, with re-
new data set that creates a new hypothesis space, consisting spect to some physical characteristics of the options, a
solely of general focal point attributes. We demonstrate that special arrangement, or a cultural convention.
using FPL results in classiﬁers (a mapping from a coordina-
tion problem to the choice selected by an arbitrary human co- We employ learning algorithms to help our agent discover
ordination partner) with a 40% to 80% higher correct classiﬁ- coordination strategies. Training samples, gathered from hu-
cation rate, and a shorter training time, than when using reg- mans playing a tactic coordination game, are used to create
ular classiﬁers, and a 35% higher rate than when using only an automated agent that performs well when faced with a
classical focal point techniques without applying any learn- new human partner in a newly generated environment. How-
ing algorithm. In another series of experiments, we show that ever, because of the aforementioned problems, applying ma-
applying these techniques can also result in agents that are chine learning on raw domain data results in classiﬁers hav-
more robust to changes in the environment.            ing poor performance. Instead, we use a Focal Point Learning
  In Section 2, we describe the motivation and approach of approach: we preprocess raw domain data, and place it into
Focal Point Learning. We then describe the experimental set- a new representation space, based on focal point properties.
                                                      Given our domain’s raw data Oi, we apply a transformation
ting (Section 3), its deﬁnitions (Section 3.1), and the domains T N  = T (O )      i, j
that were used in the experiments (Section 3.2). In Section 4 , such that j i ,where   are the number of at-
                                                      tributes before and after the transformation, respectively.
we discuss our results. Related work is considered in Sec-                   N
tion 5, and we conclude in Section 6.                   The new feature space  j is created as follows: each
                                                      v ∈ Oi is a vector of size i representing a game instance in
2  Focal Point Learning                               the domain (world description alongside its possible choices).
                                                      The transformation T takes each vector v and creates a new
Coordination in agent-human teams can be strengthened by vector u ∈ Nj, such that j =4×[number of choices]. T iter-
having agents learn how a general human partner will make ates over the possible choices encoded in v, and for each such
choices in a given domain. Learning to classify human choice computes four numerical values signifying the four fo-
choices in tactic coordination games, however, is difﬁcult: cal point properties presented above. For example, given a co-
1) No speciﬁc function to generalize — there is no known ordination game encoded as a vector v of size 25 that contains
mathematical function nor behavioral theory that predicts hu- 3 choices (c1,c2,c3), the transformation T creates a new vec-
man choices in these games; 2) Noisy data — Data collected     c  e  f  s c  e  f  s  c e  f  s
                                                      tor u =(c ,c ,c ,c ,c ,c ,c ,c ,c ,c ,c ,c ) of size 12
from humans in tactic coordination games tends to be very      1  1  1  1 2  2  2  2  3 3  3  3
                                                                      ×                        cc/e/f/s
noisy due to various social, cultural, and psychological fac- (3 possible choices 4 focal point rules), where l de-
tors, that bias their answers; 3) Domain complexity —train- notes the centrality/extremeness/ﬁrstness/singularity values
                                                                          j
ing requires a large set of examples, which in turn increases for choice l. Note that might be smaller than, equal to, or
                                                                i
required training time. On the other hand, human-human greater than , depending on the domain.
teams are relatively skilled at tactic coordination games. Ex- The transformation from raw domain data to the new rep-
periments that examined human tactic coordination strate- resentation in focal point space is done semi-automatically.
gies [Schelling, 1963; Mehta et al., 1994] showed that people To transform raw data from some new domain, one needs to
are often able to coordinate their choices on focal points, even provide a domain-speciﬁc implementation of the four general
when faced with a large set of options.               focal point rules. However, due to the generic nature of the
  Several attempts have been made to formalize focal points rules, this task is relatively simple, intuitive, and suggested
from a game theoretic, human interaction point of view by the domain itself (we will see such rules in Section 3.2).
([Janssen, 1998] provides a good overview). However, that When those rules are implemented, the agent can itself easily
work does not provide the practical tools necessary for use carry out the transformation on all instances in the data set.
in automated agents. However, Kraus et al. [2000] identiﬁed
some domain-independent rules that could be used by auto- 3 The Experimental Setting
mated agents to identify focal points (and discussed two ap-
proaches for doing so). The following rules are derived from We designed three domains for experiments in tactic coordi-
that work, but are adjusted in our presentation (by adding nation. For each domain, a large set of coordination problems
Firstness, and making minor changes in others).       was randomly generated, and the solutions to those problems
                                                      were collected from human subjects.
  • Centrality — give prominence to choices directly in the
                                                        We used the resulting data set to train three agents: 1) an
    center of the set of choices, either in the physical envi-
                                                      agent trained on the original domain data set (a Domain Data
    ronment, or in the values of the choices.
                                                      agent); 2) an agent using focal point rules without any learn-
  • Extremeness — give prominence to choices that are ex- ing procedure (an FP agent); and 3) an agent using Focal
    treme relative to other choices, either in the physical en- Point Learning (an FPL agent). We then compared coordina-
    vironment, or in the values of the choices.       tion performance (versus humans) of the three types of agent.
  • Firstness — give prominence to choices that physically In the second phase of our experiments (which tested ro-
    appear ﬁrst in the set of choices. It can be either the bustness to environmental changes), we took the ﬁrst domain
    option closest to the agent, or the ﬁrst option in a list. described in Section 3.2, and designed two variations of it;

                                                IJCAI-07
                                                  1564one variant (VSD, a very similar domain) had greater simi- experimental setup with both types of classiﬁers using exactly
larity to the original environment than the other variant (SD) the same domain encoding.
had. Data from human subjects operating in the two variant The transformation to focal point encoding provides focal-
settings was collected. We then carried out an analysis of au- ity values in terms of our low-level focal point rules (First-
tomated coordination performance in the new settings, using ness, Singularity, Extremeness, and Centrality) for each of
the agents that had been trained in the original domain. the possible choices. Their values were calculated in a pre-
                                                      processing stage, prior to the training stage (and by an agent
3.1  Deﬁnitions                                       when it needs to output a prediction). It is important to note
Deﬁnition 1 (Pure Tactic Coordination Games). Pure Tac- that following the transformation to the focal point encoding,
tic Coordination Games (also called matching games)are we deprive the classiﬁer of any explicit domain information
games in which two non-communicating players get a posi- during training; it trains only on the focal point information.
tive payoff only if both choose the same option. Both players
                                                      Changes in the Environment
have an identical set of options and the identical incentive to
                                                      To check agent robustness in the face of environment
succeed at coordination.
                                                      changes, we took the “Pick the Pile” domain (described be-
  Our experiments involve pure tactic coordination games. low), and designed two variations of it, in which one variant
Deﬁnition 2 (Focality Value). Let R be the set of selection is more similar to the original environment than the other is:
rules used in the coordination domain, c ∈ C be a possible Deﬁnition 4 (Environment Similarity). Similarity between
choice in the domain, r ∈ R be a speciﬁc selection rule, and environments is calculated as the Euclidean distance:
                                                                          
v(r, c) be its value. Then the focality value is deﬁned as:               
                                                                         n
                             v(r, c)                                d   =     (x  − x  )2,
               FV(c)=     r∈R      .                                  ij         ik   jk
                            |R|                                             k=1
  A focality value is a quantity calculated for each possible where the environment vector x is constructed from the num-
choice in a given game, and signiﬁes the level of prominence ber of goals, number of attributes per goal, number of values
of that choice relative to the domain. The focality value takes per attribute, and the attribute values themselves.
into account all of the focal point selection rules used in the In our experiments, we put original agents (i.e., agents that
coordination domain; their speciﬁc implementation is domain had been trained in the original Pick the Pile game) in the new
dependent (e.g., what constitutes Centrality in a given do- environments, and compared their classiﬁcation performance.
main). Since the exact set of selection rules used by human
players is unknown, this value represents an approximation 3.2 The Experimental Domains
based on our characterization of the focal point rule set. In We now present three experimental domains that were de-
the experiments, our FP agent will use this value to deter- signed to check FPL’s performances. The design principles
mine its classiﬁcation answer to a given game.        for constructing them were as follows: 1) create tactic coor-
Deﬁnition 3 (Focality Ratio). Let C be the set of all possible dination games (equal utility values for all possible choices);
choices in the coordination domain and FV(c) be the focality 2) minimize implicit biases that might occur due to psycho-
value of c ∈ C, max be the maximum function and 2nd max logical, cultural, and social factors; 3) consider a range of tac-
be the second maximum function. Then the focality ratio is tic coordination problems, to check the performance of FPL
deﬁned as:                                            learning in different domains.
   F  Ratio(C)=max(FV(c))   −  2nd max(FV(c)).        Pick the Pile Game
                 c∈C               c∈C
                                                      We designed a simple and intuitive tactic coordination game
  A Focality Ratio is a function that gets a set of possible that represents a simpliﬁed version of a domain where an
choices and determines the difﬁculty level of the tactic co- agent and a human partner need to agree on a possible meet-
ordination game. Naturally, a game with few choices that ing place. The game is played on a 5 by 5 square grid board.
have similar focality values is harder to solve than a game Each square of the grid can be empty, or can contain either
that might have more choices, but with one of the choices a pile of money or the game agents (all agents are situated in
much more prominent than the others.                  the same starting grid; see Figure 1). Each square in the game
  For each of the experimental domains, we built classiﬁers board is colored white, yellow, or red. The players were in-
that predict the choice selected by most human partners. We structed to pick the one pile of money from the three identical
worked with two widely used machine learning algorithms: piles, that most other players, playing exactly the same game,
a C4.5 decision learning tree [Quinlan, 1993], and a feed would pick. The players were told that the agents can make
forward back-propagation (FFBP) neural network [Werbos, horizontal and vertical moves.
1974]. Each of these was ﬁrst trained on the raw domain data In a simple binary encoding of this domain, for encoding
set, and then on the new preprocessed focal point data. 25 squares with 9 possible values (4 bits) per square, we used
  The raw data was represented as a multi-valued feature bit 100 neurons for the input layer. Training such a massive net-
vector. Each domain feature was represented by the mini- work required a large training set, and we built the game as a
mal number of bits needed to represent all its possible values. web application to be played online. The web site was pub-
This simple, low level representation helped standardize the licized in various mailing lists. To maintain the generality

                                                IJCAI-07
                                                  1565       Figure 1: Pick the Pile Game board sample             Figure 2: Candidate Selection Game sample

of the data, each game sequence was limited to 12 game in- uniqueness of each of its values (i.e., a sole female candidate
stances; thus, the data set of approximately 3000 games was in a set of males will have a high singularity value). The Ex-
generated by 250 different human players from around the tremeness property gave high values to properties that exhibit
world. Each instance of the game was randomly generated. extreme values in some characteristics of the candidate (for
  The transformation to the focal point space was done in example, a candidate who is the oldest or youngest among
the following way: the only distinguishable choice attribute the set of candidates would get a higher Extremeness value
is the color, thus the Singularity of each pile was calculated than a candidate who is not). The Firstness and Centrality
according to the number of squares having the same color. properties simply gave a constant positive value to the ﬁrst
Naturally, a pile of money sitting on a red square in a board and third candidates on the list.
having only 4 red squares, would have a higher degree of sin-
gularity than a pile of money sitting on a white square, if there Shape Matching Game
were 17 white squares on that board. The Firstness property Players were given a random set of geometric shapes, and had
was calculated as the Manhattan distance between the agent’s to mark their selected shape in order to achieve successful co-
square and each pile of money. Centrality was calculated as ordination with an unknown partner (presented with the same
an exact bisection symmetry, thus giving a positive value to a set). The shapes were presented in a single row. The game
pile that lies directly between two other piles either horizon- was randomly generated in terms of the number of shapes
tally, vertically, or diagonally. The Extremeness property was (that ranged from 3 to 7) and the shapes themselves (which
intuitively irrelevant in this domain, so we gave it a uniform could be a circle, rectangle, or triangle). Questionnaires con-
constant value.                                       taining ten game instances were distributed to students (78
                                                      students overall). As before, monetary prizes were guaran-
Candidate Selection Game                              teed to students with the highest coordination scores. Figure 3
Players were given a list of ﬁve candidates in an election for shows a sample question in the domain.
some unknown position. The candidates were described us-
ing the following properties and their possible values:
 1. sex∈{Male, Female}
 2. age∈{25, 31, 35, 42, 45}
 3. height (in meters)∈{1.71, 1.75, 1.78, 1.81, 1.85}
                                                               Figure 3: Shape Matching Game sample
 4. profession∈{Doctor, Lawyer, Businessman, Engineer,
    Professor}                                          This domain is the easiest among our games to represent
  Each list was composed of ﬁve randomly generated can- as a simple binary encoding, because each goal has only a
didates. The (pen and paper) experiments were carried out single property, its type. In any game instance, each shape
when subjects (82 ﬁrst-year university students) were seated can be a circle, rectangle, triangle, or “non-existing”, in the
in a classroom, and were told that their coordination partners case where the randomized number of shapes is lower than 7.
were randomly selected from experiments that took place in The focal point transformation was implemented as follows:
other classes, i.e., their partner’s identity is completely un- the Singularity of a choice was determined by the number of
known. For a candidate to be elected, it needs to get these two choices with the same shape (for example, in a game where
votes (the player’s and its partner’s); thus, both sides need to all shapes are circles and only a single shape is a triangle,
choose the same candidate. To create the necessary motiva- the triangular shape will have a high singularity value). The
tion for successful coordination,1 we announced a monetary Extremeness property gave higher focality values to the ﬁrst
reward for success. Figure 2 shows a sample question. and last choices in the set. Those values became higher as the
  The binary encoding for this domain is a set of 65 input number of shapes increased (the extremeness in a game with
neurons in the input layer that encodes 5 candidates, each 3 shapes was lower than the extremeness in a game with 6
with 13 possible property values. The focal point transforma- shapes). Centrality gave additional focality value to the mid-
tion had the following intuitive implementation: the Singu- dle choice, when the number of shapes was odd. In an even
larity of a candidate was calculated according to the relative number of shapes, no centrality value was given. Firstness
                                                      gave a small bias to the ﬁrst shape on the list.
  1It is a well-known phenomenon that coordination in these games This domain is an example of a transformation in which
deteriorates without sufﬁcient motivation.            j>i; the transformation actually increases the search space.

                                                IJCAI-07
                                                  15664  Results and Discussion                               Note also that in the ﬁrst domain, when using FPL instead
4.1  Prediction Performance                           of regular raw data learning, the marginal increase in perfor-
                                                      mance is higher than the improvement that was achieved in
For each of the above domains, we compared the correct clas- the second domain (an increase of 30% vs. 21%), which is in
siﬁcation performance of both C4.5 learning trees and FFBP turn higher than the marginal increase in performance of the
neural network classiﬁers. As stated above, the comparison third domain (an increase of 21% vs. 14%). From those re-
was between a domain data agent (trained on the raw domain sults, we hypothesize that the difference in the marginal per-
encoding), a focal point (FP) agent (an untrained agent that formance increase is because the ﬁrst domain was the most
used only focal point rules for prediction), and a focal point complex one in terms of the number of objects and their prop-
learning (FPL) agent. “Correct classiﬁcation” means that the erties. As the domain becomes more complex, there are more
agent made the same choice as that of the human who played
             2                                        possibilities for human subjects to use their own subjective
the same game.                                        rules (for example, in the Pick the Pile domain, we noticed
  We optimized our classiﬁers’ performance by varying the that few people looked at the different color patterns that were
network architecture and learning parameters, until attaining randomly created, as a decision rule for their selected goal).
best results. We used a learning rate of 0.3, momentum rate
  0.2 1                                               As more rules are used, the data becomes harder to gener-
of   ,  hidden layer, random initial weights, and no bi- alize. When an agent is situated in a real, highly complex
ases of any sort. Before each training procedure, the data environment, we can expect that the marginal increase in per-
set was randomly divided into a test and a training set (a stan- formance, when using FPL, will be correspondingly large.
dard 33%–66% division). Each instance of those sets con- An additional advantage of using FPL is the reduction in
tained the game description (either the binary or focal point training time (e.g., in the Pick the Pile domainwesaware-
encoding) and the human answer to it. All algorithms were duction from 4 hours on the original data to 3 minutes), due
runintheWEKA   data mining software, which is a collection to the reduction of input size. Moreover, the learning tree that
of machine learning algorithms for data mining tasks. The was created using FPL was smaller, and can be easily con-
classiﬁcation results using the neural network and the deci- verted to a rule-based system as part of the agent’s design.
sion tree algorithms were very close (maximum difference of
3%). Figure 4 compares the correct classiﬁcation percentage 4.2 Robustness to Environmental Changes
for the agents’ classiﬁcation techniques, in each of the three Follow-on experiments were designed to check the robustness
experimental domains. Each entry in the graph is a result av- of agent-human tactic interaction in a changing environment.
eraged over ﬁve runs of each learning algorithm (neural net- We created two different versions of the Pick the Pile game,
work and C4.5 tree), and the average of those two algorithms. which had different similarity values relative to the original
                                                      version. In the ﬁrst variant (VSD), we added a fourth possible
                                                      value to the set of values of the color attribute (four colors
                                                      instead of three). In the second variant (SD), in addition to
                                                      the ﬁrst change, we also changed the grid structure to a 6 by
                                                      4 grid (instead of the original 5 by 5). Moreover, in both
                                                      variants, we changed all four color values from actual colors
                                                      to various black and white texture mappings.
                                                        Additional experiments were conducted in order to collect
                                                      human answers to the two new variants of the game. The
                                                      agents that had been trained on the original environment (us-
                                                      ing the neural network algorithm), were now asked to coordi-
                                                      nate with an arbitrary human partner in the new environments.
   Figure 4: Average Correct Classiﬁcation Percentage Figure 5 summarizes performance comparison of the agents
                                                      in each of the new environment variants.
  Examining the results, we see a signiﬁcant improvement
when using the focal point learning approach to train classi-
ﬁers. In all three domains, the domain data agent is not able to
generalize sufﬁciently, thus achieving classiﬁcation rates that
are only about 5%–9% higher than a random guess. Using
FPL, the classiﬁcation rate improved by 40%–80% above the
classiﬁcation performance of the domain data agent.3 The
results also show that even the classical FP agent performs
better than the domain data agent. However, when the FP
agent faces coordination problems with low focality ratios,
its performance deteriorates to that of random guesses.
  2If there were multiple occurrences of a speciﬁc game instance,
the choice of the majority of humans was considered the solution. Figure 5: Classiﬁcation Percent in Changing Environments
  3Since even humans do not have 100% success with one another
in these games, FPL is correspondingly the more impressive. The prediction results on the ﬁrst variant (VSD) show that

                                                IJCAI-07
                                                  1567