                 Fast Incremental Square Root Information Smoothing∗

                      Michael Kaess, Ananth Ranganathan, and Frank Dellaert
                  Center for Robotics and Intelligent Machines, College of Computing
                          Georgia Institute of Technology, Atlanta, GA 30332
                                 {kaess,ananth,dellaert}@cc.gatech.edu

                    Abstract
    We propose a novel approach to the problem of
    simultaneous localization and mapping (SLAM)
    based on incremental smoothing, that is suitable for
    real-time applications in large-scale environments.
    The main advantages over ﬁlter-based algorithms
    are that we solve the full SLAM problem without
    the need for any approximations, and that we do
    not suffer from linearization errors. We achieve
    efﬁciency by updating the square-root information
    matrix, a factored version of the naturally sparse
    smoothing information matrix. We can efﬁciently
    recover the exact trajectory and map at any given
    time by back-substitution. Furthermore, our ap-
    proach allows access to the exact covariances, as it
    does not suffer from under-estimation of uncertain- Figure 1: Updating the upper triangular factor R and the right hand
    ties, which is another problem inherent to ﬁlters. side (rhs) with new measurement rows. The left column shows the
    We present simulation-based results for the linear ﬁrst three updates, the right column shows the update after 50 steps.
                                                      The update operation is symbolically denoted by ⊕, and entries that
    case, showing constant time updates for exploration remain unchanged are shown in light blue.
    tasks. We further evaluate the behavior in the pres-
    ence of loops, and discuss how our approach ex-
    tends to the non-linear case. Finally, we evalu-    Current real-time approaches to SLAM are typically based
    ate the overall non-linear algorithm on the standard on ﬁltering. While ﬁlters work well for linear problems, they
    Victoria Park data set.
                                                      are not suitable for most real-world problems, which are in-
                                                      herently non-linear [Julier and Uhlmann, 2001]. The reason
1  Introduction                                       for this is well known: Marginalization over previous poses
                                                      bakes linearization errors into the system in a way that can-
The problem  of simultaneous localization and mapping not be undone, leading to unbounded errors for large-scale
(SLAM) has received considerable attention in mobile  applications. But repeated marginalization also complicates
robotics, as it is one way to enable a robot to explore and nav- the problem by making the naturally sparse dependencies be-
igate previously unknown environments. For a wide range of tween poses and landmarks, which are summarized in the
applications, such as commercial robotics, search-and-rescue information matrix, dense. Sparse extended information ﬁl-
and reconnaissance, the solution must be incremental and ters [Thrun et al., 2005] andthinjunctiontreeﬁlters[Paskin,
real-time in order to be useful. The problem consists of two 2003] present approximations to deal with this complexity.
components: a discrete correspondence and a continuous es-
                                                        Smoothing approaches to SLAM  recover the complete
timation problem. In this paper we focus on the continuous
                                                      robot trajectory and the map, avoiding the problems inher-
part, while keeping in mind that the two components are not
                                                      ent to ﬁlters. In particular, the information matrix is sparse
completely independent, as information from the estimation
                                                      and remains sparse over time, without the need for any ap-
part can simplify the correspondence problem.
                                                      proximations. Even though the number of variables increases
  ∗This work was supported in part by the National Science Foun- continuously for smoothing, in many real-world scenarios,
dation award IIS - 0448111 “CAREER: Markov Chain Monte Carlo especially those involving exploration, the information ma-
Methods for Large Scale Correspondence Problems in Computer Vi- trix still contains far less entries than in ﬁlter based methods
sion and Robotics”.                                   [Dellaert, 2005]. When repeatedly observing a small number

                                                IJCAI-07
                                                  2129of landmarks, ﬁlters seem to have an advantage, at least when
ignoring the linearization issues. However, the correct way of
dealing with this situation is to switch to localization after a
map of sufﬁcient quality is obtained.
  In this paper, we present a fast incremental smoothing ap-
proach to the SLAM problem. We revisit the underlying
probabilistic formulation of the smoothing problem and its
equivalent non-linear optimization formulation in Section 2.
The basis of our work is a factorization of the smoothing in-
formation matrix, that is directly suitable for performing op- Figure 2: Bayesian belief network representation of the SLAM prob-
                                                      lem, where x is the state of the robot, l the landmark locations, u the
timization steps. Rather than recalculating this factorization     z
in each step from the measurement equations, we incremen- control input and the measurements.
tally update the factorized system whenever new measure-
ments become available, as described in Section 3. This fully where P (x0) is a prior on the initial state, P (xi|xi−1,ui) is
exploits the natural sparsity of the SLAM problem, resulting the motion model, parametrized by the control input ui,and
                                                      P  z |x ,l
in constant time updates for exploration tasks. We apply vari- ( k ik jk ) is the landmark measurement model, assuming
able reordering similar to [Dellaert, 2005] in order to keep the known correspondences (ik,jk) for each measurement zk.
factored information matrix sparse even when closing multi- As is standard in the SLAM literature, we assume Gaus-
ple loops. All linearization choices can be corrected at any sian process and measurement models. The process model
time, as no variables are marginalized out. From our fac- xi = fi(xi−1,ui)+wi describes the robot behavior in re-
tored representation, we can apply back-substitution at any sponse to control input, where wi is normally distributed
time to obtain the complete map and trajectory in linear time. zero-mean process noise with covariance matrix Λi.The
                                                                                 z    h  x  ,l    v
We evaluate the incremental algorithm based on the standard Gaussian measurement equation k = k( ik jk )+ k mod-
Victoria Park dataset in Section 4.                   els the robot’s sensors, where vk is normally distributed zero-
  As our approach is incremental, it is suitable for real-time mean measurement noise with covariance Σk.
applications. In contrast, many other smoothing approaches
like GraphSLAM [Thrun et al., 2005] perform batch process- 2.2 SLAM as a Least Squares Problem
ing, solving the complete problem in each step. Even though In this section we discuss how to obtain an optimal estimate
explicitly exploiting the structure of the SLAM problem can for the set of unknowns given all measurements available to
yield quite efﬁcient batch algorithms [Dellaert, 2005],for us. As we perform smoothing rather than ﬁltering, we are
large-scale applications an incremental solution is needed. interested in the maximum a posterior (MAP) estimate for
Examples of such systems that incrementally add new mea- the entire trajectory X = {xi} and the map of landmarks
surements are Graphical SLAM [Folkesson and Christensen, L = {lj}, given the measurements Z = {zk} and the control
2004] and multi-level relaxation [Frese et al., 2005].How- inputs U = {ui}. Let us collect all unknowns in X and L
ever, these and similar solutions have no efﬁcient way of ob- in the vector Θ=(X, L). The MAP estimate Θ∗ is then ob-
taining the marginal covariances needed for data association. tained by minimizing the negative log of the joint probability
In Section 5 we describe how our approach allows access to P (X, L, Z) from equation (1):
the exact marginal covariances without a full matrix inver-        ∗
                                                                  Θ  =argmin−   log P (X, L, Z)       (2)
sion, and how to obtain a more efﬁcient conservative estimate.              Θ
2  SLAM    and Smoothing                              Combined with the process and measurement models, this
                                                      leads to the following non-linear least-squares problem
In this section we review the formulation of the SLAM                      
problem in a smoothing framework, following the notation                    M
                                                             ∗                                  2
of [Dellaert, 2005]. In contrast to ﬁltering methods, no   Θ    =argmin         ||fi(xi−1,ui) − xi||Λ
                                                                        Θ                        i
marginalization is performed, and all pose variables are re-                i=1
                                                                                                  
tained. We describe the underlying probabilistic model of this              K
full SLAM problem, and show how inference on this model                        ||h x  ,l   − z ||2
                                                                          +       k( ik jk )  k Σk    (3)
leads to a least-squares problem.                                           k=1
                                                                                2     T −1
2.1  A Probabilistic Model for SLAM                   whereweusethenotation||e||Σ =  e Σ   e for the squared
We formulate the SLAM problem in terms of the belief net- Mahalanobis distance given a covariance matrix Σ.
work model shown in Figure 2. We denote the robot state at In practice one always considers a linearized version of this
    th
the i time step by xi, with i ∈ 0 ...M, a landmark by lj, problem. If the process models fi and measurement equa-
with j ∈ 1 ...N, and a measurement by zk, with k ∈ 1 ...K. tions hk are non-linear and a good linearization point is not
The joint probability is given by                     available, non-linear optimization methods solve a succession
                   M              K                 of linear approximations to this equation in order to approach
P  X, L, Z   P x      P x |x   ,u     P z |x  ,l
  (      )=    ( 0)    ( i  i−1  i)     ( k ik jk )   the minimum. We therefore linearize the least-squares prob-
                   i=1             k=1                lem by assuming that either a good linearization point is avail-
                                                (1)   able or that we are working on one iteration of a non-linear

                                                IJCAI-07
                                                  2130Figure 3: Using a Givens rotation to transform a matrix into upper
triangular form. The entry marked ’x’ is eliminated, changing some
or all of the entries marked in red (dark), depending on sparseness.

optimization method, see [Dellaert, 2005] for the derivation:
                  
                   M
    ∗                    i−1         i        2       Figure 4: Complexity of a simulated linear exploration task, show-
 δΘ    =argmin         ||Fi δxi−1 + Giδxi − ai||Λ
               δΘ                               i     ing 10-step averages. In each step, the square-root factor is updated
                   i=1                               by adding the new measurement rows by Givens rotations. The av-
                 K                                   erage number of rotations (red) remains constant, and therefore also
                    ||Hik δx   J jk δl − c ||2        the average number of entries per matrix row. The execution time
               +           ik +     jk   k Σ    (4)
                       k        k           k         per step (dashed green) slightly increases for our implementation
                 k=1
                                                      due to tree representations of the underlying data structures.
        ik  jk
where Hk , Jk are the Jacobians of hk with respect to a
                              i−1
        x      l             F                 f                          T Δ   T                       2
change in ik and jk respectively, i the Jacobian of i at where we deﬁned [d, e] = Q b.Theﬁrstterm||Rθ − d||
x       Gi    I             a     c                                                    ∗
 i−1,and  i =  for symmetry. i and k are the odometry vanishes for the least-squares solution θ , leaving the second
and observation measurement prediction errors, respectively. term ||e||2 as the residual of the least-squares problem.
  We combine all summands into one least-squares problem The square-root factor allows us to efﬁciently recover the
after dropping the covariance matrices Λi and Σk by pulling complete robot trajectory as well as the map at any given
their matrix square roots inside the Mahalanobis norms: time. This is simply achieved by back-substitution using the
                        T                         current factor R and right hand side (rhs) d to obtain an up-
||e||2 eT  −1e      −T/2e      −T/2e    || −T/2e||2
   Σ =    Σ    =  Σ          Σ        =  Σ            date for all model variables θ based on
                                                (5)
                                                                            Rθ    d                   (9)
For scalar matrices this simply means dividing each term by                    =
the measurement standard deviation. We then collect all Ja- Note that this is an efﬁcient operation for sparse R, under
cobian matrices into a single matrix A, the vectors ai and ck the assumption of a (nearly) constant average number of en-
into a right hand side vector b, and all variables into the vector tries per row in R. We have observed that this is a reasonable
θ, to obtain the following standard least-squares problem: assumption even for loopy environments. Each variable is
                ∗                  2                                                    O n
               θ =argmin||Aθ   − b||            (6)   then obtained in constant time, yielding ( ) time complex-
                        θ                             ity in the number n of variables that make up the map and
                                                      trajectory. We can also restrict the calculation to a subset of
3  Updating a Factored Representation                 variables by stopping the back-substitution process when all
We present an incremental solution to the full SLAM problem variables of interest are obtained. This constant time opera-
based on updating a matrix factorization of the measurement tion is typically sufﬁcient unless loops are closed.
Jacobian A from (6). For simplicity we initially only consider
the case of linear process and measurement models. In the 3.1 Givens Rotations
linear case, the measurement Jacobian A is independent of One standard way to obtain the QR factorization of the mea-
the current estimate θ. We therefore obtain the least squares surement Jacobian A is by using Givens rotations [Golub and
solution θ∗ in (6) directly from the standard QR matrix factor- Loan, 1996] to clean out all entries below the diagonal, one
ization [Golub and Loan, 1996] of the Jacobian A ∈ Rm×n: at a time. As we will see later, this approach readily extends
                              
                           R                          to factorization updates, as will be needed to incorporate new
                   A = Q                        (7)   measurements. The process starts from the bottom left-most
                            0
                                                      non-zero entry, and proceeds either column- or row-wise, by
where Q ∈ Rm×m   is orthogonal, and R ∈ Rn×n is upper applying the Givens rotation:
                                        Δ   T                                          
triangular. Note that the information matrix I = A A can                  cos φ  sin φ
                                               T                                                     (10)
also be expressed in terms of this factor R as I = R R,                  − sin φ cos φ
which we therefore call the square-root information matrix
R. We rewrite the least-squares problem, noting that multi- to rows i and k, with i>k. The parameter φ is chosen
plying with the orthogonal matrix QT does not change the so that the (i, k) entry of A becomes 0, as shown in Figure
                                                      3. After all entries below the diagonal are zeroed out, the
norm:                                   
                                                      upper triangular entries will contain the R factor. Note that
          R         2         R         d   2
     ||Q      θ − b||  =  ||      θ −      ||         a sparse measurement Jacobian A will result in a sparse R
          0                   0         e
                                                      factor. However, the orthogonal rotation matrix Q is typically
                                   2      2
                       =  ||Rθ − d|| + ||e||    (8)   dense, which is why this matrix is never explicitly stored or

                                                IJCAI-07
                                                  2131even formed in practice. Instead, it is sufﬁcient to update the
rhs b with the same rotations that are applied to A.
  When a new measurement arrives, it is cheaper to update
the current factorization than to factorize the complete mea-
surement Jacobian A. Adding a new measurement row wT
and rhs γ into the current factor R and rhs d yields a new
system that is not in the correct factorized form:
                                         
    QT         A         R                d           (a) Simulated double 8-loop at interesting stages of loop closing (for
                T   =     T   , new rhs:       (11)
         1     w         w               γ            simplicity only a quarter of the poses and landmarks are shown).
Note that this is the same system that is obtained by applying
Givens rotations to eliminate all entries below the diagonal,
except for the last (new) row. Therefore Givens rotations can
be determined that zero out this new row, yielding the updated
factor R. In the same way as for the full factorization, we
simultaneously update the right hand side with the same rota-
tions to obtain d. In general, the maximum number of Givens
rotations needed to add a new measurement is n.However,as
R and the new measurement rows are sparse, only a constant
number of Givens rotations are needed. Furthermore, new
measurements typically refer to recently added variables, so (b) Cholesky factor R. (c) Cholesky factor after
that often only the right most part of the new measurement                        variable reordering.
row is (sparsely) populated. An example of the locality of the
update process is shown in Figure 1.
  It is easy to add new landmark and pose variables to the
QR factorization, as we can just expand the factor R by the
appropriate number of zero columns and rows, before updat-
ing with the new measurement rows. Similarly, the rhs d is
augmented by the same number of zero entries.
  For an exploration task in the linear case, the number of
rotations needed to incorporate a set of new landmark and
odometry measurements is independent of the size of the tra-
jectory and map, as the simulation results in Figure 4 show.
Our algorithm therefore has O(1) time complexity for explo-
ration tasks. Recovering all variables after each step requires
O(n) time, but is still very efﬁcient even after 10000 steps, at
about 0.12 seconds per step. However, only the most recent
variables change signiﬁcantly enough to warrant recalcula-
tion, providing a good approximation in constant time.
3.2  Loops and Variable Reordering
Loops can be dealt with by a periodic reordering of variables.
A loop is a cycle in the trajectory that brings the robot back
to a previously visited location. This introduces correlations
between current poses and previously observed landmarks, (d) Execution time per step for different updating strategies are
                                                      shown in both linear and log scale.
which themselves are connected to earlier parts of the trajec-
tory. Results based on a simulated environment with multiple Figure 5: For a simulated environment consisting of an 8-loop that
loops are shown in Figure 5. Even though the information is traversed twice (a), the upper triangular factor R shows signiﬁ-
matrix remains sparse in the process, the incremental updat- cant ﬁll-in (b), yielding bad performance (d, continuous red). Some
ing of the factor R leads to ﬁll-in. This ﬁll-in is local and does ﬁll-in occurs when the ﬁrst loop is closed (A). Note that this has no
not affect further exploration, as is evident from the example. negative consequences on the subsequent exploration along the sec-
However, this ﬁll-in can be avoided by variable reordering, as ond loop until the next loop closure occurs (B). However, the ﬁll-in
has been described in [Dellaert, 2005]. The same factor R af- then becomes signiﬁcant when the complete 8-loop is traversed for
                                                      the second time, with a peak when visiting the center point of the
ter reordering shows no signs of ﬁll-in. However, reordering 8-loop for the third time (C). After variable reordering, the factor
of the variables and subsequent factorization of the new mea- matrix again is completely sparse (c). Reordering after each step (d,
surement Jacobian itself is also expensive when performed dashed green) can be less expensive in the case of multiple loops. A
in each step. We therefore propose fast incremental updates considerable increase in efﬁciency is achieved by using fast incre-
interleaved with occasional reordering, yielding a fast algo- mental updates interleaved with periodic reordering (d, dotted blue),
rithm as supported by the dotted blue curve in Figure 5(d). here every 100 steps.

                                                IJCAI-07
                                                  2132  When the robot continuously observes the same land-
marks, for example by remaining in one small room, this ap-
proach will eventually fail, as the information matrix itself
will become completely dense. However, in this case ﬁlters
will also fail due to underestimation of uncertainties that will
ﬁnally converge to 0. The correct solution to deal with this
scenario is to eventually switch to localization.
3.3  Non-linear Systems
Our factored representation allows changing the linearization
point of any variable at any time. Updating the linearization
point based on a new variable estimate changes the measure-
ment Jacobian. One way then to obtain a new factorization
is to refactor this matrix. For the results presented here, we
combine this with the periodic reordering of the variables for
ﬁll-in reduction as discussed earlier.
  However, in many situations it is not necessary to perform
relinearization [Steedly et al., 2003]. Measurements are typ-
ically fairly accurate on a local scale, so that relinearization
is not needed in every step. Measurements are also local and
only affect a small number of variables directly, with their
effects rapidly declining while propagating through the con-
straint graph. An exception is a situation such as a loop clos-
ing, that can affect many variables at once. In any case it is
sufﬁcient to perform selective relinearization only over vari-
                                                      Figure 6: Optimized map of the full Victoria Park sequence. Solving
ables whose estimate has changed by more than some thresh- the complete problem after every step takes less than 2 minutes on
old. In this case, the affected measurement rows are ﬁrst re- a laptop for known correspondences. The trajectory and landmarks
moved from the current factor by QR-downdating [Golub and are shown in yellow (light), manually overlayed on an aerial image
Loan, 1996], followed by adding the relinearized measure- for reference. Differential GPS was not used in obtaining the results,
ment rows by QR-updating as described earlier.        but is shown in blue (dark) where available. Note that in many places
                                                      GPS is not available.
4  Results
                                                      5   Covariances for Data Association
We have applied our approach to the Sydney Victoria Park
dataset (available at http://www.acfr.usyd.edu.au/    Our algorithm is helpful for performing data association,
homepages/academic/enebot/dataset.htm), a popu-       which generally is a difﬁcult problem due to noisy data and
lar test dataset in the SLAM community. The trajectory con- the resulting uncertainties in the map and trajectory estimates.
sists of 7247 frames along a trajectory of 4 kilometer length, First it allows to undo data association decisions, but we have
recorded over a time frame of 26 minutes. 6968 frames are not exploited this yet. Second, it allows for recovering the
left after removing all measurements where the robot is sta- underlying uncertainties, thereby reducing the search space.
tionary. We have extracted 3631 measurements of 158 land- In order to reduce the ambiguities in matching newly ob-
marks from the laser data by a simple tree detector.  served features to already existing landmarks, it is advanta-
                                                      geous to know the projection Ξ of the combined pose and
  The ﬁnal optimized trajectory and map are shown in Figure               
6. For known correspondences, the full incremental recon- landmark uncertainty Σ into the measurement space:
                                                                                  T
struction, including solving for all variables after each new         Ξ=HΣ       H   +Γ              (12)
frame is added, took s ( . minutes) to calculate on a Pen-
                 337  5 5                                   H
tium M 2 GHz laptop, which is signiﬁcantly less than the 26 where is the Jacobian of the projection process, and Γ mea-
minutes it took to record the data. The average calculation surement noise. This requires knowledge of the marginal co-
                                                      variances                        
time for the ﬁnal 100 steps is 0.089s per step, which includes                       T
                                                                             Σjj  Σij
a full factorization including variable reordering, which took        Σij =                          (13)
1.9s. This shows that even after traversing a long trajec-                    Σij   Σii
tory with a signiﬁcant number of loops, our algorithm still between the current pose mi and any visible landmark xj,
performs faster than the 0.22s per step needed for real-time. where Σij are the corresponding blocks of the full covariance
However, we can do even better by selective reordering based matrix Σ=I−1. Calculating this covariance matrix in order
on a threshold on the running average over the number of to recover all entries of interest is not an option, because it is
Givens rotations, and back-substitution only every 10 steps, always completely populated with n2 entries.
resulting in an execution time of only 116s. Note that this Our representation allows us to retrieve the exact values of
still provides good map and trajectory estimates after each interest without having to calculate the complete dense co-
step, due to the measurements being fairly accurate locally. variance matrix, as well as to efﬁciently obtain a conservative

                                                IJCAI-07
                                                  2133