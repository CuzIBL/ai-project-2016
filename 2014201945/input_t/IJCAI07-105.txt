                             Updates for Nonlinear Discriminants

     Edin Andelic´   Martin Schaffoner¨    Marcel Katz    Sven E. Kruger¨    Andreas Wendemuth
                                    Cognitive Systems Group, IESK
                                     Otto-von-Guericke-University
                                         Magdeburg, Germany
                               edin.andelic@e-technik.uni-magdeburg.de

                    Abstract                          1973][Mika, 2002] that the proposed method is closely re-
                                                      lated to the KFD and the LS-SVM. However, the proposed
    A novel training algorithm for nonlinear discrimi- method imposes sparsity on the solution in a greedy fashion
    nants for classiﬁcation and regression in Reproduc- using subset selection like in [Billings and Lee, 2002][Nair et
    ing Kernel Hilbert Spaces (RKHSs) is presented.   al., 2002]. For the SVM case similar greedy approaches exist
    It is shown how the overdetermined linear least-  [G. Cauwenberghs, 2000][Ma et al., 2003].
    squares-problem in the corresponding RKHS may       Especially in case of large data sets subset selection is a
    be solved within a greedy forward selection scheme practical method. It aims to eliminate the most irrelevant or
    by updating the pseudoinverse in an order-recursive redundant samples. However, ﬁnding the best subset of ﬁxed
    way. The described construction of the pseudoin-  size is an NP-hard combinatorial search problem. Hence, one
    verse gives rise to an update of the orthogonal de- is restricted to suboptimal search strategies. Forward selec-
    composition of the reduced Gram matrix in linear  tion starts with an empty training set and adds sequentially
    time. Regularization in the spirit of Ridge regres- one sample that is most relevant according to a certain cri-
    sion may then easily be applied in the orthogonal terion (e. g. the mean square error). In [Nair et al., 2002]
    space. Various experiments for both classiﬁcation an external algorithm which is based on elementary Givens
    and regression are performed to show the competi- rotations is used to update the QR-decomposition of the re-
    tiveness of the proposed method.                  duced Gram matrix in order to construct sparse models. The
                                                      Gram Schmidt orthogonalization is used in [Billings and Lee,
                                                      2002] and [Chen et al., 1991] for the orthogonal decomposi-
1  Introduction                                       tion of the Gram matrix. They also apply forward selection in
Models for regression and classiﬁcation that enforce square a second step to obtain sparse models. This method is known
loss functions are closely related to Fisher discriminants as Orthogonal Least Squares (OLS). However, the OLS al-
[Duda and Hart, 1973]. Fisher discriminants are Bayes- gorithm requires the computation and the storage of the full
optimal in case of classiﬁcation with normally distributed Gram matrix which is prohibitive for large datasets.
classes and equally structured covariance matrices [Duda and In this paper a very simple and efﬁcient way for construct-
Hart, 1973][Mika, 2002]. However, in contrast to SVMs, ing LSMs in a RKHS within a forward selection rule with
Least Squares Models (LSMs) are not sparse in general and much lower memory requirements is presented. The pro-
hence may cause overﬁtting in a supervised learning scenario. posed method exploits the positive deﬁniteness of the Gram
One way to circumvent this problem is to incorporate regular- matrix for an order-recursive thin update of the pseudoin-
ization controlled by a continuous parameter into the model. verse, which reveals to the best of our knowledge a novel kind
For instance, Ridge regression [Rifkin et al., 2003] penalizes of update rule for the orthogonal decomposition. The solution
the norm of the solution yielding ﬂat directions in the RKHS, is regularized in a second stage using the Generalized Cross
which are robust against outliers caused by e. g. noise. In Validation to re-estimate the regularization parameter.
[Suykens and Vandewalle, 1999] Least-Squares SVMs (LS-  The remainder of this paper is organized as follows. In
SVMs) are introduced which are closely related to Gaussian section 2, computationally efﬁcient update rules for the pseu-
processes and Fisher discriminants. A linear set of equa- doinverse and the orthogonal decomposition are derived. In
tions in the dual space is solved using e. g. the conjugate section 3, it is shown how the solution may be regularized. In
gradient methods for large data sets or a direct method for section 4 some experimental results on regression and classi-
a small number of data. The solution is pruned [De Kruif ﬁcation datasets are presented. Finally, a conclusion is given
and De Vries, 2003][Hoegaerts et al., 2004] in a second in section 5.
stage. The close relation between the LS-SVM and the Ker-
nel Fisher Discriminant (KFD) was shown in [Van Gestel et 2 Update of the Orthogonal Decomposition
al., 2002]. It follows from the equivalence between the KFD In a supervised learning problem one is faced with a training
and a least squares regression onto the labels [Duda and Hart, data set D = {xi,yi},i =1...M. Here, xi denotes an

                                                IJCAI-07
                                                   660input vector of ﬁxed size and yi is the corresponding target Noting that the pseudoinverse of a vector is given by
                       R                 { , − }
value which is contained in for regression or in 1 1 for                         qT
binary classiﬁcation. It is assumed that xi = xj , for i = j.          q†       m
                                                                           m =      2                (12)
  We focus on sparse approximations of models of the form                      qm
                     yˆ = Kα.                   (1)   equation (11) may be written as
                                                                  T            †
                       k ·, x [          ]                       q   I − Km−1K      y
The use of Mercer kernels ( ) Mercer, 1909 gives rise  α          m(           m−1)
                                        K              ˆm0   =               2                       (13)
to a symmetric positive deﬁnite Gram Matrix with ele-                   qm
ments Kij = k(xi, xj) deﬁning the subspace of the RKHS
                                                                 kT  I − K    K†    T I − K    K†     y
in which learning takes place. The weight vector α =              m(      m−1  m−1)  (     m−1   m−1)  .
                                                             =                        2
{b, α1,...,αM } contains a bias term b with a corresponding                      qm
column 1 = {1,...,1} in the Gram matrix.
                                                      The matrix
  Consider the overdetermined least-squares-problem                                   †
                                                                     Pm  = I − Km−1Km−1              (14)
                                     2
            αˆm =argminKmαm     − y           (2)
                    α                                 is an orthogonal projection matrix which implies being sym-
                     m                                metric and idempotent and thus equation (13) simpliﬁes to
in the m-th forward selection iteration with the reduced Gram
                               M×(m+1)                                    α     q† y.
matrix Km  =[1k1   ...km]  ∈ R          where ki =                        ˆm0 =   m                  (15)
                    T
 k ·, x1 ,...,k ·, xM ,i∈{  ,...,m}
( (   )      (    ))       1        denotes one pre-  Combining (15) with (8) the current weight vector αˆm may
viously unselected column of the full Gram matrix. We de- be updated as
note the reduced weight vector as αm = {b, α1,...,αm}∈                                      
Rm+1                      y    y ,...,y  T                       α         K†    − K†    k  q†
      and the target vector as =( 1    M ) . Among        α      ˆm−1        m−1     m−1  m  m  y
                      K                                   ˆm  =   α     =            †               (16)
all generalized inverses of m the pseudoinverse                   ˆm0               qm
                 †      T     −1  T
               Km  =(KmKm)      Km              (3)   revealing the update
                                                                                           
is the one that has the lowest Frobenius norm [Ben-Israel and            K†    − K†    k q†
                                                                  K†       m−1     m−1  m  m
            ]                                                      m  =           †                  (17)
Greville, 1977 . Thus, the corresponding solution                                qm
                    α     K† y
                    ˆm =    m                   (4)   for the current pseudoinverse.
                                                                            q      P  k
has the lowest Euclidean norm.                          Since every projection m =  m  m lies in a subspace
            K       α                                 which is orthogonal to Km−1 it follows immediately that
  Partitioning m and m in the form                     T
                                                      qi qj =0,  for i = j. Hence, an orthogonal decomposi-
               Km   =[Km−1km]                   (5)   tion
                                  T                                      K     Q   U
               αm   =(αm−1αm)                   (6)                        m =   m  m                (18)
                                                      of the reduced Gram matrix is given by the orthogonal matrix
and setting αm = αm0 = const, the square loss becomes
                                             2                          Qm  =[Qm−1qm]                (19)
  L(αm−1,αm0)=Km−1αm−1       − (y − kmαm0)  . (7)
                                                      and the upper triangular matrix
The minimum of (7) in the least-squares-sense is given by                                   
                      †                                                Um−1      T    −1   T
            αˆm−1 = Km−1(y  − kmαm0).           (8)           Um  =     T     (QmQm)     Qmkm   .    (20)
                                                                       0m−1
Inserting (8) into (7) yields
                                                      In the m-th iteration O(Mm) operations are required for all
                     †                       †     2                                                T
L(αm0)=(I−Km−1Km−1)kmαm0−(I−Km−1Km−1)y              these updates. Note that the inversion of the matrix QmQm
                                                (9)   is trivial since this matrix is diagonal. However, the condition
with I denoting the identity matrix of appropriate size. number of the matrix Qm increases as the number of selected
  Note that the vector                                columns m grows. Thus, to ensure numerical stability it is
             q     I − K    K†     k                  important to monitor the condition number of this matrix and
              m =(      m−1   m−1)  m          (10)   to terminate the iteration if the condition number exceeds a
is the residual corresponding to the least-squares regression predeﬁned value unless another stopping criterion is reached
onto km. Hence, qm is a nullvector if and only if km is a earlier.
nullvector unless K is not strictly positive deﬁnite. To ensure
strictly positive deﬁniteness of K, it is mandatory to add a 3 Regularization and Selection of Basis
small positive constant ε to the main diagonal of the full Gram Centers
matrix in the form K → K + εI. Forward selection may then
be performed using this strictly positive deﬁnite Gram matrix. The goal of every forward selection scheme is to select the
In the following km = 0 is assumed.                  columns of the Gram matrix that provide the greatest re-
  The minimum of (9) is met at                        duction of the residual. Methods like basis matching pur-
                                                      suit [Mallat and Zhang, 1993], order-recursive matching pur-
                   †             †
            αˆm0 = qm(I − Km−1Km−1)y           (11)   suit [Natarajan, 1995] or probabilistic approaches [Smola and

                                                IJCAI-07
                                                   661Sch¨olkopf, 2000] are several contributions to this issue. In Minimizing the GCV with respect to λ gives rise to a re-
[Nair et al., 2002], forward selection is performed by simply estimation formula for λ. An alternative way to obtain a
choosing the column that corresponds to the entry with the re-estimation of λ is to maximize the Bayesian evidence
highest absolute value in the current residual. The reasoning [MacKay, 1992].
is that the residual provides the direction of the maximum Differentiating (27) with respect to λ and setting the result
decrease in the cost function 0.5αT Kα − αTy, since the to zero gives a minimum when
Gram matrix is strictly positive deﬁnite. The latter method
                                                               ∂P˜ y                    ∂     P˜
is used in the following experiments. But note that the de- T     m               T  2   trace( m)
                                                         y P˜ m      trace(P˜ m)=y P˜ my          .  (28)
rived algorithm may be applied within any of the above for-     ∂λ                          ∂λ
ward selection rules. In the following we will refer to the pro- Noting that
posed method as Order-Recursive Orthogonal Least Squares
                                                                ∂P˜  y
(OROLS).                                                  yT P˜    m     λαT  QT Q     λI   −1α
  Consider the residual                                       m   ∂λ   =  ˜ m(  m  m +   m)   ˜ m    (29)

            e˜m = y − yˆm = y − Qmα˜ m         (21)   equation (28) can be rearranged to obtain the re-estimation
                                                      formula
in the m-th iteration. The vector α˜m contains the orthogonal
                                                                                     T  2
weights.                                                             [∂trace(P˜ m)/∂λ]y P˜ my
                                                           λ :=                                      (30)
  The regularized square residual is given by                             T    T           −1
                                                                trace(P˜ m)α˜ m(QmQm + λIm)  α˜ m
                       T        T
             E˜m  =   e˜me˜m + λα˜ mα˜ m       (22)
                                                      where                    m
                       T                                         ∂     P˜           qT q
                  =   y P˜ my                                      trace( m)          i  i   .
                                                                            =           T   2        (31)
                                                                     ∂λ           (λ + q qi)
where λ denotes a regularization paramter. The minimum of                      i=1      i
(22) is given by                                      The forward selection is stopped when λ stops changing sig-
       P˜       I − Q   QT Q     λI  −1QT             niﬁcantly.
         m  =        m(  m   m +   m)    m     (23)     The computational cost for this update is O(m). The ORO-
                          q  qT
                P˜    −    m  m   .                   LOS algortihm is summarized in pseudocode in Algorithm 1.
            =     m−1        T
                        λ + qmqm
Thus, the current residual corresponding to the regularized 4 Experiments
least squares problem may be updated as               To show the usefulness of the proposed method empirically,
                                                      some experiments for regression and classiﬁcation are per-
                              q  qT
           e        P˜    −    m  m   y               formed. In all experiments the Gaussian kernel
           ˜m  =(m−1             T    )
                            λ + qmqm                                                       

                                                                                          2
                                 T                                               x − x 
                               y  qm                              k x, x   exp   −
                   e    − q            .                           (    )=              2            (32)
               =   ˜m−1     m     T            (24)                                  2σ
                             λ + qmqm
The orthogonal weights                                is used. The kernel parameter σ is optimized using a 5-fold
                                                      crossvalidation in all experiments. For the classiﬁcation ex-
                     yT q
           α            i  ,    ≤ i ≤ m.              periments the one-vs-rest approach is used to obtain a multi-
          ( ˜ m)i =     T     1                (25)
                   λ + qi qi                          class classiﬁcation hypothesis.
can be computed when the forward selection is stopped. The 4.1 Classiﬁcation
original weights can then be recovered by
                                                      For classiﬁcation, 5 well-known benchmark datasets were
                          −1
                  αˆ m = Um α˜m                (26)   chosen. The USPS dataset contains 256 pixel values of hand-
                                                      written digits as training and testing instances.
whichisaneasyinversionsinceUm  is upper triangular.
                                                        The letter dataset contains 20000 labeled samples. The
  In each iteration one chooses the qi which corresponds to
                                                      character images were based on different fonts and each
the highest absolute value in the current residual and adds it to                 20
                                                      letter within these fonts was randomly distorted to produce
Qm−1. It is possible to determine the number of basis func-
                                                      a dataset of unique stimuli. For this dataset no predeﬁned
tions using crossvalidation or one may use for instance the
                                                      split for training and testing exist. We used the ﬁrst
Bayesian Information Criterion or the Minimum Description                                           16000
                                                      instances for training and the remaining instances for
Length as alternative stopping criteria. Following [Gu and                               4000
                                                      testing.
Wahba, 1991] and [Orr, 1995] it is possible to use the Gen-
                                                        Optdigits is a database of digits handwritten by Turkish
eralized Cross Validation (GCV ) as a stopping criterion. We
                                                      writers. It contains digits written by writers. The train-
will now summarize the results. For details see [Orr, 1995].                          44
                                                      ing set is generated from the ﬁrst writers and digits writ-
  The GCV  is given by                                                             30
                                                      ten by the remaining independent writers serve as testing in-
                                 2
                   1       P˜ my                    stances. The database was generated by scanning and pro-
         GCV                          .
              m = M                    2       (27)   cessing forms to obtain 32 × 32 matrices which were then
                      (1/M  ) trace(P˜ m)             reduced to 8 × 8.

                                                IJCAI-07
                                                   662Algorithm 1 Order-Recursive Orthogonal Least Squares
(OROLS)                                                           DATA SET   SV M    OROLS
Require: Training data X, labels y,kernel                         USPS        4.3     4.4(10)
                                                                  LETTER     2.75    2.61(4.3)
               λ ←    m  ←    K     1  K†     1 1T
  Initializations:  0,      1,  1 =   ,  1 = M    ,               OPTDIGITS  2.73   1.11(10.2)
  Q1 = 1, U1 =[1], I = {1,...,M}, Iopt = {}                       PENDIGITS   2.5    1.66(1.9)
                                                                  SATIMAGE    7.8    8.2(7.5)
  while λ changes signiﬁcantly and Qm is not illconditioned
  do                                                  Table 2: Test errors in % on 5 benchmark datasets. The one-
     Update e˜m                                       vs-rest approach is used. Average fraction of selected basis
                                                      centers in % within parantheses.
     ﬁnd the index iopt of the entry of e˜m with the highest
  absolute value
                                                      4.2  Regression
     Iopt ←{Iopt,iopt}
                                                      For regression, we ﬁrst perform experiments on a synthetic
     I ← I \{iopt}                                    dataset based on the function sinc(x)=sin(x)/x, x ∈
                                                      (−10, 10) which is corrupted by Gaussian noise. All training

     Compute kiopt                                    and testing instances are chosen randomly using a uniform

     Compute qiopt                                    distribution on the same interval. The results are illustrated in
     Km  ←  [Km−1kopt]                                ﬁgures 1-3 and table 3.
                                                        Additionally, the two real world datasets Boston and
     Qm  ←  [Qm−1qopt]                                Abalone, which are available from the UCI machine learning

             †                                        repository, are chosen. The hyperparameters are optimized in
     Update Km and Um  using kopt and qopt            a 5-fold crossvalidation procedure. For both datasets, ran-
                                                      dom  partitions of the mother data for training and testing
     Update λ                                         are generated (100 (10) partitions with 481 (3000) instances
                                                      for training and 25 (1177) for testing for the Boston and
     m ←  m +1                                        Abalone dataset, respectively). All continuous features are
                                                      rescaled to zero mean and unit variance for both Abalone and
  end while                                           Boston. The gender encoding (male / female /infant) for the
  return αˆ m,Iopt                                    Abalone dataset is mapped into {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.
                                                      The Mean Squared Error (MSE) of OROLS is compared with
                                                      a forward selection algorithm based on a QR-decomposition
  Pendigits contains pen-based handwritten digits. The digits of the Gram matrix [Nair et al., 2002]. The results in table
were written down on a touch-sensitive tablet and were then 4 show that the MSE is improved signiﬁcantly by OROLS.
resampled and normalized to a temporal sequence of eight In contrast to OROLS the QR method uses an external algo-
pairs of (x, y) coordinates. The predeﬁned test set is formed rithm with reorthogonalization for the update of the orthog-
entirely from written digits produced by independent writers. onal decomposition. We observed that our update scheme
  The satimage dataset was generated from Landsat Multi- which is to the best of our knowledge a novel update needs
Spectral Scanner image data. Each pattern contains 36 pixel not to be reorthogonalized as long as the Gram matrix has
values and a number indicating one of the six classes of the full rank. This improvement of accuracy could be one rea-
central pixel.                                        son for the good performance of OROLS. Furthermore, it
  The caracteristics of the datasets are summarized in table 1. should be noted that the best performance of OROLS for the
The results can be seen in table 2. Especially for the optdig- Boston dataset is quite favourable compared with the best per-
                                                                             . ±   .  [
its and pendigits datasets OROLS appears to be signiﬁcantly formance of SVMs (MSE 8 7 6 8) Sch¨olkopf and Smola,
                                                          ]
superior compared with SVMs. The performance on the re- 2002 .
maining 3 datasets is comparable with SVMs.

                                                                        METHOD   RMSE
                                                                         SVM     0.0519
    DATA SET   # CLASSES  # TRAINING  # TESTING
                                                                         RVM     0.0494
    USPS          10         7291       2007
                                                                        OROLS    0.0431
    LETTER        26        16000       4000
    OPTDIGITS     10         3823       1797
                                                      Table 3: Average RMSE for the sinc experiment. /
    PENDIGITS     10         7494       3498                                                     50  1000
    SATIMAGE       6         4435       2000          randomly generated points are used for training / testing. The
                                                      standard deviation of the Gaussian noise is 0.1 in all runs.
 Table 1: Datasets used for the classiﬁcation experiments. The results are avereged over 100 runs.


                                                IJCAI-07
                                                   663     1.2                                                   0.35
                                   training points
                                   basis centers
      1
                                   true function           0.3
                                   estimated function

     0.8
                                                           0.25

     0.6
                                                           0.2

     0.4
                                                          RMSE
                                                           0.15
     0.2

                                                           0.1
      0

                                                           0.05
    −0.2

    −0.4                                                    0
      −10 −8  −6 −4  −2   0   2  4   6   8   10              0    0.2  0.4   0.6  0.8   1    1.2   1.4
                                                                           noise standard deviation

Figure 1: Example ﬁt to a noisy sinc function using 50 / 1000 Figure 3: RMSE of ﬁts to a noisy sinc function w. r. t. dif-
randomly generated points for training / testing. The standard ferent noise levels. 100 / 1000 randomly generated points are
deviation of the Gaussian noise is 0.1. The Root Mean Square used for training / testing. The results are avereged over 100
Error (RMSE) is 0.0269 in this case. 9 points are selected as runs for each noise level.
basis centers.
                                                      The pseudoinverse is updated order-recursively and reveals
     0.2                                              the current orthogonal decomposition of the reduced Gram

    0.18                                              matrix within a forward selection scheme. The Generalized
                                                      Cross Validation serves as an effective stopping criterion and
    0.16
                                                      allows to adapt the regularization parameter in each iteration.
    0.14                                              Extensive empirical studies using synthetic and real-world

    0.12                                              benchmark datasets for classiﬁcation and regression suggest
                                                      that the proposed method is able to construct models with a
     0.1

    RMSE                                              very competitive generalization ability. The advantage of the
    0.08                                              proposed method compared with e. g. SVMs is its simplic-
    0.06                                              ity. Sparsity is achieved in a computationally efﬁcient way
                                                      by constuction and can hence better be controlled than in the
    0.04
                                                      SVM case where a optimization problem is to be solved. Fur-
    0.02                                              thermore, in contrast to SVMs OROLS allows an easy incor-
      0                                               poration of multiple kernels, i e. the kernel parameters may be
       0  50  100 150 200 250 300 350 400 450 500
                     number of training data          varied for different training instances in order to obtain more
                                                      ﬂexible learning machines. This possibility is not examined
                                                      here and may be an interesting direction for future work. A
Figure 2: RMSE of ﬁts to a noisy sinc function w. r. t. dif- further step for future work could be the development of the
ferent training set sizes. 1000 randomly generated points are proposed algorithm for tasks like dimensionality reduction or
used for testing. The standard deviation of the Gaussian noise online-learning.
is 0.1 in all runs. The results are avereged over 100 runs for
each size.
                                                      References

       DATASET       QR          OROLS                [Ben-Israel and Greville, 1977] A. Ben-Israel and T. N. E.
        BOSTON    8.35±5.67   7.9±3.28(26)               Greville. Generalized Inverses: Theory and Applications.
       ABALONE    4.53±0.29  4.32±0.17(10.8)             Wiley, 1977.
Table 4: Mean Square Error (MSE) with standard deviations [Billings and Lee, 2002] S. A. Billings and K. L. Lee. Non-
for the Boston and Abalone dataset using different methods. linear ﬁsher discriminant analysis using a minimum
Average fraction of selected basis centers in % within paran- squared error cost function and the orthogonal least
theses.                                                  squares algorithm. Neural Networks, 15:263–270, 2002.
                                                      [Chen et al., 1991] S.Chen,C.F.N.Cowan,andP.M.
5Conclusion                                              Grant. Orthogonal least squares learning for radial ba-
A computationally efﬁcient training algorithm for orthogo- sis function networks. IEEE Transactions on Neural Net-
nal least squares models using Mercer kernels is presented. works, 2(2):302–309, 1991.

                                                IJCAI-07
                                                   664