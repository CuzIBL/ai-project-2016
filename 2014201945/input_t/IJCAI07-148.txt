                Generalizing the Bias Term of Support Vector Machines

                        Wenye Li   and  Kwong-Sak Leung      and  Kin-Hong Lee
                           Department of Computer Science and Engineering
                                 The Chinese University of Hong Kong
                                {wyli, ksleung, khlee}@cse.cuhk.edu.hk


                    Abstract                          ﬁnd a solution to a regularized minimization problem
    Based on the study of a generalized form of rep-
                                                                       m
    resenter theorem and a speciﬁc trick in construct-               1                        2
    ing kernels, a generic learning model is proposed           min        V (yi,f(xi)) + γ fK      (2)
                                                                f∈HK m
    and applied to support vector machines. An algo-                    i=1
    rithm is obtained which naturally generalizes the
    bias term of SVM. Unlike the solution of standard
                                                      in HK for some γ>0. This expression represents a tradeoff
    SVM  which consists of a linear expansion of ker-
                                                      between the empirical error as calculated by the loss func-
    nel functions and a bias term, the generalized algo-
                                                      tion V , and ”smoothness” of the solution as represented by
    rithm maps predeﬁned features onto a Hilbert space
                                                      the norm of f in a reproducing kernel Hilbert space (RKHS)
    as well and takes them into special consideration by
                                                      H                                  2
    leaving part of the space unregularized when seek-  K which is induced by a kernel K. γ f K is also called
    ing a solution in the space. Empirical evaluations a regularizer. SVM can be partially derived from this frame-
                                                                                              (    (x)) ≡
    have conﬁrmed the effectiveness from the general- work with the choice of a hinge loss function V y,f
                                                      max (1 −   (x)  0)
    ization in classiﬁcation tasks.                            yf    , . The only incompatibility comes from
                                                      the bias term b, which makes the combined model (1) gener-
                                                      ally not in HK any more.
1  Introduction
                                                        In this paper, we follow the regularization point of view.
Support vector machines (SVM) have been shown to per- Based on the study of a generalized regularizer which leaves
form well in many machine learning applications. Based part of the hypothesis space unregularized and a speciﬁc trick
on Vapnik’s seminal work in statistical learning theory[Vap- in constructing kernels, we propose a new learning scheme
nik, 1998], the algorithm starts with the ideas of separating
                                          m           which allows user to predeﬁne features, and uses these fea-
hyperplanes and margins. Given the data (xi; yi)i=1 where
       d                                              tures and kernel expansions to derive a solution in the hy-
xi ∈R    and yi ∈{+1,   −1}, one searches for the lin- pothesis space, instead of considering kernel expansions only
ear hyperplane that separates the positive and negative sam- as in existing kernel-based methods. When the idea is applied
ples with the largest margin (the distance from the hyper- to SVM, we obtain an algorithm which naturally generalizes
plane to the nearest data point). In the nonseparable case, the bias term of SVM. The generalized term linearly com-
a soft margin method is used to choose a hyperplane that bines the predeﬁned features instead of being a constant only.
splits the examples as cleanly as possible. Using a kernel
                                                    Different from the empirical results that the existence of the
Kx (x )=K   (x, x ) (a positive deﬁnite function) to extend bias term does not make much signiﬁcance in practice[Rifkin,
the algorithm to the nonlinear case, an optimal hyperplane is 2002], we will show the term is not trivial any more when it is
found which has a simple representation as a linear expansion generalized, and appropriate choices of the term will improve
of kernel functions and a constant                    the applicability of the algorithm.
                      m
                  ∗                                     The paper is organized as follows. In section 2, we in-
                 f =     ciKxi + b              (1)
                                                      vestigate a generalized regularizer in the regularized learning
                      i=1
                                                      framework and study its associated solution when it is applied
where each ci is a real number and b is called a bias term. to SVM. In section 3, after introducing a kernel construc-
  In the case of nonseparability, some meanings of the mar- tion trick, with which predeﬁned features are mapped onto an
gin motivation are lost. [Girosi, 1998; Evgeniou et al., 2000; RKHS, we further clarify the mathematical details in deriv-
Poggio and Smale, 2003] suggest a different view of SVM ing SVM. A new algorithm is proposed in section 4 based on
based on a framework championed by Poggio and other   the previous discussions. Empirical results are demonstrated
researchers[Poggio and Girosi, 1990a; 1990b] which implic- in section 5. Finally, section 6 presents our discussions and
itly treats learning as an approximation problem and tries to conclusions.

                                                IJCAI-07
                                                   9192  Generalized Regularized Learning                   The equation holds for all g ∈HK . Speciﬁcally, letting g =
                                                         (·)
Suppose HK is the direct sum of two subspaces: HK = H0⊕ Kx  gives
H        H   =      (   ···   )              (≤   )                            m
  1, where 0   span  ϕ1,   ,ϕ is spanned by   m                       ∗    1
                                                                     P1f  =        αiyiKxi
linearly independent features. We consider the generalized                  2γ
regularized learning by minimizing                                             i=1
              1 m                                    by the reproducing property of kernels. Or,
         min         (    (x )) +     2
                   V  yi,f  i    γ  P1f K       (3)                                  m
        f∈HK  m                                                                      
                i=1                                             ∗     ∗      ∗    1
                                                               f  =(f   − P1f )+        αiyiKxi .
                                            H                                     2
where P1f is the orthogonal projection of f onto 1 and                             γ i=1
       2
   1 
γ P  f K is called a generalized regularizer. When the ∗       ∗                            ∗
model is applied to SVM, we consider as the hinge loss f − P1f  is the orthogonal projection of f onto H0,and
                                  V                                            
function mentioned above. By introducing slake variables ξi hence it can be represented as p=1 λpϕp.Sowehave,
corresponding to the empirical error at point xi, our problem
becomes                                                                          m
                    m                                                ∗ =         +
                 1                                                 f       λpϕp      ciKxi           (7)
            min       ξi + γ P1f,P1f                                   p=1       i=1
           f∈H                       K
               K m i=1
                                                              ∈R        =  1
satisfying                                            where λp     and ci  2γ αiyi.
                                                        The derived minimizer in (7) satisﬁes the reproduction
                yif (xi) ≥   1 − ξi                                          m
                                                      property. Suppose (xi; yi) comes from a model that is
                         ≥   0                                               i=1
                     ξi                               perfectly linearly related to {ϕ1, ···,ϕ}, it is desirable to
where ·, ·K denotes the inner product in HK .Wederivethe get back a solution independent of other features. As an ev-
dual quadratic program by using the technique of Lagrange ident result of (3), the property is satisﬁed. The parameters
multipliers:                                          c1, ···,cm in (7) will all be zero, which makes the regularizer
               m                                     in (3) equal to zero. As will be shown in the experiments, this
             1                                        property often has the effect of stabilizing the results from dif-
     L  =         ξi + γ P1f,P1fK             (4)
            m  i=1                                    ferent choices of kernels and regularization parameters prac-
              m                      m              tically.
            −    αi (yif (xi) − 1+ξi) −   ζiξi.
              i=1                     i=1             3   Mapping Predeﬁned Features onto RKHS
                                        i
We want to minimize L with respect to f and ξ and maxi- By decomposing a hypothesis space HK and studying a gen-
mize w.r.t. αi and ζi, subject to the constraints of the primal eralized regularizer, we have proposed a generalized regular-
problem and nonnegativity constraints on αi and ζi.Taking ized learning model and derived its associated solution which
derivative w.r.t. ξi and setting it to zero, we have  consists of a linear expansion of kernel functions and prede-
               ∂L    1                                ﬁned features. In this section, we will introduce a kernel con-
                  =    − αi − ζi =0.            (5)
               ∂ξi   m                                struction trick which maps kernel functions and predeﬁned
                                                      features simultaneously onto a Hilbert space. Then we will
Substituting (5) into (4), we have
                         m              m             show the mathematical details in deriving SVM with a gener-
                                                    alized bias term.
     L = γ P1f,P1fK −     αiyif (xi)+   αi.   (6)
                        i=1            i=1
         ∗                                            3.1  A Kernel Construction Trick
Suppose f is the minimizer to (6). For any f ∈HK ,let
  =  ∗ +          ∈R        ∈H                        Given features {ϕ1, ···,ϕ} and a strictly positive deﬁnite
f   f   δg where δ    and g    K .Wehave                      Φ
                    ∗           ∗                     function , let us consider the following reproducing kernel
       L  =   γ P1f + δP1g,P1f  + δP1gK
                m                    m                                          
                         ∗                                      (x y)=    (x y)+        (x)  (y)
              −    αiyi (f + δg)(xi)+    αi                   K   ,     H   ,        ϕp    ϕp         (8)
                i=1                   i=1                                         p=1
By taking derivative w.r.t. δ,wehave                  where
                                 m
    ∂L          ∗
       =2γ  P1f  + δP1g,P1g  −     αiyig (xi) .              H (x, y)                               (9)
    ∂δ                       K
                                  i=1                                    
                               ∂L                                                    
      ∗                                                    =Φ(x,   y)+         ϕp (x) ϕq (y)Φ(xp, xq)
Since f is the minimizer, we have ∂δ |δ=0 =0. Then the
problem becomes                                                         p=1 q=1
                         m                                                       
               ∗                                               −      (x)Φ(x   y) −      (y)Φ(x x )
        2γ P1f ,P1gK −     αiyig (xi)=0.                          ϕp        p,        ϕq       , q  ,
                          i=1                                    p=1                q=1

                                                IJCAI-07
                                                   920                                                                           
T
{  ···   }                                                 ˜     ˜     ˜                             T
 ϕ1,   ,ϕ  deﬁnes a linear transformation of the prede- where λ = λ1, ···, λ , α    =(α1,     ···,αm) ,
ﬁned features {ϕ1, ···,ϕ} w.r.t. {x1, ···x}:
                                                            =(···         )T         =(         ···   )T
                                −1             α1         α1,   ,α  ,  α2         α+1,   ,αm   ,
   ϕ1 (x)      ϕ1 (x1)  ···  ϕ1 (x)      ϕ1 (x)      Y    =       (   ···  )  Y    =       (    ···   )
                                                        1     diag y1, ,y ,   2    diag y+1,  ,ym ,
    ···    =      ···         ···          ···                      ,                  ,m
                                                     E1  =  ϕp (xi)       , E2 =   ϕp (xi)         and 1
   ϕ (x)       ϕ (x1) ···  ϕ (x)      ϕ (x)                     p=1,i=1               p=1,i=+1
                                               (10)   is a vector of ones with appropriate size. Taking derivative
                                                           ˜
and satisﬁes                                         w.r.t. λ, we obtain
                     1   1 ≤ p = q ≤ 
          ϕ  (xp)=                     .       (11)                 E1Y1α1  + E2Y2α2   = 0.          (20)
            q         0   1 ≤ p 
= q ≤ 
                                                      Then, we have
  This trick was studied in [Light and Wayne, 1999] to pro-
                                                                        T       T          T
vide an alternative basis for radial basis functions and ﬁrst     L = γ˜c H˜c − α2 Y2H˜c +1 α        (21)
used in a fast RBF interpolation algorithm[Beatson et al.,
2000]. A sketch of properties peripheral to our concerns in- Taking derivative w.r.t. ˜c and setting it to zero, we have
cludes
                                                                     2γH˜c − HY2α2  = 0.
                     Kxp = ϕp                  (12)
                         
                                                    H
                         1   p = q                    is strictly positive deﬁnite and hence invertible. So we have
              ϕp,ϕq    =                       (13)
                     K     0   p 
= q
                                                                               Y2α2
                          =0                                              ˜c =      .                (22)
                    Hxp                      (14)                             2γ
                         
                   Hxi ,ϕp   =0                (15)
                         K                          Substituting it back into (21), we have
                          =   (x  x )
              Hxi ,Hxj K    H   i, j           (16)
                                                                1  T  T           1  T  T           T
where 1 ≤ p, q ≤ ,and +1  ≤ i, j ≤ m. Another use-    L  =      α2 Y2 HY2α2  −    α2 Y2 HY2α2  + 1  α
                                          m                    4γ                2γ
ful property is that the matrix H =(H (xi, xj))  is                                        
                                          i,j=+1                 1   1
strictly positive deﬁnite, which will be used in the following          T  T              T
                                                           =   −       α2 Y2 HY2α2  − 2γ1  α
computations.                                                    2γ   2
  By property (12), we can see that predeﬁned features
                                                      So, the problem becomes
{ϕ1, ···,ϕ} are explicitly mapped onto HK , which has a
                          
subspace H0 = span (ϕ1, ···,ϕ)=span (ϕ1, ···,ϕ).By                  1
                                                                       T  T             T
            {   ···   }                                           min  α2 Y2 HY2α2   − 2γ1 α         (23)
property (13), ϕ1, ,ϕ  also forms an orthonormal basis            α  2
of H0.
                                                      satisfying
3.2  Computation                                                   1
                                                           0 ≤   ≤         E Y     + E Y     = 0
Using the kernel deﬁned in (8) and (9) and its properties in   α   m,and    1  1α1    2  2α2    .    (24)
(12) and (14), the minimizer in (7) can be rewritten as:
                             m                       The ﬁrst box constraint comes from (5) and the nonnegativ-
                                                                              1
            ∗                                         ity of α, requiring 0 ≤ αi ≤ (1 ≤ i ≤ m). The second
           f   =      λpϕp +     ciKxi         (17)                             m
                                                                                        T
                   p=1       i=1                      equality constraint comes from (20). Y2 HY2 is a strictly
               =   ···                                positive deﬁnite matrix, and the problem becomes a standard
                            m                      quadratic program(QP). Comparing with SVM’s QP[Vapnik,
                      ˜                                  ]
               =      λpϕp +      c˜iHxi              1998 , the new problem requires  equality constraints in-
                   p=1       i=+1                    stead of one equality constraint. This seems to burden the
      ˜     ˜                                         computations. Actually, because the major computations in
where λ1, ···, λ, c˜+1, ···, c˜m are m parameters to be de- solving the QP come from the box constraint, these equality
termined. Furthermore, from the orthogonal property (15)
                                                     constraints will not affect the computations much.
               x                                                                                  ˜
between ϕp and H i ,wehave                              The solution of ˜c is obtained after α by (22). Then λ comes
                        m                            from a linear program,
                    ∗
                 1   =      ˜i  x              (18)
                P f         c H  i .                                            m
                       i=+1                                                  1
                                                                          min       ξi               (25)
                                                                           λ˜
By property (16), we have                                                     m i=1
                  ∗    ∗  = ˜cT H˜c
               P1f  ,P1f  K                           satisfying
     ˜c =(˜    ··· ˜ )T                                     ⎛                      ⎞
where     c+1,   , cm . Substituting it into (6), we ob-              m
tain in a matrix representation                             ⎝     ˜   +       ˜    ⎠ (x )  ≥   1 −
                                          
               yi     λpϕp         cjHxj    i          ξi,
       T       T     T ˜   T      T ˜          T              p=1       j=+1
L =  γ˜c H˜c − α1 Y1E1 λ − α2 Y2 E2 λ + H˜c + 1 α
                                               (19)                                    ξi  ≥   0.

                                                IJCAI-07
                                                   9214  A Generalized Algorithm
4.1  Algorithm
Based on the discussions above, an algorithm which general-
izes the bias term of SVM is proposed as follows:
                       m
 1. Start with data (xi; yi)i=1.
 2. For  (≤ m) predeﬁned linearly independent features
                                       
    {ϕ1, ···,ϕ} of the data, deﬁne {ϕ1, ···,ϕ} according
    to equation (10).
 3. Choose a symmetric, strictly positive deﬁnite function
                                         d     d
    Φx (x )=Φ(x,  x ) which is continuous on R ×R .
    Get Hx according to equation (9).
            : Rd →R                                   Figure 1: Image classiﬁcation accuracies using coil20 dataset
 4. Deﬁne f          by                               with different number of training images. Twenty predeﬁned
                            m                      features along with a Gaussian kernel Φ are used by SVM-
                   ˜                                 GB; Φ  is also used by SVM. The kernel and regularization
        f (x)=    λpϕp (x)+       c˜iHxi (x) , (26)
               p=1           i=+1                    parameters are selected via cross validation. One-versus-one
                                                      strategy is used for multi-classiﬁcation.
    where  the solution of c˜+1, ···, c˜m comes from
    a quadratic program (23) and equation (22), and
    ˜      ˜                                          5   Experiments
    λ1, ···, λ is obtained by solving a linear program (25).
                                                      To evaluate the effectiveness brought by the generalized bias
4.2  Virtual Samples                                  term, two groups of empirical results are reported. The ﬁrst
The new algorithm needs the construction of an orthonormal group of experiments focused on the reproduction property
             
     {   ···   }                       {  1 ···  }   of the generalized bias term. It used Columbia university im-
basis ϕ1,   ,ϕ  from predeﬁned features ϕ ,  ,ϕ                              1
w.r.t. samples via a linear transformation. However, al- age library (coil-20) dataset for image classiﬁcation. The
                                                               1 440       x  ··· x
gorithms, which search for linear hyperplanes to separate the dataset has , images 1, , 1440 evenly distributed in
                                                      20
data, are often not invariant to linear transformations. To pro- classes. After preprocessing, each image was represented
                                                          32 × 32 = 1024
vide the invariance, we consider an alternative approach that as a       dimensional vector with each compo-
                                                                               0     255
avoids the transformation.                            nent having a value between and    indicating the gray
                          x   ··· x                   level at each pixel. Four experiments were performed. Each
  We introduce  virtual points v1 , , v satisfying
                                                     experiment used the ﬁrst 6, 12, 24 and 48 images respectively
                     1  1 ≤ p = q ≤                within each class for training and the rest for testing.
          ϕp xvq  =                     .
                       0  1 ≤ p 
= q ≤                 Using recently developed nonlinear dimensionality reduc-
                                                      tion technqiues[Tenenbaum et al., 2000; Roweis and Saul,
                             x               0
The label yvq of each virtual point vq is deﬁned to be .We 2000], we found that these images actually form a manifold
study the following problem:                          with a much lower dimension than the original 1, 024 dimen-
                                                      sions. With the prior knowledge, twenty features ϕ1, ···,ϕ20
                    min  Lv (f)                (27)
                    f∈HK                              were predeﬁned as follows:
                                                                     
where                                                                   1   3NN  (xi) ∩ trainp 
= {}
                                                            ϕp (xi)=
                                                                        0         otherwise
             1             
 Lv (f)=           1 − yvq f xvq                            1 ≤  ≤ 20 1 ≤  ≤  1440 3    (x)
             m                   +                    where    p     ,    i       , NN     deﬁnes the three
               q=1                                    nearest neighbors in geodesic distance to x,andtrainp is the
                 m
               1                           2          set of training images with class label p. The geodesic dis-
             +      (1 − yif (xi)) + γ P1f          tance was computed by the graph shortest path algorithm as
              m                 +          K
                 i=1                                  in [Tenenbaum et al., 2000]. Deﬁning features in this way, we
                    m                                implicitly used a kNN classiﬁer as a generalized bias term.
                 1                           2
         =     +       (1 − yif (xi))+ + γ P1fK .   Similar ideas can be generalized to combining different ap-
             m    m i=1                               proaches other than kNN.
                                                        Figure 1 compares classiﬁcation accuracies among SVM,
  It can be easily seen that ϕ1, ···,ϕ have formed an or-
                                                      kNN, and the new algorithm (denoted by SVM-GB). An in-
thonormal basis of H0 w.r.t. the virtual points and there is no
                                                    sightful study of the results may ﬁnd that the 20 predeﬁned
need to compute ϕ , ···,ϕ any more. The new regularized
                1                                    features have dominated the learned SVM-GB solution due to
minimization problem in (27) is equivalent to the original one
                                                      the reproduction property, and made the performance of the
in (3) with a hinge loss. The introduction of the virtual points
                                                      algorithm similar to kNN which shows better performance
does not change the scope of the classiﬁcation problem while
                                                      than standard SVM on this dataset.
having avoided the linear transformation to construct an or-
thonormal basis.                                         1http://www1.cs.columbia.edu/CAVE/software/

                                                IJCAI-07
                                                   922                                                      GB has better results than SVM in BoW representation in all
                                                      experiments, which gives us the belief that the embedding of
                                                      pLSA features improves the accuracy.

                                                      6   Discussion and Conclusion
                                                      The idea of regularized learning can be traced back to modern
                                                      regularization theory[Tikhonov and Arsenin, 1977; Morozov,
                                                      1984], which states that the existence of a regularizer helps to
                                                      provide a unique stable solution to ill-posed problems. In this
                                                      paper, we have considered the usage of a generalized regular-
                                                      izer in kernel-based learning. One straightforward reason that
                                                      part of the hypothesis space is left unregularized in the new
Figure 2: Image classiﬁcation accuracies by ﬁxing four regularizer is due to a well-accepted fact that ﬁnite dimen-
groups of kernel and regularization parameters(log-scale). sional problems are often well-posed[Bertero et al., 1988;
                                                      De Vito et al., 2005] and do not always need regularization,
                                                      as in the case of learning problems deﬁned in a subspace
  The reproduction property also helps to lessen the sensitiv- spanned by a number of predeﬁned features. Similar idea
ity of the algorithm to the choices of kernels and the regular- was explored in spline smoothing[Wahba, 1990] that leaves
ization parameter γ. To illustrate the effect, another experi- polynomial features unregularized. In kernel-based learn-
                      48×20
ment was conducted using    training images. Instead of ing, the most similar to our work is the semiparametric SVM
using cross validation for parameter selection, four groups of model[Smola et al., 1998], which combines a set of unreg-
kernel and regularization parameters were ﬁxed. Figure 2 de- ularized basis functions with SVM. However, to our knowl-
picts the results. The generalized bias term makes SVM-GB edge, few algorithms and applications have been investigated
more stable to the variations of kernel and regularization pa- for machine learning tasks from a uniﬁed RKHS regulariza-
rameters. This property is especially useful concerning the tion viewpoint in a general way.
practical situations where arbitrariness exists in designing With the new regularizer and a speciﬁc trick in construct-
kernels for some applications.                        ing kernels, predeﬁned features are explicitly mapped onto
  To further study the performance of the new algorithm, the a Hilbert space and taken into special consideration during
second group of experiments was conducted on text catego-
                                   2                  the learning, which differentiates our work from standard
rization tasks using 20-newsgroups dataset . The dataset col- kernel-based approaches that only consider kernel expansions
lects UseNet postings into twenty newsgroups and each group and a constant in learning. For projecting predeﬁned fea-
         1 000
has about ,    messages. We experimented with its four tures onto an RKHS, the idea of a conditionally positive def-
major subsets. The ﬁrst subset has ﬁve groups (comp.*), the inite function[Micchelli, 1986] is lurking in the background,
second four groups (rec.*), the third four groups (sci.*) and which goes beyond the discussion of this paper.
the last four groups (talk.*).                          When applying the idea to SVM, we have developed a new
                                   2 000
  For the dataset, we removed all but the , words with algorithm which can be regarded as having generalized the
highest mutual information with the class variable by rain- bias term of SVM. With domain speciﬁc knowledge in pre-
bow package[McCallum, 1996]. Each document was repre- deﬁning features, this generalization makes the bias term not
sented by bag-of-words (BoW) which treats individual words trivial any more. As conﬁrmed by the experimental results,
                                   10
as features. A linear kernel coupled with predeﬁned fea- correct choices of features help to improve the accuracy and
        ···
tures ϕ1, ,ϕ10, which were obtained by probabilistic la- make the algorithm more stable in terms of sensitivity to the
tent semantic analysis (pLSA) [Hofmann, 1999], was used as selection of kernels and regularization parameters.
input for SVM-GB. Experiments were carried out with dif-
             100˜3 200
ferent number (   ,   ) of training documents for each Acknowledgments
subset. One experiment consisted of ten runs and the average
accuracy was reported. In each run, the data were separated This research was partially supported by RGC Earmarked
by the xval-prep utility accompanied in C4.5 package3. Grant #4173/04E and #4132/05E of Hong Kong SAR and
  As a comparison, SVM was tested with a linear kernel, RGC Research Grant Direct Allocation of the Chinese Uni-
which is a commonly used method for text categorizations. versity of Hong Kong.
For completeness, both BoW and pLSA representations of
documents were experimented. As can been seen in ﬁgure 3, References
although SVM with pLSA representation performs well when
                                                      [Beatson et al., 2000] R.K. Beatson, W.A. Light, and
the number of training samples is small, the approach loses
                                                         S. Billings. Fast solution of the radial basis function
its advantage when the training set increases to a moderate
                                                         interpolation equations: Domain decomposition methods.
size. SVM-GB reports the best performance when we have
                                                         SIAM J. Sci. Comput., 22:1717–1740, 2000.
800 training documents or more. It is also shown that SVM-
                                                      [Bertero et al., 1988] M. Bertero, C. De Mol, and E.R. Pike.
  2http://www.cs.cmu.edu/˜TextLearning/datasets.html     Linear inverse problems with discrete data: II. stability and
  3http://www.rulequest.com/Personal/c4.5r8.tar.gz       regularisation. Inverse Probl., 4(3):573–594, 1988.

                                                IJCAI-07
                                                   923