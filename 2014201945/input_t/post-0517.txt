        Networked Distributed POMDPs: A                        Synergy of Distributed
                       Constraint Optimization and POMDPs

           Ranjit Nair       Pradeep Varakantham, Milind Tambe               Makoto Yokoo
    Knowledge Services Group           Computer Science Dept.          Dept. of Intelligent Systems
      Honeywell Laboratories       University of Southern California        Kyushu University
    ranjit.nair@honeywell.com         {varakant,tambe}@usc.edu           yokoo@is.kyushu-u.ac.jp

                    Abstract                                      N   Loc1-1    Loc2-1
                                                                W   E
    In many   real-world multiagent applications                1S         2          3
    such as distributed sensor nets, a network of                      Loc1-2             Loc2-2
    agents is formed based on each agent’s limited                              Loc1-3
    interactions with a small number of neighbors.
    While distributed POMDPs capture the real-                             5          4
    world uncertainty in multiagent domains, they
    fail to exploit such locality of interaction. Dis- Figure 1: Sensor net scenario: If present, target1 is in
    tributed constraint optimization (DCOP) cap-      Loc1-1, Loc1-2 or Loc1-3, and target2 is in Loc2-1 or
    tures the locality of interaction but fails to cap- Loc2-2.
    ture planning under uncertainty. This paper
    present a new model synthesized from distrib-     domain, are unlikely to work well for such a domain be-
    uted POMDPs and DCOPs, called Networked           cause they are not geared to take advantage of the local-
    Distributed POMDPs (ND-POMDPs).        Ex-        ity of interaction. As a result they will have to consider
    ploiting network structure enables us to present  all possible action choices of even non-interacting agents,
    a distributed policy generation algorithm that    in trying to solve the distributed POMDP. Distributed
    performs local search.                            constraint satisfaction and distributed constraint opti-
                                                      mization (DCOP) have been applied to sensor nets but
                                                      these approaches cannot capture the uncertainty in the
1   Introduction                                      domain. Hence, we introduce the networked distributed
In many real-world multiagent applications such as dis- POMDP (ND-POMDP) model, a hybrid of POMDP and
tributed sensor nets, a network of agents is formed based DCOP, that can handle the uncertainties in the domain
on each agent’s limited interactions with a small num- as well as take advantage of locality of interaction. Ex-
ber of neighbors. For instance, in distributed sensor nets, ploiting network structure enables us to present a novel
multiple sensor agents must coordinate with their neigh- algorithm for ND-POMDPs – a distributed policy gen-
boring agents in order to track individual targets mov- eration algorithm that performs local search.
ing through an area. In particular, we consider in this
paper a problem motivated by the real-world challenge 2   ND-POMDPs
in [Lesser et al., 2003]. Here, each sensor node can scan
in one of four directions — North, South, East or West We deﬁne an ND-POMDP for a group    Ag of n agents
(see Figure 1), and to track a target, two sensors with as a tuple hS, A, P, Ω, O, R, bi, where S = ×1≤i≤nSi ×
overlapping scanning areas must coordinate by scanning Su is the set of world states. Si refers to the set of
the same area simultaneously. We assume that there are local states of agent i and Su is the set of unaﬀectable
two independent targets and that each target’s move-  states. Unaﬀectable state refers to that part of the world
ment is uncertain but unaﬀected by the actions of the state that cannot be aﬀected by the agents’ actions, e.g.
sensor agents. Additionally, each sensor receives obser- environmental factors like target locations that no agent
vations only from the area it is scanning and this obser- can control. A = ×1≤i≤nAi is the set of joint actions,
vation can have both false positives and false negatives. where Ai is the set of action for agent i.
Each agent pays a cost for scanning whether the target  We   assume  a  transition independent distributed
is present or not but no cost if turned oﬀ. Existing dis- POMDP model, where the transition function is de-
                                                                     ′          ′                      ′
tributed POMDP algorithms  [Montemerlo et al., 2004;  ﬁned as P (s, a, s ) = Pu(su, su)·Q1≤i≤n Pi(si, su, ai, si),
Nair et al., 2003; Becker et al., 2004; Hansen et al., 2004] where a=ha1, . . . , ani is the joint action performed in
                                                                                 ′   ′     ′  ′
although rich enough to capture the uncertainties in this state s=hs1, . . . , sn, sui and s =hs1, . . . , sn, suiis the re-sulting state. Agent i’s transition function is deﬁned as contain agent i:
            ′        ′
Pi(si, su, ai, si) = Pr(si|si, su, ai) and the unaﬀectable                             0
                                     ′       ′        Vπ[Ni]=    bu(su)bNi (sNi )bi(si) V (sl1, . . . ,slk, su, hi)
transition function is deﬁned as Pu(su, su) = Pr(su|su).      X                    X  πl
Becker et al. [2004] also relied on transition indepen-     si,sNi ,su          l∈E s.t. i∈l
dence, and Goldman and Zilberstein [2004] introduced                                                  (1)
the possibility of uncontrollable state features. In both
                                                      While trying to ﬁnd best policy for agent i given its
works, the authors assumed that the state is collectively
                                                      neighbors’ policies, we do not need to consider non-
observable, an assumption that does not hold for our do-
                                                      neighbors’ policies. This is the property of locality of
mains of interest.
                                                      interaction that is used in the following section.
  Ω =  ×1≤i≤nΩi is the set of joint observations where
Ωi is the set of observations for agents i. We make   3   Locally optimal policy generation
an assumption of observational independence, i.e., we
deﬁne the joint observation function as O(s, a, ω) =  The locally optimal policy generation algorithm called
                                                      LID-JESP   (Locally interacting distributed joint equi-
       O (s , s , a , ω ), where s = hs1, . . . , s , s i, a =
Q1≤i≤n  i  i u  i  i                     n  u         librium search for policies) is based on the DBA algo-
 a , . . . , a ω  ω , . . . , ω   O  s , s , a , ω
h 1     ni,   =  h 1      ni, and  i( i u  i  i) =    rithm [Yokoo and Hirayama, 1996] and JESP   [Nair et
   ω s , s , a
Pr( i| 1 u  i).                                       al., 2003]. In this algorithm (see Algorithm 1), each
  The reward function,  R, is deﬁned as  R(s, a) =    agent tries to improve its policy with respect to its neigh-
Pl Rl(sl1, . . . , slk, su, hal1, . . . , alki), where each l could bors’ policies in a distributed manner similar to DBA.
refer to any sub-group of agents and k = |l|. In the  Initially each agent i starts with a random policy and
sensor grid example, the reward function is expressed exchanges its policies with its neighbors. It then evalu-
as the sum of rewards between sensor agents that have ates its contribution to the global value function, from
overlapping areas (k = 2) and the reward functions for its initial belief state b with respect to its current policy
an individual agent’s cost for sensing (k = 1). Based on and its neighbors’ policy (function Evaluate()). Agent
the reward function, we construct an interaction hyper- i then tries to improve upon its current policy by calling
graph where a hyper-link, l, exists between a subset of function GetValue, which returns the value of agent
agents for all Rl that comprise R. Interaction hypergraph i’s best response to its neighbors’ policies. Agent i then
is deﬁned as G = (Ag, E), where the agents, Ag, are the computes the gain that it can make to its local neigh-
vertices and E = {l|l ⊆ Ag ∧ Rl is a component of R}  borhood utility, and exchanges its gain its neighbors. If
are the edges. Neighborhood of i is deﬁned as Ni = {j ∈ i’s gain is greater than that one any of its neighbors,
Ag|j 6= i ∧ (∃l ∈ E, i ∈ l ∧ j ∈ l)}. SNi = ×j∈NiSj refers i changes its policy and sends its new policy to all its
to the states of i’s neighborhood. Similarly we deﬁne neighbors. This process of trying to improve the local
ANi , ΩNi , PNi and ONi .                             policy is continued until termination, which is based on
  b, the distribution over the initial state, is deﬁned as maintaining and exchanging a counter that counts the
                                                      number of cycles where gaini = 0. This is omitted from
b(s) = bu(su) · Q1≤i≤n bi(si) where bu and bi refer to the
distributions over initial unaﬀectable state and over i’s this article in order to simplify the presentation.
                                                        The algorithm for computing the best response is a
initial state, respectively. We deﬁne b i =  b (s ).
                                 N    Qj∈Ni  j  j     dynamic-programming approach similar to that used in
We assume that b is available to all agents (although it is JESP. Here, we deﬁne an episode of agent i at time t as
possible to reﬁne our model to make available to agent et = st , st, st , ~ωt . Given that the neighbors’ poli-
i     b   b     b                                      i   
 u  i  Ni  Ni 
 only  u, i and  Ni ). The goal in ND-POMDP is to     cies are ﬁxed, treating episode as the state, results in a
                  π    π , . . . , π
compute joint policy = h i     ni that maximizes the  single agent POMDP, where the transition function and
                                         T
team’s expected reward over a ﬁnite horizon starting  observation function can be deﬁned as follows:
from b. πi refers to the individual policy of agent i and
                                                        ′ t  t t+1       t  t+1      t t     t+1
is a mapping from the set of observation histories of i to P (ei, ai, ei )=Pu(su, su ) · Pi(si, su, ai, si ) ·
Ai. πNi and πl refer to the joint policies of the agents in        t   t      t+1        t+1 t+1
                                                              PNi (sNi , su, aNi , sNi ) · ONi (sNi , su , aNi , ωNi )
Ni and hyper-link l respectively.
  ND-POMDP can be thought of as an    n-ary DCOP              ′  t+1  t  t+1        t+1  t+1
                                                             O (e   , a , ω ) =Oi(s    , s  , ai, ωi)
where the variable at each node is an individual agent’s         i    i  i          i    u
policy. The reward component Rl where |l| = 1 can be  The function GetValue()   returns the optimal policy
thought of as a local constraint while the reward com- for agent i given the above deﬁnitions of transition and
ponent Rl where l > 1 corresponds to a non-local con- observation functions and the policies of Ni. In this pa-
straint in the constraint graph. In the next section, we per, GetValue() was implemented via value iteration
push this analogy further by taking inspiration from the but any other single agent POMDP algorithm could have
DBA algorithm  [Yokoo and Hirayama, 1996], an algo-   been used.
rithm for distributed constraint satisfaction, to develop
an algorithm for solving ND-POMDPs.                   4   Experimental Results
  We deﬁne  Local neighborhood utility of agent i as the For our experiments, we ran the LID-JESP algorithm on
expected reward accruing due to the hyper-links that  the sensor domain (see Figure 1). The ﬁrst benchmarkAlgorithm  1 LID-JESP(Agent i)
                                                                    LID-JESP   LID-JESP-no-n/w JESP
 1: πi ← randomly selected policy, prevV al ← 0
 2: Exchange πi with Ni                                      10000
 3: while termination not detected do                         1000
 4:  for all si, sNi , su do
                  +                                           100
 5:    prevV al   ←    bu(su) · bi(si) · bNi (sNi ) ·
       Evaluate(Agent i, si, su, sNi , πi, πNi , hi , hi , 0, T ) 10
 6:  gaini ← GetValue(Agent i, b, πNi , 0, T ) − prevV al       1
 7:  Exchange gaini with Ni
                                                               0.1
 8:  maxGain  ← maxj∈Ni∪{i}gainj
                                                             Run  time (secs)
 9:  winner ← argmaxj∈Ni∪{i}gainj                             0.01
10:  if maxGain > 0 and i = winner then
                                                             0.001
11:    initialize πi                                                1      2      3      4      5
12:    FindPolicy(Agent i, b, hi , πNi , 0, T )                                 Horizon
13:    Communicate πi with Ni
14:  else if maxGain > 0 then                                      LID-JESP    LID-JESP-no-n/w JESP

15:    Receive πwinner from winner and update πNi             300
16: return πi
                                                              250

                                                              200
(JESP) is Nair et al.’s JESP algorithm [2003], which uses
a centralized processor to ﬁnd a locally optimal joint pol-   150


icy and does not consider the interaction graph. The sec-   Value
                                                              100
ond benchmark (LID-JESP-no-nw) is LID-JESP with a
fully connected interaction graph. Figure 2(a) shows the      50
run time in seconds on a logscale on the Y-axis for in-
                                                               0
creasing ﬁnite horizon T on the X-axis, while Figure 2(b)           1      2      3      4      5
shows the value of policy found on the Y-axis and in-                           Horizon
creasing ﬁnite horizon T on the X-axis. All run times
and values were obtained by averaging 5 runs, each with Figure 2: 5 agent F-conﬁg. (a) Run time (secs), (b)
diﬀerent randomly chosen starting policies . As seen in Value
Figure 2(b), the values obtained for LID-JESP, JESP
and LID-JESP-no-nw are quite similar, although LID-
JESP and LID-JESP-no-nw often converged on a higher   [Hansen et al., 2004] E.A. Hansen, D.S. Bernstein, and
local optima than JESP. In comparing the run times, it  S. Zilberstein. Dynamic Programming for Partially
should be noted that LID-JESP outperforms LID-JESP-     Observable Stochastic Games. In AAAI, 2004.
no-nw and JESP (which could not be run for  T  > 4    [Lesser et al., 2003] V. Lesser, C. Ortiz, and M. Tambe.
within 10000 secs).                                     Distributed sensor nets: A  multiagent perspective.
                                                        Kluwer academic publishers, 2003.
5   Acknowledgments                                   [Montemerlo et al., 2004] R. E. Montemerlo, G. Gor-
This material is based upon work supported by the       don, J. Schneider, and S. Thrun. Approximate so-
DARPA/IPTO COORDINATORs program and the Air             lutions for partially observable stochastic games with
Force Research Laboratoryunder Contract No. FA8750–     common payoﬀs. In  AAMAS, 2004.
05–C–0030.The views and conclusions contained in this [Nair et al., 2003] R. Nair, D. Pynadath, M. Yokoo,
document are those of the authors, and should not be in- M. Tambe, and S. Marsella. Taming decentralized
terpreted as representing the oﬃcial policies, either ex- POMDPs: Towards eﬃcient policy computation for
pressed or implied, of the Defense Advanced Research    multiagent settings. In IJCAI, 2003.
Projects Agency or the U.S. Government.               [Yokoo and Hirayama, 1996] M. Yokoo    and  K. Hi-
                                                        rayama. Distributed breakout algorithm for solving
References                                              distributed constraint satisfaction problems. In IC-
                                                        MAS, 1996.
[Becker et al., 2004] R. Becker, S. Zilberstein, V. Lesser,
  and  C.V. Goldman.    Solving transition indepen-
  dent decentralized Markov decision processes. JAIR,
  22:423–455, 2004.
[Goldman and Zilberstein, 2004] C.V. Goldman   and
  S. Zilberstein. Decentralized control of cooperative
  systems:  Categorization and complexity  analysis.
  JAIR, 22:143–174, 2004.