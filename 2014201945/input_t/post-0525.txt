                                 Active Cost-Sensitive Learning
                                        Dragos D. Margineantu
                                         The Boeing Company
                       Mathematics & Computing Technology, Adaptive Systems
                                       P.O.Box 3707, M/S 7L-66
                                     Seattle, WA 98124-2207, USA
                                   dragos.d.margineantu@boeing.com

                    Abstract                          expected loss of the learned models, and has assumed that
                                                      all (training) instances and their labels were readily available
    For many classiﬁcation tasks a large number of
                                                      prior to the training phase.
    instances available for training are unlabeled and
    the cost associated with the labeling process varies Our approach in addressing the active cost-sensitive learn-
    over the input space. Meanwhile, virtually all these ing problem is based on learning algorithms that construct
                                                      models for conditional density (or class probability) estima-
    problems require classiﬁers that minimize a non-      P  y|x
    uniform loss function associated with the classiﬁca- tion ( ), rather than class label predictors. The proba-
    tion decisions (rather than the accuracy or number bility estimates provide an easy means for factoring in the
    of errors). For example, to train pattern classiﬁca- misclassiﬁcation losses in the classiﬁcation decision making
    tion models for a network intrusion detection task, step. To address the labeling cost problem, we employ the
    experts need to analyze network events and assign same density estimation techniques over the available unla-
    them labels. This can be a very costly procedure if beled data. The two methods are then combined into an algo-
    the instances to be labeled are selected at random. rithm that minimizes the combined cost.
    In the meantime, the loss associated with mislabel- Next section presents the general framework for our pro-
    ing an intrusion is much higher than the loss asso- posed techniques and discusses the challenges in implement-
    ciated with the opposite error (i.e., labeling a legal ing and applying these algorithms as well as some prelimi-
    event as being an intrusion).                     nary experimental results.
    As a result, to address these types of tasks, practi-
    tioners need tools that minimize the total cost com- 2 Active Learning for Minimizing the
    puted as a sum of the cost of labeling and the loss   Combined Costs for Labeling and Decisions
    associated with the decisions.
                                                      Most learning algorithms can be transformed into class prob-
    This paper describes an approach for addressing
                                                      ability estimators that compute estimates P (y|x) for the prob-
    this problem.
                                                      abilities that an instance x is in class y (y ∈{1 ...K}, where
                                                      K  is the number of classes). We employ these estimates in
1  Introduction                                       our algorithm both for minimizing the labeling costs and the
A number of applications require learning algorithms that are misclassiﬁcation decisions. We make the assumption that the
capable (1) to learn from both labeled and unlabeled exam- loss function associated with the decisions is represented as a
ples, (2) to minimize the cost non-uniform associated with static K-by-K loss matrix L available at learning time. The
the labeling efforts, and (3) to minimize the misclassiﬁcation contents of L(i, j) specify the cost incurred when an example
loss.                                                 is predicted to be in class i when in fact it belongs to class j.
  The active learning framework [Cohn et al., 1994] relies on The pseudo code for our approach, ACTIVE-CSL, is pre-
algorithms that select unlabeled instances and provide them sented in Table 1.
to the expert for labeling and learn a classiﬁer based on the The algorithm trains a base learner to compute the class
labeled data. In most of the cases the data is labeled incre- probability estimates over the unlabeled data (line 2). Then,
mentally and the classiﬁcation model tries to minimize the both the sampling step (line 3) and the decision making step
misclassiﬁcation error rate. To our knowledge, virtually all (the hypothesis h) are based on those estimates. The unla-
research efforts in active learning have assumed both a uni- beled instance for which the next label is requested from the
form loss function (for which the goal was to reduce the num- expert is selected in line 3. The selection rule chooses the
ber of misclassiﬁcations) and a uniform labeling cost function instance that, if labeled provides the most expected gain in
(for which the goal was to reduce the number of instances that terms of the total cost (labeling + decision loss) on the la-
have to be labeled to achieve a certain accuracy).    beled instances (or on a hold-out labeled validation set). It
  Meanwhile, research in cost-sensitive learning [Elkan, is important to note that iff C and L functions have a shape
2001] has focused mostly on the problem of minimizing the such that instances that are more expensive to label reduce the                                                      our experiments were generated based on some generic loss
Table 1: Pseudo code for the ACTIVE-CSL active cost-  models, and the labeling cost function mapped instances that
sensitive learning algorithm.                         were closer to the decision boundary to higher labeling costs
Input: a set Sl,ofm labeled examples:                 (the actual function we employed was a bell-shaped function
      Sl =< (xi,yi),i=1, 2,...,m>,                    with a maximum on the decision boundary).
      a set Su of v unlabeled examples:                 We tested the two implementations of ACTIVE-CSL on
      Su =<  (ξi),i=1, 2,...,v >,                     ﬁve data sets from the UC Irvine Repository [Blake and Merz,
      L (a loss matrix), C (a labeling cost function) 1998] (Breast cancer Wisconsin, Horse colic, King-rook vs.
      a stopping criterion                            king-pawn, Liver disease, and Sonar) and on the binary ver-
                                                      sion of the KDD Cup 1998 donations data.
[1] repeat                                              The preliminary results show a minor advantage of the
             j     P  y|ξ       S
[2]   for each learn (  j) using l as training data;  implementation employing conﬁdence-based estimates over
      l          C ξ
[3]    =argmin(   ( )+                                the implementation using averaged probability estimates. We
           ξ∈Su                                                                                        m
        K          K                               have also run tests by employing random forests with log
                                                      attributes tested in a node [Breiman, 2001] as the class prob-
      +    P (k|ξ)      P(ξ,k)(j|xi)L(yi,j));
        k=1        i j=1                              ability estimator and the results show no signiﬁcant differ-
                                                      ence between the classiﬁcation decisions based on random
[4]   ψl = requested label for ξl;
                                                      forests and the decisions based on bagged probability estima-
[5]   remove ξl from Su, add (ξl,ψl) to Sl;
[6] while stopping criterion not met                  tion trees.
                     K
Output: h(x) = argmin   P (j|x)L(y,j)                 References
                y∈Y  j=1                              [Blake and Merz, 1998] C. L. Blake and C. J. Merz. UCI
// the optimal classiﬁcation with respect to L and P    Repository of ML databases, 1998. [www.ics.uci.edu/
                                                        ˜mlearn/MLRepository.html].
                                                      [Breiman, 2001] L. Breiman. Random forests. Technical report,
                                                        Dept. of Statistics, University of California, Berkeley, 2001.
misclassiﬁcation cost more than instances that are less expen-
                                                      [             ]
sive to label - the selection rule will trade the loss associated Cohn et al., 1994 D. A. Cohn, L. Atlas, and R. Ladner. Improved
                                                        generalization with active learning. Machine Learning, 15:201–
with misclassiﬁcations against examples to be labeled. The 221, 1994.
procedure selecting for the next instance to be labeled, as em-
                                                      [Elkan, 2001] C. Elkan. The foundations of cost-sensitive learning.
ployed by ACTIVE-CSL, and based on computing the maxi-
                                                          Proc. of the Seventeenth Intrnl. Joint Conference on Artiﬁcial
mum expected gain upon labeling (the second sum in line 3), In
                                                        Intelligence. Morgan Kaufmann, 2001.
is somewhat similar in nature to the querying rule of uncer-
tainty sampling [Lewis and Catlett, 1994].            [Greiner et al., 2002] R. Greiner, A. J. Grove, and D. Roth. Learn-
                                                        ing cost-sensitive active classiﬁers. Artiﬁcial Intelligence. 139:2,
  One important detail that needs to be addressed when ap- pages 137–174, 2002.
plying this algorithm in practice is the choice for the algo-
                                                      [Hettich and Bay, 1999] S. Hettich and S. D. Bay. The UCI KDD
rithm used for estimating the probabilities (line 3 in Table 1), archive, 1999. [http://kdd.ics.uci.edu/].
given that ACTIVE-CSL and its predictions rely on these es-
                                                      [                  ]
timates. In general, the quality of learned density estimates is Lewis and Catlett, 1994 D. Lewis and J. Catlett. Heterogeneous
                                                        uncertainty sampling. In Proc. Eleventh Intrnl. Conference on
dependent on the amount and the distribution of the labeled Machine Learning, pages 148–156. Morgan Kaufmann, 1994.
data that is available and on the hypothesis constructed by the
base learning algorithm. Several research studies have ad- [Margineantu and Dietterich, 2000] D. D. Margineantu and T. G.
dressed the problem of learning good probability estimates Dietterich. Bootstrap methods for the cost-sensitive evaluation
                                                        of classiﬁers. In Proc. of the Seventeenth Intrnl. Conference on
and calibrating classiﬁcation scores and ranks into accurate Machine Learning, pages 583–590. Morgan Kaufmann, 2000.
probabilities [Zadrozny and Elkan, 2001].
                                                      [Margineantu, 2002] D. D. Margineantu. Class probability estima-
  We have implemented and tested ACTIVE-CSL by using    tion and cost-sensitive classiﬁcation decisions. In Proc. ECML-
bagged probability estimation trees [Provost and Domingos, 2002, 13th European Conference on Machine Learning, Proceed-
2003] as class probability estimators (in line 2 of the code). ings, pages 270–281. LNAI 2430, Springer Verlag, 2002.
We have compared two procedures for computing probabil- [Provost and Domingos, 2003] F. Provost and P. Domingos. Tree
ity estimates out of the bagged trees: (a) by averaging the induction for probability-based rankings. Machine Learning,
probabilities computed by the individual trees and (b) by es- 52:3, 2003.
timating the conﬁdence for the individual probabilities based [Saar-Tsechansky and Provost, 2001] M. Saar-Tsechansky and
on the distribution of the estimates of the individual trees (as F. Provost. Active learning for class probability estimation and
suggested by [Margineantu, 2002]). For assessing the quality ranking. In Proc. Seventeenth Intrnl. Joint Conf. on Artiﬁcial
of the misclassiﬁcation decisions we have employed active Intelligence, pages 911–917. AAAI Press/MIT Press, 2001.
learning curves (showing the gain in terms of cost compared [Zadrozny and Elkan, 2001] B. Zadrozny and C. Elkan. Obtain-
to a random selection of instances for labeling) and BDELTA- ing calibrated probability estimates from decision trees and naive
COST [Margineantu and Dietterich, 2000] - a paired test for bayesian classiﬁers. In Proc. of the Eighteenth Intrnl. Conference
cost-sensitive classiﬁcation decisions. The loss matrices for on Machine Learning, pages 609–616. Morgan Kaufmann, 2001.