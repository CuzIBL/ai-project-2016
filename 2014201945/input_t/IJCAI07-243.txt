                      Emergence of Norms Through Social Learning

                                   Sandip Sen   and  Stephane´ Airiau
                                        University of Tulsa, USA
                                     {sandip, stephane}@utulsa.edu


                    Abstract                          social milieu and play a pivotal role in all kinds of business,
                                                      political, social, and personal choices and interactions. They
    Behavioral norms are key ingredients that allow   are self-enforcing: “A norm exists in a given social setting to
    agent coordination where societal laws do not suf- the extent that individuals usually act in a certain way and are
    ﬁciently constrain agent behaviors. Whereas social often punished when seen not to be acting in this way” [Ax-
    laws need to be enforced in a top-down manner,    elrod, 1997].
    norms evolve in a bottom-up manner and are typ-     While these aspects of norms or conventions have merited
    ically more self-enforcing. While effective norms in-depth study of the evolution and economics of norms in
    can signiﬁcantly enhance performance of individ-  social situations [Epstein, 2001; Posch, 1995; Young, 1993;
    ual agents and agent societies, there has been lit- 1996], we are particularly interested in the following charac-
    tle work in multiagent systems on the formation of terization: “... we may deﬁne a convention as an equilibrium
    social norms. We propose a model that supports    that everyone expects in interactions that have more than one
    the emergence of social norms via learning from   equilibrium.” [Young, 1996]. This observation has particular
    interaction experiences. In our model, individual signiﬁcance for the study of norms2 in the context of com-
    agents repeatedly interact with other agents in the putational agents. Computational agents often have to coor-
    society over instances of a given scenario. Each  dinate their actions and such interactions can be formulated
    interaction is framed as a stage game. An agent   as stage games with simultaneous moves made by the play-
    learns its policy to play the game over repeated  ers [Genesereth et al., 1986]. Such stage games often have
    interactions with multiple agents. We term this   multiple equilibria [Myerson, 1991], which makes coordina-
    mode of learning social learning, which is distinct tion uncertain. While focal points [Schelling, 1960] can be
    from an agent learning from repeated interactions used to disambiguate such choices, they may not be available
    against the same player. We are particularly inter- in all situations. Norms can also be thought of as focal points
    ested in situations where multiple action combina- evolved through learning [Young, 1996]. Hence, the emer-
    tions yield the same optimal payoff. The key re-  gence of norms via learning in agent societies promises to be
    search question is to ﬁnd out if the entire population a productive research area that can improve coordination in
    learns to converge to a consistent norm. In addition and hence functioning of agent societies.
    to studying such emergence of social norms among
                                                        While researchers have studied the emergence of norms in
    homogeneous learners via social learning, we study
    the effects of heterogeneous learners, population agent populations, they typically assume access to signiﬁcant
                                                                                [
    size, multiple social groups, etc.                amount of global knowledge Epstein, 2001; Posch, 1995;
                                                      Young, 1993; 1996]. For example, all of these models assume
                                                      that individual agents can observe sizable fraction of interac-
1  Introduction                                       tions between other agents in the environment. While these
                                                      results do provide key insights into the emergence of norms
Norms or conventions routinely guide the choice of behav-
                                                      in societies where the assumption of observability holds, it is
iors in human societies. Conformity to norms reduces social
                                                      unclear if and how norms will emerge if all interactions were
frictions, relieves cognitive load on humans, and facilitates
                                                      private, i.e., not observable to any other agent not involved in
coordination. “Everyone conforms, everyone expects others
                                                      the interaction.
to conform, and everyone has good reason to conform be-
cause conforming is in each person’s best interest when ev- To study the important phenomenon of emergence of social
eryone else plans to conform” [Lewis, 1969]1. Conventions norms via private interactions, we use the following interac-
in human societies range from fashions to tipping, driving tion framework. We consider a population of agents, where,
etiquette to interaction protocols. Norms are ingrained in our in each interaction, each agent is paired with another agent

  1Conventions can therefore be substituted as external correlating 2Henceforth we use the term norm to refer to social norms and
signals to promote coordination.                      conventions.

                                                IJCAI-07
                                                  1507randomly selected from the population. Each agent then depends on the joint action of all the agents in the popula-
is learning concurrently over repeated interactions with ran- tion [Tumer and Wolpert, 2000]. The goal of the learning
domly selected members from the population. We refer to this agent is to maximize an objective function for the entire pop-
kind of learning social learning to distinguish from learning ulation, the world utility.
in iterated games against the same opponent [Fudenberg and The social learning framework we use to study norm emer-
Levine, 1998]. Most of our experiments involve symmetrical gence in a population is somewhat different from both of
games with multiple pure-strategy equilibria with the same these lines of research. We are considering a potentially large
payoff. In previous work on learning in games, the opponent population of learning agents. At each time step, however,
is ﬁxed but in our work, the opponent is different at each itera- each agent interacts with a single agent, chosen at random,
tion. In addition, the opponent may not use the same learning from the population. The payoff received by an agent for a
algorithm. It is unclear, apriori, if and how a social norm will time step depends only on this interaction as is the case when
emerge from such a social learning framework. Our experi- two agents are learning to play a game. In the two-agent case,
mental results and concomitant analysis throws light on the a learner can adapt and respond to the opponent’s policy. In
dynamics of the emergence of norm via social learning with our framework, however, the opponent changes at each inter-
private interactions. We also investigate a number of key re- action. It is not clear aprioriif the learners will converge to
lated issues: the effect of population size, number of choices useful policies in this situation.
available, multiple populations with limited inter-population
interactions, heterogeneous population with multiple learning
algorithms, effect of non-learners in shaping norm adoption,
norms for social dilemmas, etc.                       3   Social Learning Framework

2  Related work                                       The speciﬁc social learning situation for norm evolution that
                                                      we consider is that of learning “rules of the road”. In partic-
The need for effective norms to control agent behaviors is ular, we will consider the problem of which side of the road
well-recognized in multiagent societies [Boella and van der to drive in and who yields if two drivers arrive at an inter-
Torre, 2003; V´azquez-Salceda et al., 2005]. In particular, action at the same time from neighboring roads 3. We will
norms are key to the efﬁcient functioning of electronic in- represent each interaction between two drivers as a n-person,
stitutions [Garcia-Camino et al., 2006]. Most of the work m-action stage game. These stage games typically have mul-
in multiagent systems on norms, however, has centered on tiple pure strategy equilibria. In each time period each agent
logic or rule-based speciﬁcation and enforcement of norms is paired with a randomly selected agent from the population
[Dignum et al., 2002; V´azquez-Salceda et al., 2005]. Simi- to interact. An agent is randomly assigned to be the row or
lar to these research, the work on normative, game-theoretic column player in any interaction. We assume that the stage
approach to norm derivation and enforcement also assumes game payoff matrix is know to both players, but agents cannot
centralized authority and knowledge, as well as system level distinguish between other players in the population. Hence,
goals [Boella and Lesmo, 2002; Boella and van der Torre, each agent can only develop a single pair of policies, one as
2003]. While norms can be established by centralized dictat, a row player and the other as a column player, to play against
a number of real-life norms evolve in a bottom-up manner, any other player from the agent population. The learning al-
via “the gradual accretion of precedent” [Young, 1996].We gorithm used by an agent is ﬁxed, i.e. an intrinsic property of
ﬁnd very little work in multiagent systems on the distributed an agent.
emergence of social norms. We believe that this is an impor- When two cars arrive at an intersection, a driver will some-
tant niche research area and that effective techniques for dis- times have another car on its left and sometimes on its right.
tributed norm emergence based on local interactions and util- These two experiences can be mapped to two different roles
ities can bolster the performance of open multiagent systems. an agent can assume in this social dilemma scenario and cor-
We focus on the importance for electronic agents solving a responds to an agent playing as the row and column player
social dilemma efﬁciently by quickly adopting a norm. Cen- respectively. Consequently, an agent has a private bimatrix:
tralized social laws and norms are not sufﬁcient, in general, a matrix when it is the row player, one matrix when it is the
to resolve all agent conﬂicts and ensure smooth coordination. column player. Each agent has a learning algorithm to play as
The gradual emergence of norms from individual learning can a row player and as a column player and learns independently
facilitate coordination in such situations and make individuals to play as a row and a column player. An agent does not
and societies more efﬁcient.                          know the identity of its opponent, nor its opponent’s payoff,
  In our formulation, norms evolve as agents learn from their but it can observe the action taken by the opponent (perfect
interactions with other agents in the society using multiagent but incomplete information). The protocol of interaction is
reinforcement learning algorithms [Panait and Luke, 2005; presented in Algorithm 1.
Tuyls and Now´e, 2006]. Most multiagent reinforcement
learning literature involve two agents iteratively playing a
stage game and the goal is to learn policies to reach preferred 3It might seem to the modern reader that “rules of the road” are
equilibrium [Powers and Shoham, 2005]. Another line of re- always ﬁxed by authority, but historical records show that “Society
search considers a large population of agents learning to play often converges on a convention ﬁrst by an informal process of ac-
a cooperative game where the reward of each individual agent cretion; later it is codiﬁed into law.” [Young, 1996].

                                                IJCAI-07
                                                  1508  for a ﬁxed number of epoch do
                                                                 G     YL                0      1
     repeat                                                G   -1, -1 3, 2          0   4, 4   -1, -1
        remove randomly agents prow and pcol from the
                                                          YR    2, 3  1, 1          1   -1, -1 4, 4
        population ask each agent to select an action;
                           p        p
        send the joint action to row and col for policy   (a) social dilemma game   (b) coordination game
        update;
     until all agents have been selected during the epoch ;
         Algorithm 1: Interaction protocol.            Table 1: Stage games corresponding to social interactions.
  We use three different learning algorithms for learning
norms: Q-Learning [Watkins and Dayan, 1992] with -greedy and the other player learns to yield in both situations. Al-
exploration, WoLF-PHC [Bowling and Veloso, 2002] and  though not “fair”, this situation is possible in our framework
Fictitious Play (FP). Q-learning has been widely used in mul- since each agent independently learns to play as a row and a
tiagent systems, but converges only to pure strategies. WoLF- column player.
PHC (Win or Learn Fast - policy hill climbing) can learn When a third agent is introduced, as the agents do not know
mixed strategies. Though WoLF is guaranteed to converge the identity of the opponents, no agent can any longer beneﬁt
to a Nash equilibrium of the repeated game in a 2-person, 2- from always choosing “go”. This is because all other agents
actions game against a given opponent, it is not clear whether must always “yield” to the “go” agent, and then those agents
it is guaranteed to converge in social learning. Finally, FP is will receive relatively poor utility when playing each other.
the basic learning approach widely studied in the game theory As a result, they will also learn to “go”. To optimize perfor-
literature [Fudenberg and Levine, 1998].AnFPplayeruses mance they will have to learn to settle to a norm which every-
the historical frequency count of its opponent’s past actions one else also follows. Though we only hoped that this would
and tries to maximizing expected payoff by playing a best re- happen via social learning in a large population, our experi-
sponse to that mixed strategy, represented by this frequency mental results show that a uniform norm always emerges in
distribution.                                         a population of three or more agents. For example, in a pop-
                                                      ulation of 200 agents using WoLF, we ran 1, 000 runs, and
4  Results                                            we observed that the population converged to the “yield to
                                                      the left” norm 482 times, and “yield to the right” norm 518
4.1  Example of a social dilemma                      times. We present the averaged dynamics of the payoffs and
One typical example of the use of norms or convention is to the frequency of the joint action during learning in Figure 1.
resolve social dilemmas. A straightforward example of this From the dynamics we can see that at ﬁrst the agents avoid
is when two drivers arrive at an intersection simultaneously the collision and prefer to yield. Then, one agent notice that
from neighboring streets. While each player has the incentive it can exploit this situation by choosing to “go” as the other
of not yielding, myopic decisions by both can lead to unde- one is yielding. Depending on who notices this ﬁrst, the pop-
sirable accidents. Both drivers yielding, however, also cre- ulation converges to one norm or the other. Note that the
ates inefﬁciency. Ideally, we would like norms like “yield to plot in Figure 1 is averaged over all the runs, which explains
the driver on right”, which serves all drivers in the long run. why the (G, YL) and (YR,G) appear almost 50% of the time.
Hence, the dilemma is resolved if each member of the popu- The presence of the other joint-actions is due to exploration.
lation learns to “yield” as a row (column) player and “go” as a These results conﬁrm that only private experience is sufﬁcient
column (row) player. The player that yields gets a lesser pay- for the emergence of a norm in a society of learning agents.
off since it is losing some time compared to the other player. This is in contrast with prior work on norm evolution which
The players know whether they are playing as a row or a col- requires agents to have knowledge about non-local interac-
umn player: the row player sees a car on its right, and the tions between other agents and their strategies [Epstein, 2001;
column player sees a car on its left. The action choices for Posch, 1995; Young, 1993].
the row player are to go (G) or yield to the car on the right
(YL),andtheyarego(G) or yield to the car on the left (YL) 4.2 Inﬂuence of population size, number of actions
for the column player. We model this game using the payoffs and learning algorithm
presented in Table 1(a). Note that for a social norm to evolve, The time required for the emergence of a norm in a society of
all agents in the population has to learn any one of the follow- interacting agents, measured by the number of interaction pe-
ing policy pairs: (a) (row:G,col:YL), i.e., yield to the car on riods before most agents adopt the norm, depends on several
the left , or (b) (row:YR,col:G), i.e., yield to the car on the factors. Here we study the inﬂuence of the size of the popula-
right. We say a norm has emerged in the population when all tion, the learning algorithm used, and the number of actions
learners make the corresponding choice except for infrequent available to the agents.
random exploration.                                     First we consider the effect of population size. With a
  We ﬁrst note that in iterated play between two players, i.e., larger population, the likelihood that two particular agents in-
if the population consisted of only two agents, other policy teract decreases. Hence the variety of opponents as well as
combinations may also emerge. For example, in addition to the diversity of personal interaction history increases with the
the above two possible norms, it can be the case that one of population size and the population takes more time to evolve
the two learners learn to “go” both as row and column player a norm. In Figure 2, we present the dynamics of the aver-

                                                IJCAI-07
                                                  1509                                                                                       n
   Dynamics of the payoff for the learners Frequence of the joint action driving scenarios, can be expanded to -actions: the agents
     1.45                      0.5                    receive a payoff of 4 when they choose the same action and a
                                                              −1
      1.4                     0.45                    payoff of   when their actions differ. In Figure 3, we show
                                                      the dynamics of the probability of an agent choosing action 1
     1.35                      0.4                    for each learning algorithm in a population of 20 agents for
      1.3                     0.35                    n =2. Then, we ran this experiment with n ∈{2, 3, 4} in
     1.25                      0.3        G,G         a population of 200 agents using WoLF. The results are pre-
                                         G,YL
      1.2                     0.25       YR,G         sented in Figure 4. When the number of actions increase, the
                                        YR,YL         proportion of joint actions with high payoff decreases. When
     1.15                  frequence  0.2


  average  payoffs                                    the agents explore at the beginning, the expected utility is less
      1.1                     0.15
                                                      with a larger game. Over time a norm emerges, with the av-
     1.05                      0.1                    erage payoff of the population approaching 4. It takes longer
       1 payoff row           0.05                    to evolve norms for larger action sets as the space of joint
          payoff col
     0.95                       0                     actions increases quadratically.
        0  400  800  1200        0   400  800  1200
            iterations                iterations

Figure 1: Social dilemma game with 200 agents using WoLF,     40
averaged over 1,000 runs. The population converges 482
times to (G, YL) and 518 times to (YR,G).                     35

                                                              30
age agent reward for the social dilemma game in a population
of agents using WoLF with different population sizes: with    25
more agents, it takes longer for the entire population to con-
                                                              20
verge on a particular norm. It is well-known that tight-knit,
small societies, groups, clans develop eclectic norms that are 15

often not found in larger, open societies.                    policy of the learners
                                                              10

  Influence of the population size (average over 100 runs) with agents using WoLF 5
     2.4
                                                                    100  200  300  400   500  600  700
                                                                           number of iterations
     2.2

      2                                               Figure 3: Dynamics of the probability to play action 0 (each
                                                      agent is represented by two lines: policy to play as a row
     1.8                            2 agents          and as a column player; darker color represents probability
                                   10 agents
                                   20 agents          closer to 1): all agents converge to a probability close to 0,
     1.6                           50 agents          i.e., chooses action 1.
                                   100 agents
                                   150 agents

  average  payoff of a learner     200 agents
     1.4                           300 agents           Finally, we consider the effect of the learning algorithm
                                   400 agents         used by the agents. Since there is no clear choice of learn-
                                   500 agents
     1.2
       0    200   400  600  800   1000  1200  1400
                     number of iterations                   Dynamics of the payoff of 200 WoLF learners with different size of the game
                                                              4
Figure 2: Dynamics of the average payoff of learners us-
                                                             3.5
ing WoLF with different population sizes (average over 100
runs).                                                        3
                                                             2.5
  Next, we consider the effect of the number of actions avail-
                                                              2
able to each agent. For the rest of paper, we use the co-
                                                             1.5


ordination game presented in Table 1(b). This stage game  average  payoffs
models the situation where agents need to agree on one of     1
several equally desirable alternatives. For example, for the                                2x2
                                                             0.5                            3x3
two-action case, this game can represent the situation where                                4x4
                                                              0
agents choose on what side of the road to drive. When both      0    500   1000  1500  2000  2500  3000
agents drive on their left, or on their right, there is no col-                iterations
lision, else there is a penalty. The societal norms that we
would want to evolve are either driving on the left or driv- Figure 4: Dynamics of the payoff of learners using WoLF
ing on the right. The stylized game, representing other, non- with different game sizes (average over 100 runs).

                                                IJCAI-07
                                                  1510ing algorithms to use in general, we wanted to evaluate a few              Effect of fixed agents
representative learning algorithms. We study the inﬂuence of  1
                                                                  converged to (0,0)
the learning algorithms on a population of 200 agents play-  0.9  converged to (1,1)
ing the two-action game. When the entire population uses the
same learning algorithm, the population of Q-Learners are the  0.8
quickest to evolve a norm (≈ 100 iterations), followed by a  0.7
population of WoLF (≈ 1000 iterations), and the population  0.6
of agents using FP (≈ 40, 000 iterations). The payoff reached  0.5
at convergence is different for different algorithms due to dif-  0.4
ferent exploration schemes. We also show results of hybrid
                                                           0.3
population using equal proportions of any two or all three of
these algorithms. The time taken by mixed groups to evolve  0.2
norms are in between the time taken by the corresponding   0.1
homogeneous groups.                                     percentage  of time it converged to a norm  0
                                                              0       1       2       3        4       5
                                                                  number of additional agents playing fixed strategy 1
        Dynamics of the average payoff in a population of 200 agents 
                  using different learning algorithms Figure 6: Number of times each norm emerges (average over
      4                                               100 runs): a small imbalance in the number of agents using a
                                                      pure strategy is enough to inﬂuence an entire population
     3.5

      3                                               might therefore be some truth to the adage that most fashion
                                                      trends are decided by a handful of trend setters in Paris!
     2.5
                                       FP
                                       QL             4.4  Emergence of norms in isolated

  average  payoffs  2                WoLF
                                    FP+QL                  subpopulations
     1.5                          FP+WoLF             It is well-documented in societies that isolated populations
                                  QL+WoLF
                                QL+WoLF+FP            can be using contradictory norms, e.g., driving on the “right”
      1
             100        1000       10000      100000  or the “wrong” side of the road. We wanted to replicate this
                         iterations                   phenomenon using our social learning framework. When two
                                                      groups of agents interact only infrequently, it is possible that
Figure 5: Dynamics of the payoff of learners using different a different norm emerges in each group. In particular, we are
learning algorithms (population of 200 agents, average over interested in studying the degree of isolation required for di-
100 runs).                                            vergent norms to emerge in different groups. For our experi-
                                                      ments, we consider two groups of equal size and a probability
                                                      p that agents of different groups interact.
4.3  Inﬂuence of ﬁxed agents                            Results from this set of experiments are presented in Fig-
So far, we have observed that all norms with equal payoffs ure 7. We observe that when the probability of interaction is
were evolved roughly with the same frequency over multi- at least 0.3, a single norm pervades the entire population. In
ple runs. This is understandable because the payoff matrix roughly half of the runs all agents learn to drive on the left
in Table 1(b) does not support any preference for one norm and for the other half they learn to drive on the right. But for
over the other. Extraneous effects, however, can bias a soci- interaction probabilities of 0.2 and less there are runs where
ety of learners towards a particular norm. For example, some divergent norms emerge in the two groups (corresponding to
agents may not have learning capabilities and repeat a pre- the white space above the shaded bars in Figure 7). This is a
determined action. We study the inﬂuence of agents playing very interesting observation and we are surprised by the rel-
a ﬁxed pure strategy on the emergence of a norm. For this atively high interaction probabilities that could still sustain
study, we use the coordination game of Table 1(b) and con- divergent norms.
sider a population with 3, 000 learners, nf =30agents play-
ing the ﬁxed strategy 0 (driving on the left), and nf agents
playing strategy 1 (driving on the right). We ran experiments 5 Conclusions
where we add additional agents playing the pure strategy 1. We investigated a bottom-up process for the evolution of
Figure 6 presents the percentage of time that the norm (0, 0), social norm that depends exclusively on individual experi-
i.e., everyone driving on the left, and (1, 1), i.e., everyone ences rather than observations or hearsay. Our proposed so-
driving on the right, emerges. Note that when there are equal cial learning framework requires each agent to learn from re-
number of agents playing ﬁxed strategy 0 and ﬁxed strategy 1, peated interaction with anonymous members of the society.
one of the two norms emerges with almost equal frequency. It The goal of this work was to evaluate whether such social
is interesting to note that with only 4 additional agents choos- learning can successfully evolve and sustain a useful social
ing to drive on the right, the entire population of 3000 agents norm that resolves conﬂicts and facilitates coordination be-
almost always converges to driving on the right! There just tween population members. Our experimental results conﬁrm

                                                IJCAI-07
                                                  1511