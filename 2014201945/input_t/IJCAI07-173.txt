                 Direct Code Access in Self-Organizing Neural Networks
                                  for Reinforcement Learning
                                             Ah-Hwee Tan
                                   Nanyang Technological University
                    School of Computer Engineering and Intelligent Systems Centre
                                  Nanyang Avenue, Singapore 639798
                                          asahtan@ntu.edu.sg

                    Abstract                          function from the state space (action policy). MLP and BP
                                                      however are not designed for online incremental learning as
    TD-FALCON is a self-organizing neural network     they typically require an iterative learning process. In addi-
    that incorporates Temporal Difference (TD) meth-  tion, there is an issue of instability in the sense that learning
    ods for reinforcement learning. Despite the advan- of new patterns may erode the previously learned knowledge.
    tages of fast and stable learning, TD-FALCON still Consequently, the resultant reinforcement learning systems
    relies on an iterative process to evaluate each avail- may not be able to learn and operate in real time.
    able action in a decision cycle. To remove this deﬁ-
    ciency, this paper presents a direct code access pro- Instead of learning value functions and action policies,
    cedure whereby TD-FALCON conducts instanta-       self-organizing neural networks, such as Self-Organizing
    neous searches for cognitive nodes that match with Map (SOM), are typically used for the representation and
    the current states and at the same time provide max- generalization of continuous state and action spaces [Smith,
    imal reward values. Our comparative experiments   2002]. The state and action clusters are then used as the en-
    show that TD-FALCON with direct code access       tries in a Q-value table implemented separately. Using a lo-
    produces comparable performance with the origi-   calized representation, SOM has the advantage of more stable
    nal TD-FALCON while improving signiﬁcantly in     learning, compared with backpropagation networks. How-
    computation efﬁciency and network complexity.     ever, SOM remains an iterative learning system, requiring
                                                      many rounds to learn the compressed representation of the
                                                      state and action patterns. In addition, as mentioned in Smith
1  Introduction                                       (2002), SOM is expected to scale badly if the dimensions of
Reinforcement learning [Sutton and Barto, 1998] is an inter- the state and actions spaces are signiﬁcantly higher than the
action based paradigm wherein an autonomous agent learns dimension of the map. As such, most applications of SOM
to adjust its behaviour according to feedback from the en- are limited to low dimensional state and action spaces.
vironment. Classical solutions to the reinforcement learning A recent approach to reinforcement learning builds upon
problem generally involve learning one or more of the follow- Adaptive Resonance Theory (ART) [Carpenter and Gross-
ing mappings, the ﬁrst linking a given state to a desired ac- berg, 1987], also a class of self-organizing neural networks,
tion (action policy), and the second associating a pair of state but with very distinct characteristics from SOM. Ueda et. al
and action to a utility value (value function), using temporal (2005) use ART models to learn the clusters of state and ac-
difference methods, such as SARSA [Rummery and Niran- tion patterns. The clusters are in turns used as the compressed
jan, 1994] and Q-learning [Watkins and Dayan, 1992].The states and actions by a Q-learning module. Ninomiya (2002)
problem of the original formulation is that mappings must be couples a supervised ART system with a Temporal Differ-
learned for each and every possible state or each and every ence reinforcement learning module in a hybrid architecture.
possible pair of state and action. This causes a scalability is- Whereas the states and actions in the reinforcement mod-
sue for continuous and/or very large state and action spaces. ule are exported from the supervised ART system, the two
  Neural networks and reinforcement learning have had an learning systems operate independently. The redundancy in
intertwining relationship [Kaelbling et al., 1996]. In particu- representation unfortunately leads to instability and unnec-
lar, multi-layer feed-forward neural networks, also known as essarily long processing time in action selection as well as
multi-layer perceptron (MLP), have been used extensively in learning of value functions. Through a generalization of ART
many reinforcement learning system and applications [Ack- from one input pattern ﬁeld to multiple pattern channels, Tan
ley and Littman, 1990; Sutton, 1984]. Under the recent thread (2004) presents a neural architecture called FALCON (Fusion
of research in Approximate Dynamic Programming (ADP)  Architecture for Learning, COgnition, and Navigation), that
[Si et al., 2004], MLP and gradient descent backpropagation learns multi-channel mappings simultaneously across multi-
(BP) learning algorithms are commonly used to learn an ap- modal input patterns, involving states, actions, and rewards,
proximation of the value function from the state and action in an online and incremental manner. For handling problems
spaces (value policy) and/or an approximation of the action with delayed evaluative feedback (reward signal), a variant

                                                IJCAI-07
                                                  1071of FALCON, known as TD-FALCON   [Tan and Xiao, 2005], (a1,a2,...,am) denote the action vector, where ai ∈ [0, 1]
learns the value functions of the state-action space estimated indicates a possible action i.LetR =(r, r¯) denote the re-
through Temporal Difference (TD) algorithms. Compared ward vector, where r ∈ [0, 1] is the reward signal value and
with other ART-based systems described by Ueda et. al and r¯ (the complement of r)isgivenbyr¯ =1− r. Comple-
Ninomiya, TD-FALCON presents a truly integrated solution ment coding serves to normalize the magnitude of the input
in the sense that there is no implementation of a separate re- vectors and has been found effective in ART systems in pre-
inforcement learning module or Q-value table.         venting the code proliferation problem. As all input values of
  Although TD-FALCON provides a promising approach,   FALCON are assumed to be bounded between 0 and 1, nor-
its action selection procedure contains an inherent limitation. malization is necessary if the original values are not in the
Speciﬁcally, TD-FALCON selects an action by weighting the range of [0, 1].
                                                                          ck           ck
consequence of performing each and every possible action Activity vectors: Let x denote the F1 activity vector for
                                                                      c            c
in a given state. Besides that the action selection process k =1,...,3.Lety denote the F2 activity vector.
                                                                           ck
is inefﬁcient with a large number of actions, the numerat- Weight vectors: Let wj denote the weight vector associ-
                                                                              c
ing step also assumes a ﬁnite set of actions, rendering it in- ated with the jth node in F2 for learning the input patterns
                                                          ck                         c
applicable to continuous action space. In view of this de- in F1 for k =1,...,3. Initially, F2 contains only one un-
ﬁciency, this paper presents a direct code access procedure committed node and its weight vectors contain all 1’s. When
by which TD-FALCON can perform instantaneous searches an uncommitted node is selected to learn an association, it
for cognitive nodes that match with the current states and becomes committed.
at the same time provide the highest reward values. Be- Parameters: The FALCON’s dynamics is determined by
sides that the algorithm is much more natural and efﬁcient, choice parameters αck > 0 for k =1,...,3; learning rate
                                                                βck ∈ [0, 1]  k  =1,...,3
TD-FALCON can now operate with both continuous state  parameters           for            ; contribution pa-
                                                               ck                             3    ck
and action spaces. Our comparative experiments based on a rameters γ ∈ [0, 1] for k =1,...,3 where k=1 γ =1;
mineﬁeld navigation task show that TD-FALCON with direct and vigilance parameters ρck ∈ [0, 1] for k =1,...,3.
code access produces comparable performance with the orig- Code activation: A bottom-up propagation process ﬁrst
inal TD-FALCON system while improving vastly in terms of takes place in which the activities (known as choice function
                                                                                       c
computation efﬁciency as well as network complexity.  values) of the cognitive nodes in the F2 ﬁeld are computed.
  The rest of the paper is organized as follows. For complete- Speciﬁcally, given the activity vectors xc1, xc2 and xc3 (in
                                                                     c1  cs      c3                     c
ness, section 2 presents a summary of the FALCON architec- the input ﬁelds F1 , F1 and F1 respectively), for each F2
                                                                               c
ture and the associated learning and prediction algorithms. node j, the choice function Tj is computed as follows:
Section 3 presents the new TD-FALCON algorithm with the
                                                                          3
direct code access procedure. Section 4 introduces the mine-                   |xck ∧ wck|
                                                                    T c =   γck        j  ,
ﬁeld navigation simulation task and presents the experimental        j          αck + |wck|           (1)
results. The ﬁnal section concludes and provides a brief dis-            k=1            j
cussion of future work.                               where the fuzzy AND operation ∧ is deﬁned by (p∧ q)i ≡
                                                      min(pi,qi), and the norm |.| is deﬁned by |p|≡ i pi for
2  FALCON Dynamics                                    vectors p and q. In essence, the choice function Tj com-
                                                      putes the similarity of the activity vectors with their respec-
FALCON employs a 3-channel architecture (Figure 1), com-                     c
                      c                               tive weight vectors of the F2 node j with respect to the norm
prising a cognitive ﬁeld F2 and three input ﬁelds, namely a
             c1                                       of individual weight vectors.
sensory ﬁeld F1 for representing current states, an action
      c2                                      c3      Code competition: A code competition process follows un-
ﬁeld F1 for representing actions, and a reward ﬁeld F1 for         c
                                                      der which the F2 node with the highest choice function value
representing reinforcement values. The generic network dy- is identiﬁed. The winner is indexed at J where
namics of FALCON, based on fuzzy ART operations [Car-
                                                                 T c =max{T c :     F c     j}.
penter et al., 1991], is described below.                         J         j  for all 2 node         (2)
                                                                                           c          c
                                                      When a category choice is made at node J, yJ =1;andyj =
                                                      0 for all j = J. This indicates a winner-take-all strategy.
                                                      Template matching: Before code J can be used for learning,
                                                      a template matching process checks that the weight templates
                                                      of code J are sufﬁciently close to their respective activity pat-
                                                      terns. Speciﬁcally, resonance occurs if for each channel k,the
                                                                     ck
                                                      match function mJ of the chosen code J meets its vigilance
                                                      criterion:
                                                                           |xck ∧ wck|
                                                                    mck =         J  ≥  ρck.
                                                                      J      |xck|                    (3)
                                                      The match function computes the similarity of the activity
          Figure 1: The FALCON architecture.          and weight vectors with respect to the norm of the activity
                                                      vectors. Together, the choice and match functions work co-
Input vectors: Let S =(s1,s2,...,sn) denote the state vec- operatively to achieve stable coding and maximize code com-
tor, where si ∈ [0, 1] indicates the sensory input i.LetA = pression.

                                                IJCAI-07
                                                  1072When resonance occurs, learning ensues, as deﬁned below. 3.1 Action Selection Policy
If any of the vigilance constraints is violated, mismatch re- The simplest action selection policy is to pick the action with
                                              T c
set occurs in which the value of the choice function J is the highest value predicted by the TD-FALCON network.
set to 0 for the duration of the input presentation. With a However, a key requirement of autonomous agents is to ex-
match tracking process, at the beginning of each input pre- plore the environment. This is especially important for an
                             ρc1
sentation, the vigilance parameter equals a baseline vig- agent to function in situations without immediate evaluative
     ρ¯c1                        ρc1
ilance  . If a mismatch reset occurs, is increased until feedback. If an agent keeps selecting the optimal action that
                                    mc1
it is slightly larger than the match function J . The search it believes, it will not be able to explore and discover better
                       F c     J
process then selects another 2 node under the revised vig- alternative actions. There is thus a fundamental tradeoff be-
ilance criterion until a resonance is achieved. This search and tween exploitation, i.e., sticking to the best actions believed,
test process is guaranteed to end as FALCON will either ﬁnd and exploration, i.e., trying out other seemingly inferior and
a committed node that satisﬁes the vigilance criterion or acti- less familiar actions.
vate an uncommitted node which would deﬁnitely satisfy the The -greedy policy selects the action with the highest
                                   1s
criterion due to its initial weight values of .       value with a probability of 1 −  and takes a random ac-
Template learning: Once a node J is selected, for each chan- tion with probability  [P´erez-Uribe, 2002]. With a constant
   k                 wck
nel , the weight vector J is modiﬁed by the following  value, the agent will always explore the environment with
learning rule:                                        a ﬁxed level of randomness. In practice, it may be beneﬁ-
                                                                        
 wck(new) =(1−  βck)wck(old) + βck(xck ∧ wck(old)).   cial to have a higher value to encourage the exploration of
   J                 J                  J       (4)   paths in the initial stage and a lower  value to optimize the
The learning rule adjusts the weight values towards the fuzzy performance by exploiting familiar paths in the later stage. A
                                                           
AND of their original values and the respective weight val- decay -greedy policy is thus adopted to gradually reduce the
                                                             
ues. The rationale is to learn by encoding the common at- value of over time. The rate of decay is typically inversely
tribute values of the input vectors and the weight vectors. For proportional to the complexity of the environment as a more
an uncommitted node J, the learning rates βck are typically complex environment with larger state and action spaces will
set to 1. For committed nodes, βck can remain as 1 for fast take a longer time to explore.
               1
learning or below for slow learning in a noisy environment. 3.2 Direct Code Access
When an uncommitted node is selecting for learning, it be-
comes committed and a new uncommitted node is added to In an exploiting mode, an agent, to the best of its knowledge,
    c
the F2 ﬁeld. FALCON thus expands its network architecture selects an action with the maximal reward value given the cur-
dynamically in response to the input patterns.        rent situation (state). Through a direct code access procedure,
                                                      TD-FALCON searches for the cognitive node which matches
                                                      with the current state and has the maximal reward value. For
3TD-FALCON                                            direct code access, the activity vectors xc1, xc2,andxc3 are
TD-FALCON incorporates Temporal Difference (TD) meth- initialized by xc1 = S, xc2 =(1,...,1),andxc3 =(1, 0).
ods to estimate and learn value functions of action-state pairs TD-FALCON then performs code activation and code compe-
Q(s, a) that indicates the goodness for a learning system to tition according to equations (1) and (2) to select a cognitive
take a certain action a in a given state s. Such value functions node. The following lemma shows that if there is at least one
are then used in the action selection mechanism, also known cognitive node(s) encoding the current state, TD-FALCON
as the policy, to select an action with the maximal payoff. The with the given activity vectors will enable the selection of the
original TD-FALCON algorithm proposed by Tan and Xiao cognitive node with the maximum reward value.
(2005) selects an action with the maximal Q-value in a state
s by enumerating and evaluating each available action a by Direct Access Principle: During direct code access, given
                                                                       xc1 = S  xc2 =(1,...,1)     xc3 =
presenting the corresponding state and action vectors S and the activity vectors ,            ,and
                                                      (1, 0)
A to FALCON. The TD-FALCON presented in this paper re-     , the TD-FALCON code activation and competition pro-
                                                                                   J
places the action enumeration step with a direct code access cess will select the cognitive node with the weight vectors
                                                      wc1          S     wc3
procedure, as shown in Table 1. Given the current state s, J closest to and J representing the maximal reward
TD-FALCON ﬁrst decides between exploration and exploita- value, if one exists.
tion by following an action selection policy. For exploration, Proof (by Contradiction): Suppose TD-FALCON selects a
                                                      F c     J                       F c     K
a random action is picked. For exploitation, TD-FALCON 2 node    and there exists another 2 node of which
                                                                     wc1                 S     wc1
searches for optimal action through a direct code access pro- the weight vector K is more similar to than J and the
                                                                  wc3                                wc3
cedure. Upon receiving a feedback from the environment af- weight vector K encodes a higher reward value than J .
                                                         wc1                S     wc1
ter performing the action, a TD formula is used to compute As K is more similar to than J ,wederivethat
a new estimate of the Q value of performing the chosen ac-        |xc1 ∧ wc1|     |xc1 ∧ wc1|
                                                                          K   >           J  .
tion in the current state. The new Q value is then used as        αc1 + |wc1|     αc1 + |wc1|         (5)
the teaching signal for TD-FALCON to learn the association                K               J
of the current state and the chosen action to the estimated        c3                          c3
                                                      Likewise, as wK encodes a higher reward than wJ ,wehave
Q value. The details of the action selection policy, the direct
                                                                  |xc3 ∧ wc3|     |xc3 ∧ wc3|
code access procedure, and the Temporal Difference equation               K   >           J  .
                                                                   c3     c3        c3    c3          (6)
are elaborated below.                                             α  + |wK |      α   + |wJ |

                                                IJCAI-07
                                                  1073                            Table 1: TD−FALCON algorithm with direct code access.

  1.  Initialize the TD-FALCON network.
  2.  Sense the environment and formulate a state representation s.
  3.  Following an action selection policy, ﬁrst make a choice between exploration and exploitation.
      If exploring, take a random action.
      If exploiting, identify the action a with the maximal Q(s,a) value by presenting the state vector S, the action
      vector A=(1,...1), and the reward vector R=(1,0) to TD-FALCON.
  4.  Perform the action a, observe the next state s, and receive a reward r (if any) from the environment.
  5.  Estimate the revised value function Q(s, a) following a Temporal Difference formula such as ΔQ(s, a)=αTDerr.
  6.  Present the corresponding state, action, and reward (Q-value) vectors (S, A,andR) to TD-FALCON for learning.
  7.  Update the current state by s=s’.
  8.  Repeat from Step 2 until s is a terminal state.


                                           c     c
By equation (1), the above conditions imply that TK >TJ ,
which means node K should be selected by TD-FALCON
instead of node J (Contradiction).
[End of Proof]
                          c
  Upon selecting a winning F2 node J, the chosen node J
                                                 c2
performs a readout of its weight vector to the action ﬁeld F1
such that
              c2(new)    c2(old)   c2
             x       =  x      ∧ wJ .           (7)
An action aI is then chosen, which has the highest activation
value
        c2         c2(new)        c2
       xI =max{xi         : for all F1 node i}. (8)
3.3  Learning Value Function
A typical Temporal Difference equation for iterative estima-
tion of value functions Q(s,a) is given by
                                                             Figure 2: The mineﬁeld navigation simulator.
                ΔQ(s, a)=αT Derr                (9)
where α ∈ [0, 1] is the learning parameter and TDerr is a
function of the current Q-value predicted by TD-FALCON 4  The Mineﬁeld Navigation Task
and the Q-value newly computed by the TD formula.
  TD-FALCON    employs  a  Bounded  Q-learning rule,  The objective of the given task is to teach an autonomous ve-
wherein the temporal error term is computed by        hicle (AV) to navigate through a mineﬁeld to a randomly se-
                                                      lected target position in a speciﬁed time frame without hitting
          ΔQ(s, a)=αT Derr  (1 − Q (s, a)) .   (10)   a mine. In each trial, the AV starts from a randomly chosen
                             
where TDerr = r + γmaxa Q(s ,a) − Q(s, a),ofwhichr   position in the ﬁeld, and repeats the cycles of sense, act, and
is the immediate reward value, γ ∈ [0, 1] is the discount pa- learn. A trial ends when the AV reaches the target (success),
                     
rameter, and maxa Q(s ,a) denotes the maximum estimated hits a mine (failure), or exceeds 30 sense-act-learn cycles (out
value of the next state s. It is important to note that the Q of time). The target and the mines remain stationary during
                                    
values involved in estimating maxa Q(s ,a) are computed the trial. All results reported in this paper are based on a 16
by the same FALCON network itself and not by a separate by 16 mineﬁeld containing 10 mines as illustrated in Figure 2.
reinforcement learning system. The Q-learning update rule is The AV has a rather coarse sensory capability with a 180
applied to all states that the agent traverses. With value iter- degree forward view based on ﬁve sonar sensors. For each
                                                              i                           s =  1       d
ation, the value function Q(s, a) is expected to converge to direction , the sonar signal is measured by i d ,where i
                                                                                              i
r + γmaxa Q(s ,a) over time. By incorporating the scal- is the distance to an obstacle (that can be a mine or the bound-
ing term 1 − Q (s, a), the adjustment of Q values will be ary of the mineﬁeld) in the i direction. Other input attributes
self-scaling so that they will not be increased beyond 1.The of the sensory (state) vector include the bearing of the target
learning rule thus provides a smooth normalization of the Q from the current position. In each step, the AV can choose
values. If the reward value r is constrained between 0 and 1, one out of the ﬁve possible actions, namely move left, move
we can guarantee that the Q values will remain to be bounded diagonally left, move straight ahead, move diagonally right,
between 0 and 1.                                      and move right. At the end of a trial, a reward of 1 is given

                                                IJCAI-07
                                                  1074when the AV reaches the target. A reward of 0 is given when
the AV hits a mine. For the delayed reward scheme, no im-
mediate reward is given at each step of the trial. A reward of
0 is given when the system runs out of time.
  For learning the mineﬁeld task, we use a TD-FALCON
network containing 18 nodes in the sensory ﬁeld (represent-
ing 5x2 complement-coded sonar signals and 8 target bear-
ing values), 5 nodes in the action ﬁeld, and 2 nodes in the
reward ﬁeld (representing the complement-coded function
value). TD-FALCON with direct code access employed a set
of parameter values obtained through empirical experiments:
choice parameters αc1 =0.1,αc2 =0.001,αc3 =0.001;
learning rate βck =1.0 for k =1, 2, 3 for fast learning; con-
                   c1    c2    c3   1
tribution parameters γ = γ =  γ   = 3 ; baseline vigi-
lance parameters ρ¯c1 =0.25 and ρ¯c2 =0.2 for a marginal
level of match requirement in the state and action spaces, and
ρ¯c3 =0.5 for a stricter match criterion on reward values. For
Temporal Difference learning, the learning rate α was ﬁxed
at 0.5 and the discount factor γ was set to 0.9. The initial Q Figure 4: The average normalized steps taken by TD-
value was set to 0.5. For action selection policy,  was initial- FALCON using action enumeration (AE) and direct code ac-
ized to 0.5 and decayed at a rate of 0.0005 until it dropped to cess (DA) compared with those of BP reinforcement learner.
0.005.
                                                      senting the 5 sonar signal values, 8 possible target bearings,
                                                      and 5 selectable actions. The output layer consisted of only
                                                      one node representing the value of performing an action in a
                                                      particular state. A key issue in using a BP network is the de-
                                                      termination of the number of hidden nodes. We experimented
                                                      with a varying number of nodes empirically and obtained the
                                                      best results with 36 nodes. To enable a fair comparison, the
                                                      BP learner also made use of the same action selection module
                                                      based on the decay -greedy policy.
                                                        Figure 3 summarizes the performance of TD-FALCON
                                                      with direct code access (FALCON-DA), the original TD-
                                                      FALCON with action enumeration (FALCON-AE), and the
                                                      BP reinforcement learner in terms of success rates averaged
                                                      at 200-trial intervals over 3000 trials across 10 sets of experi-
                                                      ments. We observed that the success rates of both FALCON-
                                                      DA and FALCON-AE increased steadily right from the be-
                                                      ginning. Beyond 1000 trials, both systems can achieve over
                                                      90 percent success rates. The backpropagation (BP) based re-
Figure 3: The success rates of TD-FALCON using action inforcement learner on the other hand required a much longer
enumeration (AE) and direct code access (DA) compared exploration phase. In fact, the BP learner only managed to
with those of BP reinforcement learner.               reach around 90% success rates at 50,000 trials with a lower
                                                       decay rate of 0.00001. Therefore, in the ﬁrst 3000 trials, its
  To put the performance of FALCON in perspective, we fur- success rates remained under 10%.
ther conducted experiments to evaluate the performance of an To evaluate how well the AV traverses from its starting po-
alternative reinforcement learning system, using the standard sition to the target, we deﬁne a measure called normalized
                                                                  step  =  step       step
Q-learning rule and a multi-layer perceptron network, trained step given by n sd ,where   is the number of
with a gradient descent based backpropagation algorithm, as sense-move-learn cycles taken to reach the target and sd is
the function approximator. We have chosen the backpropaga- the shortest distance between the starting and target positions.
tion (BP) algorithm as the reference for comparison as gra- A normalized step of 1 means the system has taken the opti-
dient descent is by far one of the most widely used universal mal (shortest) path to the target. As depicted in Figure 4, after
function approximation techniques and has been applied in 1000 trials, both FALCON-AE and FALCON-DA were able
many contexts, including Q-learning [Sun et al., 2001] and to reach the target in optimal or close to optimal paths in a
adaptive critic [Werbos, 2004]. The BP learner employed great majority of the cases. The BP learner as expected pro-
a standard three-layer feedforward architecture to learn the duced unsatisfactory performance in terms of path optimality.
value function with a learning rate of 0.25 and a momentum Considering network complexity, FALCON-DA demon-
term of 0.5. The input layer consisted of 18 nodes repre- strated a great improvement over FALCON-AE by creating

                                                IJCAI-07
                                                  1075