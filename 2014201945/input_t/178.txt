                   Factored A* Search for Models over Sequences and Trees 

                             Dan Klein                                  Christopher D. Manning 
                 Department of Computer Science                     Department of Computer Science 
                        Stanford University                                 Stanford University 
                     Stanford, CA 94305-9040                            Stanford, CA 94305-9040 
                       klein@cs.stanford.edu                             manning@cs.stanford.edu 

                         Abstract                              sum of the scores of the arcs along the path, and that lower 
                                                               scores are better.1 The particular assumption in this paper 
      We investigate the calculation of A* bounds for          is that the arc scoring function has a special factored form. 
      sequence and tree models which are the explicit          Specifically, there exists a set of projections [TT\, .. .7ik] of 
      intersection of a set of simpler models or can be        nodes (and therefore also arcs and graphs) such that for any 
      bounded by such an intersection. We provide a            arc (JC, y), its score a is given by: 
      natural viewpoint which unifies various instances 
      of factored A* models for trees and sequences, 
      some previously known and others novel, includ•
      ing multiple sequence alignment, weighted finite-        Whenever the scoring function factors in this way, we have an 
      state transducer composition, and lexicalized sta•       immediate recipe for a factored A * bound, which we denote 
      tistical parsing. The specific case of parsing with      by h. Specifically, we can bound the shortest path in G from 
      a product of syntactic (PCFG) and semantic (lexi•        a node n to the goal g by the sum of the shortest paths inside 
      cal dependency) components is then considered in         each projection (G). Formally, if aG(n, g) is the length of 
      detail. We show that this factorization gives a mod•     a shortest path from n to g in a graph G, then: 
      ular lexicalized parser which is simpler than com•
     parably accurate non-factored models, and which 
      allows efficient exact inference with large treebank 
      grammars.                                                This follows immediately from the optimality of the projected 
                                                               paths and the structure of the scoring function. These pro•
                                                               jections need not be mutually compatible, and therefore the 
 1 Introduction                                                bound may not be tight. Broadly speaking, the greater the de•
                                                               gree to which each projection prefers similar paths, the better 
 The primary challenge when using A* search is to find heuris•
                                                               the bound will be, and the more efficient our search will be. 
 tic functions that simultaneously are admissible, close to ac•
 tual completion costs, and efficient to calculate. In this pa•
 per, we describe a family of tree and sequence models in      3 Projection Models for Sequences 
 which path costs are either defined as or bounded by a com•   For intuition, we first consider applications to sequence mod•
 bination of simpler component models, each of which scores    els before extending to the more complex case of tree models. 
 some projection of the full structure. In such models, we can 
 exploit the decoupled behavior over each projection to give   3.1 Example: Multiple Sequence Alignment 
 sharp heuristics for the combined space. While we focus       A situation which fits this framework perfectly is the align•
 on models of trees and sequences within NLP applications,     ment of multiple genome sequences in bioinformatics, where 
 the approach can be applied more generally (and already has   such multiple sequence alignments (MSAs) are standardly 
 been, in the case of biological sequence models). All the con• evaluated by sum-of-pairs scoring [Durbin et al., 1998]. MSA 
 crete cases we consider here involve search over spaces which is a generalization of the longest-common-subsequence prob•
 are equivalent to dynamic programming lattices, though this   lem, in which one is given sequences like those in figure la, 
 detail, too, is somewhat peripheral to the basic ideas.       and asked to produce pointwise alignments. Alignments of 
                                                               d sequences consist of t vertical timeslices which 
 2 Projection Models for Graphs                                specify, for each sequence, either a successive element of the 
                                                               sequence or a gap (-), and are such that if the gaps are re•
 The core idea of factored A* search can apply to any graph    moved, the rows contain the original sequences. The score 
 search. Assume that G = (N, A) is a very large graph, with    of a timeslice is the sum of the scores of each of the pairs 
 a single source node s and a single goal node g, and that we 

 wish to use A* search to efficiently find a best path from s to  1 We will talk about minimum sums, but other semirings work as 
g. For concreteness, assume also that the score of a path is the well, specifically maximum products. 


 1246                                                                                                           SEARCH Figure 1: An example of a multiple sequence alignment, (a) The 
original sequences, (b) A multiple alignment with one timeslice dis•
tinguished, (c) The sum-of-pairs scoring function for that timeslice. 
                                                              Figure 2: Effectiveness of the factored A* bound for the multiple 
in that slice (where each pair of symbols is assigned some    alignment of several sequence sets. Numbers in the data set names 
experimental goodness value).                                 give the number of sequences being aligned. Note the log scale: 
                                                              for example, on glob-7 the A* search is over a trillion times more 
  The well-known dynamic program for calculating opti•
                                                              efficient than exhaustive search. 
mal multiple alignments involves a lattice of position nodes 
               which specify an index along each sequence     shows that factored A* bounds can be highly effective. 
[Durbin et ai, 1998]. When each node is visited, each of         One potential worry with A* search is that the cost of com•
its successors (where each position is either in•
                                                              puting heuristics can be substantial. In this case, the 0(k2) 
cremented or not) are relaxed with the best score through p   pairwise alignments could be calculated very efficiently; in 
combined with the score of the timeslice that the change from our experiments this pre-search phase took up much less than 
p to p' represents. These nodes form a lattice of size         1% of the total time. 
where n is the maximum length of the sequences. This be•
comes extremely inefficient as k grows.                       3.2 Example: Finite State Transducers 
  The specific following idea has been used before ([Ikeda    We briefly describe another potential application of factored 
and lmai, 1994] is tht earliest reference we could find) and  A* search for sequence models: the intersection of weighted 

it has been worked on recently ([Yoshizumi et al, 2000] in•   finite-state transducers (WFSTs).3 WFSTs are probabilistic 
ter alia), though perhaps it has not received the attention it mappings between sequences in one alphabet to sequences 
deserves in the bioinformatics literature. We present it here in another, for example a transducer might map an input of 
because (1) it is a good example that can be cast into our    written text to an output of that text's pronunciation as a 
framework and (2) it gives a good starting intuition for the  phoneme sequence. Intersections of WFSTs have been ap•
novel cases below. Since the score of an arc (timeslice) is a plied to various tasks in speech and NLP [Mohri, 1997], 
sum of pairwise goodness scores, we can define a set of such as text-to-speech, and, most famously in the NLP lit•
projections, one onto each pair of indices Under  erature, modeling morphophonology [Kaplan and Kay, 1994; 
a node will project to its a and b indices,                   Albro, 2000]. In these cases, each transducer constrains some 
It is easy to see that the optimal path in this projection is just small portion of the overall output sequence. The case of find•
the optimal 2-way alignment of the portions of sequences a    ing the most likely intersected output of a set of WFSTs (A/,} 
and b which are to the right of the indices i and respec•     for an input sequence involves the following: 
tively. We can therefore bound the total completion cost of     1. For each A/,, create the projection of the full output 
the A-way alignment from n onward with the sum of the pair-        space O onto Mt's output space (note that this can be 
wise completion costs of the 2-way alignments inside each 
                                                                   the identity projection).4 
projection. 
  Figure 2 shows some experimental speed-ups given by this      2. 
method, compared to exhaustive search. We took several 
protein sequence groups from [McClure et ai, 1994], and,        3.  
for each set, we aligned as large a subset of each group as     While transduction intersection fits cleanly into the fac•
was possible using uniform-cost search with 1GB of mem•       tored framework, the primary utility of transducers lies in 
ory. The left four runs show the cost (in nodes visited) of   their composition, not their intersection [Mohri, 1997]. In 
aligning these subsets with both uniform-cost search and A*   this case, transducers are chained together, with the output 
search. In the right four runs, we added another sequence     of one serving as the input to the next. In this case, it is 
to the subsets and solved the multiple alignment using only   worth switching from talk of summed distances to talk of 
the A* search. The A* savings are substantial, usually pro•   multiplied probabilities. Say we have two transducers, 
viding several orders of magnitude over uniform-cost search,  which gives a distribution from sequences to se•
and many orders of magnitude over the exhaustive dynamic      quences X, and which gives from X to O. 
programming approach. This verifies previous findings, and 
                                                                 3 WFSTs are equivalent to HMMs which have emission weights 
                                                              assigned to their transitions (not states) and which may have epsilon 
   2Note that the DP was never run - it would not have fit in memory 
- but it is easy to calculate the size of the lattice. There are also transitions. 
subtleties in running the uniform-cost search since the score of a 4For simplicity, we assume all history relevant to any transducer 
timeslice can be negative (we add in a worst possible negative score). is encoded into the state space O. 


SEARCH                                                                                                             1247                                                                symbol, along with the region of the input it spans. The goal 
                                                               node is then a parse of the root symbol over the entire in•
                                                               put. (Ilyper)paths embody trees, and the score of a path is 
                                                               the combination of the scores of the arcs in the tree. One fine 
                                                               point is that, while a standard path from a source s to a goal 
                                                               g through a node // breaks up into two smaller paths (.v to n 
                                                               and n to g), in the tree case there will be an inside path and 
                                                               an outside path, as shown in the right of figure 5. In general, 
                                                               then, the completion structures that represent paths to the goal 
                                                               (marked by in the figure) are specified not only by a node n 
Figure 4: Two representations of a parse: (a) a tree, (b) a path in the and goal g, but also by the original source .v. 
edge hypergraph.                                                 With this modification, the recipe for the factored A* 
                                                               bound is now: 
We then wish to answer questions about their composed be•
havior. For example, we might want to find the output o 
which maximizes according to this model. The com•
                                                               Next, we present a concrete projection model for scoring lex-
mon Viterbi approximation is to settle for the o from the 
                                                               icalized trees, and construct an A* parser using the associated 
pair (0, x) which maximizes This problem would 
                                                               factored A* bound. 
fit neatly into the factored framework if the (usually false) 
                                                                 Generative models for parsing natural language typically 
conditional independence where 
                                                               model one of the kinds of structures shown in figure 3. While 
true - in fact it would then be WFST intersection. How•
                                                               word-free syntactic configurations like those embodied by 
ever, something close to this does trivially hold:  
                                                               phrase structure trees (figure 3a) are good at capturing the 
                  Given this, we can define another model 
                                                               broad linear syntax of a language [Charniak, 1996], word-
                            R is not a proper probabilistic 
                                                               to-word affinities like those embodied by lexical dependency 
model - it might well assign probability one to every trans•
                                                               trees (figure 3b) have been shown to be important in resolv•
duction - but its intersection with does upper-bound 
                                                               ing difficult ambiguities [Hindle and Rooth, 1993]. Since 
the actual composed model. Hence these two projections pro•
                                                               both kinds of information are relevant to parsing, the trend 
vide a factored bound for a non-factored model, with the prac•
                                                               has been to model lexicalized phrase structure trees like fig•
tical utility of this bound depending on how tightly 
                                                               ure 3c. 
typically bounds  
                                                                 In our current framework, it is natural to think of a lexi•
                                                               calizcd tree as a pair /, = (T, D) of a phrase structure tree 
4 Projection Models for Trees                                  T and a dependency tree D. In this view, generative mod•
                                                               els over lexicalized trees, of the sort standard in lexicalized 
Search in the case of trees is not over standard directed 
                                                               PCFG parsing [Collins, 1999; Charniak, 2000], can be re•
graphs, but over a certain kind of directed hypergraph in 
                                                               garded as assigning mass P(T, D) to such pairs. In the stan•
which arcs can have multiple sources (but have a single tar•
                                                               dard approach, one builds a joint model over P(T, D), and, 
get). This is because multiple sub-trees are needed to form a 
                                                               for a given word sequence own, one searches for the maxi•
larger tree.  Figure 4 shows a fragment of such a hypergraph 
          5                                                    mum posterior parse: 
for a small parse tree (note that all the lines going to one ar•
rowhead represent a single hyperarc). We don't give the full 
definitions of hypergraph search here (see [Gallo et al., 1993] 
for details), but the basic idea is that one cannot traverse an Since is a constant, one operationally searches instead 
arc until all its source nodes have been visited. In the parse for the maximizer of P(T, D, w). 
case, for example, we cannot build a sentence node until we      The naive way to do this is an dynamic program 
build both a noun phrase node and an (adjacent) verb phrase    (often called a tabular parser or chart parser) that works as 
node. The nodes in this graph arc identified by a grammar      follows. The core declarative object is an edge, such as 
                                                                              which encapsulates all parses of the span 

   5These directed B-hypergraphs model what has been explored as which are labeled with grammar symbol X and are headed by 
AND/OR trees in AI.                                            word Edges correspond to the nodes in the 


1248                                                                                                            SEARCH            1. Extract the PCFG projection and set up the PCFG parser. 
           2. Use the PCFG parser to find projection scores for each edge. 
           3. Extract the dependency projection and set up the dependency parser. 
           4. Use the dependency parser to find projection scores for each edge. 
           5. Combine PCFG and dependency projections into the full model. 
           6. Form the factored A* estimate  
           7. Use the combined parser, with h(s, e, g) as an A* estimate of  
            Figure 5: The top-level algorithm (left) and an illustration of how paths decompose in the parsing hypergraph (right). 


parsing hypergraph. Two edges and  length increases).8 The A* estimates were so effective that 
can be combined whenever they are contiguous (the right                 even with our object-heavy Java implementation of the com•
one starts where the left one ends) and the grammar per•                bined parser, total parse time was dominated by the initial, 

mits the combination. For example, if there were a rewrite              array-based PCFG phase (see figure 6b).9 
                             those two edges would combine to 
form [Z, z\ i, A], and that combination would be scored by              4.1 Specific Projection Models for Parsing 
some joint model over the word and symbol configuration: 
                                                                        To test our factored parser, we built several component mod•
                    These weighted combinations are the arcs 
                                                                        els, which were intended to show the modularity of the ap•
in the hypergraph. 
                                                                        proach. We merely sketch the individual models here; more 
   A natural projection of a lexicalized tree L is onto its com•        details can be found in [Klein and Manning, 2003]. For P{ 7), 
ponents T and D (though, to our knowledge, this projec•                 we built successively more accurate PCFGs. The simplest, 
tion has not been exploited previously). In this case, the              PCFG-BASIC, used the raw treebank grammar, with nontermi•
score for the combination above would be                                nals and rewrites taken directly from the training trees [Char-
                                                                        niak, 1996]. In this model, nodes rewrite atomically, in a 
   This kind of projected model offers two primary benefits.            top-down manner, in only the ways observed in the training 
First, since we are building component models over much                 data. For improved models of P(T), tree nodes' labels were 
simpler projections, they can be designed, engineered, and              annotated with various contextual markers. In PCFG-PA, each 
tested modularly, and easily. To underscore this point, we              node was marked with its parent's label as in LJohnson, 1998]. 
built three PCFG models of P(T) and two lexical dependency              It is now well known that such annotation improves the accu•
models of P(T). In section 4.2, we discuss the accuracy of              racy of PCFG parsing by weakening the PCFG independence 
these models, both alone and in combination.                            assumptions. For example, the NP in figure 3a would actu•
   Second, our A* heuristic will be loose only to the degree            ally have been labeled NP*S. Since the counts were not frag•
that the two models prefer different structures. Therefore, the         mented by head word or head tag, we were able to directly 
combined search only needs to figure out how to optimally               use the MLE parameters, without smoothing.10 The best 
reconcile these differences, not explore the entire space of            PCFG model, PCFG-LING, involved selective parent split•
legal structures. Figure 6 shows the amount of work done                ting, order-2 rule markovization (similar to [Collins, 1999; 
in the uniform-cost case versus the A* case. Clearly, the               Charniak, 2000]), and linguistically-derived feature splits. 
uniform-cost version of the parser is dramatically less ef•
ficient; by sentence length 15 it extracts over 800K edges,                8Note that the uniform-cost parser does enough work to exploit 
while even at length 40 the A* heuristics are so effective that         the shared structure of the dynamic program, and therefore edge 
                                                                        counts appear to grow polynomially. However, the A* parser does 
only around 2K edges are extracted. At length 10, the av•
                                                                        so little work that there is minimal structure-sharing. Its edge counts 
erage number is less than 80, and the fraction of edges not 
                                                                        therefore appear to grow exponentially over these sentence lengths, 
suppressed is better than 1/1 OK (and it improves as sentence          just like a non-dynamic-programming parser's would. With much 
                                                                        longer sentences, or a less efficient estimate, the polynomial behav•
                                                                        ior would reappear. 
    Most models, including ours, will also mention distance; wc 
   6                                                                        Thcre are other ways of speeding up lexicalized parsing with•
ignore this for now.                                                       9
                                                                        out sacrificing search optimally. Eisner and Satta [Eisner and Satta, 
    As a probabilistic model, this formulation is mass deficient, as•
   7                                                                    1999] propose a clever modification which separates this pro•
signing mass to pairs which are incompatible, either because they 
do not generate the same terminal string or do not embody compat•       cess into two steps by introducing an intermediate object. However, 
ible bracketings. Therefore, the total mass assigned to valid struc•    even the formulation is impractical for exhaustive parsing 
tures will be less than one. We could imagine fixing this by renor-     with broad-coverage, lexicalized treebank grammars. The essential 
malizing. In particular, this situation fits into the product-of-experts reason is that the non-terminal set is just too large. Wc did imple•
framework [Hinton, 2000], with one semantic expert and one syn•         ment a version of this parser using their formulation, but, be•
tactic expert that must agree on a single structure. However, since     cause of the effectiveness of the estimate, it was only marginally 
we are presently only interested in finding most-likely parses, no      faster; as figure 6b shows, the combined search time is very small. 
global renormalization constants need to be calculated. In any case,      10This is not to say that smoothing would not improve perfor•
the question of mass deficiency impacts only parameter estimation,      mance, but to underscore how the factored model encounters less 
not inference, which is our focus here.                                 sparsity problems than a joint model. 


SEARCH                                                                                                                               1249                                                               4.2 Parsing Performance 
                                                              In this section, we describe our various projection models and 
                                                              test their empirical performance. There are two ways to mea•
                                                              sure the accuracy of the parses produced by our system. First, 
                                                              the phrase structure of the PCFG and the phrase structure pro•
                                                              jection of the combination parsers can be compared to the 
                                                              treebank parses. The parsing measures standardly used for 
                                                              this task are labeled precision and recall.11 We also report 
                                                              Fi, the harmonic mean of these two quantities. Second, for 
                                                              the dependency and combination parsers, we can score the 
                                                              dependency structures. A dependency structure D is viewed 
                                                              as a set of head-dependent pairs (h, d), with an extra depen•
                                                              dency {root, x) where root is a special symbol and x is the 
                                                              head of the sentence. Although the dependency model gen-
                                                              crates part-of-speech tags as well, these are ignored for de•
                                                              pendency accuracy. Punctuation is not scored. Since all de•
                                                              pendency structures over n non-punctuation terminals con•
                                                              tain n dependencies (n — 1 plus the root dependency), we 
                                                              report only accuracy, which is identical to both precision and 
   Models of P(D) were lexical dependency models, which       recall. It should be stressed that the "correct" dependency 
deal with part-of-speech tagged words: pairs First structures, though generally correct, are generated from the 
 the head of a constituent is generated, then succes•         PCFG structures by linguistically motivated, but automatic, 
 sive right dependents until a STOP token is gen•             and only heuristic rules. 
erated, then successive left dependents until is generated 
                                                                 Figure 7 shows the relevant scores for the various PCFG 
again. For example, in figure 3, first wc choose fell-VBD 
                                                              and dependency parsers alone. The valence model increases 
as the head of the sentence. Then, we generate in-IN to the 
                                                              the dependency model's accuracy from 76.3% to 85.0%, and 
 right, which then generates September-UN to the right, which 
                                                              each successive enhancement improves the Fj of the PCFG 
generates on both sides. We then return to I/I-IN, gener•
                                                              models, from 72.7% to 77.7% to 82.9%. The combination 
ate to the right, and so on. The dependency models re•
                                                              parser's performance is given in figure 8. As each individ•
quired smoothing, as the word-word dependency data is very 
                                                              ual model is improved, the combination F\ is also improved, 
sparse. In our basic model, DBP-BASIC, we generate a de•
                                                              from 79.1% with the pair of basic models to 86.7% with the 
pendent conditioned on the head and direction, requiring a 
                                                              pair of top models. The dependency accuracy also goes up: 
 model of This was estimated using a 
                                                              from 87.2% to 91.0%. Note, however, that even the pair of ba•
 back-off model which interpolated the sparse bilexical counts sic models has a combined dependency accuracy higher than 
with the denser but less specific counts given by ignoring    the enhanced dependency model alone, and the top three have 
the head word or by first generating the dependent tag and    combined F] better than the best PCFG model alone. For the 
then generating the dependent word given only the dependent   top pair, figure 6c illustrates the relative F1 of the combina•
tag. The interpolation parameters were estimated on held-out  tion parser to the PCFG component alone, showing the unsur•
 data. The resulting model can thus capture classical bilexi• prising trend that the addition of the dependency model helps 
 cal selection, such as the affinity between payrolls and fell, as 
 well as monolexical preferences, such as the tendency for of 
                                                                11A tree T is viewed as a set of constituents c(T). Constituents 
to modify nouns. In the enhanced dependency model, DEP-       in the correct and the proposed tree must have the same start, end, 
 VAL, we condition not only on direction, but also on distance and label to be considered identical. For this measure, the lexical 
and valence. Note that this is (intentionally) very similar to heads of nodes are irrelevant. The actual measures used are detailed 
the generative model of [Collins, 1999] in broad structure, but in [Magerman, 1995], and involve minor normalizations like the re•
substantially less complex.                                   moval of punctuation in the comparison. 


 1250                                                                                                          SEARCH 