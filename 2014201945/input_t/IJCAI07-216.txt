                                     Augmented Experiment:  


                      Participatory Design with Multiagent Simulation 

               Toru Ishida†, Yuu Nakajima†, Yohei Murakami‡, Hideyuki Nakanishi§ 
                          †Department of Social Informatics, Kyoto University 
                ‡The National Institute of Information and Communications Technology 
                     §Department of Adaptive Machine Systems, Osaka University 
      ishida@i.kyoto-u.ac.jp, nkjm@ai.soc.i.kyoto-u.ac.jp, yohei@nict.go.jp, nakanishi@ams.eng.osaka-u.ac.jp 

                    Abstract                          1. Describe interactions between human users and socially 
                                                         embedded systems so as to define the expected user be-
 To test large scale socially embedded systems, this paper haviors with socially embedded systems (interaction 
 proposes a multiagent-based participatory design that 
                                                         model hereafter).  
 consists of two steps; 1) participatory simulation, where 2. Perform a multiagent-based simulation by modeling users 
 scenario-guided agents and human-controlled avatars     under the given interaction scenarios (agent model here-
 coexist in a shared virtual space and jointly perform 
                                                         after). The simulation takes place in virtual space and 
 simulations, and the extension of the participatory     involves a large number of simulated users. Results of the 
 simulation into the 2) augmented experiment, where an   simulation can predict how the entire system would work 
 experiment is performed in real space by human subjects 
                                                         in society.  
 enhanced by a large scale multiagent simulation. The 3. Replace some of the simulated users by human-controlled 
 augmented experiment, proposed in this paper, consist of avatars to perform a multiagent-based participatory 
 1) various sensors to collect the real world activities of 
                                                         simulation (participatory simulation hereafter). The 
 human subjects and project them into the virtual space, 2) simulation is performed in virtual space, and the avatars 
 multiagent simulations to simulate human activities in  are controlled by human subjects sitting in front of their 
 the virtual space, and 3) communication channels to in-
                                                         personal computers.  
 form simulation status to human subjects in the real 4. Perform experiments in real space to try out the entire 
 space. To create agent and interaction models incre-    system with human subjects. Since the number of sub-
 mentally from the participatory design process, we pro- jects is often limited, the experiment should be aug-
 pose the participatory design loop that uses deductive  mented by a large number of simulated users in virtual 
 machine learning technologies. Indoor and outdoor       space. We called this the multiagent-based augmented 
 augmented experiments have been actually conducted in   experiment (augmented experiment hereafter). 
 the city of Kyoto. Both experiments were intended to test To learn from the participatory design process, this paper 
 new disaster evacuation systems based on mobile      separates agent models from interaction models: the former 
 phones.  
                                                      covers the beliefs, desires, intentions, and emotions of human 
                                                      users, and the latter covers protocols, methods, rules, or laws 
1 Introduction                                        that guide users when interacting with the socially embedded 
Ubiquitous/pervasive computing systems in the public space systems. We use extended finite state automata for describing 
often interact with anonymous users. We propose to apply interaction models, while production rules are used to de-
multiagent technologies to the design of those systems that scribe agent models for approximating users. We view in-
are to be embedded in society.                        teraction models as behavioral guidelines of human users 
 Participatory simulations have been already studied inten- playing with socially embedded systems; users keep 
sively for modeling human societies [Drogoul et al., 2002, autonomy within the given guidelines. The question is 
Guyot et al., 2005]. For designing socially embedded sys- whether or not users will follow the guidelines in an actual 
tems, however, simulations in virtual space cannot reproduce service environment. In other words, the effectiveness of 
the reality of the actual application environment. The goal of interaction models depends on the agent models, which in-
this paper is to introduce real world experiments to the clude user personalities such as deliberative and reactive. 
process of participatory design, so as to bridge the gap be-   In this paper, we call the descriptions of interaction models 
tween participatory simulations and services in operation. scenarios. We use scenario description language Q, which 
We propose multiagent-based participatory design method- can simultaneously interpret scenarios for a large number of 
ologies (participatory design hereafter) to test socially em- agents [Ishida, 2002b]. Q has been already connected to 
bedded systems in the following steps.                FreeWalk [Nakanishi et al., 2004b] for virtual city experi-


                                                IJCAI-07
                                                  1341ments [Ishida, 2002a], Cormas for a natural resource simu- method and the Follow-me method. In the former, the leader 
lation [Bousquet et al., 1998], and Caribbean [Yamamoto et shouts out evacuation instructions and eventually moves 
al., 1999] for massively multiagent simulations [Ishida et al., toward the exit. In the latter, the leader tells a few of the 
2005]. Q, FreeWalk, and Caribbean have been used to con- nearest evacuees to follow him and actually proceeds to the 
duct indoor and outdoor disaster evacuation experiments. exit without verbalizing the direction of the exit.  
Those real-scale experiments confirm the feasibility and A participatory simulation was performed in which we re-
usefulness of augmented experiments.                  placed twelve out of twenty scenario-guided agents by hu-
                                                      man-controlled avatars. The virtual space was designed so 
2 Participatory Simulation                            that the human subjects could not distinguish agents from 
                                                      avatars. After the simulation, we applied deductive machine 
In our participatory design methodology, we first conduct learning technology with domain knowledge to refine the 
multiagent-based simulations. Figure 1(a) illustrates how a agent models from the logs of the participatory simulation, 
multiagent-based simulation is realized. The scenario proc- and successfully modified the agent models and original 
essor interprets interaction models and requests agents in evacuation scenarios (i.e., interaction models). 
virtual space to perform sensing and acting functions. Note Once an accurate model is acquired from participatory 
that, since agents are autonomous and have their own agent simulations, it becomes possible to simulate experiments that 
models, though agents receive instruction according to the cannot be conducted in real space. For example, Sugiman’s 
scenarios, there is no guarantee that they will behave as re- experiment was performed using two or four leaders, but we 
quested.                                              can vary this number to clarify the relation between the 
  We can easily extend multiagent-based simulations to number of leaders and the time required for evacuation. 
yield participatory simulations by replacing some of the Moreover, we can explore an effective combination of the 
scenario-guided agents with human-controlled avatars. Fig- Follow-me and Follow-direction methods. Given accurate 
ure 1(b) illustrates how human subjects and agents can co- and explainable agent models, even though the models can 
operatively perform a simulation. Just as with video games, only capture a part of the variations possible in human be-
human subjects can join the simulation by controlling avatars havior, participatory simulations must be seen as useful in 
via joy sticks, mice, or other input devices. To analyze educating or training people. People can be provided with a 
simulation results, we monitor the entire process of the learning environment wherein evacuations or other drills can 
simulation by visualizing the virtual space. In addition to be experienced.   
video taping the virtual space, we record how the human 
subjects control their avatars. Recording human behavior is 
useful for analyzing the simulation results and for improving 3 Augmented Experiment 
the agent and interaction models.                     Participatory simulations are particularly useful when con-
                                                      ducting controlled experiments: they make it easy to prepare 
                       Real Space        Human        the application environments for testing, and user behavior 
                                         Subject      can be recorded for later analysis. However, it is sometimes 
                                                      fails to provide enough reality to allow the testing of ubiq-
Scenario               Scenario                       uitous/pervasive computing environments. 
                                           Monitor     To understand how users behave with socially embedded 
                                                      systems, real-world experiments are often required. In the 
             Agent                     Avatar         case of video phones, for example, since user behavior in 
Virtual Space          Virtual Space                  everyday life is not easy to simulate, it is essential to observe 
                                                      how individual users employ their video phones in real space. 
  a) Multiagent-Based   b) Multiagent-Based           In ubiquitous/pervasive computing, however, because a large 
    Simulation            Participatory Simulation    number of electronic devices will be embedded in public 
           Figure 1. Participatory Simulation         spaces like railway stations and are in continuous service, it 
                                                      is desired but often impossible to conduct experiments with a 
 In summary, a participatory simulation consists of 1) large number of human subjects. Augmented experiments 
agents for modeling users, 2) avatars to represent human offer the ability to use just a small number of human subjects 
subjects, 3) scenarios for modeling interactions, 4) human to perform an experiment that can be enhanced by large scale 
subjects to control avatars, 5) virtual space to represent real multiagent-based simulations.  
space, and 6) a monitor to visualize simulations ongoing in Figure 2 illustrates how augmented experiments are real-
the virtual space.                                    ized. Figure 2(a) represents a real world experiment, where 
 We conducted a multiagent-based simulation in the disaster human subjects communicate and perform an experiment in 
evacuation domain. To establish the process of refining agent real space. Figure 2(b) shows how we introduce a virtual 
models and interaction models, we simulated the precisely space into the real world experiment. The sensors in the real 
controlled experiments conducted by Sugiman [Sugiman et space captured the behavior of human subjects for reproduc-
al., 1998]. He developed a simple environment with human tion in the virtual space. The sensors can be cameras, RFIDs, 
subjects to determine the effectiveness of two evacuation or GPS depending on the environment. By using the virtual 
methods (i.e., interaction models): the Follow-direction space, we can monitor the entire experiment from various 

                                                IJCAI-07
                                                  1342viewpoints. Furthermore, we can communicate with the     must receive sensing information through the communi-
human subjects in the real space through their avatars in the cation channels or from the corresponding human extras. 
virtual space. Transcendent communication is a new moni- To ensure the subjects naturally behave in real space, the 
toring interface, where a visually simulated public space selection of sensor devices and communication media 
provides a more flexible and consistent view than regular becomes crucial. Visual, auditory and textual interactions 
surveillance systems [Nakanishi et al., 2004a].          are to be organized appropriately. 
 Figure 2(c) illustrates an augmented experiment. In parallel 2. Learning from data obtained in virtual and real spaces: 
with a real world experiment, a large scale multiagent-based Learning issues are similar to those in participatory 
simulation is conducted in virtual space: the experiment is simulations, but their evaluation is more difficult, since 
augmented by the simulation. To provide social reality to the the behaviors of human subjects in real space are harder 
human subjects, scenario-guided extras are placed around the to analyze than those in participatory simulations. 
subjects. In contrast to the participatory simulations, the Though we can monitor the experiment via virtual space, 
human extras do not control avatars: the human extras in the video capture of the human subjects in real space is also 
real space are controlled by the scenario-guided agents. For indispensable for analyzing the entire experiment. 
example, we can use human extras acting as evacuation To confirm the feasibility and usefulness of augmented ex-
leaders to conduct disaster evacuation drills. In contrast to periments and to determine their future issues, we conducted 
participatory simulations where human subjects sense and act indoor and outdoor evacuation experiments in real space 
in virtual space, augmented experiments allow subjects to augmented by a large scale multiagent-based simulation. 
sense and act in real space. Conducting an augmented ex-
periment is possible if the real space is equipped with enough 4 Indoor Experiment 
sensors.  
                                                      Everyday, more than 300,000 passengers pass through Kyoto 
                            Avatar
                                                      station, the main railway station in Kyoto. In this station, 
       Human                                          using Q [Ishida, 2002b] and FreeWalk [Nakanishi et al., 
       Subject      Virtual Space                     2004b], we installed a disaster evacuation system that tracks 
                                                      passengers to help their navigation based on their current 
                                           Monitor    positions. Beyond conventional navigation systems, which 
Real Space          Real Space                        announce route information using public loudspeakers, our 
                                                      system sends instructions to individuals using mobile phones. 
a) Real-World Experiment b) Virtual Space to Monitor Experiment
                                                      Augmented experiments are required for testing the evacua-
          Scenario                                    tion system, because there is no other way to conduct ex-
                                                      periments with enough reality. 
                                       Monitor         
                               Agent
                     Avatar
                                                                                                 Camera
          Virtual Space

                                   Human
                                    Extra
          Real Space
          c) Multiagent-Based Augmented Experiment  
            Figure 2. Augmented Experiment 
                           
 In summary, an augmented experiment consists of 1) 
agents for modeling users, 2) avatars to represent human 
                                                             3D Virtual Space          Indoor Real Space  
subjects, 3) scenarios for modeling interactions, 4) human          Figure 3. Indoor Experiment 
subjects to act experiments, 5) virtual space to represent real  
space, 6) a monitor to visualize the experiment in real space The augmented experiment was designed as follows. As the 
enhanced by simulations in virtual space, 7) sensors to re- sensors, we placed twenty eight cameras in Kyoto station, 
produce human subjects in virtual space as avatars, 8) com- and captured the movements of passengers in real time. The 
munication channels between real and virtual spaces, and 9) cameras are specially designed to detect passengers' behavior 
human extras to represent agents in virtual space for inter- but not personal features. Figure 3 shows the omnidirectional 
acting with human subjects.                           cameras placed in the station. As the virtual space, we used 
 The issues on augmented experiments are as follows.  
                                                      FreeWalk, a three dimensional virtual city system and used it 
1. Seamless connections between virtual and real spaces: to reproduce the passengers' behavior. We implemented a 
   The difficulty with augmented experiments is to provide monitor based on transcendent communication [Nakanishi et 
   the human subjects with a sufficient level of reality. To al., 2004a]. Figure 3 includes a snapshot of the monitoring 
   guide human subjects in a timely fashion, the subjects system; human subjects on the station platform are projected 

                                                IJCAI-07
                                                  1343onto avatars in virtual space. A bird’s-eye view of the real As in the indoor experiments, the evacuation leader acti-
space is reproduced on the screen of the control center so that vated communication channels to particular evacuees by 
evacuation leaders in the center can easily monitor the ex- pointing at the evacuees on the screen. The leader could talk 
periment. As the communication channels, the leader can to the particular evacuees and provide customized navigation. 
point at particular passengers on the screen, and talk to them We used human extras as evacuation leaders in this experi-
through their mobile phones. When the monitor detects ment to check the evacuation routes taken by human subjects 
pointing operations, a wireless connection is immediately in real space. 
activated between the control center and the indicated pas-
sengers.  
 A multiagent-based simulation with a large number of                                              GPS
agents controlled by evacuation scenarios was performed in 
parallel with the experiment in real space. See-through 
head-mounted displays are not suitable for presenting the 
simulation of augmented experiments, since it is unsafe to 
mask the views of passengers. As described above, since we 
used mobile phones, small and low-resolution images of 
three dimensional virtual spaces are difficult to understand. 
Instead of displaying visual simulations, the mobile phones 
displayed symbols that directly represent what human sub-
jects need to understand. We did not use human extras in this 
experiment. 
 We conducted the indoor augmented experiment in Feb-         2D Virtual Space          Outdoor Real Space
ruary 2005. From this experience, we learned that insuffi-                                               
                                                                   Figure 4. Outdoor Experiment 
cient visual reality prevented the human subjects from rec-
                                                        
ognizing the crowd of agents around the staircase. It appears 
                                                       The human subjects could grasp the experiment from the 
that the usability of an augmented experiment depends sig-
                                                     screen of their mobile phones. The map around the place the 
nificantly on the user interface for interacting with the agents. 
                                                     subject was standing was displayed together with informa-
                                                     tion of fires, blocked routes, and safe areas, just as on the 
5 Outdoor Experiment                                 monitor in the control center. The moves of other evacuees 
One year after the indoor experiment, we implemented a were also displayed on their mobile phones. From this ex-
large-scale outdoor evacuation system for emergency situa- perience, we learned that a map can be an excellent interface 
tions using Q [Ishida, 2002b] and Caribbean [Yamamoto et between human subjects and agents as well as evacuation 
al., 1999], and conducted an augmented experiment in leaders. Unlike the indoor experiment, since route selection 
January 2006. This system architecture is close to that of our is the main issue in the outdoor experiment, the human sub-
indoor experiment, but the sensors were GPS devices instead jects did not have much difficulty in imagining the disaster 
of omnidirectional cameras, and as the virtual space, we used situation. However, if the number of agents was high in the 
a two dimensional map instead of a three dimensional virtual outdoor experiment, then virtual crowding would become a 
city system. The virtual space is displayed on the monitor problem. Human extras playing evacuation leaders can in-
screen of the control center in a birds-eye view, so that the crease the reality of the experiment. 
leader could grasp how evacuees were behaving in the ex-
periment. Evacuees on a screen consisted of a small number 6 Learning from Experiments 
of avatars representing the human subjects in real space, and 
                                                     Multiagent-based simulations use production rules to de-
a large number of agents controlled by evacuation scenarios 
                                                     scribe agent models for approximating human users, and 
in virtual space. In the actual experiment, ten to thirteen 
                                                     extended finite state automata to describe interaction models 
humans and three thousand agents per each time undertook 
                                                     among users and socially embedded systems. By analyzing 
the augmented experiment.  
                                                     the logs of participatory simulations or augmented experi-
 The location of human subjects in real space was projected 
                                                     ments, we can refine both agent and interaction models. The 
into virtual space based on their positions acquired by GPS. 
                                                     learning process of multiagent-based participatory design 
This map showed fires, blocks routes, and safe areas in real 
                                                     consists of two phases: the normal design loop and the par-
time. The human subjects could always get the latest map by 
                                                     ticipatory design loop.  
sending their location. The evacuation leaders assigned 
                                                       At first, scenario writers create agent and interaction mod-
evacuation destinations and directions to each evacuee 
                                                     els and verify their design by multiagent-based simulations. 
through the monitor screen shown in Figure 4. The leader 
                                                     This process is called the normal design loop and its detailed 
issued high level instructions to the evacuees using the map, 
                                                     steps can be found in a previous work [Murakami et al., 
and precise navigation instructions were automatically gen-
                                                     2003]. After verifying the agent and interaction models, 
erated for each evacuee. Dragging operations indicating a 
                                                     participatory simulations and augmented experiments are 
rectangular area enabled the leader to broadcast an an-
                                                     performed. The results of which may be different from those 
nouncement to a group of evacuees. 

                                                IJCAI-07
                                                  1344of multiagent-based simulations, because human behaviors 7 Related Work 
are often more variable than those of agents modeled by 
scenario writers. If this is the case, we first refine the agent There are two types of multiagent-based simulations and they 
models to better express the variation seen in human behavior. have different goals; a) analytic multiagent-based simula-
We then perform the multiagent-based simulations again tions often with simple agent and interaction models and b) 
with the new agent models. If the new simulations succeed to synthetic multiagent-based simulations sometimes with 
express various human behaviors but fail to produce the complex agent and interaction models. Analytic simulations 
expected results, we then refine the interaction models; i.e. have been used to model social systems [Epstein and Axtel, 
modify protocols, methods, rules, and/or laws. After the 1996]. Here, the KISS principle (Keep It Simple, Stupid) is 
improved interaction model is confirmed by multi-     often applied [Axelrod, 1997]. The KISS principle states that 
agent-based simulations, participatory simulation and aug- modeling should be simple even though the observed phe-
mented experiment take place again. This loop, called the nomenon is complex, and that complexity should be a result 
participatory design loop, is illustrated in Figure 5. of repeated interaction among agents and their environment. 
                                                      Hence, agents are often modeled with limited parameters. 
                            Start                     This approach is effective in the analysis of macro-micro 
                                                      links, the relationship between the macro properties of the 
 Normal          Refine agent and interaction models  entire system and the micro properties of the agents consti-
 Design                                               tuting the system.  
                    Multiagent-based simulation
 Loop        no                                        On the other hand, synthetic simulations are used to pro-
                   Simulation data = Expected results? duce realistic situations. Agent and interaction models can be 
                               yes                    complex reflecting the real world to make the simulation as 
            Participatory simulation / Augmented experiment realistic as possible. This approach is used in an early stage of 
                    Results of human participation    system development [Noda and Stone, 2003], and in educa-
                        = Expected results?
                                           yes        tion or training [Rickel and Johnson, 2000]. Since our pur-
                               no                     pose is to design socially embedded systems, our simulation 
                                              End
 Participatory         Refine agent models            is synthetic: our agent and interaction models can be complex 
 Design             Multiagent-based simulation       to reproduce realistic situations; after a participatory simula-
 Loop         no                                      tion, an augmented experiment is performed to understand 
                        Simulation data =
                    Results of human participation?   user behavior in a more realistic environment.  
                               yes                      Our work appears to follow various multiagent-based 
                     Refine interaction models        simulation papers including the RoboCup soccer simulator 
                                                      [Noda and Stone, 2003], but is clearly unique due to its focus 
                    Multiagent-based simulation
              no                                      on participatory design methodology. Drogoul proposed a 
                   Simulation data = Expected results? methodological process for developing multiagent-based 
                               yes                    simulations, and introduced the idea of the participatory 
           Figure 5. Participatory Design Loop        design of simulations [Drogoul et al., 2002], while we are 
                                                      proposing the participatory design for socially embedded 
It would seem, at first glance, that inductive machine learning systems. Bousquet applied role games to the modeling of 
technologies could be applied to refine agent and interaction social systems [Bousquet et al., 2002]. This paper, however, 
models. However, it is extremely rare to have a sufficient applies multiagent-based simulation to software system de-
number of human subjects to gather enough data for induc- sign, not social system analysis.  
tive learning: deductive methods with domain knowledge are We propose the use of participatory methodology includ-
more feasible [Torii et al., 2006]. To refine agent models, ing simulations and experiments for designing large-scale 
given that the training examples will be limited, explana- socially embedded systems. This methodology differs from 
                                                      the Gaia methodology [Zambonelli et al., 2003] or OperA 
tion-based learning is a reasonable approach. Though varia-
                                                      [Vázquez-Salceda et al., 2005], which focus on an early stage 
tions exist in human behavioral rules such as “escape from 
                                                      of designing multiagent systems mainly for enterprise ap-
crowds” and “follow crowds,” observing the behavior of plications [Jennings, 2000]. Our methodology can be applied 
human-controlled avatars can create a plausible explanation to designing ubiquitous/pervasive computing systems, but 
that selects one of the two rules.                    the resulting systems are not necessarily multiagent-based, 
  By combining machine learning technologies with inter- though multiagent systems are often natural solutions of 
views of human subjects, agent models have been success- socially embedded systems. 
fully refined in several domains [Murakami et al., 2005]. To 
refine interaction models, if we have enough candidate pro-
tocols, we can try them, and select the one that best fits. If we 8 Conclusion 
need to invent a new model, however, there is no available The waterfall model has been used in software development 
technology that can create such an interaction model. The for a long time. Given the increase in human-computer in-
visualization of simulations and experiments can, however, teraction, however, it has become essential to use the 
support scenario writers in inventing new models.     user-centered design approach when creating usable and 

                                                IJCAI-07
                                                  1345