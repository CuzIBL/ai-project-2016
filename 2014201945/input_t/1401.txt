                                  InterActive    Feature   Selection

         Hema   Raghavan∗                    Omid  Madani                       Rosie Jones
    University of Massachusetts               Yahoo!  Inc.                      Yahoo!  Inc.
       140  Governor’s Drive,        74 N  Pasadena Ave,  Pasadena,    74 N Pasadena  Ave, Pasadena,
     Amherst,  MA-01002,   USA              CA  91103  USA                    CA  91103  USA
        hema@cs.umass.edu               madani@yahoo-inc.com              jonesr@yahoo-inc.com

                    Abstract                          prior knowledge on features which can be used to accelerate
                                                      learning. For example, to ﬁnd documents on the topic cars in
    We  study the effects of feature selection and hu- traditional supervised learning the user would be required to
    man feedback on features in active learning set-  provide sufﬁcient examples of cars and non-cars documents.
    tings. Our experiments on a variety of text catego- However, this is not the only way in which the information
    rization tasks indicate that there is signiﬁcant po- need of a user looking for documents on cars can be satisﬁed.
    tential in improving classiﬁer performance by fea- In the information retrieval setting the user would be asked to
    ture reweighting, beyond that achieved via selec- issue a query, that is, state a few words (features) indicating
    tive sampling alone (standard active learning) if we her information need. Thereafter, feedback which may be at
    have access to an oracle that can point to the impor- a term or at a document level may be incorporated. In fact,
    tant (most predictive) features. Consistent with pre- even in document classiﬁcation, a user may use a keyword
    vious ﬁndings, we ﬁnd that feature selection based based search to locate the initial training examples. How-
    on the labeled training set has little effect. But our ever, traditional supervised learning tends to ignore the prior
    experiments on human subjects indicate that human knowledge that the user has, once a set of training examples
    feedback on feature relevance can identify a suf- have been obtained. In this work we try to ﬁnd a marriage
    ﬁcient proportion (65%) of the most relevant fea- between approaches to incorporating user feedback from ma-
    tures. Furthermore, these experiments show that   chine learning and information retrieval and show that active
    feature labeling takes much less (about 1/5th) time learning should be a dual process – at the term and at the
    than document labeling. We propose an algorithm   document-level. This has applications in email ﬁltering and
    that interleaves labeling features and documents  news ﬁltering where the user has some prior knowledge and
    which signiﬁcantly accelerates active learning.   a willingness to label some (as few as possible) documents in
                                                      order to build a system that suits her needs. We show that hu-
1  Introduction                                       mans have good intuition for important features in text clas-
                                                      siﬁcation tasks since features are typically words that are per-
A major bottleneck in machine learning applications is the ceptible to the human and that this human prior knowledge
lack of sufﬁcient labeled data for adequate classiﬁer perfor- can indeed accelerate learning.
mance, as manual labeling is often tedious and costly. Tech-
niques such as active learning, semi-supervised learning, and In summary, our contributions are: (1) We demonstrate
transduction have been pursued with considerable success in that access to a feature importance oracle can improve perfor-
reducing labeling requirements. In the standard active learn-
                                                      mance (F 1) signiﬁcantly over uncertainty sampling. (2) We
ing paradigm, learning proceeds sequentially, with the learn- show that even naive users can provide feedback on features
ing algorithm actively asking for the labels of instances from with about 60% accuracy of the oracle. (3) We show that the
a teacher. The objective is to ask the teacher to label the most relative manual costs of labeling features is about 1/5th that
informative instances in order to reduce labeling costs and of document feedback. (4) We show a method of simulta-
accelerate the learning. There has been very little work in neously soliciting class labels and feature feedback that im-
supervised learning in which the user (teacher) is queried on proves classiﬁer performance signiﬁcantly.
aspects other than class assignment of instances. In exper-
iments in this paper we study the beneﬁts and costs of fea-
ture feedback via humans on active learning. To this end we We describe the experimental setup in Sec. 2 and show
pick document classiﬁcation [Sebastiani, 2002] as the learn- how feature selection using an oracle is useful to active learn-
ing problem of choice because it represents a case of super- ing in Sec. 3. In Sec. 4 we show that humans can indeed iden-
vised learning which traditionally relies on example docu- tify useful features and show how human-chosen features can
ments as input for training and where users have sufﬁcient be used to accelerate learning in Sec. 5. We relate our work
                                                      to past work in Sec. 6 and outline directions for the future in
  ∗This work was done in part when the author was at Yahoo! Inc. section Sec. 7.2  Experimental    setup                              obtained by plotting F 1t(ACT ), Crand is the correspond-
Our test bed for this paper comes from three domains: ing curve using random sampling and CM is the straight line
(1) The 10 most frequent classes from the Reuters-21578 F 1t = F 1M then deﬁciency is the ratio of the area between
corpus (12902 documents). (2) The 20-Newsgroups corpus Cact and CM and the area between Crand and CM . The lower
(20000 documents from 20 Usenet newsgroups). (3) The ﬁrst the deﬁciency the better the active learning algorithm. We
10 topics from the TDT-2001 corpus (67111 documents in 3 aim to minimize deﬁciency and maximize F 1.
languages from broadcast and news-wire sources).
For all three corpora we consider each topic as a one versus 3 Oracle Feature Selection Experiments
all classiﬁcation problem. We also pick two binary classi- The oracle in our experiments has access to the labels of all
ﬁcation problems viz., Baseball vs Hockey and Automobiles P documents in U and uses this information to return a list of
vs Motorcycles from the 20-Newsgroups corpus. In all we the k most important features. We assume that the parameter
have 42 classiﬁcation problems.1 All the non-english stories k is input to the oracle. The oracle orders the k features in
in the TDT corpus were machine translated into English. As decreasing information gain order. Given a set of k features
features we use words, bigrams and trigrams obtained after we can perform active learning as discussed in the previous
stopping and stemming with the Porter stemmer in the Rain- section and plot Cact for each value of k.
bow Toolkit [McCallum, 1996]
  We use linear support vector machines (SVMs) and un-
certainty sampling for active learning [Scholkopf and Smola,
2002; Lewis and Catlett, 1994]. SVMs are the state of art
in text categorization, and have been found to be fairly ro-
bust even in the presence of many redundant and irrelevant
features [Brank et al., 2002; Rose et al., 2002]. Uncertainty
sampling [Lewis and Catlett, 1994] is a type of active learn-
ing in which the example that the user (teacher) is queried on
is the unlabeled instance that the classiﬁer is most uncertain
about. When the classiﬁer is an SVM, unlabeled instances
closest to the margin are chosen as queries [Tong and Koller,
2002]. The active learner may have access to all or a sub- Figure 1: Average F 1t(ACT ) for different values of k. k is
set of the unlabeled instances. This subset is called the pool the number of features and t is the number of documents.
and we use a pool size of 500 in this paper. The newly la-
beled instance is added to the set of labeled instances and the Figure 1 shows a plot of F 1t(ACT ) against number of
classiﬁer is retrained. The user is queried a total of T times. features k and number of labeled training examples t, for the
  The Deﬁciency metric [Baram et al., 2003] quantiﬁes the Earnings category in Reuters. The x, y and z axes denote k,
performance of the querying function for a given active learn- t and F 1 respectively. The number of labeled training exam-
ing algorithm. Originally deﬁciency was deﬁned in terms of ples t ranges from 2...42 in increments of 5. The number of
accuracy. Accuracy is a reasonable measure of performance features used for classiﬁcation k has values from 32 to 33718
when the positive class is a sizeable portion of the total. Since (all features). The dark dots represent the maximum Ft for
this is not the case for all the classiﬁcation problems we have each value of t, while the dark band represents the case when
chosen, we modify the deﬁnition of deﬁciency, and deﬁne it all features are used. This method of learning in one dimen-
in terms of the F 1 measure (harmonic mean of precision and sion is representative of traditional active learning. Clearly
recall [Rose et al., 2002]). Using notation similar to the orig- when the number of documents is few, performance is bet-
inal paper [Baram et al., 2003], let U be a random set of P ter when there is a smaller number of features. As the num-
labeled instances, F 1t(RAND) be the average F 1 achieved ber of documents increases the number of features needed to
by an algorithm when it is trained on t randomly picked ex- achieve best accuracy increases. From the ﬁgure it is obvious
amples and F 1t(ACT ) be the average F 1 obtained using t that we can get a big boost in accuracy by starting with fewer
actively picked examples. Deﬁciency D is deﬁned as:   features and then increasing the complexity of the model (the
                                                      number of relevent features) as the number of labeled docu-
            T
          Pt=init(F 1M (RAND)  − F 1t(ACT ))          ments increase.
   DT =    T                                    (1)     All 42 of our classiﬁcation problems exhibit behavior like
         P      (F 1 (RAND)   − F 1 (RAND))
           t=init  M               t                  that in Figure 1[Raghavan et al., 2005]. We report the aver-
                                                                      7
F 1M (RAND)  is the F 1 obtained with a large number (M) age deﬁciency, F 1 (F1 score with 7 labeled examples) and
of randomly picked examples.   For this paper we take F 122 in Fig. 2 to illustrate this point. The column labeled
M  =  1000 and t = 2, 7...42. When t = 2 we have one  Act shows performance using traditional active learning and
positive and one negative example. F 1t(•) is the average all the features. The column labeled Ora shows performance
F 1 computed over 10 trials. In addition to deﬁciency we re- obtained using a reduced subset of features using the Oracle.
port F 1t for some values of t. Intuitively, if Cact is the curve Intuitively, with limited labeled data, there is little evi-
                                                      dence to prefer one feature over another. Feature/dimension
  1
   http://www.daviddlewis.com/resources/testcollections/reuters21578/, http://kdd.ics.uci.edu/da - reduction (by the oracle) allows the learner to “focus” on di-
tabases/20newsgroups/20newsgroups.html, http://www.ldc.upenn.edu/Projects/TDT3/ mensions that matter, rather than being “overwhelmed” with                       Class ↓          D42          F 17         F 122     F 11000
                                   Act    Oracle Act    Ora    Act   Ora    Act
                       Reuters     0.421  0.319  0.345  0.446  0.569 0.621  0.727
                       20-News.    0.602  0.344  0.072  0.222  0.21  0.29   0.446
                       TDT         0.735  0.656  0.186  0.290  0.282 0.407  0.751
                       Bas. vs Hock 0.710 0.447  0.587  0.701  0.785 0.828  0.963
                       Auto vs Mot. 0.676 0.321  0.431  0.724  0.758 0.860  0.899

Figure 2: Improvements in deﬁciency, F 17 and F 122 using an oracle to select the most important features. Remember that the
objective is to minimize deﬁciency and maximize F 1. For each of the three metrics, ﬁgures in bold are statistically signiﬁcant
improvements over Uncertainty sampling using all features (the corresponding columns denoted by Act). When 1000 documents
are labeled (F 11000) using the entire feature set leads to better F 1 scores.

numerous dimensions right at the outset of learning. As problem we took the top 20 features as ranked by informa-
the number of labeled examples increases, feature selection tion gain on the entire labeled set. In this case we did not stem
becomes less important, as the learning algorithm becomes the data so that features remain as legitimate English words.
more capable of ﬁnding the discriminating hyperplane ( fea- We randomly mix these with features which are much lower
ture weights). We experimented with ﬁlter based methods for in the ranked list. We show each user one feature at a time
feature selection, which did not work very well (i.e., tiny or no and give them two options – relevant and not-relevant/don’t
improvements). This is expected given such limited training know. A feature is relevant if it helps discriminate the pos-
set sizes (see Fig. 3), and is consistent with most previous itive or the negative class. We measure the time it takes the
ﬁndings [Sebastiani, 2002]. Next we determine if humans user to label each feature. We do not show the user all the
can identify these important features.                features as a list, though this may be easier, as lists provide
                                                      some context and serve as a summary. Hence our method
4  Human    Labeling                                  provides an upper bound on the time it takes a user to judge
                                                      a feature. We compare this with the time it takes a user to
Consider our introductory example of a user who wants to judge a document. We measure the precision and recall of the
ﬁnd all documents that discuss cars. From a human perspec- user’s ability to label features. We ask the user to ﬁrst label
tive the words ‘car’, ‘auto’ etc may be important features in the features and then documents, so that the feature labeling
documents discussing this topic. Given a large number of process receives no beneﬁt due to the fact that the user has
documents labeled as on-topic and off-topic, and given a clas- viewed relevant documents. In the learning process we have
siﬁer trained on these documents, the classiﬁer may also ﬁnd proposed, though, the user would be labeling documents and
these features to be most relevant. With little labeled data features simultaneously, so the user would indeed be inﬂu-
(say 2 labeled examples) the classiﬁer may not be able to enced by the documents he reads. Hence our method is more
determine the discriminating features. While in general in stringent than the real case. We could in practice ask users to
machine learning the source of labels is not important to us, highlight terms as they read documents. Experiments in this
in active learning scenarios in which we expect the labels to direction have been conducted in information retrieval [Croft
come from humans we have valid questions to pose: (1) Can and Das, 1990].
humans label features as well as documents? (2) If the labels
people provide are noisy through being inconsistent, can we Our users were six graduate students and two employees
learn well enough? (3) Are features that are important to the of a company, none of whom were authors of this paper. Of
classiﬁer perceptible to a human?                     the graduate students, ﬁve were in computer science and one
  Our concern in this paper is asking people to give feedback from public health. All our users were familiar with the use of
on features, or word n-grams, as well as entire documents. computers. Five users understood the problem of document
We may  expect this to be more efﬁcient, since documents classiﬁcation but none had worked with these corpora. One of
contain redundancy, and results from our oracle experiments our users was not a native speaker of English. The topics were
indicate great potential. On the other hand, we also know distributed randomly, and without considering user expertise,
that synthetic examples composed of a combination of real so that each user got an average of 2-3 topics. There were
features can be difﬁcult to label [Baum and Lang, 1992]. overlapping topics between users such that each topic was
                                                      labeled by 2-3 users on average. A feedback form asking
4.1  Experiments  and Results                         the users some questions about the difﬁculty of the task was
In order to answer the above questions we conducted the handed out at the end.
following experiment. We picked 5 classiﬁcation problems We evaluated user feature labeling by calculating their av-
which we thought were perceptible to the average person on erage precision and recall at identifying the top 20 features as
the street and also represented the broad spectrum of prob- ranked by an oracle using information gain on the entire la-
lems from our set of 42 classiﬁcation problems. We took the beled set. Fig. 3 shows these results. For comparison we have
two binary classiﬁcation problems and from the remaining 40 also provided the precision and recall (against the same ora-
one-versus-all problems we chose three (earnings, hurricane cle ranking of top 20 features) obtained using 50 labeled ex-
Mitch and talk.politics.mideast). For a given classiﬁcation amples (picked using uncertainty sampling) denoted by @50. Class        Prec.        Rec.      Avg. Time (secs) 5.1  InterActive Learning Algorithm
 Problem   Hum.   @50   Hum.   @50   Feat. Docs
 Baseball.. 0.42  0.3   0.7    0.3   2.83  12.6       Let documents be represented as vectors Xi = xi1...xi|F |,
 Auto vs ... 0.54 0.25  0.81   0.25  3.56  19.84      where |F | is the total number of features. At each iteration
 Earnings  0.53   0.2   0.66   0.25  2.97  13         the active learner not only queries the user on an uncertain
 ...mideast 0.68  0.35  0.55   0.35  2.38  12.93      document, but also presents a list of f features and asks the
 ...Mitch  0.716  0.65  0.56   0.65  2.38  13.19      user to label features which she considers relevant. The fea-
 Average   0.580  0.35  0.65   0.38  2.82  14.31      tures to be displayed to the user are the top f features ob-
                                                      tained by ordering the features by information gain. To ob-
Figure 3: Ability of users to identify important features. Pre- tain the information gain values with t labeled instances we
cision and Recall against an oracle, of users (Hum.) and an trained a classiﬁer on these t labeled instances. Then to com-
active learner which has seen 50 documents(@50). Average pute information gain, we used the 5 top ranked (farthest from
labeling times for features and documents are also shown. All the margin) documents from the unlabeled set in addition to
numbers are averaged over users.                      the t labeled documents. Using the unlabeled data for term
                                                      level feedback is very common in information retrieval and is
                                                      called pseudo-relevance feedback [Salton, 1968].
Precision and Recall of the humans is high, supporting our The user labels some of the f features which he considers
hypothesis that features that a classiﬁer ﬁnds to be relevant af-
                                                      discriminative features. Let ~s = s1...s|F | be a vector con-
ter seeing a large number of labeled instances are obvious to a taining weights of relevant features. If a feature number i
human after seeing little or no labeled data (the latter case be- that is presented to the user is labeled as relevant then we
ing true of our experiments). Additionally the Precision and
                                                      set si = a, otherwise si = b, where a and b are parameters
Recall @50 is signiﬁcantly lower than that of humans, indi- of the system. The vector ~s is noisier than the real case be-
cating that a classiﬁer like an SVM needs to see much more cause in addition to mistakes made by the user we lose out on
data before it can ﬁnd the discriminatory features.   those features that the user might have considered relevant,
  The last column of Fig. 3 shows time taken for labeling had he been presented that feature when we were collecting
features and documents. On average humans require about 5 relevance judgments for features. In a real life scenario this
times longer to label documents than to label features. Note might correspond to the lazy user who labels few features
that features may be even easier to label if they are shown in as relevant and leaves some features unlabeled in addition to
context – as lists, with relevant passages etc. There are sev- making mistakes. If a user had labeled a feature as relevant in
eral other metrics and points of discussion such as user ex- some past iteration we don’t show the user that feature again.
pertise, time taken to label relevant and non-relevant features
                                                        We  incorporate the vector ~s as follows. For each Xi in
and so on, which we reserve for future work. One impor-
                                                      the labeled and unlabeled sets we multiply xij by sj to get
tant consideration though, is that document length inﬂuences X0 . In other words we scale all relevant features by a and
document labeling time. We found the two to be correlated ij
                                                      non-relevant features by b. We set a = 10 and b = 1. 2
by r = 0.289 which indicates a small increase in time for a
large increase in length. The standard deviations for precision By scaling the important features by a we are forcing the
and recall are 0.14 and 0.15 respectively. Different users vary classiﬁer to assign higher weights to these features. We
signiﬁcantly in precision, recall and the total number of fea- demonstrate this with the following example. Consider a lin-
tures labeled relevant. Based on feedback on a post-labeling ear SVM, |F | = 2 and 2 data points X1 = (1, 2) and X2 =
survey we are inclined to believe that this is due to individual (2, 1) with labels +1 and −1 respectively. An SVM trained
caution exercised during the labeling process.        on this input learns a classiﬁer with w = (−0.599, +0.599).
  Some of the highlights of the post-labeling survey are as Thus both features are equally discriminative. If feature 1 is
                                                      considered more discriminative by a user, then by our method
follows. On average users found the ease of labeling features 0        0              0
to be 3.8 (where 0 is most difﬁcult and 5 is very easy) and X1 = (10, 2) and X2 = (20, 1) and w = (0.043, −0.0043),
documents 4.2. In general users with poor prior knowledge thus assigning higher weight to f1. Now, this is a “soft” ver-
found the feature labeling process very hard, although as we sion of the feature selection mechanism of Sec. 3. But in that
will show, their labels were extremely useful to the classi- case the Oracle knew the ideal set of features; we can view
ﬁer. The average expertise (5=expert) was 2.4, indicating that those set of experiments as a special case where b = 0. We
most users felt they had little domain knowledge for the tasks expect that human labels are noisy and we do not want to
they were assigned. We now proceed to see how to use fea- zero-out potentially relevant features.
tures labeled as relevant by our naive users in active learning.
                                                      5.2  Experiments  and Results
5  A  Human    in the Loop                            To make our experiments repeatable (to compute average per-
                                                      formance and for convenience) we simulate user interaction
We saw in Sec. 3 that feature selection coupled with uncer- as follows. For each classiﬁcation problem we maintain a list
tainty sampling gives us big gains in performance when there of features that a user might have considered relevant had he
are few labeled examples. In Sec. 4 we saw that humans can
discern discriminative features with reasonable accuracy. We 2We picked our algorithm’s parameters based on a preliminary
now describe our approach of applying term and document test on 3 topics (baseball, earnings, and acquisitions) using the oracle
level feedback simultaneously in active learning.     features of Sec. 3.been presented that feature. For these lists we used the judg- a pool of unlabeled data) alone [Cohn et al., 1994]. Al-
ments obtained in Sec. 4. Thus for each of the 5 classiﬁcation though query-based learning can be very powerful in theory
problems we had 2-3 such lists, one per user who judged that [Angluin, 1992], arbitrary queries may be difﬁcult to answer
topic. For the 10 TDT topics we have topic descriptions as in practice [Baum and Lang, 1992], hence the popularity of
provided by the LDC, and we simulated explicit human feed- pool-based methods, and the motivation for studying the ef-
back on feature relevance as follows: The topic descriptions fectiveness and ease of predictive feature identiﬁcation by
contain names of people, places and organizations that are humans in our application area. That human prior knowl-
key players in this topic in addition to other keywords. We edge can accelerate learning has been investigated by [Paz-
used the words in these topic descriptions as the list of rele- zani and Kibler, 1992], but our work differs in techniques
vant features. Now, given these lists we can perform the sim- (they use prior knowledge to generate horn-clause rules) and
ulated HIL (Human in the Loop) experiments for 15 classiﬁ- applications. [Beineke et al., 2004] uses human prior knowl-
cation problems. At each iteration f features are shown to the edge of co-occurrence of words to improve classiﬁcation of
user. If the feature exists in the list of relevant features, we set product reviews. None of this work, however, considers the
the corresponding bit in ~s and proceed with the active learn- use of prior knowledge in the active learning setting. Our
ing as in Sec. 5.1. Fig. 4 shows the performance of the HIL work is unique in the ﬁeld of active learning as we extend
experiments. As before we report deﬁciency, F 17 and F 122. the query model to include feature as well as document level
As a baseline we also report results for the case when the top feedback. Our study of the human factors (such as quality
20 features as obtained by the information gain oracle are in- of feedback and costs) is also a major differentiating theme
put to the simulated HIL experiments (this represents what between our work and previous work in incorporating prior
a user with 100% precision and recall would obtain by our knowledge which did not address this issue, or might have
method). The Oracle is (as expected) much better than plain assumed experts in machine learning taking a role in train-
uncertainty sampling, on all 3 measures, reinforcing our faith ing the system [Schapire et al., 2002; Wu and Srihari, 2004;
in the algorithm of Sec. 5.1. The performance of the HIL ex- Godbole et al., 2004]. We only assume knowledge about the
periments is almost as good as the Oracle, indicating that user topic of interest. Our algorithmic techniques and the studied
input (although noisy) can help improve performance signiﬁ- modes of interaction differ and are worth further comparison.
cantly. The only relative poor performance for the HIL simu- In both [Wu and Srihari, 2004; Schapire et al., 2002], prior
lation is on the average D42 measure for the TDT categories, knowledge is given at the outset which leads to a “soft” label-
where we used all the words from the topic descriptions as a ing of the labeled or unlabeled data that is incorporated into
proxy for explicit human feedback on features. The plot on training via modiﬁed boosting or SVM training. However,
the right is of F 1t(HIL) for hurricane Mitch. As a compar- in our scheme the user is labeling documents and features si-
ison F 1t(ACT ) is shown. The HIL values are much higher multaneously. We expect that our proposed interactive mode
than for plain uncertainty sampling.                  has an advantage over requesting prior knowledge from the
  We also observed that relevant features were usually spot- outset, as it may be easier for the user to identify/recall rele-
ted in very early iterations. For the Auto vs Motorcycles prob- vant features while labeling documents in the collection and
lem, the user has been asked to label 75% (averaged over mul- being presented with candidate features. The work of [God-
tiple iterations and multiple users) of the oracle features at bole et al., 2004] puts more emphasis on system issues and
some point or the other. The most informative words (as de- focuses on multi-class training rather than a careful analy-
termined by the Oracle) – car and bike are asked of the user sis of effects of feature selection and human efﬁcacy. Their
in very early iterations. The label for car is always (100% proposed method is attractive in that it treats features as sin-
of the times) asked, and 70% of the time the label for this gle term documents that can be labeled by humans, but they
word is asked of the user in the ﬁrst iteration itself. This is also study labeling features before documents (and only in
closely followed by the word bike which the user is queried an “oracle” setting, i.e., not using actual human annotators),
on within the ﬁrst 5 iterations 80% of the time. Most relevant and do not observe much improvements using their particu-
features are queried within 10 iterations which makes us be- lar method over standard active learning in the single domain
lieve that we can stop feature level feedback in 10 iterations (Reuters) they test on.
or so. When to stop asking questions on both features and
documents and switch entirely to documents remains an area 7 Conclusions and Future Work
for future work.
                                                      We showed experimentally that for learning with few labeled
6  Related  Work                                      examples, good feature selection is extremely useful. As the
                                                      number of examples increases, the vocabulary (feature set
Our work is related to a number of areas including query size) of the system also needs to increase. A teacher, who is
learning, active learning, use of (prior) knowledge and feature not knowledgeable in machine learning, can help accelerate
selection in machine learning, term-relevance feedback in in- training the system in this early stage, by pointing out poten-
formation retrieval, and human-computer interaction, from tially important words. We also conducted a user study to see
which we can cite only a few.                         how well naive users performed as compared to a feature ora-
  Our proposed method is an instance of query-based learn- cle. We used our users’ outputs in realistic human in the loop
ing and an extension of standard (“pool-based”) active learn- experiments and found signiﬁcant increase in performance.
ing which focuses on selective sampling of instances (from This paper raises the question of what questions (other than