             Learning discontinuities for switching between local models                  ‚àó

                               Marc Toussaint   and  Sethu Vijayakumar
                              Institute of Perception, Action and Behavior
                             School of Informatics, University of Edinburgh
                    The King‚Äôs Buildings, MayÔ¨Åeld Road, Edinburgh EH9 3JZ, UK.
                          mtoussai@inf.ed.ac.uk, sethu.vijayakumar@ed.ac.uk

1  Introduction                                       by at least one model. The problem of predicting which par-
                                                      ticular model œÜ is responsible for a given input x is solved
Locally weighted learning techniques, in particular LWPR           i
[                     ]                               on a higher level as explained in the next section. In formal
Vijayakumar et al., 2002 , have successfully been used for notations, we assume a mixture model
high-dimensional regression problems. Their robustness and
                                                                          N
efÔ¨Åcient online versions are crucial in robotic domains where,           X                      U
for instance, an inverse model of an articulated dynamic robot P (y|x) = (1 ‚àí ) Ploc(i|x) P (y|i, x) +  (y) ,
has to be learned in real-time. Such models map a high-                  i=1
dimensional state (e.g., joint angles and velocities) and a de-              1         |y ‚àí œÜ (x)|2 
                                                                P (y|i, x) = ‚àö   exp  ‚àí      i       ,
sired change of state to the required motor signals (torques).              2œÄ œÉ           2 œÉ2
  While typically such mappings are assumed to be smooth,
                                                            U(y)
in real world scenarios, there are many interesting cases where  is a uniform distribution accounting for back-
                                                                                 i
where the functions of interest are truly discontinuous. Some ground noise (e.g. outliers) and is the hidden variable spec-
examples include contacts with other objects (in particular the ifying the particular model that generates a datum. Since we
ground), with other parts of the body, or with ‚Äújoint limits‚Äù. In aim for localized models, we impose a locality constraint al-
                                                                                   c
fact, many interesting interactions with the environment man- ready at this level as follows: Let i denote the mean input
                                                                           œÜ
ifest themselves through discontinuities in the sensorimotor (center) on which model i has been trained on. For a given
                                                           x    i
data.                                                 input , the th model is eligible if and only if there does not
                                                      exist a jth model which has its center ‚Äúbetween‚Äù c and x.
  In this paper, we show how discontinuous switching be-                                          i
                                                      More precisely,
tween local regression models can be learned. The general
topic of switching models has been discussed before, e.g., Ploc(i|x) = 0 ‚áê‚áí ‚àÉj : hx ‚àí cj, ci ‚àí cji < 0 ,
in the context of state space models [Ghahramani and Hin- where h¬∑, ¬∑i is the scalar product in input space:
ton, 1998; Pavlovic et al., 2000] or multiple inverse mod-        i is eligible for x i is not eligible for x

els [Wolpert and Kawato, 1998]. Generally, the question of             cj          cj
which particular model receives responsibilities for a given                              x
input can be modeled as a hidden variable i in a generative             x
mixture model. In our case, we assume that the responsibil-       ci              ci
ity index i can be predicted from the input (the robot state). Further, Ploc(i|x) is uniform over all eligible i‚Äôs. Given a
Thus, inferring a model for i corresponds to classifying the current family of models, we can infer a posterior on the
input domain into regions for each sub-model.         responsibility index i for a given datum (x, y), using Bayes
  Since in robotic domains, local learning is crucial to pre- rule:
                                                                            1
vent interference and allow for online adaptation techniques,    P (i|y, x) = P (y|i, x) Ploc(i|x).
we propose a model of the responsibility index i which is it-              Z
self a composition of local classiÔ¨Åers. Multiple pairwise clas- Calculating the MAP assignment ÀÜi allows us to associate ev-
siÔ¨Åers are concatenated to construct a complete model in the ery training datum with the most likely model. Using this, the
form of a product-of-sigmoids, which is capable of learning sufÔ¨Åcient statistics of each local model œÜi is updated. Fur-
complex, sharply bounded domains for each local model in ther, the data that is labeled as ‚Äúyet unmodeled‚Äù (which is
lieu of the typical Gaussian kernels.                 inferred to have been generated by U(y)) is used to generate
                                                      a new family member by the following heuristic (compare
2  Learning a family of models                        RANSAC): A random datum  (x, y) is selected from the un-
                                                      modeled data; the K closest neighbors of (x, y) (w.r.t., Eu-
                         M
Given training data {(xk, yk)}k=1 with inputs xk and outputs clidean input distance) are chosen as initial training data for
yk, the Ô¨Årst level goal of our algorithm is to learn a family of the new model, where K is a random Poisson number with
models {œÜ1, .., œÜN } such that every datum can be explained mean 3 d (here, d is the input dimensionality). Finally, mod-
                                                      els that receive too few MAP responsibilities (less than 10 d
  ‚àóThe Ô¨Årst author acknowledges support by the German Research in our experiments) are discarded. This iterative process can
Foundation (DFG), Emmy Noether fellowship TO 409/1-1. be repeated until no new models are generated.Figure 1: Kernels that can be represented as a product of sig-
moids in 1D and 2D.

  This general scheme of family learning can be realized
with any type of models œÜi. In the experiments, we will
choose œÜi to be linear functions, learned with Partial Least
Squares (PLS) regression. PLS, involving an intermediate
lower-dimensional projection, has been proven efÔ¨Åcient for
high-dimensional problems [Vijayakumar et al., 2002].
3  Products of sigmoids for switching
On the second level of our algorithm, the goal is to learn a pre-
dictive model P (i|x) of the latent responsibility index i that Figure 2: (a) A 1D test function with d=1, L=10, œÉ=0.1.
is more precise than the uninformed prior Ploc(i|x). Given Learned switching model after 20 iterations on M=1000
some data, it is easy to decide whether two models are ‚Äúpo- training data points. (b) The blended switching model:
                                                             P
tentially neighbored‚Äù‚Äînamely whether there exists data for y(x) = i Œ≤i(x) œÜi(x) compared to LWPR. (c) Family er-
which both models are eligible‚Äîbased on their centers. For ror (cf. Sec. 4) and (d) classiÔ¨Åcation error for 10 runs on ran-
each pair (ij) of neighbored models, we learn a sigmoidal dom test functions with d=10, L=10, œÉ=0.1, and M=10 000.
function œàij(x), where œàij ‚â° 1 ‚àí œàji. The product of such The bold line is the average over all curves.
sigmoids around a submodel i deÔ¨Ånes a coefÔ¨Åcient Œ≤i(x) that
we associate with the submodel for a given input x,
                                                      5   Discussion
         1  Y                           1
 Œ≤i(x) =       œàij(x) , œàij(x) =                 .    The presented model addresses the problem of handling the
         Z0                      1 + exp[‚àíœÜ  (x)]
             j                             ij         discontinuities that naturally arise, e.g., in sensorimotor data
                                                      during interaction with a structured environment. Our model
Here, Z0 normalizes Œ≤ over i. As indicated, we represent
                   i                                  extends earlier local learning approaches in several ways:
sigmoids œà with a scalar function œÜ . Fig. 1 illustrates the
         ij                    ij                     The responsibility region associated with each local model
kind of kernels can be represented as products of sigmoids.
                                                      (learned with the product of sigmoids) has a much more ver-
  The sigmoids œà (x) are meant to represent the likelihood
               ij                                     satile boundary shape compared to typical Gaussian kernels.
that a model i rather than j is responsible for an input x,
                                                      Problems associated with initialization of kernel shapes or
conditioned on that either i or j is responsible. The product
                                                      widths and the heuristic choice of an ad hoc number of sub-
combination is comparable to an AND voting. The MAP la-
     ÀÜ                                                models are circumvented by the robust incremental allocation
beling i we introduced in the previous section is now used to of new models. Although we consistently used PLS as the
train these sigmoids. In the experiments, we consider œÜij to underlying regression machinery, the general model allows
be linear functions, again learned with PLS.          to utilize any efÔ¨Åcient single model learner to represent the
                                                      local models œÜi as well as the classiÔ¨Åer functions œÜij. Future
4  Experiments                                        work will in particular investigate non-linear learners for the
We tested the algorithm on piecewise linear, discontinuous local models as well as the boundary classiÔ¨Åers.
test functions. A test function has 3 parameters: the in-
put dimension d, the number L of linear pieces it is com- References
                          œÉ
posed of and the output noise . The localities, slopes and [Ghahramani and Hinton, 1998] Z. Ghahramani and G.E.
boundaries of the linear pieces are sampled randomly. Fig. Hinton. Variational learning for switching state-space
2(a,b) display learning results from a 1D example in com- models. Neural Computation, 12:963‚Äì996, 1998.
parison to LWPR. Fig. 2(c,d) display two error curves on
10-dimensional test functions over the rather large input do- [Pavlovic et al., 2000] Vladimir Pavlovic, James M. Rehg,
main [‚àí1, 1]10. The family error is the MSE of the best  and John MacCormick. Learning switching linear mod-
                                                         els of human motion. In NIPS, pages 981‚Äì987, 2000.
Ô¨Åtting eligible model œÜÀÜi (averaged over an independent test
data set); the classiÔ¨Åcation error counts how often the prod- [Vijayakumar et al., 2002] Sethu Vijayakumar, Aaron
uct of sigmoids correctly predicts œÜÀÜi to be the best Ô¨Åtting D‚ÄôSouza, Tomohiro Shibata, Jorg Conradt, and Ste-
                                     ÀÜ
model for a given input (i.e., argmaxiŒ≤i = i). In the exper- fan Schaal. Statistical learning for humanoid robots.
iments we Ô¨Ånd that the algorithm reliably generates a fam- Autonomous Robot, 12:55‚Äì69, 2002.
                                         2
ily with optimal family error at the noise level (œÉ = 0.01). [Wolpert and Kawato, 1998] D.M. Wolpert and M. Kawato.
In 5 dimensions (not displayed here) the classiÔ¨Åcation error Multiple paired forward and inverse models for motor con-
rapidly converges to zero while in 10 dimensions, the classi- trol. Neural Networks, 11:1317‚Äì1329, 1998.
Ô¨Åcation error converges to around 4%. For more results see
homepages.inf.ed.ac.uk/mtoussai/projects/05-ijcai.