             Transferring Learned Control-Knowledge between Planners                       ∗

                        Susana Fernandez,´   Ricardo Aler   and  Daniel Borrajo
                                   Universidad Carlos III de Madrid.
                         Avda. de la Universidad, 30. Leganes´ (Madrid). Spain
                    email:{susana.fernandez, ricardo.aler, daniel.borrajo}@uc3m.es


                    Abstract                          is by means of machine learning. ML techniques applied to
                                                      planning range from macro-operators acquisition, case-based
    As any other problem solving task that employs    reasoning, rewrite rules acquisition, generalized policies, de-
    search, AI Planning needs heuristics to efﬁciently ductive approaches of learning heuristics (EBL), learning do-
    guide the problem-space exploration. Machine      main models, to inductive approaches (based on ILP) (see [10]
    learning (ML) provides several techniques for au- for a detailed account).
    tomatically acquiring those heuristics. Usually, a
                                                        Despite the current advance in planning algorithms and
    planner solves a problem, and a ML technique gen-
                                                      techniques, there is no universally superior strategy for plan-
    erates knowledge from the search episode in terms
                                                      ning in all planning problems and domains. Instead of imple-
    of complete plans (macro-operators or cases), or
                                                      menting an Universal Optimal Planner, we propose to obtain
    heuristics (also named control knowledge in plan-
                                                      good domain-dependent heuristics by using ML and different
    ning). In this paper, we present a novel way of
                                                      planning techniques. That is, to learn control knowledge for
    generating planning heuristics: we learn heuristics
                                                      each domain from the planning paradigms that behave well
    in one planner and transfer them to another plan-
                                                      in this domain. In the future, this could lead to the creation of
    ner. This approach is based on the fact that dif-
                                                      a domain-dependent control-knowledge repository that could
    ferent planners employ different search bias. We
                                                      be integrated with the domain descriptions and used by any
    want to extract knowledge from the search per-
                                                      planner. This implies the deﬁnition of a representation lan-
    formed by one planner and use the learned knowl-
                                                      guage or extension to the standard PDDL3.0, compatible both
    edge on another planner that uses a different search
                                                      with the different learning systems that obtain heuristics, and
    bias. The goal is to improve the efﬁciency of the
                                                      also with the planners that use the learned heuristics.
    second planner by capturing regularities of the do-
    main that it would not capture by itself due to     In this paper we describe a ﬁrst step in this direction by
    its bias. We employ a deductive learning method   studying the possibility of using heuristics learned on one
    (EBL) that is able to automatically acquire control speciﬁc planner for improving the performance of another
    knowledge by generating bounded explanations of   planner that has different problem-solving biases. Although
    the problem-solving episodes in a Graphplan-based each planner uses its own strategy to search for solutions,
    planner. Then, we transform the learned knowledge some of them share some common decision points, like, in
    so that it can be used by a bidirectional planner. the case of backward-chaining planners, what operator to
                                                      choose for solving a speciﬁc goal or what goal to select
                                                      next. Therefore, learned knowledge on some type of de-
1  Introduction                                       cision can potentially be transferred to make decisions of
                                                      the same type on another planner. In particular, we have
Planning can be described as a problem-solving task that
                                                      studied control knowledge transfer between two backward-
takes as input a domain theory (a set of states and operators)
                                                      chaining planners: from a Graphplan-based planner, TGP [7],
and a problem (initial state and set of goals) and tries to ob-
                                                      to a state-space planner, IPSS, based on PRODIGY [9].We
tain a plan (a set of operators and a partial order of execution
                                                      have used PRODIGY given that it can handle heuristics rep-
among them) such that, when executed, this plan transforms
                                                      resented in a declarative language. Heuristics are deﬁned as
the initial state into a state where all the goals are achieved.
                                                      a set of control rules that specify how to make decisions in
Planning has been shown to be PSPACE-complete [3]. There-
                                                      the search tree. This language becomes our starting heuristic
fore, redeﬁning the domain theory and/or deﬁning heuristics
                                                      representation language. Our goal is to automatically gen-
for planning is necessary if we want to obtain solutions to real
                                                      erate these control rules by applying ML in TGP and then
world problems efﬁciently. One way to deﬁne these heuristics
                                                      translate them into IPSS control-knowledge description lan-
  ∗This work has been partially supported by the Spanish MCyT guage. We have implemented a deductive learning method
under project TIC2002-04146-C05-05, MEC project TIN2005- to acquire control knowledge by generating bounded expla-
08945-C06-05, and CAM-UC3M project UC3M-INF-05-016.   nations of the problem-solving episodes on TGP. The learn-

                                                 IJCAI07
                                                  1885                                                                             m    m
ing approach builds on HAMLET [2], an inductive-deductive • an initial meta-state, s0 ∈ S
system that learns control knowledge in the form of control                             m     m
                                                        • a set of non-empty terminal states ST ⊆ S
rules in PRODIGY, but only on its deductive component, that
                                                        • a set of search operators Am, such as “apply an instan-
is based on EBL [6].                                                       d                  d
                                                          tiated action a ∈ Aσ to the current state s ”, or “select
  As far as we know, our approach is the ﬁrst one that is able                ∈   d
to transfer learned knowledge between two planning tech-  an instantiated action a A for achieving a given goal
                                                          gd”. These operators will be instantiated by bounding
niques. However, transfer learning has been successfully ap-                                    m
plied in other frameworks. For instance, in Reinforcement their variables (e.g. which action to apply), Aσ
                                                                                                 m
Learning, macro-actions or options obtained in a learning task • a function mapping non-terminal meta-states s and in-
                                                                            m                m   m  m
can be used to improve future learning processes. The knowl- stantiated operators aσ into a meta-state F (a ,s ) ∈
                                                            m
edge transferred can range from value functions [8] to com- S
plete policies [5].                                     From a perspective of heuristic acquisition, the key issue
  The paper is organized as follows. Next section provides consists of learning how to select instantiated operators of the
background on the planning techniques involved. Section 3 meta problem-space given each meta-state. That is, the goal
describes the implemented learning system and the learned will be to learn functions that map a meta-state sm into a set
                                               TGP                                          m
control rules. Section 4 describes the translation of the of instantiated search operators: H(sm) ⊆ Aσ . This is due
learned rules to be used in IPSS. Section 5 shows some ex- to the fact that the decisions made by the planner (branching)
perimental results. Finally, conclusions are drawn.   that we would like to guide are precisely on what instanti-
                                                      ated search operator to apply at each meta-state. Thus, they
2  Planning models and techniques                     constitute the learning points, from our perspective. In this
In this section we ﬁrst describe an uniﬁed planning model paper, we learn functions composed of a set of control rules.
                                                      Each rule will have the format: if conditions(sm)
for learning. Then, we describe the planners IPSS and TGP in           m
terms of that uniﬁed model. Finally, we describe our proposal then select Aσ . That is, if certain conditions on the
to transfer control knowledge from TGP to IPSS.       current meta-state hold, then the planner should apply the cor-
                                                      responding instantiated search operator.
2.1  Uniﬁed planning model for learning
                                                      2.2  IPSS planner
In order to transfer heuristics from one planner to another, we
needed to (re)deﬁne planning techniques in terms of a uniﬁed IPSS is an integrated tool for planning and scheduling that
model, that accounts for the important aspects of planning provides sound plans with temporal information (if run in
from a perspective of learning and using control knowledge. integrated-scheduling mode). The planning component is a
On the base level, we have the domain problem-space P d nonlinear planning system that follows a means-ends analy-
deﬁned by (modiﬁed with respect to [1] and focusing only sis (see [9] for details). It performs a kind of bidirectional
on deterministic planning without costs):             depth-ﬁrst search (subgoaling from the goals, and executing
                                                      operators from the initial state), combined with a branch-
  •                            d
    a discrete and ﬁnite state space S ,              and-bound technique when dealing with quality metrics. The
                 d    d
  • an initial state s0 ∈ S ,                         planner is integrated with a constraints-based scheduler that
  • a set of goals Gd, such that they deﬁne a set of non- reasons about time and resource constraints.
    empty terminal states as the ones in which all of them In terms of the uniﬁed model, it can be described as:
            d     d    d   d    d                       •                m         { d   d    d  d  d   }
    are true ST = {s ∈ S | G ⊆ s },                       each meta-state s is a tuple s ,Gp,L,g ,a ,aσ,P
                                                                 d                           d
  • a set of non-instantiated actions Ad, and a set of instan- where s is the current domain state, Gp is the set of
                 d                                        pending (open) goals, L is a set of assignments of in-
    tiated actions Aσ, and                                                         d
                                      d                   stantiated actions to goals, g is the goal in which the
  • a function mapping non-terminal states s and instanti-                     d
               d            d  d  d     d                 planner is working on, a is an action that the planner
    ated actions aσ into a state F (aσ,s ) ∈ S .                                  d  d
                                                          has selected for achieving g , aσ is an instantiated ac-
                      d      d  d  d                                                                d
  We assume that both Aσ and F (a ,s ) are non-empty.     tion that the planner has selected for achieving g , and
On top of this problem space, each planner searches in what P is the current plan for solving the problem
                               m                                             m      d  d
we will call meta problem-space, P . Thus, for our pur- • the initial meta-state, s0 = {s0,G ,,,,,}
poses, we deﬁne the components of this problem space as:                                           m
                                                        • a  terminal state will be of the  form  sT   =
  •                              m                          d                      d     d
    a discrete and ﬁnite set of states, S : we will call each {s ,,LT ,,,,P} such that G ⊆ s , LT will be the
         m  ∈   m                                           T                            T
    state s    S  a meta-state. Meta-states are planner   causal links between goals and instantiated actions, and
    dependent and include all the knowledge that the plan- P is the plan
    ner needs for making decisions during its search process.                     m
                                             m          • the set of search operators A is composed of (given the
    For forward-chaining planners each meta-state s will                  { d  d     d  d  d  }
    contain only a state sd of the domain problem-space P d. current meta-state s ,Gp,L,g ,a ,aσ,P ): “select a
                                                               d ∈   d                 d ∈  d
    In the case of backward-chaining planners, as we will goal g   Gp”, “select an action a A for achieving
                                                                         d                        d     d
    see later, they can include, for instance, the current state the current goal g ” , “select an instantiation aσ ∈ Aσ
    sd, the goal the planner is working on gd ∈ Gd, or a set of current action ad”, and “apply the current instantiated
                                                                 d                 d
    of assignments of actions to goals                    action aσ to the current state s ”

                                                 IJCAI07
                                                  1886                                                                         d     d
  In IPSS,asinPRODIGY, there is a language for deﬁn-      “for each goal g ∈ Gp  select an instantiated action
                      (  )                                 d     d
ing heuristic function H sm in terms of a set of con-     aσ ∈ Aσ for achieving it”. If the goal persists, the goal
trol rules. Control rules can select, reject,orprefer al-                  d
                                                m         will still be in the Gp of the successor meta-state. Other-
ternatives (ways of instantiating search operators, Aσ ).                             d             d
                                                          wise, the preconditions of each aσ are added to Gp and
The conditions of control rules refer to queries (called        d                d             ( d  d)
meta-predicates) to the current meta-state of the search  each g is removed from Gp. Also, the links aσ,g are
 m       d  d    d  d  d                                  added to L.
s   =  {s ,Gp,L,g ,a ,aσ,P}.PRODIGY     already pro-
vides the most usual meta-predicates, such as knowing
                                                      2.4  Transferring heuristics from TGP to IPSS
whether: some literal l is true in the current state l ∈ sd
(true-in-state), some literal l is the current goal l = gd Our proposal to transfer control knowledge between two
(current-goal), some literal is a pending goal ∈ d    planners P1 and P2 requires ﬁve steps: (1) a language
                            l               l   Gp                                          ( m)
(some-candidate-goals) or some instance is of a given for representing heuristics (or function H1 s ) for P1;
type (type-of-object). But, the language for represent- (2) a method for automatically generating H1 from P1
ing heuristics also admits coding user-speciﬁc functions. search episodes; (3) a translation mechanism between meta
                                                      problem-space of P1 and meta problem-space of P2, which
2.3  TGP planner                                      is equivalent to providing a translation mechanism between
TGP is a temporal planner that enhances Graphplan algorithm H1 and H2; (4) a language H2 for representing heuristics for
to handle actions of different durations [7]. Again, we will P2; and (5) an interpreter (matcher) of H2 in P2. Given that
only use in this paper its planning component, and we leave IPSS already has a declarative language to deﬁne heuristics,
working with the temporal part for future work.       HIpss (step 4), and an interpreter of that language (step 5),
  The planner alternates between two phases: graph expan- we had to deﬁne the ﬁrst three steps. In relation to step 1, it
sion, it extends a planning graph until the graph has achieved had to be a language as close as possible to the one used in
necessary (but potentially insufﬁcient) conditions for plan IPSS, so that step 3 could be simpliﬁed. Therefore, we built
existence; and solution extraction, it performs a backward- it using as many meta-predicates as possible from the ones in
chaining search, on the planning graph, for a solution; if no IPSS declarative language. We will now focus on steps 2 and
solution is found, the graph is expanded again and a new so- 3, and include some hints on that language.
lution extraction phase starts.
  When TGP performs a backward-chaining search for a plan 3 The learning technique
it chooses an action that achieves each goal proposition. If it In this section, we describe the ML technique we built, based
is consistent (nonmutex ) with all actions that have been cho- on EBL, to generate control knowledge from TGP.Wehave
sen so far, then TGP proceeds to the next goal; otherwise it called it GEBL (Graphplan EBL). It generates explanations
chooses another action. An action achieves a goal if it has the for the local decisions made during the search process (in
goal as effect. Instead of choosing an action to achieve a goal the meta problem-space). These explanations become control
at one level, TGP can also choose to persist the goal (selecting rules. In order to learn control knowledge for a Graphplan-
the no-op action); i.e. it will achieve the goal in levels closer based planner, we follow a standard four-steps approach:
to the initial state. After TGP has found a consistent set of ac- ﬁrst, TGP solves a planning problem, generating a trace of
tions for all the propositions in one level, it recursively tries the search tree; second, the search tree is labelled so that the
to ﬁnd a plan for the action’s preconditions and the persisted successful decision nodes are identiﬁed; third, control rules
goals. The search succeeds when it reaches level zero. Oth- are created from two consecutive successful decision points,
erwise, if backtracking fails, then TGP extends one level the by selecting the relevant preconditions; and, fourth, constants
planning graph with additional action and proposition nodes in the control rules are generalized to variables. In this dis-
and tries again. In terms of the uniﬁed model, it can be de- cussion, the search tree is the one generated while solving
scribed as:                                           the meta problem-space problem. So, each decision point (or
                  m                d
  • each meta-state s is a tuple {PG,Gp,L,l} where PG node) consists of a meta-state, and a set of successors (appli-
                     d
    is the plan graph, Gp is the set of pending (open) goals, cable instantiated operators of the meta problem-space).
    L is a set of assignments of instantiated actions to goals Now, we will present each step in more detail.
    (current partial plan), and l is the plan-graph level where
    search is                                         3.1  Labelling the search tree
                        m              d              We deﬁne three kinds of nodes in the search tree: success,
  • the initial meta-state, s = {PGn,G  ,,ln} where
                        0                             failure and memo-failure. success nodes are the ones that be-
    PGn  is the plan graph built in the ﬁrst phase up to level
    n (this second phase can be called many times), Gd is long to a solution path. The failure nodes are the ones where
                                                      there is not a valid assignment for all the goals in the node;
    the set of top level goals, and ln is the last level gener-
    ated of the plan graph                            i.e. it is not possible to ﬁnd actions to achieve all the goals
                                                      with no mutex relation violated among them or the ones in the
  •                                m = {        0}
    a terminal state will be of the form sT PGi,,L,   currently constructed plan. A node also fails if all of its chil-
    such that the solution plan can be extracted from L (ac- dren fail. And if the planner did not expand a node, the node
    tions contained in L)                             is labeled as memo-failure. This can happen when the goals
  • the set of search operators Am is composed of only one were previously memoized as nogoods (failure condition of
                                           d
    operator (given the current meta-state {PG,Gp,L,l}): Graphplan-based planning) at that level.

                                                 IJCAI07
                                                  1887  All nodes are initially labelled as memo-failure. If a node s0:
                                                       Level:(5) SUCCESS
fails during the planning process, its label changes to failure. Goals=((at person1 city0) (at person0 city1))
When the planner ﬁnds a solution, all the nodes that belong Assignments=NIL
to the solution path are labelled as success.          s1:
                                                       Level:(4) SUCCESS
  Figure 1 shows an example of a labelled search tree in Goals=((at person1 city0) (at plane1 city1) (in person0 plane1))
the Zenotravel domain solving one problem. The Zenotravel Assignments=(((debark person0 plane1 city1) (at person0 city1)))
domain involves transporting people among cities in planes, s2:
using different modes of ﬂight: fast and slow. The exam- Level:(3) SUCCESS
                                                       Goals=((at person1 city0) (in person0 plane1) (fuel-level plane1 fl1)
ple problem consists of transporting two persons: person0    (at plane1 city0))
    city0    city1     person1      city1   city0      Assignments=(((fly plane1 city0 city1 fl1 fl0) (at plane1 city1))
from       to     , and        from       to      .              ((debark person0 plane1 city1) (at person0 city1)))
Therefore, Gd = {(at person1 city0) (at person0 city1)}.
                                                       s3:
There are 7 fuel levels (fli) ranging from 0 to 6 and there Level:(2) SUCCESS
is a plane initially in city1 with a fuel level of 3. TGP ex- Goals=((in person1 plane1) (fuel-level plane1 fl1) (at plane1 city0)
                                                             (at person0 city0))
pands the planning graph until level 5 where both problem Assignments=(((board person0 plane1 city0) (in person0 plane1))
goals are consistent (nonmutex). In this example there are       ((debark person1 plane1 city0) (at person1 city0))
                                                                 ((fly plane1 city0 city1 fl1 fl0) (at plane1 city1))
no failures and therefore no backtracking. The initial meta-     ((debark person0 plane1 city1) (at person0 city1)))
state s0 is composed of the expanded plan graph, PG5, the
              d                                        s4:
problem goals G , assignments () and level 5. The search Level:(1) SUCCESS
                                                       Goals=((in person1 plane1) (at person0 city0) (fuel-level plane1 fl2)
algorithm tries to apply an instantiation of the search op-  (at plane1 city1))
erator of the meta problem-space (selecting an instantiated Assignments=(((fly plane1 city1 city0 fl2 fl1) (fuel-level plane1 fl1))
                                                                 ((board person0 plane1 city0) (in person0 plane1))
action for each goal). So, it ﬁnds the instantiated action       ((debark person1 plane1 city0) (at person1 city0))
of the domain problem-space (debark person0 plane1               ((fly plane1 city0 city1 fl1 fl0) (at plane1 city1))
                                                                 ((debark person0 plane1 city1) (at person0 city1)))
city1) to achieve the goal (at person0 city1) and per-
            (at  person1 city0)                        s5:
sists the goal                  . This generates the   Level:(0) SUCCESS
child node (meta-state s1), that has, as the goal set, the per- Goals=((at person1 city1) (at person0 city0) (fuel-level plane1 fl3)
                                                             (at plane1 city1))
sisted goal and the preconditions of the previously selected Assignments=(((board person1 plane1 city1) (in person1 plane1))
action (debark), i.e. (at plane1 city1) (in person0              ((fly plane1 city1 city0 fl2 fl1) (fuel-level plane1 fl1))
                                                                 ((board person0 plane1 city0) (in person0 plane1))
plane1). The pair (assignment) action (debark person0            ((debark person1 plane1 city0) (at person1 city0))
plane1 city1)  and goal (at person0 city1) is added              ((fly plane1 city0 city1 fl1 fl0) (at plane1 city1))
                                                                 ((debark person0 plane1 city1) (at person0 city1)))
on top of the child-node assignments. Then, the algorithm
continues the search at meta-state s2. It ﬁnds the action (fly
plane1 city0 city1 fl1 fl0)    to achieve the goal (at      Figure 1: Example of TGP success search tree.
plane1 city1)  and it persists the other goals. That opera-
tor generates the child node s3, with the preconditions of the one goal persists) and select operator rules to select
action fly and the persisted goals. The new pair action-goal the instantiated actions that achieve the goals in the decision
is added on top of the currently constructed plan. The algo- point (one rule for each achieved goal).
rithm continues until it reaches level 0 where the actions in As an example, from the ﬁrst two decision points s0 and s1
the assignment set of the last node (meta-state s5) represents of the example in Figure 1, two rules would be generated; one
the solution plan.                                    to select the goal (at person1 city0) and another one to
  Once the search tree has been labelled, a recursive algo- select the operator (debark person0 plane1 city1) to
rithm generates control rules from all pairs of consecutive achieve the goal (at person0 city1).
success nodes (eager learning). GEBL can also learn only In order to make the control rules more general and reduce
from non-default decisions (lazy learning). In this case, it the number of true-in-state meta-predicates, a goal re-
only generates control rules if there is, at least, one failure gression is carried out, as in most EBL techniques [4]. Only
node between two consecutive success nodes. The memo- those literals in the state which are required, directly or indi-
failure nodes in lazy learning are not considered, because the rectly, by the preconditions of the instantiated action involved
planner does not explore them. Also, from a “lazyness” per- in the rule (the action that achieves goal) are included.
spective they behave as success nodes. Lazy learning usually Figure 2 shows the select goal rule generated from
is more appropriate when the control knowledge is obtained the ﬁrst two decision points in the example of Figure 1.
and applied to the same planner to correct only the wrong This rule chooses between two goals of moving persons from
decisions.                                            one city to another (the arguments of the meta-predicates
                                                      target-goal  and some-candidate-goals). One person
3.2  Generating control rules                         <person1>   is in a city where there is a plane <plane1>
As we said before, control rules have the same format as in with enough fuel to ﬂy. The rule selects to work on the goal
PRODIGY. The module that generates control rules receives referring to this person giving that s/he is in the same city as
as input two consecutive success decision points (meta-states) the plane.
with their goal and assignment sets. There are two kinds of Figure 3 shows the select operator rule generated
possible rules learned from them: a select goals rule to from the example above. This rule selects the action debark
select the goal that persists in the decision point (when only for moving a person from one city to another. IPSS would try

                                                 IJCAI07
                                                  1888 (control-rule regla-ZENO-TRAVEL-PZENO-s1                 the case of IPSS it splits it in two: select an action, and
  (if (and (target-goal (at <person1> <city0>))
         (true-in-state (at <person1> <city1>))           select an instantiated action).
         (true-in-state (at <person0> <city0>))
         (true-in-state (at <plane1> <city1>))          So, according to IPSS language to deﬁne control rules,
         (true-in-state (fuel-level <plane1> <fl2>))
         (true-in-state (aircraft <plane1>))          there are three kinds of rules that can be learned in TGP to
         (true-in-state (city <city0>))
         (true-in-state (city <city1>))               guide the IPSS search process: select goals, select
         (true-in-state (flevel <fl1>))               operator   (select an action) and select bindings (se-
         (true-in-state (flevel <fl2>))
         (true-in-state (next <fl1> <fl2>))           lect an instantiated action) rules.
         (true-in-state (person <person0>))             The equivalence between meta-states is not straightforward
         (true-in-state (person <person1>))
         (some-candidate-goals ((at <person0> <city1>))))) (for translating the conditions of control rules). When IPSS
  (then select goals (at <person1> <city0>)))         selects to apply an instantiated action, the operator of the meta
                                                      problem-space changes the state sd and the action is added at
                                                      the end of the current plan . However, when TGP selects an
Figure 2: Example of select goals   rule in the Zeno-                        P
                                                      instantiated action to achieve a goal, it is added at the begin-
travel domain.
                                                      ning of the plan L (given that the search starts at the last level)
                                                                                    d
                                                      and TGP does not modify the state s . The difﬁculty arises in
(by default) to debark the person from any plane in the prob- deﬁning the state sd that will create the true-in-state
lem deﬁnition. The rule selects the most convenient plane; a conditions of the control rules. When the rules are learned in
plane that is in the same city as the person with enough fuel
                                                      TGP, we considered two possibilities: the simplest one is that
to ﬂy.                                                        d                           d
                                                      the state s is just the problem initial-state s0; and, in the sec-
                                                      ond one, we assume that when TGP persists a goal that goal
 (control-rule rule-ZENO-TRAVEL-ZENO1-e1                                                              d
  (if (and (current-goal (at <person0> <city1>))      would have already been achieved in IPSS, so the state s is
         (true-in-state (at <person0> <city0>))       the one reached after executing the actions needed to achieve
         (true-in-state (at <plane1> <city0>))
         (true-in-state (fuel-level <plane1> <fl1>))  the persisted goals in the TGP meta-state. To compute it, we
         (true-in-state (aircraft <plane1>))
         (true-in-state (city <city0>))               look in the solution plan, and progress the problem initial-
         (true-in-state (city <city1>))               state according to each action effects in such partial plan.
         (true-in-state (flevel <fl0>))
         (true-in-state (flevel <fl1>))                 Equivalent meta-states are computed during rule genera-
         (true-in-state (next <fl0> <fl1>))           tion and a translator makes several transformations after the
         (true-in-state (person <person0>))))
  (then select operators (debark <person0> <plane1> <city1>))) learning process ﬁnishes. The ﬁrst one is to split the select-
                                                      operator control rules in two: one to select the action and
                                                      another one to select its instantiation. The second trans-
Figure 3: Example of select operator rule in the Zeno- formation is to translate true-in-state meta-predicates
travel domain.                                        referring to variable types into type-of-object meta-
                                                      predicates.1 Finally, the translator deletes those rules that are
                                                      more speciﬁc than a more general rule in the set (they are sub-
4  Translation of learned knowledge                   sumed by another rule) given that GEBL does not perform an
Given that meta-states and operators of the meta problem- inductive step.
space of two different planners differ, in order to use the gen-
erated knowledge in P1 (TGP)byP2 (IPSS), we have to trans- 5 Experimental Results
late the control rules generated in the ﬁrst language to the
second one. The translation should have in mind both the We carried out some experiments to show the usefulness of
syntax (small changes given that we have built the control- the approach. Our goal is on transferring learned knowledge:
knowledge language for TGP based on the one deﬁned in that GEBL is able to generate control knowledge that improves
IPSS) and the semantics (translation of types of conditions IPSS planning task solving unseen problems. We compare
in the left-hand side of rules, and types of decisions - oper- our learning system with HAMLET, that learns control rules
ators of the meta problem-space). We have to consider the from IPSS problem-solving episodes. According to its au-
translation of the left-hand side of rules (conditions referring thors, HAMLET performs an EBL step followed by induction
to meta-states) and the right-hand side of rules (selection of a on the control rules.
search operator of the meta problem-space).             In these experiments, we have used four commonly used
  Therefore, in relation to the translation of the right-hand benchmark domains from the repository of previous planning
side of control rules, we found that the equivalent search op- competitions:2 Zenotravel, Miconic, Logistics and Driverlog
erators between IPSS and TGP are:                     (the STRIPS versions, since TGP can only handle the plain
                                                      STRIPS version).
  • to decide which goal to work on ﬁrst. When TGP selects
    the no-op to achieve a goal, this is equivalent to persist In all the domains, we trained separately both HAMLET
    the goal (it will be achieved in levels closer to the initial and GEBL with randomly generated training problems and
    state). IPSS has an equivalent search operator for choos-
                                                         1TGP does not handle variable types explicitly; it represents them
    ing a goal from the pending goals.
                                                      as initial state literals. However, IPSS domains require variable type
  • to choose an instantiated operator to achieve a particular deﬁnitions as in typed PDDL.
    goal (both planners have that search operator, though, in 2http://www.icaps-conference.org

                                                 IJCAI07
                                                  1889