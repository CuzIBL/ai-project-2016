                        Learning and Transferring Action Schemas

                        Paul R. Cohen, Yu-Han Chang, Clayton T. Morrison
                                   USC Information Sciences Institute
                                       Marina del Rey, California


                    Abstract                          the sequence move-to, contact and apply-force. Jean’s
                                                      learning method is a kind of state splitting in which state de-
    Jean is a model of early cognitive development    scriptions are iteratively reﬁned (split) to make the transitions
    based loosely on Piaget’s theory of sensori-motor between states as predictable as possible, giving Jean progres-
    and pre-operational thought. Like an infant, Jean sively more control over the outcomes of actions.
    repeatedly executes schemas, gradually transfer-    The following sections introduce Jean’s action schemas
    ring them to new situations and extending them as (Sec. 2), and then establish correspondences between action
    necessary to accommodate new experiences. We      schemas and the more familiar states and ﬁnite-state ma-
    model this process of accommodation with the Ex-  chines (Sec. 3). The state splitting algorithm is described in
    perimental State Splitting (ESS) algorithm. ESS   Section 4.1. Its application to transfer learning and empirical
    learns elementary action schemas, which comprise  results with a small-unit tactical military simulator are pre-
    controllers and maps of the expected dynamics of  sented in Sections 5 and 6, respectively.
    executing controllers in different conditions. ESS
    also learns compositions of action schemas called
    gists. We present tests of the ESS algorithm in three 2 Action Schema Components
    transfer learning experiments, in which Jean trans- Action schemas have three components: controllers, maps,
    fers learned gists to new situations in a real time and decision regions. Controllers control Jean’s behavior. For
    strategy military simulator.                      example, the (move-to Jean Obj) controller moves Jean
                                                      from its current location to the speciﬁed object. Jean has very
                                                      few innate controllers — move-to, turn, rest, apply force. It
1  Introduction                                       learns to assemble controllers into larger plan-like structures
One goal of the Jean project is to develop representations and called gists as described in Section 4.1.
learning methods that are based on theories of human cog- As Jean moves (or executes any other controller) certain
nitive development, particularly Piaget’s theories of sensori- variables change their values; for instance, the distance be-
motor and pre-operational thought. Piaget argued that in- tween Jean’s current location and Obj usually decreases when
fants acquire knowledge of the world by repeatedly execut- Jean executes the move-to controller. Similarly, Jean’s ve-
ing action-producing schemas [Piaget, 1954]. This activity locity will typically ramp up, remain at a roughly constant
was assumed to be innately rewarding. Piaget introduced as- level, then ramp down as Jean moves to a location. The val-
similation of new experience into extant schemas and accom- ues of these variables ground out in Jean’s sensors, although
modation of schemas to experiences that don’t quite “ﬁt” as some variables correspond to processed rather than raw sen-
the principal learning methods for infants. This paper gives sory information.
a single computational account of both assimilation and ac- These variables serve as the dimensions of maps. Each ex-
commodation.                                          ecution of a particular schema produces a trajectory through
  Although it seems to be a relatively recent focus in ma- a map — a point that moves, moment by moment, through
chine learning, transfer of learned knowledge from one situ- a space deﬁned by distance and velocity, or other bundles of
ation or scenario to another is an old idea in psychology and variables. Each map is deﬁned by the variables it tracks, and
is fundamental to Piaget’s account of cognitive development. different maps will be relevant to different controllers.
This paper demonstrates that the schemas learned by Jean can Each invocation of a controller creates one trajectory
be transferred between situations, as any Piagetian schema through the corresponding map, so multiple invocations will
should be.                                            create multiple trajectories. Figure 1 shows several such tra-
  Jean learns action schemas and gists. An action schema jectories for a map of distance for the move-to controller.
comprises a controller, a representation of the dynamics of One can see that move-to has a “typical” trajectory, which
executing the controller, and one or more criteria for stop- might be obtained by taking the mean of the trajectories in the
ping executing the controller. Gists are compositions of ac- map. Also, when operating in Jean’s environment, move-to
tion schemas for common tasks; for instance, push involves produces trajectories that lie within some distance of the

                                                IJCAI-07
                                                   720mean trajectory. In a manner reminiscent of Quality Con- These regions are sometimes called envelopes, [Gardiol and
trol Jean can assess whether a particular trajectory is “going Kaelbling, 2003] and REMOVED-FOR-BLIND-REVIEW
out of bounds.”                                         Jean is not the only agent in its environment and some maps
  The idea that sensori-motor and pre-operational devel- describe how relationships between Jean and other agents
opment should rely on building and splitting maps or re- change. The upper two panels of Figure 4 illustrates how
lated dynamical representations was anticipated by Thelen distance, relative velocity, heading, and contact change in an
and Smith [Thelen and Smith, 1994] and has been explored environment that includes Jean and another agent, called the
by other researchers in developmental robotics and psy- “cat,” an automaton that moves away from Jean if Jean moves
chology (e.g., REMOVED-FOR-BLIND-REVIEW [Siskind, 2001; too close, too quickly.
Barsalou, 1999; Mandler, 1992]).
                                                      3   Action Schemas as Finite State Machines
                                                      It will help to draw parallels between Jean’s maps and the
                                                      more familiar elements of ﬁnite state machines (FSMs). Con-
                                                      ventionally, states in FSMs represent static conﬁgurations
                                                      (e.g., the cat is asleep in the corner) and arcs between states
                                                      represent actions (e.g., (move-to Jean cat)). For us, arcs
                                                      correspond to the intervals during which Jean executes con-
                                                      trollers (i.e., actions), and states correspond to decision re-
                                                      gions of maps. That is, the elements of action schemas are
                                                      divided into the intervals during which a controller is “in
                                                      bounds” (the arcs in FSMs) and the intervals during which
                                                      Jean is thinking about what to do next (the states). Both take
                                                      time, and so require a rethink of the conventional view that
                                                      states in FSMs persist and transitions over arcs are instan-
                                                      taneous. However, the probabilistic semantics of FSMs are
                                                      retained: A controller invoked from a decision region (i.e, an
Figure 1: Trajectories through a map of distance between action invoked in a state) will generally take Jean into one of
Jean and a simulated cat. Sometimes, when Jean gets close several decision regions (i.e., states), each with some prob-
to the cat, the cat moves away and the distance between them ability. Figure 3 redraws Figure 2 as a ﬁnite state machine.
increases, in which case a trajectory may go “out of bounds.” Starting from a decision region of some action schema A, the
                                                      move-to  controller will with some probability (say .7) drop
                                                      Jean in the decision region associated with achieving its goal,
                     move-to                          and, with the complementary probability, in the region asso-
                 distance                             ciated with being unable to achieve the goal in time.

                                                                         move-to
                                                                              .3
                                                                    A
                 0                                                            .7
                        time
                                                                         move-to

Figure 2: This schematic of a map, for the (move-to Jean
Obj) controller, has one decision region associated with the Figure 3: The action schema from Figure 2 redrawn as a ﬁnite
distance between Jean and the object Obj being zero. The state machine in which arcs correspond to the execution of
other decision region bounds the area in which Jean cannot controllers and states correspond to decision regions.
reach loc even moving at maximum speed.
                                                        There is one special case: Sometimes the world changes
  Every map has one or more decision regions within which when Jean is doing nothing. We model this as a schema in
Jean may decide to switch from one controller to another. which no controller is speciﬁed, called a dynamic schema to
One kind of decision region corresponds with achieving a distinguish it from an action schema. Dynamic schemas do
goal; for example, there is a decision region of the move-to have maps, because variables such as distance between Jean
map in which distance to the desired location is effectively and another agent can change even when Jean does nothing;
zero (e.g., the thin, horizontal grey region in Fig. 2). An- and they have decision regions, because Jean may want to in-
other kind of decision region corresponds to being unable voke a controller when these variables take particular values
to achieve a goal; for instance, there is a region of a time- (e.g., moving away when another agent gets too close). The
distance map from which Jean cannot move to a desired lo- FSMs that correspond to dynamic schemas have no controller
cation by a desired time without exceeding some maximum names associated with arcs but are otherwise as shown in Fig-
velocity (e..g., the inverted wedge-shaped region in Fig. 2). ure 3.

                                                IJCAI-07
                                                   7214  Learning Action Schemas and Gists                    ESS uses a simple heuristic to ﬁnd threshold values for fea-
                                                           f
Jean has a small set of innate controllers and a few “empty” tures and, thus to split a state: States change when sev-
maps, and it “ﬁlls in” these maps with trajectories and learns eral state variables change more or less simultaneously. This
decision regions. Jean also learns compositions of action heuristic is illustrated in Figure 4. The upper two graphs show
schemas called gists for their story-like or plan-like struc- time series of ﬁve state variables: headings for Jean and the
ture. One principle underlies how Jean learns decision re- cat (in radians), distance between Jean and the cat, and their
gions. It is to maximize predictability, or minimize the en- respective velocities. The bottom graph shows the number
tropy of “what’s next.” Within an action schema, what’s next of state variables that change value (by a set amount) at each
is a location in a map. At the boundaries of action schemas tick. When the number of state variables that change simul-
(that is, in decision regions, or states) what’s next is the next taneously exceeds a threshold, Jean concludes that the state
                                                                                        f
state. We call the entropy of what’s next the boundary en- has changed. The value of the schema at the moment of
tropy. Jean learns decision regions that minimize boundary the state change is likely to be a good threshold for splitting
                                                      f
entropy between states. Interestingly, this criterion tends to . For example, between time period 6.2 and 8, Jean is ap-
minimize boundary entropy within maps as a side effect. proaching the cat, and the heuristic identiﬁes this period as
                                                      one state. Then, at time period 8, several indicators change
4.1  Experimental State Splitting Algorithm           at once, and the heuristic indicates Jean is in a new state, one
We give a formal outline of the Experimental State Splitting that corresponds to the cat moving away from Jean.
(ESS) algorithm in this section. Jean receives a vector of fea-
      t
tures F = {f1,...,fn} from the environment at every time                   Sensor readings: Distance, Velocities
   t                                                     40
tick . Some features will be map variables, others will be in-  30

puts that have not yet been associated with maps. Jean is ini-  20
                                                       Value
tialized with a goal state sg and a non-goal state s0. St is the  10
                                                          0
entire state space at time t. A is the set of all controllers, and  0  2  4  6     8     10    12    14
                                                                                Time
A(s) ⊆ A are the controllers that are executed in state s ∈ S.         Sensor readings: Contact, JeanRobot and Cat Headings in Radians
        A(s)                         A  H(s  ,a )
Typically    should be much smaller than .  i  j is      2
                             s                   a
the boundary entropy of the state i in which controller i  0
                                                       Value

is executed. A small boundary entropy corresponds to a sit- -2
                             a           s
uation where executing controller j from state i is highly  0   2     4      6     8     10    12    14
                                      p(s ,a ,s )                               Time
predictive of the next observed state. Finally, i j k is                     Segment Indicators
                                                         4
the probability that executing controller aj in state si will lead
      s
to state k.                                              2
  For simplicity, we will focus on the version of ESS that Number

                                                         0
only splits states; an alternative version of ESS is also capable  0  2  4   6     8     10    12    14
of learning specializations of parameterized controllers. The                   Time
ESS algorithm follows:
                                                      Figure 4: New states are indicated when multiple state vari-
  •                                S  = {s ,s }
    Initialize state space with two states, 0 0 g .   ables change simultaneously.
  • While -optimal policy not found:
      – Gather experience for some time interval τ to esti-
        mate the transition probabilities p(si,aj,sk).
      – Find a schema feature  f  ∈   F , a thresh-   5   Transferring Learning
        old θ  ∈   Θ, and a state si  ∈  S  to split
        that maximizes the boundary entropy score re- Although ESS can learn action schemas and gists for new sit-
        duction of the split: maxS,A,F,Θ H(si,ai) −
        min(H(s   ,a ),H(s  ,a ))       s       s     uations from scratch, we are much more interested in how
                k1  i     k2  i , where  k1 and  k2   previously learned policies can accommodate or transfer to
        result from splitting si using feature f and thresh-
           θ  s   =  {s ∈ s |f<θ}       s  =  {s ∈    new situations. Gists capture the most relevant states and ac-
        old :  k1          i        and  k2
        s |f ≥ θ}                                     tions for accomplishing past goals. It follows that gists may
         i      .                                     be transferred to situations where Jean has similar goals and
             s ∈ S     s      s             s
      – Split i   t into k1 and k2 , and replace i with the conditions in the situation are similar.
        new states in St+1.
                                         p    S         The version of ESS that we described above is easily mod-
      – Re-solve for optimal plan according to and t+1 iﬁed to facilitate one sort of transfer: After each split we re-
  Finding a feature f and the value on which to split states is move the transition probabilities on all action transitions be-
equivalent to ﬁnding a decision region to bound a map. tween each state. This allows the state machine to accommo-
  Without heuristics to reduce the effort, the splitting proce- date new experience while maintaining much of the structure
dure would iterate through all state-controller pairs, all fea- of the machine (see REMOVED-FOR-BLIND-REVIEW for a pre-
tures f ∈ F , and all possible thresholds in Θ, and test each vious example of this idea). In the experiments in the next
such potential split by calculating a reduction in boundary en- section we explore the effects of transfer using this mecha-
tropy. This is clearly an expensive procedure.        nism in several conditions.

                                                IJCAI-07
                                                   7226  Experiments                                            ponent. If Jean’s forces are crawling, and they have not
To measure transfer we adopt a protocol sometimes called  begun ﬁring, then Jean’s forces won’t be sighted (until
B/AB: In the B condition the learner learns to perform some they are within range i).
tasks in situation or scenario B. In the AB condition, the ii. Firing Range (up to 200 meters): Within this range, ei-
learner ﬁrst learns to perform tasks in situation or context A ther force can ﬁre on the other. Once Jean’s forces have
and then in B. By comparing performance in situation B in ﬁred, they are considered sighted, even if crawling, and
the two conditions after different amounts of learning in situ- the opponent can return ﬁre with the same effectiveness
ation B one can estimate the effect of learning in A and thus as Jean’s forces.
the knowledge transferred from situation A to situation B.
           A                  B                         i. Full Contact (up to 100 meters): At this range, even if
For instance, might be tennis and squash, and the B/AB    Jean’s forces are crawling, they will be sighted by the
protocol compares learning curves for squash alone (the B or opponent. Direct ﬁre has full effect.
control condition) with learning curves for squash after hav-
ing learned to play tennis (the AB or transfer condition). We Each experiment had a transfer condition AB and a control
say positive transfer has occurred between A and B if the condition, B. In the former, Jean learned gists to accomplish
learning curves for squash in the transfer condition are better its goal in a scenario designated A and then learned to accom-
than those in the control condition.                  plish its goal in scenario B. In the latter, control condition,
  In general “better” learning curves means faster-rising Jean tried to learn in scenario B without beneﬁt of learning
curves (indicating faster learning), or a higher value after in A.
some amount of learning (indicating better performance), The experiments differ in their A and B scenarios:
or higher value after no learning (indicating that knowledge Experiment 1 : All action takes place in open terrain. A sce-
from A helps in performing B even before any learning has narios all have Jean’s forces starting near enemy forces.
occurred in B). In our experiments, better learning perfor- B scenarios are an equal mix of starting near the enemy
mance means less time to learn a gist to perform a task at or far away from the enemy.
a criterion level. Thus, a smaller area beneath the learning                                       A
curve indicates better learning performance, and we com- Experiment 2 : All action takes place in open terrain. sce-
                                                          narios all have Jean’s forces starting far from the enemy
pare conditions B and AB by comparing the area beneath           B
the learning curves in the respective conditions. A bootstrap- forces. scenarios are an equal mix of starting near the
randomization procedure, described shortly, is used for sig- enemy or far away from the enemy.
niﬁcance testing.                                     Experiment 3 : The terrain for A scenarios is open, whereas
  We tested Jean’s transfer of gists between situations in the the terrain for B scenarios has a mountain that, for some
3-D real time strategy game platform ISIS. ISIS can be con- placements of Jean’s and the enemy’s forces, prevents
ﬁgured to simulate a wide variety of military scenarios with them seeing each other. (The advantage goes to Jean,
parameters for specifying different terrain types, unit types, however, because Jean knows the location of the enemy
a variety of weapon types, and multiple levels of unit control forces.) The A scenario is an equal mix of starting near
(from individual soldiers to squad-level formations).     or far from the enemy, the B scenario is an equal mix
  In each of three experiments, Jean controlled a single squad of starting near and far from the enemy in the mountain
at the squad level, with another squad controlled by an auto- terrain.
mated but non-learning opponent. Jean’s squad ranged in size
from 7 to 10 units while the opponent force ranged from 1- 6.1 Metrics and Analysis
3 units. Although the opponent was smaller, it could move We plot the performance of the Jean system in the various
faster than Jean’s forces. In each experiment, Jean’s goal is to experimental scenarios as learning curves over training trials.
move its units to engage and kill the opponent force. Better learning performance is indicated by a smaller number
  Jean is provided four innate action schemas: run,   of training instances required by Jean to achieve a criterion
crawl, move-lateral, and   stop-and-fire.   It must   level of performance. Thus, a smaller area beneath the learn-
learn to compose these into gists that are appropriate for dif- ing curve indicates better learning.
ferent engagement ranges, possible entrenchment of the op- Given n learning curves for the B and AB conditions, we
ponent, and some terrain features (mountains).        test the null hypothesis of “no transfer” as follows: Let B
  All experiment scenarios were governed by a model of en- and AB denote the sets of n learning curves in the B and AB
gagement ranges that determined how the squads interact and conditions, respectively, X be the mean learning curve for
how the opponent controller would respond to Jean’s actions. a set of learning curves X , and Area(X ) be the area under
Engagement ranges are deﬁned as follows:              learning curve X . We measure the beneﬁt of transfer learning
 iv. Outer Range (beyond 250 meters): As long as the oppo- with the transfer ratio
    nent is within line of sight (i.e., not obscured by terrain                  Area(B)
    features), no matter what the distance, Jean can locate          r(B, AB)=           .
    the opponent. However, beyond 250 meters, the oppo-                         Area(AB)
    nent cannot see Jean’s forces.                    Values greater than one indicate that the area under the learn-
 iii. Visual Contact (up to 250 meters): At this range, if ing curves in the control condition B is larger than in the
    Jean’s forces are standing they will be sighted by the op- transfer condition, or a positive beneﬁt of transfer.

                                                IJCAI-07
                                                   723  The null hypothesis is r =1.0, that is, learning proceeds exploring a continuous, high-dimensional feature space us-
at the same rate in the control and transfer conditions. To test ing her four available actions. Many of these actions result
whether a particular value of r is signiﬁcantly different from in the enemy soldiers detecting Jean’s presence and running
1.0 we require a sampling distribution for r under the null away, thus reducing Jean’s chances of ever reaching her goal
hypothesis. This is provided by a randomization-bootstrap by simple exploration. Since Jean does not learn anything
procedure [Cohen, 1995]. First, we combine the sets of learn- useful in the A scenario, her performance in the AB transfer
ing curves, C = B∪AB. Then, we randomly sample, with  condition is no better than in the control condition B.
replacement, n curves from C and call this a psuedosample The learning curves for the A and B scenarios in Ex-
B∗; and again randomly sample, with replacement, another n
                                 ∗                    periment 1 are shown in Figure 5. Note that the vertical
curves from C to get pseudosample AB . Then, we compute axis is “time to achieve the goal,” in the scenarios, so a
   ∗    ∗
r(B , AB ) and store its value in the sampling distribution of downward-sloping curve corresponds to good learning per-
r. Repeating this process a few hundred times provides an formance. Jean learned a good policy in the A scenario where
estimate of the sampling distribution. For one-tailed tests, the the enemy units are initially close to Jean’s position. Jean re-
p-value is simply the proportion of the sampling distribution ceives a signiﬁcant beneﬁt when it learns in the B scenario af-
with values greater than or less than r (depending on the di- ter having learned in the A scenario (i.e., in the transfer condi-
rection of the one-tailed alternative hypothesis). Because we tion AB). The AB learning curve starts out immediately with
expect learning rates in the transfer condition to be higher, much better performance than the B learning curve. In fact,
our alternative hypothesis is r>1 and the p value is the pro- it takes 200 trials for learning in the B condition, in which no
portion of the sampling distribution with values greater than transfer happens, to reach the level of performance observed
r. (N.B.: When there is a possibility of negative transfer, in at all levels of training in the transfer condition AB.How-
which the knowledge acquired in A actually impedes learning ever, the AB curve is roughly ﬂat, which means learning in
in B, it might be more appropriate to run two-tailed tests.) scenario A provides a boost in performance in B but has no
  To obtain a conﬁdence interval for r, as opposed to a p impact on the rate of learning in B. We rejected the null hy-
value, we again construct a sampling distribution, but this pothesis that B and AB have the same mean learning curve
time, instead of ﬁrst combining the two sets B and AB,we
                                     ∗                with a p value of 0.035; the AB condition has signiﬁcantly
sample B∗ with replacement from B, and AB similarly from
   ∗                         ∗                        better learning curves.
AB  . Then we calculate r(B∗, AB ). Repeating this process
yields a sampling distribution for r. A 95% conﬁdence in-
terval around r is the interval between the 2.5% and 97.5%
quantiles of this distribution [Cohen, 1995].
6.2  Results
Let us start with a qualitative assessment of what Jean
learned. In Experiment 1, Jean learned in scenario A to run
at the enemy and kill them. In scenario B, Jean learned a gist
that included a conditional: When starting near the enemy,
use the gist from scenario A, but when starting far from the
enemy crawl — don’t run — until one is near the enemy and
then use the gist from scenario A. The alternative, running
at the enemy from a far starting location, alerts the enemy
                                                                                                 A
and causes them to run away. State splitting did what it was Figure 5: Learning curves for learning in scenario , in sce-
                                                           B               B
supposed to do: Initially, Jean’s gist for scenario B was its nario , and in scenario after having ﬁrst learned in sce-
                                                           A
gist for A, so Jean would always run at the enemy, regardless nario . Each point is averaged over eight replications of
of starting location. But through the action of state splitting, the experiment. Error bars are two standard deviations wide.
Jean eventually learned to split the state in which it ran at Each point of each curve is the average of ten ﬁxed test tri-
the enemy into a run-at and a crawl-toward state, and it suc- als. Test trials are conducted every 20 training trials. The
cessfully identiﬁed the decision region for each. For instance, x-axis plots the number of training trials that the agent has
the decision region for the crawl-toward state identiﬁes a dis- completed.
tance (corresponding to being near the enemy), from which
Jean makes a transition to the state in which it runs at and In contrast, in Experiment 2 Jean does not succeed in
shoots the enemy.                                     achieving any transfer. Jean does not ever learn anything use-
  Similar results are obtained in Experiment 3, where Jean ful in the A condition because, as noted above, when Jean
learns to run at the enemy from a far starting location as long starts far from the enemy the search space of possible gists
as the mountain prevents the enemy from sighting Jean, oth- is too large. The transfer ratio r = .97 is nearly equal to the
erwise to crawl.                                      expected value under the null hypothesis of no transfer, and
  In Experiment 2, Jean learned nothing in scenario A and the p value is accordingly high, p = .557.
was no more successful in scenario B. This is due to the difﬁ- In Experiment 3, Jean transfers learned knowledge from
culty of the scenario. Jean is always initialized far away from the A scenario, which involves open terrain to “jump-start”
the enemy units, and must learn a policy for killing them by performance in the B scenario, which includes a mountain.

                                                IJCAI-07
                                                   724