                      Parametric Kernels for Sequence Data Analysis

                                  Young-In Shin and Donald Fussell
                                   The University of Texas at Austin
                                   Department of Computer Sciences
                             {codeguru,fussell}@cs.utexas.edu


                    Abstract                          ated with a point in a parameter space, and similarity metrics
                                                      are deﬁned independently for the elements and their parame-
    A key challenge in applying kernel-based methods  ters. The parameter space is decomposed into possibly over-
    for discriminative learning is to identify a suitable lapping ranges, and the overall similarity of the sequences
    kernel given a problem domain. Many methods in-   is computed by taking the summation kernel over the ele-
    stead transform the input data into a set of vectors ments and their associated parameters for each range. The
    in a feature space and classify the transformed data intuition behind the use of parameters is to encode informa-
    using a generic kernel. However, ﬁnding an effec- tion about the ordering of input data in the kernel in a ﬂexi-
    tive transformation scheme for sequence (e.g. time ble way. Such information is frequently lost in feature-space
    series) data is a difﬁcult task. In this paper, we in- based approaches. Since we can choose to apply a wide range
    troduce a scheme for directly designing kernels for of order-preserving transformations on the distances between
    the classiﬁcation of sequence data such as that in elements in our parameterizations, we obtain additional con-
    handwritten character recognition and object recog- trol over the similarity metrics we can deﬁne as well. Note
    nition from sensor readings. Ordering information that our use of the term “parametric” here does not imply that
    is represented by values of a parameter associated we assume any sort of a priori parametric model of the data,
    with each input data element. A similarity met-   as for example in the case of generative approaches [Jaakkola
    ric based on the parametric distance between corre- and Haussler, 1998]. Note also that sequences of any ﬁxed
    sponding elements is combined with their problem- dimensional feature vectors can be used instead of input data
    speciﬁc similarity metric to produce a Mercer ker- elements in our method as needed since all we assume about
    nel suitable for use in methods such as support   elements is that they are ﬁxed-dimensional.
    vector machine (SVM). This scheme directly em-      Consider an input x represented as a sequence of n ele-
    beds extraction of features from sequences of vary-
                                                      ments xi ∈ X,i.e. x =[x1, ··· ,xn]. We could choose to
    ing cardinalities into the kernel without needing
                                                      associate each element xi with parameter τ (xi) in parameter
    to transform all input data into a common feature space T as follows
    space before classiﬁcation. We apply our method to
    object and handwritten character recognition tasks             
                                                                      i
    and compare against current approaches. The re-                         xk − xk−1  if i>1,
                                                            τ(xi)=      k=2                           (1)
    sults show that we can obtain at least comparable                 0                  otherwise.
    accuracy to state of the art problem-speciﬁc meth-
    ods using a systematic approach to kernel design.   The associated parameters form a non-decreasing sequence
    Our contribution is the introduction of a general of non-negative real numbers, starting from zero. For any two
    technique for designing SVM kernels tailored for  such sequences, a parametric kernel at each iteration picks
    the classiﬁcation of sequence data.               one element from each of the sequences and computes the
                                                      similarities between the elements and their associated param-
                                                      eters separately. These are multiplied to return an overall sim-
1  Introduction                                       ilarity between the elements. This product of the similarities
A common technique in mechanical classiﬁcation and regres- of the elements and their parameters implies that two similar
sion is to deﬁne a feature space for the domain of interest, elements may contribute signiﬁcantly to the overall sequence
transform the input data into vectors in that space, and then similarity only when their associated parameters are similar
apply an appropriate classiﬁcation or regression technique in as well. This step is repeated for all pairs of elements from
the feature space. However, some data are more naturally rep- the two sequences, and the results are summed to return the
resented in a much more irregular form than feature vectors. overall sequence similarity.
  In this paper, we propose a new approach based on direct Na¨ıve application of this approach has the potential to be
matching using parametric kernels, where inputs are vari- swamped by the computational expense of performing many
able length sequences of elements. Each element is associ- comparisons between elements with widely divergent param-

                                                IJCAI-07
                                                  1047            x
             2                    z  z                transformed into a set of uniform size feature vectors. The
                   x               34z
                    3         z           5           similarity of two input data is then assessed by evaluating
                               2
                                           z          generic kernel functions on the feature vectors. To handle
                                           6
   x                      z                           sequence data in this traditional kernel framework, features
    1                      1                          based on distance metrics are often extracted, e.g. the mean
                   x                    z             coordinates, second order statistics such as median, variance,
                   4                     7
     x =[x , ··· ,x ]       z =[z  , ··· ,z ]         minimum and maximum distances, area, etc. However, such
          1      4                1      7            features do not generalize well because they often impose
      x            x    x     x                       restrictions that input patterns must be of equal lengths or
       1            2    3     4
      z      z     z z z z     z                      sampled at equal rates. Alternatively, histograms can be con-
       1      2     3 4 5 6     7                     structed from locations, speed, size, aspect ratio, etc. His-
                                              T       tograms are an effective scheme to map varying length se-
                                                                                                  [
                Δ2Δ3Δ                                 quences into uniform dimensional feature vectors Porikli,
      0                                               2004; Grauman and Darrell, 2005]. Unfortunately, structural
     Figure 1: Mapping Sequence to Parameter Space    information between elements in input patterns is inevitably
                                                      lost in histogramming.
                                                        Methods based on direct matching do not suffer from such
eters which contribute little or nothing to the ﬁnal result. In- drawbacks as they retain the structural information. They
tuitively, we could handle this by limiting the comparisons map input sequences into sequences of equal dimensional lo-
only to subsets of elements that are close in parameter space. cal features, which are further compared using well known
This closeness in parameter space is easily speciﬁed by the direct sequence matching techniques. For instance, a number
decomposition of parameter space into ranges, so that close of methods based on Dynamic Time Warping (DTW) have
elements are deﬁned to be those whose parameters fall into recently been proposed for classifying sequence data such
                                                                                     [
the same range. For instance, we could decompose T into as speech or handwritten characters Bahlmann et al., 2002;
                                                                           ]
non-overlapping intervals of equal length Δ, and elements Shimodaira et al., 2002 . Indeed, an extensive survey has
from the two sequences are close if their associated parame- shown that DTW methods are among the most effective tech-
                                                                                      [
ters fall into the same interval. See Figure 1, where elements niques for classifying sequence data Lei and Govindaraju,
                                                                                ]   [                   ]
in x and z are grouped into three ranges based on the afore- 2005; Keogh and Kasetty, 2002 .In Bahlmann et al., 2002 ,
                                                      normalized coordinates and the tangent slope angle is com-
mentioned decomposition scheme. x1, for instance, will be
                                                      puted at each of the points in a stroke sequence to form a
compared against only z1 and z2, both in X and T .Any
sequence data types fall into this category, e.g. handwritten feature vector. DTW computes the optimal distance between
characters, laser sensor readings, digital signals, and so on. two sequences along a Viterbi path, which is then used as
                                                      the exponent of a radial basis function (RBF). In [Tapia and
  Ideally, we wish to ﬁnd a method that meets all of the fol-
                                                                ]
lowing requirements : a) computationally efﬁcient to handle Rojas, 2003 , a ﬁxed size feature vector is computed from
large inputs, b) no need to assume any probabilistic models, c) strokes and an SVM classiﬁer is used. They achieved a high
                                                      accuracy over 98% but since they allowed only ﬁxed feature
kernels are positive semi-deﬁnite, d) no requirement of ﬁxed
form inputs, and e) the ability to ﬂexibly embed structural in- vectors for each stroke, the method’s application is quite lim-
                                                              [                      ]          R
formation from the input. While previous approaches fail to ited. In Lei and Govindaraju, 2005 , Extended -squared
                                                      (ER2) is proposed as the similarity measure for sequences.
satisfy some or all of these, all of the requirements are sat-
isﬁed by our approach. With an appropriate decomposition It uses coordinates of points directly as features, but can only
scheme, parametric kernels may be computed in time linear to operate on ﬁxed-length windows of such points.
the number of elements in inputs. Parametric kernels are posi- DTW methods deﬁne kernels which can be shown to be
                                                                                                     [
tive semi-deﬁnite, thus they are admissible by kernel methods symmetric and satisfy the Cauchy-Schwartz inequality Shi-
such as SVMs that require Mercer kernels for optimal solu- modaira et al., 2002]. SVM classiﬁers may employ such ker-
tion.                                                 nels to classify sequential data. However, kernels based on
  We have applied our method to handwritten character DTW   are not metrics. The triangle inequality is violated in
recognition and object recognition from sensor readings. Our many cases and the resulting kernel matrices are not positive
results compare favorably to the best previously reported semi-deﬁnite. Therefore, they are not admissible for kernel
schemes using only very simple similarity metrics.    methods such as SVM in that they cannot guarantee the exis-
                                                      tence of a corresponding feature space and any notion of opti-
                                                      mality with respect to such a space. In contrast, our paramet-
2  Related Work                                       ric kernels are positive and semi-deﬁnite and are thus well-
A great deal of work has been done on the application of suited for use in kernel-based classiﬁers.
kernel-based methods to data represented as vectors in a fea-
ture space [Shawe-Taylor and Christianini, 2004; Sch¨olkopf 3 Approach
and Smola, 2002]. Discriminative models can ﬁnd complex
and ﬂexible decision boundaries and their performance is of- Kernel-based discriminative learning algorithms can ﬁnd
ten superior to that of alternative methods, but they often rely complex non-linear decision boundaries by mapping input
on a heuristic preprocessing step in which the input data is examples into a feature space F with an inner product that can

                                                IJCAI-07
                                                  1048be evaluated by a kernel function κ : X × X → R on exam- every pair of elements of the sets being compared whose pa-
ples in the input space X. A linear decision function in F is rameters fall within the range. For T1, we will compare x2
found, which corresponds to a non-linear function in X.This with z3, x2 with z4, ··· , and x3 with z6. The similarity for
“kernel trick” lets us ﬁnd the decision boundary without ex- a given pair of elements is obtained by taking the product of
plicit evaluation of the feature mapping function φ : X → F the similarity κτ : T × T → R between those elements’
                                                                                    d    d
and taking the inner product, which is computationally very parameters and a similarity κx : R × R → R deﬁned di-
expensive and may even be intractable. For this to guarantee rectly on the elements themselves, each of which is a Mercer
a unique optimal solution, the kernel functions must satisfy kernel function. Then, the feature extraction function φ of a
Mercer’s condition, i.e. the Gram matrix obtained from the parametric kernel is deﬁned as
kernel functions must be positive and semi-deﬁnite.

3.1  Parameters                                                 φ(x)=[φ0(x),φ1(x), ···φN−1(x)],       (3)
The underlying intuition in our work is to associate a param- where
eter with each of the elements so that enforcing parametric
similarity is equivalent to the similarity in the structure of el-   
                                                            φ (x)=        w   κ (x , ·)κ (τ (x ), τ (·)),
ements in the input patterns. Our work assumes that input    t              xi x  i   τ    i          (4)
patterns are varying length sequences of elements from some        xi∈It(x)
common input or feature space. For example, handwritten
                                                      and wxi is a non-negative weighting factor. Given two se-
characters are sequences of 2D points, while images are 2D   x =[x  , ··· ,x ]   z =[z  , ··· ,z ]
grids of ﬁxed dimensional color values. The structure of a quences 1      |x| and      1      |z| , the para-
sequence is a 1D manifold of 2D vectors and that of an image metric kernel function before normalization is then deﬁned as
is a 2D manifold of 3D vectors, assuming colors are repre- the sum of similarity for all ranges :
sented as RGB for instance. A parameter of an element is
then a point in this manifold that corresponds to the element.                    N−1
Therefore, if two parameters of any two elements are close, κ(x, z)=φ(x) · φ(z) =   φt(x) · φt(z), (5)
then they are structurally close. This parametric association                      t=0
must be deﬁned as part of making the choice of kernel func-
                                                      where
tions.
                                                                φt(x) · φt(z)                       (6)
3.2  Parametric Kernel                                            
                                                            =          w  w  κ (x ,z )κ (τ (x ), τ (z )).
Our input pattern x is a sequence of |x| elements, where each           xi zj x  i  j  τ    i     j   (7)
                                                                x ∈I (x)
       xi    d                     x =[x1, ··· ,x|x|]            i  t
element  is a -dimensional vector, i.e.                         z ∈I (z)
          d                                                      j  t
and xi ∈ R . The number of elements may vary across se-
quences. We associate each element with a parameter in pa- Note that since the product of κx and κτ is taken in (4),
rameter space T via a function τ : Rd → T . Consider a both must score high to have signiﬁcance in (5). Also note
decomposition of T into N non-overlapping ranges      that no comparison is made between elements that are not
                                                      from a common range. If, instead, we have to compare all
                        N−1                          elements from one input with all from the other input, we will
                    T =     T .                       face a number of undesirable consequences. For instance,
                             t                  (2)                             x       {z , ··· ,z }
                        t=0                           in Figure 1, we will compare 1 with 1      7 ,rather
                                                      than {z1,z2}. This will result in a higher similarity value,
  For instance, recall the earlier sequence example, where T which may be helpful for certain cases. But, at the same time,
was the set of non-negative real numbers and τ is deﬁned as we are more likely to be confused by inputs where there are
(1). T was decomposed as shown in Figure 2.           too many far elements, in which case we are swamped by
                                                      bad comparisons. Of course, we may need dramatically more
                                         ...
          T0    T12TT3             T4                 time to compute (5) as the number of kernel evaluations is
             Δ     2Δ3Δ4Δ5Δ                           signiﬁcantly increased.
       0                                                Parameter space decomposition solves such problems.
                                                      However, such a decomposition scheme can introduce quanti-
Figure 2:  Parameter space is decomposed  into non-
                                                      zation errors. To overcome this problem, we allow the ranges
overlapping ranges of length Δ.
                                                      to overlap. For instance, ranges in Figure 2 may overlap by
                                                      Δ/2, as shown in Figure 3. To suppress over-contribution of
  For the derivation of parameter kernel functions, we elements that fall into the intersections of ranges, we intro-
ﬁrst deﬁne decomposed element set for Tt as It(x)=    duce weighting factors in (4).
{xi|τ(xi) ∈ Tt}, which is the set of elements of x whose The simplest weighting scheme is to take the average of the
associated parameters are in Tt. In our previous example similarity at the overlapped regions. In this scheme, the de-

shown in Figure 1, for instance, I1(x)={x2,x3} and    fault value for wxi is 1/|Txi |,whereTxi = {Tt|τ(xi) ∈ Tt}.

I1(z)={z3,z4,z5,z6}. We then compute a similarity for When no ranges overlap, we have |Txi | =1and there-

each range by taking a weighted sum of the similarities of fore, wxi =1. Otherwise, overlapped ranges may yield

                                                IJCAI-07
                                                  1049             T0           T2           T4             Irregular Decomposition Parameter ranges are of varying
                                                      lengths. Implementation is complicated and often decom-
                    T           T        ...
                    1            3                    posed element sets take longer to compute. But we can freely
       0     Δ/2   Δ      3Δ/2  2Δ    5Δ/2            decompose the parameter space to better ﬁt the data.

           Figure 3: Ranges overlap by Δ/2.
                                                      Multi-scale Decomposition Parameter ranges form a hier-
                                                      archical structure at different resolutions. For instance, we

wxi <  1. For instance, with the decomposition scheme in may consider a decomposition where ranges form a pyramid

Figure 3, at intersection [Δ/2, Δ), we will set wxi =1/2, as shown in Figure 4. Elements from non-adjacent ranges

since |Txi | =2. Note that this scheme will not result in could be compared at coarser resolutions. Along with a

|Txi | =0,i.e. wxi →∞, since (4) will be evaluated only proper weighting scheme, this may improve the performance.
when It(x) = ∅.IfIt(x)=∅,thenφt(x) ≡  0 and the term But implementation is more complex and kernel evaluation
is just ignored. Further discussion of different decomposition may take longer.
schemes is given in 3.3.
  Finally, to avoid favoring large inputs, we normalize (5) by                              ...
                                                                          T
dividing it with the product of their norms,                               6
                                                                                            ...
                                                                    T4          T5
                                                                                               ...
                          κ(x, z)                               T      T     T     T      T
             κ(x, z)=               .          (8)              0      12          3      7
                        κ(x, x)κ(z, z)                       0      Δ2Δ3Δ             4Δ5Δ
  This is equivalent to computing the cosine of the angle be-
tween two feature vectors in a vector space model.       Figure 4: Pyramidal Parameter Space Decomposition
3.3  Parameter Space Decomposition Scheme
In this section, our decomposition scheme is discussed in gen-
eral in terms of pros and cons with respect to additional cost 3.4 Mercer Condition
of computation and changes in classiﬁcation performance due According to Mercer’s theorem, a kernel function corre-
to range overlapping and similarity weighting. Then, a num- sponds to an inner product in some feature space if and only
ber of different decomposition schemes are presented. if it is positive and semi-deﬁnite. A unique optimal solution
  As mentioned above, decomposition lets us avoid swamp- is guaranteed by kernel methods only when kernels are posi-
ing by bad comparisons and dramatically reduces the compu- tive and semi-deﬁnite. To see that our proposed method pro-
tational cost of kernel evaluation but introduces quantization duces positive and semi-deﬁnite kernels, we ﬁrst note that (4)
error. This is alleviated by allowing for range overlapping is positive and semi-deﬁnite. In [Haussler, 1999],suchker-
and similarity weighting. However, overlapping must be al- nels are called summation kernels and proven to be positive
lowed with care. Increasing the size of range overlaps will re- and semi-deﬁnite. It is not difﬁcult to see that (5) is just a sum
quire additional computation since it is likely to involve more of summation kernels. Since we can synthesize a new Mercer
kernel evaluations as more elements are found in each inter- kernel by adding Mercer kernels, (5) is a Mercer kernel.
section. The gain of decreasing quantization error may pro-
vide little improvement in classiﬁcation performance if we 3.5 Efﬁciency
are swamped by bad comparisons. Thus there is a tradeoff
between quantization error and classiﬁcation performance. The time complexity to compute parametric kernels depends
  One issue left is the time to compute the decomposed ele- greatly on the particular decomposition scheme used. Here
ment sets. The more complicated a decomposition scheme we provide a brief analysis only for regular decomposition
gets, the more difﬁcult the implementation becomes and schemes.
the more time it takes to run. Fortunately, our experimen- Assume that constant time is needed to evaluate κx and
tation has revealed that classiﬁcation performance is rel- κτ and that we are using a regular decomposition scheme
atively insensitive to minor changes in the decomposition as in Figure 2 or 3. Then the time complexity of evaluat-
scheme. Therefore, we can often favor simpler decomposi- ing (5) for sequences x and z composed of |x| and |z| el-
tion schemes for ease of implementation and efﬁcient kernel ements, respectively, is, on average, O(|x||z|Δ/L),where
evaluation with the expectation of only minimal losses in per- L =max(τ(x|x|), τ (z|z|)). In the worst case without de-
formance.                                             composition, Δ=L, so the complexity is O(|x||z|).Ingen-
  We now present a number of example parameter space de- eral, we expect decomposition to produce Δ 
 L since we
composition schemes.                                  would like to have only a reasonably small subset of elements
                                                      in each range.
Regular Decomposition  Parameter ranges all have a com- The storage complexity is O(1) since we only keep the sum
mon length Δ as in Figure 2 or 3. Implementation is simple, of kernel evaluations in memory. The time complexity to de-
and it shows good classiﬁcation performance in general. But compose the sequences into their respective element sets is
we have limited freedom to ﬁt the data.               O(|x| + |z|), if constant time is taken for each element.

                                                IJCAI-07
                                                  10504  Results                                               Class  # SVs    Error   Class  # SVs    Error
                                                          ’1’     17    0.98 %    ’6’    18     1.15 %
For our handwritten character and object recognition experi- ’2’  18    1.12 %    ’7’    17     0.41 %
ments, an implementation based on Matlab SVM Toolbox in   ’3’     15    0.10 %    ’8’    19     0.23 %
[         ]
Gunn, 1997  was used. For handwritten character recogni-  ’4’     19    0.89 %    ’9’    14     0.09 %
tion, we limited the scope of our testing to the recognition of ’5’ 17  1.01 %    ’0’    18     0.01 %
isolated characters. We built our own training and test sets,
similar to those in UNIPEN data1.
                                                             Figure 6: Handwritten Number Recognition
4.1  Handwritten Character Recognition
Handwritten characters are represented as sequences of 2D 2002; Bahlmann et al., 2002]. However, unlike those ap-
(x, y) coordinates of pixels (points) on the screen. Our ob- proaches, we achieved this without any sophisticated feature
jective is to learn from a training set a function that correctly extraction or restrictions on the input structures.
classiﬁes an unseen handwritten character. We normalize by
scaling inputs to ﬁt a 300×300 pixel bounding box and align- 4.2 Object Recognition from Sensor Data
ing them at the center. The input data points in each sequence We also analyzed sensor data captured at 0.1 second intervals
are chosen as the elements. Our training set is composed of by a Hokuyo URG-04LX laser range ﬁnder2 mounted on the
200 labeled examples created by two writers, each of whom front side of a Segway RMP3. The Segway RMP navigates
wrote numeric characters from ’0’ to ’9’ ten times. Our test in its environment under the control of an attached tablet PC
data is composed of 500 labeled examples created by other communicating with it via USB commands. Our tablet PC
authors, 50 for each character. Figure 5 shows some training program reads the sensor data and detects nearby objects and
examples for characters ’1’ and ’5’, with the number of points obstacles during the navigation. The speciﬁc objective here
in each of them shown below.                          is to locate a subregion in the sensor data that corresponds to
                                                      a soccer ball. Each frame of input data is represented as an
  400          400         400          300

                           380          280                                                             ◦
  350          350
                           360          260           array of 768 regular directional samples in a range of −120
                           340          240
  300          300
                           320          220                 ◦
  250          250         300          200           to 120 . Each of these is the distance to the nearest object
                           280          180
  200          200
                           260          160

                           240          140                                                           1%
  150          150                                    in millimeters, ranging from 20 to 4095 with maximum
                           220          120

  100          100         200          100
  100 150 200 250 300 350 400 −100 −50 0 50 100 150 200 150 200 250 300 350 150 200 250 300 350 error. See Figure 7 (a) for a snapshot.
  19 points    9 points     14 points   12 points

  360          440                      400
  340          420         360                                90   1            90   1              90   1
  320          400         340
                                        350                120   60          120   60
  300          380         320                                                                120
  280          360         300
                                        300
  260          340
                           280                                   0.5               0.5
  240          320                                       150       30     150        30
                           260
                                        250
  220          300
                           240
  200          280
                           220
                                        200
  180          260                                                                                      0.5
                           200                                                            150
  160          240                                      180          0    180         0
                           180
  140          220                      150
  150 200 250 300 350 150 200 250 300 350 200 250 300 350 50 100 150 200 250 300
  14 points    15 points    17 points   21 points        210       330    210       330
                                                           240   300         240  300
                                                              270              270
  Figure 5: Sample Raw Inputs for Handwritten Numbers    (a) Raw Input    (b) Segmented     (c) Detected

  We trained one-versus-all multi-class support vector nov- Figure 7: Raw sensor input is shown in (a) and thick curves
elty detectors (SVNDs) with the parametric kernel. See in (b) are the blobs after segmentation. In (c), the thick round
[Shawe-Taylor and Christianini, 2004; Sch¨olkopf and Smola, blob at angle about 90◦ and distance 0.2 is detected as the
2002] for an introduction to SVND. Parameter space was soccer ball.
regularly decomposed with overlapping allowed as shown in
Figure 3. Window and hop sizes were set to 60 and 30, re- After normalizing the sensor data by dividing it by the
spectively. We chose RBF for κx and κτ with σ =30and  maximum distance, it is segmented into several subregions
ν =0.8,whereν  is the lower bound on the ratio of support called blobs. A blob is a subregion of a frame where all dis-
vectors and at the same time the upper bound on the ratio of tance values are in (θ, 1) and every consecutive values are
outliers. SNVD predicts how novel a test sample is in terms at most δ apart. In this experiment, we used θ =0.05 and
of novelty. Each of the test examples is classiﬁed as the class δ =0.02. Blobs are further normalized by scaling and trans-
that has the minimum novelty value, and the result is com- lating so that the min and max distance values in each blob
pared against its label. The result is shown in Figure 6. The are 0 and 1.
classiﬁcation error was measured as the percentage of mis- Blobs are sequences of scalar values. Ball blobs resemble
classiﬁed examples in the test set. We obtained an error rate semi-circles distorted by some small sensor noise because the
less than 1% in most cases.                           laser range ﬁnder scans from only one side of the ball. How-
  This represents a classiﬁcation accuracy that is compara- ever, since this is also the case for all round objects such as
ble to the recognizer in [Tapia and Rojas, 2003] and superior human legs or beacons, they will confuse our classiﬁer. For
to those in [Lei and Govindaraju, 2005; Keogh and Kasetty,
                                                         2http://www.hokuyo-aut.jp/
  1http://unipen.nici.kun.nl/                            3http://www.segway.com/products/rmp/

                                                IJCAI-07
                                                  1051