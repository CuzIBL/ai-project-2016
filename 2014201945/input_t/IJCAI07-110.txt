       Heuristic Selection of Actions in Multiagent Reinforcement Learning                      ∗

      Reinaldo A. C. Bianchi             Carlos H. C. Ribeiro                Anna H. R. Costa
       FEI University Center     Technological Institute of Aeronautics      Escola Polit´ecnica
 Electrical Engineering Department     Computer Science Division          University of S˜ao Paulo
  S˜ao Bernardo do Campo, Brazil.     S˜ao Jos´e dos Campos, Brazil.         S˜ao Paulo, Brazil.
         rbianchi@fei.edu.br              carlos@comp.ita.br               anna.reali@poli.usp.br

                    Abstract                          of the state-action space, worsening the RL algorithms per-
                                                      formance when applied to multiagent problems. Despite that,
    This work presents a new algorithm, called Heuris- several MRL algorithms have been proposed and success-
    tically Accelerated Minimax-Q (HAMMQ), that al-   fully applied to some simple problems, such as the Minimax-
    lows the use of heuristics to speed up the well-  Q [Littman, 1994], the Friend-or-Foe Q-Learning [Littman,
    known Multiagent Reinforcement Learning algo-     2001] and the Nash Q-Learning [Hu and Wellman, 2003].
                                      H
    rithm Minimax-Q. A heuristic function that in-      An alternative way of increasing the convergence rate of
    ﬂuences the choice of the actions characterises the an RL algorithm is to use heuristic functions for selecting
    HAMMQ algorithm. This function is associated      actions in order to guide the exploration of the state-action
    with a preference policy that indicates that a cer- space in an useful way. The use of heuristics to speed up re-
    tain action must be taken instead of another. A   inforcement learning algorithms has been proposed recently
    set of empirical evaluations were conducted for the by Bianchi et al. [2004], and it has been successfully used in
    proposed algorithm in a simpliﬁed simulator for   policy learning for simulated robotic scenarios.
    the robot soccer domain, and experimental results
                                                        This paper investigates the use of heuristic selection of ac-
    show that even very simple heuristics enhances sig-
                                                      tions in concurrent and on-policy learning in multiagent do-
    niﬁcantly the performance of the multiagent rein-
                                                      mains, and proposes a new algorithm that incorporates heuris-
    forcement learning algorithm.
                                                      tics into the Minimax-Q algorithm, the Heuristically Acceler-
                                                      ated Minimax-Q (HAMMQ). A set of empirical evaluation
1  Introduction                                       of HAMMQ were carried out in a simpliﬁed simulator for the
                                                      robot soccer domain [Littman, 1994]. Based on the results of
Reinforcement Learning (RL) techniques are very attractive these simulations, it is possible to show that using even very
in the context of multiagent systems. The reasons frequently simple heuristic functions the performance of the learning al-
cited for such attractiveness are: many RL algorithms guar- gorithm can be improved.
antee convergence to equilibrium in the limit [Szepesvari and
                                                        The paper is organised as follows: section 2 brieﬂy reviews
Littman, 1996], they are based on sound theoretical founda-
                                                      the MRL approach and describes the Minimax-Q algorithm,
tions, and they provide model-free learning of adequate sub-
                                                      while section 3 presents some approaches to speed up MRL.
optimal control strategies. Besides that, they are also easy to
                                                      Section 4 shows how the learning rate can be improved by
use and have been applied to solve a wide variety of control
                                                      using heuristics to select actions to be performed during the
and planning problems when neither an analytical model nor
                                                      learning process, in a modiﬁed formulation of the Minimax-
a sampling model is available apriori.
                                                      Q algorithm. Section 5 describes the robotic soccer domain
  In RL, learning is carried out on line, through trial-and- used in the experiments, presents the experiments performed,
error interactions of the agent with its environment, receiv- and shows the results obtained. Finally, Section 6 provides
ing a reward (or penalty) at each interaction. In a multia- our conclusions.
gent system, the reward received by one agent depends on the
behaviour of other agents, and a Multiagent Reinforcement
Learning (MRL) algorithm needs to address non-stationary 2 Multiagent Reinforcement Learning
scenarios in which both the environment and other agents
must be represented. Unfortunately, convergence of any RL Markov Games (MGs) – also known as Stochastic Games
algorithm may only be achieved after extensive exploration (SGs) – are an extension of Markov Decision Processes
of the state-action space, which can be very time consuming. (MDPs) that uses elements from Game Theory and allows the
The existence of multiple agents increases by itself the size modelling of systems where multiple agents compete among
                                                      themselves to accomplish their tasks.
  ∗
   The support of FAPESP (Proc. 2006/05667-0), CNPq (Proc. Formally, an MG is deﬁned by [Littman, 1994]:
305558/2004-8) and CAPES/GRICES (Proc. 099/03) is acknowl-
edged.                                                  •S: a ﬁnite set of environment states.

                                                IJCAI-07
                                                   690  •A...A                      A
      1    k: a collection of sets i with the possible ac- Initialise Qˆt(s, a, o).
                    i
    tions of each agent .                             Repeat:
  •T:  S×A1   × ...×Ak  → Π(S): a state transition func- Visit state s.
    tion that depends on the current state and on the actions Select an action a using the  − Greedy rule (eq. 4).
    of each agent.                                       Execute a, observe the opponent’s action o.
  •R    S×A    × ...×A   →                              Receive the reinforcement r(s, a, o)
      i :    1         k     : a set of reward functions                    s
    specifying the reward that each agent i receives.    Observe the next state .
                                                         Update the values of Qˆ(s, a, o) according to:
  Solving an MG consists in computing the policy π : S×
A ×...×A                                                   Qˆt+1(s, a, o) ← Qˆt(s, a, o)+
 1        k that maximizes the reward received by an agent                                
                                                                         α[r(s, a, o)+γVt(s ) − Qˆt(s, a, o)].
along time.                                                    
  This paper considers a well-studied specialization of MGs s ← s .
in which there are only two players, called agent and oppo- Until some stop criterion is reached.
nent, having opposite goals. Such specialization, called zero-
sum Markov Game (ZSMG), allows the deﬁnition of only            Table 1: The Minimax-Q algorithm.
one reward function that the learning agent tries to maximize
while the opponent tries to minimize.
                                                      case, as the agent knows in advance the action taken by the
  A two player ZSMG  [Littman, 1994] is deﬁned by the                                       π  S×A×O
quintuple S, A, O, T , R,where:                     opponent, the policy becomes deterministic, :
                                                      and equation 2 can be simpliﬁed:
  •S: a ﬁnite set of environment states.
                                                                    V (s)=maxmin   Q(s, a, o).        (3)
  •A: a ﬁnite set of actions that the agent can perform.                   a∈A o∈O
  •O
      : a ﬁnite set of actions that the opponent can perform. In this case, the optimal policy is π∗   ≡
                                                                     ∗
  •T:   S×A×O→Π(S): the state transition function,    arg maxa mino Q (s, a, o). A  possible action choice
    where Π(S) is a probability distribution over the set of rule to be used is the standard  − Greedy:
    states S. T (s, a, o, s) deﬁnes a probability of transition  
             s       s         t                                   arg max min Qˆ(s, a, o) if q ≤ p,
    from state to state (at a time +1) when the learn-                   a   o
                          a                                 π(s)=                                     (4)
    ing agent executes action and the opponent performs             arandom               otherwise,
    action o.
                                                            q
  •R:  S×A×O→: the reward function that speciﬁes     where  is a random value with uniform probability in [0,1]
                                                          p    ≤ p ≤
    the reward received by the agent when it executes action and (0   1) is a parameter that deﬁnes the explo-
                                                                                                    p
    a and its opponent performs action o, in state s. ration/exploitation trade-off: the greater the value of ,the
                                                      smaller is the probability of a random choice, and arandom is
  The choice of an optimal policy for a ZSMG is not trivial a random action selected among the possible actions in state
because the performance of the agent depends crucially on s. For non-deterministic action policies, a general formu-
the choice of the opponent. A solution for this problem can lation of Minimax-Q has been deﬁned elsewhere [Littman,
                                 [
be obtained with the Minimax algorithm Russell and Norvig, 1994; Banerjee et al., 2001].
    ]
2002 . It evaluates the policy of an agent regarding all possi- Finally, the Minimax-Q algorithm has been extended to
ble actions of the opponent, and then choosing the policy that cover several domains where MGs are applied, such as
maximizes its own payoff.                             Robotic Soccer [Littman, 1994; Bowling and Veloso, 2001]
  To solve a ZSMG, Littman [1994] proposed the use of a and Economy [Tesauro, 2001].
similar strategy to Minimax for choosing an action in the Q- One of the problems with the Minimax-Q algorithm is that,
Learning algorithm, the Minimax-Q algorithm (see Table 1). as the agent iteratively estimates Q, learning at early stages is
Minimax-Q works essentially in the same way as Q-Learning
                                       a         s    basically random exploration. Also, update of the Q value is
does. The action-value function of an action in a state made by one state-action pair at a time, for each interaction
when the opponent takes an action o is given by:
                                                     with the environment. The larger the environment, the longer
                                          
   Q(s, a, o)=r(s, a, o)+γ   T (s, a, o, s )V (s ), (1) trial-and-error exploration takes to approximate the function
                                                      Q
                         s∈S                           . To alleviate these problems, several techniques, described
                                                      in the next section, were proposed.
and the value of a state can be computed using linear pro-
gramming [Strang, 1988] via the equation:
                                                     3   Approaches to speed up Multiagent
         V (s)=  max   min    Q(s, a, o)πa,     (2)       Reinforcement Learning
                π∈Π(A) o∈O
                          a∈A                         The Minimax-SARSA Algorithm  [Banerjee et al., 2001] is a
where the agent’s policy is a probability distribution over ac- modiﬁcation of Minimax-Q that admits the next action to be
tions, π ∈ Π(A),andπa is the probability of taking the action chosen randomly according to a predeﬁned probability, sep-
a against the opponent’s action o.                    arating the choice of the actions to be taken from the update
  An MG where two players take their actions in consecutive of the Q values. If a is chosen according to a greedy policy,
turns is called an Alternating Markov Game (AMG). In this Minimax-SARSA becomes equivalent to Minimax-Q. But if

                                                IJCAI-07
                                                   691a is selected randomly according to a predeﬁned probabil- variable used to weight the inﬂuence of the heuristic (usually
ity distribution, the Minimax-SARSA algorithm can achieve 1).
better performance than Mimimax-Q [Banerjee et al., 2001]. As a general rule, the value of Ht(s, a, o) used in HAMMQ
  The same work proposed the Minimax-Q(λ) algorithm,  should be higher than the variation among the Qˆ(s, a, o) val-
by combining eligibility traces with Minimax-Q. Eligibility ues for the same s ∈S, o ∈O,insuchawaythatitcan
traces, proposed initially in the TD(λ) algorithm [Sutton, inﬂuence the choice of actions, and it should be as low as
1988], are used to speed up the learning process by track- possible in order to minimize the error. It can be deﬁned as:
ing visited states and adding a portion of the reward received   
to each state that has been visited in an episode. Instead of      max Qˆ(s, i, o) − Qˆ(s, a, o)+η if a = πH (s),
                                                      H  s, a, o     i
updating a state-action pair at each iteration, all pairs with ( )=
                                                                   0 otherwise.
eligibilities different from zero are updated, allowing the re-
                                                                                                      (6)
wards to be carried over several state-action pairs. Banerjee η                               πH s
et al. [2001] also proposed Minimax-SARSA(λ), a combi- where  is a small real value (usually 1) and ( ) is the
nation of the the two algorithms that would be more efﬁcient action suggested by the heuristic policy.
than both of them, because it combines their strengths. As the heuristic function is used only in the choice of the
  A different approach to speed up the learning process is to action to be taken, the proposed algorithm is different from
use each experience in a more effective way, through tempo- the original Minimax-Q in the way exploration is carried out.
                                                      Since the RL algorithm operation is not modiﬁed (i.e., up-
ral, spatial or action generalization. The Minimax-QS algo-             Q
rithm, proposed by Ribeiro et al. [2002], accomplishes spa- dates of the function are the same as in Minimax-Q), our
tial generalization by combining the Minimax-Q algorithm proposal allows that many of the theoretical conclusions ob-
with spatial spreading in the action-value function. Hence, at tained for Minimax-Q remain valid for HAMMQ.
receiving a reinforcement, other action-value pairs that were Theorem 1 Consider a HAMMQ agent learning in a deter-
not involved in the experience are also updated. This is done ministic ZSMG, with ﬁnite sets of states and actions, bounded
by coding the knowledge of domain similarities in a spread- rewards (∃c ∈;(∀s, a, o), |R(s, a, o)| <c), discount factor
ing function, which allows a single experience (i.e., a single γ such that 0 ≤ γ<1 and with values used in the heuristic
loop of the algorithm) to update more than a single cost value. function bounded by (∀s, a, o) hmin ≤ H(s, a, o) ≤ hmax.
The consequence of taking action at at state st is spread to For this agent, Qˆ values will converge to Q∗, with probability
other pairs (s, a) as if the real experience at time t actually one uniformly over all states s ∈S, provided that each state-
was s, a, st+1,rt.                                  action pair is visited inﬁnitely often (obeys the Minimax-Q
                                                      inﬁnite visitation condition).
4  Combining Heuristics and Multiagent
                                                      Proof: In HAMMQ, the update of the value function approxi-
   Reinforcement Learning: the HAMMQ                  mation does not depend explicitly on the value of the heuristic
   Algorithm                                          function. Littman and Szepesvary [1996] presented a list of
The Heuristically Accelerated Minimax Q (HAMMQ) algo- conditions for the convergence of Minimax-Q, and the only
rithm can be deﬁned as a way of solving a ZSMG by making condition that HAMMQ could put in jeopardy is the one that
explicit use of a heuristic function H : S×A×O→to    depends on the action choice: the necessity of visiting each
inﬂuence the choice of actions during the learning process. pair state-action inﬁnitely often. As equation 5 considers an
                                                                       
H(s, a, o) deﬁnes a heuristic that indicates the desirability of exploration strategy – greedy regardless of the fact that the
performing action a when the agent is in state s and the op- value function is inﬂuenced by the heuristic function, this vis-
ponent executes action o.                             itation condition is guaranteed and the algorithm converges.
                                                      q.e.d.
  The heuristic function is associated with a preference pol-
icy that indicates that a certain action must be taken instead of The condition that each state-action pair must be visited an
another. Therefore, it can be said that the heuristic function inﬁnite number of times can be considered valid in practice
deﬁnes a “Heuristic Policy”, that is, a tentative policy used – in the same way that it is for Minimax-Q – also by using
to accelerate the learning process. The heuristic function can other strategies:
be derived directly from prior knowledge of the domain or • Using a Boltzmann exploration strategy [Kaelbling et
from clues suggested by the learning process itself and is used al., 1996].
only during the selection of the action to be performed by the
                                                        • Intercalating steps where the algorithm makes alternate
agent, in the action choice rule that deﬁnes which action a
                                                          use of the heuristic function and exploration steps.
should be executed when the agent is in state s. The action
choice rule used in HAMMQ is a modiﬁcation of the standard • Using the heuristic function during a period of time
 − Greedy rule that includes the heuristic function:     shorter than the total learning time.
                                       
                                                        The use of a heuristic function made by HAMMQ explores
        arg max min  Qˆ(s, a, o)+ξHt(s, a, o) if q ≤ p,
π(s)=        a   o                                    an important characteristic of some RL algorithms: the free
        arandom otherwise,                            choice of training actions. The consequence of this is that
                                                (5)   a suitable heuristic function speeds up the learning process,
where H : S×A×O →is the heuristic function. The sub- otherwise the result is a delay in the learning convergence
script t indicates that it can be non-stationary and ξ is a real that does not prevent it from converging to an optimal value.

                                                IJCAI-07
                                                   692Initialise Qˆt(s, a, o) and Ht(s, a, o).
Repeat:
   Visit state s.
   Select an action a using the modiﬁed −Greedy rule                               B
                                   (Equation 5).
   Execute a, observe the opponent’s action o.                           A
   Receive the reinforcement r(s, a, o)
   Observe the next state s.
   Update the values of Ht(s, a, o).
   Update the values of Qˆ(s, a, o) according to:
     Qˆt+1(s, a, o) ← Qˆt(s, a, o)+
                                    
                   α[r(s, a, o)+γVt(s ) − Qˆt(s, a, o)]. Figure 1: The environment proposed by Littman [1994]. The
   s ← s.                                            picture shows the initial position of the agents.
Until some stop criterion is reached.

           Table 2: The HAMMQ algorithm.

  The complete HAMMQ algorithm is presented on table
2. It is worth noticing that the fundamental difference be-
tween HAMMQ and the Minimax-Q algorithm is the action
choice rule and the existence of a step for updating the func-
tion Ht(s, a, o).

5  Robotic Soccer using HAMMQ
Playing a robotic soccer game is a task for a team of multi-
ple fast-moving robots in a dynamic environment. This do- Figure 2: The heuristic policy used for the environment of
main has become of great relevance in Artiﬁcial Intelligence ﬁgure 1. Arrows indicate actions to be performed.
since it possesses several characteristics found in other com-
plex real problems; examples of such problems are: robotic
automation systems, that can be seen as a group of robots in The heuristic policy used was deﬁned using a simple rule:
an assembly task, and space missions with multiple robots if holding the ball, go to the opponent’s goal (ﬁgure 2 shows
[Tambe, 1998], to mention but a few.                  the heuristic policy for player A of ﬁgure 1). Note that the
  In this paper, experiments were carried out using a sim- heuristic policy does not take into account the opponents po-
ple robotic soccer domain introduced by Littman [1994] and sition, leaving the task of how to deviate from the opponent to
modelled as a ZSMG between two agents. In this domain, the learning process. The values associated with the heuristic
twoplayers,AandB,competeina4x5gridpresentedin         function are deﬁned based on the heuristic policy presented
ﬁgure 1. Each cell can be occupied by one of the players, in ﬁgure 2 and using equation 6.
which can take an action at a turn. The action that are allowed The parameters used in the experiments were the same for
indicate the direction of the agent’s move – north, south, east the two algorithms, Minimax-Q and HAMMQ. The learning
                                                                        α    .                   .
and west – or keep the agent still.                   rate is initiated with =10 and has a decay of 0 9999954
  The ball is always with one of the players (it is represented by each executed action. The exploration/ exploitation rate
                                                      p     .                     γ     .
by a circle around the agent in ﬁgure 1). When a player ex- =02 and the discount factor =09 (these parameters
ecutes an action that would ﬁnish in a cell occupied by the are identical to those used by Littman [1994]). The value of
                                                      η
opponent, it looses the ball and stays in the same cell. If an was set to 1. The reinforcement used was 1000 for reach-
action taken by the agent leads it out the board, the agent ing the goal and -1000 for having a goal scored by the op-
stands still.                                         ponent. Values in the Q table were randomly initiated, with
                                                        ≤ Q  s, a, o ≤
  When a player with the ball gets into the opponent’s goal, 0 (  )   1. The experiments were programmed in
the move ends and its team scores one point. At the beginning C++ and executed in a AMD K6-II 500MHz, with 288MB of
of each game, the agents are positioned in the initial position, RAM in a Linux platform.
depicted in ﬁgure 1, and the possession of the ball is randomly Thirty training sessions were run for each algorithm, with
determined, with the player that holds the ball making the each session consisting of 500 matches of 10 games. A game
ﬁrst move (in this implementation, the moves are alternated ﬁnishes whenever a goal is scored by any of the agents or
between the two agents).                              when 50 moves are completed.
  To solve this problem, two algorithms were used:      Figure 3 shows the learning curves (average of 30 train-
  •                                                   ing sessions) for both algorithms when the agent learns how
    Minimax-Q, described in section 2.                to play against an opponent moving randomly, and presents
  • HAMMQ, proposed in section 4.                     the average goal balance scored by the learning agent in each

                                                IJCAI-07
                                                   693   10                                                   16
                                  Minimax−Q                                               T module
                                    HAMMQ                                                 5% Limit
                                                        14                              0.01% Limit
    8
                                                        12

                                                        10
    6

                                                         8
  Goals
    4
                                                         6

                                                         4
    2
                                                         2

    0                                                    0
         50  100 150 200 250  300 350 400  450 500            50  100 150  200 250 300  350 400  450 500
                        Matches                                               Matches

Figure 3:  Average goal balance for the Minimax-Q     Figure 5: Results from Student’s t test between Minimax-Q
and HAMMQ    algorithms against a random opponent for and HAMMQ algorithms, training against a random oppo-
Littman’s Robotic Soccer.                             nent.

    7                                                   12
                                  Minimax−Q                                               T Module
    6                               HAMMQ                                                 5% Limit
                                                                                        0.01% Limit
                                                        10
    5

    4                                                    8

    3
                                                         6

  Goals 2

    1                                                    4

    0
                                                         2
   −1

   −2                                                    0
     0   50  100 150 200 250  300 350 400  450 500            50  100 150  200 250 300  350 400  450 500
                        Matches                                               Matches

Figure 4: Average goal balance for the Minimax-Q and  Figure 6: Results from Student’s t test between Minimax-
HAMMQ algorithms against an agent using Minimax-Q for Q and HAMMQ algorithms, training against an agent using
Littman’s Robotic Soccer.                             Minimax-Q.

match. It is possible to verify that Minimax-Q has worse per-
formance than HAMMQ at the initial learning phase, and that
                                                      see that HAMMQ is better than Minimax-Q until the 150th
as the matches proceed, the performance of both algorithms
                                                      match, after which the results are comparable, with a level
become similar, as expected.
                                                      of conﬁdence greater than 5%. Figure 6 presents similar re-
  Figure 4 presents the learning curves (average of 30 train- sults for the learning agent playing against an opponent using
ing sessions) for both algorithms when learning while playing Minimax-Q.
against a learning opponent using Minimax-Q. In this case, it
can be clearly seen that HAMMQ is better at the beginning of Finally, table 3 shows the cumulative goal balance and ta-
the learning process and that after the 300th match the perfor- ble 4 presents the number of games won at the end of 500
mance of both agents becomes similar, since both converge to matches (average of 30 training sessions). The sum of the
equilibrium.                                          matches won by the two agents is different from the total
  Student’s t–test [Spiegel, 1998] was used to verify the hy- number of matches because some of them ended in a draw.
pothesis that the use of heuristics speeds up the learning pro- What stands out in table 4 is that, due to a greater number
cess. Figure 5 shows this test for the learning of the Minimax- of goals scored by HAMMQ at the beginning of the learning
Q and the HAMMQ algorithms against a random opponent  process, this algorithm wins more matches against both the
(using data shown in ﬁgure 3). In this ﬁgure, it is possible to random opponent and the Minimax-Q learning opponent.

                                                IJCAI-07
                                                   694