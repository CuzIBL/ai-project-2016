                       Shallow Semantics for Coreference Resolution

                                              Vincent Ng
                            Human Language Technology Research Institute
                                      University of Texas at Dallas
                                         Richardson, TX 75083
                                         vince@hlt.utdallas.edu

                    Abstract                          the president to George W. Bush if such background informa-
                                                      tion was not provided explicitly in the associated document.
    This paper focuses on the linguistic aspect of noun Our goal in this paper is to improve the performance of
    phrase coreference, investigating the knowledge   a learning-based coreference system by introducing features
    sources that can potentially improve a learning-  that encode semantic knowledge as well as knowledge that
    based coreference resolution system. Unlike       is potentially useful for identifying non-anaphoric NPs (i.e.,
    traditional, knowledge-lean coreference resolvers, NPs that do not have an antecedent and hence do not need
    which rely almost exclusively on morpho-syntactic to be resolved). To evaluate the utility of the new linguistic
    cues, we show how to induce features that encode  features, we augment a baseline feature set (which comprises
    semantic knowledge from labeled and unlabeled     knowledge sources commonly employed by existing corefer-
    corpora. Experiments on the ACE data sets indicate ence engines) with these new features. In an evaluation on the
    that the addition of these new semantic features to ACE datasets, the coreference system using the augmented
    a coreference system employing a fairly standard  feature set yields a statistically signiﬁcant improvement of
    feature set signiﬁcantly improves its performance. 2.2-2.3% in F-measure over the baseline system.
                                                        Another contribution of our work lies in the use of corpus-
1  Introduction                                       based methods for inducing features for coreference resolu-
Recent years have seen an intensifying interest in noun phrase tion. Although there have been a few attempts on inducing
                                                            [            ]               [
(NP) coreference — the problem of determining which NPs gender Ge et al., 1998 , path coreference Bergsma and Lin,
                                                          ]                [                  ]
refer to the same real-world entity in a document, owing in 2006 , NP anaphoricity Bean and Riloff, 1999 , and selec-
                                                                      [                                 ]
part to the Automatic Content Extraction (ACE) evaluations tional preferences Dagan and Itai, 1990; Yang et al., 2005
initiated by NIST as well as the surge of interest in structured for coreference resolution, most of the existing coreference
prediction in the machine learning community. As a result, resolvers rely on heuristic methods for feature computation.
various new models and approaches to NP coreference are
developed. For instance, coreference has been recast as the 2 New Features for Coreference Resolution
problem of ﬁnding the best path from the root to a leaf in a In this section we describe the new linguistic features for our
Bell tree [Luo et al., 2004], and tackled both as a relational learning-based coreference resolution system.
learning task (see McCallum and Wellner [2004])andasa
supervised clustering task (see Li and Roth [2005]).  2.1  Inducing a Semantic Agreement Feature
  Equally important to the development of new coreference A feature commonly employed by coreference resolvers for
models is the investigation of new linguistic features for determining whether two NPs are coreferent is the semantic
the problem. However, until recently, research in anaphora class (SC) agreement feature, which has the value true if the
and coreference resolution has largely adopted a knowledge- SCs of two NPs match and false otherwise. The accuracy
lean approach, in which resolvers operate by relying on a of the SC agreement feature, therefore, depends on whether
set of morpho-syntactic cues. While these knowledge-lean the SC values of the two NPs are computed correctly. For
approaches have been reasonably successful (see Mitkov et a named entity (NE), the SC is typically determined using
al. [2001]), Kehler et al. [2004] speculate that deeper linguis- an NE recognizer; on the other hand, determining the SC of
tic knowledge needs to be made available to resolution sys- a common noun proves more difﬁcult, in part because many
tems in order to reach the next level of performance. In fact, words are polysemous and it is non-trivial to determine which
it should not be surprising that certain coreference relations sense corresponds to the intended meaning of the noun.
cannot be identiﬁed by using string-matching facilities and To determine the SC of a common noun, many existing
syntactic knowledge alone. For instance, semantic knowl- coreference systems use WordNet (e.g., Soon et al. [2001]),
edge is needed to determine the coreference relation between simply assigning to the noun the ﬁrst (i.e., most frequent)
two lexically dissimilar common nouns (e.g., talks and nego- WordNet sense as its SC. It is not easy to measure the ac-
tiations), and world knowledge might be required to resolve curacy of this heuristic, but the fact that the SC agreement

                                                IJCAI-07
                                                  1689  PERSON, ORGANIZATION, TIME, DAY, MONEY, PERCENT,      PERSON        human
  MEASURE, ABSTRACTION, PSYCHOLOGICAL FEATURE, PHE-     ORGANIZATION  corporation, agency, government
  NOMENON, STATE, GROUP, OBJECT, UNKNOWN                FACILITY      man-made structure (e.g., building)
                                                        GSP           geo-political region (e.g., country, city)
Table 1: List of the possible semantic class values of a com- LOCATION geographical area and landmass, body of
mon noun returned by the ﬁrst-sense heuristic method.                 water, geological formation

feature was not used by Soon et al.’s decision tree corefer-      Table 2: ACE semantic classes.
ence classiﬁer seems to suggest that the SC values of the NPs
were not computed accurately by this “ﬁrst-sense” heuristic. ORGANIZATION social group
  Motivated by related work on semantic lexicon construc- FACILITY    establishment, construction, building, facil-
tion (e.g., Hearst [1992], Phillips and Riloff [2002]), we de-        ity, workplace
velop the following method for learning the SC of a com- GSP          country, province, government, town, city,
mon noun, with the goal of improving the accuracy of the              administration, society, island, community
SC agreement feature. Given a large, unannotated corpus1, LOCATION    dry land, region, landmass, body of water,
                                                                      geographical area, geological formation
we use (1) an in-house NE recognizer (which achieves an F-
measure of 93% on the MUC-6 test set) to label each NE Table 3: List of keywords used in WordNet search for deter-
                               [     ]
with its semantic class, and (2) Lin’s 1998b MINIPAR de- mining the ACE semantic class of a common noun.
pendency parser to extract all the appositive relations. An ex-
ample extraction would be <Eastern Airlines, the carrier>,
where the ﬁrst entry is a proper noun labeled with either one is deﬁned, we may be able to improve system performance on
of the seven MUC-style NE types2 or OTHERS3 and the sec- the ACE data if we develop another semantic class agreement
ond entry is a common noun. If the proper noun is not labeled feature with the ACE guidelines in mind. Speciﬁcally, the
as OTHERS, we may infer the SC of the common noun from ACE coreference task is concerned with resolving references
that of the proper noun. However, since neither MINIPAR nor to NPs belonging to one of the ﬁve ACE semantic classes
the NE recognizer is perfect, we use a more robust method (ASCs), namely, PERSON, ORGANIZATION, FACILITY, GSP
for inferring the SC of a common noun: (1) we compute the (a geographical-social-political region), and LOCATION [see
probability that the common noun co-occurs with each of the Table 2 for a brief description of the ASCs]. In particular, ref-
eight NE types4 based on the extracted appositive relations, erences to NPs belonging to other SCs are not to be marked
and (2) if the most likely NE type has a co-occurrence prob- up. Hence, we desire an ACE SEM CLASS feature that con-
ability above a certain threshold (we set the threshold to 0.7), siders two NPs semantically compatible if and only if the two
we label the common noun with the most likely NE type. NPs have a common ASC. The rest of this subsection de-
  An examination of the induced SC values indicates that our scribes how we determine the ASC of an NP. As we will see,
method ﬁxes some of the errors commonly made by the ﬁrst- we allow an NP to possess more than one ASC in some cases.
sense heuristic. For instance, common nouns such as carrier Our method for determining the ASC of an NP is based
and manufacturer are typically used to refer to organizations in part on its SC value as computed by the SEM CLASS fea-
in news articles, but were labeled as PERSON by the heuristic. ture. In particular, the method hinges on the observation
  Nevertheless, our method has a potential weakness: com- that (1) SEM CLASS’s ORGANIZATION class roughly corre-
mon nouns that do not belong to any of the seven NE seman- sponds to two ASCs: FACILITY and ORGANIZATION,and(2)
tic classes remain unlabeled. To address this problem, we will SEM CLASS’s LOCATION class roughly corresponds to two
set the SC of an unlabeled common noun to be the value re- ASCs: GSP and LOCATION. Given this observation, we can
turned by the ﬁrst-sense heuristic. (In our implementation of determine the ASC of an NP as follows:
the ﬁrst-sense heuristic, we determine which of the 14 SCs • If its SEM CLASS value is not PERSON, ORGANIZATION,
listed in Table 1 a common noun belongs to based on the ﬁrst or LOCATION, its ASC will be OTHERS.
WordNet sense.) However, we expect that our method will be • If its SEM CLASS is PERSON, its ASC will be PERSON.
able to label most of the common nouns, because in ACE we
                                                        • If its SEM CLASS is LOCATION, we will have to deter-
are primarily interested in nouns referring to a person, orga-
                                                          mine whether its ASC is LOCATION or GSP, according to
nization, or location, as we will see in the next subsection. our observation above. Speciﬁcally, we ﬁrst use Word-
2.2  Inducing an ACE-Speciﬁc Semantic Feature             Net to determine whether the head noun of the NP is a
                                                          hypernym of one of the GSP keywords listed in Table 3.5
The SEM CLASS feature described in the previous subsection We then repeat this WordNet lookup procedure using the
was developed for use in a general-purpose coreference sys-
                                                          LOCATION  keywords. If both lookups are successful, the
tem. However, because of the way the ACE coreference task
                                                          ASC of the NP will be both GSP and LOCATION;other-
  1We used (1) the BLLIP corpus (30M words), which consists of wise the ASC will be one of these two classes.
Wall Street Journal articles from 1987 to 1989, and (2) the Reuters • If its SEM CLASS is ORGANIZATION, we will have to
Corpus (3.7GB data), which has 806,791 Reuters articles.
                                                          determine whether its ASC is ORGANIZATION or FA-
  2Person, organization, location, date, time, money, and percent.
  3This indicates the proper noun is not a MUC NE.       5The keywords are obtained via our experimentation with Word-
  4For simplicity, OTHERS is viewed as an NE type here. Net and the ASCs of the NPs in the training data.

                                                IJCAI-07
                                                  1690    CILITY. We can similarly use the procedure outlined in extract coreferent NP pairs. Speciﬁcally, we design a pattern
    the previous bullet to determine whether its ASC is OR- learner that induces from each CS in a given annotated corpus
    GANIZATION, FACILITY, or both.                    three extraction patterns, each of which represents a different
                                                      degree of generalization from the CS. In this work, we only
2.3  Inducing a Semantic Similarity Feature           consider segments in which the antecedent and the anaphor
Many reference resolvers use WordNet to compute the se- are fewer than three sentences apart.
mantic similarity between two common nouns (e.g., Poesio Table 4 shows the three patterns that the learner will induce
et al. [2004] and Daum´e and Marcu [2005]). However, this for the example CS above. The ﬁrst pattern (see row 1) is cre-
approach to determining semantic similarity may not be ro- ated by (1) representing the antecedent and the anaphor as a
bust, since its success depends to a large extent on the ability set of attribute values indicating its gender, number, semantic
                                                                                     6
to determine the correct WordNet sense of the given nouns. class, grammatical role, and NP type ; (2) representing each
  Motivated by research in lexical semantics, we instead of the remaining NPs in the CS by the token NP; (3) repre-
adopt a distributional approach to computing the semantic senting each non-verbal and non-adverbial token that is not
similarity between two common nouns: we capture the se- enclosed by any NP by its part-of-speech tag; and (4) repre-
mantics of a noun by counting how frequent it co-occurs with senting each verbal token as it is. The reason for retaining
other words, determining a pair of common nouns to be se- the verbal tokens is motivated by the intuition that verbs can
mantically similar if their co-occurrence patterns are similar. sometimes play an important role in identifying coreferent
  Instead of acquiring semantic similarity information from NP pairs. On the other hand, adverbs are not represented be-
scratch, we use the semantic similarity values provided by cause they generally do not contain useful information as far
Lin’s [1998a] dependency-based thesaurus, which is con- as identifying coreferent NP pairs is concerned.
structed using a distributional approach combined with an The second pattern (shown in row 2 of Table 4) is created
information-theoretic deﬁnition of similarity. Each word w via the same procedure as the ﬁrst pattern, except that each
in the thesaurus is associated with a list of words most simi- verbal token is replaced by its part-of-speech tag. The third
lar to w together with the semantic similarity values. pattern (see row 3 of Table 4) is created via the same proce-
  Given the thesaurus, we can construct a semantic similarity dure as the preceding two patterns, except that only the NPs
feature, SEM SIM, for coreference resolution, in which we use are retained. So, the three patterns represent three different
a binary value to denote whether two NPs, NPx and NPy,are levels of generalizations from the CS, with the ﬁrst one being
semantically similar. Speciﬁcally, the feature has the value the most speciﬁc and the third one being the most general.
true if and only if NPx is among the 5-nearest neighbors of Note that some of the induced patterns may extract both
NPy according to the thesaurus or vice versa.         coreferent and non-coreferent NP pairs, thus having a low
                                                      extraction accuracy. The reason is that our pattern learner
2.4  Inducing a Pattern-Based Feature                 does not capture evidence outside a CS segment, which in
                                                      some cases may be crucial for inducing high-precision rules.
Next, we induce a PATTERN BASED feature using informa- Hence, we need to estimate the accuracy of each pattern, so
tion provided by an algorithm that learns patterns for extract- that a coreference system can decide whether it should dis-
ing coreferent NP pairs, each of which involves a pronoun and card NP pairs extracted by patterns with a low accuracy.
its antecedent. Bean and Riloff [2004] also learn extraction
patterns for coreference resolution, but unlike our method, Estimating pattern accuracy. After we acquire a list of
their method is unsupervised and domain-speciﬁc.      extraction patterns L (with the redundant patterns removed),
  Before showing how to compute our PATTERN BASED fea- we can estimate the accuracy of each pattern on an anno-
ture, let us describe the pattern learner, which operates as fol- tated corpus simply by counting the number of coreferent NP
lows: (1) patterns are acquired from a corpus annotated with pairs it extracts divided by the total number of NP pairs it ex-
coreference information, and (2) the accuracy of each learned tracts, as described below. First, we collect all the CS’s and
pattern is estimated. Below we elaborate these two steps. non-coreference segments (NCS’s) from the same annotated
                                                      corpus that we used to induce our patterns, where an NCS
Acquiring the patterns. Recall that a pattern is used to ex- is deﬁned as a text segment beginning with an NP, NPx,and
tract coreferent NP pairs; hence a good pattern should cap- ends with a pronoun, NPy, such that NPx is not coreferent with
ture features of the two NPs involved as well as the context NPy. (As before, we only consider CS’s and NCS’s where
in which they occur. To illustrate how to induce a pattern, let the two enclosing NPs are separated by fewer than three sen-
us consider the following coreference segment (CS), which tences.) From each CS/NCS, we use our pattern learner to
we deﬁne as a text segment that starts with an NP, NPx,and induce three patterns in the same way as before, but this time
ends with a pronoun that co-refers with NPx: “John is study- we additionally label each pattern with a ’-’ if it was induced
ing hard for the exam. He”. From this CS, we can induce a from an NCS and a ’+’ otherwise. We then insert all these
pattern that simply comprises all the tokens in the segment. If labeled patterns into a list S, with the redundant patterns re-
we see this sequence of tokens in a test text, we can apply this tained. Finally, we compute the accuracy of a pattern l ∈ L as
pattern to determine that John and He are likely to co-refer. the number of times l appears in S with the label ’+’ divided
  The above pattern, however, may not be useful because it by the total number of times l appears in S.
is unlikely that we will see exactly the same text segment in
an unseen text. Hence, we desire a pattern learner that can 6The possible NP types are PRONOUN, PROPER NOUN,and
generalize from a CS and yet retain sufﬁcient information to COMMON NOUN.

                                                IJCAI-07
                                                  1691      1.  { GENDER:M, NUMBER:SING, SEMCLASS:PERSON, GRAMROLE:SUBJ, NPTYPE:PN } is studying IN NP . { GEN-
          DER:M, NUMBER:SING, SEMCLASS:PERSON, GRAMROLE:SUBJ, NPTYPE:PRO }
      2.  { GENDER:M, NUMBER:SING, SEMCLASS:PERSON, GRAMROLE:SUBJ, NPTYPE:PN } VBZ VBG IN NP . { GEN-
          DER:M, NUMBER:SING, SEMCLASS:PERSON, GRAMROLE:SUBJ, NPTYPE:PRO }
      3.  { GENDER:M, NUMBER:SING, SEMCLASS:PERSON, GRAMROLE:SUBJ, NPTYPE:PN } NP { GENDER:M, NUM-
          BER:SING, SEMCLASS:PERSON, GRAMROLE:SUBJ, NPTYPE:PRO }

        Table 4: The three patterns induced for the coreference segment “John is studying hard for the exam. He”.

  Using the list of extraction patterns L sorted in decreasing feature is useful at all for coreference resolution is an empiri-
order of accuracy, we can create our PATTERN BASED feature cal question, and we will evaluate its utility in Section 4.
as follows. Given a pair of NPs, we march down the pattern
list L and check if any of the patterns can extract the two NPs. 3 The Baseline Feature Set
If so, the feature value is the accuracy of the ﬁrst pattern that
extracts the two NPs; otherwise the feature value is 0. The previous section introduced our new features for corefer-
                                                      ence resolution. As mentioned in the introduction, these fea-
2.5  Inducing an Anaphoricity Feature                 tures will be used in combination with a set of baseline fea-
                                                      tures. This section describes our baseline feature set, which
Anaphoricity determination refers to the problem of deter-
                                                      comprises 34 selected features employed by high-performing
mining whether an NP has an antecedent or not. Knowledge
                                                      coreference systems such as Soon et al. [2001],Ngand
of anaphoricity can potentially be used to identify and ﬁlter
                                                      Cardie [2002], and Ponzetto and Strube [2006].
non-anaphoric NPs prior to coreference resolution, thereby
improving the precision of a coreference system. There have Lexical features. We use nine features to allow different
                                                      types of string matching operations to be performed on the
been attempts on identifying non-anaphoric phrases such                          7
as pleonastic it (e.g., Lappin and Leass [1994]) and non- given pair of NPs, NPx and NPy , including (1) exact string
anaphoric deﬁnite descriptions (e.g., Bean and Riloff [1999]). match for pronouns, proper nouns, and non-pronominal NPs
                                                      (both before and after determiners are removed); (2) substring
  Unlike previous work, our goal here is not to build a full-
                                                      match for proper nouns and non-pronominal NPs; and (3)
ﬂedged system for identifying and ﬁltering non-anaphoric
                                                      head noun match. In addition, a nationality matching feature
NPs. Rather, we want to examine whether shallow anaphoric-
                                                      is used to match, for instance, British with Britain.Further-
ity information, when encoded as a feature, could beneﬁt a
                                                      more, we have a feature that tests whether all the words that
learning-based coreference system. Speciﬁcally, we employ a
                                                      appear in one NP also appear in the other NP.
simple method for inducing anaphoricity information: given
a corpus labeled with coreference information, we compute Grammatical features. 23 features test the grammatical
                                                      properties of one or both of the NPs. These include ten fea-
the anaphoricity value of an NP, NPx, as the probability that
                                                      tures that test whether each of the two NPs is a pronoun, a def-
NPx has an antecedent in the corpus. If NPx never occurs in
the annotated corpus, we assign to it the default anaphoric- inite NP, an indeﬁnite NP, a nested NP, and a clausal subject.
ity value of -1. Hence, unlike previous work, we represent A similar set of ﬁve features is used to test whether both NPs
anaphoricity as a real value rather than a binary value. are pronouns, deﬁnite NPs, nested NPs, proper nouns, and
  Now we can encode anaphoricity information as a feature clausal subjects. In addition, ﬁve features determine whether
for our learning-based coreference system as follows. Given the two NPs are compatible with respect to gender, number,
                                                      animacy, and grammatical role. Furthermore, two features
a coreference instance involving NPx and NPy, we create a fea-
                                                      test whether the two NPs are in apposition or participate in
ture whose value is simply NPy’s anaphoricity value.
  Conceivably, data sparseness may render our ANAPHORIC- a predicate nominal construction (i.e., the IS-A relation). Fi-
                                                      nally, motivated by Soon et al. [2001], we have a feature that
ITY feature less useful than we desire. However, a glimpse at
the anaphoricity values computed by this feature shows that determines whether NPy is a demonstrative NP.
it can capture some potentially useful information. For in- Semantic features. There are two semantic features, both
stance, the feature encodes that it only has a moderate proba- of which are employed by Soon et al.’s coreference system.
bility of being anaphoric, and the NP the contrary taken from The ﬁrst feature tests whether the two NPs have the same
the phrase on the contrary is never anaphoric.        semantic class. Here, the semantic class of a proper noun and
                                                      a common noun is computed using an NE ﬁnder and WordNet
2.6  Inducing a Coreferentiality Feature              (by choosing the ﬁrst sense), respectively. The second feature
We can adapt the above method for generating the anaphoric- tests whether one NP is a name alias or acronym of the other.
ity feature to create a COREFERENTIALITY feature, which Positional features. We have one positional feature that
encodes the probability that two NPs are coreferent. These measures the distance between the two NPs in sentences.
coreferentiality probabilities can again be estimated from
a corpus annotated with coreference information; in cases 4 Evaluation
where one or both of the given NPs do not appear in the cor- In this section, we evaluate the effectiveness of our newly pro-
pus, we set the coreferentiality value of the NP pair to -1. posed features in improving the baseline coreference system.
  Our method of inducing the COREFERENTIALITY feature
                                                         7
may also suffer from data sparseness. However, whether this We assume that NPx precedes NPy in the associated text.

                                                IJCAI-07
                                                  16924.1  Experimental Setup                               In addition, we replace the heuristic-based SC agreement fea-
We use the ACE-2 (Version 1.0) coreference corpus for eval- ture in the baseline feature set with our SEM CLASS feature
uation purposes. The corpus is composed of three data sets (see Section 2.1). We employ the same methods for training
taken from three different news sources: Broadcast News instance creation and antecedent selection as in the baseline.
(BR), Newspaper (PA), and Newswire (WI). Each data set  Recall that the PATTERN BASED, ANAPHORICITY,and
comprises a set of training texts for acquiring coreference COREFERENTIALITY features are all computed using a data
classiﬁers and a set of test sets for evaluating the output of set annotated with coreference information. Hence, we need
the coreference system. We report performance in terms of to reserve a portion of our training texts for the purpose of
recall, precision, and F-measure using two different scoring computing these features. Speciﬁcally, we partition the avail-
programs: the commonly-used MUC scorer [Vilain et al., able training texts into two sets of roughly the same size: the
1995] and the recently-developed CEAF scorer [Luo, 2005]. training subset and the development subset. The development
According to Luo, CEAF is designed to address a potential subset will be used for computing those features that require
problem with the MUC scorer: partitions in which NPs are an annotated corpus, and the training subset will be used to
over-clustered tend to be under-penalized. For all of the ex- train the coreference classiﬁer using the expanded feature set.
periments conducted in this paper, we use NPs automatically Results using the expanded feature set are shown in row 3
extracted by an in-house NP chunker and an NE recognizer. of the two tables. In comparison to the baseline results in row
                                                      1, we see that F-measure increases from 62.0 to 64.2 (MUC)
4.2  The Baseline Coreference System                  and 60.0 to 62.3 (CEAF). Although the gains may seem mod-
Our baseline system uses the C4.5 decision tree learning al- erate, the performance difference as measured by both scor-
gorithm [Quinlan, 1993] in conjunction with the 34 baseline ers is in fact highly statistically signiﬁcant, with p=0.0004 for
features described in Section 3 to acquire a coreference clas- MUC and p=0.0016 for CEAF.
siﬁer on the training texts for determining whether two NPs
are coreferent. To create training instances, we pair each NP 4.4 Feature Analysis
in a training text with each of its preceding NPs, labeling an
instance as positive if the two NPs are in the same coreference To better understand which features are important for coref-
chain in the associated text and negative otherwise.  erence resolution, we examine the decision tree learned us-
  After training, the decision tree classiﬁer is used to select ing the expanded feature set (not shown here due to space
an antecedent for each NP in a test text. Following Soon et limitations). At the top of the tree are the two lexical fea-
al. [2001], we select as the antecedent of each NP, NPj,the tures that test exact string match for proper nouns and for
                                                      non-pronominal NPs. This should not be surprising, since
closest preceding NP that is classiﬁed as coreferent with NPj.
                                                      these string matching features are generally strong indicators
If no such NP exists, no antecedent is selected for NPj .
  Row 1 of Table 5 and Table 6 shows the results of the of coreference. Looking further down the tree, we see the
baseline system obtained via the MUC scorer and the CEAF SEM CLASS, ANAPHORICITY,andCOREFERENTIALITY  fea-
scorer, respectively. Each row of the two tables corresponds tures appearing in the third and fourth levels of the tree. This
to an experiment evaluated on four different test sets: the en- indicates that these three features play a signiﬁcant role in
tire ACE test set (comprising all the BR, PA, and WI test determining whether two NPs are coreferent.
texts) and each of the BR, PA, and WI test sets. These four To further investigate the contribution of each of our new
sets of results are obtained by applying the same corefer- features to overall performance, we remove each new feature
ence classiﬁer that is trained on the entire ACE training cor- (one at a time) from the expanded feature set and re-train the
pus (comprising all the training texts from PA, WI, and BR). coreference classiﬁer using the remaining features. Results
Owing to space limitations, we will mainly discuss results are shown in rows 4–9 of Tables 5 and 6, where an asterisk
obtained on the entire test set. As we can see, the baseline (*) is used to indicate that the corresponding F-measure is
achieves an F-measure of 62.0 (MUC) and 60.0 (CEAF).  signiﬁcantly different from that in row 3 (at p=0.05). From
  To get a better sense of how strong these baseline results these results, we make two observations. First, removing
are, we repeat the above experiment except that we replace ANAPHORICITY, COREFERENTIALITY or ACE SEM CLASS
the 34 features with the 12 features employed by Soon et precipitates a signiﬁcant drop in F-measure, whichever scor-
al.’s [2001] coreference resolver. Results of the Soon et al. ing program is used. Interestingly, even though we are faced
system, shown in row 2 of the two tables, indicates that our with data sparseness when computing ANAPHORICITY and
baseline features yield signiﬁcantly better results than Soon et COREFERENTIALITY, both features turn out to be useful.
al.’s8: F-measure increases by 5.4 (MUC) and 3.7 (CEAF). Second, although removing SEM CLASS does not result in
                                                      a signiﬁcant drop in performance, it does not imply that
4.3  Coreference Using the Expanded Feature Set       SEM  CLASS is not useful. In fact, as mentioned at the be-
Next, we train a coreference resolver using the baseline fea- ginning of this subsection, SEM CLASS appears near the top
ture set augmented with the ﬁve new features described in of the tree. An inspection of the relevant decision tree reveals
                                                                             ACE SEM CLASS     SEM CLASS
Sections 2.2–2.6, namely, ACE SEMCLASS, SEM SIM, PAT- that the learner substitutes          for
                                                      when the latter feature is absent. This explains in part why
TERN BASED,  ANAPHORICITY,andCOREFERENTIALITY.
                                                      we do not see a large drop in F-measure. Hence, we can
  8Like the MUC organizers, we use Noreen’s [1989] Approximate only claim that SEM CLASS is not important in the presence
Randomization method for signiﬁcance testing, with p set to 0.05. of ACE SEM CLASS, a feature with which it is correlated.

                                                IJCAI-07
                                                  1693