              State Abstraction Discovery from Irrelevant State Variables

                     Nicholas K. Jong                                Peter Stone
             Department of Computer Sciences              Department of Computer Sciences
               University of Texas at Austin                University of Texas at Austin
                    Austin, Texas 78712                          Austin, Texas 78712
                 nkj@cs.utexas.edu                          pstone@cs.utexas.edu

                    Abstract                          is given, or if the user manually determines that the condi-
                                                      tions hold and supplies the corresponding state abstraction to
    Abstraction is a powerful form of domain knowl-   the RL algorithm.
    edge that allows reinforcement-learning agents to   We propose an alternative basis to state abstraction that is
    cope with complex environments, but in most cases more conducive to automatic discovery. Intuitively, if it is
    a human must supply this knowledge. In the ab-    possible to behave optimally while ignoring a certain aspect
    sence of such prior knowledge or a given model,   of the state representation, then an agent has reason to ig-
    we propose an algorithm for the automatic discov- nore that aspect during learning. Recognizing that discover-
    ery of state abstraction from policies learned in one ing structure tends to be slower than learning an optimal be-
    domain for use in other domains that have similar havior policy [Thrun and Schwartz, 1995], this approach sug-
    structure. To this end, we introduce a novel condi- gests a knowledge-transfer framework, in which we analyze
    tion for state abstraction in terms of the relevance policies learned in one domain to discover abstractions that
    of state features to optimal behavior, and we ex- might improve learning in similar domains. To test whether
    hibit statistical methods that detect this condition abstraction is possible in a given region of the state space, we
    robustly. Finally, we show how to apply temporal  give two statistical methods that trade off computational and
    abstraction to beneﬁt safely from even partial state sample complexity.
    abstraction in the presence of generalization error.
                                                        We must take care when we apply our discovered ab-
                                                      stractions, since the criteria we use in discovery are strictly
1  Introduction                                       weaker than those given in prior work on safe state abstrac-
Humans can cope with an unfathomably complex world due tion. Transferring abstractions from one domain to another
to their ability to focus on pertinent information while ignor- may also introduce generalization error. To preserve conver-
ing irrelevant detail. In contrast, most of the research into gence to an optimal policy, we encapsulate our state abstrac-
artiﬁcial intelligence relies on ﬁxed problem representations. tions in temporal abstractions, which construe sequences of
Typically, the researcher must engineer a feature space rich primitive actions as constituting a single abstract action [Sut-
enough to allow the algorithm to ﬁnd a solution but small ton et al., 1999]. In contrast to previous work with temporal
enough to achieve reasonable efﬁciency. In this paper we abstraction, we discover abstract actions intended just to sim-
consider the reinforcement learning (RL) problem, in which plify the state representation, not to achieve a certain goal
an agent must learn to maximize rewards in an initially un- state. RL agents equipped with these abstract actions thus
known, stochastic environment [Sutton and Barto, 1998]. The learn when to apply state abstraction the same way they learn
agent must consider enough aspects of each situation to in- when to execute any other action.
form its choices without spending resources worrying about In Section 2, we describe our ﬁrst contribution, an alterna-
minutiae. In practice, the complexity of this state represen- tive condition for state abstraction and statistical mechanisms
tation is a key factor limiting the application of standard RL for discovery. In Section 3, we describe our second contribu-
algorithms to real-world problems.                    tion, an approach to discovering state abstractions and then
  One approach to adjusting problem representation is state encapsulating them within temporal abstractions. In Sec-
abstraction, which maps two distinct states in the original tion 4, we present an empirical validation of our approach.
formulation to a single abstract state if an agent should treat In Section 5, we discuss related work, and in Section 6, we
the two states in exactly the same way. The agent can still conclude.
learn optimal behavior if the Markov decision process (MDP)
that formalizes the underlying domain obeys certain con- 2 Policy irrelevance
ditions: the relevant states must share the same local be-
havior in the abstract state space [Dean and Givan, 1997; 2.1 Deﬁning irrelevance
Ravindran and Barto, 2003]. However, this prior research First we recapitulate the standard MDP notation. An MDP
only applies in a planning context, in which the MDP model hS, A, P, Ri comprises a ﬁnite set of states S, a ﬁnite setof actions A, a transition function P : S × A × S →   2.2  Testing irrelevance
[0, 1], and a reward function R : S × A → R.  Exe-    If we have access to the transition and reward functions,
cuting an action a in a state s yields an expected immedi- we can evaluate the policy irrelevance of a candidate set of
ate reward R(s, a) and causes a transition to state s0 with state variables Y by solving the MDP using a method, such
probability P (s, a, s0). A policy π : S → A speciﬁes as policy iteration, that can yield the set of optimal actions
an action π(s) for every state s and induces a value func- π∗(s) ⊆ A at each state s. Then Y is policy irrelevant at s if
      π
tion V  : S  →   R thatP satisﬁes the Bellman equations some action is in each of these sets for each assignment to Y:
 π                                   0  π  0          T        ∗  0
V  (s) = R(s, π(s)) + γ s0∈S P (s, π(s), s )V (s ), where 0   π (s ) 6= ∅.
                                                        s |=[s]X
γ ∈  [0, 1] is a discount factor for future reward that may However, testing policy irrele-
be necessary to make the equations satisﬁable. For every vance in an RL context is trick-  2.93  3.00
                              ∗                                                      Y=1
MDP at least one optimal policy π exists that maximizes ier if the domain has more than    1.48  2.60
the value function at every state simultaneously. We denote one optimal policy, which is of-
the unique optimal value function V ∗. Many learning algo-                                 2.81    0
                                                      ten the case for domains that con- Y=0
rithms converge to optimal policies by estimating the opti- tain structure or symmetry. Most 1.92 0.99
                             ∗
mal state-action value function Q : S × A → R, with   current RL algorithms focus on
 ∗                  P               0  ∗  0                                               X=0    X=1
Q (s, a) = R(s, a) + γ s0∈S P (s, π(s), s )V (s ).    ﬁnding a single optimal action
  Without loss of generality, assume that the state space is the at each state, not all the optimal Figure 2: The domain of
cartesian product of (the domains of) n state variables X =                         Figure 1 with some learned
                                                      actions. For example, Figure 2 Q values.
{X1, . . . , Xn} and m state variables Y = {Y1, . . . , Ym}, so shows the Q values learned from
                                                                        1
S = X1 × · · · × Xn × Y1 × · · · × Ym. We write [s]X to de- a run of Q-learning, a standard algorithm that employs
                               0                                                    ∗
note the projection of s onto X and s |= [s]X to denote that stochastic approximation to learn Q [Watkins, 1989]. Even
s0 agrees with s on every state variable in X . Our goal is to though the state variable Y is actually policy irrelevant, from
determine when we can safely abstract away Y. In this work this data we would conclude that an agent must know the
we introduce a novel approach to state abstraction called pol- value of Y to behave optimally when X = 1. In this trial
icy irrelevance. Intuitively, if an agent can behave optimally we allowed the learning algorithm enough exploration to ﬁnd
while ignoring a state variable, then we should abstract that an optimal policy but not enough to converge to accurate Q
state variable away. More formally, we say that Y is policy ir- values for every state-action pair. We argue that this phe-
relevant at s if some optimal policy speciﬁes the same action nomenon is quite common in practical applications, but even
         0         0
for every s such that s |= [s]X :                     with sufﬁcient exploration the inherent stochasticity of the
                                                      domain may disguise state variable irrelevance. We the pro-
                       ∗  0       ∗  0 0
             0      0
         ∃a∀s |=[s]X ∀a Q (s , a) ≥ Q (s , a ). (1)   pose two methods for detecting policy irrelevance in a manner
                                                      robust to this variability.
If Y is policy irrelevant for every s, then Y is policy irrelevant
for the entire domain.                                Statistical hypothesis testing
  Consider the illustrative toy                       Hypothesis testing is a method for drawing inferences about
                                          0      3    the true distributions underlying sample data. In this section,
domain shown in Figure 1. It has Y=1
just four nonterminal states de-          0      3    we describe how to apply this method to the problem of in-
scribed by two state variables, X                     ferring policy irrelevance. To this end, we interpret an RL al-
                                          1      0
and Y . It has two deterministic Y=0                  gorithm’s learned value Q(s, a) as a random variable, whose
actions, represented by the solid         1      1    distribution depends on both the learning algorithm and the
and dashed arrows respectively.     X=0   X=1         domain. Ideally, we could then directly test that hypothe-
When X  =  1, both actions ter-                       sis (1) holds, but we lack an appropriate test statistic. Instead,
                              Figure 1: A domain with we assume that for a reasonable RL algorithm, the means of
minate the episode but determine four nonterminal states and
                                                      these distributions share the same relationships as the corre-
the ﬁnal reward, as indicated in two actions. When X = 1                                  0      ∗
                              both actions transition to an sponding true Q values: Q(s, a) ≥ Q(s, a ) ≡ Q (s, a) ≥
the ﬁgure. This domain has two                          ∗    0
optimal policies, one of which absorbing state, not shown. Q (s, a ). We then test propositions of the form
we can express without Y : take the solid arrow when X = 0             Q(s, a) ≥ Q(s, a0),            (2)
and the dashed arrow when X = 1. We thus say that Y is using a standard procedure such as a one-sided paired t-test
policy irrelevant across the entire domain.           or Wilcoxon signed ranks test [Degroot, 1986]. These tests
  Note however that we cannot simply aggregate the four output for each hypothesis (2) a signiﬁcance level ps,a,a0 . If
states into two states. As McCallum pointed out, the state Q(s, a) = Q(s, a0) then this value is a uniformly random
distinctions sufﬁcient to represent the optimal policy are not number from the interval (0, 1). Otherwise, ps,a,a0 will tend
necessarily sufﬁcient to learn the optimal policy [McCallum, towards 1 if hypothesis (2) is true and towards 0 if it is false.
1995]. In this example, observe that if we treat X = 1 as a We combine these values in a straightforward way to obtain a
single abstract state, then in X = 0 we will learn to take the conﬁdence measure for hypothesis (1):
dashed arrow, since it transitions to the same abstract state
                                                                   p = max  min  min  ps0,a,a0 .      (3)
                                                                            0     0
as the solid arrow but earns a greater immediate reward. We             a  s |=[s]X a 6=a

demonstrate how to circumvent this problem while still ben- 1
eﬁtting from the abstraction in Section 3.2.             No discounting, learning rate 0.25, Boltzmann exploration with
                                                      starting temperature 50, cooling rate 0.95, for 50 episodes  Figure 3 shows these p values for our toy domain. To ob- value of p = 0 for cases in which Y actually is relevant; we
tain the data necessary to run the test, we ran 25 independent obtain a value near 1 when only one action is optimal; we
trials of Q-learning. We used the Wilcoxon signed-ranks test, obtain a uniformly random number in (0, 1) when more than
which unlike the t-test does not assume that Q(s, a) is Gaus- one action is optimal. Although it achieves similar results
sian. In Figure 3a we see “random” looking values, so we using less data, this method incurs a higher computational
accept that Y is policy irrelevant for both values of X. In cost due to the need to solve multiple MDPs.4
Figure 3b we see values very close to 0, so we must reject our
hypothesis that X is policy irrelevant for either value of Y . 3 Abstraction discovery
In our work, we use 0.05 as a threshold for rejecting hypoth-
esis (1). If p exceeds 0.05 for every s, then Y is irrelevant 3.1 Discovering irrelevance
across the entire domain. In practice this number seems quite The techniques described in Section 2.2 both involve two
conservative, since in those cases when the hypothesis is false stages of computation. In the ﬁrst stage, they acquire sam-
we consistently see p values orders of magnitude smaller. ples of state-action values, either by solving the task repeat-
                                                      edly or by solving sampled MDPs repeatedly. In the second
         Y=1              Y=1     0.001               stage, they use this data to test the relevance of arbitrary sets
              0.367 0.731                             of state variables at arbitrary states. Any one of these tests in
                                                      the second stage is very cheap relative to the cost of the ﬁrst
         Y=0              Y=0     0.000
                                                      stage, but the number of possible tests is astronomical. We
              X=0  X=1         X=0  X=1               must limit both the sets of state variables that we test and the
               (a)              (b)                   states at which we test them.
                                                        First consider the sets of state variables. It is straightfor-
Figure 3: The value of p for each of the two abstract states when ward to prove that if Y is policy irrelevant at s, then every
testing the policy irrelevance of (a) Y and (b) X.    subset of Y is also policy irrelevant at s.5 A corollary is that
                                                      we only need to test the policy irrelevance of {Y1, . . . , Yk}
Monte Carlo simulation                                at s if both {Y1, . . . , Yk−1} and {Yk} are policy irrelevant
The hypothesis testing approach is computationally efﬁcient, at s. This observation suggests an inductive procedure that
but it requires a large amount of data. We explored an alter- ﬁrst tests each individual state variable for policy irrelevance
native approach designed to conserve experience data when and then tests increasingly larger sets only as necessary. This
interaction with the domain is expensive. We draw upon inductive process will continue only so long as we ﬁnd in-
work in Bayesian MDP models [Dearden et al., 1999] to creasingly powerful abstractions.
reason more directly about the distribution of each Q(s, a). We can afford to test each state variable at a given state,
This technique regards the successor state for a given state- since the number of variables is relatively small. In contrast,
action pair as a random variable with an unknown multino- the total number of states is quite large: exponential in the
mial distribution. For each multinomial distribution, we per- number of variables. We hence adopt an heuristic approach,
form Bayesian estimation, which maintains a probability dis- which tests for policy irrelevance only at those states visited
tribution over multinomial parameters. After conditioning on on some small number of trajectories through the task. For
state transition data from a run of an arbitrary RL algorithm, these states, we then determine what sets of state variables
the joint distribution over the parameters of these multinomi- are policy irrelevant, as described above. For each set of state
als gives us a distribution over transition functions. The vari- variables we can then construct a binary classiﬁcation prob-
ance of this distribution goes to 0 and its mean converges on lem with a training set comprising the visited states. An ap-
the true transition function as the amount of data increases.2 propriate classiﬁcation algorithm then allows us to generalize
  Once we have a Bayesian model of the domain, we can ap- the region over which each set of state variables is policy ir-
ply Monte Carlo simulation to make probabilistic statements relevant. Note that in Section 3.2 we take steps to ensure that
about the Q values. We sample MDPs from the model and the classiﬁers’ generalization errors do not lead to the appli-
solve them3 to obtain a sample for each Q value. Then we cation of unsafe abstractions.
can estimate the probability that Q∗(s, a) ≥ Q∗(s, a0) holds
as the fraction of the sample for which it holds. We use this 3.2 Exploiting irrelevance
probability estimate in the same way that we used the signif- Section 3.1 describes how to represent as a learned classi-
icance levels in the hypothesis testing approach to obtain a ﬁer the region of the state space where a given set of state
conﬁdence measure for the policy irrelevance of Y at some s: variables is policy irrelevant. A straightforward approach to
   p = max  min  min  Pr(Q∗(s0, a) ≥ Q∗(s0, a0)). (4) state abstraction would simply aggregate together all those
            0     0
        a  s |=[s]X a 6=a
  This method seems to yield qualitatively similar results to 4We ameliorate this cost somewhat by initializing each MDP’s
the hypothesis testing method. We almost always obtain a value function with the value function for the maximum likelihood
                                                      MDP, as in [Strens, 2000].
  2It is also possible to build a Bayesian model of the reward func- 5The converse is not necessarily true. Suppose we duplicate an
tion, but all the domains that we have studied use deterministic re- otherwise always relevant state variable. Then each copy of the state
wards.                                                variable is always policy irrelevant given the remainder of the state
  3We use standard value iteration.                   representation, but the pair of them is not.states in this region that differ only on the irrelevant vari- state abstractions join the set of optimal actions at each ap-
ables. However, this approach may prevent an RL algorithm propriate state. The smaller state representation should allow
from learning the correct value function and therefore the op- the option policies to converge quickly, so RL algorithms will
timal policy. In Section 2.1 we gave a simple example of such learn to exploit these optimal policy fragments instead of un-
an abstraction failure, even with perfect knowledge of policy covering the whole optimal policy the hard way. We illustrate
irrelevance. Generalizing the learned classiﬁer from visited this process in the next section.
states in one domain to unvisited states in a similar domain
introduces another source of error. A solution to all of these
problems is to encapsulate each learned state abstraction in- 4 Results
side a temporal abstraction. In particular, we apply each state We use Dietterich’s Taxi do-
space aggregation only inside an option [Sutton et al., 1999], main [Dietterich, 2000], illus-
which is an abstract action that may persist for multiple time trated in Figure 4, as the setting
steps in the original MDP.                            for our work. This domain has
  Formally, for a set of state variables Y that is policy irrele- four state variables. The ﬁrst
vant over some S0 ⊆ S, we construct an option o = hπ, I, βi, two correspond to the taxi’s cur-
                             0
comprising an option policy π : [S ]X → A, an initiation set rent position in the grid world.
I ⊂ S, and a termination condition β : S → [0, 1]. Once The third indicates the passen-
an agent executes an option o from a state in I, it always ex- ger’s current location, at one of
ecutes primitive action π(s) at each state s, until terminating
                                                      the four labeled positions (Red, Figure 4: The Taxi domain.
with probability β(s). We set I = S0 and β(s) = 0.01 for Green, Blue, and Yellow) or in-
s ∈ I and β(s) = 1 otherwise.6 Since Y is policy irrelevant side the taxi. The fourth indicates the labeled position where
over S0, we may choose an option policy π equal to the pro- the passenger would like to go. The domain therefore has
            0
jection onto [S ]X of an optimal policy for the original MDP. 5 × 5 × 5 × 4 = 500 possible states. At each time step, the
An agent augmented with such options can behave optimally taxi may move north, move south, move east, move west, at-
in the original MDP by executing one of these options when- tempt to pick up the passenger, or attempt to put down the
ever possible.                                        passenger. Actions that would move the taxi through a wall
  Although we believe that the discovery of this structure is or off the grid have no effect. Every action has a reward of -1,
interesting in its own right, its utility becomes most appar- except illegal attempts to pick up or put down the passenger,
ent when we consider transferring the discovered options to which have reward -10. The agent receives a reward of +20
novel domains, for which we do not yet have access to an for achieving a goal state, in which the passenger is at the des-
optimal policy. To transfer an option to a new domain, we tination (and not inside the taxi). In this paper, we consider
simply copy the initiation set and termination condition. This the stochastic version of the domain. Whenever the taxi at-
straightforward approach sufﬁces for domains that share pre- tempts to move, the resulting motion occurs in a random per-
cisely the same state space as the original domain. Even when pendicular direction with probability 0.2. Furthermore, once
the state space changes, our representation of I and β as a the taxi picks up the passenger and begins to move, the desti-
learned classiﬁer gives us hope for reasonable generalization. nation changes with probability 0.3.
We can also copy the option policy π, if we expect the opti- This domain’s representation requires all four of its state
mal behavior from the original domain to remain optimal in variables in general, but it still affords opportunity for ab-
the new domain.                                       straction. In particular, note that the passenger’s destination
  In this paper we assume only that the policy irrelevance is only relevant once the agent has picked up the passenger.
remains the same. We thus relearn the option policy con- We applied the methodology described in Sections 2 and 3
currently with the learning of the high-level policy, which to the task of discovering this abstraction, as follows. First,
chooses among the original primitive actions and the discov- we ran 25 independent trials of Q-learning to obtain samples
ered options. For each option, we establish an RL subproblem of Q∗. For each trial, we set the learning rate α = 0.25 and
with state space [I]X and the same action space A. Whenever used ²-greedy exploration with ² = 0.1. Learning to conver-
an option terminates in a state s, we augment the reward from gence required about 75000 time steps for each trial. This
the environment with a pseudoreward equal to the current es- data allows us to compute the policy irrelevance of any state
timate of the optimal high-level value function evaluated at variable at any state. For example, consider again the pas-
s. We therefore think of the option not as learning to achieve senger’s destination. To demonstrate the typical behavior of
a subgoal but learning to behave while ignoring certain state the testing procedure, we show in Figure 5a the output for ev-
variables. In other words, the option adopts the goals of the ery location in the domain, when the passenger is waiting at
high-level agent, but learns in a reduced state space. the upper left corner (the Red landmark), using the Wilcoxon
  Since each option is just another action for the high-level signed-ranks test. The nonzero p values at every state im-
agent to select, RL algorithms will learn to disregard options ply that the passenger’s destination is policy irrelevant in this
as suboptimal in those states where the corresponding ab- case. Note that the values are extremely close to 1 when-
stractions are unsafe. The options that correspond to safe ever the agent has only one optimal action to get to the upper
                                                      left corner, which the procedure can then identify conﬁdently.
  6The nonzero termination probability for s ∈ I serves as a prob- The squares with intermediate values are precisely the states
abilistic timeout to escape from bad abstractions.    in which more than one optimal action exists. Now considerFigure 5b, which shows the output of the same test when the relevant only when the passenger is in the taxi. The other
passenger is inside the taxi. The p values are extremely close three rules classify state variables as usually relevant, except
to 0 in every state except for the four at the bottom middle, in narrow cases. For example, rule 1a holds because the Red
where due to the layout of the domain the agent can always destination is in the upper half of the map, y = 1 speciﬁes
behave optimally by moving north.                     that the taxi is in the lower half, and all the obstacles in this
                                                      particular map are vertical. Rule 2a is an example of an over-
                                                      generalization. When holding the passenger on the rightmost
    0.9999 0.9999 0.9999 0.9919 0.3784 0.0000 0.0000 0.0001 0.0002 0.0000 column, it is usually optimal just to go left, unless the passen-

    0.9999 0.3188 0.9999 0.4731 0.5799 0.0000 0.0000 0.0004 0.0003 0.0000 ger wants to go the Green landmark in the upper-right corner.
                                                        We tested the generalization performance of these learned
    0.9999 0.1766 0.9999 0.9999 0.9999 0.0000 0.0000 0.0000 0.0000 0.0000 abstractions on 10 × 10 instances of the Taxi domain with
                                                      randomly generated obstacles, running both horizontally and
    0.9999 0.9999 0.5799 0.9999 0.4095 0.0000 0.4412 0.4946 0.0000 0.0003 vertically. We placed one landmark near each corner and oth-
                                                      erwise gave these domains the same dynamics as the origi-
    0.9999 0.9994 0.7940 0.9999 0.7703 0.0000 0.9171 0.6518 0.0000 0.0000
                                                      nal. Each abstraction was implemented as an option, as dis-
             (a)                   (b)                cussed in Section 3.2. Since the locations of the landmarks
                                                      moved, we could not have simply transferred option policies
Figure 5: The results of the Wilcoxon signed-ranks test for deter- from the original Taxi domain. In all our experiments, we
mining the policy irrelevance of the passenger’s destination in the used Q-learning with ²-greedy exploration7 to learn both the
Taxi domain. We show the result of the test for each possible taxi option policies and the high-level policy that chose when to
location for (a) a case when the passenger is not yet in the taxi and apply each option and thus each state abstraction.8 To im-
(b) the case when the passenger is inside the taxi.   prove learning efﬁciency, we added off-policy training [Sut-
                                                      ton et al., 1999] as follows. Whenever a primitive action a
  Rather than compute the outcome of the test for every sub- was executed from a state s, we updated Q(s, a) for the high-
set of state variables at every state, we followed the approach level agent as well as for every option that includes s in its
described in Section 3.1 and sampled 20 trajectories from the initiation set. Whenever an option o terminated, we updated
domain using one of the learned policies. We tested each indi- Q(s, o) for every state s visited during the execution of o.
vidual state variable at each state visited, again using the hy- Each state-action estimate in the system therefore received
pothesis testing approach. We created a binary classiﬁcation exactly one update for each timestep the action executed in
problem for each variable, using the visited states as the train- the state. Figure 6 compares the learning performance of this
ing set. For the positive examples, we took each state at which system to a Q-learner without abstraction. The abstractions
the hypothesis test returns a p value above the conservative allowed the experimental Q-learner to converge much faster
threshold of 0.05. Finally, we applied a simple rule-learning to an optimal policy, despite estimating a strict superset of the
classiﬁer to each problem: the Incremental Reduced Error parameters of the baseline Q-learner.
Pruning (IREP) algorithm, as described in [Cohen, 1995]. A
typical set of induced rules follows:                 5   Related work
 1. Taxi’s x-coordinate:                              Our approach to state abstraction discovery bears a strong re-
     (a) y = 1 ∧ passenger in taxi ∧ destination Red  semblance to aspects of McCallum’s U-tree algorithm [Mc-
        ⇒  policy irrelevant                          Callum, 1995], which uses statistical hypothesis testing to
     (b) otherwise, policy relevant                   determine what features to include in its state representation.
                                                      U-tree is an online instance-based algorithm that adds a state
 2. Taxi’s y-coordinate:
                                                      variable to its representation if different values of the variable
     (a) x = 4 ∧ passenger in taxi ⇒ policy irrelevant predict different distributions of expected future reward. The
     (b) otherwise, policy relevant                   algorithm computes these distributions of values in part from
                                                      the current representation, resulting in a circularity that pre-
 3. Passenger’s destination:
                                                      vents it from guaranteeing convergence on an optimal state
     (a) passenger in taxi ⇒ policy relevant          abstraction. In contrast, we seek explicitly to preserve opti-
     (b) otherwise, policy irrelevant                 mality.
 4. Passenger’s location and destination                Our encapsulation of partial state abstractions into options
                                                      is inspired by Ravindran and Barto’s work on MDP homo-
     (a) (x = 1 ∧ y = 2) ∨ (x = 1 ∧ y = 1)            morphisms [Ravindran and Barto, 2003] and in particular
        ⇒  policy irrelevant                          their discussion of partial homomorphisms and relativized
     (b) otherwise, policy relevant                   options. However, their work focuses on developing a more

  The sets of state variables not mentioned either had no pos- 7² = 0.1 and α = 0.25
itive training examples or induced an empty rule set, which 8In general, SMDP Q-learning is necessary to learn the high-
classiﬁes the state variables as relevant at every state. Rule set level policy, since the actions may last for more than one time step.
3 captures the abstraction that motivated our analysis of this However, this algorithm reduces to standard Q-learning in the ab-
domain, specifying that the passenger’s destination is policy sence of discounting, which the Taxi domain does not require.