          Effective Control Knowledge Transfer Through Learning Skill and
                                  Representation Hierarchies          ∗
                      Mehran Asadi                                Manfred Huber
             Department of Computer Science       Department of Computer Science and Engineering
         West Chester University of Pennsylvania        The University of Texas at Arlington
                  West Chester, PA 19383                        Arlington, TX 76019
                    masadi@wcupa.edu                             huber@cse.uta.edu

                    Abstract                          extended actions using the framework of Semi-Markov De-
                                                      cision Processes (SMDPs) [Sutton et al., 1999], for learning
    Learning capabilities of computer systems still lag subgoals and hierarchical action spaces, and for learning ab-
    far behind biological systems. One of the rea-    stract representations [Givan et al., 1997].However,mostof
    sons can be seen in the inefﬁcient re-use of control these techniques only address one of the aspects of transfer
    knowledge acquired over the lifetime of the arti- and do frequently not directly address the construction of ac-
    ﬁcial learning system. To address this deﬁciency, tion and representation hierarchies in life-long learning.
    this paper presents a learning architecture which   The work presented here focuses on the construction and
    transfers control knowledge in the form of behav- transfer of control knowledge in the form of behavioral skill
    ioral skills and corresponding representation con- hierarchies and associated representational hierarchies in the
    cepts from one task to subsequent learning tasks. context of a reinforcement learning agent. In particular,
    The presented system uses this knowledge to con-  it facilitates the acquisition of increasingly complex behav-
    struct a more compact state space representation  ioral skills and the construction of appropriate, increasingly
    for learning while assuring bounded optimality of abstract and compact state representations which acceler-
    the learned task policy by utilizing a represen-  ate learning performance while ensuring bounded optimality.
    tation hierarchy. Experimental results show that  Moreover, it forms a state hierarchy that encodes the func-
    the presented method can signiﬁcantly outperform  tional properties of the skill hierarchy, providing a compact
    learning on a ﬂat state space representation and  basis for learning that ensures bounded optimality.
    the MAXQ method for hierarchical reinforcement
    learning.                                         2   Reinforcement Learning
                                                      In the RL framework, a learning agent interacts with an en-
1  Introduction                                       vironment over a series of time steps t =0, 1, 2, 3, ... .At
                                                      each time t, the agent observes the state of the environment,
Learning capabilities in biological systems far exceed the
                                                      st , and chooses an action, at , which causes the environment
ones of artiﬁcial agents, partially because of the efﬁciency
                                                      to transition to state st+1 andtoemitareward,rt+1.Ina
with which they can transfer and re-use control knowledge Markovian system, the next state and reward depend only on
acquired over the course of their lives.              the preceding state and action, but they may depend on these
  To address this, knowledge transfer across learning tasks in a stochastic manner. The objective of the agent is to learn
has recently received increasing attention [Ando and Zhang, to maximize the expected value of reward received over time.
2004; Taylor and Stone, 2005; Marthi et al., 2005; Marx et It does this by learning a (possibly stochastic) mapping from
al., 2005]. The type of knowledge considered for transfer states to actions called a policy. More precisely, the objective
includes re-usable behavioral macros, important state fea-
                                                      is to choose each action at so as to maximize the expected
tures, information about expected reward conditions, and         ∞    i
                                                      return, E{ i=0 γ rt+i} ,whereγ  ∈ [0, 1] is a discount-
background knowledge. Knowledge transfer is aimed at im- rate parameter. Other return formulations are also possible.
proving learning performance by either reducing the learning A common solution strategy is to approximate the optimal
problem’s complexity or by guiding the learning process. action-value function, or Q-function, which maps each state
  Recent work in Hierarchical Reinforcement Learning  and action to the maximum expected return starting from the
(HRL) has led to approaches for learning with temporally given state and action and thereafter always taking the best
  ∗                                                   actions.
   This work was supported in part by DARPA FA8750-05-2-0283
and NSF ITR-0121297. The U. S. Government may reproduce and To permit the construction of a hierarchical learning system,
distribute reprints for Governmental purposes notwithstanding any we model our learning problem as a Semi-Markov Decision
copyrights. The authors’ views and conclusions should not be inter- Problem (SMDP) and use the options framework [Sutton et
preted as representing ofﬁcial policies or endorsements, expressed al., 1999] to deﬁne subgoals. An option is a temporally ex-
or implied, of DARPA, NSF, or the Government.         tended action which, when selected by the agent, executes

                                                IJCAI-07
                                                  2054                                                                                                 Reward
until a termination condition is satisﬁed. While an option is
                                                                                    QValue    Learning/Planning
                                                                                    Inconsistencies
executing, actions are chosen according to the option’s own             ActionEligibility
policy. An option is like a traditional macro except that in-
                                                                                             State   Q(S,A)
                                                           BehaviorMemory    DecisionLevel
stead of generating a ﬁxed sequence of actions, it follows a                 Actions
                                                                                            Hierarchical
closed-loop policy so that it can react to the environment. ConceptMemory                   BPSMDPModel
                                                                        StateSpace
                                                                        Construction
By augmenting the agent’s set of primitive actions with a                         Model DecisionLevel
set of options, the agent’s performance can be enhanced.                ModelRefinement Construction
More speciﬁcally, an option is a triple oi =(Ii,πi,βi),                                 EvaluationLevel
                                                            NewStateConcepts
where Ii is the option’s input set, i.e., the set of states in         Subgoal/Subtask
                                                                       Identification
which the option can be initiated; πi is the option’s pol-             PolicyGeneralization
                                                              NewActions
icy deﬁned over all states in which the option can execute;                               State   Decision
and βi is the termination condition, i.e., the option termi-
nates with probability βi(s) for each state s. Each option Figure 1: System Overview of the Approach for Hierarchical
that we use in this paper bases its policy on its own inter- Behavior and State Concept Transfer.
nal value function, which can be modiﬁed over time in re-
sponse to the environment. The value of a state s under
an SMDP policy πo is deﬁned as [Boutilier et al., 1999; 3.1 Learning Transferable Skills
Sutton et al., 1999]:                               To learn skills for transfer, the approach presented here tries
                                                     to identify subgoals. Subgoals of interest here are states that
       π                               π  
     V  (s)=E   R(s, oi)+    F (s |s, oi)V (s )       have properties that could be useful for subsequent learning
                          s                          tasks. Because the new tasks’ requirements, and thus their
  where                                               reward functions, are generally unknown, the subgoal crite-
                   ∞                                 rion used here does not focus on reward but rather on local
                                        k
       F (s |s, oi)=   P (st = s |st = s, oi)γ  (1)   properties of the state space in the context of the current task
                   k=1                                domain. In particular, the criterion used attempts to iden-
  and                                                 tify states which locally form a signiﬁcantly stronger “attrac-
                           2            o            tor” for state space trajectories as measured by the relative
 R(s ,oi)=E[rt+1 + γrt+2 + γ rt+3 + ...|(πi ,s,t)] (2)
                                      o               increase in visitation likelihood.
where rt denotes the reward at time t and (π ,s,t) denotes
                               o                        To ﬁnd such states, the subgoal discovery method ﬁrst gen-
the event of an action under policy π being initiated at time erates N random sample trajectories from the learned policy
t and in state s [Sutton et al., 1999].               and for each state, s, on these trajectories and determines the
                                                                                  ∗            ∗
                                                      expected visitation likelihood, CH (s),where,CH (s) is the
3  Hierarchical Knowledge Transfer                    sum over all states in sample trajectories hi ∈ H weighed by
In the approach presented here, skills are learned within their accumulated likelihood to pass through s. The change
the framework of Semi-Markov Decision Processes (SMDP) of visitation likelihoods along a sample trajectory, hi,isthen
                                                                              ∗        ∗
where new task policies can take advantage of previously determined as ΔH (st)=CH (st) − CH (st−1),wherest is
learned skills, leading from an initial set of basic actions to the tth state along the path. The ratio of this change along the
the formation of a skill hierarchy. At the same time, abstract path is then computed as
representation concepts are derived which capture each skill’s
goal objective as well as the conditions under which use of                 ΔH (st)
the skill would predict achievement of the objective. Figure 1         max(1, ΔH (st+1))
shows the approach.                                   for every state in which ΔH (st) > 0. Finally, a state st is
  The state representations are formed here within the frame- considered a potential subgoal if its average change ratio is
work of Bounded Parameter Markov Decision Processes (BP- signiﬁcantly greater than expected from the distribution of
MDPs)  [Givan et al., 1997] and include a decision-level the ratios for all states 1. For all subgoals found, correspond-
model and a more complex evaluation-level model. Learning ing policies are learned off-line as SMDP options, oi,and
of the new task is then performed on the decision-level model added to the skill hierarchy.
using Q-learning, while a second value function is maintained
on the evaluation-level model. When an inconsistency is dis- Deﬁnition 1 A state s is a direct predecessor of state
covered between the two value functions, a reﬁnement aug- s, if under a learned policy the action in state s can lead to
ments the decision-level model by including the concepts of s i.e., P (s|s,a) > 0.
the action that led to the inconsistency.
  Once a policy for the new task is learned, subgoals are Deﬁnition 2 The count metric for state s under a learned
extracted from the system model and corresponding subgoal policy, π, is the sum over all possible state space trajectories
skills are learned off-line. Then goal and probabilistic af- weighed by their accumulated likelihood to pass through
fordance concepts are learned for the new subgoal skills and state s.
both, the new skills and concepts are included into the skill
and representation hierarchies in the agent’s memory, mak- 1The threshold is computed automatically using a t-test based
ing them available for subsequent learning tasks.     criterion and a signiﬁcance threshold of 2.5%.

                                                IJCAI-07
                                                  2055     ∗
Let Cπ(s) be the count for state s, then:             3.2  Learning Functional Descriptions of State
                      
               1                                    The power of complex actions to improve learning per-
             Cπ(s)=      P (s|s ,π(s ))         (3)   formance has two main sources; (i) their use reduces the
                     s=s                            number of decision points necessary to learn a policy, and
                                                      (ii) they usually permit learning to occur on a more com-
and                                                  pact state representation. To harness the latter, it is nec-
           t                     t−1 
          Cπ(s)=     P (s|s ,π(s ))Cπ (s )      (4)   essary to automatically derive abstract state representations
                  s=s                               that capture the functional characteristics of the actions. To
                                                      do so, the presented approach builds a hierarchical state
                         n                           representation within the basic framework of BPMDPs ex-
                  ∗           i
                 Cπ(s)=     Cπ(s)               (5)   tended to SMDPs, forming a hierarchical Bounded Parame-
                         i=1                          ter SDMP (BPSMDP). Model construction occurs in a multi-
                    n        n+1                      stage, action-dependent fashion, allowing the model to adapt
where n is such that Cπ (s)=Cπ  (s) or n = |S|.The
          =                                         rapidly to action set changes.
condition s   s prevents the counting of self loops and The BPSMDP state space is a partition of the original state
P (s|s,π(s)) is the probability of reaching state s from state
                                     ∗              space where the following inequalities hold for all blocks
  by executing action ( ). The slope of ( t) along a
s                  π  s              Cπ  s            (BPSMDP states) Bi and actions oj [Asadi and Huber, 2005]:
path, ρ, under policy π is:                                                                  
                                                                                             
                       ∗       ∗                                                           
            Δπ(st)=Cπ(st)   − Cπ(st−1)          (6)                                     
                                                                  F (s |s, oi) −   F (s |s ,oi) ≤ δ (8)
              th                                             s∈B            s∈B          
where st is the t state along the path. In order to identify     j                 j
                           Δ  ( )  max(1 Δ  (    ))                               
subgoals, the gradient ratio π st /     ,  π st+1                    |R(s, oi) − R(s ,oi)|≤          (9)
                          Δ  ( )    0
is computed for states where π st >  .Astatest   is          (   )
considered a potential subgoal candidate if the gradient ratio where R s, o is the expected reward for executing option o in
                                                      state s,andF (s|s, o) is the discounted transition probability
is greater than a speciﬁed threshold μ>1. Appropriate                                             
values for this user-deﬁned threshold depend largely on the for option o initiated in state s to terminate in state s .These
characteristics of the state space and result in a number of properties of the BPSMDP model ensure that the value of
subgoal candidates that is inversely related to the value of the policy learned on this model is within a ﬁxed bound the
μ. This approach is an extension of the criterion in [Goel optimal policy value on the initial model, where the bound is
                                                      a function of  and δ [Givan et al., 1997].
and Huber, 2003] with max(1, Δπ(st+1)) addressing the
effects of potentially obtaining negative gradients due to To make the construction of the BPSMDP more efﬁcient,
nondeterministic transitions.                         the state model is constructed in multiple steps. First func-
In order to reduce the computational complexity of the tional concepts for each option, o, are learned as termina-
above method in large state spaces, the gradient ratio is here tion concepts Ct,o, indicating the option’s goal condition, and
computed using Monte Carlo sampling.                  probabilistic prediction concepts (“affordances”), Cp,o,x,in-
                                                      dicating the context under which the option will terminate
                                                      successfully with probability x ± . These conditions guar-
Deﬁnition 3 Let H  =   {h1, ..., hN } be N sample tra-
jectories induced by policy π, then the sampled count metric, antee that any state space utilizing these concepts in its state
C∗ (s), for each state s that is on the path of at least one factorization fulﬁlls the conditions of Equation 8 for any sin-
 H                                                    gle action.
path hi can be calculated as the average of the accumulated
                         1 ≤    ≤                       To construct an appropriate BPMDP for a speciﬁc action
likelihoods of each path, hi, i   N, rescaled by the        = {  }
total number of possible paths in the environment.    set Ot   oi , an initial model is constructed by concatenat-
                                                      ing all concepts associated with the options in Ot. Additional
We can show that for trajectories hi and sample size N such
that                                                  conditions are then derived to achieve the condition of Equa-
                   ∗                                  tion 8 and, once reward information is available, the reward
             maxt C  (st)             2
          ≥        H    2(1 +   )   (     )           condition of Equation 9. This construction facilitates efﬁcient
        N         2           N log 1 −        (7)
                 N                     p             adaptation to changing action repertoires.
the following statement is true with probability p:     To further utilize the power of abstract actions, a hierarchy
                                                      of BPSMDP models is constructed here where the decision-
                 ∗        ∗
              |CH (st) − Cπ(st)|≤N                   level model utilizes the set of options considered necessary
                                                      while the evaluation-level uses all actions not considered re-
Theorem  1 Let H =  {h1, ..., hN } be N sample trajecto- dundant. In the current system, a simple heuristic is used
ries induced by policy π with N selected according to Equa- where the decision-level set consists only of the learned sub-
              Δ (s )               2 (μ+1)
tion 7. If     H  t     >μ+         N        ,then    goal options while the evaluation-level set includes all ac-
          max(1,ΔH (st+1))      max(1,ΔH (st+1))      tions.
    Δπ (st)                    ≥
max(1,Δπ (st+1)) >μwith probability p.                  Let P  =  {B1,...,Bn}  be a partition for state space
Theorem 1 implies that for a sufﬁciently large sample size the S derived by the action-dependent partitioning method,
exhaustive and the sampling method predict the same sub- using subgoals {s1,...,sk} and options to these subgoals
goals with high probability.                          {o1,...,ok}.  If the goal state G belongs to the set of

                                                IJCAI-07
                                                  2056subgoals {s1,...,sk},thenG  is achievable by options  bounds if the policy only utilizes the actions in the decision-
{o1,...,ok} and the task is learnable according to Theorem level action set. Since, however, the action set has to be se-
2.                                                    lected without knowledge of the new task, it is generally not
                                                      possible to guarantee that it contains all required actions.
Theorem  2 For any policy π for which the goal G can    To address this, the approach maintains a second value
be represented as a conjunction of terminal sets (subgoals) function on top of the evaluation-level system model. While
of the available actions in the original MDP M,thereis decisions are made strictly based on the decision-level states,
a policy πP in the reduced MDP, MP , that achieves G  the evaluation-level value function is used to discover value
as long as for each state st in M for which there exists a inconsistencies, indicating that a signiﬁcant aspect of the state
path to G , there exists a path such that F (G|st,πP (st)) >δ. space is not represented in the evaluation-level state model.
                                                      The determination of inconsistencies here relies on the fact
                                                                                              ∗
If G/∈{s1,...,sk}   then the task may not be solvable that the optimal value function in a BPMDP, VP , is within a
using only the options that terminate at subgoals. The ﬁxed bound of the optimal value function, V ∗, on the under-
proposed approach solves this problem by maintaining a lying MDP [Givan et al., 1997].
separate value function for the original state space while Inconsistencies are discovered when the evaluation-level
learning a new task on the partition space derived from only value for a state signiﬁcantly exceeds the value of the cor-
the subgoal options. During learning, the agent has access to responding state at the decision level. In this case, the action
the original actions as well as all options, but makes decisions producing the higher value is included for the corresponding
only based on the abstract partition space information. block at the decision level and the block is reﬁned with this
While the agent tries to solve the task on the abstract partition action to fulﬁll Equations 8 and 9 as illustrated in Figure 2.
space, it computes the difference in Q-values between the

best actions in the current state in the abstract state space                         B2
and in the original state space. If the difference is larger           B
than a constant value (given by Theorem 2), then there is a             1
signiﬁcant difference between different states underlying the
particular block that was not captured by the subgoal options.
Theorem 3 [Kim and Dean, 2003] showsthatifblocksare                             B3
stable with respect to all actions the difference between the
Q-values in the partition space and in the original state space
                                                                                          3
is bounded by a constant value.                       Figure 2:  Decision-level model with   initial blocks
                                                      (B1,B2,B3) where block B3 has been further reﬁned.
Theorem  3 Given an MDP   M   =(S, A, T, R)  and a
partition P of the state space MP , the optimal value function
               ∗
of M given as V  and the optimal value function of MP 4   Experiments
given as ∗ satisfy the bound on the distance
       VP                                             To evaluate the approach, it has been implemented on
                                      
                                                      the Urban Combat Testbed (UCT,), a computer game
            ∗ −   ∗  ≤ 2  1+    γ
            V    VP  ∞          1 −  p               (http://gameairesearch.uta.edu). For the experiments pre-
                                   γ                  sented here, the agent is given the abilities to move through
        =min      ∗ −  ∗                            the environment shown in Figure 3 and to retrieve and deposit
where p     Vp  V     VP  ∞ and
                                                     objects.
        ( )=max[   (   )+         ( |  ) ( )]
     LV  s     a  R s, a   γ    P s  s, a V s
                            s∈S

When the difference between the Q-values for states in block
                        γ
Bi are greater than 2(1 + 1−γ p), then the primitive action
that achieves the highest Q-value on the original state in the
MDP will be added to the action space of those states that are
in block Bi and block Bi is reﬁned until it is stable for the
                                                           Figure 3: Urban Combat Testbed (UCT) domain.
new action set. Once no such signiﬁcant difference exists, the
goal will be achievable in the resulting state space according The state is here characterized by the agent’s pose as well
to Theorem 2.
                                                      as by a set of local object precepts, resulting in an effective
                                                      state space with 20, 000 states.
3.3  Learning on a Hierarchical State Space             The agent is ﬁrst presented with a reward function to learn
To learn new tasks, Q-learning is used here at the decision- to move to a speciﬁc location. Once this task is learned, sub-
level of the BPSMDP hierarchy.  Because the compact   goals are extracted by generating random sample trajectories
decision-level state model encodes only the aspects of the en- as shown in Figure 4.
vironment relevant to a subset of the actions, it only ensures As the number of samples increases, the system identiﬁes
the learning of a policy within the pre-determined optimality an increasing number of subgoals until, after 2, 000 samples,

                                                IJCAI-07
                                                  2057         30                                                    90

                                                               80
         25
                                                               70

         20                                                    60

                                                               50
         15

                                                               40
                                                              Q−Value

         10
                              Exhaustive Computing             30
                              Monte Carlo Sampling
                                                               20
         5                                                                         No Transfer
        Discovered  Subgoals / Skills                                              Skill Transfer
                                                               10                  Skill and Representation Transfer

         0
          0    500  1000 1500 2000 2500  3000                  0
                     Number of Samples                          0    500  1000 1500  2000 2500  3000
                                                                            Number of Iterations
 Figure 4: Number of subgoals discovered using sampling.
                                                      Figure 6: Learning performance with and without skill and
                                                      representation/concept transfer.
all 29 subgoals that could be found using exhaustive calcula-
tion have been captured.
                                                      ferred, illustrating the utility of the presented subgoal crite-
  Once subgoals are extracted, subgoal options, oi,are
                                                      rion. Including the representation transfer and hierarchical
learned and termination concepts, Ct,oi and probabilistic out-
                                                      BPSMDP learning approach results in signiﬁcant further im-
come predictors, Cp,oi,x are generated. These subgoal op-                          ≈ 5
tions and the termination and prediction concepts are then provement with a transfer ratio of .
transferred to the next learning tasks.
  The system then builds a hierarchical BPSMDP system 5   Comparison with MAXQ
model where the decision-level only utilizes the learned sub- Dietterich [Dietterich, 2000] developed an approach to hi-
goal actions while the evaluation-level model is built for all erarchical RL called the MAXQ Value Function Decompo-
available actions. On this model, a second task is learned sition, which is also called the MAXQ method. Like options
where the agent is rewarded for retrieving a ﬂag (from a and HAMs, this approach relies on the theory of SMDPs. Un-
different location than the previous goal) and return it to like options and HAMs, however, the MAXQ approach does
the home base. During learning, the system augments its not rely directly on reducing the entire problem to a single
decision-level state representation to allow learning of a pol- SMDP. Instead, a hierarchy of SMDPs is created whose so-
icy that is within a bound of optimal as shown in Figure 6. lution can be learned simultaneously. The MAXQ approach
                                                      starts with a decomposition of a core MDP M into a set of
         85
                                                      subtasks {M0,...,Mn}. The subtasks form a hierarchy with
         80                                           M0  being the root subtask, which means that solving M0
         75                                           solves M. Actions taken in solving M0 consist of either exe-

         70                                           cuting primitive actions or policies that solve other subtasks,

         65                                           which can in turn invoke primitive actions or policies of other
                                                      subtasks, etc.
         60
                                                      Each subtask, Mi, consists of three components. First, it has
         55
                            Task−Independent Blocks   a subtask policy, pi, that can select other subtasks from the
                            Task−Specific Refinement
         50                                           set of Mi ’s children. Here, as with options, primitive actions


        Number  of BPSMDP States 45                   are special cases of subtasks. We also assume the subtask

         40                                           policies are deterministic. Second, each subtask has a ter-
          0    500  1000 1500 2000 2500  3000
                   Number of Iterations               mination predicate that partitions the state set, s,ofthecore
  Figure 5: Size of the decision-level state representation. MDP into si, the set of active states in which Mi’s policy
                                                      can execute, and ti, the set of termination states which, when
  Figure 5 shows that the system starts with an initial state entered, causes the policy to terminate. Third, each subtask
representation containing 43 states. During learning, as value mi has a pseudo-reward function that assigns reward values
function inconsistencies are found, new actions and state to the states in ti. The pseudo-reward function is only used
splits are introduced, eventually increasing the decision-level during learning.
state space to 81 states. On this state space, a bounded op- Figure 7 shows the comparison between the MAXQ decom-
timal policy is learned as indicated in Figure 6. This graph position and the learning of an SMDP with the sampling-base
compares the learning performance of the system against a subgoal discovery but without action-dependent partitioning.
learner that only transfers the discovered subgoal options and This experiment illustrates that MAXQ will outperform an
a learner without any transfer mechanism. These graphs show SMDP with options to the subgoals that are discovered by
a transfer ratio2 of ≈ 2.5 when only subgoal options are trans- sampling-based subgoal discovery. The reason for this is that
                                                      while subgoals are hand designed in the MAXQ decompo-
  2The transfer ratio is the ratio of the area over the learning curve sition, the sampling-based method is fully autonomous and
between the no-transfer and the transfer learner.     does not rely on human decision. As a result, subgoal dis-

                                                IJCAI-07
                                                  2058