 An Integrated Multilevel Learning Approach to Multiagent Coalition Formation 

                                            Leen-Kiat Soh and Xin Li 
                                Department of Computer Science and Engineering 
                                           University of Nebraska-Lincoln 
                                 115 Ferguson Hall, Lincoln, NE 68588-0115 USA 
                                             { lksoh, xinli@cse.unl.edu} 


                        Abstract                               [Sen and Dutta, 2000]), or in coalition formation among 
                                                               cooperative agents (e.g., [Shchory et al, 1997]), little work 
   In this paper we describe an integrated multilevel learn•   has been done in coalition formation among both self-
   ing approach to multiagent coalition formation in a         interested and cooperative agents. Furthermore, there have 
   real-time environment. In our domain, agents negotiate      been no attempts to study coalition formation among such 
   to form teams to solve joint problems. The agent that       agents in a dynamic, real-time, uncertain, and noisy envi•
   initiates a coalition shoulders the responsibility of over• ronment, which is a typical real-world environment and in 
   seeing and managing the formation process. A coali•         which a sub-optimal coalition needs to be formed in a real•
   tion formation process consists of two stages. During       time manner. 
   the initialization stage, the initiating agent identifies the 
                                                                 We propose an integrated multilevel learning approach to 
   candidates of its coalition, i.e., known neighbors that 
                                                               multiagent coalition formation. In our approach, agents are 
   could help. The initiating agent negotiates with these 
                                                               assumed to be cautiously cooperative—they are willing to 
   candidates during the finalization stage to determine the 
                                                               help only when they think they benefit from it—and honest. 
   neighbors that are willing to help. Since our domain is 
                                                               However, due to the noisy, uncertain, dynamic and real-time 
   dynamic, noisy, and time-constrained, the coalitions are 
                                                               nature of our domain, not every agent can be correct in its 
   not optimal. However, our approach employs learning 
                                                               perceptions and assumptions. Thus, to achieve a coalition, 
   mechanisms at several levels to improve the quality of 
                                                               an initiating agent has to negotiate with other agents. 
   the coalition formation process. At a tactical level, we 
                                                               Through concurrent, multiple 1 -to-1 negotiations, the initiat•
   use reinforcement learning to identify viable candidates 
                                                               ing agent identifies the agents that are willing to help. The 
   based on their potential utility to the coalition, and 
                                                               formation process is successful if the initiating agent suc•
   case-based learning to refine negotiation strategies. At 
                                                               cessfully persuades enough agents to join the coalition. 
   a strategic level, we use distributed, cooperative case-
                                                                 Note that in this paper, we focus on improving the quality 
  based learning to improve general negotiation strate•
                                                               of the coalition formation process, and not on the quality of 
  gies. We have implemented the above three learning 
                                                               the coalition after it is formed and executed. 
  components and conducted experiments in multisensor 
  target tracking and CPU re-allocation applications.            Note also that our approach is an example of the "good 
                                                               enough, soon enough" design paradigm. In our domain, an 
1 Introduction                                                 agent has incomplete information about the environment, 
                                                               the task execution is time constrained, and the communica•
Multiagent coalition formation is important for distributed    tion between agents is not reliable, so an optimal coalition 
applications ranging from electronic business to mobile and    formed from the deep learning is impractical. Thus, a sub-
ubiquitous computing where adaptation to changing re•          optimal yet fast coalition formation process is warranted. 
sources and environments is crucial. It increases the ability 
of agents to execute tasks and maximize their payoffs.         2 Coalition Formation 
Moreover, coalitions can dynamically disband when they 
                                                               In our problem domain, when an agent cannot solve a task 
are no longer needed or effective. Thus the automation of 
                                                               execution or resource allocation problem by itself or can get 
coalition formation will not only save considerable labor 
                                                               more benefits from collaborating with other agents, it initi•
time, but also may be more effective at finding beneficial 
                                                               ates a coalition formation process to form a coalition and 
coalitions than human in complex settings [Jennings, 2002]. 
                                                               solve the problem jointly. Figure 1 depicts our coalition 
  Although considerable research has been conducted either 
                                                               formation modules that make up the two stages: initializa•
in coalition formation among self-interested agents (e.g., 
                                                               tion and finalization. The feasibility study and the ranking 
[Tohme and Sandholm, 1999], [Sandholm et al, 1999], 
                                                               of candidates are the initialization stage whereas the nego-


MULTIAGENT SYSTEMS                                                                                                    619  tiations and their management the finalization stage. This    dynamic persuasion threshold, then the responding agent 
 two-stage model [Soh and Tsatsoulis, 2001] allows an agent    will agree to the request. The responding agent also has the 
 to form an initial coalition hastily and quickly to react an  ability to counter-offer due to time constraints or poor evi•
 event and to rationalize to arrive at a working final coalition dence. This is based on the work of [Soh and Tsatsoulis, 
 as time progresses, as a result of our previously described   2001]. 
 domain nature.                                                (7) Acknowledgment: Once all negotiations are com•
                                                               pleted, if a coalition has been formed, the agent confirms the 
                                                               success of the coalition to all agents who have agreed to 
                                                               help. If the agent has failed to form a coalition, it informs 
                                                               the agents who have agreed to help so they can release 
                                                               themselves from the agreements. 
                                                                 In the next section, we will discuss the learning mecha•
                                                               nisms, a critical part of our coalition formation approach. 

                                                               3 Learning 
                                                               Our learning approach incorporates reinforcement learning 
                                                               and case-based learning at two levels. At a tactical level, we 
                                                               use reinforcement learning to identify viable candidates 
                                                               based on their potential utility to the coalition, and case-
     Figure 1. An overview of the coalition formation process  based learning to refine specific negotiation strategies. At a 
                                                               strategic level, we use distributed, cooperative case-based 
 Here we briefly describe each module of the design:           learning to improve general negotiation capabilities. 
(1) Dynamic Profiling: Every agent dynamically profiles 
each neighbor as a vector in the agent about the negotiation   3.1 Reinforcement Learning 
relationship between them, and profiles each negotiation       Reinforcement learning is evident at the coalition initializa•
task as a case in the casebase about the negotiation strategy  tion and finalization stages. During initialization, the initiat•
description and negotiation outcome.                           ing agent measures the potential utility of a candidate based 
(2) Feasibility Study: This module analyzes the problem        on a weighted sum of (1) its past cooperation relationship 
and computes (a) whether the agent has the resources to do     with the initiator such as the candidate's helpfulness, friend•
something about it, and (b) if yes, the list of agents that the liness, and the agent's helpfulness and importance to the 
agent thinks could help.                                       candidate, (2) its current cooperation relationship with the 
(3) Ranking of Candidates: This module scores and ranks        initialing agent such as whether the two agents have already 
each candidate, and proportionately assigns the requested      been negotiating about other problems, and (3) its ability to 
demand to each candidate, based on its potential utility (sec• help towards the current problem. An initiating agent thus 
tion 3.1).                                                     will more likely approach the agents that have been helpful 
(4) Management: This module initiates negotiations with        before, thus reinforcing the cooperation relationship among 
top-ranked candidates. That is, the module manages multi•      them. 
ple, concurrent 1 -to-1 negotiations. For each negotiation       During finalization, an initiating agent also appeals to a 
task, it first finds a negotiation strategy through CBR.       candidate about how helpful the initiating agent has been in 
Then, it spawns a thread to execute that negotiation task.     the past. A candidate is more easily persuaded if it realizes 
The module oversees the various negotiation threads and        that a particular agent has been helpful in the past, and thus 
modifies the tasks in real-time. For example, the module       once again reinforcing their cooperation relationship. For 
will terminate all remaining negotiations once it finds out    details, please refer to [Soh and Tsatsoulis, 2001]. 
that it no longer can form a viable coalition. The module 
will reduce its requests or demands once it has secured        3.2 Case-Based Learning 
agreements from successful negotiations. And so on. In         We use CBR to retrieve and adapt negotiation strategies 
effect, this management simulates a 1 -to-many negotiation.    for negotiations during coalition finalization. We also 
(5) CBR: Given the problem description of a task, the          equip our CBR module with both individual and coopera•
CBR module retrieves the best case from the casebase, and      tive learning capabilities (Figure 2). Individual learning 
adapts the solution of that best case to the current problem.  refers to learning based on an agent's perceptions and 
This is based on the work of [Soh and Tsatsoulis, 2001].       actions, without direct communication with other agents. 
                                                               Cooperative learning refers to learning other agents' ex•
(6) Negotiation: Our negotiation protocol is argumenta•
                                                               perience through interaction among agents. When an 
tive. The initiating agent provides evidence for its request 
                                                               agent identifies a problematic case in its casebase, it ap•
to persuade the responding agent. The responding agent         proaches other agents to obtain a possibly better case. 
evaluates these evidence pieces and if they are higher than a 


620                                                                                             MULTIAGENT SYSTEMS                                                                case is deemed problematic, then a cooperative learning 
                                                               will be triggered and the case will be replaced. Table 2 
                                                               shows the heuristics we use in tandem with the chrono•
                                                               logical casebase. When a negotiation completes, if the 
                                                               new case adds to the casebase's diversity, the agent 
                                                               learns it. If the casebase's size has reached a preset limit, 
                                                               then the agent considers replacing one of the existing 
                                                               cases with the new case. For our individual case-based 
                                                               learning, we use heuristics ///, //2, and H3. 


  Figure 2. The relationship between case learning and CBR as 
             well as negotiation tasks in an agent 
  There has been research in distributed and cooperative 
CBR. [Prasad and Plaza, 1996] proposed treating corporate 
memories as distributed case libraries. Resource discovery 
was achieved through (1) negotiated retrieval that dealt with 
retrieving and assembling case pieces from different re•         Table 1. The usage history that an agent profiles of each ease 
sources in a corporate memory to form a good overall case, 
and (2) federated peer learning that dealt with distributed 
and collective CBR [Plaza et ai, 1997]. [Martin et ah, 
 1999] extended the model using the notion of competent 
agents. [Martin and Plaza, 1999] employed an auction-
based mechanism that focused on agent-mediated systems 
where the best case was selected from the bid cases. 
  Our methodology employs a cautious utility-based adap•
tive mechanism to combine the two learning approaches, an 
interaction protocol for soliciting and exchanging informa•
tion, and the idea of a chronological casebase. It empha•
sizes individual learning and only triggers cooperative learn•
ing when necessary. Our cooperative learning also differs 
from collective CBR in that it does not merge case pieces 
into one as it considers entire cases. In addition, our re•
search focus here is to define a mechanism that combines 
individual and cooperative learning.                             Table 2. Heuristics that support the chronological casebase 
  Note that the communication and coordination overhead 
of cooperative learning may be too high for cooperative        3.2.2 Cooperative Learning 
learning to be cost-effective or timely. Moreover, since an 
agent learns from its own experience and its own view of       Figure 3 depicts our cooperative learning design. We 
the world, its solution to a problem may not be applicable     adhere to a cautious approach to cooperative learning: 
for another agent facing the same problem. This injection of   (1) The agent evaluates the case to determine whether it is 
foreign knowledge may also be risky as it may add to the       problematic. To designate a case as problematic, we use 
processing cost without improving the solution quality of an   heuristics H4 and H5\ a (frequently used) case is problem•
                                                               atic if it has a low success rate (TSU/TU) and a high incur•
agent [Marsella et al.% 1999]. 
                                                               rence rate (TINC/TU). The profiling module keeps track of 
3.2.1 Chronological Casebase and Case Utility                  the utility of the cases. 
We have utilized the notion of a chronological casebase        (2) The agent only requests help from a selected agent that 
in which each case is stamped with a time-of-birth (when       it thinks is good at a particular problem. We want to ap•
it was created) and a time-of-membership (when it joined       proach neighbors who have initiated successful negotiations 
the casebase). All initial cases are given the same time-      with the current agent, with the hope that the agent may be 
of-birth and time-of-membership. In addition, we profile       able to learn how those neighbors have been able to be suc•
each case's usage history (Table 1). An agent evaluates        cessful. This is determined based on the profile of each 
the utility of a case based on its usage history. If the case  neighbor that the agent maintains. The exchange protocol is 
has a low utility, it may be replaced (or forgotten). If the   carried out by the case request and case response modules. 


MULTIAGENT SYSTEMS                                                                                                   621  (3) If the foreign case is similar to the problematic case, the ing (CBRRL), (2) only case-based reasoning (NoRL), (3) 
 agent adapts the foreign case before adopting it into its     only reinforcement learning (NoCBR), and (4) no learning 
 cascbase. At the same time, the usage history parameters of   at all (NoCBRRL). Figure 4 shows the result in terms of the 
 the new case are reset.                                       success rates for negotiations and coalition formations. 
 (4) If a problematic case cannot be fixed after K times, it 
 will be removed (Heuristics H6 and H7). 


                                                               Figure 4. Success rates of negotiations and coalition formations for 
                                                                               different learning mechanisms 
           Figure 3. The cooperative learning design 
                                                                 The agent design with both case-based reasoning/learning 
                                                               and reinforcement learning outperformed others in both 
 4 Experiments and Results                                     negotiation success rate and coalition formation success 
 We have implemented a multiagent syrtem with multiple         rate. That means with learning, the agents were able to ne•
 agents that perform multi-sensor target tracking and adap•    gotiate more effectively (perhaps more efficiently as well) 
 tive CPU reallocation in a noisy environment (simulated by    that led to more coalitions formed. Without either learning 
 a JAVA-based program called RADSIM). Each agent has           (but not both), the negotiation success rates remained about 
 the same capabilities, but is located at a unique position.   the same but the coalition formation rate tended to deterio•
 Each agent controls a sensor and can activate the sensor to   rate. This indicates that without one of the learning meth•
 search-and-detect the environment. When an agent detects a    ods, the agents were still able to negotiate effectively, but 
 moving target, it tries to implement a tracking coalition by  may be not efficiently (resulting in less processing time for 
 cooperating with at least two neighbors. And this is when a   the initiating agent to post-process an agreement). With no 
 CPU shortage may arise: the activity may consume more         learning, the agents fared noticeably poorly. 
 CPU resource. When an agent detects a CPU shortage, it 
 needs to form a CPU coalition to address the crisis.          4.2 Resource Allocation and System Coherence 
   The multiagent system is implemented in C++. In the         We conducted experiments in CPU re-allocation to test the 
current design, each agent has 3 + JV threads. The core        coherence of our system. We refer to the CPU allocation as 
thread is responsible for making decisions and managing        a sustenance resource since in order for an agent to obtain 
tasks. A communication thread is used to interact with the     more CPU, it needs to incur CPU usage while negotiating 
message passing system of the sensor. An execution thread      for the resource. By varying the amount of the initial CPU 
actuates the physical sensor: calibration, search-and-detect   allocation to each agent, we created mildly-constrained, 
for a target, etc. Each agent also has N negotiation threads   overly-constrained, and unevenly-constrained scenarios. 
to conduct multiple, concurrent negotiations.                  Tables 3 and 4 compared the agents' behavior in terms of 
   We used two simulations for our experiments. We con•        successes in negotiations and coalition formations. In par•
ducted experiments in a simulation called RADSIM where         ticular, the coalition success rate is the number of success•
communication may be noisy and unreliable, and one or two      fully formed coalitions over the number of coalitions initi•
targets may appear in the environment. We also designed        ated, where a coalition is successfully formed when the 
and implemented our own CPU shortage simulation module.        CPU obtained satisfies the agent's need. We observed the 
Each task is designated with a CPU usage amount plus a         following: 
random factor. When an agent detects a CPU shortage, the       (1) In all experiments, the reduction in CPU shortage of 
tasks that it currently performs slow down. Thus, a CPU        each agent and the whole system was obvious. Gradually, 
shortage that goes unresolved will result in failed negotia•   the CPU resource was reallocated more evenly among 
tions since our negotiations are time-constrained.             agents. The possibility of a CPU shortage decreases and 
                                                               each agent's shortage amount decreases. This shows a 
4.1 Impacts of Learning                                        coherent, cooperative behavior among the agents. 
We also conducted experiments with four versions of learn•
ing: (1) both case-based reasoning and reinforcement learn•


622                                                                                             MULTIAGENT SYSTEMS (2) In all experiments, after some period of time, each 
agent's CPU allocation converged to an average level 
(14%). After that, each agent fluctuated around that level. 
(3) We also observed that the coalition formation was the 
most successful for the system as a whole when there were 
roughly the same number of resourceful and resource-                Table 5. Experiment sets. For example, in ESI, every agent 
starved agents (Experiment #3) and this type of system also                  has 16 cases in its casebase; and so on 
required the least number of negotiations and coalitions to 
converge. 


                                                                         Table 6. Utility of each outcome for a case 

                                                                The average utility of the case base is the average product 
                                                              of each case's TU value and the utility value of its outcome. 
                                                              The diversity measure of a casebase is computed as the av•
    Table 3. Comparison between negotiations in experiments 
                                                              erage difference between each pair of cases in the casebase. 
                                                              Three slopes, sizeSlope, diffSlope, and utilSlope, were com•
                                                              puted as growth rate between the first learning point and the 
                                                              last learning point, for size, diversity, and utility, respec•
                                                              tively. Table 7 shows one example of the results on initiat•
                                                              ing casebases. 


     Table 4. Comparison between coalitions in experiments 

4.3 Individual & Cooperative Case-Based Learning 
For our investigation of individual and cooperative case-
based learning, we conducted two sets of experiments, 
Comprehensive Experiment A (CEA) and Comprehensive 
Experiment B (CEB). We carried out CEA to study the 
effects of individual learning in subsequent cooperative 
learning and the roles of cooperative learning in agents of 
different initial knowledge. We performed CEB to investi•
gate the effects of the environment on the agents' learning. 

4.3.1 Comprehensive Experiment A (CEA) 
We conducted four sets of experiments in CEA as shown in        Table 7. Utility and difference gains for both Sub-Experiments 
Table 5. The goal of these experiment sets was to investi•      Expl and Exp2, after the second stage, for initiating casebases 
gate how learning differed given different casebase sizes, 
                                                              Looking at all our results, we observed the following: 
and how learning differed given different types of initial 
                                                              (1) Cooperative learning results in more utility and diver•
casebases (some had cases collected from different agents 
                                                              sity per learning occurrence than individual learning, 
from an earlier run, some had only their own cases). Note 
that for the following experiments we set the limit on the    (2) A small casebase learns more effectively in terms of 
casebase size as 30 where case replacement started to take    utility and diversity, but not faster since our learning is 
place. We used two main parameters to evaluate the case-      problem-driven. A large casebase learns in a similar man•
bases: utility and diversity. First, we rank the outcome of   ner as an average casebase except when it is greater than the 
each case following the utility values of Table 6.            preset limit that triggers case replacement. 


MULTIAGENT SYSTEMS                                                                                                   623 