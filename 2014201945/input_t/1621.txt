    Combining Memory and Landmarks with Predictive State Representations

                      Michael R. James    and Britton Wolfe   and  Satinder Singh
                                   Computer Science and Engineering
                                         University of Michigan
                                {mrjames, bdwolfe, baveja}@umich.edu


                    Abstract                          Our goal is to provide a model class that is as expressive as
                                                      PSRs but that has some of the efﬁcient learning properties of
    It has recently been proposed that it is advantageous history-based models.
    to have models of dynamical systems be based        In addition to accelerating learning, mPSRs can also ex-
    solely on observable quantities. Predictive state ploit landmarks or observations that capture the state of the
    representations (PSRs) are a type of model that uses system uniquely, by “resetting” the approximate state of the
    predictions about future observations to capture the learned model during prediction whenever these landmark-
    state of a dynamical system. However, PSRs do     observations are encountered. We present a method for ﬁnd-
    not use memory of past observations. We propose   ing landmarks in mPSRs, develop the basic theory of mPSRs
    a model called memory-PSRs that uses both mem-    and provide preliminary empirical results exploring the rela-
    ories of the past, and predictions of the future. We tive efﬁciency of learning mPSRs and PSRs.
    show that the use of memories provides a number of
    potential advantages. It can reduce the size of the 2 PSRs and memory-PSRs
    model (in comparison to a PSR model). In addi-
    tion many dynamical systems have memories that    PSRs depart from other models of dynamical systems in that
    can serve as landmarks that completely determine  their representation of state is a vector of predictions of the
    the current state. The detection and recognition of outcomes of tests that may be performed on the dynamical
    landmarks is advantageous because they can serve  system. A test t = a1o1, ...akok is a sequence of alternating
    to reset a model that has gotten off-track, as of- actions ai ∈ A and observations oj ∈ O, and its prediction,
    ten happens when the model is learned from sam-   p(t), is the conditional probability that the observation se-
    ples. This paper develops both memory-PSRs and    quence occurs, given that the action sequence is taken, and so
    the use and detection of landmarks.               p(t) = prob(o1, ...ok|a1, ...ak). Of course, the prediction of
                                                      a test is dependent on the actions and observations that have
                                                      occurred so far, called the history. The prediction of a test t
1  Introduction                                       at history h is p(t|h) = prob(o1, ...ok|ha1, ...ak).
This paper explores the use of two types of observable quanti- A PSR’s state representation consists of predictions of a
ties — the history of past observations and predictions about special set Q of tests, called the core tests. The core tests
future observations — in creating models for complex dy- are special because at any history, the predictions for any test
namical systems. Models that use only predictions of future can be computed as a linear function of the predictions of the
observations to capture state, called predictive state repre- core tests. The prediction vector p(Q|h) is the (n = |Q| × 1)
sentations or PSRs [Littman et al., 2001], have been shown vector of predictions for the tests in Q at history h. Thus,
(surprisingly) to be at least as expressive and compact [Singh p(Q|h) is the PSR’s representation of state and is the coun-
et al., 2004] as classical models such as partially observable terpart to belief-states in POMDPs and the last k observations
Markov decision processes (POMDPs) that use hidden state in k-order Markov models.
variables. Models that only use observation history, such as In addition to the set of core tests, a PSR has model param-
k-order Markov models, are known to be not as expressive eters: a set of (n×n) matrices Mao, and (n×1) vectors mao,
and hence not as widely applicable as PSRs or POMDPs. On for all a, o. The model parameters allow linear computation
the other hand, history-based models are easier to learn from of the prediction for any test t = {a1o1 . . . akok} as follows:
data than either PSRs and POMDPs. In this paper, we pro-
                                                             p(t|h) = p(Q|h)T M  ...M       m     .   (1)
pose an extension to PSRs, called memory-PSRs (mPSRs),                        a1o1   ak−1ok−1 akok
that combines past observations (memories) with predictions Updating the current state, or prediction vector, when action
about the future to deﬁne the state of a dynamical system.1 a is taken in history h and o is observed, is accomplished by

  1                                                                                        T
   Sutton and Tanner [2004] have also recently proposed a model       T   p(aoQ|h)   p(Q|h)  Mao
                                                             p(Q|hao)   =          =        T    .    (2)
that combines memory and predictions.                                      p(ao|h)   p(Q|h)  maoThe matrices Mao and vectors mao have a special form. parameters. The set of memories and core tests for each par-
    th
The i  column of Mao is the (n × 1) constant vector that tition form the basis of mPSR models. It is straightforward
computes the prediction of the (ao) one-step extension of that the rank of each submatrix is at most the rank of the full
    th
the i  core test qi ∈ Q, i.e., of test aoqi. The vector matrix D. In the worst case, the ranks of all the submatri-
mao is the constant (n × 1) vector that computes the pre- ces are the same as the rank of D, in which case the resulting
diction for the one-step test t = ao. The fact that the mPSR model may have (many) more parameters than the PSR
model parameters have these meanings is the foundation of model. But in other cases the ranks of the submatrices may
existing PSR learning algorithms [James and Singh, 2004; be much smaller than the rank of D and then the resulting
Rosencrantz et al., 2004] as well as the new mPSR learning mPSR model may be more compact than the PSR model. We
algorithm presented here.                             provide examples of both cases in Section 5. The size of the
                                                      model is important because a model with fewer parameters
                         Tests                        should in general be more efﬁcient (i.e., require less data) to
                                                      learn. We also test this empirically in Section 5.
                     t1          t j                    In this paper we do not address the question of automat-
            φ = h p(t  |h  )   p(t  |h  )             ically discovering useful ways of partitioning histories and
                1    1  1        j  1                 instead assume that partitions correspond to history sufﬁxes


           es                                         of some ﬁxed length.
           i
                                                        Deﬁnition of memory:  For this paper, given an mPSR

           stor                                       using length k sufﬁxes, a memory is a speciﬁc sequence of

           Hi                  p(t  |h  )             length k that identiﬁes a partition. So, when considering a
               h i p(t 1 |hi  )  j  i
                                                      history h and memories of length one, the memory at h is
                                                      just the last (most recent) observation in h, and there will be
                                                      |O| memories in the mPSR. For memories of length two, the
                                                      memory at h is the last (most recent) action-observation pair
         Figure 1: The system dynamics matrix.        in h, and there will be |A| ∗ |O| memories in the mPSR. In
                                                      general, the set of possible memories in an mPSR that uses
                                                      length-k sufﬁxes is the set of all length-k sequences of alter-
System Dynamics Matrix                                nating actions and observations that end in an observation; we
The theoretical development of mPSRs is best explained us- will denote the size of such a set as mk.

ing the system dynamics matrix, D, that was developed in Let µ1 . . . µmk represent all the distinct memories in an
Singh et al. [2004]. The matrix D, shown in Figure 1, has all mPSR model. Also, let the memory at history h be denoted
                                                                                                       µi
possible histories as rows and all possible tests as columns. µ(h). Each memory µi has a corresponding submatrix D
The columns (rows) are arranged by test (history) length and created by the partition of histories corresponding to µi. The
within the same length in lexicographic ordering. The entry core tests for partition Dµ are referred to as µ-core tests to
                                    th           th
Dij =  p(tj|hi) is the prediction for the j test at the i distinguish them from the core tests of the PSR model of
history. There is a one to one correspondence between sys- the same system. Thus, the µ-core tests for the submatrices
                                                        µ      µ               µ      µ
tem dynamics matrices and dynamical systems. The rank of D 1 . . . D mk are denoted Q 1 . . . Q mk respectively. The
D is the dimension of the corresponding dynamical system prediction vector p(Qµ(h)|h) contains the predictions for the
                                                                  µ(h)
and is the number of core tests needed by the PSR model. µ-core tests Q at memory µh.
The core tests of a PSR model correspond to a maximal set Deﬁnition of mPSR state: The mPSR state at history
of linearly independent columns. Similarly, we can deﬁne a h is denoted by the concatenation of the memory at h
notion of core histories that correspond to a maximal set of and the prediction vector for that memory at h, i.e., as
linearly independent rows. We will only be interested in ﬁnite
                                                      [µ(h), p(Qµ(h)|h)]. Thus, the mPSR state contains both a
dimensional systems. This assumption is not terribly restric-
                                                      memory of the past as well as predictions about the future
tive because all POMDPs with n underlying or nominal states
                                                      while a PSR state contains only predictions about the future.
are dynamical systems of dimension at most n. A full deriva-
tion of PSRs through use of the system dynamics matrix is Deﬁnition of mPSR update parameters: For each mem-
                                                                                         µi            µi
presented in Singh et al. [2004].                     ory, µi, we will keep a set of matrices Mao and vectors mao
                                                      for all a, o. There is a subtle issue in deﬁning these parame-
memory-PSRs                                           ters. It is not the case that each submatrix is a separate dynam-
The central idea behind mPSRs is to partition D into a set ical system. Indeed if the memory corresponding to current
                 1     m
of m submatrices D . . . D by partitioning the set of his- history h is say µi and we take action a and observe o, the
tories (rows) but not the tests (columns), i.e., each submatrix memory corresponding to the next history hao could be dif-
  i
D  contains all the columns but only a subset of the histories. ferent from µi. Lets say it is µj. Thus the update parameters
                                                        µi
How do memories enter this picture? We will use memory Mao must transform the current prediction vector that makes
of past observations to partition histories, and every mem- prediction for µ-core tests Qµi in history h to the prediction
ory will correspond to a partition; we elaborate on this later. vector for µ-core tests Qµj in history hao. Note that all histo-
For now, note that each submatrix will have its own rank and ries belonging to memory µi will transition to the same mem-
its own core tests and core histories as well as model-update ory µj for action-observation pair ao, i.e., j is uniquely deter-                                                 µi
mined by i and the pair ao; and so the model parameter Mao Lemma 2. For any dynamical system of ﬁnite dimension and
need not explicitly reference to the next memory µj. Thus any choice of (ﬁxed-length sufﬁx) memories, the size of the
one can deﬁne the state update for mPSRs as follows: upon resulting mPSR model for that system is at most the number
taking action a in history h and observing o          of memories times the size of a PSR model for that system.

                  p(aoQµj |h)   p(Qµi |h)T M µi       Proof. In the worst case, the rank of each of the submatrices
     p(Qµj |hao) =           =             ao   (3)
                                   µ    T  µi
                    p(ao|h)     p(Q i |h) mao         in the mPSR is exactly the rank of the full system dynamics
                                                      matrix. In such a case, if we happen to ﬁnd different µ-core
                                            µi
where µ(h) = µi and µ(hao) = µj. The matrix Mao is of tests for each submatrix, then the model for each submatrix
      µi     µj                µi           µi
size (|Q |×|Q  |) and the vector mao is of size (|Q |×1). will be exactly as large as the PSR model.
As in PSRs the update parameters in mPSRs have meaning.
               th            µi        µi
For instance, the k column of Mao is the (|Q | × 1)-sized The above lemma holds no matter how one chooses the
constant parameter vector that computes the prediction for the memories. But what if we can choose the memories to use
             th
test that is the k µ-core test for memory µj.         in constructing an mPSR judiciously (by which we mean to
  The mPSR model parameters allow linear computation of minimize the size of the resulting model)?
the prediction for any test t = {a o . . . a o } at history h as
                           1 1    k k                 Theorem 1. With a judicious choice of memories, the size of
follows:
                                                      the corresponding mPSR for any dynamical system is at least
                    T   µ0     µk−2     µk−1          as compact as the size of the PSR for that dynamical system.
      p(t|h) = p(Q|h) Ma o ...Ma  o   ma o  .   (4)
                        1 1    k−1 k−1   k k          Furthermore, there exist dynamical systems for which some
where µ0 is the memory corresponding to history h, µ1 is the choice of memories leads to an mPSR model that is more com-
memory corresponding to history ha1o1 and so on.      pact than the PSR model of the same dynamical system.
  Deﬁnition of mPSR: An mPSR model is deﬁned by the
                       µ      µ              µ
                        1      mk             0       Proof. The proof of the ﬁrst statement follows from the fact
tuple hA, O, µ1 . . . µmk , Q . . . Q , M, [µ0, p(Q |∅)]i
where A is the set of actions; O is the set of observations; that a PSR is also an mPSR with the null set as memories. We
                                   µi                 prove the second statement by constructing such a dynamical
µ1 . . . µmk are the possible memories; Q is the set of µ-
core tests for memory µi; M is the set of update parameters system. Table 1 compares the size of mPSRs and PSRs for
  µi      µi
Mao and mao for all a, o, µi; µ0 is the initial memory; and various standard test dynamical systems (based on POMDPs).
p(Qµ0 |∅) is the initial prediction vector.           In all instances, a sufﬁx of length one was used to partition
  A special and interesting case arises when a memory by the histories. For at least three problems, Cheese, Shuttle and
itself serves as state and no predictions are needed. Four-Three, the mPSR is more compact than the PSR.
  Deﬁnition of landmark: Given an mPSR, a landmark is a
memory that serves as state, i.e., is a sufﬁcient statistic of his- 3.1 Landmarks
tory. Landmarks can be quite beneﬁcial for making accurate We show that landmarks are equivalent to memories for
predictions. We will discuss the identiﬁcation of landmarks which there is only one µ-core test and furthermore that the
and exploiting them for prediction in Section 3.1.    prediction of any test at a landmark is the same at all histories
                                                      that map to that landmark.
3  Basic Theory of mPSRs                              Lemma 3.  For an mPSR model of a dynamical system,
Our ﬁrst result is to show that for any dynamical system it • any memory that is a landmark has only one µ-core test,
is possible to ﬁnd µ-core tests for all memories in an mPSR and every memory that has a µ-core set of size one is a
model from the set Q of core tests for the corresponding PSR landmark.
model.
                                                        • for all histories that map to a landmark the prediction
Lemma 1. Let a dynamical system be modeled as a PSR with  vector is a constant, i.e., independent of history.
core tests Q. Then there always exists an mPSR model for the
system such that for each memory µi the corresponding µ- • for all histories that map to a landmark the prediction of
core tests Qµi satisfy Qµi ⊂ Q.                           any test t is a constant, i.e., independent of history.

Proof. We provide a constructive proof which shows how to Proof. If µ is a landmark, then the predictions for all tests
                µi
derive the subset Q from Q. Recall that all columns of D are fully determined when µ is known. Therefore, at any two
are linear combinations of the columns corresponding to Q in histories that have memory µ, the prediction of every test is
                         µi
D. Consider the submatrix D for memory µi. It must be the same at both histories. This means that every row of Dµ
                          µi
the case that all columns of D are linear combinations of must be identical, so the rank of Dµ is one, and only one
                                µi
the columns corresponding to Q in D . Thus, there exists µ-core test exists for µ.
 µi
Q   ⊂ Q.                                                For a memory µ that has a single µ-core test, qµ, let Hµ
                                                      denote the histories that map to memory µ. For any h ∈ Hµ,
  In this paper, we don’t exploit Lemma 1 for two reasons: 1)                     P
the core tests of PSRs are not unique, and 2) in any case when and any action a, it must hold that o p(ao|h) = 1 which im-
                                                              P      µ  T   µ
learning a model of a dynamical system from experience data plies that o p(q |h) mao = 1, which in turn implies that
                                                         µ   T     P     µ             µ
we do not have a PSR model and thus its core tests Q to begin p(q |h) = 1/ o mao. Recall that mao are independent of
with.                                                 history, and thus the prediction of qµ must be independent of                                                      Determining µ-core tests and histories
   Table 1: Comparison of the size of PSRs and mPSRs
                                                      The algorithm proceeds in iterations. At iteration t, the algo-
                  # Core Tests    # Param. Entries    rithm has a current set of linearly independent or µ-core tests
   Problem                                                                             µi  µi
               PSR     mPSR       PSR    mPSR         and histories for each memory. Let Ht (Tt ) be the set of
   Tiger        2        2,2       36      72         µ-core histories (µ-core tests) found by iteration t for mem-
   Paint        2        2,2       48      96         ory µi. These µ-core tests (histories) start off as the empty
   Cheese      11    1,1,1,1,2,2,3 3696   792         set at the ﬁrst iteration. We also keep a set of candidate tests
   Network      7       4, 6      448     480         and histories at each iteration. The set of candidate tests for
   Bridge       5     2,2,4,4,5   1800    4488        memory  µi is the set of one-step tests; and for every ao pair,
   Shuttle      7     1,1,2,2,4   840     450         the set of ao-extensions for all the current µ-core tests for
   Four Three  10     1,1,1,1,3,4 2640    748         memory  µj, where µj is the next memory when action a is
   Float Reset  5        1,5      120      96         taken in memory µi and o is observed. The set of candidate
                                                      histories is similarly generated. We ask the oracle for the
                                                      predictions of all the candidate tests at all the candidate his-
the history. This calculation exploits the fact that the predic- tories. We compute the rank of this oracle-generated matrix,
tion vector and update parameters are scalars.        denoted Xt. The linearly independent rows and columns of
                                    µ                                    µi      µi
  Furthermore, because the prediction of q is independent Xt become the new Ht+1 and Tt+1 respectively. In selecting
                             µ
of history, and the update vector mt for any test t is indepen- the new µ-core tests and histories we always include the pre-
                                     µ  T  µ
dent of history, the prediction p(t|h) = p(q |h) mt of t at vious µ-core tests and histories. We stop at iteration s if the
history h must also be independent of history. So, all rows of rank of Xs is the same as the rank of Xs−1 for all memories.
Dµ must be identical, and µ is therefore a landmark.    The above algorithm is an adaptation of the similar al-
                                                      gorithm for determining core test and histories for PSRs by
  Landmarks come in handy because they can be used to James and Singh [2004]. Just like for PSRs this algorithm is
keep a learned — and therefore only approximately correct not guaranteed to ﬁnd all µ-core tests and histories, but also
— model from progressively drifting farther and farther away just like for PSRs it seems to work for all empirical work done
from reality as we make longer and longer term predictions so far.
from such a model. Every time we observe a landmark mem-
ory, we can reset the prediction vector to the constant corre- Computing the model-update parameters
sponding to the landmark irrespective of the actual observed Now we discuss how to compute the model-update matrix
                                                        µ
history. This can keep the error in the prediction for very long Mao for any ao pair and any memory µ. We deﬁne a matrix A
tests from growing with the length of the test. We exploit this for memory µ that contains the predictions for all the µ-core
ability in our empirical results below.               tests at all the µ-core histories for memory µ. This matrix will
                                                      have full rank and is computed in the algorithm above. Let µj
4  Learning mPSR models from data                     be the memory achieved when action a is taken in memory µ
                                                                           th           µ
In this section we present an algorithm for learning mPSR and o is observed. The k column of Mao, denoted x, is the
                                                      constant parameter vector that computes the prediction for the
models from data under the assumption that the algorithm has            th
the ability to reset the dynamical system being modeled to an ao-extension of the k µ-core test, denoted y, of memory µj.
initial state. We present the algorithm in two stages. In the Let b be the column vector of predictions of test y for the µ-
ﬁrst stage, we will assume that the algorithm has access to an core histories of memory µ. We ask the oracle for the entries
                       h       t                      of b. Then from Equation 3 above, Ax = b. Since A is
oracle that if given a history and test will return the predic-                           −1
tion p(t|h) for the dynamical system. In the second stage we full rank, it is invertible and hence x = A b. We can use
                                                      the same idea to compute the model-update parameter vector
will show how the oracle can be replaced by the use of sample µ
data from the dynamical system with reset. Because we wish mao.
to minimize the amount of sample data needed by our algo- In the next section we show how the oracle needed for the
rithm, we will attempt to minimize the number of calls to the above algorithm can be replaced by using data from the dy-
oracle in the ﬁrst stage. Our algorithm is a relatively straight- namical system with reset.
forward adaptation to mPSRs of the algorithm presented by
James and Singh [2004] for PSRs.                      4.2  Learning mPSRs from sample data
                                                      We show how to obtain an estimate for the prediction p(t|h),
4.1  Oracle based algorithm                           something we went to the oracle for in the previous algorithm.
Recall that there are two components to an mPSR: the µ-core We generate samples from the distribution p(t|h) by ﬁrst tak-
tests for the different memories and the model-update param- ing the action sequence in h after a reset. If the observation
eters for the memories. In our empirical work below, we al- sequence in h happens, then we have succeeded in generating
ways use memories of length one. Clearly the model param- history h. If we don’t succeed in generating the history, we
eters for a memory depend on the choice of µ-core tests for reset the system and try again. If we do succeed in generat-
that memory (recall that the µ-core tests are not unique). The ing the history, we take the action sequence in test t. If the
process of computing µ-core tests and update parameters for observation sequence in test t happens, the test succeeds, else
each memory is identical and so we can just discuss how to the test fails. We get an estimate of p(t|h) from the empirical
do this for any one memory, say µi.                   success rate of the test t at history h. Of course, this is ex-                  A                                  B                                  C
      PSR and mPSR Learning for Tiger    PSR and mPSR Learning for Paint   PSR and mPSR Learning for Network
                                         0.01                               0.1
                       PSR                                PSR                               PSR
       0.1            mPSR                               mPSR                              mPSR
                                        0.001                               0.01
                                                                         ng  Error
     0.001                             1e−04                             i  0.001
                                                                         Test
   Testing  Error                     Testing  Error
    1e−05                              1e−05                              1e−04
         0   300000 600000 900000           0     2e+06  4e+06   6e+06         0    1e+06  2e+06   3e+06
           Total Number of Samples            Total Number of Samples            Total Number of Samples

                  D                                  E                                  F
       PSR and mPSR Learning for Bridge PSR and mPSR Learning for Cheese  PSR and mPSR Learning for Float−Reset
         1                                                                   0.1
                       PSR                               PSR                               PSR
        0.1           mPSR               0.01           mPSR                              mPSR
                                                mPSR with landmarks         0.01  mPSR with landmarks
       0.01
                                                                         ng  Error
                                      ng  Error
                                                                         i
                                       i 1e−04                             0.001
      0.001
                                                                         Test
   Testing  Error
                                      Test
      1e−04                            1e−06                               1e−04
          0  1e+08 2e+08 3e+08 4e+08        0  4e+07 8e+07 1.2e+08 1.6e+08     0      200000   400000
            Total Number of Samples           Total Number of Samples            Total Number of Samples

                  G                                  H
     PSR and mPSR Learning for Shuttle    PSR and mPSR Learning for 4x3
                                           1
                      PSR                                 PSR
      0.1            mPSR                 0.1            mPSR
             mPSR with landmarks                mPSR with landmarks
                                         0.01
   ng  Error
                                      ng  Error
   i
     0.001                            i
                                        0.001
   Test
                                      Test
    1e−05                               1e−04
         0 6e+06   1.2e+07 1.8e+07           0   4e+07  8e+07 1.2e+08
           Total Number of Samples             Total Number of Samples

         Figure 2: Results of running mPSR learning algorithms on a suite of test problems. See text for details.

tremely wasteful in the number of data samples needed. We second is to test whether the oracle-based algorithm for com-
implement this basic idea much more efﬁciently by execut- puting mPSRs ﬁnds exact mPSR models. The third is to test
ing the action sequence in ht regardless of the observations the efﬁcacy of the sample-data based learning algorithm for
obtained and then mining the data generated for all possible mPSRs. Finally, we are also interested in whether using land-
history and test pairs that can be extracted from it. marks, where available, to reset prediction vectors during the
  One problem that occurs with this estimation technique process of computing predictions for long test sequences pro-
is that the sampled entries of the system dynamics matrix vides a measurable beneﬁt. All of these experiments were on
will almost certainly result in inaccurate calculations of rank a suite of standard POMDP-based dynamical systems [Cas-
(needed in the above oracle-based algorithm). We use a more sandra, 1999] that have been used for both POMDP-learning
robust rank calculation procedure developed in the PSR learn- as well for PSR-learning.
ing algorithm by James and Singh [2004]. The central idea is Again, for all of the results presented below, the memories
that we can use the number of samples that went into each used for the mPSR models were the most recent observation.
entry of the matrix we wish to ﬁnd the rank of to generate a
threshold on singular values obtained via singular value de- 5.1 Comparing PSRs and mPSRs
composition. This threshold is more conservative when we For PSRs and mPSRs, the size of the model can be measured
have fewer samples and generally leads to a smaller rank than both in terms of the number of core tests (µ-core tests), and
otherwise. As more samples are included, the threshold be- in the number of entries in the model parameters. Table 1
comes less conservative and the calculated rank is closer to compares the sizes of PSR and mPSR models for the suite
the straightforward calculation of rank.              of test problems. For mPSRs, the number of µ-core tests at
                                                      each memory is listed. Fully half of the test problems have
5  Empirical results                                  landmarks (memories with only one µ-core test), indicating
We conducted experiments with a view towards validating that this may be a fairly common situation.
four major ideas presented above. The ﬁrst experiments are The number of entries in the model parameters are listed
meant to compare the relative size of PSRs and mPSRs. The for both PSRs and mPSRs. On three of the problems (Cheese,