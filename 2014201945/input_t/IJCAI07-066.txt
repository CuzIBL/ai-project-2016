            Extracting Chatbot Knowledge from Online Discussion Forums*

                               Jizhou Huang1, Ming Zhou2, Dan Yang1
          1School of Software Engineering, Chongqing University, Chongqing, China, 400044
                                   {jizhouhuang, dyang}@cqu.edu.cn
   2Microsoft Research Asia, 5F Sigma Center, No.49 Zhichun Road, Haidian, Bejing, China, 100080
                                       mingzhou@microsoft.com

                    Abstract                            An online discussion forum is a web community that al-
                                                      lows people to discuss common topics, exchange ideas, and
    This paper presents a novel approach for extracting
                                                      share information in a certain domain, such as sports, mov-
    high-quality <thread-title, reply> pairs as chat
                                                      ies, and so on. Creating threads and posting replies are ma-
    knowledge from online discussion forums so as to  jor user behaviors in forum discussions. Large repositories
    efficiently support the construction of a chatbot for
                                                      of archived threads and reply records in online discussion
    a certain domain. Given a forum, the high-quality
                                                      forums contain a great deal of human knowledge on many
    <thread-title, reply> pairs are extracted using a
                                                      topics. In addition to rich information, the reply styles from
    cascaded framework. First, the replies logically
                                                      authors are diverse. We believe that high-quality replies of a
    relevant to the thread title of the root message are
                                                      thread, if mined, could be of great value to the construction
    extracted with an SVM classifier from all the re- of a chatbot for certain domains.
    plies, based on correlations such as structure and
                                                        In this paper, we propose a novel approach for extracting
    content. Then, the extracted <thread-title, reply>
                                                      high-quality <thread-title, reply> pairs from online discus-
    pairs are ranked with a ranking SVM based on their
                                                      sion forums to supplement chatbot knowledge base. Given a
    content qualities. Finally, the Top-N <thread-title,
                                                      forum, the high-quality <thread-title, reply> pairs are ex-
   reply> pairs are selected as chatbot knowledge. Re-
                                                      tracted using a cascaded framework. First, the replies logi-
   sults from experiments conducted within a movie    cally relevant to the thread title of the root message are ex-
   forum show the proposed approach is effective.
                                                      tracted with an SVM classifier from all the replies, based on
                                                      correlations such as structure and content. Then, the ex-
1   Introduction                                      tracted <thread-title, reply> pairs are ranked with a ranking
A chatbot is a conversational agent that interacts with users SVM based on their content qualities. Finally, the Top-N
in a certain domain or on a certain topic with natural lan- <thread-title, reply> pairs are selected as chatbot knowl-
guage sentences. Normally, a chatbot works by a user ask- edge.
ing a question or making a comment, with the chatbot an- The rest of this paper is organized as follows. Important
swering the question, or making a comment, or initiating a related work is introduced in Section 2. Section 3 outlines
new topic. Many chatbots have been deployed on the Inter- the characteristics of online discussion forums with the ex-
net for the purpose of seeking information, site guidance, planations of the challenges of extracting stable <thread-
FAQ  answering, and so on, in a strictly limited domain. title, reply> pairs. Section 4 presents our proposed cascaded
Existing famous chatbot systems include ELIZA [Weizen- framework. Experimental results are reported in Section 5.
baum, 1966], PARRY [Colby, 1973] and ALICE.1  Most    Section 6 presents comparison of our approach with other
existing chatbots consist of dialog management modules to related work. The conclusion and the future work are pro-
control the conversation process and chatbot knowledge vided in Section 7.
bases to response to user input. Typical implementation of
chatbot knowledge bases contains a set of templates that 2 Related Work
match user inputs and generate responses. Templates cur- By “chatbot knowledge extraction” throughout this paper,
rently used in chatbots, however, are hand coded. Therefore, we mean extracting the pairs of <input, response> from on-
the construction of chatbot knowledge bases is time con- line resources.
suming, and difficult to adapt to new domains.          Based on our study of the literature, there is no published
                                                      work describing the use of online communities like forums
                                                      for automatic chatbot knowledge acquisition. Existing work
   * This work was finished while the first author was visiting Mi- on automatic chatbot knowledge acquisition is mainly based
crosoft Research Asia during Feb.2005-Mar.2006 as a component on human annotated datasets, such as the work by Shawar
of the project of AskBill Chatbot led by Dr. Ming Zhou. and Atwell [2003] and Tarau and Figa [2004]. Their ap-
   1 http://www.alicebot.org/                         proaches are helpful to construct commonsense knowledge


                                                IJCAI-07
                                                   423for chatbots, but are not capable of extracting knowledge for 4. There is no evidence to reveal who has replied to
specific domains.                                           which reply unless the participants have quoted the
  Notably, there is some work on knowledge extraction       entire entries or parts of a previously posted reply to
from web online communities to support QA and summari-      preserve context [Eklundh, 1998].
zation. Nishimura et al. [2005] develop a knowledge base To overcome these sorts of difficulties, lexical and struc-
for a QA system that answers type “how” questions. Shres- tural information from different replies within threads are
tha and McKeown [2004] present a method to detect <ques- analyzed in our experiments, as well as user behaviors in
tion, answer> pairs in an email conversation for the task of discussions.
email summarization. Zhou and Hovy [2005] describe a    Therefore, to extract valid pairs of <input, response>
summarization system for technical chats and emails about from a forum, we first need to extract relevant replies to ini-
Linux kernel. These researchers’ approaches utilize the tial root messages. In this process, replies that are relevant
characteristics of their corpora and are best fit for their spe- to the previous replies rather than to the initial root message
cific tasks, but they limit each of their corpora and tasks, so are ignored and the replies logically directly relevant to the
they cannot directly transform their methods to our chatbot thread title are extracted. The replies to the initial root mes-
knowledge extraction approach.                        sage, in spite of being relevant, may have different qualities.
                                                      To select high-quality replies, a ranking SVM is employed
3   Our Approach                                      to rank the replies. Finally, the pairs of the title of the root
An online discussion forum is a type of online asynchronous message and the extracted Top-N replies are used as the
communication system. A forum normally consists of sev- chatbot knowledge.
eral discussion sections. Each discussion section focuses on
a specific discussion theme and includes many threads. Peo- 4 Cascaded Hybrid Model
ple can initiate new discussions by creating threads, or ask An input online discussion forum F contains discussion
(answer) questions by posting questions (replies) to an ex- sections s1,s2,…,sk. A section consists of T threads t1,t2,…,tu.
isting section. In a section, threads are listed in chronologi- Each thread t is a sequence of replies t={r0,r1,r2,…,rn},
cal order. Within a thread, information such as thread title, where r0 is the root message posted by the thread starter and
thread starter, and number of replies are presented. The ri is the ith (i 1) reply. A reply r is posted by a participant
thread title is the title of the root message posted by the p at a specific moment m with content c. A thread t can be
thread starter to initiate discussion. One can access a thread modeled as a sequence of triplets:
from the thread list and see the replies listed in chronologi- =
                                                           t  {r 0,r1,r2 ,...,rn}
cal order, with the information of the authors and posting   = {( p ,m ,c ),( p ,m ,c ),( p ,m ,c ),...,( p ,m ,c )}
times.                                                           0 0  0  1  1 1   2 2 2     n  n n
                                                                                            
  Compared with other types of web communities such as  We define an RR as a direct reply rj ( j 1) to the root
newsgroups, online discussion forums are better suited for message r0 where rj is not correlated with the other reply
                                                              
chatbot knowledge extraction for the following reasons: rj’ ( j' 1 j' j) in the thread.
  1.  In a thread within a forum, the root message and its Therefore, chatbot knowledge (CK) can be viewed as the
      following up replies can be viewed as <input, re- pairs of <input, response> that fulfill the following con-
      sponse> pairs, with same structure of chat template straints:
                                                                     =
      of a chatbot.                                              CK   {(input,response)}
  2.  There is popular, rich, and live information in an on-        = {(thread-title, high-quality RR)}
      line discussion forum.                            A thread title is used to model the user input of a chatbot
  3.  Diverse opinions and various expressions on a topic and RRs of this thread are used to model the chatbot re-
      in an online discussion forum are useful to extract sponses. The high-quality pairs of <thread-title, RR> will be
      diverse <input, response> pairs for chatbots.   selected as chatbot knowledge.
  Due to technical limitations of current chatbots in han- A high-quality pair of <thread-title, RR> for the chatbot
dling dialogue management, we think that pairs of <input, should meet the following requirements:
response> for a chatbot should be context independent,  1.  The thread-title is meaningful and popular.
which means that the understanding inputs and responses 2.  The RR  provides descriptive, informative and trust-
will not rely on the previous <input, response>.            worthy content to the root message.
  However, because of the nature of a forum, it is difficult 3. The RR has high readability, neatly short and concise
to extract high-quality <input, response> pairs that meet   expressive style, clear structure.
chatbot requirements:                                   4.  The RR  is attractive and can capture chatter’s inter-
  1.  Replies are often short, elliptical, and irregular, and est.
      full of spelling, usage, and grammar mistakes which 5. Both thread-title and RR should have NO intemper-
      results in noisy text.                                ate sentiment, no obscene words and exclusive per-
  2.  Not all of replies are related to root messages.      sonal information.
  3.  A reply may be separated in time or place from the 6. Both thread-title and RR should have proper length.
      reply to which it responds, leading to a fragmented In this paper, identifying the qualified thread-title is not
      conversational structure. Thus, adjacent replies our focus. Instead, we focus on selecting qualified RR.Fig-
      might be semantically unrelated.                ure 1 illustrates the structure of the cascaded model. The


                                                IJCAI-07
                                                   424first pass (on the left-hand side) applies an SVM classifier to new comments to the replies. Therefore, the added replies
the candidate RR to identify the RR of a thread. Then the gradually diverge from the original root message. If a par-
second pass (in the middle) filters out the RR that contains ticipant wants to supplement or clarify his previous reply, he
intemperate sentiment, obscene words and personal infor- can add a new reply. Therefore, the participant’s new reply
mation with a predefined keyword list. The RR which is is often the supporting reason or argument to his previous
longer than a predefined length is also filtered out. Finally reply if they are close to each other.
the RR ranking module (on the right-hand side) is used to Content features include the features about the number of
extract the descriptive, informative and trustworthy replies words and the number of content words in the current reply,
to the root message.                                  the overlapping words and content words between the root
                                                      message and the current reply. In our work, words that do
                     Filter  out 
            RR                   RR     (input, response)                         2
A thread             non-eligible                     not appear in the stop word list are considered as content
         identification         ranking    pairs
                       RR                             words. Feature 2-7 estimates the specialization of the cur-
            Figure 1. Structure of Cascaded Model.    rent reply by the number of domain specific terms. To sim-
                                                      plify the identification of domain specific terms, we simply
4.1 RR Identification                                 extract words as domain specific words if they do not ap-
The task of RR identification can be viewed as a binary clas- pear in a commonly used lexicon (consists of 73,555 Eng-
sification problem of distinguishing RR from non-RR. Our lish words). Feature 2-8 estimates a reply’s pertinence to
                                                      other replies, because some participants might insert the
approach is to assign a candidate reply ri (i 1) an appropri-
ate class y (+1 if it is an RR, -1 or not). Here Support Vector registered nicknames of other participants and sometimes
Machines (SVMs) is selected as the classification model be- add clue words such as “P.S.” to explicitly correlate their
cause of its robustness to over-fitting and high performance replies with certain participants.
[Sebastiani, 2002].                                       RR
  SVMlight [Joachims, 1999] is used as the SVM toolkit 4.2    Ranking
for training and testing. Table 1 lists the feature set to iden- Further, after the RRs have been identified, non-eligible RRs
tify RR for a pair of <thread-title, reply>.          are filtered out with a keyword list with 33 obscenities, 62
                                                      personal information terms (terms beginning with “my”,
 1   Structural features                              such as my wife, my child) and 17 forum specific terms
    1-1  Does this reply quote root message?          (such as Tomatometer, Rotten Tomato, etc.). Replies with
    1-2  Does this reply quote other replies?         more than N words are eliminated because people may be-
    1-3  Is this reply posted by the thread starter?  come bored in chatbot scenarios if the response is too long.
    1-4  # of replies between same author’s previous and cur- In our experiments, N is set as 50 based on our observation.3
         rent reply                                     We analyzed the resulting RRs set of 4.1. For some RRs,
 2   Content features                                 there is certain noise left from the previous pass, while for
    2-1  # of words                                   other RRs, there are too many RRs with varied qualities.
    2-2  # of content words of this reply             Therefore, the task of RR ranking is to select the high-
    2-3  # of overlapping words between thread-title and re- quality RRs. The ranking SVM [Joachims, 2002] is em-
         ply                                          ployed to train the ranking function using the feature set in
    2-4  # of overlapping content words between thread-title Table 2.
         and reply                                      The number of being quoted of a reply is selected as a
    2-5  Ratio of overlapping words
                                                      feature (feature 1-1) because a reply is likely to be widely
    2-6  Ratio of overlapping content words between thread-
         title and reply                              quoted within a thread as it is popular or the subject of de-
    2-7  # of domain words of this reply              bate. In other words, the more times a reply is quoted, the
    2-8  Does this reply contain other participants’ registered higher quality it may have. This motivates us to extract the
         nicknames in forum?                          quoted number of all the other replies posted by an author
             Table 1. Features for RR Classifier.     within a thread (feature 2-9) and throughout the forum
                                                      (feature 2-10).
  In our research, both structural and content features are We also take “author reputation” into account when as-
selected. In structural features, quotation maintains context sessing the quality of a reply. The motivation is that if an
coherence and indicates the relevance between the current author has a good reputation, his reply is more likely to be
reply and the quoted root message or reply, as discussed in reliable. We use the author behavior related features to as-
[Eklundh and Macdonald, 1994; Eklundh, 1998]. Two quo- sess his “reputation.” An earlier work investigates the rela-
tation features (feature 1-1 and feature 1-2) are employed in tionship between a reader’s selection of a reply and the
our classifier. Feature 1-1 indicates that the current reply author of this reply, and found that some of the features
quoting the root message is relevant to the root message. On raised from authors’ behavior over time, correlate to how
the contrary, feature 1-2 indicates the current reply might be
irrelevant to the root message because it quotes other re-
plies. We use features 1-3 and 1-4 based on the observation 2 http://dvl.dtic.mil/stop_list.html
of behaviors of posting replies in forums. The thread starter, 3 50 is the average length of 1,200 chatbot responses which
when participants reply to the starter’s thread, usually adds preferred by three chatters through sample experiments.


                                                IJCAI-07
                                                   425likely a reader is to choose to read a reply from an author human experts were hired to manually identify the relevance
[Fiore et al., 2002]. Features 2-1 to 2-7 are author behavior of the replies to the thread-title in each thread. Experts an-
related features in the forum. Feature 2-8 models how many notated each reply with one of the three labels: a) RR,b)
people have chosen to read the threads or replies of an non-RR and c) Unsure. Replies that received two or three
author in the forum by using the measurement of the influ- RR labels were regarded as RR, replies with two or three
ence of participants. This is described in detail in [Matsu- non-RR labels were regarded as non-RR. All the others were
mura et al., 2002].                                   regarded as Unsure.
                                                        After the labeling process, we found out that 1,719 replies
 1   Feature of the number of being quoted            (56.08%) were RR, 1,336 replies (43.59%) were non-RR,10
    1-1 # of quotations of this reply within the current thread (0.33%) were Unsure. We then removed 10 unsure replies
 2   Features from the author of a reply              and 60 replies with no words. We randomly selected 35
    2-1  # of threads the author starts in the forum  threads for training (including 1,954 replies) and 18 threads
    2-2  # of replies the author posts to others’ threads in the for testing (including 1,041 replies). Our baseline system
         forum                                        used the number of replies between the root message and
    2-3  The average length of the author’s replies in the fo- the responding reply [Zhou and Hovy, 2005] as the feature
         rum                                          to classify RRs.
    2-4  The longevity of participation                 Table 3 provides the performance using SVM with the
    2-5  # of the author’s threads that get no replies in the fo- feature set described in Table 1.
         rum
                                                           Feature set   Precision    Recall     F-score
    2-6  # of replies the author’s threads get in the forum
    2-7  # of threads the author is involved in the forum   Baseline      73.24%      66.86%     69.90%
    2-8  The author’s total influence in the forum         Structural     89.47%      92.29%     90.86%
    2-9  # of quotations of the replies that are posted by the
                                                            Content       71.80%      85.86%     78.20%
         author in current thread
    2-10 # of quotations of all the replies that are posted by All        90.48%      92.29%     91.38%
         the author in the forum                                    Table 3. RR Identification Result.
             Table 2. Features for RR Ranking.          With only the structural features, the precision, recall and
                                                      f-score reached 89.47%, 92.29%, and 90.86%. Content fea-
5   Experimental Results                              tures, when used alone, the precision, recall and f-score are
                                                      low. But after adding content features to structural features,
5.1 Data for Experiments                              the precision improved by 1.01% while recall stayed the
In our experiments, the Rotten Tomatoes forum4 is used as same. This indicates that content features help to improve
test data. It is one of the most popular online discussion fo- precision.
rums for movies and video games. The Rotten Tomatoes fo-
rum discussion archive is selected because each thread and Root message
its replies are posted by movie fans, amateur and profes- Title: Recommend Some Westerns For Me?
sional filmmakers, film critics, moviegoers, or movie pro- Description: And none of that John Wayne sh*t.
ducers. This makes the threads and replies more heteroge- 1. The Wild Bunch It's kickass is what it is.
neous, diverse, and informative.                       2. Once Upon a Time in the West
  For research purposes, the discussion records are col- 3. Does Dances With Wolves count as a western? Doesn't
lected by crawling the Rotten Tomatoes Forum over the     matter, I'd still recommend it.
time period from November 11, 1999 to June 15, 2005. The 4. White Comanche This masterpiece stars ……
downloaded collection contains 1,767,083 replies from  5. Here's some I'm sure nobody else ……
65,420 threads posted by 12,973 distinctive participants, so 6. for Dances with Wolves.
there are, on average, 27.0 replies per thread, 136.2 replies 7. : understands he's a minority here: ……
per participant, and 5.0 threads per participant. The number 8. Open Range is really good.  Regardless ……
of thread titles in question form is 16,306 (24.93%) and in 9. One of the best films I've ever seen.
statement form is 49,114 (75.07%). We use part of these 10. The Good the Bad and the Ugly ……
discussion records in our experiments.
                                                                     Figure 2. A Sample of RRs.
5.2 RR Identification                                   Figure 2 presents some identified RRs listed in chrono-
                                                      logical order for the root message with the title, “Recom-
To build the training and testing dataset, we randomly se-
                                                      mend Some Westerns For Me?” and description for the title,
lected and manually tagged 53 threads from the Rotten To-
                                                      “And none of that John Wayne sh*t.”.
matoes movie forum, in which the number of replies was
between 10 (min) and 125 (max). There were 3,065 replies 5.3 Extract High-quality RR
in 53 threads, i.e., 57.83 replies per thread on average. Three
                                                      To train the ranking SVM model, an annotated dataset was
                                                      required. After the non-eligible RRs were filtered out from
   4 http://www.rottentomatoes.com/vine/


                                                IJCAI-07
                                                   426the identified RRs, three annotators labeled all of the re-
                                                       95%
maining RRs with three different quality ratings. The ratings
and their descriptions are listed in Table 4.          90%
                                                       85%

    Rating                 Description                 80%
              This reply is informative and interesting, and
  Fascinating                                          75%
              it is suitable for a chatbot
  Acceptable  The reply is just so-so but tolerable    70%
                                                           1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
              This reply is bad and not suitable for a chat-
   Unsuitable
              bot                                                    Precision   Precision (Baseline)
               Table 4. RR Rating Labels.                          Figure 3. Precision at Different N.

  After the labeling process, there were 568 (71.81%) fas- Input: Recommend Some Westerns For Me?
cinating RRs, 48 (6.07%) acceptable RRs, and 175 (22.12%) Chatbot responses:
unsuitable RRs in the 791 RRs of the 35 training threads. 6.    for Dances with Wolves.
And in the 511 RRs of the 18 test threads, there were 369 11.  Young Guns! & Young Guns 2!
(72.21%) fascinating RRs, 25 (4.89%) acceptable RRs, and 2.    Once Upon a Time in the West
117 (22.90%) unsuitable RRs.                           9.    One of the best films I’ve ever seen.
  We used mean average precision (MAP) as the metric to 27.  I second the dollars trilogy and also Big Hand ……
evaluate RR ranking. MAP is defined as the mean of aver- 18. Classic Anthony Mann Westerns: The Man from Laramie
age precision over a set of queries and average precision (1955) ……
(AvgP ) for a query q  is defined as:
     i           i                                                       Figure 4. Top-6 RRs.
                M
               =         p( j)* pos( j)
          AvgPi 
                j=1 number of positive instances      6   Comparison with Related Work
where j is the rank, M is the number of instances retrieved,
pos(j) is a binary function to indicate whether the instance in Previous works have utilized different datasets for knowl-
the rank j is positive (relevant), and p(j) is the precision at edge acquisition for different applications. Shrestha and
the given cut-off rank j.                             McKeown   [2004] use an email corpus. Zhou and Hovy
  The baseline ranked the RRs of each thread by their [2005] use Internet Relay Chat and use clustering to model
chronological order. Our ranking function with the feature multiple sub-topics within a chat log. Our work is the first to
set in Table 2 achieved high performance (MAP score is explore using the online discussion forums to extract chat-
86.50%) compared with the baseline (MAP score    is   bot knowledge. Since the discussions in a forum are pre-
82.33%). We also tried content features such as the cosine sented in an organized fashion within each thread in which
similarity between an RR and the root message, and found users tend to respond to and comment on specific topics, we
that they could not help to improve the ranking perform- only need to identify the RRs for each thread. Hence, the
ance. The MAP score was reduced to 85.23% when we     clustering becomes unnecessary. Furthermore, a thread can
added the cosine similarity feature to our feature set. be viewed as <input, response> pairs, with the same struc-
                                                      ture of chat template of a chatbot, making a forum better
5.4 Chat Knowledge Extraction with Proper N Set-      suited for the chatbot knowledge extraction task.
   ting                                                 The use of thread title as input means that we must iden-
                                                      tify relevant replies to the root message (RRs), much like
The chat knowledge extraction task requires that the ex- finding adjacent pairs (APs) in [Zhou and Hovy, 2005] but
tracted RRs should have high quality and high precision. for the root message. They utilize AP to identify initiating
After we got the ranked RRs of each thread, the Top-N RRs and responding correspondence in a chat log since there are
were selected as chatbot responses. The baseline system just multiple sub-topics within a chat log, while we use RR to
selected Top-N RRs ranked in chronological order. Figure 3 identify relevant response to the thread-title. Similarly, we
shows the comparison of the performances of our approach apply an SVM classifier to identify RRs but use more effec-
and the baseline system at different settings of N.   tive structural features. Furthermore, we select high-quality
  Figure 4 shows the Top-N (N=6, N can be adjusted to get RRs with a ranking function.
proper equilibrium between quantity and quality of RRs  Xi et al. [2004] use a ranking function to select the most
when extracting chatbot knowledge) RRs after ranking the relevant messages to user queries in newsgroup searches,
RRs in Figure 2. As an instance, we uniformly extracted and in which the author feature is proved not effective. In
Top-6 high-quality RRs from each thread. Altogether 108 our work, the author feature also proves not effective in
<thread-title, reply> pairs were generated from 18 threads. identifying relevant replies but it is proved effective in se-
Among these extracted pairs, there were 97 fascinating pairs lecting high-quality RRs in RR ranking. This is because ir-
and 11 wrong pairs, which showed that 89.81% of the ex- relevant replies are removed in the first pass, making author
tracted chatbot knowledge was correct.                features more salient in the remaining RRs. This also indi-


                                                IJCAI-07
                                                   427