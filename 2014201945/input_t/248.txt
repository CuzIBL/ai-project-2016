                         A Revised Algorithm for Latent Semantic Analysis 


Xiangen Hu, ZhiqiangCai, Max Louwerse, AndrewOlney, Phanni Penumatsa, Art Graesser, and TRC 
                 Department of Psychology, The University of Memphis, Memphis, TN 38152 


                         Abstract                              a scale of-1 to 1 between the latent semantic structure of 
                                                               terms and texts [Dccrwester et al., 1990]. The input to LS A is 
       The intelligent tutoring system AutoTutor uses la•
                                                               a set of corpora segmented into documents. These documents 
       tent semantic analysis to evaluate student answers 
                                                               are typically paragraphs or sentences. Mathematical transfor•
       to the tutor's questions. By comparing a student's 
                                                               mations create a large term-document matrix from the input. 
       answer to a set of expected answers, the system 
                                                               Forcxample, if there are m terms in n documents (usually m 
       determines how much information is covered and 
                                                               and n are very large, for now, assume n m), then a matrix 
       how to continue the tutorial. Despite the success 
                                                               of. is obtained. The value of 
       of LSA in tutoring conversations, the system some•
                                                               f  is a function of the integer that represents the number of 
       times has difficulties determining at an early stage    tJ
                                                               times term 7 appears in document is a local weight•
       whether or not an expectation is covered. A new 
                                                               ing of term 7. in document j and is the global weighting 
       LSA algorithm significantly improves the precision 
                                                               for term v. Such a weighting function is used to differentially 
       of AutoTutor's natural language understanding and 
                                                               treat terms and documents to reflect knowledge that is be•
       can be applied to other natural language under•
                                                               yond the collection of the documents. This matrix of A has, 
       standing applications. 
                                                               however, lots of redundant information. Singular value de•
                                                               composition reduces this noise by linearly decomposing the 
  1 Introduction                                               matrix A into three matrices A = is m x m 
                                                               and V is square matrices, such that  
  The use of intelligent technology in education is on the rise. 
                                                                             and is m x n diagonal matrix with singu•
  Intelligent tutoring systems, once restricted to artificial in•
                                                               lar values on the diagonal. By removing dimensions corre•
  telligence labs at major universities, arc migrating to main-
                                                               sponding to small singular values and keeping the dimensions 
  stream schools [Koedinger et al, 1997]. Intelligent tutoring 
                                                               corresponding to larger singular values, the representation of 
  systems (ITS) in this environment face a difficult challenge: 
                                                               each term is further reduced to a smaller vector witli only k 
  to understand the student and manage the tutoring session in 
                                                               dimensions. The new representation for the terms (the re•
  the face of vague orungrammatical input. For most ITSs, lan•
                                                               duced U matrix) arc no longer orthogonal, but the advan•
  guage understanding and dialog management are core com•
                                                               tage of this is that only the most important dimensions that 
  ponents. Particularly in a classroom, these systems live or die 
                                                               correspond to larger singular values are kept. This method 
  by their ability to understand what the student is trying to say. 
                                                               of statistically representing knowledge has proven to be use•
  One technique, Latent Semantic Analysis has been success•
                                                               ful in a range of studies. For instance, studies have shown 
  fully developed for such purposes [Landauer et ai, 1998b; 
                                                               that LSA performs as well as students on TOEFL (test of 
  Landauer and Dumais, 1997; Foltz et ai, 1998; Graesser et 
                                                               English as a foreign language) tests lLandauer and Dumais, 
  ai, 2002]. LSA, a statistical technique utilizing unsupervised 
                                                               1997], that it grades essays as reliably as humans [Landauer 
  learning, is both highly portable to other domains and adept 
                                                               ct al, 1998a] and that it reliably measures the coherence 
  at recognizing vague or incomplete input. 
                                                               between a sentence and successive sentences [Foltz ct ah, 
    This paper outlines some problems inherent in the tradi•
                                                               1998]. Finally, LSA has successfully been used in intelli•
  tional LSA algorithm and solutions to this problem by using 
                                                               gent tutoring systems like AutoTutor [Gracsser ct ai, 2002; 
  a different LSA algorithm. Not only does this new algorithm 
                                                               1999]. 
  increase the precision of ITS language understanding, but it 
  also offers a new perspective on a commonly used technique   1.2 AutolWor 
  in cognitive science, computational linguistics and artificial 
  intelligence. 
                                                               AutoTutor is a conversational agent that assists students in 
  1.1 Latent Semantic Analysis                                 actively constructing knowledge by holding a conversation in 
  Latent Semantic Analysis is a statistical, corpus-based lan• natural language with them [Graesser et ai, 2001], In ad•
  guage understanding technique that estimates similarities on dition to latent semantic analysis, at least four other com-


  POSTER PAPERS                                                                                                    1489  ponents can be distinguished: 1) a dialog management sys•                         Type of feedback       relevant     irrelevant 
 tem guides the student through the tutor-student conversa•                         New                   + +          -
 tion. Fuzzy production rules and a Dialog Advancer Network                         Old                   + 
 form the basis of these conversational strategics; 2) curricu•
 lum scripts organize the pedagogical macrostructure of the              Table 1: Four types of feedback the system provides on the 
 tutorial. These scripts keep track of the topic coverage and            basis of relevance and newness of student contribution. 
 follow up on any problems the student might have; 3) a talk•
 ing head with facial expressions and synthesized speech is                 • Question: Suppose a runner is running in a straight 
 used for the interface. Parameters of the facial expressions                  line at constant speed and throws a pumpkin straight up. 
 and rudimentary gestures are generated by fuzzy production                    Where will the pumpkin land? Explain why. 
 rules; 4) mixed-initiative dialog, including the appropriate use           • Expectation: The pumpkin will land in the runner's 
 of discourse markers to make the conversation smoother; a                     hands. 
 speech act classifier that accounts for the pragmatics of in•
 coming expressions; and a question answering tool that dy•                 • Student contributions: 
 namically answers student questions on a variety of topics.                    (1)1 think, correct me if I am wrong, it will not land 
    Auto Tutor was originally developed for computer literacy.                      before or behind the man. 
 Over the last two years a web version of AutoTutor was de•                     (2) The reason is clear, they have the same horizontal 
 veloped that tutors in conceptual physics. In both domains                         speed. 
 world knowledge is provided by LSA spaces of domain spe•
                                                                                (3) The pumpkin will land in the runner's hand. 
 cific text books. 
                                                                                (4) Did I say anything wrong? 
 2 An improved LSA algorithm                                                    (5) Come on, I thought I have said that. 
                                                                                    Old         New           New                  New 
 At the beginning of a session, AutoTutor presents a question 
                                                                                    LSA         Infor.       contribution          LSA 
 to the student, and the student responds to this question. Au•
                                                                                   0.431       100%         0.431                0.431 
 toTutor analyzes the accuracy of this answer by comparing                  (1) 
                                                                            (2)   0.430        99%          0.175                0.466 
 the student's answer with a series of expected ideal answers. 
                                                                                   0.751       88%          0.885                1.0 
 Over the course of the tutoring session, the student covers                (3) 
 each of the expectations. The tutor allows the student to move             (4)    0.713       98%          0.000                1.0 
 to the next problem only once all expectations arc covered.              (5)      0.667       97%          0.000                1.0 
   Although possible in theory, a single contribution from a 
                                                                         Table 2: Example of student contribution and evaluation 
 student usually does not cover all expectations at once. In•
                                                                         based on two LSA methods 
 stead, the student simply types one sentence at a time in the 
 conversation with the tutor. Based on the student's responses, 
 the tutor will then provide appropriate feedback based on the              In earlier versions of the system, AutoTutor put all the stu•
 quality of responses. To provide feedback to a student's con•           dent contributions (from the first response to the most recent 
 tributions, the tutoring system needs to know the following:            response) together as one document and would then compare 
   1. information related to the expected answer elements that           with the expectation. One of the reasons for this was that 
      is 1) new (what was not in the previous contributions);            it has often been claimed that the best performance in LSA 
      2) old (what was in the previous contributions)                    comes from paragraphs rather than sentences (see [Foltz et 
                                                                         al., 1998]). However, simple vector algebra shows that vec•
   2. information not related to the expected answer elements            tor summation of term-vectors for the combined contributions 
      that is 1) new (what was not in the previous contribu•             may in fact reduce the similarity between the expectation and 
      tions); 2) old (what was in the previous contributions)            contributions when contributions are added. This reduction 
   Depending on these four components, AutoTutor chooses                 is evident in the example given in second column of Table 2. 
 the most appropriate feedback. This mechanism for student               The tutor asks the student a question at the start of a concep•
 contributions is illustrated in Table 1. For example, Auto•             tual physics problem. The student's answer is matched with 
 Tutor needs to provide highly positive feedback when stu•               an ideal expected answer. The question now is what happens 
 dents provide new relevant information (cell labeled ++). For           to the LSA coverage scores if the student submits (new/old) 
 relevant but repeated information (cell labeled +), AutoTutor           (relevant/irrelevant) multiple contributions. 
 needs to provide only some non-negative feedback. For irrel•               From the expected answer we know that a student's answer 
 evant contributions, AutoTutor needs to point out the repeated          like (1) is almost correct. Now imagine that the student an•
 misconceptions, eventually with negative feedback (cell with            swers (2). The cosine match drops, resulting in AutoTutor 
—). The system returns non-positive feedback in cases of                 asking for more information. Now assume that the student 
 irrelevant information occurring the first time (cell with -).          also answers (3), which is the exact ideal answer. Using a 
   The challenge for AutoTutor is to obtain information that             traditional vector addition algorithm, the similarity is not 1.0. 
 belongs to each of the cells in the table. That is, the system          This loss of precision results from the noise introduced by the 
 needs to take into account both the relevance of the informa•           irrelevant information in the student's answers. By adding 
 tion and whether or not the information is new.                         the contributions' vectors, the system cannot distinguish be-


 1490                                                                                                                   POSTER PAPERS tween the different parts (new/old) (relevant/irrelevant) of the 3 Conclusion 
student's contributions. So although LSA effectively com•
                                                               This paper addressed the use of latent semantic analysis in 
pares semantic similarities between two large documents, 
                                                               intelligent tutoring systems like AutoTutor. Despite the suc•
LSA lacks precision for comparing smaller documents in a 
                                                               cess of LSA in AutoTutor, previous versions were not able to 
progressive sequence. Under a vector-addition model, a stu•
                                                               differentiate between relevant/irrelevant or new/old informa•
dent whose answers improve dramatically over the course of 
                                                               tion in student contributions. Replacing the vector-addition 
the tutoring session is "penalized" for an initial bad answer. 
                                                               based algorithm with a span-based algorithm does not only 
To solve this limitation, we propose an alternative solution. 
                                                               improve AutoTutor's evaluation of student contributions, but 
Instead of simply combining contributions into a larger docu•  is most likely to improve LSA performance in a wide range 
ment, each contribution is treated as "independent" in a vec•  of other natural language understanding applications. 
tor subspace. The combination of the contributions is then 
not represented as a simple vector summation, but instead as   References 
a span in the subspace. This way, the vector for any new 
contribution can be algebraically decomposed into two com•     iDeerwester et al., 1990] S. Dccrwester, S. T. Dumais, G. W. 
ponents. One component is the projection of the vector to the     Furnas, T. K. Landauer, and R. Harshman. Indexing by 
spanned subspace of the previous contributions, and the other     latent semantic analysis. Journal of the American Society 
component is perpendicular to the subspace. These two com•        For Information, pages 391-407, 1990. 
ponents of the most recent contribution correspond to relevant [Foltz et a/., 1998] P. W. Foltz, W. Kintsch, and T. K. Lan•
information (projection to the subspace) and new information      dauer The measurement of textual coherence with latent 
(the perpendicular component). Finally, the cosine match of       semantic analysis. Discourse Processes, pages 285-307, 
the new information with the expectation is the measure of        1998. 
new (additional) coverage of the expectations. We applied 
this method to the example such as Table 2 and observed        [Graesscr et al, 1999] A. C. Gracsscr, K. Wiemer-Hastings, 
desirable increase in LSA's precision, as illustrated in col•     P. Wiemer-Hastings, R. Kreuz, and the Tutoring Re•
umn 3,4, and 5 of Table 2. The rows of Table 2 present the        search Group. AutoTutor: A simulation of a human tutor. 
five student contributions. The column 'New Info' gives the      Journal of Cognitive Systems Research, 1:35—51, 1999. 
percentage of new information for each of the contributions,   [Gracsser et al., 2001] A.C. Gracsscr, N. Person, D. Harter, 
compared to the previous contributions. The third column is       and TRG. Teaching tactics and dialog in AutoTutor. In-
the relevance of the new contribution to the span. The final      ternational Journal of Artificial Intelligence in Education, 
two columns give the old and new LSA scores.                      12:257-279,2001. 
                                                               [Gracsscr et al, 2002] A. C. Graesser, X. Hu, B. A. Olde, 
   Although contribution (3) is identical to the expectation,     M. Ventura, A. Olney, M. Louwerse, D. R. Franc esc hetti, 
it still is only 88% new. The reason is that contributions (1)    and N. Person. Implementing latent semantic analysis in 
and (2) already contain some of the information from (3). For     learning environments with conversational agents and tu•
example, although (2) contains 99% new information, it has        torial dialog. In Proceedings of the 24th Annual Meeting 
only marginally (0.175) contributed as coverage. Notice that      of the Cognitive Science Society, page 37. Mahwah, NJ: 
the quality in the subsequent student contributions does not      Erlbaum, 2002 
deteriorate, but the old LSA values do. The new LSA values, 
on the other hand, account for additional relevant informa•    [Koedinger et al., 19971 K.R. Koedinger, J.R. Anderson, 
tion, even bringing the coverage score to the maximum value       W.H.. Hadley, and M.A. Mark. Intelligent tutoring goes 
of 1.                                                             to school in the big city. International Journal of Artificial 
                                                                  Intelligence in Education, 8:30-43, 1997. 
   The method provided here can be used to compute all four    [Landauer and Dumais, 1997] T. K. Landauer and S. T Du•
cells in Table 1, because it differentiates whether the informa•  mais. A solution to plato's problem: The latent semantic 
tion is new or old, and whether it is relevant or not. Further•   analysis theory of the acquisition, induction, and represen•
more, since it provides information at every step, numerical      tation of knowledge. Psychological Review, pages 211 
information of the values can be used to provide secondary        240, 1997. 
information for feedback. For example, the rate of increase    [Landauer et ai, 1998a] T. K. Landauer, P. W. Foltz, and 
in the new LSA algorithm provides us with information on          D. Laham. Introduction to latent semantic analysis. Dis•
the development student performance on a step-by-stcp ba•         course Processes, pages 259-284, 1998. 
sis. By being able to localize LSA scores, AutoTutor can 
now determine the effectiveness of its dialog moves.           lLandauer et al., 1998b] T. K. Landauer, D. Laham, and 
                                                                  P. W. Foltz. Learning human-like knowledge by singu•
                                                                  lar value decomposition: A progress report. In M. J. 
  The proposed new algorithm can potentially be used in ap•
                                                                  M. 1. Jordan, Kearns, and S. A. Solla, editors, Advances 
plications like essay grading, where the student's composition 
                                                                  in Neural Information Processing Systems, pages 45-51. 
covers the key elements for a given essay. The algorithm can 
                                                                  Cambridge: MIT Press, 1998. 
measure development of student performance and can take 
into account whether information is old or new, relevant or 
irrelevant. 


POSTER PAPERS                                                                                                        1491 