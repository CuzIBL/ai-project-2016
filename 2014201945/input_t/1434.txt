                     Learning and Inference over Constrained Output

              Vasin Punyakanok           Dan Roth         Wen-tau Yih          Dav Zimak
                                    Department of Computer Science
                              University of Illinois at Urbana-Champaign
                       {punyakan, danr, yih, davzimak}@uiuc.edu


                    Abstract                          tional random ﬁelds [Lafferty et al., 2001], Perceptron-based
                                                      learning of structured output [Collins, 2002; Carreras and
    We study learning structured output in a discrimi- M`arquez, 2003] and Max-Margin Markov networks which
    native framework where values of the output vari- allow incorporating Markovian assumptions among output
    ables are estimated by local classiﬁers. In this  variables [Taskar et al., 2004].
    framework, complex dependencies among the out-
                                                        Incorporating constraints during training can lead to solu-
    put variables are captured by constraints and dictate
                                                      tions that directly optimize the true objective function, and
    which global labels can be inferred. We compare
                                                      hence, should perform better. Nonetheless, most real world
    two strategies, learning independent classiﬁers and
                                                      applications using this technique do not show signiﬁcant ad-
    inference based training, by observing their behav-
                                                      vantages, if any. Therefore, it is important to discover the
    iors in different conditions. Experiments and theo-
                                                      tradeoffs of using each of the above schemes.
    retical justiﬁcation lead to the conclusion that using
    inference based learning is superior when the lo-   In this paper, we compare three learning schemes. In
    cal classiﬁers are difﬁcult to learn but may require the ﬁrst, classiﬁers are learned independently (learning only
    many examples before any discernible difference   (LO)), in the second, inference is used to maintain struc-
    can be observed.                                  tural consistency only after learning (learning plus inference
                                                      (L+I)), and ﬁnally inference is used while learning the pa-
                                                      rameters of the classiﬁer (inference based training (IBT)). In
1  Introduction                                       semantic role labeling (SRL), it was observed [Punyakanok
Making decisions in real world problems involves assigning et al., 2004; Carreras and M`arquez, 2003] that when the lo-
values to sets of variables where a complex and expressive cal classiﬁcation problems are easy to learn, L+I outperforms
structure can inﬂuence, or even dictate, what assignments are IBT. However, when using a reduced feature space where the
possible. For example, in the task of identifying named enti- problem was no longer (locally) separable, IBT could over-
ties in a sentence, prediction is governed by constraints like come the poor local classiﬁcations to yield accurate global
“entities do not overlap.” Another example exists in scene in- classiﬁcations.
terpretation tasks where predictions must respect constraints Section 2 provides the formal deﬁnition of our problem.
that could arise from the nature of the data or task, such as For example, in Section 3, we compare the three learning
“humans have two arms, two legs, and one head.”       schemes using the online Perceptron algorithm applied in the
  There exist at least three fundamentally different solutions three settings (see [Collins, 2002] for details). All three set-
to learning classiﬁers over structured output. In the ﬁrst, tings use the same linear representation, and L+I and IBT
structure is ignored; local classiﬁers are learned and used share the same decision function space. Our conjectures of
to predict each output component separately. In the sec- the relative performance between different schemes are pre-
ond, learning is decoupled from the task of maintaining struc- sented in Section 4. Despite the fact that IBT is a more pow-
tured output. Estimators are used to produce global out- erful technique, in Section 5, we provide an experiment that
put consistent with the structural constraints only after they shows how L+I can outperform IBT when there exist accu-
are learned for each output variable separately. Discrimina- rate local classiﬁers that do not depend on structure, or when
tive HMM, conditional models [Punyakanok and Roth, 2001; there are too few examples to learn complex structural depen-
McCallum  et al., 2000] and many dynamic programming  dencies. This is also theoretically justiﬁed in Section 6.
based schemes used in the context of sequential predictions
fall into the this category. The third class of solutions in- 2 Background
corporates dependencies among the variables into the learn-
ing process to directly induce estimators that optimize a Structured output classiﬁcation problems have many ﬂavors.
global performance measure. Traditionally these solutions In this paper, we focus on problems where it is natural both
were generative; however recent developments have pro- to split the task into many smaller classiﬁcation tasks and to
duced discriminative models of this type, including condi- solve directly as a single task. In Section 5.2, we consider thesemantic role-labeling problem, where the input X are nat- 3 Learning
ural language features and the output Y is the position and We present several ways to learn the scoring function pa-
type of a semantic-role in the sentence. For this problem, rameters differing in whether or not the structure-based in-
one can either learn a set of local functions such as “is this ference process is leveraged during training. Learning con-
phrase an argument of ’run’,” or a global classiﬁer to pre-
                                                      sists of choosing a function h : X ∗ → Y∗ from some hy-
dict all semantic-roles at once. In addition, natural structural
                                                      pothesis space, H. Typically, the data is supplied as a set
constraints dictate, for example, that no two semantic roles
                                                      D  =  {(x , y ), . . . , (x , y )} from a distribution P
for a single verb can overlap. Other structural constraints, as 1 1        m   m                      X ,Y
                                                      over X ∗ × Y∗. While these concepts are very general, we fo-
well as linguistic constraints yield a restricted output space in cus on online learning of linear representations using a variant
which the classiﬁers operate.                         of the Perceptron algorithm (see [Collins, 2002]).
  In general, given an assignment x  nx to a collec-
                                 ∈  X                   Learning Local Classiﬁers: Learning stand-alone local
tion of input variables, X = (X , . . . , X ), the struc-
                              1       nx              classiﬁers is perhaps the most straightforward setting. No
tured classiﬁcation problem involves identifying the “best”
                                                      knowledge of the inference procedure is used. Rather, for
assignment y ∈  Yny to a collection of output variables
                                                      each example (x, y) ∈ D, the learning algorithm must en-
Y = (Y  , . . . , Yn ) that are consistent with a deﬁned struc-
       1       y                                      sure that f (x, t) > f 0 (x, t) for all t = 1, . . . , n and all
ture on Y. This structure can be thought of as constraining    yt         y                      y
                                                      y0 6= y . In Figure 3(a), an online Perceptron-style algorithm
the output space to a smaller space C(Yny ) ⊆ Yny , where   t
     ∗      ∗                                                                                       [
    Y     Y                                           is presented where no global constraints are used. See Har-
C : 2  → 2   constrains the output space to be structurally Peled et al., 2003] for details and Section 5 for experiments.
consistent.                                             Learning Global Classiﬁers: We seek to train classiﬁers
  In this paper, a structured output classiﬁer is a function so they will produce the correct global classiﬁcation. To this
      nx       ny
h :  X    →   Y  , that uses a global scoring function, end, the key difference from learning locally is that feed-
     nx    ny
f : X   × Y   →  IR to assign scores to each possible ex- back from the inference process determines which classiﬁers
ample/label pair. Given input x, it is hoped that the correct to modify so that together, the classiﬁers and the inference
output y achieves the highest score among consistent outputs: procedure yield the desired result. As in [Collins, 2002;
            yˆ = h(x) = argmax f(x, y0),        (1)   Carreras and M`arquez, 2003], we train according to a global
                       y0∈C(Yny )                     criterion. The algorithm presented here is an online proce-
                                                      dure, where at each step a subset of the classiﬁers are up-
where nx and ny depend on the example at hand. In addition, dated according to inference feedback. See Figure 3(b) for
we view the global scoring function as a composition of a set details of a Perceptron-like algorithm for learning with infer-
                                              nx
of local scoring functions {fy(x, t)}y∈Y , where fy : X × ence feedback.
{1, . . . , ny} → IR. Each function represents the score or Note that in practice it is common for problems to be mod-
conﬁdence that output variable Yt takes value y:      eled in such a way that local classiﬁers are dependent on part
                              ny                      of the output as part of their input. This sort of interaction can
                                                      be incorporated directly to the algorithm for learning a global
          f(x, (y1, . . . , yny )) = fyt (x, t)
                             Xt=1                     classiﬁer as long as an appropriate inference process is used.
                                                      In addition, to provide a fair comparison between LO, L+I,
  Inference is the task of determining an optimal assign- and IBP in this setting one must take care to ensure that the
ment y given an assignment x. For sequential structure learning algorithms are appropriate for this task. In order to
of constraints, polynomial-time algorithms such as Viterbi remain focused on the problem of training with and without
or CSCL [Punyakanok and Roth, 2001] are typically used inference feedback, the experiments and analysis presented
for efﬁcient inference. For general structure of constraints, concern only the local classiﬁers without interaction.
a generic search method (e.g., beam search) may be ap-
plied. Recently, integer programming has also been shown
to be an effective inference approach in several NLP applica- 4 Conjectures
tions [Roth and Yih, 2004; Punyakanok et al., 2004].  In this section, we investigate the relative performance of
  In this paper, we consider classiﬁers with linear rep- classiﬁer systems learned with and without inference feed-
resentation. Linear local classiﬁers are linear functions, back. There are many competing factors. Initially, if the lo-
            y    y             y      dy
fy(x, t) = α  · Φ (x, t), where α ∈ IR   is a weight  cal classiﬁcation problems are “easy”, then it is likely that
vector and Φy(x, t) ∈ IRdy is a feature vector. Then, learning local classiﬁers only (LO) can yield the most accu-
it is easy to show that the global scoring function can rate classiﬁers. However, an accurate model of the structural
be written in the familiar form f(x, y) = α · Φ(x, y), constraints could additionally increase performance (learning
                   n
       y            y   yt                            plus inference (L+I)). As the local problems become more
where Φ (x, y) =   t=1 Φ (x, t)I{yt=y} is an accumula-
tion over all outputP variables of features occurring for class difﬁcult to learn, an accurate model of the structure becomes
y, α =  (α1, . . . , α|Y|) is concatenation of the αy’s, and more important, and can, perhaps, overcome sub-optimal lo-
Φ(x, y) = (Φ1(x, y), . . . , Φ|Y|(x, y)) is the concatenation cal classiﬁers. Despite the existence of a global solution, as
of the Φy(x, y)’s. Then, the global classiﬁer is      the local classiﬁcation problems become increasingly difﬁ-
                                                      cult, it is unlikely that structure based inference can ﬁx poor
          h(x) = yˆ = argmax α · Φ(x, y0).            classiﬁers learned locally. In this case, only training with in-
                     y0∈C(Yny )                       ference feedback (IBT) can be expected to perform well.                                                      Claim 4.1 With a ﬁxed number of examples:
         Algorithm ONLINELOCALLEARNING
                     X,Y     ∗    ∗ m                   1. If the local classiﬁcation tasks are separable, then L+I
             INPUT: D    ∈ {X × Y  }                      outperforms IBT.
             OUTPUT: {fy}   ∈ H
                         y∈Y                            2. If the task is globally separable, but not locally sepa-
                            y
             Initialize αy ∈ IR|Φ | for y ∈ Y             rable then IBT outperforms L+I only with sufﬁcient ex-
             Repeat until converge                        amples. This number correlates with the degree of the
               for each (x, y) ∈ DX,Y do                  separability of the local classiﬁers.
                 for t = 1, . . . , ny do
                              αy    y
                   yˆt = argmaxy · Φ (x, t)           5   Experiments
                   if yˆt 6= yt then
                      y     y    y                    We present experiments to show how the relative performance
                     α t = α t + Φ t (x, t)
                      y     y    y                    of learning plus inference (L+I) compares to inference based
                     αˆt = αˆt − Φˆt (x, t)
                                                      training (IBT) when the quality of the local classiﬁers and
                                                      amount of training data varies.
             (a) Without inference feedback
                                                      5.1  Synthetic Data
                                                      In our experiment, each example x is a set of c points in d-
        Algorithm ONLINEGLOBALLEARNING                                                                d
                   X,Y     ∗    ∗ m                   dimensional real space, where x = (x1, x2, . . . , xc) ∈ IR ×
            INPUT: D   ∈ {X  × Y }                           d
            OUTPUT: {fy}   ∈ H                        . . . × IR and its label is a sequence of binary variable, y =
                       y∈Y                                              c
                                                      (y1, . . . , yc) ∈ {0, 1} , labeled according to:
            Initialize α ∈ IR|Φ|
            Repeat until converge
                             X,Y                          y = h(x) = argmax    yifi(xi) − (1 − yi)fi(xi),
             for each (x, y) ∈ D do                                       c
                                                                     y∈C(Y ) Xi
               yˆ = argmax    ny α · Φ(x, y)
                         y∈C(Y  )                              c                   c
               if yˆ 6= y then                        where C(Y ) is a subset of {0, 1} imposing a random con-
                                                           1
                 α = α + Φ(x, y) − Φ(x, yˆ)           straint on y, and fi(xi) = wixi +θi. Each fi corresponds to

                                                      a local classiﬁer yi = gi(xi) = Ifi(xi)>0. Clearly, the dataset
              (b) With inference feedback             generated from this hypothesis is globally linearly separable.
                                                      To vary the difﬁculty of local classiﬁcation, we generate ex-
                                                      amples with various degree of linear separability of the lo-
Figure 1: Algorithms for learning without and with infer- cal classiﬁers by controlling the fraction κ of the data where
ence feedback. The key difference lies in the inference step h(x) 6= g(x) = (g1(x1), . . . , gc(xc))—examples whose la-
(i.e. argmax). Inference while learning locally is trivial bels, if generated by local classiﬁers independently, violate
and the prediction is made simply by considering each la- the constraints (i.e. g(x) ∈/ C(Yc)).
bel locally. Learning globally uses a global inference (i.e. Figure 2 compares the performance of different learning
argmaxy∈C(Yny )) to predict global labels.            strategies relative to the number of training examples used.
                                                      In all experiments, c = 5, the true hypothesis is picked at ran-
                                                      dom, and C(Yc) is a random subset with half of the size of
  As a ﬁrst attempt to formalize the difﬁculty of classiﬁca- Yc. Training is halted when a cycle complete with no errors,
tion tasks, we deﬁne separability and learnability. A classi- or 100 cycles is reached. The performance is averaged over
ﬁer, f ∈ H, globally separates a data set D iff for all exam- 10 trials. Figure 2(a) shows the locally linearly separable case
                              0         0    n
ples (x, y) ∈ D, f(x, y) > f(x, y ) for all y ∈ Y y \ y where L+I outperforms IBT. Figure 2(c) shows results for the
and locally separates D iff for all examples (x, y) ∈ D, case with the most difﬁcult local classiﬁcation tasks(κ = 1)
                                        0    ny
fyt (x, t) > fy(x, t) for all y ∈ Y \ yt, and all y ∈ Y \ y. where IBT outperforms L+I. Figure 2(b) shows the case
A learning algorithm A is a function from data sets to a H. where data is not totally locally linearly separable(κ = 0.1).
We say that D is globally (locally) learnable by A if there In this case, L+I outperforms IBT when the number of train-
exists an f ∈ H such that f globally (locally) separates D. ing examples is small. In all cases, inference helps.
  The following simple relationships exist between local and
global learning: 1. local separability implies global separa- 5.2 Real-World Data
bility, but the inverse is not true; 2. local separability implies In this section, we present experiments on two real-world
local and global learnability; 3. global separability implies problems from natural language processing – semantic role
global learnability, but not local learnability. As a result, it is labeling and noun phrase identiﬁcation.
clear that if there exist learning algorithms to learn global sep-
arations, then given enough examples, IBT will outperform Semantic-Role Labeling
L+I. However, learning examples are often limited either be- Semantic role labeling (SRL) is believed to be an important
cause they are expensive to label or because some learning task toward natural language understanding, and has imme-
algorithms simply do not scale well to many examples. With diate applications in tasks such Information Extraction and
a ﬁxed number of examples, L+I can outperform IBT.
                                                         1Among the total 2c possible output labels, C(·) ﬁxes a random
                                                      fraction as legitimate global labels.           1                                             80

        0.95                                             75

         0.9
                                                         70
        0.85


        Accuracy                                         65
         0.8                                            F1
                                     LO
        0.75                         L+I                 60
                                     IBT
         0.7
           0    2000  4000 6000  8000  10000             55                                   LO
                   # Training examples
                                                                                              L+I
                                                                                              IBT
                                                         50
                  (a) κ = 0, d = 100                        3        4         5         6        7
                                                          10        10       10        10       10
                                                                   Separability (# Features)

        0.85                                          Figure 3: Results on the semantic-role labeling (SRL) problem. As
                                                      the number of features increases, the difﬁculty of the local classiﬁ-
                                                      cation problem becomes easier, and the independently learned clas-
         0.8                                          siﬁers (LO) perform well, especially when inference is used after
                                                      learning (L+I). Using inference during training (IBT) can aid perfor-
        0.75                                          mance when the learning problem is more difﬁcult (few features).
        Accuracy

                                     LO
         0.7                         L+I                  [AM-LOC in my will].
                                     IBT              Here A0 represents leaver, A1 represents thing left, A2 rep-
          1000   2000    3000   4000    5000          resents benefactor, AM-LOC is an adjunct indicating the lo-
                   # Training examples                cation of the action, and V determines the verb.
                                                        We model the problem  using classiﬁers that map con-
                 (b) κ  . , d
                     = 0 15  = 300                    stituent candidates to one of 45 different types, such as
                                                      fAO  and fA1 [Kingsbury and Palmer, 2002; Carreras and
          1                                           M`arquez, 2003]. However, local multiclass decisions are in-
                                                      sufﬁcient. Structural constraints are necessary to ensure, for
         0.9                                          example, that no arguments can overlap or embed each other.
                                                      In order to include both structural and linguistic constraints,
                                                      we use a general inference procedure based on integer linear
         0.8                                          programming [Punyakanok et al., 2004]. We use data pro-


        Accuracy                                      vided in the CoNLL-2004 shared task [Carreras and M`arquez,
                                                          ]
         0.7                         LO               2003 , but we restrict our focus to sentences that have greater
                                     L+I              than ﬁve arguments. In addition, to simplify the problem, we
                                     IBT              assume the boundaries of the constituents are given – the task
         0.6
           0    2000 4000  6000  8000  10000          is mainly to assign the argument types.
                   # Training examples                  The experiments clearly show that IBT outperforms lo-
                                                      cally learned LO and L+I when the local classiﬁers are in-
                  (c) κ = 1, d = 100                  separable and difﬁcult to learn. The difﬁculty of local learn-
                                                      ing was controlled by varying the number of input features.
Figure 2: Comparison of different learning strategies in various With more features, the linear classiﬁer are more expressive
degrees of difﬁculties of the local classiﬁers. κ = 0 implies locally and can learn effectively and L+I outperforms IBT. With less
linearly separability. Higher κ indicates harder local classiﬁcation. features the problem becomes more difﬁcult and IBT outper-
                                                      forms L+I. See Figure 3.
Question Answering. The goal is to identify, for each verb in Noun Phrase Labeling
the sentence, all the constituents which ﬁll a semantic role, Noun phrase identiﬁcation involves the identiﬁcation of
and determine their argument types, such as Agent, Patient, phrases or of words that participate in a syntactic relation-
Instrument, as well as adjuncts such as Locative, Temporal, ship. Speciﬁcally, we use the standard base Noun Phrases
Manner, etc. For example, given a sentence “ I left my pearls (NP) data set [Ramshaw and Marcus, 1995] taken from the
to my daughter-in-law in my will”, the goal is to identify dif- Wall Street Journal corpus in the Penn Treebank [Marcus et
ferent arguments of the verb left which yields the output: al., 1993].
    [A0 I] [V left ] [A1 my pearls] [A2 to my daughter-in-law] The phrase identiﬁer consists of two classiﬁers: one that   100                                                  We begin by deﬁning the growth function to measure the
                                                      effective size of the hypothesis space.

    80                                                Deﬁnition 6.1 (Growth Function) For a given hypothesis
                                                      class H consisting of functions h : X → Y, the growth func-
                                                      tion, NH(m), counts the maximum number of ways to label
    60                                                any data set of size m:


  F1                                                    NH(m) =      sup    |{(h(x1), . . . , h(xm)) |h ∈ H}|
                                                                           m
    40                                                           x1,...,xm∈X
                                                        The well-known VC-style generalization bound expresses
    20                                                expected error, , of the best hypothesis, hopt on unseen data.
                                                      In the following theorem adapted from [Anthony and Bartlett,
                                         L+I
                                         IBT          1999][Theorem 4.2], we directly write the growth function
     0
      0            2           4           6          into the bound,
    10           10          10          10
              Separability (# Features)               Theorem 6.2 Suppose that H is a set of functions from a set
                                                      X  to a set Y with growth function NH(m). Let hopt ∈ H
Figure 4: Results on the noun phrase (NP) identiﬁcation prob- be the hypothesis that minimizes sample error on a sample of
lem.                                                  size m drawn from an unknown, but ﬁxed probability distri-
                                                      bution. Then, with probability 1 − δ

detects the beginning, f[, and a second that detects the end,           32(log(N  (2m)) + log(4/δ))
                                                                                H                     (2)
f] of a phrase. The outcome of these classiﬁers are then com-  ≤ opt + r                        .
bined in a way that satisﬁes structural constraints constraints                     m
(e.g. non-overlapping), using an efﬁcient constraint satisfac- For simplicity, we ﬁrst describe the setting in which a sep-
tion mechanism that makes use of the conﬁdence in the clas- arate function is learned for each of a ﬁxed number, c, of
siﬁers’ outcomes [Punyakanok and Roth, 2001].         output variables (as in Section 5.1). Here, each example has
                                                                                             d          d
  In this case, L+I trains each classiﬁer independently, and c components in input x = (x1, . . . , xc) ∈ IR × . . . × IR
                                                                                     c
only during evaluation, the inference is used. On the other and output y = (y1, . . . , yc) ∈ {0, 1} .
hand, IBT incorporates the inference into the training. For Given a dataset D, the aim is to learn a set of linear scor-
                                                                                                d
each sentence, each word position is processed by the clas- ing functions fi(xi) = wixi, where wi ∈ IR for each
siﬁers, and their outcomes are used by the inference process i = 1, . . . , c. For LO and L+I, the setting is simple: ﬁnd
to infer the ﬁnal prediction. The classiﬁers are then updated a set of weight vectors that, for each component, satisfy
based on the ﬁnal prediction not on their own prediction be- yiwixi > 0 for all examples (x, y) ∈ D. For IBT, we ﬁnd
fore the inference.                                                                           0
                                                      a set of classiﬁers such that i yiwixi > i yiwixi for all
                                                       0                                0      c
  As in the previous experiment, Figure 4 shows perfor- y 6= y (and that satisfy the constraints,P y ∈PC(Y )).
mance of two systems varied by the number of features. Un- As previously noted, when learning local classiﬁers inde-
like the previous experiment, the number of features in each pendently (LO and L+I), one can only guarantee convergence
experiment was determined by the frequency of occurrence. when each local problem is separable – however, it is often
Less frequent features are pruned to make the task more difﬁ- the case that global constraints render these problems insepa-
cult. The results are similar to the SRL task in that only when rable. Therefore, there is a lower bound, opt, on the optimal
the problem becomes difﬁcult IBT outperforms L+I.     error achievable. Since each component is a separate learning
                                                      problem, the generalization error is thus
6  Bound Prediction                                   Corollary 6.3 When H is the set of separating hyperplanes
                                                          d
In this section, we use standard VC-style generalization in IR ,
bounds from learning theory to gain intuition into when learn-
                                                                        32(d log((em/d)) + log(4/δ))
ing locally (LO and L+I) may outperform learning globally    ≤ opt +                            .   (3)
(IBT) by comparing the expressivity and complexity of each            r             m
hypothesis space. When learning globally, it is possible to
                                                      Proof sketch: We show that N (m) ≤ (em/d)d  when H
learn concepts that may be difﬁcult to learn locally, since the                  H
                                                      is the class of threshold linear functions in d dimensions.
global constraints are not available to the local algorithms.
                                                      NH(m)   is precisely the maximum number of continuous
On the other hand, while the global hypothesis space is more                                   d
expressive, it has a substantially larger representation. here regions an arrangement of m halfspaces in IR , which is
                                                          d   m−1                  d
we develop two bounds—both for linear classiﬁers on a re- 2 i=1 i  ≤  2(e(m − 1)/d) . For m >  1, the result
stricted problem. The ﬁrst upper bounds the generalization holds.P See  [Anthony and Bartlett, 1999][Theorem 3.1] for
error for learning locally by assuming various degrees of sep- details.
arability. The second provides an improved generalization On the other hand, when learning collectively with IBT,
bound for globally learned classiﬁers by assuming separabil- examples consist of the full vector x ∈ IRcd. In this set-
ity in the more expressive global hypothesis space.   ting, convergence is guaranteed (if, of course, such a function