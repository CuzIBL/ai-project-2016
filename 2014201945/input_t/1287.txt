             Semantic annotation of unstructured and ungrammatical text
                             Matthew Michelson and Craig A. Knoblock
                                    University of Southern California
                                     Information Sciences Institute,
                                          4676 Admiralty Way
                                    Marina del Rey, CA 90292 USA
                                     {michelso,knoblock}@isi.edu

                    Abstract
    There are vast amounts of free text on the inter-
    net that are neither grammatical nor formally struc-
    tured, such as item descriptions on Ebay or internet
    classiﬁeds like Craig’s list. These sources of data,
    called “posts,” are full of useful information for
    agents scouring the Semantic Web, but they lack the
    semantic annotation to make them searchable. An-
    notating these posts is difﬁcult since the text gen-
    erally exhibits little formal grammar and the struc-
    ture of the posts varies. However, by leveraging
    collections of known entities and their common at-
    tributes, called “reference sets,” we can annotate
    these posts despite their lack of grammar and struc-
    ture. To use this reference data, we align a post
    to a member of the reference set, and then exploit
    this matched member during information extrac-
    tion. We compare this extraction approach to more         Figure 1: A post from Bidding For Travel
    traditional information extraction methods that rely
    on structural and grammatical characteristics, and
    we show that our approach outperforms traditional posts is especially difﬁcult because the data is neither struc-
    methods on this type of data.                     tured enough to use wrapper technologies such as Stalker
                                                      [Muslea et al., 2001] nor grammatical enough to exploit Nat-
                                                      ural Language Processing (NLP) techniques such as those
1  Introduction                                       used in Whisk [Soderland, 1999].
                                                        This lack of grammar and structure can be overcome by
The Semantic Web will revolutionize the use of the internet, adding knowledge to IE. This extra knowledge consists of
but the idea faces some major challenges. First, construc- collections of known entities and their common attributes,
tion of the Semantic Web requires a lot of extra markup on which we call “reference sets.” A reference set can be an
documents, but this work should not be forced upon every- online set of reference documents, such as the CIA World
day users. Second, there is a lot of information that would be Fact Book. It can also be an online (or ofﬂine) database, such
more useful if it were annotated for the Semantic Web, but as the Comics Price Guide.2 With the Semantic Web we en-
the nature of the data makes it difﬁcult to do so. Examples vision building reference sets from the numerous ontologies
of this type of data are the text of EBay posts, internet clas- that already exist. Continuing with our hotel example from
siﬁeds like Craig’s list, bulletin boards such as Bidding For Figure 1, assume there is an ontology of U.S. hotels, and from
Travel,1 or even the summary text below the hyperlinks re- it we build a reference set with the following attributes: city,
turned after querying Google. We call each piece of text from state, star rating, hotel name, area name, etc.
these sources a “post.” It would be beneﬁcial to add semantic To use reference sets for semantic annotation we exploit
annotation to such posts, like that shown in Figure 1, but the the reference set to determine which, if any, of the attributes
annotation task should carry no burden to human users. appear in the post. To do this, we ﬁrst determine which mem-
  Information extraction (IE) can be used to extract and se- ber of the reference set best matches the post. We call this
mantically annotate pieces of some text. However, IE on the record linkage step. Then we exploit the attributes of this

  1www.biddingfortravel.com                              2www.comicspriceguide.comreference set member for the information extraction step by 2 Aligning Posts to a Reference Set
identifying and labeling attributes from the post that match To correctly parse the attributes from the post, we need to
those from the matching member of the reference set. We ﬁrst decide what those attributes are. To aid in this process,
annotate the post in this manner.                     we match the post to a member of the reference set.
  For instance, the circled hotel post in Figure 1 matches the Since it is infeasible to compare the post to all members of
reference set member with the hotel name of “Holiday Inn the reference set, we construct a set of candidate matches in a
Select” and the hotel area of “University Center.” Using this process known as “blocking”. Many blocking methods have
match we label the tokens “univ. ctr.” of the post as the “Hotel been proposed in the record linkage community (see [Baxter
Area,” since they match the hotel area attribute of the match- et al., 2003] for a recent survey of some), but the basic idea is
ing reference set record. In this manner we annotate all of to cluster candidates together around a blocking key. In our
the attributes in the post that match those of the reference set. work, a candidate for a post is any member of the reference
Figure 2 illustrates our approach on the example post from set that shares some n-gram with that post. The choice of
Figure 1. For purposes of exposition, the reference set shown algorithm here is independent of the overall alignment algo-
only has 2 attributes: name and area.                 rithm.
                                                        Next we ﬁnd the candidate that best matches the post. That
                                                      is, we must align one data source’s record (the post) to a
                                                      record from the other data source (the reference set candi-
                                                      dates). This alignment is called record linkage [Fellegi and
                                                      Sunter, 1969].
                                                        However, our record linkage problem differs and is not
                                                      well studied. Traditional record linkage matches a record
                                                      from one data source to a record from another data source
                                                      by relating their respective, decomposed attributes. Yet the
                                                      attributes of the posts are embedded within a single piece of
                                                      text. We must match this text to the reference set, which is
                                                      already decomposed into attributes and which does not have
                                                      the extraneous tokens present in the post. With this type of
                                                      matching traditional record linkage approaches do not apply.
                                                        Instead, we create a vector of scores, VRL, for each candi-
                                                      date. VRL is composed of similarity scores between the post
                                                      and each attribute of the reference set. We call these scores
                                                      RL scores. VRL also includes RL scores between the post and
                                                      all of the attributes concatenated together. In the example ref-
                                                      erence set from Figure 2, the schema has 2 attributes <Hotel
            Figure 2: Annotation Algorithm            Name, Hotel Area>. If we assume the current candidate is
                                                      <“Hyatt”, “PIT Airport”>. Then we deﬁne VRL as:
  In addition to annotating attributes in the post from the ref-
                                                      VRL=<RL   scores(post,“Hyatt”),
erence set, we also annotate attributes that are identiﬁable, but RL scores(post,“PIT Airport”),
not easily represented in reference sets. Examples of such at- RL scores(post,“Hyatt PIT Airport”)>
tributes include prices or dates. This is shown by the price of
Figure 1. Also, we include annotation for the attributes of the Each RL scores(post,attribute) is itself a vector, composed of
matching reference member. (These are called “Ref hotel...” three other vectors:
in Figure 1.) Since attribute values differ across posts, these RL scores(post,attribute)=<token scores(post,attribute),
reference member attributes provide a set of normalized val-                  edit scores(post,attribute),
ues for querying. Also, the reference set attributes provide                  other scores(post,attribute)>
a simple visual validation to the user that the IE step iden- The three vectors that compose RL scores represent dif-
tiﬁed things correctly. Lastly, by including attributes from ferent similarity types. The vector token scores is the set
the matching reference member, we can provide values for of token level similarity scores between the post and the
attributes that were not included in the post. In our exam- attribute, including Jensen-Shannon distance (both with a
ple post of Figure 1, the user did not include a star rating. Dirichlet prior and a Jelenik-Mercer mixture model) and Jac-
By including the reference set member’s attribute for this star card similarity. The vector edit scores consists of the follow-
rating, we add useful annotation for information that was not ing edit distance functions: Smith-Waterman distance, Lev-
present initially.                                    enstein distance, Jaro-Winkler similarity and character level
  This paper describes our algorithm for semantic annotation Jaccard similarity. (All of the scores in token scores and
using reference sets. Section 2 describes the record linkage edit scores are deﬁned in [Cohen et al., 2003].) Lastly, the
step and Section 3 describes the extraction step. Section 4 vector other scores consists of scores that did not ﬁt into ei-
presents experimental evaluation, and section 5 presents dis- ther of the other categories, speciﬁcally the Soundex score be-
cussion of results. Section 6 presents related research, and tween the post and the attribute and the Porter stemmer score
section 7 describes our conclusions.                  between the two.  Using RL scores of just the reference attributes themselves becomes,
gives a notion of the ﬁeld similarity between the post and the
                                                      IE scores(token,attribute)=<edit scores(token,attribute),
match candidate. The RL scores that uses the concatenation
                                                                               other scores(token,attribute)>
of the reference attributes gives an idea of the record level
match. Since the post is not yet broken into attributes, this So, using our example from Figure 2, we match the post to
is how we determine these ﬁeld and record level similarities. the reference set member {“Holiday Inn Select”,“University
We could use only the concatenated scores since all we desire Center”}. So, if we generate the VIE for the post token
is a record level match. However, it is possible for differ- “univ.” it would look like:
ent records in the reference set to have the same record level
                                                      V   = <common  scores(“univ.”),
score, but different scores for the attributes. If one of these IE
                                                              IE scores(“univ.”,“Holiday Inn Select”),
records had a higher score on a more discriminative attribute,
                                                              IE scores(“univ.”,“University Center”)>
we would like to capture that.
  After all of the candidates are scored, we then rescore each Since each VIE is not a member of a cluster where the winner
VRL. For each element of VRL, the candidates with the max- takes all, there is no binary rescoring.
imum value at that index map this element to 1. The rest of Each VIE is then passed to a multiclass SVM [Tsochan-
the candidates map this element to 0.                 taridis et al., 2004] trained to give it a class label, such as hotel
  For example, assume we have 2 candidates, with the vec- name, hotel area, or price. Intuitively, similar attribute types
tors V1RL and V2RL:                                   should have a similar VIE. We expect that hotel names will
                                                      generally have high scores against the reference set attribute
           V1RL  = <.999,1.2,...,0.45,0.22>
                                                      of hotel names, and small scores against the other attributes,
           V2RL  = <.888,0.0,...,0.65,0.22>
                                                      and the vector will reﬂect this.
After rescoring they become:                            The SVM   learns that any vector that does not look like
                V1RL = <1,1,...,0,1>                  anything else should be labeled as “junk”, which can then be
                V2RL = <0,0,...,1,1>                  ignored. This is an important idea because without the bene-
The rescoring helps to determine the best possible candidate ﬁts of a reference set this would be an extraordinarily difﬁcult
match for the post. Since there might be a few candidates task. If features such as capitalization and token location were
with similarly close values, and only one of them is a best used, who is to say “Great Deal” is not a car name? Also,
match, the rescoring separates out the best candidate as much many traditional IE systems that work in this unique domain
as possible.                                          of ungrammatical, unstructured text, such as addresses and
                                                      bibliographies, assume that each token of the text must be
  After rescoring, we pass each VRL to a Support Vector
Machine (SVM) [Joachims, 1999] trained to label them as classiﬁed as something, an assumption that cannot be made
matches or non-matches. When the match for a post is found, when users are entering text.
the attributes of the matching reference set member are added However, by treating each token in isolation there is a
as annotation to the post.                            chance that a junk token will be mislabeled. For example,
                                                      a junk token might have enough letters to be labeled as a ho-
3  Extracting Data from Posts                         tel area. Then when we extract the hotel area from the post,
                                                      it will have a noisy token. Therefore, labeling each token in-
To exploit the reference set for extraction we use the attributes dividually gives an approximation of the data to be extracted.
of the best match from the reference set as a basis for identi- To improve extraction, we can exploit the power of the ref-
fying similar attributes in the post.                 erence set by comparing the whole extracted ﬁeld to its ana-
  To begin the extraction process, we break the post into to- logue reference set attribute. Thus, once all of the tokens from
kens. In our example from Figure 2, the post becomes the a post are processed and we have whole attributes labeled, we
set of tokens, {“$25”, “winning”, “bid”,...}. Each of these take each attribute and compare it to the corresponding one
tokens is then scored against each attribute of the record from from the reference set. Then we can remove the tokens that
the reference set that was deemed the match. This scoring introduce noise in the extracted attribute.
consists of building a vector of scores which we call VIE. To do this, we ﬁrst get two baseline scores between the
Although similar to VRL, the VIE vector does not include the extracted attribute and the reference set attribute. One is a
vector token scores because we are comparing single tokens Jaccard similarity, which demonstrates token level similarity.
of the post. Instead, we include a vector common scores. The However, since there are many misspellings and such, we also
common  scores vector includes user deﬁned functions such need an edit-distance based similarity metric. For this we use
as regular expressions, which help identify the attributes that the Jaro-Winkler metric. These baselines give us an idea of
are not present in the reference set, such as price or date. So, how accurately we performed on the approximate extraction.
VIE consists of the vector common scores and an IE scores Using our post from Figure 2, assume the phrase “holiday
vector between each token and each attribute from the refer- inn sel. univ. ctr.” was “holiday inn sel. in univ. ctr.”. In
ence set.                                             this case, we might extract “holiday inn sel. in” as the hotel
  Our RL scores vector from the record linkage step,  name. In isolation, the token “in” could be the “inn” of a hotel
RL scores(post,attribute)=<token scores(post,attribute), name. Comparing this extracted hotel name to the reference
                        edit scores(post,attribute),  attribute, “Holiday Inn Select,” we get a Jaccard similarity of
                        other scores(post,attribute)> 0.4 and a Jaro-Winkler score of 0.87.  Next, we go through the extracted attribute, removing one ing fold was 30% of the total posts, and the testing fold was
token at a time and calculating the new Jaccard and Jaro- the remaining 70%. We ran 10 trials and report the average
Winkler similarities. If both of these new scores are higher results for these 10 trials.
than the baseline, then that token is a candidate for removal.
Once all of the tokens are processed in this way, the candi- 4.1 Alignment Results
date for removal that has the highest scores is removed, and Our approach hinges on exploiting reference sets, so the
we repeat the whole process. The process ends when there alignment step should perform well. We report the results of
are no more tokens that yield improved scores when they are the alignment step in Table 1. According to the usual record
removed. In our example, we ﬁnd that “in” is a removal can- linkage statistics we deﬁne:
didate since it yields a Jaccard score of 0.5 and a Jaro-Winkler
                                                                             #CorrectMatches
score of 0.92. Since it has the highest scores after the itera- P recision =
tion, it is removed. Then we see that removing any of the                  #T otalMatchesMade
remaining tokens does not provide improved scores, so the                  #CorrectMatches
process ends.                                                     Recall =
                                                                           #P ossibleMatches
  Aside from increasing the accuracy of extraction, this ap-
                                                                             2 ∗ P recision ∗ Recall
proach has the added beneﬁt of disambiguation. For instance,  F − Measure  =
a token might be both a hotel name and a hotel area, but not                   P recison + Recall
both at the same time? As an example, “airport” could be part We compare our record linkage approach to WHIRL [Co-
of a hotel name or a hotel area. In this case, we could label hen, 2000]. WHIRL is record linkage system that does soft
it as both, and the above approach would remove the token joins across tables by computing vector-based cosine similar-
from the attribute that it is not. However, our implementation ities between the attributes. All other record linkage systems
currently assigns only one label per token, so we did not test require matching based on decomposed ﬁelds, so WHIRL
this disambiguation technique.                        served as a benchmark because it does not have this require-
  Thus, the whole extraction process takes a token of the text, ment. As input to WHIRL, one table was the test set of posts
creates the VIE and passes this to the SVM which generates a (70% of the posts) and the other table was the reference set
label for the token. Then each ﬁeld is cleaned up and we add with the attributes concatenated together. As in our record
the annotation to the post. This produces the output shown at linkage step, using the concatenation of attributes can best
the end of Figure 2.                                  mirror ﬁnding a record level match. This was also done be-
                                                      cause joining across each reference set attribute separately
4  Results                                            leaves no way to combine the matches for these queries. For
To validate our approach, we implemented our algorithm in example, would we only count the reference set members that
a system named Phoebus and tested the technique in two do- score highest for every attribute as matches?
mains: hotel postings and comic books.                  We ran a similarity join across these tables, which pro-
  In the hotel domain, we attempt to parse the hotel name, duced a list of matches, ordered by descending similarity
hotel area, star rating of the hotel, price and dates booked score. For each post that had matches from the join, the refer-
from the Bidding For Travel website. This site is a forum ence set member with the highest similarity score was called
where users share successful bids for Priceline. We lim- its match. These results are also reported in Table 1. Since
ited our experiment to posts about hotels in Sacramento, San Phoebus is able to represent both an attribute level and record
Diego and Pittsburgh. As a reference set, we use the Bidding level similarity in its score, using more than just token based
For Travel hotel guides. These guides are special posts that cosine similarity, it outperformed WHIRL.
list all of the hotels that have ever been posted about in a given
area. These posts provide the hotel name, hotel area and the             Prec. Recall  F-measure
star rating, which are used as the reference set attributes.    Hotel
  The comic domain uses posts from EBay about comic            Phoebus  93.60   91.79    92.68
books for sale searched by keyword “Incredible Hulk” and       WHIRL    83.52   83.61    83.13
“Fantastic Four”. Our goal is to parse the title, issue number, Comic
price, condition, publisher, publication year and the descrip- Phoebus  93.24   84.48    88.64
tion from each post. (Note: the description is a few word      WHIRL    73.89   81.63    77.57
description commonly associated with a comic book, such as
1st appearance the Rhino.) As a reference set for this do-        Table 1: Record linkage results
main, we used the Comics Price Guide3 for lists of all of the
Incredible Hulk and Fantastic Four comics, as well as a list
of all possible comic book conditions. In this case, the ref- 4.2 Extraction Results
erence set included the attributes title, issue number, descrip-
tion, condition and publisher.                        We also performed experiments to validate our approach to
  Experimentally, we split the posts in each domain into 2 extraction. Speciﬁcally, we compare our technique to two
folds, one for training and one for testing, where the train- other IE methods as baselines.
                                                        One baseline is Simple Tagger from the MALLET [Mc-
  3http://www.comicspriceguide.com/                   Callum, 2002] suite of text processing tools. Simple Taggeris an implementation of Conditional Random Fields (CRF).                      Hotel
CRFs have been effectively used in IE. As an example, one                  Prec. Recall F-Measure Freq
algorithm combines information extraction and coreference Area Phoebus     89.25 87.5   88.28     809.7
                   [                ]                         Simple Tagger 92.28 81.24 86.39
resolution using CRFs Wellner et al., 2004 .                  Amilcare     74.20 78.16  76.04
  We also present our results versus Amilcare [Ciravegna, Date Phoebus     87.45 90.62  88.99     751.9
2001], which uses shallow Natural Language Processing for     Simple Tagger 70.23 81.58 75.47
information extraction. It has been empirically shown that    Amilcare     93.27 81.74  86.94
Amilcare does much better in extraction versus other sym- Name Phoebus     94.23 91.85  93.02     1873.9
bolic systems [Ciravegna, 2001]. It presents a good bench-    Simple Tagger 93.28 93.82 93.54
mark versus NLP based systems, which we expect will not       Amilcare     83.61 90.49  86.90
do well on our domains. For the tests, we supplied our refer- Price Phoebus 98.68 92.58 95.53     850.1
                                                              Simple Tagger 75.93 85.93 80.61
ence data as gazetteers to Amilcare.                          Amilcare     89.66 82.68  85.86
  Our IE technique includes scores that we deemed “com- Star  Phoebus      97.94 96.61  97.84     766.4
mon” scores, which are used to help identify attributes not   Simple Tagger 97.16 97.52 97.34
in the reference set. We make these clear for each domain,    Amilcare     96.50 92.26  94.27
since they are the only domain speciﬁc scores we include. For
the hotel domain, our common scores are matchPriceRegex       Table 2: Extraction results: Hotel domain
and matchDateRegex, which give a positive score if a to-
ken matches a price or date regular expression, and 0 oth-                   Comic
erwise. For the comic domain, we use matchPriceRegex and                     Prec. Recall F-Measure Freq
                                                        Condition Phoebus    91.80 84.56 88.01     410.3
matchYearRegex.                                                  Simple Tagger 78.11 77.76 77.80
  We present our results using Precision, Recall and F-          Amilcare    79.18 67.74 72.80
Measure as deﬁned above. Tables 2 and 3 show the results of Descript. Phoebus 69.21 51.50 59.00    504.0
correctly labeling the tokens within the posts with the correct  Simple Tagger 62.25 79.85 69.86
attribute label for the Hotel and Comic domains, respectively.   Amilcare    55.14 58.46 56.39
                                                        Issue    Phoebus     93.73 86.18 89.79     669.9
Attributes in italics are attributes that exist in the reference Simple Tagger 86.97 85.99 86.43
set. The column Freq shows the average number of tokens          Amilcare    88.58 77.68 82.67
that have the associated label.                         Price    Phoebus     80.00 60.27 68.46     10.7
  Table 4 shows the results reported for all possible tokens,    Simple Tagger 84.44 44.24 55.77
which is a weighted average, since some attribute types are      Amilcare    60.0  34.75 43.54
                                                        Publisher Phoebus    83.81 95.08 89.07     61.1
more frequent than others. Also included in Table 4 are “ﬁeld    Simple Tagger 88.54 78.31 82.83
level” summary results. Field level results regard a piece of    Amilcare    90.82 70.48 79.73
extracted information as correctly labeled only if all of the Title Phoebus  97.06 89.90 93.34     1191.1
tokens that should be present are, and there are no extraneous   Simple Tagger 97.54 96.63 97.07
tokens. In this sense, it is a harsh, all or nothing metric, but it Amilcare 96.32 93.77 94.98
                                                        Year     Phoebus     98.81 77.60 84.92     120.9
is a good measure of how useful a technique would really be.     Simple Tagger 87.07 51.05 64.24
  We tested the F-Measures for statistical signiﬁcance using     Amilcare    86.82 72.47 78.79
a two-tailed paired t-test with α=0.05. In Table 2 the only
F-Measure difference that was not signiﬁcant was the Star at- Table 3: Extraction results: Comic domain
tribute between Phoebus and Simple Tagger. In Table 3 the
F-Measures for Price were not signiﬁcant between Phoebus tions, such that they are almost never broken up in the middle.
and Simple Tagger and between Phoebus and Amilcare. In For instance, many descriptions go from the token “1st” to a
Table 4 all differences in F-Measure proved statistically sig- few words after, with nothing interrupting them in the mid-
niﬁcant.                                              dle. Simple Tagger, then, would label all of these tokens as
  Phoebus outperforms the other systems on almost all at- a description. However, lots of times it labeled too many.
tributes, and for all summary results. There were 3 attributes This way, it had a very high recall for description, by labeling
where Phoebus was outperformed, and two of these warrant so much data, but it suffered in other categories by labeling
remarks. (We ignore hotel name since it was so similar.) On things such as conditions as descriptions too.
Comic titles, Phoebus had a much lower recall because it was Phoebus had the highest level of precision for Comic de-
unable to extract parts of titles that were not in the reference scriptions, but it had a very low recall because it ignored many
set. Consider the post, “The incredible hulk and Wolverine of the description tokens. Part of this problem stemmed from
#1 Wendigo”. In this post, Phoebus extracts “The incredible classifying tokens individually. Since many of the description
hulk,” but the actual title is “The incredible hulk and Wolver- tokens were difﬁcult to classify from a single token perspec-
ine.” In this case, the limited reference set hindered Phoebus, tive, they were ignored as junk.
but this could be corrected by including more reference set
data, such as other comic book price guides.
  The Comic description is the other attribute where Simple 5 Discussion
Tagger outperformed Phoebus. Simple Tagger learned that One drawback when using supervised learning systems is the
for the most part, there is an internal structure to descrip- cost to label training data. However, our entire algorithm gen-