                  Evaluating Coverage for Large Symbolic NLG Grammars 

                                                Charles B. Callaway 
                             ITC-irst Istituto per la Ricerca Scientifica e Tecnologica 
                                              via Sommarive, 18 - Povo 
                                                  38050 Trento, Italy 
                                                    callaway@itc.it 


                        Abstract                               statistical processes, pairs of subgraphs of knowledge bases 
                                                               and their texts, do not currently exist in large quantities. 
     After many successes, statistical approaches that 
                                                                 Because of this representation problem, most statistical 
     have been popular in the parsing community are 
                                                               systems have concentrated on replacing existing individual 
     now making headway into Natural Language Gen•
                                                               components in the standard NLG pipelined architecture [Re-
     eration (NLG). These systems are aimed mainly at 
                                                               iter, 1994] without changing the remaining original sym•
     surface realization, and promise the same advan•
                                                               bolic modules. The most popular candidate has been the 
     tages that make statistics valuable for parsing: ro•
                                                               surface realization module [Elhadad, 1991; Bateman, 1995; 
     bustness, wide coverage and domain independence. 
                                                               Lavoie and Rambow, 1997; White and Caldwell, 1998], 
     A recent experiment aimed to empirically verify 
                                                               which is responsible for converting the syntactic representa•
     the linguistic coverage for such a statistical surface 
                                                               tion of a sentence into the actual text seen by the user. Thus 
     realization component by generating transformed 
                                                               current statistical generators are still dependent on remaining 
     sentences from the Penn TreeBank corpus. This ar•
                                                               architectural modules in a system to function and do not by 
     ticle presents the empirical results of a similar ex•
                                                               themselves account for a large amount of linguistic phenom•
     periment to evaluate the coverage of a purely sym•
                                                               ena: pronominalization, revision, definiteness, etc. 
     bolic surface realizer. We present the problems fac•
                                                                 However, statistical surface realizers [Langkilde and 
     ing a symbolic approach on the same task, describe 
                                                               Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 
     the results of its evaluation, and contrast them with 
                                                               2000; Langkilde-Geary, 2002] have focused attention on a 
     the results of the statistical method to help quan•
                                                               number of problems facing standard, pipelined NLG that 
     titatively determine the level of coverage currently 
                                                               have until now been generally considered future work: large-
     obtained by NLG surface realizers. 
                                                               scale, data-robust and language- and domain-independent 
                                                               generation. In addition, as Langkilde points out, empirical 
1 Introduction                                                 evaluation has not been standard practice in the NLG com•
                                                               munity, which has instead relied either on the software engi•
Like parsing, text generation offers enormous potential bene•
                                                               neering practice of regression testing with a suite of examples 
fits for more natural interaction with computers. Examples of 
                                                               or theoretical evaluations [Robin and McKeown, 1995]. 
applications which could be greatly improved include auto•
                                                                 This paper presents the analogue of this recent statistical 
matic technical documentation, intelligent tutoring systems, 
                                                               experiment using a well-known off-the-shelf symbolic sur•
and machine translation, among many others. Historically, 
                                                               face realizer, using an augmented generation grammar that 
natural language generation (NLG) has focused on the study 
                                                               includes support for dialogue and additional syntactic cov•
of symbolic pipelined architectures which receive knowledge 
                                                               erage. We first describe in the following section the repre•
structures and goals from knowledge-based applications and 
                                                               sentations and processes needed to understand its evaluation. 
which proceed to progressively add linguistic information. 
                                                               We then detail our implemented system for converting sen•
   In the last few years, the same paradigm shift which 
                                                               tences from a large corpus into a systemic functional nota•
occurred in the parsing community, the use of statisti•
                                                               tion, present an evaluation of that system and the grammar 
cal/empirical methods, has begun to influence the NLG com•
                                                               itself using Section 23 of the Penn TreeBank [Marcus el al, 
munity as well. As with parsing, statistical generation 
                                                               1993], and finally discuss the implications of that evaluation. 
promises benefits such as robustness in the face of bad data, 
wider coverage, domain and language independence, and less     2 Sentence Representations 
need for costly resources such as grammars. But unlike pars•
ing, which starts with a very flat representation (text) which To undertake a large-scale evaluation of a symbolic surface 
is easily accessible in large quantities to both statistical and realizer, we must first find a large quantity of sentence plans 
symbolic methods, the semantic input for NLG is typically      with which to produce text. However, most text planners can•
associated with large knowledge-based systems. The types of    not generate either the requisite syntactic variation or quan•
corresponding corpora which would be necessary for using       tity of text, and we thus cannot turn to implemented gener-


NATURAL LANGUAGE                                                                                                      811            Figure 1: A Perm TreeBank Annotated Sentence and Corresponding FUF/SURGE Functional Description 

ation systems as a source. To solve this problem, Langkilde 
trained a statistical algorithm [Langkilde-Geary, 2002] on a 
substitute set of sentence plans: the Penn TreeBank [Marcus 
et ai, 1993], a collection of sentences from newspapers such 
as the Wall Street Journal, which have been hand-annotated 
for syntax by linguists. An example sentence is shown on the 
left side of Figure 1. Hierarchical syntactic/semantic bracket•  Figure 2: Penn TreeBank Notation and Normalized Form 
ing is provided along with the syntactic categories of lexemes 
and symbols in the newspaper texts. 
   Unfortunately, text planners currently in use do not gener• builds the corresponding functional description. The pre•
ate representations of the form found in the Penn TreeBank,    processor is organized as a context-sensitive, proceduralized 
opting instead to use more fully-developed syntactic theories, rewriting grammar which matches input symbols to output 
such as HPSG [Pollard and Sag, 1994], from the linguis•        symbols. The resulting functional descriptions can then be 
tics community. Because annotated texts do not exist in this   given to the FUF/SURGE surface realizer, and the sentence 
form, Langkilde created a pre-processing system to translate   string it produces can be lexically compared to the original 
from the TreeBank annotation into the language accepted by     sentence in various ways to determine how well the surface 
the HALOGEN statistical surface realizer [Langkilde-Geary,     realizer performs at sentence generation. 
2002]. HALOGEN uses these translations to create a forest 
lattice whose paths from start to finish represent many possi• 3 Implementation 
ble versions of a single sentence. Separately, a larger corpus 
is processed to obtain bigram or trigram frequencies, which    The implementation necessary for evaluating the coverage 
are then used to rank the possible sentence versions based     of FUF/SURGE comprised three processes: (1) normalizing 
on word adjacency. The highest ranked sentence is then pre•    the syntactic/semantic representations, (2) transforming the 
sented as the final output of the system.                      normalizations into functional descriptions, and (3) generat•
                                                               ing the sentence itself with a surface realizer. The normaliz•
   In contrast, most deep surface realizers are symbolic rather 
                                                               ing phase is necessary to convert the original Penn TreeBank 
than statistical, and consist of components that check gram•
                                                               structures into a LlSP-readable format (Figure 2), which was 
matical constraints, appropriately linearize constituents, and 
                                                               accomplished with a series of regular expression transforma•
adjust for morphology and formatting. One such system 
                                                               tions on the original text file. 
in wide use, FUF/SURGE [Elhadad, 1991], combines ideas 
                                                                 The most time-consuming aspect of the procedure was cre•
from systemic functional grammars and head-driven phrase 
                                                               ating the transformation component, which was highly anal•
structure grammars. An example of the FUF representation, 
                                                               ogous to writing parsing rules by hand. The resulting com•
known as afunctional description is shown on the right side 
                                                               ponent contained 4000 lines of code and approximately 900 
of Figure 1. SURGE is the largest generation grammar for En•
                                                               rules, although most of the actual computational effort was 
glish, and has the largest regression test suite available. But 
                                                               spent instead in surface realization. Most of the problems 
as Langkilde pointed out, 500 test examples are insufficient 
                                                               encountered were the result of differences in the underlying 
to empirically demonstrate the coverage of a grammar. 
                                                               grammars themselves. For example, the Penn TreeBank has 
  To arrive at a set of sentence plans which is representa•    a more hierarchical noun phrase structure than the flatter rep•
tive of English, as well as to evaluate the coverage of the    resentation of SURGE'S systemic functional grammar. 
FUF/SURGE surface realizer in a way which can be directly        The final task involved changing the surface realization 
compared to the HALOGEN evaluation, we likewise used the       component (1) to add additional branches and surface forms 
Penn TreeBank as a sentence source. Because our represen•      to the grammar that were not originally present in order to 
tations are also different, we (as Langkilde) needed a pre•    produce surface forms not previously possible, (2) to add new 
processing system to convert from the TreeBank notation into 
                                                               punctuation and capitalization rules , and (3) to update irreg-
the functional descriptions expected by the surface realizer.                                    1

  Our pre-processor thus performs top-down structure traver•      ]The Penn TreeBank, because it is a newspaper corpus, contains 
sal of a sentence annotated in Penn TreeBank format and        many newspaper headlines and stock quotes with domain-specific 


812                                                                                                NATURAL LANGUAGE                                       Table 1: Comparison with HALOGEN fLangkilde, 2002J 


ular morphology due to the vast number of words the system              algorithm parameters, and gets exponentially worse if it uses 
had not previously seen. The principal linguistic problems              trigrams or larger models in an attempt to improve quality. 
uncovered by this phase include: 

   • Quotations: Newspaper text generally contains large                4 Experiments and Results 
     amounts of complex quotations, such as splitting a                 In order to evaluate the coverage of the SURGE grammar, we 
     quoted phrase to insert the speaker in the middle, or              used the standard train and test methodology. Unlike typi•
     merging a quote into an unquoted part of the sentence:             cal machine learning experiments, adjustments to the trans•
     "1 have this feeling that it's built on sand," she says, that      formation rule set were done by hand, although the evalua•
     the market rises "but there's no foundation to it."                tion of the resulting sentences was performed automatically. 
   • Punctuation scoping: Problems related to the use of                Training took place over a period of several months, consist•
     punctuation with tree structureslDoran, 19981. For ex•             ing of multiple iterations over Penn TreeBank Sections 0-22 
     ample, SURGE has a flat representation for noun phrases,           and 24 to both improve the number of sentences which could 
     causing difficulties with phrases such as The major "cir•          be generated and to match as closely as possible the origi•
     cuit breakers" where SURGE cannot insert punctuation               nal sentences. These two goals were accomplished solely by 
     between the adjective and nominal classifier.                      adding rules to the transformation set and by updating SURGE 
                                                                        grammar rules, notably aspects pertaining to the stock market 
   • Adverb and clause ordering: Because satellite clauses              domain, in addition to support for extended quotations which 
     in SURGE are placed using semantic information, they               was added in previous work iCallaway and Lester, 20021. 
     sometime appear in different (though still grammatically 
                                                                           We considered two types of string matches: exact 
     acceptable) positions than were specified in the original 
                                                                        matches that were identical character-by-character, and 
     sentence [Elhadad et ai, 2001 J. This can oftentimes 
                                                                        "close" matches which were two words or less longer or 
     cause a perfectly acceptable sentence to be produced, 
                                                                        shorter than the original sentence. There were several motiva•
     but highly skew automatic measurements of correctness 
                                                                        tions for this second choice: (1) many sentences were equiv•
     such as tree edit distance (simple string accuracy). For 
                                                                        alent except for a minor missing/extra punctuation mark or 
     example, contrast: "Exports fell 29% in the first few 
                                                                        wrongly capitalized word (especially with newspaper head•
     months" vs. "In the first few months, exports fell 29%." 
                                                                        lines); (2) as mentioned previously, movable clauses (so-
   • Semantic roles: SURGE has a hybrid syntactic/semantic              called circumstantials in SURGE nomenclature) could be put 
     representation, whereas the Penn TreeBank is purely                in multiple acceptable locations; and (3) sentences with al•
     syntactic. Thus some guessing must be done to fill in              most the exact number of words, especially sentences with 
     semantic roles in the corresponding functional descrip•            more than 15 words, were much more likely to at least have 
     tion. Wrong guesses can lead to incorrect surface forms            all of the various phrases present when they were within 
     even when the remainder of the transformation was ac•              two words or the original sentence's length. We utilized the 
     complished successfully.                                           N1ST Simple String Accuracy (SSA) as an automatic eval•
                                                                        uation score (the same as used in Langkilde's work), where 
  While the normalization process is slow (10-12 hours each 
                                                                        the smallest number of Adds, Deletions, and Insertions were 
for WSJ23 and WSJ24), it occurs offline and only once. The 
                                                                        used to calculate accuracy: 1 -(A + D + T)/#Characters. 
transformation process is quite fast, requiring on average 0.12 
                                                                          The only previous measure of generation coverage for Sec•
seconds per sentence. Meanwhile, the FUF/SURGE system is 
                                                                        tion 23 of the Penn TreeBank is that of [Langkilde-Geary, 
relatively slow, as it requires the use of functional unification, 
                                                                        2002], who defined coverage as the number of sentences for 
a task of inherent complexity due to backtracking. Section 
                                                                        which the surface realizer produced strings. As seen in Ta•
23 needed 8,397.2 seconds to generate 2372 sentences (with 
                                                                        ble 1, our system achieved 87.8% on one of the training sets 
a longest exact match of 48 words), while section 24 needed 
                                                                        and 88.7% on the test set compared to between 76.2% and 
5,590.4 seconds to generate 1,326 sentences (with a longest 
                                                                        83.3% for HALOGEN depending on its algorithm parameters. 
exact match of 44), for a combined average of 3.72 seconds 
                                                                          A more detailed examination of the coverage and accuracy 
per sentence.   This contrasts with statistical approaches like 
              2                                                         of the system is found in Table 2 for WSJ Section 24 and 
Langkilde's, which require 27.1-55.5 seconds depending on 
                                                                        Table 3 for Section 23. Both tables are broken down by sen•
formatting not typically used in NLG systems, such as: "8 13/16%        tence length, which shows that the results are highly skewed 
high, 8 1/2% low, 8 5/8% near closing bid, 8 3/4% offered."             towards sentence of smaller length, as would be expected in 
   Additionally, a compiled C version of FUF can produce sen•           a test for exact matches. It should be noted, however, that 
tences using the same grammar on the order of 0.1 seconds.              surface realizers are rarely called upon to generate sentences 


NATURAL LANGUAGE                                                                                                                       813                Table 3: Sentence coverage/accuracy for the unseen WSJ23 sentences grouped by word length 

with the extended lengths and complexities found in highly    matches, i.e., they were not in the "combined" match cate•
educated newspaper text. Finally, the "valid FD" column in•   gory. While these sets included some of the problems listed 
dicates that the transformation program is very good at pro•  in Section 3, 41 of the 50 were capable of being rendered by 
ducing valid functional descriptions, even if they eventually hand as FDs which produced exact matches without changes 
are discarded by the grammar as being erroneous.              to the grammar, 6 required minor changes to the grammar 
  The test set had a slightly higher coverage than the train• which were quickly performed, and the remaining 3 sen•
ing set shown above, as well as a higher number of perfect    tences required major grammar changes which have still not 
matches and a better score using the NIST SSA measure. Al•    yet been made. The latter were sentences that still do not have 
though we trained on other sections of the Penn TreeBank,     satisfactory linguistic analyses in the linguistic literature. We 
time constraints due to the large amount of time required to  thus conclude that with better transformation rules, we could 
generate all test sentences prevented us from having a fuller then obtain close to 95% coverage. 
comparison set. Additionally, Section 24 was the first sec•
tion we trained on, and it is likely that later sections were 5 Discussion 
more similar to Section 23, or that the amount of domain-
specific stock market constructions were imbalanced. Finally, Surface realization is probably the most understood and com•
the "close matches" column shows how many candidate sen•      petent task in NLG today. There is a high possibility that sur•
tences might be nearly exactly matching, and the sum of these face realization can already be considered a solved problem, 
two is reflected in the final category "combined."            except with regard to problems introduced by new languages 
  One interesting observation is that this evaluation (and cor• or highly specialized domains. However, there are two re•
respondingly, the evaluation of HALOGEN) is not only an       lated unsolved problems inherent in the process described in 
evaluation of the underlying surface realizer, but also of the this paper. 
accompanying transformation program that converts the Penn 
TreeBank notation into the specifications it expects. We thus 5.1 Automatic Evaluation of Output 
set out to perform a minor, secondary evaluation to determine Evaluation of NLG systems face the same problems as those 
if it were possible to find a baseline metric for how many sen• that confront Machine Translation systems: Given a set 
tences could still be generated by the surface realizer even if of generated sentences, how do you tell how "good" they 
the corresponding FDs could not be produced for them by the   are in general, and how often you can produce good sen•
transformation program.                                       tences in a given context or application. Work in machine 
  We thus randomly selected 25 sentences from each of the     translation has shifted to large-scale evaluations which re•
two sections which were not either perfect matches or "close" quire automatic evaluation techniques [Papineni et al., 2001; 


814                                                                                              NATURAL LANGUAGE Doddington, 2002] because human graders cannot hope to        offer, for example, the capability of adding formatting state•
examine all of the responses in a short enough period of time. ments in HTML, modifying punctuation, generating dialect 
   Yet current evaluation techniques are completely nu•       differences, adding prosody for TTS, etc. While this may be 
meric/statistical in nature and do not attempt to measure se• due to the multiple decades of history over which symbolic-
mantic content (such as the well-known example of a missing   systems have been developed, it may also be due to the lack of 
"not" in a system's output). Furthermore, these techniques    annotated corpora that support statistical algorithms, or even 
are ill-equipped to evaluate the types of sentences produced  potentially the impossibility of having a corpus at all, such as 
by symbolic NLG systems. For example, by changing a fea•      in large narratives [Callaway and Lester, 2002]. 
ture specifying that, say, a particularly lengthy purpose clause Additionally, there are some disadvantages to the current 
should go at the beginning rather than the end of a sentence, a approaches undertaken in statistical NLG research. For in•
string edit distance metric will report a very large error when stance, symbolic NL generation systems are already consid•
in terms of the system's input, only one "error" has occurred. ered slow, and FUF/SURGE is generally considered to be the 
   As an example of this type of problem, consider that       slowest in the NLG community. And yet the data from Ta•
the string edit distance between the following original sen•  ble 3 shows that HALOGEN is anywhere from 6.5 to 16 times 
tence and that produced by the transformation program and     slower, and thus a 10-sentencc paragraph might need 4 min•
FUF/SURGE with input from the Penn TreeBank would be          utes or longer to be generated. Moreover, these approaches 
83, resulting in a N1ST SSA 50.4% match:                      use techniques such as n-gram models, where n must be in•
                                                              creased to improve quality, but results in even slower genera•
     Freddie Mac said the principal-only securities 
                                                              tion times and exponentially larger storage space. 
     were priced at 58 1/4 to yield 8.45%, assuming an 
     average life of eight years and a prepayment of 
                                                              5.3 Potential Applications 
     160% of the PSA model. 
                                                              The transformation program presented here has additional 
     Freddie Mac said assuming an average life                side benefits besides helping calculate the coverage of a 
     of eight years and a prepayment of 160% of               grammar. For example, in generation systems where non-
     the PSA model, the principal-only securities were        linguists must maintain old data and add new data, such 
     priced at 58 1/4 to yield 8.45%.                         a program allows them to write sample sentences in the 
                                                              syntax-only TreeBank notation, which is much easier for 
   Obviously 50.4% is a poor score for such sentences, but    non-linguists, and then convert those sentences directly into 
many such examples were found in our corpus and their low     a more linguistically-manageable form for generation (e.g., 
scores were factored into the NIST Simple String Accuracy     functional descriptions). Graphical editing tools for linguistic 
ratios in Tables 2 and 3.                                     data such as GATE [Bontcheva et a/., 2002] or similar author•
                                                              ing tools could quicken the process even more. 
5.2 Symbolic vs. Statistical Approaches 
                                                                 Additionally, the transformation program can be used as 
Almost all fully-developed NLG systems to-date operate on     an error-checker for the well-formedness of sentences con•
data specified in a knowledge base from some other sys•       tained in the TreeBank. Rules could be (and have been) added 
tem. The fact that this data has typically been represented   alongside the normal transformation rules that detect when er•
as highly-structured data has been an impediment to tradi•    rors are encountered, categorize them, and make them avail•
tional machine learning techniques which have previously op•  able to the corpus creator for correction. This extends not 
erated mostly on fiat, unstructured text data. It is also gener• only to annotation errors by the corpus creator detectable at 
ally stated that statistical methods are more robust than their the syntax tree level, but even morphology errors such as in-
symbolic counterparts and more easily adapted to new data     correct verbs, typos, or British/American English differences 
sets. The data in the previous section seems to indicate that by the original author of the text. Both of these tasks are much 
HALOGEN, a statistical system, performs substantially better  more difficult for a statistical system to accomplish, requiring 
on longer sentences, even if it has lower overall coverage. But separate retraining in the first case and locating or creating a 
there are several advantages in favor of symbolic techniques. corpus of possible mistakes in the second, much like what is 
   First, the transformation program presented above can be   done with tutoring systems where databases of potential stu•
tweaked to an arbitrary level of perfection by progressively  dent errors are painstakingly constructed. 
adding more rules. Most statistical and machine learning 
systems however have eventually reached a boundary where      6 Conclusions and Future Work 
progress becomes seemingly exponentially more difficult. 
Second, errors which are encountered during processing can    Recent years have seen the arrival of statistical approaches 
be examined and fixed because the grammars and other re•      to the field of Natural Language Generation, much as was 
sources are logically and semantically connected to the lan•  seen in parsing a decade ago. Of the many possible compo•
guage being generated, rather than being a set of numbers. If nents in the standard NLG pipelined architecture, almost all 
however a statistical generator must create new output forms  of these statistical systems have focused on the surface real•
not contained in the initial corpus or model, it must be re•  ization component, offering the same robustness, wide cover•
trained from scratch.                                         age, and domain- and language-independence as for parsing. 
   Finally, symbolic surface realizers allow a wide range of    Recent experiments with one statistically-based system, 
optional operations which statistical programs currently can't HALOGEN, showed that it could achieve respectable cover-


 NATURAL LANGUAGE                                                                                                   815 