            SVMC: Single-Class Classification With Support Vector Machines 

                                                      H wan jo Yu 
                                          Department of Computer Science 
                                    University of Illinois at Urbana-Champaign 
                                               Urbana,IL 61801 USA 
                                                 hwanjoyu@uiuc.edu 


                        Abstract                               (2) the absence of negative samples in the labeled data set 
                                                               makes unfair the initial parameters of the model and thus it 
     Single-Class Classification (SCC) seeks to distin•
                                                               leads to unfair guesses for the unlabeled data. 
     guish one class of data from the universal set of 
                                                                 Active learning methods also try to minimize the labeling 
     multiple classes. We present a new SCC algorithm 
                                                               labors to construct an accurate classification function by a dif•
     that efficiently computes an accurate boundary of 
                                                               ferent approach that involves an interactive process between 
     the target class from positive and unlabeled data 
                                                               the learning system and users [Tong and Koller, 2000]. 
     (without labeled negative data). 
                                                                 Valiant in 1984 [Valiant, 1984] pioneered learning theory 
                                                              from positive examples based on rule learning. In 1998, F. De•
1 Introduction                                                 nis defined the Probably Approximately Correct (PAC) learn•
Single-Class Classification (SCC) seeks to distinguish one     ing model for positive and unlabeled examples, and showed 
class of data from the universal set of multiple classes, (e.g., that k-DNF (Disjunctive Normal Form) is learnable from pos•
distinguishing apples from fruits, identifying "waterfall" pic• itive and unlabeled examples [Denis, 1998]. After that, some 
tures from image databases, or classifying personal home-      experimental attempts to learn using positive and unlabeled 
pages from the Web) (Throughout the paper, we call the target  data have been tried using k:-DNF or C4.5 [Letouzey et al., 
class positive and the complement set of samples negative.)    2000; DeComite et al, 1999]. Rule learning methods are 
   Since it is not natural to collect the "non-interesting" ob• simple and efficient for learning nominal features but tricky to 
jects (i.e., negative data) to train the concept of the "interest• use for the problems of continuous features, high dimensions, 
ing" objects (i.e., positive data), SCC problems are prevalent or sparse instance spaces. 
in real world where positive and unlabeled data are widely       Positive Example-Based Learning (PEBL) framework was 
available but negative data are hard or expensive to acquire   proposed for Web page classification [Yu et ai, 2002]. Their 
[Yu et al. 2002; Letouzey et al. , 2000; DeComite et a/.,      method is limited to the Web domain with binary features, 
 1999]. For example, in text or Web page classification (e.g., and its training efficiency is poor because of using SVM iter-
personal homepage classification), collecting negative train•  atively whose training time is already at least quadratic to the 
ing data (e.g., a sample of "non-homepages") is delicate and   size of training data set. This problem becomes critical when 
arduous because manually collected negative data could be      the size of unlabeled data set is large. 
easily biased because of a person's unintentional prejudice,     A probabilistic method for the SCC problem has been re•
which could be detrimental to classification accuracy. In an   cently proposed for the text domain [Liu et ai, 2002]. As 
example of diagnosis of a disease, positive data are easy to   they specified in the paper, their method - a revision of the 
access (e.g., all patients who have the disease) and unlabeled EM algorithm - performs badly on "hard" problems due to 
data are abundant (e.g., all patients), but negative data are ex• the fundamental limitations of the generative model assump•
pensive if detection tests for the disease are expensive since tion, the attribute independence assumption which results in 
all patients in the database cannot be assumed to be negative  linear separation, and the requirement of good estimation of 
samples if they have never been tested. Further applications   prior probabilities. 
can be also found in pattern recognition, image retrieval, clas• OSVM (One-Class SVM) also distinguishes one class of 
sification for data mining, rare class classification, etc. In this data from the rest of the feature space given only a pos•
paper, we focus on this SCC problem from positive and unla•    itive data set [Tax and Duin, 2001; Manevitz and Yousef, 
beled data (without labeled negative data).                    2001]. Based on a strong mathematical foundation, OSVM 
                                                               draws a nonlinear boundary of the positive data set in the fea•
1.1 Previous Approaches for SCC                                ture space using two parameters - v (to control the noise in 
Traditional (semi-)supervised learning schemes are not suit•   the training data) and (to control the "smoothness" of the 
able for SCC without labeled negative data because: (1) the    boundary). They have the same advantages as SVM, such 
portions of positive and negative spaces are seriously unbal•  as efficient handling of high dimensional spaces and system•
anced without being known (i.e., Pr(P) « Pr(P)), and           atic nonlinear classification using advanced kernel functions. 


LEARNING                                                                                                              567       Figure 1: Boundaries of SVM and OSVM on a synthetic data set. big dots: positive data, small dots: negative data 

 However, OSVM requires a much larger amount of positive       lems (with nominal or continuous attributes, linear or nonlin•
 training data to induce an accurate class boundary because its ear separation, and low or high dimensions) (Section 4). 
 support vectors (SVs) of the boundary only comes from the 
 positive data set and thus the small number of positive SVs   1.3 Notation 
 can hardly cover the major directions of the boundary espe•   We use the following notation throughout this paper. 
 cially in high dimensional spaces. Due to the SVs coming 
                                                                 • x is a data instance such that 
 only from positive data, OSVM tends to ovcrfit and undcrfit 
 easily. Tax proposed a sophisticated method which uses artif-   • V is a subspace for positive class within U, from which 
 ically generated unlabeled data to optimize the OSVM's pa•         positive data set P is sampled. 
 rameters that "balance" between ovcrfitting and undcrfitting    • U (unlabeled data set) is a uniform sample of the univer•
 [Tax and Duin, 2001]. However, their optimization method           sal set. 
 is infeasibly inefficient in high dimensional spaces, and even 
                                                                 • U is the feature space for the universal set such that U C 
 with the best parameter setting, its performance still lags far 
                                                                    R  where m is the number of dimensions. 
 behind the original SVM with negative data due to the short•        m
 age of SVs which makes "incomplete" the boundary descrip•       For an example of Web page classification, the universal 
 tion. Figure 1(a) and (b) show the boundaries of SVM and      set is the entire Web, U is a uniform sample of the Web, P 
 OSVM on a synthetic data set in a two-dimensional space.      is a collection of Web pages of interest, and x € Rm is an 
 (We used L1BSVM version 2.33' for SVM implementation.)        instance of a Web page. 
 In this low-dimensional space with "enough" data, the ob-
 stensibly "smooth" boundary of OSVM is not the result of the  2 Mapping Convergence (MC) Framework 
good generalization but instead is from the poor expressibility 
                                                               2.1 Motivation 
caused by the "incomplete" SVs, which will become much 
worse in high-dimensional spaces where more SVs around         In machine learning theory, the "optimal" class boundary 
the boundary are needed to cover major directions in the high- function (or hypothesis) h(x) given a limited number of train•
dimensional spaces. When we increase the number of SVs in      ing data set {(x,/)} (I is the label of x) is considered the 
OSVM, it ovcrfits rather than being more accurate as shown     one that gives the best generalization performance which de•
in Figure l(c)and(d).                                          notes the performance on "unseen" examples rather than on 
                                                               the training data. The performance on the training data is not 
 1.2 Contributions and Paper Layout                            regarded as a good evaluation measure for a hypothesis be•
                                                               cause the hypothesis ends up overfitting when it tries to fit the 
We first discuss the "optimal" SCC boundary, which moti•
                                                               training data too hard. (When a problem is easy (to classify) 
vates our new SCC framework Mapping-Convergence (MC), 
                                                               and the boundary function is complicated more than it needs 
where the algorithms under the MC framework generate the 
                                                               to be, the boundary is likely overfitted. When a problem is 
boundary close to the optimum (Section 2). In Section 3, we 
                                                               hard and the classifier is not powerful enough, the boundary 
present an efficient SCC algorithm Support Vector Mapping 
                                                               becomes undcrfit.) SVM is an excellent example of super•
Convergence (SVMC) under the MC framework. We prove 
                                                               vised learning that tries to maximize the generalization by 
that although SVMC iterates under the MC framework for 
                                                               maximizing the margin and also supports nonlinear separa•
the "near-optimal" result, its training time is independent of 
                                                               tion using advanced kernels, by which SVM tries to avoid 
the number of iterations, which is asymptotically equal to that 
                                                               overfitting and underfitting [Burges, 1998]. 
of a SVM. We empirically verify our analysis of SVMC by 
extensive experiments on various domains of real data sets       The "optimal" SCC classifier without labeled negative data 
such as text classification (e.g., Web page classification), pat• also needs to maximize the generalization somehow with 
tern recognition (e.g., letter recognition), and bioinformatics highly expressive power to avoid ovcrfitting and undcrfitting. 
(e.g., diagnosis of breast cancer), which shows the outstand•  To illustrate an example of the "near-optimal" SCC bound•
ing performance of SVMC in a wide spectrum of SCC prob-        ary without labeled negative data, consider the synthetic data 
                                                               set (Figure 2) simulating a real situation where within U, (1) 

   1 http://www.csie.ntu.edu.tw/~cjlin/libsvm                  the universal set is composed of multiple groups of data, (2) 


568                                                                                                           LEARNING                                                                           Figure 3: Example of the spaces of the MC framework in U 

                                                                         the boundary around the positive data set in the feature space 
Figure 2: Synthetic data set simulating a real situation. P: big 
                                                                         which also maximizes the margin. 
dots, U: all dots (big and small dots) 

the positive class V is one of them (supposing V is the data             Input: - positive data set P, unlabeled data set U 
                                                                         Output: - a boundary function h, 
group in the center), and (3) the positive data set P is a sam•
ple from V (assuming that the big dots are the sample P).                   : an algorithm identifying "strong negatives" from U 
OSVM draws V, a tight boundary around P, as shown in                        : a supervised learning algorithm that maximizes the margin 
Figure 2(a), which overfits the true class area V due to the 
absence of the knowledge of the distribution of U. However,              Algorithm: 
the "near-optimal" SCC classifiers must locate the boundary              1. Use to construct a classifier ho from P and U which classifies 
between V and U outside V (Figure 2(b)) and thus maximize                   only "strong negatives" as negative and the others as positive 
the generalization. The MC framework using U systemati•                  2. Classify U by h0 
cally draws the boundary of Figure 2(b).                                            examples from U classified as negative by ho 
                                                                                 := examples from U classified as positive by ho 
2.2 Negative Strength                                                    3. Set TV := and i := 0 
                                                                         4. Do loop 
Let h(x) be the boundary function of the positive class in U, 
                                                                            4.1. N := NUN, 
which outputs the distance from the boundary to the instance 
                                                                            4.2. Use to construct h     1 from P and TV 
x in U such that                                                                                      i
                                                                            4.3. Classify < 

                           if x is a positive instance,                                := examples from Pi classified as negative by hi+1 
                           if x is a negative instance,                                  examples from Px classified as positive by hi+1 
                                                                            4.4. i := i + 1 
                           if x is located farther than x'                  4.5. Repeat until 
                           from the boundary in U.                       5. return hi 

Definition 1 (Strength of negative instances). For two neg•
ative instances x and x' such that h(x) < 0 and h(x') < 0,                                  Figure 4: MC framework 
if\h(x)\ > \h(x')\, then x is stronger than x'. 

Example 1. Consider a resume page classification function                  Assume that V is a subspace tightly subsuming P within 
h(x) from the Web (U). Suppose there are two negative data               U where the class of the boundary function for V is from the 
objects x and x' (non-resumepages) in U such that h(x) < 0 
                                                                         algorithm (e.g., SVM). In Figure 4, let N          be the negative 
and h(x') < 0: x is "how to write a resume" page, and                                                                     o
                                                                         space and be the positive space within U divided by h
x' is "how to write an article " page. In U, x' is considered                                                                              0 
more distant from the boundary of the resume class because x             (a boundary drawn by ), and let Ni be the negative space 
has more relevant features to the resume class (e.g., the word           and be the positive space within divided by hi (a 
"resume " in text) though it is not a true resume page.                  boundary drawn by Then, we can induce the follow•
                                                                         ing formulae from the MC framework of Figure 4. (Figure 3 
2.3 MC Framework                                                         illustrates an example of the spaces of the framework in U.) 
The MC framework is composed of two stages: the mapping 
stage and the convergence stage. In the mapping stage, the 
                                                                                                                                         (1) 
algorithm uses a weak classifier , which draws an initial 
approximation of "strong negatives" - the negative data lo•
cated far from the boundary of the positive class in U (steps 
                                                                                                                                         (2) 
1 and 2 in Figure 4). Based on the initial approximation, the 
convergence stage runs in iteration using a second base classi•
fier $2, which maximizes the margin to make a progressively              where I is the number of iterations in the MC framework. 
better approximation of negative data (steps 3 through 5 in              Theorem 1 (Boundary Convergence). Suppose U is uni•
Figure 4). Thus the class boundary eventually converges to              formly distributed in U. If algorithm does not generate 


LEARNING                                                                                                                                569 false negatives, and algorithm maximizes margin, then (I)      Validity of the component algorithms 
the class boundary of the MC framework converges into the 
boundary that maximally separates P and U outside and          J. must not generate false negatives. 
 (2) I (the number of iterations) is logarithmic in the margin 
between and                                                    Most classification methods have a threshold to control the 
                                                               trade-off between precision and recall. We can adjust the 
Proof , because a classifier constructed by the                threshold of so that it makes near 100% recall by sacrific•
algorithm does not generate false negative. A classifier h\    ing precision. (Some violations of this can be handled by the 
constructed by the algorithm trained from the separated        soft constraint of (e.g., SVM).) Determining the threshold 
                                                               can be intuitive or automatic when not concerning the pre•
space and divides the rest of the space 
                                                               cision quality much. The precision quality of does not 
which is equal to into two classes with a boundary 
                                                               affect the accuracy of the final boundary as far as it approxi•
that maximizes the margin between and The first part           mates a certain amount of negative data because the boundary 
becomes and the other becomes Repeatedly, a                    will converge eventually. Figure 5 visualizes the boundary af•
classifier constructed by the same algorithm trained ter each iteration of SVMC. The mapping stage only identi•
from the separated space and divides the rest of               fies very strong negatives by covering a wide area around the 
the space into and with equal                                  positive data (Figure 5(a)). (We used OSVM for the algorithm 
                                                                  of the mapping stage. We intuitively set the parameters of 
margins. Thus, always has the margin of half of 
                                                               OSVM such that it covers all the positive data without much 
(for Therefore, I will be logarithmic in the margin 
                                                               concern for false positives.) Although the precision quality 
between and                                                    of mapping is poor, the boundary at each iteration converges 
  The iteration stops when where there exists no               (Figures 5(b) and (c)), and the final boundary is very close 
sample of U outside Therefore, the final boundary will         to the true boundary drawn by SVM with P and N (Figure 
be located between P and U outside while maximizing the        1(a) and 5(d)). Our experiments in Section 4 also show that 
margin between them.                                           the final boundary becomes very accurate although the initial 
                                                               boundary of the mapping stage is very rough by the "loose" 
  Theorem 1 proves that under certain conditions, the final    setting of the threshold of . 
boundary will be located between P and U outside How•
ever, in the example of Figure 2(b), our framework generates   2. 2 must maximize margin. 
the "better" boundary located between P and U outside V 
                                                               SVM and Boosting are currently the most popular supervised 
because in theorem 1, we made a somewhat strong assump•        learning algorithms that maximize the margin. With a strong 
tion, i.e., U is uniformly distributed, to guarantee the bound• mathematical foundation, SVM automatically finds the opti•
ary convergence. In a more realistic situation where there is  mal boundary without a validation process and without many 
some distance S between classes-Figure 2 shows some gaps      parameters to tune. The small numbers of theoretically moti•
between classes-if the margin between and becomes              vated parameters also work well for an intuitive setting. For 
smaller than at some iteration, the convergence stops be•      these reasons, we use SVM for for our research. In prac•
cause becomes empty. The margin betweenand                    tice, the soft constraint of SVM is necessary to cope with 
   reduces by half at each iteration as the boundary ap•      noise or outliers. The soft constraint of SVM can affect / 
proaches to and thus the boundary is not likely to stop con•  and the accuracy of the final boundary. However, P is not 
verging when it is far from unless U is severely sparse.      likely to have a lot of noise in practice because it is usually 
                                                              carefully collected by users. In our experiments, a low setting 
Thus, we have the following claim: 
                                                              (i.e., v = 0.01) of (the parameter to control the rate of noise 
Claim 1. The boundary of MC is located between P and U        in the training data) performs well for all cases for this reason. 
outside V if U and P are not severely sparse and there exists (We used I/-SVM for the semantically meaningful parameter 
visible gaps between V and U.                                  [Chang and Lin, 2001].) 


570                                                                                                           LEARNING 3 Support Vector Mapping Convergence                           iteration because positive SVs are determined depending on 
     (SVMC)                                                    negative SVs, and it is hard to determine the positive data that 
                                                               cannot be SVs independent of negative SVs or SVM parame•
3.1 Motivation                                                 ters. 
The classification time of the final boundary of SMC ("Sim•      Surprisingly, adding the following statement between step 
ple" MC with = SVM) is equal to that of SVM because            4.4 and 4.5 of the original MC framework of Figure 4 com•
the final boundary is a boundary function of The training      pletes the SVMC algorithm. 
time of SMC can be very long if is very large because the          Reset N with negative SVs of 
training time of SVM highly depends on the size of data set 
         and SMC runs iteratively.                             Theorem 2 (Training time of SVMC). Suppose 
assuming the number of iterations andtsvM = 
0(|f/|2) where is the training time of a classifier 
(tsvM is known to be at least quadratic to and linear to      Proof. For simplicity of the proof, we approximate each 
the number of dimensions). Refer to [Chang and Lin, 2001]      value as follows. 
for more discussion on the complexity of SVM. However, de•
creasing the sampling density of U to reduce the training time 
hurts the accuracy of the final boundary because the density 
of U will directly affect the quality of the SVs of the final 
boundary. 

3.2 SVMC 
SVMC prevents the training time from increasing dramat•
ically as the sample size grows. We prove that although 
SVMC iterates under the MC framework for the "near-
optimal" result, its training time is independent of the number 
of iterations, and thus its training time is asymptotically equal 
to that of a SVM. 
  The approach of SVMC is to use minimally required data 
set at each iteration such that the data set does not degrade 
the accuracy of the boundary while it saves the training time    Theorem 2 states that the training complexity of SVMC 
of each SVM maximally. To illustrate how SVMC achieves         is asymptotically equal to that of SVM. Our experiments in 
this, consider the point of starting the third iteration (when Section 4 also show that SVMC trains much faster than SMC 
       in SMC. (See step 4.1 in Figure 4.) After we merge     while it remains the same accuracy. Figure 5 visualizes the 
    into N, we may not need all the data from N in order to   boundary after each iteration of SVMC on the same data set 
                                                              of Figure 1. 
construct h3 because the data far from h3 may not contribute 
to the SVs. The set of negative SVs of h2 is the representative 
data set for and so we only keep the negative SVs of          4 Empirical Results 
h2 and the newly induced data set to support the negative 
                                                              In this section, we show the empirical verification of our anal•
side of/13. 
                                                              ysis on SVMC by extensive experiments on various domains 
Claim 2 (Minimally required negative data). Minimally         of real data sets - Web page classification, letter recognition, 
required negative data at th iterationthat and diagnosis of breast cancer - which show the outstand•
makes h{+1 is as accurate as the boundary constructed from    ing performance of SVMC in a wide spectrum of SCC prob•
                                                              lems (with nominal or continuous attributes, linear or nonlin•
        and negative support vectors of h{. 
                                                              ear separation, and low or high dimensions). 
Rationale. The negative SVs of will be fromand 
the negative SVs of hi because is the closest data set to     4.1 Datasets and Methodology 
/ij+i and because the directions not supported by in the      Due to space limitations, we reports only the main results. 
feature space will be supported by the negative SVs of h^     Our evaluation is based on the F\ measure 
which are the representing data set for However, r), p is precision and r is recall) as was used in [Liu et al, 
if any of the negative SVs of hi is excluded in constructing  2002] - one of the most recent works on SCC from positive 
           might suffer because the negative SVs of hi need   and unlabeled data2. We also report the accuracy. 
                                                                 We used the letter recognition and breast cancer data sets 
to support the direction that TVj does not support in the feature 
                                                              from the UC1 machine learning repository    for direct com•
space. Thus, negative support vectors of hi are the                                                     3
                                                              parisons with OSVM. (OSVM is often used for letter or digit 
minimally required negative data set at (i + l)th iteration. 

                                                        □        2Refer to [Liu et al., 2002] for the justification of using the Fl 
  For the minimally required data set for the positive side,  measure for SCC. 
we cannot definitely exclude any data object from P at each      3 http://www.ics.uci.cdu/~mlcarn/M LRepository.html 


LEARNING                                                                                                             571 