      Information Extraction from Web Documents Based on Local Unranked Tree 
                                              Automaton Inference 

           Raymond Kosala1, Maurice Bruynooghe1, Jan Van den Bussche2, Hendrik Blocked1 

               1 K.U.Leuven, Dept. of Computer Science, Celestijnenlaan 200A, B-3001 Leuven 
                                   {raymond,maurice,hendrik}@cs.kuleuven.ac.be 

                2 University of Limburg, Dept. WNI, Universitaire Campus, B-3590 Diepenbcek 
                                            jan.vandenbussche@luc.ac.bea 

                         Abstract                               from structured documents, there is some structural context 
                                                                close to the target. After linearisation in a string, this context 
       Information extraction (IE) aims at extracting spe•
                                                                can be arbitrarily far away, making the learning task very dif•
       cific information from a collection of documents. 
                                                                ficult for string based methods. While binarisation may also 
       A lot of previous work on 10 from semi-structured        increase the distance between the context and the target, they 
       documents (in XML or HTML) uses learning tech•           remain closer, and the learning task should be easier. This 
       niques based on strings. Some recent work con•           is confirmed by the experiments in [Kosala et al, 2002b; 
       verts the document to a ranked tree and uses tree        2002a]. If distance between the relevant context and the 
       automaton induction. This paper introduces an al•        target is indeed a main factor determining the ability to 
       gorithm that uses unranked trees to induce an auto•      learn an appropriate automaton, then an algorithm inducing 
       maton. Experiments show that this gives the best         a wrapper directly from the unranked tree should perform 
       results obtained so far for IE from semi-structured      even better. This path is pursued in the current paper. As 
       documents based on learning.                             in [Kosala et al. , 2002b; 2002a] user intervention is lim•
                                                                ited to annotating the field to be extracted in a few rep•
  1 Introduction                                                resentative examples. String based methods require sub•
                                                                stantially more user intervention, such as splitting the doc•
  Information extraction aims at extracting specific information 
                                                                ument into small fragments, and selecting some of them 
  from a collection of documents. One can distinguish between 
                                                                for use as a training example, e.g. [Soderland, 1999]; the 
  IE from unstructured and from (semi-) structured texts [Mus-
                                                                manual specification of the length of a window for the pre•
  lea, 1999]. Extracting information from web documents be•
                                                                fix, suffix and target fragments [Freitag and McCallum, 1999; 
  longs to the latter category and gains importance [Levy et al, 
                                                                Freitag and Kushmerick, 2000], and of the special tokens or 
  1998]. These documents are not written in natural language, 
                                                                landmarks such as ''>" or'' [Freitag and Kushmerick, 2000; 
  but rather involve explicit annotations such as HTML/XML 
                                                                Muslea et al., 2001]. 
  tags to convey the structure of the information, making the 
  methods tuned towards natural language unusable.                The rest of the paper is organized as follows. Section 2 
                                                                provides some background on unranked tree automata and 
    While special query languages exist [Bry and Schaffert, 
                                                                their use for IE. Section 3 describes our methodology and 
  2002; XQL, 2002], their use is time consuming and re•
                                                                introduces our unranked tree inference algorithm. Results are 
  quires nontrivial skill. As argued in [Muslea et al, 2001; 
                                                                described in Section 4, related work in Section 5. Section 6 
  Kushmerick, 2000], there is a need for systems that learn to 
                                                                concludes. 
  extract information from a few annotated examples {wrap•
  per induction). Several machine learning techniques for in•
  ducing wrappers have been proposed. Examples are multi-       2 Preliminaries 
  strategy approaches [Freitag, 2000] and various grammatical   Grammatical inference and information extraction. 
  inference techniques that induce a kind of delimiter-based    Grammatical inference (also called automata induction, 
  patterns [Muslea et al, 2001; Freitag and McCallum, 1999;     grammar induction, or automatic language acquisition) refers 
  Freitag and Kushmerick, 2000; Soderland, 1999; Freitag,       to the process of learning rules from a set of labeled ex•
  1997; Hsu and Dung, 1998; Chidlovskii et a/., 2000]. All      amples. The target domain is a formal language (a set of 
  these methods treat the document as a string of characters.   strings over some alphabet and the hypothesis space is 
    Structured documents such as HTML and XML docu•             a family of grammars. The inference process aims at find•
  ments, however, have an explicit tree structure. In [Kosala   ing a minimum automaton (the canonical automaton) that is 
  et al, 2002b; 2002a], it is argued that one can better exploit compatible with the examples. There is a large body of work 
  this tree structure and use tree automata [Comon et al, 1999]. on grammar induction, for a survey see e.g. [Murphy, 1996; 
  The document tree is converted in a ranked binary tree and k- Parekh and Honavar, 1998]. 
  testable tree automata [Rico-Juan et al., 2000] are induced     In grammar induction, we have a finite alphabet and a 
  and then used for the extraction task. Typically in a IE task formal language . Given a set of examples in 


INFORMATION EXTRACTION                                                                                              403    and a (possibly empty) set of examples not in , the          field of interest. E.g., the text "Organization:" may be relev•
   task is to infer a deterministic finite automaton (DFA) that ac• ant for the extraction, hence this text should not be changed 
   cepts the examples in and rejects those in . In [Freitag,    into CDATA. In each data set, at most one text field is identi•
   1997], an IE task is mapped into a grammar induction task.   fied as distinguishing context. It is found automatically using 
   A document is converted into a sequence of tokens (from      the approach described in [Kosala et al., 2002b]. 
   Examples are transformed by replacing the token to be ex•
   tracted by the special token x. Then a DFA is inferred that 
   accepts all the transformed examples. In our case, a docu•
   ment is converted into a tree and the token to be extracted (at 
   a leaf) is replaced by the special token x. Then a tree auto•
   maton is inferred that accepts all the transformed examples. 
   When using the learned automaton, a similar transformation 
   is done. Each token that is a candidate for extraction is in 
   turn replaced by x. The token replaced by x is extracted iff 
   the transformed document is accepted by the automaton. 

   Unranked tree automata. Some existing algorithms for 
   string automaton induction have been upgraded to ranked tree 
  automaton induction (e.g. [Rico-Juan et al, 2000; Abe and 
   Mamitsuka, 1997])1. By converting documents (which are 
   unranked) into binary trees (which are ranked), tree auto•
  maton induction can be used for IE as shown in [Kosala et 
  al, 2002b]. The present work avoids binarisation and uses 
  unranked trees. Unranked tree automata have been studied 
  since the late 60's, see e.g. [Bruggemann-Klein et al, 2001] 
  for a survey. To our knowledge, algorithms for inducing them  Figure 1: The fields to be extracted are the fields following 
  do not yet exist. This paper is a first step in this direction. the Alt.Name and Organization fields. A document 
     An unranked label is a label with a variable rank (arity). consists of a variable number of records. The number of oc•
  Thus the number of children is not fixed by the label. Given  currences of the fields to be extracted is variable (from zero 
  a set V of labels in an unranked alphabet, we can define , to many). Also the position is not fixed. 
  the set of all (unranked) trees, as follows: 
     • a is a tree where 
     • a(u1,..., un) is a tree, where and each is a 
       tree. 
     An unranked tree automaton (UTA) is a quadruple 
                , where V is a set of unranked labels, Q is a 
  finite set of states, is a set of final (accepting) states, 
  and is a set of transitions where each transition is of the 
  form and e is a regular 
                                                                Figure 2: The left figure is an HTML tree. The right one is 
  expression over Q. 
                                                                the same tree after abstracting the text nodes 
     A bottom up UTA processes trees bottom up. When a leaf 
  node is labeled v and there is a transition such that         Approach. Our approach for information extraction has the 
  e matches the empty string, then the node is assigned state   following characteristics: 
  q. When an internal node is labeled v, its children have been   • Strings stored at the text nodes are treated as single la•
  assigned states qi,..., qn, and there is a transition             bels. If extracted, the whole string is returned. 
  such that the string matches the regular expression 
                                                                  • One automaton is learned for one type of field to be ex•
  e, then the node is assigned state q. A tree is accepted if the 
                                                                    tracted, e.g., the field following "Organization". 
  state of its root is assigned an accepting state 
                                                                  • In examples used during learning, one target field (a text 
                                                                    node) is replaced by x. A document gives rise to several 
  3 Approach and algorithm                                          examples when several targets occur. 
  Preprocessing. Fig. 1 shows a representative task. For        The learning phase proceeds as follows: 
  dealing with text nodes, we follow the approach described       • Replace in the examples the target by "a:", the distin•
  in [Kosala et al., 2002b]: we replace most text nodes by          guishing context(s) (if present) by and all other 
  CDATA2, making an exception for so-called distinguishing          text fields by CDATA. 
  context, specific text that is useful for the identification of the • Map examples to trees and learn a tree automaton. 
                                                                The extraction phase repeats for all candidate targets: 
     'Ranked: the label determines the number of children.        • Map the document to a tree and replace the candidate 
     2 See Fig. 2 for an example.                                   target by uz", the distinguishing context(s) (if present) 


404                                                                                       INFORMATION EXTRACTION        by "ctx" and all other text fields by CDATA.             before these steps, the node labels of each input tree t are re•
    • Run the tree automaton; if the tree is accepted, then out• written using the function convert Jabels(t). This function 
       put the original text of the node labeled with x.        (Algorithm 2) rewrites node labels. The label v of the root of 
                                                                a subtree is changed into vx if that subtree contains a "x" and 
  3.1 Local unranked tree automaton inference 
                                                                into vctx if that subtree contains a "ctx". The effect is that the 
        algorithm                                               special labels are remembered up to the root3. 
  Preliminaries. A partition of a set 5 is a set of dis•
  joint nonempty subsets of S such that the union of 
  the subsets is S. The height of a tree is defined 
  as:1 + 
                                                       then 

  the ti are called subtrees oft, and subtrees of ti are also sub•
  trees of t. When a tree t is "cut off" at level k, this means all 
  subtrees at level k become leaves. Thus the height of the cut-
  off tree can be at most k. Given a tree t, the set of roots 
  is the singleton containing t cut off at A;; the set of forks 
  contains all subtrees of t of height at least k, cut off at k\ and 
  the set ofsubtrees contains all subtrees oft of height at 
  most k. Roughly, these sets respectively collect all subtrees 
  of height k at the top, in the middle, and at the bottom oft. 
  See [Rico-Juan et al., 2000] for a formal definition. 


                                                                  Next, the states are collected (the 1-root, the 1-subtrees, 
                                                                and the 1-roots of the forks) in Q, the transitions are initial•
                                                                ized with one transition for each 1-subtree and the 2-forks 
                                                                are partitioned according to the label of the forks' root. The 
                                                                latter results in a set of pairs (v, Str) with Str a set of se•
                                                                quences, each sequence being the children of a fork. E.g., 
                                                                {a, {(b,c), (b,c, c)}} represents two forks with root label a. 
                                                                  In the second for-loop, the k.contextual algorithm (Al•
                                                                gorithm 3) [Muggleton, 1990; Ahonen, 1996] is used to learn 
                                                                a deterministic finite automaton (DFA) that can be used as the 
                                                                representation of the regular expressions e to be used in the 
                                                                transitions v(e) -> q of the UTA. As an illustration, consider 
                                                                an input string ab for k = 3. The value of ind 
                                                                one obtains -¥ 
  The algorithm. We have designed a procedure, which is                        , where # is a distinguished label that is not 
  shown in Algorithm 1, to learn an unranked tree automaton     in . It is capable of identifying in the limit any k-contextual 
  from a set T of positive examples. The inferred automaton is  string automaton, a subset of the finite automata, from posit•
  "local" in the sense that it identifies a tree with its 2-forks as ive examples only. In principle, we could use any string auto•
  defined above. Note that DTDs for XML do the same [Murata     maton inference algorithm for this purpose (see e.g. [Murphy, 
  et al,, 2001], In addition to the input T, it takes as additional 1996; Parekh and Honavar, 1998] for others). We choose 
  parameter a positive integer which is the parameter           the k.contextual algorithm because it is efficient, simple, and 
  for the k.contextual subroutine that it calls.                works well in practice. 
    In a first for-loop, our algorithm collects all 2-forks, 1-
  subtreesand 1-roots. The latter become final states. However,    As a result, the tree automaton is not purely local. 


INFORMATION EXTRACTION                                                                                             405      An element (v, Str) of the partition gives all positive ex• Moreover, the results obtained so far [Muslea et al., 1999; 
  amples for a particular label v. The regular expression e cap• Hsu and Chang, 1999] indicate that they are difficult tasks. In 
  tures the regularity in these examples (the document content  fact one of the authors in [Muslea et al., 1999] has tried to 
  model in XML terminology). To obtain sufficient generaliz•    build a hand-crafted extractor given all available documents 
  ation, we decided, after some experimentation, to distinguish from the QS dataset and achieved only 88% accuracy (or re•
  three cases. If all children of a vx node are long enough,    call in our criteria below). We also test our method on a signi•
  we construct a DFA using the k-contextual algorithm with      ficantly reduced Shakespeare XML dataset (available online 
  k-value kc, otherwise with value 2. For other labels (either  from http://www.ibiblio.org/bosak/). We use the same train•
  a Vetx label or an original label), we ignore the content of  ing and test set as in [Kosala et al., 2002b]. The task on this 
  the children and accept any sequence (the regular expression  dataset is to extract the second scene from one act in a partic•
                                                                ular play. 
    Using a result of [Muggleton, 1990], it is quite straight-     We apply the commonly used criteria of IE research for 
  forward to make an incremental version of Algorithm 1. We     evaluating our method. Precision P is the number of cor•
  omit it here due to lack of space. Using a basic result of    rectly extracted objects divided by the total number of extrac•
  [Angluin, 1980], we can prove:                                tions, while recall R is the number of correct extractions di•
  Theorem 1 Every unranked tree language that is definable      vided by the total number of objects present in the answer 
  by a local unranked tree automaton with k-contextual regular  template. The Fl score is defined as 2PR/(P + R), the 
  expressions is identifiable in the limit, from positive examples harmonic mean of P and R. Table 1 shows the results we 
  only, by our algorithm.                                       obtained as well as those obtained by some current state-of-
                                                                the-art string-based methods: an algorithm based on Hidden 
                                                                Markov Models (HMMs) [Freitag and McCallum, 1999], the 
                                                                Stalker wrapper induction algorithm [Muslea et al., 2001 ] and 
                                                                BWI [Freitag and Kushmcrick, 2000]. We also include the 
                                                                results of the k-testable algorithm in [Kosala et al., 2002b] 
                                                                which works on ranked trees. The results of 11MM, Stalker 
                                                                and BWI are taken from [Freitag and Kushmerick, 2000]. All 
                                                                tests are performed with 10-fold cross validation following 
                                                                the splits used in [Freitag and Kushmerick, 2000], except in 
                                                                the small Shakespeare dataset which uses 2-fold cross valida•
                                                                tion. Each split has 5 documents for training and 5 for testing. 
                                                                We refer to Section 5 for a description of these methods. 
                                                                  Table 1 shows the best results of the unranked method with 
                                                                a certain kc (cross-validation on one fold of 50% random 
                                                                training and test examples.) As can be seen, our method is 
                                                                the only one giving optimal results. 

                                                                  Table 2 shows the value of k and kc used respectively 
                                                                by the k-testable and the unranked algorithm. It is well-
                                                                known that when learning from positive examples only, there 
                                                                is a problem of over-generalization. Our algorithm re•
                                                                quires a cross-validation on the value of to avoid over-
  4 Experimental results                                        generalization. 
  We evaluated our method on two semi-structured data             Algorithm 1 runs in time , where n is the total num•
  sets commonly used in IE research (available online from      ber of nodes in the training examples and c is a constant. It 
  http://www.isi. edu/-muslea/RISE/): a collection of web pages takes an average time between 19 and 26 ms in a 1.7 Ghz 
  containing people's contact addresses (the Internet Address   Pentium 4 PC for Algorithm 1 to learn an example in the IAF 
  Finder (IAF) database) and a collection of web pages about    and QS datasets. 
  stock quotes (the Quote Server (QS) database). For each 
  dataset, there are two tasks; they are the extraction of al•  5 Related work 
  ternative and organization fields in the IAF dataset and of 
  the date and volume fields in the QS dataset. Each data-      The IE work for (semi-) structured texts can be divided into 
  set consists of 10 documents. The number of fields to be      systems built manually using a knowledge engineering ap•
  extracted is respectively 94 (IAF-organization), 12 (IAF-     proach, e.g. [Hammer et al., 1997] and systems built (semi-) 
  alt.name), 24 (QS-date), and 25 (QS-vol). We choose these     automatically using machine learning techniques or other al•
  datasets because they are benchmark datasets that are com•    gorithms. The latter are called wrapper induction methods. 
  monly used for research in IE; hence they allow us to com•    We briefly survey them. 
  pare results. In order to provide a close comparison, we        The three systems referred to in Table 1 learn wrappers 
  use the same train and test splits as in [Freitag and Kush-   based on regular expressions. BWI [Freitag and Kushmerick, 
  merick, 2000]. In addition, they require the extraction of a  2000] uses a boosting approach in which the weak learner 
  whole leaf node (our algorithms are designed for that task).  learns a simple regular expression with high precision but low 


406                                                                                        INFORMATION EXTRACTION   recall. The HMM approach [Freitag and McCallum, 1999]           The most apparent limitation of our method is that it can 
  learns a hidden Markov model; it solves the problem of es•    only output a whole text node. To overcome this, it could be 
  timating probabilities from sparse data by using a statistical extended with a second step where string based methods are 
  technique called shrinkage. This model has been shown to      used to extract part of the text node. For example, to extract 
  achieve state-of-the-art performance on a range of IE tasks.  the substring "the web" from the whole string "Data on the 
  Stalker [Muslea et al, 2001] induces extraction rules that    web" (Fig. 2). Another limitation is that our method only 
  are expressed as simple landmark grammars, a class of fi•     output a single field (slot) in one run. 
  nite automata; it performs hierarchical extraction guided by a  Finally, a disadvantage that is not apparent from the res•
  manually built embedded catalog tree that describes the struc• ults reported above, is that when the identification of target 
  ture of the fields to be extracted.                           fields does not require dependencies between nodes in the tree 
    Several techniques based on naive-Bayes, two regular lan•   but can rely on a local pattern (e.g., the field to be extracted 
  guage inference algorithms, and their combinations for IE     is always surrounded by specific delimiters), our tree based 
  from unstructured texts are described in [Freitag, 1997].     method needs more examples to learn the same extraction 
  WHISK fSoderland, 1999] learns extraction rules based on      rule as methods that automatically focus on local patterns. In•
  a form of regular expression patterns with a top-down rule in• tuitively, more variations in further-away nodes need to be ob•
  duction technique. [Chidlovskii et ai, 2000] describe an in•  served before these variations are considered irrelevant. This 
  cremental grammar induction approach; they use a subclass     is simply an instance of the well-known trade-offbetween the 
  of deterministic finite automata that do not contain cyclic pat• generality of a hypothesis space and the efficiency with which 
  terns. The SoftMcaly system [Hsu and Dung, 1998] learns       the correct hypothesis can be extracted from it. 
  separators that identify the boundaries of the fields of interest. Some other approaches that exploit the structure of the doc•

  iHsu and Chang, 1999] propose two classes of SoftMealy ex•    uments are: WL2 [Cohen et al, 2002], a logic-based wrapper 
  tractors: single pass, which is biased for tabular documents  learner that uses multiple (string, tree, visual, and geometric) 

  such as QS data (they reach up to 97% recall), and multi      representations of the HTML documents. In fact, WL2 is able 
  pass, which is biased for tagged-list document such as IAF    to extract all four tasks in the IAF and QS datasets with 100% 
  data (they reach up to 57% recall). We cannot really compare  recall; and wrappers [Sakamoto et al, 2002] that identify a 
  results because the experimental setting is different.        field with a path from root to leaf, imposing conditions on 
    All above methods use algorithms for learning string lan•   each node in the path. 
  guages and require some manual intervention. HMMs and 
  BWI require to specify a window length for the prefix, suffix 6 Conclusion 
  and the target fragments. Stalker and BWI require to spe•
  cify special tokens or landmarks such as or ";". Soft•        We have presented an algorithm for the inference of a local 
  Mcaly extractors in [Hsu and Chang, 1999] requires to choose  unranked tree automaton with k-contextual regular expres•
  between single and multi pass bias.                           sions and have shown that it can be used for IE from struc•
    In [Kosala et al, 2002b], the document is converted in a    tured documents. Our results confirm the claim of [Kosala et 
  ranked (binary) tree and an algorithm is used that induces a  al, 2002b] that utilizing the tree structure of the documents is 
  k-testable tree automaton. However, as binarisation increases worthwhile for structured IE tasks. Whereas the latter work 
  the distance between target and distinguishing context, large transforms the positive examples into binary ranked trees, we 
  k are needed and the resulting automaton is precise but does  use them directly as unranked trees. Our results are optimal 
  not generalize enough (Table 1). In [Kosala et al, 2002a],    for the previously considered benchmarks, substantially im•
  the same authors generalize the obtained automaton by se•     proving upon the published results of other string and tree 
  lectively introducing wild-card labels. This gives some mod•  based methods and are a strong indication that unranked tree 
  est improvement in recall but does not solve the problem. Our automata are much better suited than ranked ones for struc•
  unranked tree automaton induction algorithm does.             tured IE tasks. 


INFORMATION EXTRACTION                                                                                              407 