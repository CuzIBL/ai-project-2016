                      Locating Complex Named Entities in Web Text

                        Doug Downey, Matthew Broadhead, and Oren Etzioni
                                             Turing Center
                           Department of Computer Science and Engineering
                                       University of Washington
                                              Box 352350
                                        Seattle, WA 98195, USA
                             {ddowney,hastur,etzioni}@cs.washington.edu

                    Abstract                          word in the name is capitalized (e.g., “United Kingdom”) but
                                                      others are far more difﬁcult. Consider the two phrases:
    Named Entity Recognition (NER) is the task of
    locating and classifying names in text. In previous (i) ...companies such as Intel and Microsoft...
    work, NER was limited to a small number of pre-    (ii) ...companies such as Procter and Gamble...
    deﬁned entity classes (e.g., people, locations, and The ﬁrst phrase names two entities (“Intel” and “Mi-
    organizations). However, NER on the Web is a far  crosoft”), whereas the second names a single entity (“Procter
    more challenging problem. Complex names (e.g.,    and Gamble”). However, this distinction is not readily appar-
    ﬁlm or book titles) can be very difﬁcult to pick out ent from the text itself. Similar situations occur in ﬁlm titles
    precisely from text. Further, the Web contains a  (“Dumb and Dumber”), book titles (“Gone with the Wind”),
    wide variety of entity classes, which are not known and elsewhere (the “War of 1812”).1 The challenging prob-
    in advance. Thus, hand-tagging examples of each   lem is to precisely determine the boundaries of the entity
    entity class is impractical.                      names in the text. We refer to this problem as the entity de-
    This paper investigates a novel approach to the   limitation problem. Since entity delimitation does not deter-
    ﬁrst step in Web NER: locating complex named      mine the entity’s class, it is a subproblem whose solution is
    entities in Web text. Our key observation is that necessary but not sufﬁcient for successful NER.
    named entities can be viewed as a species of multi- Another fundamental challenge for standard, supervised
    word units, which can be detected by accumulat-   NER techniques is that the set of entity classes is not known
    ing n-gram statistics over the Web corpus. We     in advance for many Web applications, including information
    show that this statistical method’s F1 score is 50% extraction [Etzioni et al., 2005], question answering [Banko,
    higher than that of supervised techniques includ- 2002], and search [Pasca, 2004]. Thus, it is impractical to
    ing Conditional Random Fields (CRFs) and Con-     hand tag elements of each entity class to train the supervised
    ditional Markov Models (CMMs) when applied        techniques. Instead, we are forced to create a training cor-
    to complex names. The method also outperforms     pus where entities of any type are labeled “entity”, and non-
    CMMs and CRFs by 117% on entity classes ab-       entities are labeled as such. This solution is problematic be-
    sent from the training data. Finally, our method  cause NER techniques rely on orthographic and contextual
    outperforms a semi-supervised CRF by 73%.         features that can vary widely across entity classes. We refer
                                                      to this problem as the unseen classes problem.
                                                        This paper introduces the LEX method, which addresses
1  Introduction                                       both the delimitation and unseen classes problems by utiliz-
Named Entity Recognition (NER) is the task of identifying ing n-gram statistics computed over a massive Web corpus.
and classifying names in text. In previous work, NER was LEX does not address the entity classiﬁcation problem. LEX
carried out over a small number of pre-deﬁned classes of is based on the observation that complex names tend to be
entities—for example, the named entity task deﬁnition from multi-word units (MWUs), that is, sequences of words whose
the Message Understanding Conference identiﬁed the three meaning cannot be determined by examining their constituent
classes PERSON, ORGANIZATION, and LOCATION     [Gr-   parts [da Silva et al., 2004]; MWUs can be identiﬁed with
ishman and Sundheim, 1996]. However, the Web contains a high accuracy using lexical statistics, the frequency of words
variety of named entities that fall outside these categories, in- and phrases in a corpus [da Silva and Lopes, 1999].
cluding products (e.g., books, ﬁlms), diseases, programming 1While uncapitalized words within entities tend to come from a
languages, nationalities, and events, to name a few.  small set of terms (e.g. “of,” “and,” etc.), it is not sufﬁcient to simply
  NER requires ﬁrst locating entity names in text. Some ignore these terms as they often appear between distinct entities as
named entities are easy to detect in English because each well (as in “Microsoft and Intel” above).

                                                IJCAI-07
                                                  2733  LEX  requires only a single threshold that is easily esti- More formally, the lexical method, denoted by LEX,isde-
mated using a small number of hand-tagged examples drawn ﬁned by two thresholds τ and δ, and a function f(s0,s1,s2)
from a few entity classes. However, LEX does require an un- mapping three strings to a numerical value. The function
tagged corpus that contains the relevant lexical items a hand- f(s0,s1,s2) measures the degree to which the concatenated
ful of times in order to compute accurate lexical statistics. string s0s1s2 occurs “more than chance” – several such col-
Thus, LEX is well-suited to the Web where entity classes are location identiﬁers exist in the MWU literature, and we detail
not known in advance, many entity names are complex, but a the two measures we experiment with below. The threshold
massive untagged corpus is readily available.         τ is the value f(s0,s1,s2) must exceed in order for the string
  LEX is a semi-supervised learning method, so in addition s0s1s2 to be considered a single name. Lastly, if a capitalized
to Conditional Random Fields (CRFs) [Lafferty et al., 2001] word’s appearances come at the beginning of a sentence more
and Conditional Markov Models (CMMs) [McCallum et al., than δ of the time, we assume it is not a name. We heuristi-
2000], we compare it with CRFs augmented with untagged cally set the value of δ to 0.5, and set τ using training data as
data using co-training and self-training. We ﬁnd that LEX described in Section 3.
outperforms these semi-supervised methods by more than  Given a sentence S, a function f(s0,s1,s2), and thresh-
70%.                                                  olds τ and δ,LEX proceeds as follows:
  Our contributions are as follows:                     1. Initialize a sequence of names N =(n0,n1,...,nM )
  1. We introduce the insight that statistical techniques for equal to the maximal contiguous substrings of S that
    ﬁnding MWUs can be employed to detect entity names     consist entirely of capitalized words, where elements of
    of any class, even if the classes are unknown in advance. N are ordered from left to right in S. If when the ﬁrst
  2. We demonstrate experimentally that LEX, a surpris-    word of S appears capitalized in the corpus, it is at the
    ingly simple technique based on the above insight, sub- beginning of a sentence more than δ of the time, omit it
    stantially outperforms standard supervised and semi-   from N.
    supervised approaches on complex names rarely con-  2. Until all consecutive name pairs in N have been evalu-
    sidered in NER such as the titles of ﬁlms and books.2  ated:
                                                                                                (       )
  3. We show preliminary results demonstrating that LEX     (a) Choose the unevaluated pair of names ni,ni+1
                                                               with minimum i.
    can be leveraged to improve Web applications, reducing        (          )
    the error rate of a Web information extraction system by (b) If f ni,wi,ni+1 >τ, where wi is the uncapital-
    60-70% in one case.                                        ized phrase separating ni and ni+1 in S, and wi is
                                                               at most three words in length,
  4. Based on an analysis of the limitations of LEX, we pro-
                                                               • replace ni and ni+1 in N with the single name
    pose an enhanced algorithm (LEX++) which attaches
                                                                 niwini+1.
    common preﬁxes and sufﬁxes to names, and uses addi-
    tional lexical statistics to detect spurious collocations. 2.1 Measures of Collocation
    LEX++ offers an additional 17% performance improve-
    ment over LEX.                                    The function f(s0,s1,s2) is intended to measure whether a
                                                      string s0s1s2 occurring in text is a single name, or if in-
  Section 2 describes LEX. Section 3 presents our experi-
                                                      stead s0 and s2 are distinct names separated by the string
mental results comparing LEX with previous supervised and
                                                      s1. The functions f we consider are based on two standard
semi-supervised methods. Section 4 presents an analysis
                                                      collocation measures, Pointwise Mutual Information (PMI)
LEX’s limitations, along with an experimental evaluation of
                                                      and Symmetric Conditional Probability (SCP) [da Silva and
LEX++. Section 5 considers related work, and the paper con-
                                                      Lopes, 1999]. Deﬁne the collocation function ck(a, b) for two
cludes with a discussion of future work.
                                                      strings a and b:
                                                                                 p(ab)k
2  The  LEX  Method for Locating Names                                 ck(a, b)=                      (1)
                                                                                p(a)p(b)
The lexical statistics-based approach we use for locating
                                                               ( )
names is based on the key intuition that names are a type of where p s is the probability of the string s occurring in
multi-word unit (MWU). Broadly, our algorithm assumes that the corpus.
contiguous capitalized words3 are part of the same name, and In the above, c2 gives the SCP between a and b, and c1
                                                                                        4
that a mixed case phrase beginning and ending with capital- is (for our purposes) equivalent to PMI. PMI and SCP are
ized words is a single name if and only if it is a MWU—that deﬁned for only two arguments; a variety of techniques have
is, if it appears in the corpus sufﬁciently more frequently than been employed to extend these binary collocation measures
chance. Capitalized words at the start of sentences are only to the n-ary case, for example by averaging over all binary
considered names if they appear capitalized sufﬁciently often partitions [da Silva and Lopes, 1999] or by computing the
when not at the beginning of a sentence.              measure for only the split of highest probability [Schone and
                                                      Jurafsky, 2001]. Here, we generalize the measures in the fol-
  2All of the above techniques achieve close to 100% accuracy on lowing simplistic fashion:
simple names.
  3                                                      4
   Here, “capitalized,” words are those beginning with a capital More precisely, PMI(a, b)=log2c1(a, b). But in MWU iden-
letter, and all other text (including punctuation and digits) is “un- tiﬁcation we consider only the ordering of the collocation measure,
capitalized.”                                         so the monotonic log function can be ignored.

                                                IJCAI-07
                                                  2734                           p(abc)k                                    F1            Recall  Precision
              gk(a, b, c)=                      (2)
                         p(a)p(b)p(c)                      LEX-PMI    0.38            0.43      0.34
                                                           LEX-SCP    0.63 (66%)      0.66      0.59
  We take g1 as our PMI function for three variables, and g3
as our SCP function for three variables.              Table 1: Performance of LEX using collocation measures
                                                      PMI and SCP. SCP outperforms PMI by 66% in F1.
3  Experimental Results
This section describes our experiments on Web text, which left and right boundaries. Precision is deﬁned as the fraction
compare LEX’s performance with both supervised and semi- of putative entities suggested by the algorithm that are indeed
supervised learning methods. Experiments over the Web cor- bona ﬁde entities. Because our test set contains only a proper
pus can be tricky due to both scale and the diversity of entity subset of the entities contained in the test text, for the pur-
classes encountered, so we begin by carefully describing the pose of computing precision we ignore any entities identiﬁed
experimental methodology used to address these difﬁculties. by the algorithm that do not overlap with an element of the
                                                      test set.
3.1  Experimental Methodology
Many classes of named entities are difﬁcult to delimit pre- 3.2 PMI versus SCP
cisely. For example, nested entities like the phrase “Director Our ﬁrst experiment investigates which of the candidate col-
of Microsoft” could be considered a single entity (a job ti- location functions (PMI or SCP) is most effective for use in
tle), two entities (a job title and a company), or both. To the LEX method. In these experiments and those that follow,
make evaluation as objective as possible, we performed our the threshold τ is set to the value that maximizes performance
experiments on entities from four classes where the bound- over the training set, found using simple linear search.
aries of names are relatively unambiguous: (Actor , Book, The performance of each metric on difﬁcult cases is shown
Company, and Film).                                   in Table 1. The SCP metric outperforms PMI by a consider-
  The LEX method requires a massive corpus in order to ac- able margin. This performance difference is not merely a con-
curately compute its n-gram statistics. To make our experi- sequence of our method for acquiring thresholds; the perfor-
ments tractable, we approximated this corpus with 183,726 mance discrepancy is essentially unchanged if each threshold
sentences automatically selected as likely to contain named is chosen to maximize performance on the test set. Further,
entities from the above four classes.5                although we are focusing on difﬁcult cases, the SCP metric
  We formed a training set containing a total of 200 sen- also outperforms PMI on the non-difﬁcult cases as well (0.97
tences, evenly distributed among the four classes. In these vs. 0.92 in F1).
training sets, we manually tagged all of the entities,6 result- The large performance difference between LEX-PMI and
ing in a total of 805 tagged examples. We formed a test set LEX-SCP is a more extreme version of similar results exhib-
by hand-tagging a sample of 300 entities from each class; be- ited for those measures in MWU identiﬁcation [Schone and
cause the lexical approach requires estimates of the probabili- Jurafsky, 2001]. In our case, PMI performs poorly on our
ties of different phrases, we ignored any entities that appeared task due to its well-known property of returning dispropor-
fewer than ﬁve times in our corpus. To ensure that this was a tionately low values for more frequent items [Manning and
reasonable restriction, we queried Google and found that over Schutze,¨ 1999]. PMI fails to correctly join together several of
98% of our original test entities appeared on at least 5 distinct the more frequent names in the test set. In fact, of the names
Web pages. Thus, the requirement that entities appear at least that PMI fails to join together, 53% appear in the corpus more
ﬁve times in the corpus is a minor restriction. Given the mas- than 15 times – versus 0% for SCP.
sive size of the Web corpus, a lexical statistics method like In the remainder of our experiments the lexical method em-
LEX is likely to be effective for most entities.      ploys the SCP function and is referred to simply as LEX.
  We further separated our test set into “easy” and “difﬁ-
cult” cases. Precisely, the easy cases are those that are cor- 3.3 Comparison with NER approaches
rectly identiﬁed by a baseline method that simply ﬁnds maxi- We experimented with the following NER methods:
mal substrings of contiguous capitalized words. The difﬁcult • SVMCMM – a supervised Conditional Markov Model
cases are those on which this simple baseline fails. Our ﬁ- trained with a probabilistic Support Vector Machine.
nal test set contained 100 difﬁcult cases, and 529 easy cases, • CRF – a supervised Conditional Random Field ap-
suggesting that difﬁcult entity names are quite common on  proach.
the Web.                                                 • CAPS  – a baseline method that locates all maximal con-
  In the test phase, we ran each of our methods on the sen- tiguous substrings of capitalized words.
tences from the test set. Recall is deﬁned as the fraction of test • MAN – an unsupervised name recognizer, based on
set entities for which an algorithm correctly identiﬁes both the manually-speciﬁed rules. This method is taken from the

  5                                                        name location subcomponent of the KnowItAll Web in-
   We downloaded sentences from the Web matching patterns like formation extraction system [Etzioni et al., 2005].
“actors such as...” or “...and other ﬁlms,” etc.
  6We also constructed training sets tagging only those entities of Both Conditional Random Fields (CRF) [Lafferty et al.,
the target classes (Actor, Book, Company, and Film) – however, this 2001] and Conditional Markov Models (CMM) [McCallum
approach decreased the performance of the supervised approaches. et al., 2000] are state of the art supervised approaches for

                                                IJCAI-07
                                                  2735                 F1          Recall  Precision                         F1           Recall  Precision
     SVMCMM      0.42          0.48      0.37              SVMCMM      0.29           0.34      0.25
     CRF         0.35          0.42      0.31              CRF         0.25           0.31      0.21
     MAN         0.18          0.22      0.16              MAN         0.18           0.22      0.16
     LEX         0.63 (50%)    0.66      0.59              LEX         0.63 (117%)    0.66      0.60

Table 2: Performance on Difﬁcult Cases LEX’s F1 score is Table 4: Performance on Unseen Entity Classes (Difﬁcult
50% higher than the nearest competitor, SVMCMM.       Cases) LEX outperforms its nearest competitor (SVMCMM)
                                                      by 117%.
                 F1          Recall  Precision                         F1           Recall  Precision
     SVMCMM      0.96          0.96      0.96              SVMCMM      0.93           0.92      0.94
     CRF         0.94          0.94      0.95              CRF         0.94           0.93      0.95
     MAN         0.97          0.96      0.98              MAN         0.97           0.96      0.98
     LEX         0.97          0.97      0.97              LEX         0.95           0.95      0.95
     CAPS        1.00 (3%)     1.00      1.00              CAPS        1.00 (3%)      1.00      1.00

Table 3: Performance on Easy Cases All methods perform Table 5: Performance on Unseen Entity Classes (Easy
comparably near the perfect performance of the CAPS base- Cases) CAPS outperforms all methods by a small margin,
line; CAPS outperforms LEX and MAN by 3%.             performing 3% better than its nearest competitor (MAN).

text delimitation tasks. Our CRF and CMM implementations learned value of τ by a factor of 2 in either direction resulted
were taken from the MinorThird text classiﬁcation package in LEX still outperforming SVMCMM by at least 40% on dif-
[Cohen, 2004]. We obtained settings for each algorithm via ﬁcult cases. Also, altering the value of δ to either 0.4 or 0.6
cross-validation on the training set. We chose to train the resulted in LEX outperforming SVMCMM by at least 48%
CMM   using a Support Vector Machine (SVMCMM) rather  on difﬁcult cases.
than with the maximum entropy approach in [McCallum et
al., 2000] because the SVMCMM substantially outperformed 3.4 Unseen Entity Classes
the maximum-entropy classiﬁer in cross-validation. The su- As argued in the introduction, in many Web applications, the
pervised classiﬁers create feature vectors for each word; these target entity classes are not known in advance. This poses
features include the lower-case version of the word, the ortho- particular problems for standard NER approaches, which rely
graphic pattern for the word (e.g. “Xx+” for “Bill” or “9+” on textual cues that can vary widely among entity classes. In
for “1995”, etc), and analogous features for tokens in a small this experiment, we simulate the performance of each of our
window around the word (of size selected by cross valida- entity delimitation methods on unseen entity classes. Specif-
tion).                                                ically, when evaluating performance on an entity class, we
  By deﬁnition, the CAPS baseline performs perfectly on remove from the training set any sentences containing enti-
easy entity names, but does not correctly delimit any difﬁcult ties of that class. The results of these experiments on difﬁcult
entity names. CAPS is equivalent to Step 1 in the LEX algo- cases are shown in Table 4. LEX outperforms the other ap-
rithm in Section 2. Although LEX includes CAPS as an initial proaches, with F1 performance more than 100% higher than
step to handle easy names, the two algorithms will differ on the best competing method (SVMCMM). The precision and
easy names when the lexical statistics, computed in Step 2 of recall differences between LEX and SVMCMM on difﬁcult
LEX, indicate that two easy names delimited by CAPS ought cases are each statistically signiﬁcant at p<0.001 (Fisher
to be concatenated. Consider, for example, the phrase “...as Exact Test). As in the previous experiment, all methods per-
Intel and Microsoft have...”. CAPS will always delimit this form relatively well on easy cases (Table 5).
phrase’s two names correctly, but LEX could potentially de-
limit them incorrectly (by, for example, outputting the phrase 3.5 Semi-supervised Comparison
“Intel and Microsoft” as a single name). In practice, this sit- LEX outperforms supervised NER methods by leveraging
uation occurs infrequently (see Table 3).             lexical statistics computed over a large, untagged corpus.
  The performance of the different methods on difﬁcult cases Augmenting tagged examples with untagged data is known as
is shown in Table 2. Overall, LEX’s F1 score of 0.63 is semi-supervised learning, an increasingly popular approach
50% higher than that of its nearest competitor (SVMCMM, for text processing tasks, in which untagged data is plenti-
at 0.42). The precision differences between LEX and SVM- ful. In this section, we compare LEX with previous semi-
CMM are signiﬁcant at p<0.01, and the recall differences supervised algorithms.
are signiﬁcant at p<0.02 (Fisher Exact Test). Of course, To create a semi-supervised baseline for our task, we aug-
performance on the easy cases is also important. As shown in mented the supervised classiﬁers from our previous experi-
Table 3, all methods perform comparably well on easy cases, ments to employ untagged data. As in other recent work on
with F1 scores in the 0.95 and higher range.          semi-supervised text segmentation [Ando and Zhang, 2005],
  The performance of LEX is fairly robust to parameter we experimented with two semi-supervised approaches: co-
changes. A sensitivity analysis revealed that altering the training and self-training.

                                                IJCAI-07
                                                  2736  One of the ﬁrst semi-supervised algorithms for named en-  Self-tagged
tity classiﬁcation [Collins and Singer, 1999] was based on  Sentences    F1     Recall   Precision
semi-supervised co-training [Blum and Mitchell, 1998]. Co-  0           0.35      0.42        0.31
training relies on a partition of the feature set to produce in- 400    0.37      0.43        0.32
dependent “views” of each example to be classiﬁed, where it 800         0.36      0.42        0.31
is assumed each individual view is sufﬁcient to build a rela-
tively effective classiﬁer. In co-training, separate classiﬁers Table 6: Performance of semi-supervised CRF (Difﬁcult
are trained for each view. Then each classiﬁer is applied to Cases) The additional training sentences generated by the
the untagged examples, and the examples whose classiﬁca- semi-supervised algorithm (“Self-tagged Sentences”) do not
tions are most certain are added as new training examples. signiﬁcantly improve performance over the original 200 sen-
By iteratively bootstrapping information from distinct views, tences from the training set.
co-training algorithms are able to create new training exam-
ples from untagged data.                                  0.7
  The co-training approach developed in [Collins and Singer, 0.6
1999] does not actually perform entity delimitation. Instead,
it assumes the entity delimitation problem is solved, and ad- 0.5
dresses the task of classifying a given set of entities into cat- 0.4
egories. In our attempts to utilize a similar co-training al-
                                                          0.3
gorithm for our entity delimitation task, we found the algo-                                 LEX
rithm’s partitioning of the feature set to be ill suited to our Precision 0.2
task. The approach in [Collins and Singer, 1999] employed                                    SVMCMM
two views to classify named entities: the context surround- 0.1                              CRF
ing a named entity, and the orthographic features of the en-                                 MAN
tity itself. While the context surrounding a known entity is 0
a good indicator of that entity’s class, in our experiments we 0204060
found that context was not effective for determining an un-                    Recall
known entity’s boundaries. The co-training classiﬁer trained
on our training set using only the “context” view achieved F1 Figure 1: Information Extraction performance for the
performance of 0.16 on difﬁcult cases, and 0.37 on easy cases Film class (Difﬁcult Cases). LEX’s precision is 60-70%
(versus 0.35 and 0.94 for the classiﬁer trained on all features). higher than its closest competitor (SVMCMM).
This violates the central assumption in co-training, that each
view be independently capable of producing a relatively ef- traction system on our evaluation corpus, using each of our
fective classiﬁer.                                    methods to delimit entities. We ranked all extracted entities
  Because co-training was unsuited to our task, we exper- in order of decreasing frequency to obtain precision/recall
imented with “single-view” co-training, also known as self- curves for each class. Figure 1 shows the precision/recall
training. This approach involved training a supervised classi- curves for the 100 most frequent extractions of difﬁcult cases
ﬁer on the training set, and then iteratively applying the clas- in the Film class. As the graph shows, LEX enables sig-
siﬁer to the untagged data, adding to the training set the r niﬁcantly higher precision than the other methods (as before,
sentences on which the classiﬁer is most conﬁdent.    all methods perform comparably well on easy extractions).
  Our experimental results are shown in Table 6. Because Additional experiments are necessary to assess information
only the CRF classiﬁer returned reliable probability esti- extraction performance on other classes.
mates for sentences, it is the only classiﬁer tested in a semi-
supervised conﬁguration. In these experiments, r = 20, and 4 Error Analysis and Enhancements
the algorithm was executed for a total of 40 self-training it-
erations. As shown in the table, the bootstrapped training LEX is a simple algorithm, and there are ways in which its
examples make positive but insigniﬁcant impacts on the per- simplistic assumptions about the structure of names hinder its
formance of the baseline supervised classiﬁer. LEX’s F1 per- performance. In this section, we detail three of LEX’s primary
formance of 0.63 is 73% higher than that of the best semi- error types, and we introduce LEX++, an enhanced algorithm
supervised CRF in Table 6 (0.37).                     which addresses LEX’s limitations. LEX++ follows the LEX
                                                      algorithm given in Section 2, with modiﬁcations for each of
3.6  Information Extraction Performance               LEX’s error types as detailed below.
One important application of Web named entity delimitation
is found in unsupervised information extraction systems, e.g. Numbers and Punctuation at Name Boundaries
[Etzioni et al., 2005]. These systems use generic patterns LEX’s assumption that names begin and end with capitalized
(e.g. “ﬁlms such as”) to extract entities, but require some words is violated by any names that begin or end with num-
method to identify the boundaries of the entities to extract. bers or punctuation—for example, the ﬁlm “8 Mile” or a com-
Given a particular set of patterns, a more accurate name de- pany name including trailing punctuation such as “Dell Inc.”
limitation system should result in better extraction accuracy. This limitation of LEX accounted for 38% of its false nega-
In this experiment, we tested a pattern-based information ex- tives on difﬁcult cases in the experiments in Section 3.3.

                                                IJCAI-07
                                                  2737