  Extracting and Visualizing Trust Relationships from Online Auction Feedback
                                             Comments
           John O’Donovan     and  Barry Smyth           Vesile Evrim  and  Dennis McLeod
                  Adaptive Information Cluster           Semantic Information Research Laboratory
           School of Computer Science and Informatics        Department of Computer Science
                   University College Dublin                 University of Southern California
                           Ireland                                    Los Angeles
                ﬁrstname.lastname@ucd.ie                         lastname@usc.edu

                    Abstract                          2   Background Research
    Buyers and sellers in online auctions are faced with Background for this paper is in three main areas. Firstly, we
    the task of deciding who to entrust their business will examine related research in the area of trust and reputa-
    to based on a very limited amount of information. tion in online environments. The core algorithm in this paper
    Current trust ratings on eBay average over 99%    uses techniques borrowed from the ﬁeld of natural language
    positive [13] and are presented as a single num-  processing (NLP). Next we discuss relevant work in this area
    ber on a user’s proﬁle. This paper presents a sys- of NLP. Lastly, a brief examination of research on construct-
    tem capable of extracting valuable negative infor- ing and visualising social networks is presented.
    mation from the wealth of feedback comments on
    eBay, computing personalized and feature-based    2.1  Trust and Reputation on the Web
    trust and presenting this information graphically.
                                                      A large amount of recent research has examined the vary-
1  Introduction                                       ing notions of trust and reputation on the web. [7][10][13]
                                                      Marsh’s work in [7] goes some way towards formalising trust
In 2005, eBay Inc. had an annual growth rate of 42.5% and in a computational sense, taking into account both it’s so-
in February 2006 was receiving over three million user feed- cial and technological aspects, deﬁning “context-speciﬁc in-
back comments every day. This growth reﬂects the increased terpersonal trust” as the trust one individual must place in
access to online markets throughout the world. As a result of another with respect to a speciﬁc situation. This is the no-
this rapid growth rate, users ﬁnd themselves more frequently tion of trust examined in this paper. Previous work by the
in the situation where they must trust a complete stranger to authors in [10] examines trust and reputation in the context of
uphold their part of a business transaction. Traditional mech- recommender systems, wherein a user’s history of contribu-
anisms for evaluating the trust of a potential buyer or seller tions to the recommendation process was examined and used
do not apply in the online world, where in many cases the to weight future contributions.
only information provided is a buyer or seller’s username Resnick highlights some relevant points which affect the
and there are orders of magnitude more potential transactors.                     [  ]   [  ]
[  ]                                                  current eBay reputation system in 13 and 14 . Buyers repu-
14  In this paper we address the problem of providing a re- tation matter less since they hold the goods until they are paid.
liable mechanism for trusting users in online auctions. We Feedback can be affected by the person who makes the ﬁrst
argue that the current trust system on eBay is massively bi- comment, ie: feedback can be reciprocated. Retaliatory feed-
ased towards positive comments and show how AuctionRules, back and potential for lawsuits are strong disincentives for
a trust-extraction algorithm can be applied to the wealth of leaving negative comments. Anonymity is possible in eBay
freetext comments to capture subtle indications of negativity, since real names are not revealed and the only thing validated
producing a more scaled range of trust based on the sentiment at registration is an email address. Users can choose not to
found in comments. To achieve this, the algorithm classiﬁes display feedback comments. “Unpaid item” buyers cannot
freetext comments using a small list (<30 nouns) of salient leave feedback [1], and users can agree to mutually withdraw
features in the domain. Our evaluations show this algorithm feedback [1]. Furthermore eBay inc admit that negative feed-
outperforms seven popular classiﬁcation algorithms by up to back is removed from newer users of the system [1] All of
21%.                                                  these points help to explain the lack of negative comments on
  Currently, eBay displays a non-personalized trust score for
                                                      eBay. Of course this does not mean that customers are sat-
a potential transactor. We describe a trust propagation mech-              1
anism which uses output from AuctionRules to generate per- isﬁed. The eBay forums highlight the fact that false adver-
sonalized trust between two potential transactors. For demon- tising does occur on eBay. This should lead to more negative
                                                                                                     [  ]
stration purposes, we present a replica interface to a standard comments, but they are not being displayed. Xiong et al. 15
auction site in which trust between speciﬁc users and per- provide further reasoning for the imbalance of positive com-
feature trust can be computed by AuctionRules on the ﬂy and ments on eBay. Our proposed model of trust for eBay should
visualized as a social network graph with edge length and provide a more realistic scale than the existing system.
thickness as functions of both strength and goodness of the
trust relationship.                                      1http://forums.ebay.com

                                                IJCAI-07
                                                  28262.2  Classifying Natural Language Comments            isations derived from the FilmTrust movie ratings website in
                                                      [4].
For many years research effort has been dedicated to the use
of lexical techniques to classify free responses in areas rang-
ing from online auctions [5] to movies [11] and for correct- 3 Extracting Trust From Feedback
ing students short-answer exams. [2]. Processing of freetext Comments
comments requires four main components, morphology (pre- To address the problem of unnaturally high trust ratings on
ﬁxes and sufﬁxes), syntax checking, deciding on a semantic eBay, we look to the freetext comments and apply a clas-
meaning and ﬁnally, taking some action based on that mean- siﬁcation algorithm tailored for capturing subtle indications
ing. In this paper we are especially interested in the clas- of negativity in those comments. The situation arises fre-
siﬁcation of comments with negative sentiment. Yukari et quently where users are afraid to leave a negative comment
al. [6] proposed a method to extract potentially unsatisﬁed for fear of retaliatory feedback comments which could dam-
customers by applying text mining to data from a customer age their own reputation. [13]. In many of these cases, a pos-
satisfaction survey. This work introduced the idea of salient itive feedback rating is made, but the commenter still voices
satisfaction factors as a mechanism for classifying negative some grievance in the freetext comment. This is the type of
comments. Later in this paper a feature-based trust metric is subtle but important information the AuctionRules algorithm
introduced, which was inspired by work in [6]. Pang et al. attempts to extract.
attempt to classify freetext documents according to sentiment
in [11], highlighting bad performance of machine learning al- 3.1 The AuctionRules Algorithm
gorithms such as naïve bayes and support vector machines in AuctionRules is a classiﬁcation algorithm with the goal of cor-
the task of sentiment classiﬁcation. Pang et al. conclude that rectly classifying online auction comments into positive or
classiﬁcation of free text according to sentiment is a challeng- negative according to a threshold. AuctionRules capitalizes
ing problem. Gamon et al address the same sentiment classi- on the restrictive nature of online markets: there are a limited
ﬁcation problem in [3], using data from car reviews. Gamon number of salient factors that a user (buyer or seller) is con-
et al. also applied machine learning techniques to this task cerned about. This is reﬂected in feedback comments. We
and found poor performance. They developed a tailor-made deﬁne a set of seven core feature sets for which the algorithm
clustering algorithm which uses a small amount of domain will compute granular trust scores. The following sets have a
knowledge in the form of a stop-list and go-list of features coverage of 62% of the comments in our database. The algo-
known to be salient in the domain. The AuctionRules algo- rithm can obtain semantic information from 62% of the com-
rithm described later in this paper deﬁnes small negative and ments at a ﬁne grained level. It is shown in our experimental
positive lists in a similar manner to that in [3].    analysis how we can maintain over 90% coverage using this
  Gamon et al. [3] introduce the concept of varying the level algorithm. The terms in brackets are contents of each feature
of granularity during the classiﬁcation procedure, ﬁnding that set.
"varying the level of granularity of analysis allows the dis- • Item - The quality/condition of the product being bought
covery of new information". The AuctionRules algorithm   or sold. (item, product)
can record ﬁner grained information during classiﬁcation, to •
present extra information to users.                      Person - The person the user makes the transaction with.
  Hijikata et al. [5] focus on summarization of freetext com- (buyer, seller, eBayer, dealer)
ments in online auctions, proposing a technique for analysing • Cost - Cost of item, cost of shipping, hidden costs etc.
the underlying social networks in online auctions and using (expense, cost)
this information to present a summary of comments to users. • Shipping - Delivery of the item, security, time etc. (deliv-
In this work, the social network for their test data is deﬁned by ery, shipping)
a crawler algorithm which takes all the feedback comments • Response - Communication with the other party, emails,
written by a buyer who wrote a comment for a target seller. feedback comment responses. (response, comment,
As a result of this data crawling technique it appears that the email, communication)
data collected does not represent a natural social network, and
if large portions of freetext data come from single individuals • Packaging - The packaging quality/condition of the item
there is a potential for overﬁtting during computation. A web (packaging)
application is presented in [5] where users can see comment • Payment - how the payment will be made to the seller, or
summaries represented as a set of bar charts for easy analysis. back to buyer for return (payment)
                                                       • Transaction - the overall transaction quality (service,
2.3  Construction and Visualisation of Social            transaction, business)
     Network Graphs                                     This technique enables us not only to compute a personal
As people conduct larger proportions of their daily business trust score between individual users, but also to provide more
on the web, the ability to analyse the resultant social networks granular information on a potential transactor. For example:
is becoming more important. Technologies such as FOAF "User x is very trustworthy when it comes to payment, but
(Friend-Of-A-Friend) are becoming more popular in blog- shipping has been unsatisfactory in the past", This granular or
ging communities, allowing users to directly specify their so- contextual trust draws on the wealth of information in com-
cial networks. Visualisation tools such as PieSpy [9] (used ments and can uncover hidden problems which the current
later in this paper) allow for dynamic display and fast assess- trust system on eBay might overlook.
ment of complex graphs. Mitton [9] assesses a range of so- Figure 1 details this algorithm working on a sample com-
cial networks mined from IRC channels. Golbeck uses social ment which had a positive rating on eBay. Each term in the
networks to compute trust scores and shows resulting visual- comment and up to four preceding terms are passed into an

                                                IJCAI-07
                                                  2827                                                      implementation of the Porter stemming algorithm [12]. The
                                                      standard porter stemmer uses many rules for removing suf-
                                                      ﬁxes. For example, all of the terms in c are conﬂated to the
                                                      root term “connect”. c = connect, connected, connecting,
                                                      connection, connections. This reduces the number of terms
                                                      and therefore the complexity of the data. The stemmer algo-
                                                      rithm was modiﬁed to also stem characters not used by Auc-
                                                      tionRules, such as “?, !, *, (, )” for example.
                                                        Data dimension is reduced by removal of stop-words.
                                                      Google’s stop-word list2 was used as the removal key. A fu-
                                                      ture addition to the algorithm would put an automatic spelling
                                                      corrector at this point. There were 11 different spelling occur-
                                                      rences of the word ’packaging’ for example. Each stemmed
                                                      term is compared against the stemmed terms from the feature
                                                      list. If a match is found the algorithm examines the preced-
                                                      ing term types. This is shown graphically in Figure 1. The
                                                      algorithm recognises ﬁve term types:
                                                       • nouns the words contained in the feature sets.
                                                       • adjectives (eg: ’nice’, ’good’) from a web list
                                                       • intensiﬁers (e.g ’very’, ’more’) list of 20.
                                                       • articles ’a’ or ’the’
                                                       • negators (e.g. not, ’anything but’) from a stored list.
                                                        The table on the left in Figure 1 shows the order in which
                                                      these terms are analysed. From the ﬁve terms, two can
                                                      provide semantic meaning about the feature: adjectives and
                                                      negators. If an adjective is found without a negator, it is com-
                                                      pared to an arrays 20 positive and an array of 20 negative
                                                      adjectives. If a negator is found as the second or third preced-
                                                      ing term, the process works with positive and negative arrays
                                                      switched. If a match is found, the polarity for that feature is
                                                      recorded.
                                                        If no match is found, AuctionRules uses query expansion,
                                                      by calling an interface to an online thesaurus which returns an
                                                      array of 20 synonyms. If a negator is present, the interface re-
                                                      turns an array of 20 antonyms, and these lists are compared in
                                                      a similar manner to our short lexicon of positive and negative
                                                      adjectives. The matching results are recorded in three ways:
                                                            (       )      (   )        neg
                                                      a) max pos, neg b) any neg and c) pos+neg . In the case of
                                                      (c) the polarity is recorded according to a conﬁgurable thresh-
                                                      old α. Two separate trust databases are maintained: granu-
                                                      lar or contextual trust which is the trust computed for each
                                                      feature for a user over all of the comments analysed. Equa-
                                                      tion 1 shows contextual trust t as a multi valued set of trust
                                                      scores associated with each feature. Here, f denotes a partic-
                                                      ular feature and tf n is the associated trust score. The second
                                                      trust database is interpersonal trust which is the average trust
                                                      value recorded for all features on every comment made be-
                                                      tween two users.
                                                        Of course not every comment will contain words from the
                                                      feature lists above. In cases where no features are found, the
                                                      algorithm performs a straightforward count of positive and
                                                      negative terms using the query expansion module where nec-
                                                      essary. In this manner, coverage is maintained at over 90%,
                                                      and many of the unclassiﬁable comments are foreign lan-
                                                      guage or spelling mistakes.
                                                      3.2  Personalizing Trust by Propagation
Figure 1: The AuctionRules comment classiﬁcation algo-
rithm.                                                Interpersonal trust is given by Equation 2. This is a person-
                                                      alised trust score between two users. If users have been di-

                                                         2http://www.ranks.nl/tools/stopwords.html

                                                IJCAI-07
                                                  2828Figure 2: Visualizing the trust-network generated by Auc-
tionRules. (usernames have been obfuscated at the request Figure 3: Integration of the trust system into an online auc-
of eBay inc.)                                         tion. (usernames have been obfuscated at the request of eBay
                                                      inc.)
rectly involved in a transaction, this is simply the classiﬁers
value of their transactions (a, b, or c above). Equation 2 shows visualisation output from a demonstration set of users in our
how these trust scores can be computed along a path connect- database. (this graph has much higher than average intercon-
ing the two users. In Equation 2 ⊕ represents some combina- nectedness.) The graphing tool caters for trust value and trust
tion of the scores at each node in the path between the target strength by representing value as the thickness of an edge and
and source users. Currently this can be any of the following strength of the trust-relationship as the length of a line. In the
four techniques below. The problem of combining values in graph, shorter lines mean a higher number of comments were
the trust graph is a research in itself. This will be dealt with used in the classiﬁcation. The demonstration system in Fig-
in more detail in a future work.                      ure 3 shows the trust graph popup when a mouseOver occurs
 • weightedDistance - The average trust score over all the on a username. To visualise contextual trust, a popup table
   edges in the shortest path, discounted by the distance from with feature names, associated trust scores and strengths is
   the source.                                        used. This is shown as the smaller popup in Figure 3.
 • meanPath - The average trust score over all the edges in 4 Experimental Evaluation
   the shortest path between the source node and target node.
 •                                                    We examine four factors in our evaluations. The accuracy of
   twoPathMean - The average of the meanPath of the short- the AuctionRules classiﬁer with respect to other techniques
   est path in the graph and the meanPath of the second from machine learning. We examine accuracy from a Mean
   shortest path.                                     Absolute Error perspective and by creating confusion matri-
 • SHMPath - The simple harmonic mean of the trust scores ces to examine performance with respect to false negative
   over all edges in the shortest path.               and positive classiﬁcations. As the system uses a very small
                                                      amount of domain knowledge, and a limited number of fea-
              tgranularε{tf1,tf2, ...tfn}       (1)   tures, we must examine the coverage of AuctionRules over
                                                      our comments database. Finally we make a comparison be-
                                                      tween the scale of trust achieved by AuctionRules against the
                =     ⊕    ⊕   ⊕                (2)
             ti,n  ti,j tj,k ... tm,n                 current eBay scale.
3.3  Visualisation of Trust Information               4.1  Data
                                                      Figure 4 explains the environment in which we test our algo-
Before introducing our presentation strategy, we deﬁne two
                                                      rithm. Data is crawled from the online auction site according
important metrics, ﬁrstly a scalar trust value which is rep-
                                                      to the crawler algorithm below. Importantly, unlike it’s ma-
resents trust either between two users, or for one user on a
                                                      chine learning counterparts, AuctionRules requires no knowl-
particular feature. Secondly, we deﬁne trust strength. This
                                                      edge of the comment set it has to classify. The feature lists
is based on the number of transactions between two users in
                                                      used by the algorithm are generic and should work on any set
the case of interpersonal trust, or as the number of comments
                                                      of online auction comments.
used to calculate the feature trust score in the case of contex-
tual trust.                                           Algorithm 4.1: CRAWL(String url_list, int maxbound)
  To generate on-the-ﬂy visualisations of the resultant trust
network, we modiﬁed a version of PieSpy an open source while n<maxbound
graphing tool [9] by Mutton for visualising social networks. for i ← 1 to #Items_on_page
Our interpersonal trust database is a set of triples of the form: followSellerLink();
     =(                )
e(i,j)  useri,userj,t(i,j) , where t(i,j) comes from Equa- for j ← 1 to #Comments_on_page
tion 2. This was translated into XML and fed to the graphing db.add(bId, sId, comment, sT rust, bT rust);
tool. Modiﬁcations were made to the existing code to high- n ← n +1;
light target and source users in the graph. Figure 2 shows the return ;

                                                IJCAI-07
                                                  2829                                                      Figure 5: Classiﬁcation Accuracy [classiﬁcation distribution
                                                      from user evaluations: 36% positive, 63% negative, using a
                                                      threshold of 4 or higher for a positive comment.]
                                                      learner C4.5 rules, two Bayes learners, Naive Bayes and
                                                      BayesNet and a lazy learning algorithm K-Star.
Figure 4: Graphical overview of the trust-modelling process. Figure 5 shows results of this experiment. For each algo-
(Current implementation only uses eBay as a source for rat- rithm we performed three runs. a 60:40 train-test split, an
ings.)                                                80:20 split, and a 10-fold cross validation of the training set,
                                                      which randomly selects a training set from the data over 10
  10,000 user comments were collected from the auction site runs of the classiﬁer and averages the result. In the experi-
using the crawler. As a social network visualisation was ment each algorithm made a prediction for every value in the
planned, data was collected from within a domain with highly test set, and this prediction was compared against the training
interconnected and contained repeat-purchase users. After set. AuctionRules beat all of the other classiﬁers in every test
a manual analysis of a range of sub-domains of the auction we performed, achieving over 90% accuracy in all of the eval-
site, Egyptian antiques was chosen as the data domain as it uations, 97.5% in the 80:20 test, beating the worst performer
appeared to meet the prerequisites to a reasonable degree. K-Star by 17.5%, (relative 21.2%) and it’s closest competitor
Although a large number of comments were collected, only Naive Bayes by 10.5%, giving a relative accuracy increase of
1000 were used in our experimental analysis.          12.7%.
                                                        In addition to numerical accuracy, we examined where the
User-Provided Training Data                           high accuracy results were coming from more closely by as-
In order to test any comment classiﬁcation algorithm a bench- sessing the confusion matrix output by the algorithms. This
mark set was required. 1000 comments were compiled into was necessary since prediction of false negatives would have
                                                      an adverse effect on the resulting trust graph. This phe-
an online survey3 and rated by real users. In this survey, users
                                                      nomenon has been discussed by Massa in [8] with respect to
were asked to rate the positiveness of each comment on a Lik-
                                                      the Moleskiing application, and Golbeck in [4] with respect to
ert scale of 1 to 5. 10 comments were presented to a user in
                                                      the TrustMail application. Table 4.2 shows AuctionRules out-
each session. Each comment was made by different buyers
                                                      performing all of the other algorithms by predicting no false
about one seller. Users were required to answer the follow-
                                                      negatives. This is taken as a good result since in a propaga-
ing:
                                                      tion computation negative values should contain more weight
 • How positive is the comment (Average rating: 3.8442) because of their infrequency. When a value is presented to
 • How  informative is the comment (Average rating:   a user directly however, false positives are more damaging
   3.1377)                                            for an auction environment. AuctionRulesalso performs very
 • Would you buy from this seller (Average rating: 4.0819) well for false positives with a rate of 4.5%, half that of the
                                                      closest competitor One-r. All of the algorithms displayed
Currently only results from the ﬁrst question are used to de- similar trend to the ones in Table 4.2, which shows results
velop and test AuctionRules. For future experiments we may of the 80:20 classiﬁcation experiment which had a test set of
incorporate results from the other questions. Permission was 234 comments. It was found during accuracy evaluations that
sought from eBay inc. to use the information from the eBay there was a strong correlation between the number of feature
website in our experiments.                           terms recognised and the ﬁnal accuracy of the classiﬁcation.
                                                      For our coverage experiments, we addressed the number of
4.2  Comparing Accuracy Against Machine               hits found with respect to coverage. This is detailed in the
     Learning Techniques                              following section.
To examine classiﬁcation accuracy of AuctionRules,itwas
tested against 7 popular algorithms. We chose three rule-
based learners, Zero-r, One-r, Decision Table, one tree 4.3 Coverage and Distribution Experiments
                                                      To assess the coverage of the AuctionRules feature-based
  3www.johnod.net/Surveyone.jsp                       trust calculator we examined the number of feature term hits

                                                IJCAI-07
                                                  2830