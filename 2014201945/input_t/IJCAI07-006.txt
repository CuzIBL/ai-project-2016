                             Query-driven Constraint Acquisition

  Christian Bessiere            Remi Coletta             Barry O’Sullivan            Mathias Paulin
    LIRMM-CNRS                      LRD            4C, Computer Science Dept.        LIRMM-CNRS
U. Montpellier, France       Montpellier, France           UCC, Ireland          U. Montpellier, France
  bessiere@lirmm.fr           coletta@l-rd.fr         b.osullivan@4c.ucc.ie         paulin@lirmm.fr


                    Abstract                          egy is one in which, regardless of the classiﬁcation of the
                                                      query, the size of the version space is reduced by half. There-
    The modelling and reformulation of constraint net- fore, convergence of the version space can be achieved using
    works are recognised as important problems. The   a logarithmic number of queries. Furthermore, in the clas-
    task of automatically acquiring a constraint net- sic setting, a query can be generated in time polynomial in
    work formulation of a problem from a subset of its the size of the version space. When acquiring constraint net-
    solutions and non-solutions has been presented in works, query generation becomes NP-hard. This is further
    the literature. However, the choice of such a subset aggravated by the fact that in constraint acquisition, while the
    was assumed to be made independently of the ac-   ordering over the hypothesis space is most naturally deﬁned
    quisition process. We present an approach in which in terms of the solution space of constraint networks, we usu-
    an interactive acquisition system actively selects a ally learn at the constraint level, i.e. a compact representation
    good set of examples. We show that the number of  of the set of solutions of a hypothesis. Our main contribution
    examples required to acquire a constraint network is a number of algorithms for identifying good queries for ac-
    is signiﬁcantly reduced using our approach.       quiring constraint networks. Our empirical studies show that
                                                      using our techniques the number of examples required to ac-
1  Introduction                                       quire a constraint network is signiﬁcantly reduced. This work
Constraint Programming (CP) provides a powerful paradigm is relevant to interactive scenarios where users are actively in-
for solving combinatorial problems. However, the speciﬁca- volved in the acquisition process.
tion of constraint networks still remains limited to special-
ists in the ﬁeld. An approach to automatically acquiring con- 2 Constraint Acquisition using CONACQ
straint networks from examples of their solutions and non-
                                                      A constraint network is deﬁned on a (ﬁnite) set of variables X
solutions has been proposed by [Bessiere et al., 2005]. Con-
                                                      and a (ﬁnite) set of domain values D. This common knowl-
straint acquisition was formulated as a concept learning task.
                                                      edge shared between the learner and the user is called the
The classical version space learning paradigm [Mitchell,
                                                      vocabulary. Furthermore, the learner has at its disposal a
1982] was extended so that constraint networks could be
                                                      constraint library from which it can build and compose con-
learned efﬁciently. Constraint networks are much more com-
                                                      straints. The problem is to ﬁnd an appropriate combination of
plex to acquire than simple conjunctive concepts represented
                                                      constraints that is consistent with the examples provided by
in propositional logic. While in conjunctive concepts the
                                                      the user. For the sake of notation, we shall assume that every
atomic variables are pairwise independent, in constraint sat-
                                                      constraint deﬁned from the library is binary. However, the re-
isfaction there are dependencies amongst them.
                                                      sults presented here can be easily extended to constraints of
  In [Bessiere et al., 2005] the choice of the subset of solu-
                                                      higher arity, and this is demonstrated in our experiments.
tions and non-solutions to use for learning was assumed to be             c
made before and independently of the acquisition process. In A binary constraint ij is a binary relation deﬁned on D
this paper we present an approach in which the acquisition that speciﬁes which pairs of values are allowed for variables
                                                      xi,xj. The pair of variables (xi,xj) is called the scope of cij .
system actively assists in the selection of the set of exam-     ≤                                (     )
ples used to acquire the constraint network through the use For instance, 12 denotes the constraint speciﬁed on x1,x2
of learner-generated queries. A query is essentially a com- with relation “less than or equal to”. A binary constraint net-
                                                      work is a set C of binary constraints. A constraint bias is a
plete instantiation of values to the variables in the constraint B
network that the user must classify as either a solution or non- collection of binary constraints built from the constraint li-
                                                      brary on the given vocabulary. A constraint network C is said
solution of her ‘target’ network. We show that the number of                 B                    c    C
examples required to acquire a constraint network is signiﬁ- to be admissible for a bias if for each constraint ij in
                                                                                 {b1 ··· bk }  B
cantly reduced if queries are selected carefully.     there exists a set of constraints ij, , ij in such that
                                                            1         k
  When acquiring constraint networks computing good   cij = bij ∩···∩bij .
queries is a hard problem. The classic query generation strat- An instance e is a map that assigns to each variable xi in

                                                IJCAI-07
                                                   50X a domain value e(xi) in D. Equivalently, an instance e
                           n                          Table 1: An example of the clausal representation built by
can be regarded as a tuple in D . An instance e satisﬁes a                       ?
               c           ( ( )  (  ))               CONACQ, where each example ei =(x1,x2,x3,x4).
binary constraint ij if the pair e xi ,e xj is an element f
  c                      c                              E     example clauses added to K
of ij ; otherwise we say that ij rejects e. If an instance e +
                                                       {e1 }  (1,2,3,4) ¬≥12 ∧¬ ≥13 ∧¬ ≥14 ∧¬ ≥23 ∧¬ ≥24 ∧¬ ≥34
satisﬁes every constraint in C,thene is called a solution of C; +
                                                       {e2 }  (4,3,2,1) ¬≤12 ∧¬ ≤13 ∧¬ ≤14 ∧¬ ≤23 ∧¬ ≤24 ∧¬ ≤34
                                C                        −
otherwise, e is called a non-solution of .             {e3 }  (1,1,1,1) (=12 ∨ =13 ∨ =14 ∨ =23 ∨ =24 ∨ =34)
  Finally, a training set Ef consists of a set E of instances
and a classiﬁcation function f : E →{0, 1}.Anelement
e in E such that f(e)=1is called positive example (of- acquire contains only one constraint, namely x1 = x4;there
ten denoted by e+) and an element e such that f(e)=0is is no constraint between any other pair of variables. For each
called negative example (often denoted by e−). A constraint example e (ﬁrst column), Table 1 shows the clausal encoding
                                               f
network C is said to be consistent with a training set E if constructed by CONACQ after e is processed, using the set
every positive example e+ in Ef is a solution of C and every κ(e) of constraints in the bias B that can reject e. 
                −     f                  C
negative example e in E is a non-solution of .Wealso    The learning capability of CONACQ can be improved by
say that C correctly classiﬁes Ef . Given a constraint bias B
                f                                     exploiting domain-speciﬁc knowledge [Bessiere et al., 2005].
and a training set E ,theConstraint Acquisition Problem is In constraint programming, constraints are often interdepen-
to ﬁnd a constraint network C admissible for the bias B and                        ≥      ≥
                           f                          dent, e.g. two constraints such as 12 and 23 impose a re-
consistent with the training set E .                  striction on the relation of any constraint deﬁned on the scope
  A SAT-based algorithm, called CONACQ, was presented (x1,x3). This is a crucial difference with conjunctive con-
in [Bessiere et al., 2005] for acquiring constraint networks cepts where atomic variables are pairwise independent. Be-
based on version spaces. Informally, the version space of a cause of such interdependency, some constraints in a network
constraint acquisition problem is the set of all constraint net- can be redundant. cij is redundant in a network C if the con-
works that are admissible for the given vocabulary and bias, straint network obtained by deleting cij from C has the same
and that are consistent with the given training set. We denote   C               ≥
       f                                              solutions as . The constraint 13 is redundant each time
as VB(E ) the version space corresponding to the bias B and ≥ ≥
              f                                         12 and 23 are present.
the training set E . In the SAT-based framework this version Redundancy must be carefully handled if we want to have
                               K
space is encoded in a clausal theory . Each model of the a more accurate idea of which parts of the target network are
      K                        (  f )
theory  is a constraint network of VB E .             not precisely learned. One of the methods to handle redun-
  More formally, if B is the constraint bias, a literal is either dancy proposed in [Bessiere et al., 2005], was to add redun-
an atom bij in B, or its negation ¬bij . Notice that ¬bij is dancy rules to K based on the library of constraints used to
not a constraint: it merely captures the absence of bij in the build the bias B. For instance, if the library contains the
acquired network. A clause is a disjunction of literals (also constraint type ≤, for which we know that ∀x, y, z, (x ≤
                                             K
represented as a set of literals), and the clausal theory is a y) ∧ (y ≤ z) → (x ≤ z), then for any pair of constraints
conjunction of clauses (also represented as a set of clauses). ≤ij , ≤jk in B, we add the Horn clause ≤ij ∧≤jk→≤ik in
An interpretation over B is a map I that assigns to each con- K. This form of background knowledge can help the learner
straint atom bij in B avalueI(bij ) in {0, 1}.Atransforma- in the acquisition process.
tion is a map φ that assigns to each interpretation I over B
the corresponding constraint network φ(I) deﬁned according
                                           p         3   The Interactive Acquisition Problem
to the following condition: cij ∈ φ(I) iff cij = {bij ∈ B :
   p                                                  In reality, there is a cost associated with classifying instances
I(b )=1}.  An interpretation I is a model of K if K is true in
   ij                                                 to form a training set (usually because it requires an answer
I according to the standard propositional semantics. The set
                                                      from a human user) and, therefore, we should seek to min-
of all models of K is denoted Models(K). For each instance
                                                      imise the size of training set required to acquire our target
e, κ(e) denotes the set of all constraints bij in B rejecting e.
                                  f                   constraint network. The target network is the constraint net-
For each example e in the training set E ,theCONACQ al-
                                                      work CT expressing the problem the user has in mind. That is,
gorithm iteratively adds to K a set of clauses so that for any              C
 ∈        (K)             ( )                         given a vocabulary X, D, T is the constraint network such
I   Models    , the network φ I correctly classiﬁes all al- that an instance on X is a positive example if and only if it is
ready processed examples plus e.Whenanexamplee is pos-           C
               {¬b  }            K       b  ∈  ( )    a solution of T .
itive, unit clauses ij are added to for all ij κ e .   During the learning process the acquisition system has
When an example e is negative, the clause {    bij }
                                        bij ∈κ(e)     knowledge that can help characterise what next training ex-
is added to K. The resulting theory K encodes all candi- ample would be ideal from the acquisition system’s point of
date networks for the constraint acquisition problem. That
       f                                              view. Thus, the acquisition system can carefully select ‘good’
is, VB(E )={φ(m)  | m ∈ Models(K)}.                   training examples (which we will discuss in Section 4 in more
Example 1 (CONACQ’s Clausal Representation) We wish   depth), that is, instances which, depending on how the user
to acquire a constraint network involving 4 variables, classiﬁes them, can help reduce the expected size of the ver-
                                                      sion space as much as possible. We deﬁne a query and the
x1,...,x4, with domains D(x1)=...     =   D(x4)=
{1, 2, 3, 4}. We use a complete and uniform bias, with L = classiﬁcation assigned to it by the user as follows.
{≤, =, ≥} as a library. That is, for all 1 ≤ i<j≤ 4, B con- Deﬁnition 1 (Queries and Query Classiﬁcation) A query q
tains ≤ij , =ij and ≥ij . Assume that the network we wish to is an instance on X that is built by the learner. The user

                                                IJCAI-07
                                                   51classiﬁes a query q using a function f such that f(q)=1if q This can be seen by considering the literals that would be
is a solution of CT and f(q)=0otherwise.              added to K by this query. If the query is classiﬁed as positive,
                                                                (¬≥   ) (¬≥   )    (¬ =  )
         [            ]                               the clauses    12 ,    13 and     23 will be added to
  Angluin Angluin, 2004 deﬁnes several classes of queries, K               (≥   ∨≥    ∨ = )
among which the membership query is exactly the kind used , otherwise the clause 12 13    23 will be added.
                                                                                +        ≥       ≥
here. The user is presented with an unlabelled instance, and Since we know from example e1 that both 12 and 13 must
is asked to classify it. We can now formally deﬁne the inter- be set to false, the only extra literal this new example adds is
                                                            (¬ = )   (= )        ( )  =  {= }
active constraint acquisition problem.                either    23 or   23 (indeed κ e [K]   23 ). Regard-
                                                      less of the classiﬁcation of e, something new is learned, so
Deﬁnition 2 (Interactive Constraint Acquisition Problem) this is an irredundant query.                 
Given a constraint bias B and an unknown user classiﬁcation
function f,theInteractive Constraint Acquisition Problem is 4.2 Towards Optimal Query Generation
to ﬁnd a converging sequence Q = q1,...,qm of queries,
                                                 B    The technique presented in Section 4.1 guarantees that each
that is, a sequence such that: qi+1 is a query relative to newly classiﬁed query e adds something new to K.How-
      ( f )        = {        }     |  ( f )| =1
and VB Ei  where Ei   q1,...,qi , and VB Em     .     ever, different irredundant examples give us different gains
  Note that the sequence of queries is built incrementally, in knowledge. In fact, the gain for a query q is directly re-
                                                                          ( )                      ( )
that is, each query qi+1 is built according to the classiﬁca- lated to the size k of κ q [K] and its classiﬁcation f q .If
tion of q1,...,qi. In practice, minimising the length of Q is f(q)=1, k unary negative clauses will be added to K,then
impossible because we do not know in advance the answers k literals will be ﬁxed to 0.IntermsofCONACQ,wedo
from the user. However, in the remainder of the paper we not have direct access to the size of the version space, unless
propose techniques that are suitable for interactive learning. we wish to perform very expensive computation through the
                                                      clausal representation K. But assuming that the models of K
4  Query Generation Strategies                        are uniformly distributed, ﬁxing k literals divides the number
                                                      of models by 2k.Iff(q)=0, a positive clause of size k
4.1  Polynomial-time Query Generation                 is added to K, thus removing 1/2k models. We can distin-
In practice, it can be the case that an example e from the train- guish between queries that can be regarded as optimistic,or
ing set does not bring any more information than that which as optimal-in-expectation.
has already been provided by the other examples that have An optimistic query is one that gives us a large gain in
been considered so far. If we allow for queries to be gener- knowledge when it is classiﬁed “in our favour”, but which
ated whose classiﬁcation is already known based on the cur- tells us very little when it is classiﬁed otherwise. More specif-
                                  K
rent representation of the version space, , then we will ask ically, in CONACQ the larger the κ(q)[K] of a query q,the
the user to classify an excessive number of examples for no more optimistic it is. When classiﬁed as positive, such a
improvement in the quality of our representation of the ver- query allows us to set |κ(q)[K]| literals to 0. If the query is
sion space of the target network. We exemplify this problem classiﬁed as negative we just add a clause of size |κ(q)[K]|.
with a short example.                                 Therefore, an optimistic query is maximally informative –
Example 2 (A Redundant Query) Consider an acquisition sets all literals it introduces to 0 – if it is classiﬁed as positive,
problem over the three variables x1,x2,x3, with the do- but is minimally informative if it is classiﬁed as negative.
mains D(x1)=D(x2)=D(x3)={1,         2, 3, 4} using the  The optimal query strategy is one that involves proposing
same constraint library as in Example 1. Given the posi- a query that will reduce the size of the version space in half
            +
tive example e1 = 
(x1, 1), (x2, 2), (x3, 3), K = ¬≥12 regardless of how the user classiﬁes it. We deﬁne a query
∧¬  ≥13  ∧¬   ≥23.  Asking the user to classify e2 =  as being optimal-in-expectation if we are guaranteed that one

(x1, 1), (x2, 2), (x3, 4) is redundant, since all constraints literal will be ﬁxed to either a 0 or a 1 regardless of the clas-
rejecting it are already forbidden by K. Then any constraint siﬁcation provided by the user. Formally, such a query will
network in the version space accepts e2.             have a κ(q)[K] of size 1, therefore, if it is classiﬁed as positive,
                                                                           ( )
  We propose a simple (poly-time) technique that avoids we can set the literal in κ q [K] to 0, otherwise it is set to a 1.
proposing such redundant queries to the user. This irredun- We illustrate a sequence of queries that are sufﬁcient for
dant queries technique seeks a classiﬁcation only for an ex- the version space of the problem presented as Example 1 to
ample e that cannot be classiﬁed, given the current represen- converge using queries that are optimal-in-expectation.
tation K of the version space. An example e can be classiﬁed Example 4 (Optimal-in-Expectation Queries) We want to
       f                                        f
by VB(E ) if it is either a solution in all networks in VB(E ) converge on the target network from Example 1 (i.e., the only
                                   f
or a non-solution in all networks in VB(E ). e is a solution constraint x1 = x4 in a network with four variables and the
                    f
in all networks in VB(E ) iff the subset κ(e)[K] of κ(e), ob- complete bias of constraints {≤, =, ≥}). Recall that hav-
                                                                                            +  +  −
tained by removing from κ(e) all constraints that appear as ing processed the set of examples E = {e1 ,e2 ,e3 },the
negated literals in K, is empty. Alternatively, e is a non- unique positive clause in K is Cl =(=12 ∨ = 13 ∨ = 14
                            f
solution in all networks in VB(E ),ifκ(e)[K] is a superset ∨ =23 ∨ =24 ∨ =34). All other atoms in K are ﬁxed to 0
                                                                 +      +
of an existing clause of K.                           because of e1 and e2 . In the following K + + refers to
                                                                                           {e1 ,e2 }
Example 3 (An Irredundant Query) Consider again Ex-   (¬≥12)  ∧ ...(¬≥34) ∧ (¬≤12) ∧ ...∧ (¬≤34). Accord-
                                  +
ample 2 in which the positive example e1 has been consid- ing to this notation, the clausal theory K built by CONACQ
ered. The query e = 
(x1, 1), (x2, 2), (x3, 2) is irredundant. having processed E is K = K + + ∧ Cl. Table 2 shows
                                                                                {e1 ,e2 }

                                                IJCAI-07
                                                   52                      Table 2: Optimal-in-expectation query generation strategy on Example 4.

                   e       κ(e)[K] f(e)                         K

              e4 =(1, 1, 2, 3) {=12} +  K + +  ∧ (¬ =12)  ∧      (=13 ∨ =14 ∨ =23 ∨ =24 ∨ =34)
                                          {e ,e }
                                           1 2
              e5 =(2, 1, 1, 3) {=23} +  K + +  ∧ (¬ =12) ∧ (¬ =23) ∧ (=13 ∨ =14 ∨ =24 ∨ =34)
                                          {e1 ,e2 }
              e6 =(2, 3, 1, 1) {=34} +  K + +  ∧ (¬ =12) ∧ (¬ =23) ∧ (¬ =34) ∧ (=13 ∨ =14 ∨ =24)
                                          {e1 ,e2 }
              e7 =(1, 3, 1, 2) {=13} +  K + +  ∧ (¬ =12) ∧ (¬ =23) ∧ (¬ =34) ∧ (¬ =13) ∧ (=14 ∨ =24)
                                          {e1 ,e2 }
              e8 =(2, 1, 3, 1) {=24} +  K + +  ∧ (¬ =12) ∧ (¬ =23) ∧ (¬ =34) ∧ (¬ =13) ∧ (¬ =24)∧(=14)
                                          {e ,e }
                                           1 2

a sequence of queries that are optimal-in-expectation on the uncertainty in the number of constraints rejecting an instance.
version space obtained after the three ﬁrst examples are pro- We implement the query generation problem as a two
cessed. The goal is to reduce VB(E) to contain a single hy- step process. First, Algorithm 1 tries to ﬁnd an interpre-
pothesis. The ﬁrst column is a query e generated according to tation I on B such that any solution s of φ(I) is such that
the optimal-in-expectation strategy. The second column gives t −  ≤|κ(s)[K]|≤t + ,where is the variation accepted
       ( )
the set κ e [K] of constraints still possible in a network of the on the size of the κ(q)[K] of the query q we want to generate.
version space that could reject e. The third column is the clas- This algorithm takes another input parameter which is the set
siﬁcation of e by the user, and the fourth column is the update L of constraints in which κ(q)[K] must be included. We will
  K                         =
of  . The query e4 is such that 12 is the only constraint explain later that this is a way to monitor the ‘direction’ in
still possible in the version space that can reject it. Because which we want to improve our knowledge of the target net-
                                =
it is classiﬁed as positive, we are sure 12 cannot belong to work of the user. Second, once I has been found, we take a
                                       (¬ = )   K
a network in the version space. CONACQ adds 12 to     solution of φ(I) as a query. We ﬁrst present the algorithm,
            =
and the literal 12 is removed from Cl by unit propagation. then we will discuss its complexity and describe how we can
The process repeats with e5, e6 and e7, decreasing the size use it to implement our strategies (by choosing the values t
of Cl by one literal at a time, and thus reducing the version and ).
space by half. Finally, e8 is the last example required to en-
sure that the version space converges on the target network,
which contains the single constraint x1 = x4.         Algorithm 1:QUERY  GENERATION  PROBLEM
  Note that at the beginning of this example, the version
                     6                                  input : B the bias, K the clausal theory, L asetof
space VB(E) contained 2 possible constraint networks, and
                      O(    |  ( )|)                           literals, t atargetsizeand the variation
we could converge using log2 VB E   queries, which is   output: An interpretation I
                   [            ]                
an optimal worst-case Mitchell, 1982 .                1 F ←  K
  In Example 4, we always found an example  e  with   2 foreach bij ∈ B \{bij | (¬bij ) ∈ K} do
|κ(e)[K]| =1, as the optimal-in-expectation strategy requires. 3 if bij ∈ L then F ← F ∧ (bij )
However, redundancy can prevent us from being able to gen- else F ← F ∧ (bij ∨ bij )
erate an example e with a given size for its κ(e)[K].Forin- 4 lower ← max(|L|−t − , 1)
stance, consider the acquisition problem, using a complete 5 upper ← min(|L|−t + , |L|)
and uniform bias, with L = {≤, =, ≥} as a library, and with 6 F ← F ∧ atLeast(lower, L) ∧ atMost(upper, L)
x1 = x2 = x3 as a target network. After processing an initial 7  (F) = ∅                  F
                           +                            if Models       then return a model of
positive example (for instance e1 =(2, 2, 2)), the possible 8 else return “inconsistency”
constraints in the version space are ≤12, ≤13, ≤23, ≥12, ≥13
, ≥23. Hence, every further negative example e has either a
κ(e)[K] of size 3 (if no variables equal) or a κ(e)[K] of size 2 Algorithm 1 works as follows. It takes as input the tar-
(if two variables equal). Therefore, no example with a κ(e)[K] get size t, the allowed variation  and the set L of literals
of size 1 can be generated. Redundancy prevents us from gen- on which to concentrate. The idea is to build a formula F
erating such examples.                                for which every model I will satisfy the requirements listed
                                                      above. F is initialised to K to guarantee that any model will
5  Implementing our Strategies                        correspond to a network in the version space (line 1). For
                                                                b                     K           b
In Section 4.2, we presented two strategies for generating each literal ij not already negated in (line 2), if ij does
                                                                  L                 (b )   F
queries: optimal-in-expectation and optimistic. These two not belong to ,weaddtheclause ij to to enforce the
                                                               b                        ( )
strategies are characterised by the target number of con- constraint ij to belong to the network φ I for all models I
                                           t             F
straints still possible in the version space that reject the in- of (‘then’ instruction of line 3). Hence, any solution s of
                                                       ( )                                 L
stances they try to produce. However, it may be the case φ I will be rejected either by a constraint in or a constraint
      q                                               b                  K
that, due to redundancy between constraints, there does not ij already negated in (so no longer in the version space).
                                                             ( )  ⊆ L                               ( )
exist any network in the version space that has a solution s Thus, κ s [K] . We now have to force the size of κ s [K]
                                                                              bij          L
with |κ(s)[K]| = t. (And it is useless to ask classiﬁcation of to be in the right interval. If belongs to (‘else’ instruc-
an instance if it is not a solution of some network in the ver- tion of line 3), we add the clause (bij ∨ bij ) to F to ensure
sion space – see Section 4.1). We then must allow for some that either bij or its complementary constraint bij is in the re-

                                                IJCAI-07
                                                   53sulting network.1 bij is required because ¬bij only expresses If we have tried all the clauses without success, we have to
the absence of the constraint bij . ¬bij is not sufﬁcient to en- increase . We have two options. The ﬁrst one, called closest,
force bij to be violated. We now just add two pseudo-Boolean will look for a query generated with a set L instantiated to
constraints that enforce the number of constraints from L vi- the clause that permits the smallest . The second one, called
olated by solutions of φ(I) to be in the interval [t − ..t+ ]. approximate, increases  by ﬁxed steps. It ﬁrst tries to ﬁnd a
This is done by forcing at most |L|−t +  constraints and at set L where a query exists with  =0.25 ·|L|. If not found, it
least |L|−t −  constraints to be satisﬁed (lines 4-6). The looks (repeatedly) with 0.50 ·|L|, 0.75 ·|L| and then |L|.
‘min’ and ‘max’ ensure we avoid trivial cases (no constraint We thus have four policies to generate queries: optimistic
from L is violated) and to remain under the size of L.Line7 and optimal-in-expectation combined with closest and ap-
searches for a model of F and returns it. But remember that proximate: optimistic means t = L/2 whereas optimal-in-
redundancy may prevent us from computing a query q with a expectation means t =1; closest ﬁnds the smallest  whereas
given κ(q)[K] size (Section 4.2). So, if  is too small, F can be approximate increases  by steps of 25%.
unsatisﬁable and an inconsistency is returned (line 8).
  The following property tells us when the output of Algo- 6 Experimental Results
rithm 1 is guaranteed to lead to a query.
                                                      We implemented CONACQ   using SAT4J2 and Choco3.In
Property 1 (Satisﬁability) Given a bias B, a clausal theory our implementation we exploit redundancy to the largest ex-
K, and a model I of K.IfK contains all existing redundancy tent possible, using both redundancy rules and backbone de-
rules over B,thenφ(I) has solutions.                  tection [Bessiere et al., 2005].
  If not all redundancy rules belong to K, Algorithm 1 can
return I such that φ(I) is inconsistent. In such a case, we Problem Classes. We used a mix of binary and non-binary
extract a conﬂict set of constraints S from φ(I) and add the problem classes in our experiments. We studied random bi-
            ¬bij  K
clause bij ∈S   to  to avoid repeatedly generating mod- nary problems, with and without structure, as well as ac-
els I with this hidden inconsistency in φ(I).       quiring a CSP deﬁning the rules of the logic puzzle Sudoku.
  The next property tells us that generating a given type of CONACQ used a learning bias deﬁned as the set of all edges
query can be hard.                                    in each problem using the library {≤, ≥, =}. The random
                                                      binary problems comprised 14 variables, with a uniform do-
Property 2 Given a bias B,atheoryK,asetL of constraints,
                                                      main of size 20. We generated target constraint networks by
atargetsize and a variation , generating a query such
           t                                q        randomly selecting a speciﬁed number of constraints from
that: κ(q)[K] ⊂ L and t −  ≤|κ(q)[K]|≤t +  is NP-hard.
                                                      {<, ≤, =, ≥,>,=}, retaining only those that were soluble.
  The experimental section will show that despite its com- We also considered instances in which we forced some con-
plexity, this problem is handled very efﬁciently by the tech- straint patterns in the constraint graph to assess the effect of
nique presented in Algorithm 1. The algorithm can be used structure [Bessiere et al., 2005]. We did this by selecting the
to check if there exists a query rejected by a set of constraints same constraint relation to form a path in the target network.
from the version space of size t ±  included in a given set L. Finally, we used a 4 × 4 Sudoku as the target network. The
The optimal-in-expectation strategy requires t =1and op- acquisition problem in this case was to learn the rules of Su-
timistic requires a larger t. In the following, we chose to be doku from (counter)examples of grid conﬁgurations.
“half-way” optimistic and to ﬁx t to |L|/2. There still remains As an example of a non-binary problem, we considered
the issue of which set L to use and which values of  to try.  the Schur’s lemma, which is Problem 15 from the CSPLIB4.
is always initialised to 0. Concerning L, we take the smallest In this case, CONACQ used the library of ternary constraints
non-unary positive clause of K. A positive clause represents {ALLDIFF, ALLEQUAL, NOTALLDIFF, NOTALLEQUAL}.
the set of constraints that reject a negative example already
processed by CONACQ. So, we are sure that at least one of
                        L                             Results. In Table 3 we report averaged results for 100 ex-
the constraints in such a set rejects an instance. Choosing periments of each query generation approach on each of the
the smallest one increases the chances to quickly converge problem classes we studied. In each case the initial training
on a unary clause. If K does not contain any such non-unary
                                                K     set contained a single positive example. In the table the ﬁrst
clauses we take the set containing all non-ﬁxed literals in . column contains a description of the target networks in terms
  Since Algorithm 1 can return an inconsistency when called of number of variables and constraints. We report results for
for a query, we have to ﬁnd another set of input parameters on each of the query generation approaches we studied. Random
which to call the algorithm. t is ﬁxed by the strategy, so we is a baseline approach, generating queries entirely at random,
can change L or . If there are several non-unary clauses in
K       L                         K                   which may produce queries that are redundant with respect
 ,weset   to the next positive clause in (ordered by size). to each other. The Irredundant approach generates queries at
  1Not all libraries of constraints contain the complement of each random, but only uses those that can provide new information
constraint. However, the complements may be expressed by a con- to reﬁne the version space. Finally, Optimistic and Optimal-
junction of other constraints. For instance, in library ≤, =, ≥, ≤ in-expectation refer to approaches described in Section 5 and
does not exist but it can be expressed by (≥∧=). If no conjunction
                                                         2
can express the complement of a constraint, we can post an approx- Available from: http://www.sat4j.org.
imation of the negation (or nothing). We just lose the guarantee on 3Available from: http://choco.sourceforge.net.
the number of constraints in L that will reject the generated query. 4Available from: http://www.csplib.org.

                                                IJCAI-07
                                                   54