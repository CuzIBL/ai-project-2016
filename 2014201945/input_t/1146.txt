              Sophia: A novel approach for Textual Case-based Reasoning 

             David Patterson1, Niall Rooney1, Vladimir Dobrynin2, Mykola Galushka1 
                         1Northern Ireland Knowledge Engineering Laboratory 
                          University of Ulster, Jordanstown, BT37OQB, U.K. 
                         {wd.patterson, nf.rooney, mg.galushka}@ulster.ac.uk 
                                     2St Petersburg State University  
                               198904 Petrodvoretz, St Petersburg, Russia 
                                      vdobr@oasis.apmath.spbu.ru 
 

                    Abstract                          ments into a vector space model [Sebastiani, 2002], 
                                                      whereby the case knowledge representation formalism is a 
    In this paper we present a novel methodology for 
    textual case-based reasoning. This technique is   vector of weights representing the individual words present 
    unique in that it automatically discovers case and in each document and similarity is based on a simplistic 
    similarity knowledge, is language independent, is comparison of overlap among case vectors. However, as is 
    scaleable and facilitates semantic similarity be- clear, word order and negation information is lost in the 
    tween cases to be carried out inherently without the transformation, which may be vital for reasoning in certain 
    need for domain knowledge. In addition it provides domains of interest. Additionally, word sense disambigua-
    an insight into the thematical content of the case- tion is lost in the process e.g. the word “Java” can have dif-
    base as a whole, which enables users to better    ferent meanings depending on its context of usage. Most 
    structure queries. We present an analysis of the  importantly of all, there is a loss in transparency to users, 
    competency of the system by assessing the quality which is a major drawback during case retrieval. NLP tech-
    of the similarity knowledge discovered and show   niques can determine word order but they can be brittle and 
    how it is ideally suited to case-based retrieval (que- computationally expensive. Some researchers in the CBR 
    rying by example).                                community have followed this approach and developed 
                                                      novel but often domain specific approaches to the task of 
                                                      TCBR. Examples of such systems include work by Brün-
1 Introduction                                        inghaus & Ashley, [2001], Cunningham et al., [2004], 
Textual Case-based reasoning (CBR) is very different in Kunze & Hubner [1998] and Lenz [1998]. These systems 
many respects from more conventional CBR applications are very domain-dependent, require considerable knowledge 
where case knowledge is usually more easily acquired, engineering effort and are usually designed for the situation 
structured and adequately (and often simplistically) repre- where all documents have a similarly structured content. 
sented as simple vectors or objects. In textual CBR (TCBR) Other researchers have instead focused on advancing IR 
the knowledge to be embodied within a case is much more techniques using technologies such as Latent Semantic In-
intricate in that it contains complex linguistic terms and dexing [Deerwester et al., 1990] and Probabilistic Latent 
concepts on various topics which are often encompassed Semantic Indexing [Hoffman, 1999]. These go beyond the 
within the same case. Acquiring and representing this bag-of-words model providing a more advanced view of the 
knowledge adequately, without loosing its meaning and document space that may require less knowledge engineer-
with a low knowledge engineering overhead to users, re- ing activity within a TCBR context [Zelikovitz and Hirsh, 
mains a challenging prospect. Equally challenging is the 2002]. Unfortunately these have crucial drawbacks also. For 
prospect of discovering, implementing and maintaining use- example they use a transformation to project documents into 
ful similarity knowledge within such systems where it is cases and as such they become non transparent to a user. 
often vital to identify similar cases that perhaps do not nec- Transparency is an important component of CBR systems to 
essarily contain the same words but semantically similar enable the user gain insight into the reasoning and explana-
themes or concepts. It may initially seem that the problem of tion processes of the system. As such they provide little 
TCBR could be adequately addressed by standard Informa- insight into the knowledge contained within a case or how 
tion retrieval (IR) techniques.  However there are limitations similarity is determined during retrieval. A further limitation 
with these approaches. Perhaps the best known IR approach, is that these approaches are essentially dimensionality re-
commonly known as the bag of words, transforms docu-  duction techniques and as such important information may be lost. Additionally word order and negation knowledge is document collections where each document has a similar 
also lost. Wiratunga et al., [2004], combine IR and Machine internal structure. However in terms of its application to 
Learning techniques in their approach. A strength is its abil- document collections such as presented in this paper (where 
ity to facilitate automated semantic similarity determination each document does not necessarily have a similar internal 
but unfortunately their approach is completely supervised content structure), we show that SOPHIA is capable of pro-
and as such the class of each document within the collection viding a competent TCBR system. Firstly we present the 
must be known apriori to enable the discovery of similarity SOPHIA technology, then we carry out an initial experiment 
knowledge. This limits the application of the technology. In to demonstrate the quality of the discovered case and simi-
addition the effectiveness of the technique to multi class larity knowledge. This is followed by an additional experi-
domains has not been addressed and remains an open ques- ment which investigates the potential of the system to case-
tion. Cunningham et al., [2004], have also recently proposed based retrieval (query by example). Finally we discuss the 
a very different approach to TCBR based on graph theory, results and present future work. 
that maintains the original document structure and word  
order. A disadvantage of their approach is that they require 
an expert to identify domain dependant indexes, which they 2  SOPHIA CBR Methodology  
rely on to assess case similarity using graph distance tech-
                                                      Knowledge is automatically discovered at various stages 
niques. Therefore there is a significant knowledge engineer-
                                                      within the SOPHIA Framework. In step 1 we describe how 
ing burden in terms of similarity knowledge. In addition it 
                                                      case knowledge is discovered and represented. In step 2 we 
cannot determine semantic similarity between cases or cope 
                                                      show how numerous specific narrow context words are 
with ambiguities that may occur. One final open question 
                                                      automatically identified. These narrow contexts act as at-
with this technique is its efficiency with medium to large 
                                                      tractors for clustering cases and this step can be regarded as 
case-bases as assessing sub graph similarity is a complex 
                                                      global similarity knowledge discovery. In step 3 cluster 
matter. 
                                                      level similarity knowledge is discovered and used to deter-
  In this research, we present a new approach for TCBR 
                                                      mine which narrow context each case should be assigned to. 
called SOPHIA CBR, based upon a scaleable contextual 
                                                      Step 4, is strictly not part of the clustering algorithm itself 
document clustering approach [Dobrynin et al., 2004], 
                                                      but is an additional processing step that provides extra 
which facilitates an advanced and rich knowledge discovery 
                                                      knowledge about the internal case structure of clusters and 
framework for case-based retrieval. SOPHIA’s case repre-
                                                      provide a means for visualizing them. This can be regarded 
sentation formalism is similar to a classical vector space 
                                                      as localized similarity knowledge discovery. As will be-
model except, rather than using word frequencies, it is based 
                                                      come apparent, not only does this empower the user with 
on the conditional probability distributions of terms within 
                                                      extra domain knowledge about the problem area but it im-
documents. It then intelligently discovers important contexts 
                                                      proves both the case index and the ability of the system to 
within the case-base and organizes cases into one of a large 
                                                      identify similar cases.  
number of clusters which have these contexts as attractors. 
                                                       
This produces groups of semantically related cases (i.e. 
                                                      Step 1 Case knowledge Discovery. Here we describe how 
cases which are on the same or similar subjects but use dif-
                                                      case knowledge is automatically extracted from a document 
ferent terminology, can be recognized as similar) for a given 
                                                      corpus. In the following definitions, a term refers to a word 
cluster context. This process of forming clusters, allows 
                                                      in the document corpus. Let Ξ  denote the set of all docu-
both a very efficient and competent case retrieval process. It 
                                                      ments in the document corpus and Ψ denote the set of all 
is this unique feature that provides much of the power be-
                                                      terms present in Ξ . For every document in the corpus a case 
hind the technology.  
                                                      is automatically extracted and represented by a probability 
  SOPHIA CBR is advantageous in that, it is domain inde-
                                                      distribution over all terms occurring in that document. 
pendent, and has low knowledge engineering overheads as it 
                                                                 tf(, x y )
does not require any user intervention to acquire domain  py(|) x=        
knowledge. As such, all knowledge can be discovered auto-       ∑tf(,) x t
matically (although if background knowledge is already          t∈Ψ
present it can be utilized). Additionally it uses a transparent where tf(, x y ) is the term frequency of the term y  in  
case knowledge representation, automatically discovers and document x  and t  is a term from the set of all terms pre-
provides users with additional knowledge about the domain sent in the document collection. Although by itself this does 
that they can use to refine queries, is language independent, not provide a richer case representation than using conven-
is scaleable and can differentiate between the different con- tional IR approaches, it does facilitate the process of group-
texts of potentially ambiguous terms. The novel technology ing, indexing and retrieving semantically similar cases, 
presented in SOPHIA CBR is useful for both classification which forms the centerpiece of the power of this technol-
tasks and for retrieval, browsing and searching by example. ogy.  
SOPHIA does not have a mechanism to identify word order  
or negation, features which undoubtedly are important for Step 2 Global Similarity Knowledge Discovery. Given a related cases, where cluster membership (similarity) is 
term z ∈Ψ, we define its context as the probability distribu- measured using the Jensen-Shannon (JS) divergence [Lin, 
tion of a set of words which co-occur with the given term. 1991]. In this respect contexts are regarded as global simi-
More specifically the context of the term z  is represented in larity knowledge. 
the form of a conditional probability distribution p (Yz | ) ,  
where the random variable Y takes values from Ψ and   Step 3 Cluster Level Similarity Knowledge Discovery. Nar-
p(|)yz is equal to the probability of randomly selecting row contexts {pY ( | z )}z∈Ζ , discovered in step 2, are consid-
the term y  in a randomly selected case within which the ered as cluster attractors. Within this study all cases are 
                                                      grouped into at most one cluster based on the case similarity 
term z co-occurs. We can approximate this distribution as:
                                                      to the attractor, i.e. this is a hard clustering approach but 
       
                                                      equally a softer approach could be applied. In this way cases 
             ∑  tf(, x y )                            are associated with the contexts they most closely match to 
            ∈Ξ
   py(|) z = xz()                                     form clusters of cases that are on similar (closely related) 
             ∑   tf(,) x t                            subjects or themes. The similarity between a case x and the 
           ∈Ξ  ∈Ψ
           xzt(),                                     context for the term z  is estimated by the JS divergence 
 
                                                      between the probability distributions p and p representing 
where tf( x , y )  is the term frequency of the term y in  case                        1     2
                                                      the case and the context respectively: 
       Ξ
x  and  (z ) is the set of all cases from the corpus which  
contain the term z . It is obvious that in most cases the con-        =−              −
                                                         JSppHpHpHp{0.5,0.5}[, 1 2 ] []0.5[]0.5[ 1 2 ]  
text of the term z is too general in scope to present useful 
                                                       
information about the corpus. So we are interested only in 
                                                      where H[]p  denotes the entropy of the probability distribu-
identifying narrow context terms z . The narrowness of the 
term z  is estimated by the entropy of the probability tion p and p denote the average probability distribution 
                                                       =+
distributionp (Yz | ) :                                 0.5p12 0.5 p . A case x  is therefore assigned to a cluster 
                                                      with attractor z if, 
  H(Yz | )=−∑  pyz ( | )log( pyz ( | ))                
             y                                            =
                                                         z  arg minJS{0.5,0.5} [ p ( Y | x ), p ( Y | t )]   
 Let  Ψ (z )  denote the set of all different terms from cases t∈Ζ
                                                      in other words a case is assigned to the cluster whose attrac-
  Ξ
in (z ) . When there is a uniform distribution of terms from  tor it has the highest semantic similarity to. 
Ψ()z the entropy H (Yz | )  is equal to  log |Ψ(z ) |. Accord- Once these three stages are completed, we have discov-
ing to Heaps Law  log |Ψ= (z ) |Oz (log(| Ξ ( |))  [Baeza-Yates ered all case knowledge, all narrow contexts within the 
                                                      case-base and assigned cases to the context they are most 
& Ribeiro-Neto, 1999] there is a   relationship between the 
                   =Ξ                                 semantically similar to. In an equivalent fashion, the simi-
case frequency df ( z ) | ( z ) |  of the term z  and the en- larity between cases within a cluster can be discovered using 
tropy of its context. To allow for this dependency, we divide the JS divergence. As such the lower the JS divergence, the 
the whole set of words into r disjoint subsets:       higher the similarity and as will be seen in step 4, it is this 
                                                      similarity knowledge that forms the key to discovering se-
  Ψ=    Ψ
         i                                           mantically related cases.  
       i                                                       
  Ψ=       ∈Ψ     ≤      <
    iii{:z zdfdfzdf , ()+1    }                       Step 4 Localized Similarity Knowledge Discovery. Up to 
  ir= 1..                                             this point all knowledge discovered has been at the 
  Here the threshold df  satisfies the condition df= α df  global/cluster level. In this step we discover localized simi-
                    i                     ii+1        larity knowledge that defines the inner case structure of each 
      α >
where 1is a constant. Choosing narrow word contexts   cluster. We represent the relationships (similarities) between 
are based on the assumption that in total there are N  nar- cases by a graph where each vertex in the graph represents a 
                             =          Ζ⊂Ψ
row word contexts. For every ir1,.., a set ii, is     case. Any two vertices are connected by an undirected edge, 
selected such that                                    whose weight denotes the distance between corresponding 
          N.|Ψ  |                                     cases. This weight is determined as before using the JS di-
    ||Ζ=       i       
      i       Ψ                                       vergence. The standard Kruskal's algorithm is used to find 
          ∑  ||i
          jr=1,..                                     the minimum spanning tree (MST) which spans all graph 
                                                      vertices and has the minimum total weight for its edges. The 
and z ∈Ζ,(|)(|)zHYzHYz ∈Ψ −Ζ → ≤        . Then 
    12iii                    1        2               knowledge within the MST can then be presented to a user 
Ζ=∪    Ζ , where Ζ is the set of selected narrow contexts. 
      i i                                             as a complete description of the internal structure of a clus-
These contexts form the seeds for clustering semantically ter relating to a narrow context. Useful knowledge includes, cases nearer to the top of the tree are most similar to the used in the clustering/retrieval process and as such the ap-
narrow context, while those further away are less similar. proach is totally unsupervised in nature and therefore has as 
Cases in close proximity within the MST are more similar a consequence, no manual knowledge engineering over-
than cases that are far apart. Cases in one branch of the tree heads. In total, 38,088 different terms remained after pars-
are more similar than those in separate branches. This local- ing. 
ized similarity knowledge can either be used to interactively 
browse the relevant cluster structure looking for useful cases 3.2  Experiment on similarity knowledge quality 
or as a means of accurately classifying a new case.   In the first experiment we use the topic categories of cases 
                                                      as a means of independently assessing the quality of our 
3 Experiments                                         similarity knowledge and hence our clustering process. Note 
In this work we demonstrate the efficacy of the SOPHIA that this topic categorization domain knowledge is not util-
                                                      ized as part of the process of case similarity determination 
CBR system to textual case retrieval. In the first experiment 
                                                      (which is based solely on the textual content of cases) but as 
we investigate the quality of the case and similarity knowl-
                                                      a means for assessing the effectiveness of the discovered 
edge by demonstrating the high degree of (semantic) simi-
                                                      similarity knowledge only. This experiment provides us 
larity between cases within clusters. In the second, we in-
vestigate the quality of the retrieval process itself and show with insight into the quality and meaningfulness of the 
how it is ideally suited to case-based retrieval.     SOPHIA TCBR clustering process. Our hypothesis is that if 
                                                      the system identifies cases as semantically similar then there 
                                                      should be a high probability that they share many of the 
3.1  Case base description                            same categories. To facilitate this we determine the degree 
                                                      of overlap between adjacent cases in the MST. We therefore 
 In our experiments we use the well known Modified Apte 
                                                      evaluate the actual similarity of 2 cases a and b based on the 
("ModApte") split of the Reuters-21578 collection (dav-
                                                      similarity of the topics assigned to them by experts. Let T(x) 
idlewis.com/resources/testcollections/reuters-21578) con-
                                                      be the set of topics assigned to case x by an expert. To 
taining 9,603 training documents and 3,299 test documents. 
                                                      evaluate the similarity between T(a) and T(b) we use the 
Not all documents are actually assigned a category. There-
                                                      Jaccard coefficient (JC).  
fore we only use those that have at least one topic (7,775 
                                                             |()Ta∩ Tb ()|
from the training set and 3,019 from the test set) in our ex-  JC =       
periments. All other documents were considered as back-      |()Ta∪ Tb ()|
ground information. We use these documents to accumulate A coefficient of 1 indicates that both cases have identical 
information about the document corpus. All training and topics assigned to them and have maximum similarity, 
background documents were used for case knowledge and while a value of 0 signifies no overlap between topics, indi-
similarity knowledge discovery (clustering & MST forma- cating minimum similarity. Figure 1 shows the results of 
tion). Test documents were used as queries in the retrieval this experiment. The most striking observation from this 
experiments only. It should be noted at this point that the graph is that the vast majority of cases (5796 pairs –75%) 
majority of research carried out with this corpus in the past have an extremely high JC (0.9-1.0). This provides ex-
(mostly within the Information Retrieval community) has tremely strong evidence for the high quality of our similarity 
tended to focus on evaluations using the 10 most popular knowledge and confirms that semantically similar cases, i.e. 
categories only [Sebastiani, 2002]. As such our task is much those linked in the MST, have a large degree of overlap in 
more challenging in that we attempt to use all 120 catego- their categories and therefore must of necessity be genuinely 
ries to evaluate our technique. The experiments described in very similar. This also confirms that forming clusters based 
this section could equally be applied to other document col- on narrow contexts combined with a MST based on JS di-
lections as SOPHIA CBR is domain and language inde-   vergence is a powerful approach for determining textual 
pendent.                                              case similarity which also provides real meaning and trans-
  Initially documents were parsed (transformed to cases) by parency to users. That is users can easily see the context of 
converting all words from the title and body of a document all cases within a cluster and also which cases are most 
into lower case, deleting stop-words using a standard stop- similar to others within the cluster. At the opposite end of 
words list (SMART, 571 words) and using the Porter algo- the graph, where we can observe case pairs with poor cate-
rithm for stemming. We consider a word as any maximal gory overlap, it can be seen that there are 1472 pairs of 
sequence of symbols which start with a symbol in the range cases (19%) with very poor JC and almost no overlap be-
a-z, and ends with any symbol between a and z or any sym- tween their categories. An interesting phenomenon can be 
bol between 0 and 9 and in between contains symbols from observed at a JC value of 0.5-0.6. Here we see a slight rise 
the set {a-z0-9_-/}. It should be noted that no additional in the number of case pairs. The reason for this is that many 
information about the document set is used, apart from the cases have only 2 categories and this peak represents the 
title and body of the documents themselves. In particular, situation whereby they agree on one category and disagree 
expert assigned knowledge such as category labels are not on the other (e.g. one case could have 2 categories and an- topics of neighboring cases. This process is demonstrated in 
other only one, which they share).                    the subsequent experiment where we consider the following 
                                                      indexing and browsing scenario. The user has a target case d 
     6000                                             (test case/query example) and the system presents the MST 
     5000                                             of the most relevant cluster, based on the closest matching 
                                                      context attractor, for browsing. This tree is the MST of a 
     4000                                             cluster C(z(d)) whose context z(d) is the smallest distance 
     3000                                             from the target case (as before we use JS divergence to 
     2000                                             evaluate distances). The nearest neighbor, NN(z(d),d), to 
     1000                                             case d from within cluster C(z(d)), is selected from the 
                                                      MST. The user then starts browsing from this case. We will 
   number of case pairs  case number  of 0
                                                      consider three notions of relevance in this experiment. Let 
                               )
          1)  .2)     4)          7)      9)  .0]
         ,0. 0   ,0.3) 0.    0.6 0.      0.  1        T(x) be the set of all topics assigned by the expert to case x. 
        [0      2   .3,     5,          8,
           [0.1, [0. [0 [0.4,0.5)[0. [0.6, [0.7,0.8)[0. [0.9, Then 

                intervals for Jaccard coefficient     Predicate R= (xy , ) means that Tx ( )= Ty ( )  

                                                      Predicate R⊆ (xy , ) means that Tx ( )⊆ Ty ( )  
Figure 1 Assessment of similarity for MST edges Using JC                                 ≠∅
                                                      Predicate R∩ (xy , ) means that Tx ( ) Ty ( )  

  For all other JC values there are practically no case pairs. In other words if R= (xy , )  then cases x and y are considered 
It is reasonable to conclude that clusters are not entirely to be very relevant as they have exactly the same set of top-
homogeneous in terms of their topics. That is, each cluster 
                                                      ics assigned by experts. If R⊆ (xy , ) then the relevance of 
may contain cases on different topics. Therefore it is to be 
expected that, within the MST when traversing from one case x to case y is slightly weaker than in the previous defi-
case to another, topic shifts should be encountered. When- nition but the user can be sure that all topics of case x are 
ever this occurs adjacent cases in the MST will differ on also contained in case y. If R∩ (xy , ) then we have the weak-
their topics. These shifts may be gradual, as evidenced by est notion of relevance in that both cases share at least one 
the rise in the graph around a JC of 0.5 (ie adjacent cases common topic. We say that the target case d is successfully 
overlap on 50% of their topics) or radical, as evidenced by matched with an existing case if, in the minimum spanning 
the rise at a JC of 0 (adjacent cases overlap on none of their tree MST(C(z(d))) of the cluster C(z(d)) there exists at least 
topics). These results provide compelling evidence for qual- one relevant training case y within a distance of k edge links 
ity of the case and similarity knowledge discovered and from the nearest neighbor case NN(z(d). In this evaluation 
utilized by SOPHIA CBR.                               we include the nearest neighbor (NN) case as part of the 
                                                      evaluation and only consider edges which connect to train-
                                                      ing cases (edges connecting to background cases are ig-
3.3  Experiment on Case Retrieval                     nored). Let predicate S1(d,k)  indicate that case d is success-
In this section, we propose a case retrieval approach de- fully matched with an existing case within distance k, where 
signed to provide flexible querying plus a high retrieval 
                                                      relevance is determined by predicate R= (xy , ) . Similar 
accuracy and a good explanatory facility. SOPHIA enables 
                                                      predicates S2(d,k) and S3(d,k) are used when relevance is 
queries to be generated using the traditional key word ap-
                                                      determined by predicates R (xy , )  and R (xy , ) respec-
proach or by using query by example, where entire docu-                        ⊆             ∩
ments (or parts thereof) can be used as the query. SOPHIA tively. Let P1(k), (P2(k), P3(k)) be the probability that for a 
then supplies the user with quality knowledge about “all” randomly selected test case d,  the predicate  S1(d, k)  (S2(d, 
possibly relevant cases within the case base. It is important k),  S3(d, k)) is true.  Figure 2 shows how values of P1(k), 
that this knowledge should be presented to the user in the P2(k) and P3(k)  depend on k. These results show that the 
context of the whole case collection. In other words, the knowledge discovered and utilized by SOPHIA CBR, when 
user should have the facility to estimate:            exploited by the process of interactive browsing, provides 
   • which region of the whole case base contains the an ideal medium for locating and retrieving relevant cases. 
     most relevant cases,                             Using the most stringent definition of relevance, where all 
   • how large this region is,                        cases must have exactly he same topics (P1), it can be seen 
   • which semantically similar documents are relevant      that 75% of the NN cases themselves are relevant (distance 
                                                      0). It can be seen that this rises to almost 85% when cases 
  We propose that the MST is ideally suited to presenting within a vicinity of 3 links are considered. Considering lar-
this knowledge to the user. Through a process of system ger vicinities, add little more to the relevancy. Defining 
supported browsing, the user can evaluate the relevancy of relevancy as in P2, a similar picture is observed. This time 
different parts of the tree, estimate its size and estimate the the NN cases are relevant 80% of the time, rising to 87% 