                     Efﬁcient Bayesian Task-Level Transfer Learning

                                Daniel M. Roy   and  Leslie P. Kaelbling
                                 Massachusetts Institute of Technology
                        Computer Science and Artiﬁcial Intelligence Laboratory
                                       {droy, lpk}@csail.mit.edu


                    Abstract                          must be evaluated on real data, and, to that end, we evaluate
                                                      the resulting model in a meeting domain. The learned system
    In this paper, we show how using the Dirichlet Pro- decides, based on training data for a user, whether to accept
    cess mixture model as a generative model of data  or reject the request on his or her behalf. The model that
    sets provides a simple and effective method for   shares data outperforms its no-sharing counterpart by using
    transfer learning. In particular, we present a hierar- data from other users to inﬂuence its predictions.
    chical extension of the classic Naive Bayes classi-
                                                        When faced with a classiﬁcation task on a single data set,
    ﬁer that couples multiple Naive Bayes classiﬁers by
                                                      well-studied techniques abound (Boser et al., 1992; Lafferty
    placing a Dirichlet Process prior over their parame-
                                                      et al., 2001). A popular classiﬁer that works well in practice,
    ters and show how recent advances in approximate
                                                      despite its simplicity, is the Naive Bayes classiﬁer (Maron,
    inference in the Dirichlet Process mixture model
                                                      1961). We can extend this classiﬁer to the multi-task setting
    enable efﬁcient inference. We evaluate the result-
                                                      by training one classiﬁer for each cluster in the latent par-
    ing model in a meeting domain, in which the sys-
                                                      tition. To handle uncertainty in the number of clusters and
    tem decides, based on a learned model of the user’s
                                                      their membership, we deﬁne a generative process for data
    behavior, whether to accept or reject the request on
                                                      sets that induces clustering. At the heart of this process is
    his or her behalf. The extended model outperforms
                                                      a non-parametric prior known as the Dirichlet Process. This
    the standard Naive Bayes model by using data from
                                                      prior couples the parameters of the Naive Bayes classiﬁers
    other users to inﬂuence its predictions.
                                                      attached to each data set. This approach extends the applica-
                                                      bility of the Naive Bayes classiﬁer to the domain of multi-task
1  Introduction                                       learning when the tasks are deﬁned over the same input space.
In machine learning, we are often confronted with multiple, Bayesian inference under this clustered Naive Bayes model
related data sets and asked to make predictions. For example, combines the contribution of every partition of the data sets,
in spam ﬁltering, a typical data set consists of thousands of la- weighing each by the partition’s posterior probability. How-
beled emails belonging to a collection of users. In this sense, ever, the sum over partitions is intractable and, therefore, we
we have multiple data sets—one for each user. Should we employ recent work by Heller and Ghahramani (2005a) to
combine the data sets and ignore the prior knowledge that dif- implement an approximate inference algorithm. The result is
ferent users labeled each email? If we combine the data from efﬁcient, task-level transfer learning.
a group of users who roughly agree on the deﬁnition of spam,
we will have increased the available training data from which 2Models
to make predictions. However, if the preferences within a
population of users are heterogeneous, then we should expect In this paper, we concentrate on classiﬁcation settings where
                                                                X   f     ,...,F          Y
that simply collapsing the data into an undifferentiated col- the features f , =1 and labels are drawn from
                                                               V , L
lection will make our predictions worse.              ﬁnite sets f  , respectively. Our goal is to learn the rela-
  The process of using data from unrelated or partially re- tionship between the input features and output labels in order
lated tasks is known as transfer learning or multi-task learn- to predict a label given an unseen combination of features.
ing and has a growing literature (Baxter, 2000; Guestrin et al., Consider U tasks, each with Nu training examples com-
2003; Thrun, 1996; Xue et al., 2005). While humans effort- posed of a label Y and a feature vector X. To make the dis-
lessly use experience from related tasks to improve their per- cussion more concrete, assume each task is associated with
formance at novel tasks, machines must be given precise in- a different user performing some common task. Therefore,
                                                                   (u)                            (u,i)
structions on how to make such connections. In this paper, we will write D to represent the feature vectors X and
                                                                          (u,i)
we introduce such a set of instructions, based on the statis- corresponding labels Y , i =1,...,Nu, associated with
tical assumption that there exists some partition of the tasks the u-th user. Then D is the entire collection of data across
into clusters such that the data for all tasks in a cluster are all users, and D(u,j) =(X(u,j),Y(u,j)) is the j-th data point
identically distributed. Ultimately, any such model of sharing for the u-th user.

                                                IJCAI-07
                                                  2599  Let mu,y denote the number of data points labeled y ∈L                                       DP
in the data set associated with the u-th user and let nu,y,f,x
denote the number of instances in the u-th data set where the
                                                                  φu                   φu
f-th feature takes the value x ∈Vf when its parent label takes
                            (u)        (u)
value y ∈L.Letθu,y,f,x  P(Xf   = x | Y   = y),and
          (u)
φu,y  P(Y   = y). We can now write the general form of           Yuj    θuyf          Yuj     θuyf

the probability mass function of the data conditioned on the              |L|F                 |L|F
parameters θ and φ of the Naive Bayes model:
            U                                                    Xujf                 Xujf
         (a)               (b) 
                  (u)                   (u,n)                       F                    F
p(D|θ, φ) =    p(D   |θu,φu) =      p(D     |θu,φu)
                                                                    N                    N
           u=1                1≤u≤U                                  u       U             u      U
                              1≤n≤Nu                        (a)                  (b)

    U  Nu               F
(c)                  
          p Y (u,n)|φ     p X(u,n)|Y (u),θ            Figure 1: Graphical models for the (a) No-Sharing and (b) Clus-
 =         (       u,y )   (  f         u,y,f )       tered Naive Bayes (right) models. Each user has its own parame-
   u=1 n=1             f=1                            terization in the no-sharing model. The parameters of the Clustered
   U          F                                   Naive Bayes model are drawn from a Dirichlet Process. Here, the
(d)                     n
           mu,y          u,y,f,x                      intermediate measure G has been integrated out.
 =        φu,y         θu,y,f,x ,
   u=1 y∈L      f=1 x∈Vf
                                                      assumption of independence, training the entire collection of
In (a), p(D|θ, φ) is expanded into a product of terms, one for models is identical to training each model separately on its
each data set, reﬂecting that the data sets are independent con- own data set. We therefore call this model the no-sharing
ditioned on the parameterization. Step (b) assumes the data model.
points are exchangeable; in particular, the label/feature pairs Having speciﬁed that the prior factors into parameter dis-
are independent of one another given the parameterization. tributions for each user, we must specify the actual parameter
In step (c), we have made use of the Naive Bayes assump- distributions for each user. A reasonable (and tractable) class
tion that the features are conditionally independent given the of distributions over multinomial parameters are the Dirichlet
label. Finally, in step (d), we have used the fact that the distri- distributions which are conjugate to the multinomials. There-
butions are multinomials. The maximum likelihood parame- fore, the distribution over φu, which takes values in the |L|-
terizations are                                       simplex, is
             mu,y                   nu,y,f,x                              
   φu,y =          ,θu,y,f,x =              .                                      
                mu,y                   nu,y,f,x                         Γ(  y∈L αu,y)
            y∈L                    x∈Vf                                                  αu,y −1
                                                               f(φu)=                  φu,y   .
                                                                             Γ(αu,y)
  Because each data set is parameterized separately, it is no             y∈L        y∈L
surprise that the maximum likelihood parameterization for
                                                                                 θ
each data set depends only on the data in that data set. In Similarly, the distribution over u,y,f , which takes values in
                                                      |V |
order to induce sharing, we must somehow constrain the pa- f -simplex, is
rameterization across users. In the Bayesian setting, the prior        
                                                                     Γ(       βu,y,f,x) 
distribution F (θ, φ) can be used to enforce such constraints. f θ       x∈Vf            θβu,y,f,x−1.
                                                            ( uy ,f )=                     u,y,f,x
Given a prior, the resulting joint distribution on the data is         x∈V  Γ(βu,y,f,x)
                                                                         f           x∈Vf
          p(D)=        p(D|θ, φ) dF (θ, φ).           We can write the resulting model compactly as a generative
                   Θ×Φ                                process:1
Both models introduced in this section are completely spec-          φu ∼ Dir([αu,y : y ∈L])          (1)
iﬁed by a prior distribution over the parameters of the Naive
                                                                      (u,n)
Bayes model. As we will see, different priors result in differ-     Y     | φu ∼ Discrete(φu)
ent types of sharing.
                                                                  θu,y,f ∼ Dir([βu,y,f,x : x ∈Vf ])
2.1  No-Sharing Baseline Model
We have already seen that a ML parameterization of the Naive (u,n) (u,n)
                                                      Xf     | Y   , {θu,y,f : ∀y ∈L}∼Discrete(θu,(Y (u,n)),f )
Bayes model ignores related data sets. In the Bayesian set-
ting, any prior density f over the entire set of parameters that The No-Sharing model will function as a baseline against
factors into densities for each user’s parameters will result in which we can compare alternative models that induce shar-
no sharing. In particular,                            ing. We turn now to specifying a model that induces sharing.
                      U
             f θ, φ         f θu,φu ,                    1   ∼
              (   )=     u=1 (      )                    The   symbol denotes that the variable to the left is distributed
                                                      according to the distribution speciﬁed on the right. It should be noted
is equivalent to the statement that the parameters for each user that the Dirichlet distribution requires an ordered set of parameters.
are independent of the parameters for other users. Under this We therefore deﬁne an arbitrary ordering of the elements of L, Vf .

                                                IJCAI-07
                                                  2600                                                                               y∈L
2.2  The Clustered Naive Bayes Model                               θu =(θu,y,f )f=1,2,...,F ∼ G
A plausible assumption about a collection of related data sets (u,n) (u,n)
                                                      X      | Y   , {θu,y,f : ∀y ∈L}∼Discrete(θ(u,Y (u,n)),f )
is that they can be clustered into closely-related groups. To f
                                                                        G
make this more precise, we will consider two tasks t1 and t2 The discrete measure is a draw from the Dirichlet Process;
over some space X × Y to be related if the data associated in practice, this random variable is marginalized over. Be-
with both tasks are identically distributed. While this is a very cause the tasks are being clustered, we have named this model
coarse model of relatedness, it leads to improved predictive the Clustered Naive Bayes model (and denote its distribution
                                                                                 F
performance with limited training data.               function over the parameters as CNB). We now explain how
  As a ﬁrst step we must specify a distribution over parti- to use the model to make predictions given training data.
tions of tasks. There are several properties we would like
this distribution to have: ﬁrst, we want exchangeability of 3 Approximate Inference
tasks (users); e.g., the probability should not depend on the                   (u)     (u,i) (u,i)
                                                      Given labelled training data D = {Y  ,X    }1≤i≤Nu
ordering (i.e. identity) of the tasks (users); second, we want for all tasks u ∈{1, 2,...,U} and an unlabeled feature vec-
exchangeability of clusters; e.g., the probability should not ∗  (v,N +1)
                                                      tor X   X    v   for some task v, we would like to com-
depend on the ordering/naming of the clusters; ﬁnally, we                                  ∗     (v,N +1)
                                                      pute the posterior distribution of its label Y  Y v .
want consistency; e.g., a priori, the (hypothetical) existence
                                                      Using Bayes rule, and ignoring normalization constants,
of an unobserved task should not affect the probability that
                                                         p Y ∗  y|X∗,D   ∝ p X∗|Y ∗  y,D  p Y ∗   y,D
any group of tasks are clustered together.                (   =        )   (      =     ) (   =     )
  The Chinese Restaurant Process (CRP) is a stochastic pro-
                                                                         ∝      p D|θ, φ dF   θ, φ ,
cess that induces a distribution over partitions that satisﬁes all               (      )   CNB(  )
these requirements (Aldous, 1985). The following metaphor                   Θ×Φ
                                                              
was used to deﬁne the process: imagine a restaurant with a where D is the data set where we imagine that the (v, Nv +
countably inﬁnite number of indistinguishable tables. The 1)-th data point has label y. Therefore, Bayesian inference
ﬁrst customer sits at an arbitrary empty table. Subsequent requires that we marginalize over the parameters, including
customers sit at an occupied table with probability propor- the latent partitions of the Dirichlet process. Having cho-
tional to the number of customers already seated at that table sen conjugate priors, the base distribution can be analytically
and sit at an arbitrary, new table with probability proportional marginalized. However, the sum over all partitions makes
to a parameter α>0. The resulting “seating chart” parti- exact inference under the Dirichlet Process mixture model
tions the customers. It can be shown that, in expectation, the intractable. While Markov Chain Monte Carlo and varia-
number of occupied tables after n customers is Θ(log n) (An- tional techniques are the most popular approaches, this paper
toniak, 1974; Navarro et al., 2006).                  uses a simple, recently-proposed approximation to the DPM
  The tasks within each cluster of the partition will share known as Bayesian Hierarchical Clustering (BHC) (Heller
the same parameterization. Extending our generative model, and Ghahramani, 2005a), which approximates the sum over
imagine that when a new user enters the restaurant and sits at all partitions by ﬁrst greedily generating a hierarchical clus-
a new table, they draw a complete parameterization of their tering of the tasks and then efﬁciently summing over the ex-
Naive Bayes model from some base distribution. This param- ponential number of partitions “consistent” with this hierar-
eterization is then associated with their table. If a user sits at chy. This approach leads to a simple, yet efﬁcient, algorithm
an occupied table, they adopt the parameterization already as- for achieving task-level transfer.
sociated with the table. Therefore, everyone at the same table Consider a rooted binary tree T where each task is associ-
uses the same rules for predicting.                   ated with a leaf. It will be convenient to identify each internal
  This generative process corresponds with the well known node, Tn, with the set of leaves descending from that node. A
Dirichlet Process mixture model (DPM) and has been used tree-consistent partition of the tasks is any partition such that
very successfully to model latent groups (Ferguson, 1973). each subset corresponds exactly with a node in the graph (Fig-
The underlying Dirichlet process has two parameters, a mix- ure 2). It can seen that, given any rooted tree over more than
ing parameter α, which corresponds to the same parameter of two objects, the set of all tree-consistent partitions is a strict
the CRP, and a base distribution, from which the parameters subset of the set of all partitions. Exact inference under the
are drawn at each new table. It is important to specify that we DPM requires that we marginalize over the latent partition,
draw a complete parameterization of all the feature distribu- requiring a sum over the super-exponential number of parti-
tions, θy,f , at each table. We have decided not to share the tions. The BHC approximation works by efﬁciently comput-
marginal distributions, φ, because we are most interested in ing the sum over the exponential number of tree-consistent
knowledge relating the features and labels.           partitions, using a divide-and-conquer approach to combine
  Again, we can represent the model compactly by specify- the results from each subtree. Intuitively, if the tree is chosen
ing the generative process:                           carefully, then the set of tree-consistent partitions will cap-
                                                      ture most of the mass in the posterior. BHC tries to ﬁnd such
              φu ∼  Dir([αu,y : y ∈L])
                                                      a tree by combining Bayesian model selection with a greedy
                (u,n)
              Y     | φ ∼ Discrete(φu)                heuristic.
                  F                                   Just as in classic, agglomerative clustering (Duda et al.,
       G  ∼ DP(α,       Dir([βy,f,x : x ∈Vf ])        2001), BHC starts with all objects assigned to their own clus-
                 f=1 y∈L                              ter and then merges these clusters one by one, implicitly

                                                IJCAI-07
                                                  2601                        9                               Heller and Ghahramani (2005a) show that a speciﬁc choice
                          8                                     π     p H
                     67                               of the prior k = (  k) leads to an approximate inference
                                                      scheme for the DPM. Let α be the corresponding parameter
                     1 2345
                  {9}         [12345]                 from the DPM. Then, we can calculate the prior probability
                {6,8}         [12][345]               for each cluster Tj in the tree built by BHC.
               {3,6,7}        [12][3][45]
              {3,4,5,6}       [12][3][4][5]            initialize each leaf i to have di = α, πi =1
               {1,2,8}        [1][2][345]              for each internal node k do
              {1,2,3,7}       [1][2][3][45]
                                                          dk = αΓ(nk) + d   d
            {1,2,3,4,5}       [1][2][3][4][5]                            leftk rightk
                                                               αΓ(nk )
                                                          πk =
           Inconsistent       [123][45]                          dk
                                                       end for
Figure 2: All tree-consistent partitions represented both as sets of Having built a tree that approximates the posterior distribu-
nodes (left) and collection of leaves (right), and one partition that is tion over partitions, we can use the tree to compute the pos-
not tree-consistent (the sets of leaves [123] is not representable by
an internal node).                                    terior probability of an unseen label. Assume we have an
                                                      unlabeled example Xk associated with the k-th task. Let Ak
                                                                                               k
forming a tree that records which merges were performed. be the set of nodes along the path from the node to the root
                                                      in the tree generated by the BHC algorithm (e.g. in Figure 2,
However, instead of using a distance metric and merging the A { , , , }                    T  ∈  A
nearest clusters, BHC merges the two clusters that maximize 5 = 5 7 8 9 ). Note that the elements i k corre-
                                                      spond to all clusters that task k participates in across all tree-
a statistical hypothesis test. At each step, the algorithm must                                    Y
determine which pair in the set of clusters T1,T2,...,Tm to consistent partitions. Our predictive distribution for will
merge next. Consider two particular clusters Ti and Tj and then be the weighted average of the predictive distributions
let Di and Dj be the set of tasks in each respectively. The for each partition:
                                                                          
BHC algorithm calculates the posterior probability that these                     wi
                                T    T   T               p(Yk|Xk,D,T)=                 p(Yk|Xk,Di),  (4)
two clusters are in fact a single cluster k = i + j. Specif-                         wj
                                                                         T ∈A    j∈Ak
ically, the hypothesis Hk is that the data in Dk = Di ∪ Dj                i  k
are identically distributed with respect to the base model (in     
                                                      where wk = rk          (1 − ri) and p(Yk|Xk,Dk) is the
this case, some Naive Bayes model). The probability of the           i∈Ak /{k}
                          H   p D |H                  predictive distribution under the base model after combining
data in this new cluster under k, ( k k) is simply the                              k
marginal likelihood of the data.                      the data from all the tasks in cluster .
  The alternative hypothesis, H¯k is that the data Di and Dj While the computational complexity of posterior computa-
                                                      tion is quadratic in the number of tasks, Heller and Ghahra-
are, in fact, split into two or more clusters. Computing the                   O n    n      O n
probability associated with this hypothesis would normally mani (2005b) have proposed ( log ) and ( ) random-
require a sum over the super-exponential number of partitions ized variants.
associated with the tasks in Di and Dj. However, the clever
trick of the BHC algorithm is to restrict its attention to tree- 4 Results
consistent partitions. Therefore, the probability of the data
                                                      The utility of the type of sharing that the Clustered Naive
Dk = Di ∪ Dj under H¯k, p(Dk|H¯k)=p(Di|Ti) p(Dj|Tj),
                                                      Bayes supports can only be assessed on real data sets. To that
where p(Di|Ti) is the probability of the data associated with
                                                      end, we evaluated the model’s predictions on a meeting clas-
the tree Ti.Letπk = p(Hk) be the prior probability of the
                                                      siﬁcation data set collected by Rosenstein et al. (2005). The
cluster Tk. Then, we can write p(Dk|Tk) recursively
                                                      data set is split across 21 users from multiple universities, an
 p(Dk|Tk)=πkp(Dk|Hk)+(1−πk)p(Di|Ti)p(Dj|Tj).    (2)   industry lab and a military training exercise. In total, there
                                                      are 3966 labeled meeting requests, with 100-400 meeting re-
Then the posterior probability of Hk is
                                                      quests per user. In the meeting acceptance task, we aim to
                         πkp(Dk|Hk)                   predict whether a user would accept or reject an unseen
             p(Hk|Dk)=              .           (3)
                          p(Dk|Tk)                    meeting request based on a small set of features that describe
                                                      various aspects of the meeting.
We now present the BHC algorithm, whose output is sufﬁ-
                                                        To evaluate the clustered model, we assessed its predictive
cient for approximate Bayesian predictions under our model.
                                                      performance in a transfer learning setting, predicting labels
                 (1)      (n)
 input data D = {D  ,...,D  }                         for a user with sparse data, having observed all the labeled
      model p(X|Y,θ) and prior density f(θ)           data for the remaining users. In particular, we calculated the
 initialize number of clusters c=n, and               receiver-operator characteristic (ROC) curve having trained
                 (i)
         Di = {D   } for i =1,...,n                   on 1,2,4,8,16,32,64, and 128 training examples for each user
 while c>1  do                                        (conditioned on knowledge of all labels for the remaining
      Find the pair Di and Dj with the highest posterior users). Each curve was generated according to results from
           probability of Hk where Tk = Ti + Tj.      twenty random partitions of the users’ data into training and
      Merge Dk ←  Dk ∪ Dj, Tk ← (Ti,Tj)               testing sets. Figure 3 plots the area under the ROC curve as
      Delete Di and Dj, c ← c − 1                     a measure of classiﬁcation performance versus the number of
 end while                                            training examples.

                                                IJCAI-07
                                                  2602       User #1: AB (Mil)    User #3: ED (Mil) User #20: TD (Oregon S.) User #17: TLP (MIT) User #7: NH (Mil)
    1                   1                    1                    1                    1


  0.9                  0.9                  0.9               [20] 0.9                0.9
                     [1 3]
                   [1 2 3]
                                                             ←
                                                      [9 20]
              [1 2]
                    ←
                                                    [20]
                  ←
                                                     ←
  0.8                  0.8                  0.8                  0.8               [12 17] 0.8
                                                                                                        [4 7]
             ←
                             [1 3]
            [1 4 7]
                                                   ←
                                                                                 [17]
                                                                                  ←
                                                                                                       ←
                            ←
                                                  [9 20]
           ←
          [1 3]
                                                                                ←
                                                                            [9 17]

  0.7   [1 5 8]        0.7                  0.7                  0.7                  0.7
                                                 ←
         ←
                                                                           ←
       ←
                                                                       [17]
  0.6                  0.6                  0.6                  0.6                  0.6             [1 5 7]
                                                                                            [1 7]
                                                                      ←
                                                                                                     ←
 area  under ROC
  0.5           Clus.  0.5                  0.5                  0.5                  0.5  ←
                NS
      0 1 2 3 4 5 6 7      0 1 2 3 4 5 6 7      0 1 2 3 4 5 6 7      0 1 2 3 4 5 6 7      0 1 2 3 4 5 6 7
         2n samples           2n samples           2n samples           2n samples          2n samples

Figure 3: Area under the curve (AUC) vs. training size for ﬁve representative users. The AUC varies between 1 (always correct), 0 (always
wrong), and 0.5 (chance). For each experiment we label the (MAP) cluster of users to which the user belongs. If the cluster remains the same
for several experiments, we omit all but the ﬁrst mention. The ﬁrst three examples illustrate improved predictive performance. The last two
examples demonstrate that it is possible for performance to drop below that of the baseline model.


    16 examples 32 examples  64 examples 128 examples        0.1
                                                            NB

                                                             0.08


                                                            − AuROC 0.06
                                                            CNB
                                                             0.04

                                                             0.02

                                                             0

                                                            −0.02


 MMMMMMMMPPPPSSPSSPPPP MMMMMMMMPPPPPPPPSPSSS MMMMMMMMPPPPPPSPPSPSS MMMMMMMMPPPPPPPSSPSPS Mean  Difference AuROC −0.04
                                                              1    2    3    4     5    6    7    8
                                                                             2n Training Samples
Figure 4: Progression of trees found by BHC for 16, 32, 64 and
128 examples per user. Short vertical edges indicate that two tasks Figure 5: The clustered model has more area under the ROC curve
are strongly related. Long vertical edges indicate that the tasks are than the standard model when there is less data available. After 32
unrelated. Key: (M)ilitary, (P)rofessor, (S)RI researcher. training examples, the standard model has enough data to match the
                                                      performance of the clustered model. Dotted lines are standard error.
  From the 21 users, we have selected ﬁve representative
samples. The ﬁrst three examples (users 1, 3 and 20) show most often grouped with other military personnel, and pro-
how the model performs when it is able to use related user’s fessors and SRI researchers are grouped together until there
data to make predictions. With a single labeled data point, is enough data to warrant splitting them apart.
the model groups user 1 with two other military personnel Figure 5 shows the relative performance of the clustered
(users 5 and 8). While at each step the model makes pre- versus standard Naive Bayes model. The clustered variant
dictions by averaging over all tree-consistent partitions, the outperforms the standard model when faced with very few
MAP partition listed in the ﬁgure has the largest contribution. examples. After 32 examples, the models perform roughly
For user 1, the MAP partition changes at each step, provid- equivalently, although the standard model enjoys a slight ad-
ing superior predictive performance. However, for the third vantage that does not grow with more examples.
user in the second ﬁgure, the model chooses and sticks with
the MAP partition that groups the ﬁrst and third user. In the 5 Related Work
third example, User 20 is grouped with user 9 initially, and Some of the earliest work related to transfer learning focused
then again later on. Roughly one third of the users witnessed on sequential transfer in neural networks, using weights from
improved initial performance that tapered off as the number networks trained on related data to bias the learning of net-
of examples grew.                                     works on novel tasks (Caruana, 1997; Pratt, 1993). More re-
  The fourth example (user 17) illustrates that, in some cases, cently, these ideas have been applied to modern supervised
the initial performance for a user with very few samples is not learning algorithms, like support vector machines (Wu and
improved because there are no good candidate related users Dietterich, 2004). More work must be done to understand the
with which to cluster. Finally, the last example shows one connection between these approaches and the kind of sharing
of the four cases where predictions using the clustered model one can expect from the Clustered Naive Bayes model.
leads to worse performance. In this speciﬁc case, the model This work is related to a large body of transfer learning
groups the user 7 with user 1. It is not until 128 samples research conducted in the hierarchical Bayesian framework,
that the model recovers from this mistake and achieves equal in which common prior distributions are used to tie together
performance.                                          model components across multiple data sets. The clustered
  Figure 4 shows the trees and corresponding partitions re- model can be seen as an extension of the model ﬁrst pre-
covered by the BHC algorithm as the number of training ex- sented by Rosenstein et al. (2005) for achieving transfer with
amples for each user is increased. Inspecting the partitions, the Naive Bayes model. In that work, they ﬁt a Dirichlet dis-
they fall along understandable lines; military personnel are tribution for each shared parameter across all users. Unfortu-

                                                IJCAI-07
                                                  2603