                          Corpus-Based Knowledge Representation 

                      Alon Y. Halevy                              Jayant Madhavan 
                  University of Washington                     University of Washington 
                Seattle, Washington, U.S.A.                   Seattle, Washington, U.S.A. 
                  alon@cs.washington.edu                      jayant@cs.washington.edu 


                      Abstract                         fragments of knowledge in the corpus can include individ•
                                                       ual KBs, database schemas with or without data instances of 
  A corpus-based knowledge representation system consists 
                                                       the schema, queries written over KBs and databases, and any 
of a large collection of disparate knowledge fragments or 
                                                       form of meta-data associated with them. Unlike a KB that 
schemas, and a rich set of statistics computed over the cor•
                                                       needs careful ontological design, the corpus is a set indepen•
pus. We argue that by collecting such a corpus and comput•
                                                       dent uncoordinated contributions. The intuition is that if the 
ing the appropriate statistics, corpus-based representation of•
                                                       corpus is large enough, then the patterns we identify in it can 
fers an alternative to traditional knowledge representation for 
                                                       be of great use for knowledge intensive tasks. 
a broad class of applications. The key advantage of corpus-
based representation is that we avoid the laborious process of Corpus-based knowledge representation is an outgrowth of 
building a (often brittle) knowledge base. We describe the our work on schema and ontology matching [Doan et al., 
basic building blocks of a corpus-based representation sys• 2001; 2002]. The matching problem (which we elaborate on 
tem and a set of applications for which such a paradigm is in Section 2), is to find a semantic mapping between two dis•
appropriate, including one application where the approach is parate representations, be they database schemas or ontolo•
already showing promising results.                     gies. While a detailed KB about the domain of the represen•
                                                       tations being matched could be of great value for the match•
                                                       ing task, such KBs rarely exist, and their construction is not 
1 Introduction                                         cost effective. Instead, our approach to matching is based 
Declarative representation of knowledge has long been ac• on analyzing the variations of representations in a corpus of 
knowledged as a key building block of Al systems. A knowl• schemas. This paper extends the idea of using a corpus to a 
edge base (KB) with associated reasoning mechanisms can general approach to knowledge representation. Specifically, 
serve as a backbone for tasks such as query answering, learn• we identify other tasks in which corpus-based representation 
ing and diagnosis. One of the key challenges in deploy• can be useful, and then define preliminary version for the con•
ing knowledge representation (KR) systems is the cost and tents of a corpus and the interfaces it should support. 
the complexity of building the knowledge base, especially in We emphasize that corpus-based representation is not a re•
cases where it needs to cover a broad domain. Building a placement for traditional knowledge representation. There 
KB is highly labor intensive for several reasons. First, the are many tasks in which very finely tuned reasoning is re•
acquisition of domain knowledge and articulating it in a for• quired, and such reasoning can only be done with a very well 
mal language can be incredibly difficult. Second, since the designed knowledge base (e.g., medical diagnosis, monitor•
KB needs to form a coherent logical whole, it is hard to dis• ing spacecraft, and making sense of tax laws). Our goal is 
tribute its creation of a knowledge base over multiple experts to explore the space of problems in which the laborious con•
or knowledge engineers. Finally, no matter how broad the struction of knowledge bases can be avoided. 
KB, it will be brittle at the edges of its knowledge - it can 
only be used for tasks whose knowledge needs have been an• 2 A motivating application: matching 
ticipated in advance. While all of the aforementioned diffi• disparate representations 
culties have received significant attention, together they still 
remain a fundamental challenge for knowledge representation The discussion in this paper spans many kinds of representa•
technology.                                            tions, ranging from database schema to ontologies in expres•
  This paper argues that there is an alternative approach to sive KR formalisms. Collectively, we use the term domain 
knowledge representation that has the potential of providing model to refer to representations in any of these formalisms. 
competitive functionality for a broad class of applications. The term schema refers to intensional knowledge, such as the 
In a nutshell, this approach, which we term corpus based names of database tables and their respective columns, or to 
representation, is based on collecting a large corpus of dis• the terminological component of a KB. The ground facts (or, 
parate fragments of knowledge, and building a set of tools rows in a database) are called instance data. 
that are based on analyzing properties of the corpus. The Sharing data among multiple data sources and applica-


INVITED SPEAKERS                                                                                      1567  tions is a problem that arises time and again in large en•    Do and Rahm, 2002]. 
 terprises, B2B settings, coordination between government 
 agencies, large-scale science projects, and on the World-     Corpus-based matching 
 Wide Web. The problem has received significant attention      Conceivably, detailed knowledge about the domain in which 
 in both the Al and Database communities, and over the         the matching is being performed can be an important resource 
 years, several paradigms for data sharing have been imple•    in a schema matching system. However, creating an appro•
 mented, each appropriate for certain scenarios. Examples in•  priate KB is often hard, and furthermore, the result may be 
 clude data warehousing oriented architectures, data integra•  brittle in the sense it only helps on its domain of coverage, 
tion systems [Lenzerini, 2002; Draper et al, 2001], message    and only provides a single perspective on the domain. 
passing systems (e.g., [MQSERIES, 2003]), web services,          We are pursuing an alternate approach in which knowledge 
and peer-data management systems LHalevy et al, 2003b;         is gleaned by analyzing a large corpus of database schemas 
 Kalnis et al, 2002; Bernstein et al, 2002].[                  and previously validated mappings. There are two types of 
   No matter what paradigm we employ to share the data, a      knowledge that we can glean from such a corpus. First, we 
key problem is the semantic heterogeneity between the do•      can learn the different ways in which words (or terms) are 
main models of data sources that were originally designed      used in database structures (i.e., as relation names, attribute 
independently. Thus, to obtain meaningful interoperation,      names and data values). Second, the validated mappings 
one needs a semantic mapping between the schemas. A            show how variations in term usages correspond to each other 
semantic mapping is a set of expressions that specify how      in disparate structures. 
the data in one database corresponds to the data in another      Although such a corpus is not easy to construct, it is a very 
database [Madhavan et al, 2002]. While languages for spec•     different kind of activity than building a detailed and com•
ifying semantic mappings have been developed and are well      prehensive knowledge base. It does not require the careful 
understood (see [Lenzerini, 2002; Ha1evy, 2001 ] for surveys), ontological design as a knowledge base does, nor the careful 
the creation of semantic mappings has become a key bottle•     control on its contents, thereby removing the key bottlenecks 
neck as it is labor intensive and error-prone.                 present in the design of knowledge bases. The corpus of•
   The goal of schema matching is to assist a human to relate  fers multiple perspectives on modeling a particular domain, 
two domain models. Complete automation of the process is       including different coverages of the domain. Thereby, it is 
unlikely to be possible, but the goal is to significantly increase more likely to provide knowledge that is useful for matching 
the productivity of human experts. The matching problem is     two disparate schemas. 
difficult because it requires understanding the underlying se•   In our initial work on the LSD system [Doan et al, 2001], 
mantics of the domain models being matched. While a do•        we investigated the benefit of learning from previously val•
main model (with its instance data) provides many clues on     idated mappings. In [Doan et al, 2001] we considered the 
its intended semantics, it does not suffice in order to relate it case where multiple data sources are mapped to a single me•
to a different domain model.                                   diated schema, on which users pose queries. We provided 
   The process of generating a semantic mapping has tradi•     LSD with the mediated schema and a set of training matches 
tionally been divided into two phases. The first phase finds a for some data sources. LSD used these matches to learn mod•
match between the two schemas. The match result is a set       els of the elements of the mediated schema. Since no single 
of correspondences between elements in the two schemas,        learning algorithm captures all the cues from the domain, we 
stating that these elements are somehow related. For ex•       used a multi-strategy approach that combined the predictions 
ample, a correspondence may state that agent in one do•        of several learners. We then asked LSD to predict matches be•
main model corresponds to contact in another. The second       tween the mediated schema and a set of test schemas. Our ex•
phase builds on the correspondences by creating the mapping    periments showed that (1) it is possible to achieve high accu•
expressions. The mapping expressions, often expressed as       racy with multi-strategy learning, and (2) additional accuracy 
queries or rules, enable translating data from one data source is obtained by considering domain constraints (i.e., a simple 
to another, or reformulating a query over one data source into form of domain knowledge). Overall, LSD achieved match•
a query on the other. A plethora of techniques have been pro•  ing accuracy of 75-90% on small to medium sized schemas 
posed for schema matching: see [Rahm and Bernstein, 2001]      of data sources on the Web. We extended LSD to consider 
for a survey, and [Noy and Musen, 2002; Doan et al, 2002;      simple taxonomies of concepts in [Doan et al, 2002]. 
Do and Rahm, 2002] for some work since then. Collectively,       In recent work [Madhavan et al, 2003], we investigate the 
these techniques mirror the heuristics that a human designer   benefit of a corpus of schemas and matches, and the ability 
may follow. For example, techniques have considered ex•        to use such a corpus to predict mappings between a pair of 
ploiting relationships between names of elements in the do•    schemas that have not been previously seen. Like in LSD, we 
main models, structural similarities between them, similari•   learn models for elements in the corpus, using both the infor•
ties in data values, and even correlations between values in   mation available in the schema and validated matches that are 
different attributes. Several recent works on schema match•    provided in the corpus. Given two schemas, S1 and S2 we 
ing are based on combining multiple techniques in a prin•      calculate for each element in them a similarity vector w.r.t. 

cipled fashion [Madhavan et al, 2001; Doan et al, 2001;        the corpus, i.e., how similar each element in Si is to each el•
                                                               ement in the corpus. Very roughly speaking, if the similarity 
    This paper accompanies an invited talk at the conference that vectors of two elements a\ E S1 and a2 € S2 are similar to 
surveys the successes and challenges in data integration to date. each other, then we predict that a1 matches a2. The results of 


1568                                                                                                INVITED SPEAKERS our experiments show that (1) even with a modest corpus of 
 10 schemas we are able to achieve good accuracy, and (2) the 
correct matches found by using the corpus and those found 
by other previously known techniques overlap, but have sig•
nificant differences. Hence, the use of the corpus is finding 
matches that would not have been predicted by other tech•
niques. 

3 The corpus-based representation principle 
This paper argues that using a corpus of domain models is a 
general technique that can be applied in many applications as 
an alternative or complement to traditional knowledge repre•
sentation. Section 3.1 describes a few such applications, and  Figure 1: A corpus-based representation system contains a large 
Section 3.2 presents the components of a corpus-based repre•   collection of disparate domain models, instance data, validated map•
sentation system.                                              pings between domain models and meta-data. Applications access 
                                                               the corpus and the statistics computed on it through a set of inter•
3.1 Applications of corpus-based representation                faces. The interfaces support different types factual queries and sim•
                                                               ilarity queries. 
The following are classes of applications that can benefit from 
corpus-based representation. 
                                                               certain standards, when these are applicable. One can also 
                                                               imagine using corpus-based representation during the phase 
Web search and query answering: the first class of ap•         of verifying the correctness of a knowledge base. Hence, 
plications concerns various information finding tasks on the   corpus-based representation can be an aid in creating tradi•
Web (or intranet). The first application is query answering on tional knowledge-based systems. 
the web (e.g., [Kwok et al, 2001; Radev et al, 2002]): a nat•    On the querying side, corpus-based representation can fa•
ural language query is posed to a web search interface, and    cilitate the querying of unfamiliar data. Specifically, consider 
rather than finding relevant pages, the search engine tries to a tool that enables you to pose a query using your own termi•
find the answers to the query. A second, very related appli•   nology to any database. The tool would then use the corpus to 
cation, is focused crawling (e.g., [Sizov et al, 2003]), where propose reformulations of the your query that are well formed 
the search engine is given a particular topic, and it tries to find w.r.t. the schemas of the database at hand. The tool may pro•
pages relevant to it by starting at a set of initial pages, and pose a few such queries (possibly with example answers), and 
crawling from them in a focused fashion. In both of these ap•  let you choose among them or refine them. 
plications, the presence of additional domain knowledge has 
been shown or argued to be useful. However, the cost of con•
structing knowledge bases with such wide domain coverage       General properties of candidate applications: the exact 
would be prohibitive.                                          characterization of which applications lend themselves to 
                                                               corpus-based representation is a subject for future research. 
                                                               One property of such applications that immediately stands 
Creating and querying structured knowledge: one of the 
                                                               out is that they use the knowledge base as an aid in resolv•
greatest impediments to using database and knowledge base 
                                                               ing ambiguity. That is, the applications use a set of tech•
technology is the conceptual difficulty of dealing with struc•
                                                               niques to propose plausible answers to a task, and then use 
tured data versus unstructured text (the so called structure 
                                                               the knowledge base to help rank the answers, rule out certain 
chasm [Halevy et al, 2003a]). It is well known that creating 
                                                               ones, or guide the search for answers. Hence, beyond the ap•
domain models (whether for database schema or knowledge 
                                                               plications mentioned above, corpus-based representation can 
bases) is a conceptual effort. Querying requires a different 
                                                               be useful in learning tasks, natural language processing and 
kind of effort - understanding someone else's domain model. 
                                                               interfaces [Popescu et al, 2003], and information retrieval. 
Hence, a significant challenge for the KR and Database com•
                                                               In fact, as we explain in Section 4, corpus-based representa•
munities is to create tools that facilitate the creation and 
                                                               tion was inspired by the use of corpora (of different kinds!) 
querying of structured data. [Halevy et al, 2003a] argues 
                                                               in natural language processing and information retrieval. In 
that a corpus of schemas enables building several tools for 
                                                               contrast, corpus-based representation would be of limited use 
this purpose. 
                                                               in applications that require intricate logical inference on care•
   One such tool would be a schema design advisor, which 
                                                               fully designed domain models (e.g., medical diagnosis, con•
assists in the authoring of structured data, much in the spirit 
                                                               trol of spacecraft, or reasoning about income-tax rules). 
of an auto-complete feature. A user of the such a tool cre•
ates a schema fragment and some data in a particular domain, 
                                                               3.2 Corpus-based representation systems 
and the tool then proposes extensions to the schema using 
the corpus. The user may choose a schema from the list and     We now offer some specific details on how corpus-based rep•
modify it further to fit the local context. Note that besides  resentation systems can be built and used. We describe the 
time savings, such a tool has other advantages, such as pos•   types of contents we may put in a corpus, and the interfaces 
sibly resulting in better designs and helping users conform to that the corpus provides to an application (see Figure 1). 


INVITED SPEAKERS                                                                                                     1569  Contents of a corpus                                          2. Co-occurring schema elements: For each of the different 
 A corpus can include any kind of information related to struc• uses of a term, which relation names and attributes tend to 
 tured data. In particular, it includes the following.         appear with it? What tend to be the names of related tables 
 1. Various forms of domain-model information: on the          and their attribute names? What tend to be the join predicates 
 less expressive end, the corpus can include relational and    on pairs of tables? Are there clusters of attribute names that 
 object-oriented database schema or entity/relationship dia•   appear in conjunction? Are there mutually exclusive uses of 
 grams, XML DTDs or schemas, possibly with associated in•      attribute names? 
 tegrity constraints (e.g., functional dependencies). On the   3. Similar names: For each of the uses of a term, which other 
 more expressive end, the corpus can include terminologies.    words tend to be used with similar statistical characteristics? 
 2. Instance data: we can include the actual rows of tables    Composite statistics: the same statistics can be applied to 
 (or representative rows), XML documents, or the assertion     partial structures. Examples of partial structures are sets of 
 (A-Box) component of a terminological knowledge base or       data instances, relations with associated attribute names, a re•
 Horn theory. In fact we can include data sets that do not have lation with some data (possibly with missing values). 
a schema (e.g., certain file formats). Note that it is very often Clearly, we need to significantly limit the number of par•
the case that elements in the schema of one model may be in•   tial structures for which we keep statistics (e.g., use tech•
stance data in another model. Hence, the distinction between   niques for discovering partial structures that occur frequently 
schema and instance data is not clean cut.                     e.g., [Polyzotis and Garofalkis, 2002]). Given statistics for 
3. Validated mappings: mappings can be given directly be•      certain partial structures, we can estimate the statistics for 
tween pairs of domain models, or go through an intermediate    other related structures. 
domain model [Madhavan et al., 2002].                          Statistics for schema elements: the same word, used in dif•
4. Queries: queries (posed by users or applications) provide   ferent structures, can have different meanings. Hence, we 
important information about how certain data is used. For ex•  may want to characterize the specific usages of terms in struc•
ample, when a database query performs a join over attributes   tures, and relate them to usage of terms in other structures. 
of two different tables, that indicates that the columns are   For example, in [Madhavan et al., 2003] we learn a classifier 
modeling the same domain (and this often not evident from      for every relation and attribute name in the corpus. Follow•
the schema that only specifies the data type).                 ing [Doan et al, 2001], we use meta-strategy learning. The 
5. Other meta-data: there are many forms of meta-data that     training data used for learning is gleaned from the schema to 
accompany domain models. They range from text descrip•         which the element belongs and the training data of elements 
tions of the meaning of different fields to statistics about ta• that have been mapped to it by a validated mapping in the 
ble cardinalities or histograms on the set of possible values  corpus. Intuitively, the classifier is meant to recognize the 
within a column.                                               particular usage of the term, even if it appears differently in 
   It is important to emphasize that a corpus is not a logi•   another structure. 
cally coherent universal database. Rather, it is a collection 
                                                               Application interfaces to the corpus 
of disparate structures, and there is relatively little control on 
the logical design of elements of the corpus. We expect that   Applications built on a corpus should be able to access di•
the domain-model information of the corpus will be stored      rectly the statistics computed on the corpus, and perform the 
and accessed using tools for model management [Bernstein,      obvious lookups and queries on the corpus. In addition, just 
2003], which attempt to provide a set of operators for manip•  as knowledge based systems provide a set of higher-level in•
ulating models of data (as opposed to the data itself).        terfaces (e.g., in the spirit of [Chaudhri et al., 1998]), the 
                                                               same could be done for corpus-based representation systems. 
Statistics on the corpus                                       The following are examples of functions such an interface 
There is a plethora of possible analyses that can be performed could support. Note that the implementation of these inter•
on such a corpus, and finding the most effective ones is a long faces poses many research challenges. 
term research challenge. Below we describe certain kinds of      We classify the interfaces on whether they ask for factual 
statistics that can be computed over the corpus. We classify   queries, or similarity queries. In all cases, we expect that the 
them according to whether they apply to individual words or    answers returned will be a ranked list of answers. In addition, 
terms, partial structures, or to elements of particular schemas. as in traditional knowledge bases, we should be able to obtain 
Word and term statistics: these statistics are associated with an explanation for the answers. In the notation used below, 
individual words (in any language) and with noun or verb       variables are prefaced by $. 
phrases. These statistics indicate how a word is used in dif•  Factual information: in the simplest case, a query is a non-
ferent roles in structured data. For each of these statistics, ground formula, and the corpus should return the possible 
we can maintain different versions, depending on whether       groundings of the formula. As a simple example, we can ask 
we take into consideration only word stemming, synonym ta•     for the values of $x, where Killed($x, Elvis). A different ex•
bles, inter-language dictionaries, or any combination of these ample would be a higher-order query, e.g., find all the $P, 
three. The statistics include:                                 such that $P(Toyota, Honda). 
1. Term usage: How frequently the term is used as a relation     A more complicated function would take as input a set 
name, attribute name, or in data (as a percent of all of its uses of constants and return formulas that include all of these 
or as a percent of structures in the corpus).                  constants. Here, the goal is to find how two terms are re-


1570                                                                                                INVITED SPEAKERS lated without specifying exactly what role they play in a for• that facilitate the creation, querying and sharing of structured 
mula. For example, the input may be GPA and student ID,        data. That work was partially inspired by the use of large 
and the answers could be GPA(studentlD, $value) and Stu-       corpora in information retrieval and in statistical natural lan•
dent(studentlD, GPA, address). Note that the two terms         guage processing (e.g. [Charniak, 1997J). 
play different roles in the two answers, and that the answers    Several works have considered creating knowledge bases 
are templates for formulas (i.e., schema description), rather  by a large set of independent contributors. The Open Minds 
than particular sets of formulas.                              Project [Singh et al., 2002] collects knowledge from contrib•
   In both of these cases, one can also complement the query   utors on the web and builds a KB of common sense. Al•
with background knowledge. The knowledge may eliminate         though the contributes are independent, the knowledge en•
some answers or result in a different ranking of answers.      try is guided by a set of templates, and hence the creation 
Similarity information: one of the key imports of the cor•     of a coherent KB is possible. [Richardson and Domingos, 
pus is that it contains different ways of saying similar facts. 2003] describe how to create a feedback loop by which con•
Similarity queries try to get at these different perspectives. In sumers of knowledge provide indirect feedback to contribu•
the simplest case, a query can be a ground fact, and the cor•  tors of knowledge as to the usefulness of their contributions, 
pus will return similar ways of saying the same thing. For     thereby ultimately filtering out the bad contributions. 
example, the input can be Class(Lexus, luxury), and the 
output could be LuxuryCar(Lexus, Toyota), or even Car-
Review(Lexus, LuxuryClass, VeryGood, goodTires).               5 Conclusions 
   In a more complex case, the query would consist of two 
terms, and the answer would be sets of facts in which these    The analysis of large corpora is the key to the success of Infor•
terms tend to play similar roles. For example, the input       mation Retrieval techniques and to recent progress on Natural 
could be Review and Referee, and the answer would be           Language Processing. Corpus-based representation is predi•
ljcaiReview(paper1, reviewer2, accept) and A|Journal-          cated on the idea that large corpora of domain models can be 
Referees(round2, paper3, reviewed, reject). The answer         leveraged to create novel tools in service of knowledge repre•
could also specify where the terms are used differently, such  sentation. The key advantage of corpus-based representation 
as refereeing a soccer game and reviewing a movie.             is that it avoids the need for careful logical design of a single 
   In summary, we note some of the fundamental challenges      comprehensive ontology. In addition, since a corpus will con•
in building an interface to a corpus. First, unlike logical    tain multiple ways of representing the same information, it 
knowledge/databases, answers are ranked rather than defining   opens up opportunities for exploring transformations on data 
a boolean condition on data. Second, we are often interested   and addressing problems related to representational hetero•
in similarity querying, looking for different ways of saying   geneity. This paper laid the foundation for corpus-based rep•
the same thing 2. Third, the answers returned often summa•     resentation, by describing preliminary versions of the con•
rize sets of facts or describe schema fragments. Finally, it is tents of a corpus, the interfaces to it, some applications in 
often useful to tell the user what is not an answer.           which it would be a powerful representational tool, and an 
                                                               application from which the approach was inspired. 
4 Related work                                                   Corpus-based representation offers many exciting research 
The corpus-based representation approach stands in stark       challenges, not the least of which is actually collecting a large 
contrast to an approach like the Cyc Project [Lenat and Guha,  enough corpus to be of interest. In addition, there is a ques•
1990] that attempts to build a comprehensive knowledge base    tion about how focused the corpus needs to be in order to be 
of all common sense. A corpus is a collection of any set of    useful in a particular domain. That is, can it only have domain 
small domain models that are not centrally designed or co-     models in that domain? Can domains models in other (pos•
ordinated. Importantly, the corpus is likely to include sev•   sibly related) domains be useful, or do they only introduce 
eral models of the same domain. One of the key engineer•       noise that degrades the performance? Finally, no matter how 
ing challenges that arose in building Cyc was how to enable    a corpus is constructed, it can probably be manually tuned to 
several knowledge engineers to add to such a large knowl•      perform even better. What form does such tuning take? While 
edge base without having to understand the entire knowl•       the challenges for corpus-based representation are enormous, 
edge base. Contexts (e.g., [Guha, 1991; Nayak, 1994;           we believe the payoffs could be huge, and the results can pro•
Giunchiglia and Ghidini, 1998]) are a mechanism proposed       foundly impact how we create and use structured knowledge. 
to address this complexity, where each knowledge engineer 
can focus on a small knowledge base, and then a set of lifting 
rules makes sure the entire knowledge base is semantically re• Acknowledgments 
lated. Unlike a corpus, contexts are still logically coordinated 
                                                               Many colleagues have contributed to the ideas that led to 
through a set of lifting rules. 
                                                               corpus-based representation. We would like to thank Phil 
  The use of a corpus of structured data was first proposed 
                                                               Bernstein, Anhai Doan, Pedro Domingos, Oren Etzioni, Zack 
in [Halevy et al., 2003a]. There, the goal was to create tools 
                                                               Ives, Pradeep Shenoy, and Igor Tatarinov. Funding was pro•
                                                               vided by an NSF CAREER/PECASE Grant 11S-9985114, 
   2There is a large literature on similarity querying in databases 
and content based image retrieval, but these focus on similarities ITR Grant IIS-0205635, a Sloan Fellowship, and gifts from 
between objects represented as points in multi-dimensional space. Microsoft Research Intel, NEC and NTT. 


INVITED SPEAKERS                                                                                                     1571 