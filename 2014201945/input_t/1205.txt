                 Learning Web Page Scores by Error Back-Propagation

                        Michelangelo Diligenti, Marco Gori, Marco Maggini
                             Dipartimento di Ingegneria dell’Informazione
                                           Universita` di Siena
                                   Via Roma 56, I-53100 Siena (Italy)
                                  {michi, marco, maggini}@dii.unisi.it

                    Abstract                          random walker minimizing the distance of the scores assigned
                                                      by the walker from the target values provided by a supervisor
    In this paper we present a novel algorithm to learn a for some nodes in the graph.
    score distribution over the nodes of a labeled graph Previous work in the same ﬁeld achieved very limited re-
    (directed or undirected). Markov Chain theory is  sults. For example, in [Kondor and Lafferty, 2002] the au-
    used to deﬁne the model of a random walker that   thors present an algorithm that processes only non labeled
    converges to a score distribution which depends   graphs, while the technique presented in [Chang et al., 2000]
    both on the graph connectivity and on the node la- is very application speciﬁc and features very limited general-
    bels. A supervised learning task is deﬁned on the ization capabilities. In [Tsoi et al., 2003] a method to modify
    given graph by assigning a target score for some  PageRank scores computed over a collection of pages on the
    nodes and a training algorithm based on error back- basis of a supervisor’s feedback is proposed. This algorithm
    propagation through the graph is devised to learn has limited approximation capabilities, since it controls only
    the model parameters. The trained model can as-   a term of the PageRank equation and it does not optimize how
    sign scores to the graph nodes generalizing the cri- the scores ﬂow though the connectivity graph.
    teria provided by the supervisor in the examples.   The algorithm presented in this paper allows to learn target
    The proposed algorithm has been applied to learn a scores assigned as real numbers, and this makes it more gen-
    ranking function for Web pages. The experimental  eral than the one proposed in [Diligenti et al., 2003] which
    results show the effectiveness of the proposed tech- allows only to specify a set of nodes whose scores should be
    nique in reorganizing the rank accordingly to the increased or decreased.
    examples provided in the training set.              The paper is organized as follows. In the next section the
                                                      web surfer model is introduced. Then, in section 3 the learn-
1  Introduction                                       ing algorithm based on error back-propagation is described,
  In this paper we propose an algorithm able to learn a while the experimental results are presented in section 4. Fi-
generic distribution of values over the nodes of a graph with nally, in section 5 the conclusions are drawn.
symbolic labels. In many applications, the input data is nat-
urally represented by a labeled graph. However, the actual 2 Random Walks
lack of adequate algorithms to directly process this structured We consider the model of a graph walker that can perform
data commonly forces to convert the input patterns to vectors one out of two atomic actions while visiting the Web graph:
of real numbers, loosing signiﬁcant information. In particu- jumping to a node of the graph (action J) and following a
lar, “Web like” data structures are common in many pattern hyperlink from the current node (action l).
recognition tasks and possible applications range from object In general, the action taken by the surfer will depend on
recognition in segmented images to molecular analysis in bio- the label and outlinks from the node where it is located. We
informatics, and so on.                               can model the user’s behavior by a set of probabilities which
  Like in the PageRank algorithm [Page et al., 1998], the depend on the current node: x(l|q) is the probability of fol-
score of a node is assumed to correspond to the probability lowing one hyperlink from node q, and x(J|q) = 1 − x(l|q)
that a random walker will visit that node at any given time. is the probability of jumping from node q.
The score distribution computed by the random walker de- The two actions need to specify their targets: x(p|q, J)
pends both on the connectivity graph and the labels assigned is the probability of jumping from node q to node p, and
to each node. Markov Chain theory allows to deﬁne a set x(p|q, l) is the probability of selecting a hyperlink from node
of constrains on the parameters of the random walker in or- q to node p. x(p|q, l) is not null only for the nodes p linked
der to guarantee some desired features like convergence and directly by node q, i.e. p ∈ ch(q), being ch(q) the set
independence of the ﬁnal distribution on the initial state. A of the children of node q in the graph G. These sets of
learning algorithm which is based on error back-propagation values must satisfy the probability normalization constraints
                                                              P                     P
through the graph is used to optimize the parameters of the ∀q ∈ G p∈G x(p|q, J) = 1 and p∈ch(q) x(p|q, l) = 1.  The model considers a temporal sequence of actions per- tendency of the surfer to follow a hyperlink from a node with
formed by the surfer and it can be used to compute the prob- label cq to a node with label cp, the parameter x(cp|cq, J) is
                                                                                                        q
ability xp(t) that the surfer is located in node p at time t. The the probability of the surfer to jump from a node with label c
                                                                        p
probability distribution on the nodes of the Web is updated by to a node with label c and x(l|ci) = 1 − x(J|ci) represents
taking into account the actions at time t + 1 using the follow- the probability that the surfer decides to follow a link out of
ing equation                                          a node with label ci. In the following, we indicate as P the
                   X                                  set of model parameters. T , J , B indicate transition, jump
    xp(t + 1)  =      x(p|q, J) · x(J|q) · xq(t) + (1) and behavior parameters, respectively. In particular, it holds
                  q∈G                                 that: P = {T , J , B}, T = {x(ci|cj, l) i, j = 1, . . . , n},
                    X                                 J  = {x(ci|cj, J) i, j = 1, . . . , n}, and B = {x(l|ci) i =
               +        x(p|q, l) · x(l|q) · xq(t) ,  1, . . . , n}. Under these assumptions, the random surfer equa-
                  q∈pa(p)                             tion becomes,
                                                                            p q
where pa(p) is the set of the parents of node p. The score           X   x(c |c , J)     q
                                                       xp(t + 1) =                 · x(J|c ) · xq(t) + (2)
xp of a node p is computed using the stationary distri-                     |cp|
bution of the Markov Chain deﬁned by equation (1) (i.e.              q∈G
x  =  lim    x (t)). The probability distribution over all                      p  q
 p      t→∞   p                                                     X        x(c |c , l)        q
the nodes at time t is represented by the vector x(t) =          +       P          k  q   · x(l|c )·xq(t) ,
               0                                                           k∈ch(q) x(c |c , l)
[x1(t), . . . , xN (t)] , being N the total number of nodes.       q∈pa(p)
  An interesting property of this model is that, under the as-             p q
sumption that x(J|q) 6= 0 and x(p|q, J) 6= 0, ∀p, q ∈ G, the Typically, the values x(c |c , l) are initialized to 1, such that
stationary distribution x? does not depend on the initial state all links are equally likely to be followed. Because of the
vector x(0) (see [Diligenti et al., 2002]). For example this is model assumptions, the stronger is the tendency of the surfer
true for the PageRank equation for which x(J|q) = 1 − d, to move to a node with a speciﬁc label, the higher are the
being 0 < d < 1, and x(p|q, J) = 1 .                  scores associated to the nodes with that label. These assump-
                             N                        tions reduce the number of free parameters to 2·(n2−n)+n =
  The random walker model of equation (1) is a simpliﬁed  2
version of the model proposed in [Diligenti et al., 2002], 2 · n − n, being n the number of distinct labels.
where two additional actions are considered by allowing the 3.1 Learning the parameters
surfer to follow backlinks or to remain in the same node. The
learning algorithm proposed in the next section can be easily We assume that a supervisor provides a set of examples E
extended to this more general model.                  in the graph G, where each example s ∈ E is a node associ-
                                                                         d
                                                      ated to a target score xs.
3  Learning the score distribution                      Consider the following cost function for the scores com-
                                                      puted by a surfer model using the set of parameters P,
  The random surfer assigns the scores according to the prob-
                                                                   1 X         1 X   1
abilities x(p|q, J), x(J|q), x(p|q, l), and x(l|q). In most of EP =      eP =          · (xP − xd)2 , (3)
                                                                  |E|     s   |E|    2    s    s
the approaches proposed in the literature, these parameters          s∈E         s∈E
are predeﬁned or computed from some features describing
                                                             P
the node p and/or the node q. For example, setting the param- where es is called instant error for the node s. Such error
                                             1
eters to x(l|q) = d, x(J|q) = 1 − d, x(p|q, J) = N , and takes into account the distance between the target score and
            1                                         the value returned by the ranking function.
x(p|q, l) = ch(q) , we obtain the popular PageRank random
surfer [Page et al., 1998].                             We want to minimize the cost function (3) by gradient de-
  In the general case, the model has a number of parame- scent with respect to a generic parameter w ∈ P, i.e.,
ters quadratic in the number of nodes and it is infeasible to            ∂EP         η  X  ∂eP
estimate these parameters without any assumption to reduce     w0 = w − η     = w −          s  .     (4)
                                                                          ∂w         |E|    ∂w
their number. In particular, we assume that the labels of nodes                         s∈E
can assume n distinct values C = {c , . . . , c } and that the
                               1      n                 Using a procedure similar to the back-propagation through
surfer’s behavior for any node p is dependent only on its label
                                                      time algorithm (BPTT) [Haykin, 1994], commonly used in
cp ∈ C and the labels of the nodes pointed by p. Therefore,
                                                      training recurrent neural networks, for each node s in the
the parameters of the model are rewritten as:
                                                      training set we unfold the graph starting from s at time T
                      x(cp|cq, l)                     and moving backward in time through the path followed by
     x(p|q, l) =
                  P        x(ck|cq, l)                the random surfer. The graph unfolding is basically a back-
                    k∈ch(q)                           ward breadth-ﬁrst visit of the graph, starting from the target
                  x(cp|cq, J)
    x(p|q, J) =                                       node at time T , where each new unfolded level corresponds
                     |cp|                             to moving backward of one time step. Each unfolded node is
       x(l|p) =   x(l|cp)   x(J|p) =   x(J|cp)        associated to the variable xq(T −t) where q is the correspond-
                                                      ing graph node, t is the number of backward steps from the
where cq ∈ C is the label of node q and |cp| is the number of target node. Similarly, the parameters of the model are split
nodes with label cp. The parameter x(cp|cq, l) represents the into an independent set of parameters corresponding to the                                                                                          z p
                           x   (T)                                   X                 x(c |c , l)(t)
            b               b          U(T)={b}
                                                               =         δz(t + 1) · P        k  p     ·
       a              P(t−1) P(t−1)                                                        x(c |c , l)(t)
                                                                   z∈ch(p)           k∈ch(p)
                 x a  (T−1)    x c  (T−1)  U(T−1)={a,c}
             c                                                  ·  x(l|cp)(t) =
                      P(t−2) P(t−2)
                                       U(T−2)={a}
                           x a  (T−2)                                X
                                                               =         δ (t + 1) · P (p, t → z, t + 1) =
                           P(t−3)                                         z
                     x   (T−3)         U(T−3)={a}
                     a                                             z∈ch(p)
        (a)               (b)                                        P    d
                                                               =   (xs − xs) · P (p, t → s, T ) ,
Figure 1: (a) A graph composed by three nodes. (b) The ﬁrst four where ch(p) is the set of children of node p in the unfolding
levels of the unfolding for the graph, starting from node b at time and P (p, t1 → z, t2) indicates the probability of the surfer to
T . U(t) indicates the set of nodes at the level t of the unfolding follow a path in the unfolding connecting the node p at time
(corresponding to the time step t). In the example, U(T ) = {b}, t1 to the node z at time t2. Only the inﬂuence of transition
U(T − 1) = {a, c}, U(T − 2) = {a} and U(T − 3) = {a}. parameters on the score of node p was taken into account in
                                                      the propagation of the delta values.
                                                        Neglecting the inﬂuence of the jump parameters (i.e. tak-
                                                      ing into account only the inﬂuence due to link following) is
different time steps. We indicate with P(t) the model param- an accurate approximation when the probability of following
eters at time t and with w(t) ∈ P(t) the value of any given a link is much higher than the probability of jumping (like it is
parameter at time t. An inﬁnite unfolding is perfectly equiv- commonly assumed in PageRank). However, the experimen-
alent to the original graph with respect to the computation of tal results show that such an approximation provides good
         P
the score xs (T ) for a target node s. However, the unfolding results in the general case.
takes into account only the score propagation due to the for- A generic node p at time t has inﬂuence over a supervised
ward links, whereas the contribution of jumps (coming from node s at time T proportional to the probability P (p, t →
any node in the graph) is neglected.                  s, T ) that the surfer will follow the path from the node p at
  In the following, we indicate as Us(t) the set of nodes con- time t to the central node s at time T . An implication of this
tained in the unfolding centered on node s for the time step t. result is that farther nodes will have less inﬂuence, given the
See ﬁgure 1 for an example of graph unfolding.        exponential decrease of the probability of following a path
                                                                                                  P
  Thanks to the parameter decoupling in the unfolding, a pa-                                    ∂es
                                                      with its length. In particular, a node p inﬂuences P only
rameter w(t) inﬂuences only the scores at the following time                                   ∂xp (t)
step. Thus, the derivative of the instant error with respect to a if P (p, t → s, T ) 6= 0. Since Us(T − t) is the set of nodes
parameter w(t) can be written as,                     from which it is possible to reach node s at time T starting
                                                      at time t, only for the nodes in this set it holds that P (p, t →
          P              P        P
        ∂es     X      ∂es      ∂xp (t + 1)           s, T ) 6= 0. Thus, in equation (8) not all the score variables at
             =        P        ·          .     (5)
       ∂w(t)        ∂xp (t + 1)   ∂w(t)               time t + 1 must be considered in the application of the chain
                p∈G                                   rule. Inserting equation (8) into (6), and limiting the chain
  Assuming that the parameters of the surfer are stationary rule only to the nodes with a non-zero inﬂuence, yields,
(i.e. they do not depend on time: w(t) = w ∀t), the derivative P            T −1
                                                        ∂es         P    d  X      X
with respect to a single parameter can be obtained summing    =   (xs − xs)              P (p, t → s, T ) ·
all the contributions for the single time steps,         ∂w
                                                                           t=−∞ p∈Us(t+1)
        P    T −1          P        P                                P
      ∂e                 ∂e       ∂x (t + 1)                      ∂xp (t + 1)
        s    X    X        s        p                          ·             .
          =             P       ·           .   (6)
      ∂w              ∂xp (t + 1)   ∂w(t)                            ∂w(t)
            t=−∞ p∈G
                                                        Since the stable point of the system does not depend on the
              P
            ∂es                                       initial state, without loss of generality, the unfolding can be
  The term ∂xP (t+1) in equation (6) can not be directly com-
            p                                         interrupted at time 1 (the graph is unfolded T times) assuming
puted, unless when t = T − 1, s = p. In this latter case, it that T is large enough so that the state before time 1 has no
holds that,
                  P                                   effect on the ﬁnal state (this is also expressed by the fact that
                ∂es       P       d                   the probability of following a path in the unfolding P (p, t →
                 P    = xs (T ) − xs .          (7)
              ∂xs (T )                                s, T ) converges to 0 when the path length T − t increases),
                     P
                   ∂es                                     P                T −1
  Let δp(t) indicate P . If p is not a target node, then ∂e
                  ∂xp (t)                                  s         P    d X     X
   P                                                           =   (xs − xs)            P (p, t → s, T ) ·
 ∂es                                                      ∂w
  P   can be rewritten as,                                                  t=1 p∈Us(t+1)
∂xp (t)
                                                                     P
    P                        P        P                            ∂xp (t + 1)
  ∂es               X      ∂es      ∂xz (t + 1)                 ·             .                       (9)
   P     =   δp(t) =       P       ·    P     ≈                      ∂w(t)
 ∂xp (t)                ∂xz (t + 1)   ∂xp (t)
                    z∈G                                 In the following sections, we compute the derivative
                        P        P                    ∂xP (t+1)
              X       ∂es      ∂xz (t + 1)              p
         ≈                    ·          =      (8)     ∂w(t) to be inserted into equation (9) for a transition, jump
                   ∂xP (t + 1)   ∂xP (t)
             z∈ch(p)  z            p                  and behavior parameter.3.2  Learning the transition parameters               where the surfer model is assumed to respect the probabilistic
  For a transition parameter w = x(ci|cj, l), equation (2) constrains at every time step, i.e. x(J|ci)(t) = 1−x(l|ci)(t).
             P                                          Merging equation (9) and (11) under the assumption of
           ∂xp (t+1)       p
shows that          = 0 if c 6= ci. On the other hand if
         ∂x(ci|cj ,l)(t)                              model stationarity and convergence at time 0, yields the
 p
c = ci, it holds that,                                derivative of the instant error with respect to a behavior pa-
         P                                            rameter. All parameters can be simultaneously updated as
       ∂xp (t + 1)       X        q    P
                    =         x(l|c )(t)xq (t) ·      described in section 3.2, 3.3 and 3.4 to train the model. Af-
      ∂x(ci|cj, l)(t)
                        q∈pa(p)                       ter the updating, the jump and behavior parameters must be
                          h   x(cp|cq ,l)(t) i        normalized to respect the probabilistic constrains.
                        ∂  P        k  q
                             k∈ch(q) x(c |c ,l)(t)      Finally, using an EM-style algorithm [Dempster et al.,
                     ·                                    ]
                            ∂x(ci|cj, l)(t)           1977  the adapted surfer model can re-surf the Web graph
  The application of the derivative rule for a fraction of func- yielding a new estimate of the scores. The procedure can be
                    p                                 repeated till a stop criterion is satisﬁed. Since the parame-
tions, yields that when c = ci,
          P                                           ters are estimated from a few levels deep unfoldings, only a
        ∂xp (t+1)  P            q     P
                 =    q∈pa(p) x(l|c )(t) · x (t) · (10) small set of nodes must be considered at each iteration. The
       ∂x(ci|cj ,l)(t)  q             q
                      q:c =cj                         main overhead of the proposed learning algorithm is the full
      P         k q       p q            k
        k∈ch(q) x(c |c ,l)(t)−x(c |c ,l)(t)·|k∈ch(q):c =ci| PageRank computation that must be completed at each step.
     ·
                 P        k  q    2
                [  k∈ch(q) x(c |c ,l)(t)]             Our experiments show that usually only very few iterations
                 k                                    are needed to converge.
where |k ∈ ch(q) : c = ci| is the number of children nodes
of q having label ci.
  Without loss of generality, it can be assumed that the sys- 4 Experimental results
tem has already converged at time 0 (i.e. xq = xq(t), t = The experiments were performed on two datasets, which
0, . . . , T, ∀q ∈ G). Inserting equation (10) into (9) and, after were collected by focus crawling the Web [Diligenti et al.,
removing all temporal dependencies from the parameters and 2000], on the topics “wine” and “Playstation”, respectively.
from the node scores under the assumption of model station- The “wine” dataset contains 1.000.000 documents, while the
arity and convergence at time 0, the derivative of the instant “Playstation” one 500.000. The documents were clustered
error for the considered transition parameter is obtained. using a hierarchical version of the k-means algorithm [Duda
  Averaging over all the examples, the model parameters can and Hart, 1973]. In particular, in two different runs, we clus-
be updated by gradient descent according to equation (4). The tered the pages from the dataset on topic “Playstation” into 25
new parameters can be used to compute a new score distri- sets, whereas the dataset on topic “wine” was clustered into
                    P
bution over the nodes xp . Function optimization and score 25 and 100 sets.
estimation can be iterated through multiple epochs, till a ter- Two topic-speciﬁc search engines were built containing the
mination criterion is satisﬁed.                       downloaded documents. The rank of each page in the dataset
                                                      was computed using a random walker producing the standard
3.3  Learning the jump parameters                     PageRank scores. We tested the quality of the ranking func-
  For a jump parameter w = x(ci|cj, J), equation (2) shows tion by submitting a set of queries to the search engine whose
that it holds that,                                   results were sorted by the scores assigned by the random
                        p
    P           ( 0 if c  6= ci                       surfer. By browsing the results, we found pages which were
  ∂xp (t + 1)
                  x(J|cj )(t) P     P       p         authorities on the speciﬁc topic but were (incorrectly) not in-
              =           ·        x  (t) if c =ci
                     |ci|     q∈G   q
∂x(ci|cj, J)(t)                q
                             q:c =cj                  serted in the top positions of the result list. Such pages were
where |c | indicates the number of nodes in the graph G with selected as examples of pages which should get a higher rank
       i                                              (positive examples). A target score equal to one (the maxi-
label ci. The previous equation can be inserted into (9) and,
removing all temporal dependencies under the assumption of mum reachable score according to the model) was assigned
model convergence and stationarity, yields the derivative of to the positive examples. On the other hand, we found pages
the instant error with respect to a jump parameter.   having a high rank which were not authorities for the topic.
                                                      Such pages were selected as negative examples and a target
3.4  Learning the surfer action bias                  score equal to zero was assigned to those pages. The learn-
  Following a procedure similar to that shown for the transi- ing algorithm, as previously described was applied to these
tion and jump parameters, we start computing the derivative datasets, using the selected examples. Only a small set of ex-
∂xP (t+1)                                             amples (3-15) was typically labeled as positive or negative in
  p     for a single parameter.
∂x(l|ci)(t)                                           any single experiment.
  In particular, equation (2) shows that,
   P                          p                       4.1  Analysis of the effects of learning
 ∂xp (t + 1)      X        x(c |ci, l)(t)    P
             =         P          k        · xq −       This ﬁrst group of experiments aims at demonstrating that
 ∂x(l|ci)(t)                    x(c |ci, l)(t)
                 q∈pa(p) k∈ch(q)                      the surfer model as learned by the proposed training algo-
                   q
                 q:c =ci                              rithm, could not be generated by a non-adaptive schema as
                          p
                  X    x(c |ci, J)(t)
             −                     · xP (t) ,  (11)   the focused versions of PageRank proposed in the literature.
                           |cp|      q                  In ﬁgure 2, we show the values of the transition, jump
                  q∈G
                   q                                  and behavior parameters for each cluster, resulting from the
                 q:c =ci                                                                              (a)


                                                                              (b)

                                                      Figure 3: Plots of the normalized probability of the surfer of being
                                                      located in a page of a given cluster before and after learning. (a)
                                                      results on the crawl for the topic “wine” with 100 clusters. (b) results
                                                      on a random graph with 200.000 nodes and 25 clusters.

                                                             1

                                                            0.9

                                                                            Transition
                                                            0.8             Behavior
                                                                            Transition and Behavior
                                                            0.7             Jump
Figure 2: Values of the transition, jump and behavior parameters            Transition and Jump
                                                                            Behavior and Jump
                                                            0.6
after learning for the “Playstation” dataset. For each cluster (x axis),    All

all the n parameters to the other clusters are sequentially plotted. 0.5


                                                            Cost  function 0.4
training sessions for a surfer on the topic “Playstation”. The 0.3
learned parameters represent a complex interaction of the   0.2
resulting surfer with the page clusters where a few clus-   0.1
                                                             0
ters are strongly boosted and a few other strongly demoted.   0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
This behavior (expecially targeted demotions) can not be ex-                   Epoch
pressed by the focused versions of PageRank [Richardson and
Domingos, 2002; Diligenti et al., 2002], which simply boost Figure 4: Values of cost function as a function of the iteration
all the parameters leading to the clusters of relevant pages number. Each plot reports the cost function when a speciﬁc set of
(according to the simplistic assumption that relevant pages parameters is considered during training.
are more likely to point to other relevant pages). This demon-
strates that the trained surfer is able to exploit hierarchies of learning process, the cost function reaches its minimum.
topics to model the complex path leading to good/bad pages. In ﬁgure 5, there are shown the scores of the pages of clus-
  In ﬁgure 3-(a) and 3-(b), there are plotted the probabilities ter 8 of the dataset “Playstation” before and after learning.
that the surfer is located in a page of a speciﬁc cluster. In This cluster was manually detected as strongly on the topic
particular we show the results for the dataset on topic “wine” “Playstation” and 3 pages belonging this cluster were explic-
clustered into 100 groups and a random graph with 200.000 itly inserted in learning set as pages that should get an higher
nodes clustered into 25 groups. The learning algorithm is able score. As expected the rank of the other pages in this cluster
to increase the likelihood of the random surfer to visit pages was boosted up thanks to the capability of the algorithm to
belonging to a set of clusters, while decreasing its probability generalize from a small number of training examples.
of visiting pages belonging to other clusters.
  Figure 4 shows the value of the cost function EP at each 4.2 Qualitative results
iteration. Each plot reports the cost function when a speciﬁc This experiment compares the quality of the page ranks
set of parameters is optimized during training. The behavior before and after training. In particular, some authoritative
parameters are less effective than the jump parameters in op- pages on the topic “wine” were provided as positive exam-
timizing the surfer model. However, neither the behavior or ples, while some authoritative pages on other topics were pro-
the jump parameters are as effective as the transition parame- vided as negative examples. Figure 6 reports the pages which
ters. As expected, when all parameters considered during the obtained the largest variation in their rank. The pages that ob-