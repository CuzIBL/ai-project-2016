                                   Learning Restart Strategies
                          Matteo Gagliolo∗†    and  Jurgen¨ Schmidhuber∗†‡
                                       {matteo,juergen}@idsia.ch
                        ∗ IDSIA, Galleria 2, 6928 Manno (Lugano), Switzerland
                             † University of Lugano, Faculty of Informatics,
                                Via Bufﬁ 13, 6904 Lugano, Switzerland
                                     ‡ TU Munich, Boltzmannstr. 3,
                                  85748 Garching, M¨unchen, Germany

                    Abstract                          be used to determine an optimal uniform strategy, based on
                                                      a constant cutoff [Luby et al., 1993]. In the same paper, the
    Restart strategies are commonly used for minimiz- authors argue that such a strategy is of little practical inter-
    ing the computational cost of randomized algo-    est, as a model of performance is usually unavailable. They
    rithms, but require prior knowledge of the run-time then present a universal restart strategy, whose performance
    distribution in order to be effective. We propose a is worse than the optimal strategy by a logarithmic factor.
    portfolio of two strategies, one ﬁxed, with a prov- These two strategies are based on opposite hypotheses
    able bound on performance, the other based on     about the availability of the RTD. As a third alternative, a run-
    a model of run-time distribution, updated as the  time sample for a set of problem instances could be collected,
    two strategies are run on a sequence of problems. andusedtolearnamodel  of the underlying RTD. There are
    Computational resources are allocated probabilis- two potential obstacles to this approach. First, gathering a
    tically to the two strategies, based on their perfor- meaningful sample of run-time data can be computationally
    mances, using a well-known K-armed bandit prob-   expensive, especially in the very case where a restart strategy
    lem solver. We present bounds on the performance  would be most useful, i. e., when the run-time exhibits huge
    of the resulting technique, and experiments with a variations [Gomes et al., 2000]. Second, a given problem
    satisﬁability problem solver, showing rapid conver- set might contain instances with signiﬁcantly different RTDs:
    gence to a near-optimal execution time.           the obtained model would capture the overall behavior of the
    Keywords: Randomized algorithms, restart strate-  algorithm on the set, but the corresponding restart strategy
    gies, performance modeling, heavy-tailed distribu- would be suboptimal for each instance. Problems met after
    tions, algorithm portfolios, satisﬁability, lifelong- the initial training phase could also be captured poorly by the
    learning, adversarial multi-armed bandit problem. estimated RTD, again leading to a suboptimal strategy.
                                                        Both obstacles are not difﬁcult to circumvent. Training
1  Introduction                                       cost can be reduced by using a small censored sample of
                                                      run-time values, obtained aborting runs that exceed some
When trying to communicate across a lossy communication cutoff time. This obviously has an impact on the model’s
channel with unbounded delays, how long should we wait precision, but, the requirements in this sense are less strict
before resending an unanswered message? In multimodal than one would expect, and a coarse model already allows to
function optimization using gradient descent, how many steps evaluate a near-optimal strategy [Gagliolo and Schmidhuber,
should we take before starting from a new random initial 2006b]. The second problem is unavoidable in a worst case
point? In randomized search over a tree, how deep should setting; but in practical situations we do not expect the RTD
we go before abandoning all hopes of ﬁnding the goal, and of each instance of a problem set to be uncorrelated, and a
starting the search anew? In similar situations, it is desirable sub-optimal uniform strategy based on the set RTD can still
to minimize the time to solve a given problem instance, with have a great advantage over the large bound of the universal.
a given algorithm, whose run-time is a random variable with In this paper we show how Luby’s universal and uniform
a possibly unknown distribution. A restart strategy can be restart can be effectively interleaved, to solve a set of problem
used, to generate a sequence of “cutoff” times, at which the instances. The uniform strategy is based on a non-parametric
algorithm is restarted if unsuccessful.               model of the RTD on the problem set, which is updated every
  It has been proved that knowledge of the run-time distribu-
                                              1       time a new problem instance is solved, in an online fashion.
tion (RTD) for a given algorithm/problem combination can The resulting exploration vs. exploitation tradeoff, is treated
                                                      as a bandit problem: a solver from [Auer et al., 1995] is used
  1In the following, for the sake of readability, we will often refer
to the RTD of a problem instance, meaning the RTD of different to allocate runs among the two strategies, such that, as the
runs of the randomized algorithm of interest on that instance; and model converges, the uniform strategy is favored if the RTD
the RTD of a problem set, meaning the RTD of different runs of the
randomized algorithm of interest, each run on a different instance, uniform randomly picked without replacement from the set.

                                                IJCAI-07
                                                   792of the set is close to the RTD of most instances, and worst- In both cases, an obvious trade-off is faced, between ex-
case bounds on performance are preserved otherwise.   ploration of the performance of the various ak,andexploita-
  In the following, restart strategies (Sect. 2) are brieﬂy in- tion of the solver(s) estimated to be fastest. A well-known,
troduced, followed by a discussion of an “adversarial bandit” and well aged, paradigm for addressing such a trade-off is the
approach to algorithm selection (Sect. 3), which will be used multi-armed bandit problem. In its most basic form [Robbins,
in our strategy (Sect. 4). We give references to other related 1952], it is faced by a greedy gambler, playing a sequence of
work in Sect. 5. Sect. 6 presents experiments with a random- trials against a K-armed slot machine, each trial consisting of
ized satisﬁability problem solver. Sect. 7 discusses potential a choice of one of K available arms, whose rewards are ran-
impact, and viable improvements, of the presented approach. domly generated from different stationary distributions. The
                                                      gambler can then receive the corresponding reward, and re-
2  Optimal restart strategies                         gret what he would have gained, if only he had pulled any
                                                      of the other arms. The aim of the game is to minimize this
A restart strategy consists in executing a sequence of runs of
                                                      regret, deﬁned as the difference between the expected cumu-
a randomized algorithm, in order to solve a same problem
                                                      lative reward of the best arm, and the one cashed in by the
instance, stopping each run r after a time T (r) if no solu-
                                                      gambler. Generally speaking, a bandit problem solver (BPS)
tion is found, and restarting the algorithm with a different
                                                      can be described as a mapping from the entire history of ob-
random seed; it can be operationally deﬁned by a function
                                                      served rewards xk for each arm k to a probability distribution
T : N → R+ producing the sequence of thresholds T (r) em-
                                                      p =(p   , ..., pK ), from which the choice for the successive
ployed. Luby et al. [1993] proved that the optimal restart   1
                                                      trial is picked.
strategy is uniform, i. e. , one in which a constant T (r)=T
is used to bound each run. They show that in this case the In recent works, the original restricting assumptions have
                                                      been progressively relaxed. In the partial information case,
expected value of the total run-time tT can be evaluated as
                                                      only the reward of the pulled arm is revealed to the gam-
                          
                        −   T                         bler. In the non-oblivious adversarial setting, the rewards
                      T    0 F (τ)dτ
              E(tT )=                           (1)   for a given trial can be an arbitrary function of the entire
                           F (T )                     history of the game, and are thus allowed to be deceptive,
  where F (t) is the cumulative distribution function (CDF) but have to be set before the current choice is made. Based
of the run-time for an unbounded run of the algorithm, i. e., on these pessimistic hypotheses, Auer et al. [1995] devised
a function quantifying the probability that the problem is a probabilistic gambling scheme (Exp3), whose regret on a
solved before time t.                                 ﬁnite number of trials is bounded with high probability as
                                              ∗
  If such distribution is known, an optimal cutoff time T can O(M 2/3(K log K)1/3), K being the number of arms, M the
be evaluated minimizing (1). Otherwise, the authors suggest number of trials (see Alg. 1). The algorithm features a ﬁxed
a universal non-uniform restart strategy, with cutoff sequence lower bound γ on the exploration probability, and a parame-
{1, 1, 2, 1, 1, 2, 4, 1, ...},2 proving that its performance tU is ter α controlling the amount of exploitation. Both can be set
bounded with high probability with respect to the expected based on M to obtain the optimal regret rate above.3
run-time E(tT ∗ ) of the optimal uniform strategy, as
                                                      Algorithm 1 Exp3(M) by Auer et al.
          tU <= 192E(tT ∗ )(logE(tT ∗ )+5)      (2)
                                                        set α = ((4K log K)/M )1/3
3  Algorithm selection as a bandit problem              set γ =min{1, ((K log K)/(2M))1/3}
                                                        initialize sk := 0 for k =1, ..., K;
Consider now a sequence B = {b1, ..., bM } of problem in-
                                {           }           for each trial do  
stances, and a set of algorithms A = a1,a2, ..., aK ,such               sk   K
                                                          set pk ∝ (1 + α) , k   pk =1
that each bm is solvable by at least one ak. Without loss                     =1
                                                          pick arm k with probability pˆk := (1 − γ)pk + γ/K
of generality, we assume that the execution of each ak can
                                                          observe reward xk ∈ [0, 1]
be subdivided in sequential steps, and indicate with tk(r),                xkγ
                                                          update sk := sk +
r =1, 2, ..., the duration of the r-thstepofthek-th algorithm             pˆkK
on the current problem. Each ak has its own mechanism for end for
determining tk(r): e. g., for an iterative algorithm, tk(r) can
be the cost of the r-th loop . In this setting, one generally The selection of a best algorithm for a set of problems can
wants to solve the incoming problem instances as fast as pos- be naturally described in a K-armed bandit setting,4 where
sible. This could mean identifying the single best algorithm
for the whole set, or for each instance (algorithm selection 3The original formulation is based on a ﬁnite upper bound g on
[Rice, 1976]); or, in the more general setting of algorithm the cumulative reward of the best arm, which is at most M as each
portfolios [Huberman et al., 1997], the optimal schedule for reward is in [0, 1].Avariant(Exp3.1) of the algorithm is proposed
interleaving the execution of different algorithms, each per- if M is unknown. The authors also present a better bound on the
forming well on different subsets of B.               expected reward.
                                                         4The cases of selection on an per-instance basis, and of algorithm
                                          j−1
  2The sequence is composed of powers of 2:when2 is used portfolios, are more difﬁcult to treat: bandit problem solvers usually
     j                                        j−1
twice 2 is the next. More precisely, r =1, 2, ..., T (r):=2 if provide bounds on regret with respect to a single arm, but the best
    j                  j−1      j−1       j
r =2 − 1; T (r):=T (r − 2 +1)if 2   ≤ r<2  − 1        algorithm might be different for each problem.

                                                IJCAI-07
                                                   793                                                                                                     ∗
“pick arm k” means “execute the next step of algorithm ak” ing instances. Assuming no a priori information about T ,an
(Alg. 2).                                             ideal candidate is the universal strategy of Luby (Sect 2), as
                                                      it invests equal portions of run-time on a set of exponentially
Algorithm 2 Algorithm selection using BPS             spaced restart thresholds. Note that the restart thresholds of
  initialize BPS, p.                                  unsuccessful runs can be exploited in the form of censored
  for each problem do                                 samples. Such a simple scheme would allow to keep the per-
    set tk := 0, rk := 0, k =1, ..., K                formance bound of the universal strategy during training, and
    repeat                                            use the gathered data to estimate Fˆ, and the corresponding
      pick k ∼ p                                      strategy Tˆ, to be exploited for future problem instances. To
      execute step rk +1of ak                         limit the cost of learning, and allow exploitation of the model
      update timer rk := rk +1, tk := tk + tk(rk)     to start as soon as possible, we will instead adopt an online,
      if problem solved then                          or life-long learning approach. We will use the “bandit” al-
        observe reward xk := 1/tk                     gorithm selection scheme described in Sect. 3 at an upper
      else                                            level, to control allocation of computational resources. In
        observe reward xk := 0                        this case, the arms are the two restart strategies, each gen-
      end if                                          erating a sequence of restart thresholds Tk(rk), to bound a
      update p = BPS(k, xk)                           same algorithm s; “pull arm k” simply means “start s on the
    until problem solved                              current problem, with threshold Tk(rk)”. If the problem is
  end for                                             not solved, tk(rk)=Tk(rk),andxk := 0;ifitissolvedin
                                                      a time ts <= Tk(rk), tk(rk)=ts, and a positive reward is
  Note that this simple scheme falls in the partial informa- observed.
tion, non-oblivious adversarial setting, as we only observe re- Our strategy GAMBLER (Alg. 3) starts by solving the ﬁrst
ward for successful algorithms, and this reward depends on problem using the universal strategy alone. After a problem
previous choices: for example, if ak takes R steps to solve a instance is solved, the solution time of the successful run, and
problem, we will not see any reward for the ﬁrst R − 1 pulls. all the collected censored samples from the unsuccessful runs,
Our adversary is not malevolent, but it can be deceptive, es- can be used to update the estimate Fˆ, and evaluate a new Tˆ,
pecially if the tk(r) vary among algorithms:5 even so, any to be used on next problem. To model F ,wewillusethe
bounds on performance of BPS will hold, and given our def- non-parametric product-limit estimator by Kaplan and Meier
inition of reward, the expected performance of (Alg. 2) will [1958]. It guarantees large sample convergence to an arbi-
converge to the one of the best solver for the set of problems. trary distribution, and can account for censored samples as
                                                      well. The non-parametric form is particularly appealing, as it
4  Mixing restart strategies                          does not require any a-priori assumptions about the RTD, it is
                                                      fast to update, and the resulting CDF is a step-wise function,
Traditional statistical tests assess the goodness of a model which makes the evaluation of the integral in (1) inexpensive.
measuring the ﬁt of the corresponding probability density
                                                        To use Exp3 as the BPS we only need to normalize the
function (pdf) along the whole spectrum of observed sam-
                                                      rewards. We can do so by setting a lower and upper bound
                                           ˆ
ples. In our case, the sole purpose of the model F of the on ts, tmin ≤ ts ≤ tmax. In order to limit the impact of
RTD is to set up a restart strategy, in order to gain on future this choice on the convergence rate of Exp3, we adopt a log-
performance, so the only quantity of practical interest is the arithmic reward scheme: upon solution of a problem during a
loss in performance induced by the mismatch of our model. restart with strategy k, rk := (log tmax −log tk)/(log tmax −
  [                            ]
In Gagliolo and Schmidhuber, 2006b we studied the impact  min   k
                            6                         log t  ), t being the total time spent by strategy k on that
of increasingly censored sampling on the performance of an problem, including unsuccesful runs. In this way 1 is the
estimated uniform restart strategy. This impact is quite low, maximum reward, that would be obtained by a strategy that
due to the fact that (1) depends only on the lower quantiles of solved a problem in a time tmin. Note that the use of a log-
the distribution: a rough model of F , obtained from a heavily arithm allows to set tmin and tmax, respectively, to a very
censored sample from the lower portion of the time scale, suf- small and a very large value, such that only the knowledge of
ﬁces to evaluate a near-optimal strategy, allowing to reduce its a very loose bound on ts is required.7
training cost.                                          Let us now analyze how the bound (2) combines with the
  The practical implementation of such a technique requires optimal regret of Exp3. In the following, E(y) is the expected
some censoring strategy to limit the cost of solving the train-
                                                      value of a random variable y, tk represents solution time of
  5                                                   strategy k on a single problem, Rk the number of restarts
   Consider a situation in which a1 solves a problem in one step of ∈
time 10, and gets reward 0.1, while a2 would have solved the same performed, pk [0, 1] the probability of picking a restart
problem with 10 steps of duration 0.1 each, scoring reward 1. strategy, and indices T ∗, U, Tˆ, will label quantities related to
  6Censored sampling is a commonly used technique in lifetime
                                                         7
distribution estimation (see, e. g., [Nelson, 1982]), which allows to Also, the bound is not required to hold: if we always set ts :=
reduce the duration of a sequence of experiments, simply aborting max{ts,tmin}, Exp3 will not be able to discriminate among two
runs exceeding a time threshold. The information carried by these algorithms that always have ts ≤ tmin but will otherwise work; if
runs can still be used for modeling, both in the parametric and non- we always restart algorithms exceeding tmax, Exp3 will still work
                                                         ∗
parametric settings.                                  if F (tmax) > 0). In both cases we can keep xk ∈ [0, 1].

                                                IJCAI-07
                                                   794Algorithm 3 GAMBLER(M)   restart strategy for algorithm s natural combination with the portfolio approach: its solution
  set tmin, tmax                                      is not adaptive, as it simply consists in restarting each algo-
  initialize Exp3(M), pU =1.                          rithm with the universal strategy U. The literature on meta-
  for each problem 1, ..., M do                       learning and algorithm selection [Giraud-Carrier et al., 2004;
    set tk := 0, rk := 0, k =1, 2                     Rice, 1976], algorithm portfolios [Huberman et al., 1997;
    repeat                                            Gomes and Selman, 2001], anytime algorithms [Boddy and
      pick k ∼ p                                      Dean, 1994], provides many examples of the application of
      run s with cutoff Tk(rk +1)                     performance modeling to resource allocation. Most works in
      update timer rk := rk +1, tk := tk +min{ts,Tk(rk)} these ﬁeld adopt a more traditional ofﬂine approach, in which
      if problem solved then                          run-time samples are collected during an often impractically
                            log tmax−log tk
        observe reward xk :=                          long initial training phase, and the obtained model is used
                           log tmax−log tmin          without further tuning on subsequently met problems. As dis-
      else
                                                      cussed in the introduction, this also implies stronger assump-
        observe reward xk := 0
                                                      tions about the representativeness of the training set, and does
      end if
                                                      not guarantee upper bounds on execution time.
      update p =Exp3(k, xk)
    until problem solved                                The Max  K-armed bandit problem is a variation on the
  end for                                             original formulation, in which the estimated reward is only
                                                      updated for the arm obtaining the maximum reward value, on
                                                      a set of plays. Solvers for this game are used in [Cicirello and
the unknown optimal uniform, universal, and estimated opti- Smith, 2005; Streeter and Smith, 2006] to maximize perfor-
mal uniform strategies, respectively, while G will identify our mance quality, on a single problem instance.
novel strategy GAMBLER, interleaving the execution of the Allocation of resources is usually set before each problem
latter two. On a given problem, for which the RTD of our al- instance solution, and possibly updated afterward. Dynamic
gorithm s is F (t), a restart strategy with thresholds T (r) can approaches, in which feedback information from the algo-
be viewed as sequence of independent Bernoulli processes, rithms is exploited to adapt resource allocation during prob-
with success probabilities F (T (r)). The number of restarts lem solution, represent a notable recent trend. In [Lagoudakis
                                                                                   ]
R required to solve the problem is then distributed with pdf and Littman, 2000; Petrik, 2005 , algorithm selection is
         R−1   −                                      modeled as a reinforcement learning problem, a more gen-
p(R)=    r=1 (1  F (T (r)))F (T (R)): for a uniform strat-
egy T (r)=T , p(RT ) is geometric, with E(RT )=1/F (T ). eral setting than the bandit problem proposed in (Sect. 3).
                                                      Models of performance conditional on the dynamic state af-
For any deterministic strategy, the expected time to solve a
                       E(R)                           ter a ﬁxed execution time are used in [Kautz et al., 2002;
problem is T r                 , a monotonic function
          E ( )(t)=    r=1 T (r)                      Wu, 2006] to implement dynamic context-sensitive restart
of E(R).ForU  this implies that the bound (2) can be trans- policies for SAT solvers: also in this case the main differ-
lated into a bound on RU , with the same high probability. ence of our approach is the online learning setting. [Gagliolo
Given our simple reward scheme Exp3 will keep a constant                   ]
p                                                     and Schmidhuber, 2006a present a heuristic dynamic online
  =(pU ,pTˆ) during solution of a single problem. Consider learning approach to resource allocation, in which a condi-
now the worst-case setting in which Tˆ is such that F (Tˆ)=0, tional model of performance is trained and progressively ex-
i. e., the uniform restart Tˆ will never solve the problem. If U ploited during training.
spends RU restarts, in the meantime another RTˆ restarts will
                  ˆ                      −
have been wasted on T , with E(RTˆ)=E(RU )(1 pU )/pU . 6  Experiments
As Exp3  always keeps pU ≥  γ/2, the expected perfor-
mance of GAMBLER on this problem will then be bounded The experiments were conducted using Satz-Rand [Gomes et
                                                      al., 2000],aversionofSatz[Li and Anbulagan, 1997] in
by E(tU + RU Tˆ(2 − γ)/γ) with high probability, and upper
                                                      which random noise inﬂuences the choice of the branching
bounds on tU and RU will also guarantee an upper bound on
                                                      variable. Satz is a modiﬁed version of the complete DPLL
tG.
                                                      procedure, in which the choice of the variable on which to
                                                      branch next follows an heuristic ordering based on ﬁrst and
5  Originality and related work                       second level unit propagation. Satz-rand differs in that, after
Restart strategies are particularly effective if the RTD of the the list is formed, the next variable to branch on is randomly
controlled algorithm exhibits “heavy” tails, , i. e., is Pareto picked among the top h fraction of the list. We present results
for both very large and very small values of time. Such with the heuristic starting from the most constrained vari-
behavior is observed for backtracking search on structured ables, as suggested in [Li and Anbulagan, 1997], and noise
underconstrained problems in [Hogg and Williams, 1994; parameter set to 0.4.
Gomes et al., 2000], but also in other problem domains, such A benchmark from SATLIB was used, consisting of differ-
as computer networks, as in [van Moorsel and Wolter, 2004]. ent sets of “morphed” graph-coloring problems [Gent et al.,
An alternative solution is to run multiple copies of the same 1999]: each graph is composed of the set of common edges
algorithm in parallel (algorithm portfolios [Huberman et al., among two random graphs, plus fractions p and 1−p of the re-
1997; Gomes and Selman, 2001]). In [Luby et al., 1993], maining edges for each graph, chosen as to form regular ring
Luby presents parallel restart strategies, which represent the lattices. Each of the 9 problem sets contains 100 instances,

                                                IJCAI-07
                                                   795                                                                9
generated with a logarithmic grid of 9 different values for the x 10
                      −                                     4
parameter p, from 20 to 2 8, to which we henceforth refer      L*(inst)
with labels 0, ..., 8. Satz-Rand has an initialization cost which 3.5 L*(set)
depends only on the size of the problem, which in this case 3  G
is the same for all sets. This cost was found to be always     U
                                                          2.5  S
bigger than 105.Wethensettmin  =105,   tmax =1012
(about 100 minutes on our machine), and, to be fair with    2
                                                          Time
the universal strategy, we multiplied and shifted its TU (r) as 1.5
tmin(1 + TU (r)). With M = 100, parameters of Exp3 are
                                                            1
determined as α =0.38, γ =0.19.
  Each experiment was repeated 20 times with different ran- 0.5
dom seeds, and a different random order of the instances. For 0
                                                                0  1  2  3   4  5  6  7   8     A
comparison purposes, we repeated the experiments running                    Problem set
the original algorithm, without restarts (labeled S)andthe
universal strategy alone (U). To compare with the ideal per- Figure 1: Experiments with Satz-Rand. Time to solve each set
            ∗                                                     9
formance of T , we evaluated, a posteriori for each run, the of problems (10 ≈ 1 min.). G is our mixed strategy GAMBLER,
T that minimized the cost of the problem set (TL(set)), and whose performance is initially as U, the universal restart strategy,
                                                                             ∗
the cost of each instance (TL(inst)),basedontheactual run- and quickly converges near L (set), a lower bound on the perfor-
                                                      mance of the unknown optimal restart strategy for the whole set.
time outcomes. Note that these can be different for each run, ∗
and their performances are lower bounds on the performances L (inst) is instead a lower bound on the performance of a distinct
              ∗                                       optimal restart strategy for each instance. A is the heterogeneous
of the optimal T , evaluated from the unknown actual RTD                   0   8       95
of, respectively, the problem set, and each problem instance.8 set obtained mixing all sets , ..., . Upper % conﬁdence bounds
                                                      estimated on 20 runs.
The difference of these two bounds is particularly interesting,
as it is an indirect measure of the heterogeneity of the RTD
of the instances in each set. To show the effect of a more and 4.6 × 1012 (3). In practice, only a few “unlucky” runs
heterogeneous problem set, we also run experiments with all have very long completion times, but this is enough to pe-
the 900 instances grouped as a single set (labeled A in the nalize its performance dramatically. GAMBLER scores fairly
graph; M =  900, α =0.18,  γ =0.09), again randomly   against the lower bounds, and U is between 3 and 5 times
mixed. In ﬁgure 6 we present, for each set, the total compu- worst. From problem 6 on, we see that the heavy tail effect
tation time9 for our mixture (G), and the comparison terms. is less marked, but the two lower bounds diverge noticeably:
Upper 95% conﬁdence bounds are given, estimated on the 20 instances in these sets present more heterogeneous RTDs, and
                                                                         ∗
runs. Plots of the RTD and restart cost for each set can be the instance-optimal T (inst) may vary of more than one or-
found in [Gagliolo and Schmidhuber, 2006b].           der of magnitude. Here U is only 2.5-3 times worse than
                                                      GAMBLER. The worst performance of GAMBLER compared
  The results are quite impressive, and further conﬁrm that ∗
the estimated restart strategy Tˆ is not very sensitive to the ﬁt to L (set) can be seen on problem 8,wheretG is about 1.5
                                                      times tL∗ set . Here most problems require a small threshold
of the model Fˆ. In a typical run, between 40% and 80% of     (  )
                                                      T , and a few require one about ten times larger: in this sit-
the sample is censored, as there is only one uncensored run-              ∗
                                                      uation also threshold T (set) varies visibly among different
time for each task; the ﬁt of the model is visibly bad, and it is
                                                               ˆ
limited to the lower portion of the time scale, near or below runs, and T sometimes overestimates the optimal threshold.
                                           ∗          Results on the whole set of problems are better than ex-
Tˆ,butTˆ itself quickly converges close enough to T ,giving                            A
a near-optimal strategy.                              pected: tG/tL∗(set) is about 1.12, but the performance com-
                                                      pared to tL∗(inst) is obviously worst (1.65). This is natural,
  Problems in 0 are easy. Satz-rand solves all instances in                ∗
                                                      as both GAMBLERandT   (set) cannot discriminate different
a similar time, any larger T is an optimal restart value, i.e.,                                    ×   9
restarts are never executed, and the performance of a single restarts for each problem. U completes the set at 1.17 10 ,
                                                      about   times worse than GAMBLER.
copy S is the same as the ones of the optimal restarts. Also 3.4
our GAMBLER quickly learns that, while U has to reach this
value for every problem, resulting in a ﬁve times worst perfor- 7 Conclusions and future work
mance. In problems 1 to 5 we see the effect of a heavy-tailed
                                                      We presented a novel restart strategy (GAMBLER), combin-
RTD.   solves each set with times between ×   10 ( )
     S                                1.6  10    1    ing the universal and optimal strategies by [Luby et al., 1993],
                                                      based on a bandit approach to algorithm selection. GAM-
  8    (  ∗ )=min  {  ( )}≥    (min {  })=   (  )
   As E tT        T E tT     E     T tT    E  tL in   BLER takes the best of two worlds: it preserves the upper
both cases.
  9                                                   bounds of the universal strategy in a worst-case setting, and
   As we are also conducting experiments with parallel portfolios it proved to be effective in a realistic application, with only
of heterogeneous algorithms, and thus we need a common measure
of time, we modiﬁed the original code of Satz-rand adding a counter, a small overhead over an ideal lower bound. It can save a
that is incremented at every loop in the code. The resulting time few orders of magnitude in computation time for algorithms
measure was consistent with the number of backtracks. All results with a heavy-tailed RTD, and, unlike the universal strategy,
                                            9
reported are in these loop cycles: on a 2.4 GHz machine, 10 cycles it does not worsen the performance otherwise. It has a neg-
take about one minute.                                ligible overhead compared to the controlled algorithm, and

                                                IJCAI-07
                                                   796