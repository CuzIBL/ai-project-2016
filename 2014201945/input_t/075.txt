    Constructing Diverse Classifier Ensembles using Artificial Training Examples 

                                     Prem Melville and Raymond J. Mooney 
                                         Department of Computer Sciences 
                                                  University of Texas 
                                             1 University Station, C0500 
                                                  Austin, TX 78712 
                                 melville@cs.utexas.edu, mooney@cs.utexas.edu 

                        Abstract                                 We present a new meta-learner (DECORATE, Diverse En•
                                                               semble Creation by Oppositional Relabeling of Artificial 
     Ensemble methods like bagging and boosting that 
                                                               Training Examples) that uses an existing "strong" learner 
     combine the decisions of multiple hypotheses are 
                                                               (one that provides high accuracy on the training data) to build 
     some of the strongest existing machine learning 
                                                               an effective diverse committee in a fairly simple, straightfor•
     methods. The diversity of the members of an 
                                                               ward manner. This is accomplished by adding different ran•
     ensemble is known to be an important factor in 
                                                               domly constructed examples to the training set when building 
     determining its generalization error. This paper 
                                                               new committee members. These artificially constructed ex•
     presents a new method for generating ensembles 
                                                               amples are given category labels that disagree with the cur•
     that directly constructs diverse hypotheses using 
                                                               rent decision of the committee, thereby easily and directly 
     additional artificially-constructed training exam•
                                                               increasing diversity when a new classifier is trained on the 
     ples. The technique is a simple, general meta-
                                                               augmented data and added to the committee. 
     learner that can use any strong learner as a base 
                                                                 Boosting and bagging provide diversity by sub-sampling 
     classifier to build diverse committees. Experimen•
                                                               or re-weighting the existing training examples. If the train•
     tal results using decision-tree induction as a base 
                                                               ing set is small, this limits the amount of ensemble diversity 
     learner demonstrate that this approach consistently 
                                                               that these methods can obtain. DECORATE ensures diversity 
     achieves higher predictive accuracy than both the 
                                                               on an arbitrarily large set of additional artificial examples. 
     base classifier and bagging (whereas boosting can 
                                                               Therefore, one hypothesis is that it will result in higher gen•
     occasionally decrease accuracy), and also obtains 
                                                               eralization accuracy when the training set is small. This pa•
     higher accuracy than boosting early in the learning 
                                                               per presents experimental results on a wide range of UCI data 
     curve when training data is limited. 
                                                               sets comparing boosting, bagging, and DECORATE, all using 
                                                               J48 decision-tree induction (a Java implementation of C4.5 
1 Introduction                                                 [Quinlan, 1993] introduced in [Witten and Frank, 1999]) as a 
                                                               base learner. Cross-validated learning curves support the hy•
One of the major advances in inductive learning in the past 
                                                               pothesis that "DECORATEd trees" generally result in greater 
decade was the development of ensemble or committee ap•
                                                               classification accuracy for small training sets. 
proaches that learn and retain multiple hypotheses and com•
bine their decisions during classification [Dietterich, 2000]. 
For example, boosting [Freund and Schapire, 1996], an en•      2 Ensembles and Diversity 
semble method that learns a series of "weak" classifiers each  In an ensemble, the combination of the output of several 
one focusing on correcting the errors made by the previous     classifiers is only useful if they disagree on some inputs 
one, has been found to be one of the currently best generic    [Krogh and Vedelsby, 1995]. We refer to the measure of 
inductive classification methods [Hastie et al. , 2001 ].      disagreement as the diversity of the ensemble. There have 
   Constructing a diverse committee in which each hypothesis   been several methods proposed to measure ensemble diver•
is as different as possible (decorrelated with other members   sity [Kuncheva and Whitaker, 2002] — usually dependent 
of the ensemble) while still maintaining consistency with the  on the measure of accuracy. For regression, where the mean 
training data is known to be a theoretically important property squared error is commonly used to measure accuracy, vari•
of a good committee [Krogh and Vedelsby, 1995]. Although       ance can be used as a measure of diversity. So the diver•
all successful ensemble methods encourage diversity to some    sity of the ith classifier on example x can be defined as 
extent, few have focused directly on the goal of maximizing    di\x) = are 

diversity. Existing methods that focus on achieving diversity  the predictions of the iih classifier and the ensemble respec•
[Opitz and Shavlik, 1996; Rosen, 1996] are fairly complex      tively. For this setting Krogh et al 11995] show that the gen•
and are not general meta-learners like bagging [Breiman,       eralization error, E", of the ensemble can be expressed as 
1996] and boosting that can be applied to any base learner     E = E- D, where E and D are the mean error and diversity 
to produce an effective committee [Witten and Frank, 1999].    of the ensemble respectively. 


LEARNING                                                                                                             505    For classification problems, where the 0/1 loss function is non-zero probability of occurrence. In constructing artificial 
most commonly used to measure accuracy, the diversity of       data points, we make the simplifying assumption that the at•

the ith classifier can be defined as:                          tributes are independent. It is possible to more accurately es•
                                                               timate the joint probability distribution of the attributes; but 
                                                        (1)    this would be time consuming and require a lot of data. 
                                                                 In each iteration, the artificially generated examples are la•
However, in this case the above simple linear relationship     beled based on the current ensemble. Given an example, we 
does not hold between E, E and D. But there is still strong    first find the class membership probabilities predicted by the 
reason to believe that increasing diversity should decrease en• ensemble, replacing zero probabilities with a small non-zero 
semble error [Zenobi and Cunningham, 20011. The underly•       value. Labels are then selected, such that the probability of 
ing principle of our approach is to build ensembles of classi• selection is inversely proportional to the current ensemble's 
fiers that are consistent with the training data and maximize  predictions. 
diversity as defined in (1). 
3 DECORATE: Algorithm Definition 
In DECORATE (see Algorithm 1), an ensemble is generated 
iteratively, learning a classifier at each iteration and adding it 
to the current ensemble. We initialize the ensemble to contain 
the classifier trained on the given training data. The classifiers 
in each successive iteration are trained on the original training 
data and also on some artificial data. In each iteration artifi•
cial training examples are generated from the data distribu•
tion; where the number of examples to be generated is spec•
ified as a fraction, , of the training set size. The labels 
for these artificially generated training examples are chosen 
so as to differ maximally from the current ensemble's predic•
tions. The construction of the artificial data is explained in 
greater detail in the following section. We refer to the labeled 
artificially generated training set as the diversity data. We 
train a new classifier on the union of the original training data 
and the diversity data. If adding this new classifier to the cur•
rent ensemble increases the ensemble training error, then we 
reject this classifier, else we add it to the current ensemble. 
This process is repeated until we reach the desired committee 
size or exceed the maximum number of iterations. 
  To classify an unlabeled example, x, we employ the fol•

lowing method. Each base classifier, Ci, in the ensemble 
C* provides probabilities for the class membership of x. If 

Pci,y(x) is the probability of example x belonging to class 
y according to the classifier Ci, then we compute the class 
membership probabilities for the entire ensemble as: 


where is the probability of x belonging to class y. 
We then select the most probable class as the label for x i.e. 

3.1 Construction of Artificial Data 
We generate artificial training data by randomly picking data 
points from an approximation of the training-data distribu•
tion. For a numeric attribute, we compute the mean and stan•
dard deviation from the training set and generate values from  4 Experimental Evaluation 
the Gaussian distribution defined by these. For a nominal 
attribute, we compute the probability of occurrence of each    4.1 Methodology 
distinct value in its domain and generate values based on this To evaluate the performance of DECORATE we ran experi•
distribution. We use Laplace smoothing so that nominal at•     ments on 15 representative data sets from the UCI repository 
tribute values not represented in the training set still have a [Blake and Merz, 19981 used in similar studies [Webb, 2000; 


506                                                                                                           LEARNING Quinlan, 1996]. We compared the performance of DECO•           DECORATE also outperforms Bagging on the geometric mean 
 RATE to that of Adaboost, Bagging and J48, using J48 as the   ratio. This suggests that even in cases where Bagging beats 
base learner for the ensemble methods and using the Weka       DECORATE the improvement is less than DECORATE'S im•
 implementations of these methods [Witten and Frank, 1999].    provement on Bagging on the rest of the cases. 
For the ensemble methods, we set the ensemble size to 15.        DECORATE outperforms AdaBoost early on the learning 
Note that in the case of DECORATE, we only specify a max•      curve both on significant wins/draw/loss record and geomet•
imum ensemble size, the algorithm terminates if the number     ric mean ratio; however, the trend is reversed when given 75% 
of iterations exceeds the maximum limit even if the desired    or more of the data. Note that even with large amounts of 
ensemble size is not reached. For our experiments, we set      training data, DECORATE'S performance is quite competitive 
the maximum number of iterations in DECORATE to 50. We         with Adaboost - given 100% of the data DECORATE produces 
ran experiments varying the amount of artificially generated   higher accuracies on 6 out of 15 data sets. 
data, Rsize, and found that the results do not vary much for     It has been observed in previous studies [Webb, 2000; 
the range 0.5 to 1. However, Rsize values lower than 0.5 do    Bauer and Kohavi, 1999] that while AdaBoost usually sig•
adversely affect DECORATE, because there is insufficient ar•   nificantly reduces the error of the base learner, it occasionally 
tificial data to give rise to high diversity. The results we report increases it, often to a large extent. DECORATE does not have 
are for RSize set to 1, i.e. the number of artificially generated this problem as is clear from Table 1. 
examples is equal to the training set size.                      On many data sets, DECORATE achieves the same or higher 
   The performance of each learning algorithm was evaluated    accuracy as Bagging and AdaBoost with many fewer training 
using 10 complete 10-fold cross-validations. In each 10-fold   examples. Figure 1 show learning curves that clearly demon•
cross-validation each data set is randomly split into 10 equal- strate this point. Hence, in domains where little data is avail•
size segments and results are averaged over 10 trials. For each able or acquiring labels is expensive, DECORATE has an ad•
trial, one segment is set aside for testing, while the remaining vantage over other ensemble methods. 
data is available for training. To test performance on vary•     We performed additional experiments to analyze the role 
ing amounts of training data, learning curves were generated   that diversity plays in error reduction. We ran DECORATE at 
by testing the system after training on increasing subsets of 
                                                               10 different settings of Rsize ranging from 0.1 to 1.0, thus 
the overall training data. Since we would like to summarize    varying the diversity of ensembles produced. We then com•
results over several data sets of different sizes, we select dif• pared the diversity of ensembles with the reduction in gener•
ferent percentages of the total training-set size as the points alization error. Diversity of an ensemble is computed as the 
on the learning curve.                                         mean diversity of the ensemble members (as given by Eq. 1). 
   To compare two learning algorithms across all domains       We compared ensemble diversity with the ensemble error re•
we employ the statistics used in [Webb, 2000], namely the      duction, i.e. the difference between the average error of the 
win/draw/loss record and the geometric mean error ratio. The   ensemble members and the error of the entire ensemble (as in 
win/draw/loss record presents three values, the number of      [Cunningham and Carney, 2000]). We found that the correla•
data sets for which algorithm A obtained better, equal, or     tion coefficient between diversity and ensemble error reduc•
worse performance than algorithm B with respect to classi•     tion is 0.6225 , which is fairly strong. Further•
fication accuracy. We also report the statistically significant more, we compared diversity with the base error reduction, 
win/draw/loss record; where a win or loss is only counted if   i.e. the difference between the error of the base classifier and 
the difference in values is determined to be significant at the the ensemble error. The base error reduction gives a better in•
0.05 level by a paired t-test. The geometric mean error ratio  dication of the improvement in performance of an ensemble 
is defined as where EA and EB are the mean                     over the base classifier. The correlation of diversity versus 
                                                               the base error reduction is . We note that 
errors of algorithm A and B on the same domain. If the ge•     even though this correlation is weak, it is still a statistically 
ometric mean error ratio is less than one it implies that algo• significant positive correlation. These results reinforce our 
rithm A performs better than B, and vice versa. We compute     belief that increasing ensemble diversity is a good approach 
error ratios so as to capture the degree to which algorithms   to reducing generalization error. 
out-perform each other in win or loss outcomes. 
                                                                 To determine how the performance of DECORATE changes 
4.2 Results                                                    with ensemble size, we ran experiments with increasing sizes. 
                                                               We compared results for training on 20% of available data, 
Our results are summarized in Tables 1-3. Each cell in the     since the advantage of DECORATE is most noticeable low on 
tables presents the accuracy of DECORATE versus another al•    the learning curve. Due to lack of space, we do not include 
gorithm. If the difference is statistically significant, then the the results for all 15 datasets, but present five representative 
larger of the two is shown in bold. We varied the training     datasets (see Figure 2). The performance on other datasets is 
set sizes from 1-100% of the total available data, with more   similar. We note, in general, that the accuracy of DECORATE 
points lower on the learning curve since this is where we      increases with ensemble size; though on most datasets, the 
expect to see the most difference between algorithms. The      performance levels out with an ensemble size of 10 to 25. 
bottom of the tables provide summary statistics, as discussed 
above, for each of the points on the learning curve. 
                                                                  lThe p-value is the probability of getting a correlation as large as 
   DECORATE has more significant wins to losses over Bag•      the observed value by random chance, when the true correlation is 
ging for all points along the learning curve (see Table 2).    zero. 


LEARNING                                                                                                             507                                 Figure 1: DECORATE compared to AdaBoost and Bagging 

                                                             5 Related Work 
                                                             There have been some other attempts at building ensembles 
                                                             that focus on the issue of diversity. Liu et al [1999] and Rosen 
                                                             [1996] simultaneously train neural networks in an ensemble 
                                                             using a correlation penalty term in their error functions. Opitz 
                                                             and Shavlik [1996] use a genetic algorithm to search for a 
                                                             good ensemble of networks. To guide the search they use 
                                                             an objective function that incorporates both an accuracy and 
                                                             diversity term. Zenobi et al [2001] build ensembles based 
                                                             on different feature subsets; where feature selection is done 
                                                             using a hill-climbing strategy based on classifier error and 
                                                             diversity. A classifier is rejected if the improvement of one of 
                                                             the metrics lead to a "substantial" deterioration of the other; 
                                                             where "substantial" is defined by a pre-sct threshold. 
                                                                In all these approaches, ensembles are built attempting to 
                                                        80   simultaneously optimize the accuracy and diversity of indi•
                                                             vidual ensemble members. However, in DECORATE, our goal 
                                                             is to minimize ensemble error by increasing diversity. At no 
                                                             point does the training accuracy of the ensemble go below 


508                                                                                                         LEARNING that of the base classifier; however, this is a possibility with 6 Future Work and Conclusion 
previous methods. Furthermore, none of the previous stud•
                                                              In our current approach, we are encouraging diversity using 
ies compared their methods with the standard ensemble ap•
                                                              artificial training examples. However, in many domains, a 
proaches such as Boosting and Bagging (fOpitz and Shavlik, 
                                                               large amount of unlabeled data is already available. We could 
1996] compares with Bagging, but not Boosting). 
                                                              exploit these unlabeled examples and label them as diversity 
  Compared to boosting, which requires a "weak" base          data. This would allow DECORATE to act as a form of semi-
learner that does not completely fit the training data (boosting supervised learning that exploits both labeled and unlabeled 
terminates once it constructs a hypothesis with zero training data [Nigam et al., 2000]. 
error), DECORATE requires a strong learner, otherwise the ar•    Our current study has used J48 as a base learner; how•
tificial diversity training data may prevent it from adequately ever, we would expect similarly good results with other base 
fitting the real data. When applying boosting to strong base   learners. Decision-tree induction has been the most com•
learners, they must first be appropriately weakened in order   monly used base learner in other ensemble studies, but there 
to benefit from boosting. Therefore, DECORATE may be a         has been some work using neural networks and naive Bayes 
preferable ensemble meta-learner for strong learners.          [Bauer and Kohavi, 1999; Opitz and Maclin, 1999]. Exper•
  To our knowledge, the only other ensemble approach to uti•   iments on "DECORATing" other learners is another area for 
lize artificial training data is the active learning method intro• future work. 
duced in ICohn et al, 1994]. The goal of the committee here      By manipulating artificial training examples, DECORATE 
is to select good new training examples rather than to improve is able to use a strong base learner to produce an effective, 
accuracy using the existing training data. Also, the labels of diverse ensemble. Experimental results demonstrate that the 
the artificial examples are selected to produce hypotheses that approach is particularly effective at producing highly accurate 
more faithfully represent the entire version space rather than ensembles when training data is limited, outperforming both 
to produce diversity. Cohn's approach labels artificial data ei- bagging and boosting low on the learning curve. The empir•
ther all positive or all negative to encourage, respectively, the ical success of DECORATE raises the issue of developing a 
learning of more general or more specific hypotheses.         sound theoretical understanding of its effectiveness. In gen-


LEARNING                                                                                                             509 