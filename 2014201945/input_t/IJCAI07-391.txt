                          Efﬁcient and Robust Independence-Based
                            Markov Network Structure Discovery

                             Facundo Bromberg      and Dimitris Margaritis
                                    Department of Computer Science
                                         Iowa State University
                                            Ames, IA 50011


                                                                      0
                    Abstract                                                     6

    In this paper we introduce a novel algorithm for             3        2            1
    the induction of the Markov network structure of                                          4
    a domain from the outcome of conditional inde-
                                                                                    7
    pendence tests on data. Such algorithms work by                        5
    successively restricting the set of possible struc- Figure 1: Example Markov network. The nodes represent variables
    tures until there is only a single structure consistent in the domain V = {0, 1, 2, 3, 4, 5, 6, 7}.
    with the conditional independence tests executed.
    Existing independence-based algorithms have well- the MN from data, which is frequently the most challenging
    known shortcomings, such as rigidly ordering the  of the two tasks.
    sequence of tests they perform, resulting in poten- The structure of a MN encodes graphically a set of con-
    tial inefﬁciencies in the number of tests required, ditional independencies among the variables in the domain.
    and committing fully to the test outcomes, result- These independencies are a valuable source of information in
    ing in lack of robustness in case of unreliable tests. a number of ﬁelds that rely more on qualitative than quanti-
    We address both problems through a Bayesian par-  tative models (e.g., social sciences). Markov networks have
    ticle ﬁltering approach, which uses a population of also been used in the physics and computer vision commu-
    Markov network structures to maintain the poste-  nities [Geman and Geman, 1984; Besag et al., 1991],where
    rior probability distribution over them, given the they have been historically called Markov random ﬁelds. Re-
    outcomes of the tests performed. Instead of a ﬁxed cently there has been interest in their use for spatial data min-
    ordering, our approach greedily selects, at each  ing, which has applications in geography, agriculture, clima-
    step, the optimally informative from a pool of can- tology, ecology and others [Shekhar et al., 2004].
    didate tests according to information gain. In ad- 2  Motivation and Related Work
    dition, it maintains multiple candidate structures
    weighed by posterior probability, which makes it  There exist two broad classes of algorithms for learning
    more robust to errors in the test outcomes. The re- the structure of graphical models: score-based [Heckerman,
    sult is an approximate algorithm (due to the use of 1995] and independence-based or constraint-based [Spirtes
    particle ﬁltering) that is useful in domains where et al., 2000]. Score-based approaches conduct a search in
    independence tests are uncertain (such as applica- the space of legal structures (of size super-exponential in the
    tions where little data is available) or expensive number of variables in the domain) in an attempt to discover a
    (such as cases of very large data sets and/or dis- model structure of maximum score. Independence-based al-
    tributed data).                                   gorithms rely on the fact that a graphical model implies that a
                                                      set of independencies exist in the distribution of the domain,
                                                      and therefore in the data set provided as input to the algorithm
1  Introduction                                       (under assumptions, see below); they work by conducting a
In this paper we focus on the task of learning the structure set of statistical conditional independence tests on data, suc-
of Markov networks (MNs), a subclass of graphical mod- cessively restricting the number of possible structures consis-
els, from data in discrete domains. (Other graphical models tent with the results of those tests to a singleton (if possible),
include Bayesian networks, represented by directed graphs.) and inferring that structure as the only possible one.
MNs consist of two parts: an undirected graph (the model In this work we present an algorithm that belongs to the
structure), and a set of parameters. An example Markov net- latter category, that presents advantages in domains where
work is shown in Fig. 1. Learning such models from data independence tests are (a) uncertain or (b) expensive. The
consists of two interdependent problems: learning the struc- ﬁrst case may occur in applications where data sets are small
ture of the network, and, given the learned structure, learning relative to the number of variables in the domain. Case (b)
the parameters. In this work we focus on structure learning of occurs in applications involving very large data sets (number

                                                IJCAI-07
                                                  2431of data points) and/or domains where the data are heteroge-         X                     X
neously distributed i.e., where columns of the data set (at-
                                                             Y    Y       Y        Y    Y       Y
tributes) may be located in geographically distinct locations 1   2        t        1   2        t
(e.g., sensor networks, weather modeling and prediction, traf-
ﬁc monitoring etc). In such settings conducting a conditional
                                                                    D              D    D       D
independence test may be expensive, involving transfers of                          1   2        t
large amounts of data over a possibly slow network, so it is Figure 2: Generative model of domain. Left: Correct. Right:
important to minimize the number of tests done.       Assumed.
  Interesting work in the area of structure learning of undi-
rected graphical models includes learning decomposable for proofs of correctness. It is therefore a common assump-
(also called chordal) MNs [Srebro and Karger, 2001] or more tion of independence-based algorithms for graphical model
general (non-decomposable) MNs [Hofmann and Tresp,    discovery. We assume faithfulness in the present work.
1998], which is a score-based approach. An independence- As described later in our main algorithm, we maintain pop-
based approach to MN structure learning is the GSIMN al- ulations of structures at each time step t; slightly abusing our
gorithm [Bromberg et al., 2006], which uses Pearl’s infer- notation we denote these populations by Xt. We denote a se-
ence axioms [Pearl, 1988] to infer the result of certain inde- quence of t tests Y1,...,Yt by Y1:t and a sequence of value
pendence tests without actually performing them. However, assignments to these tests (independence or dependence, cor-
GSIMN has two disadvantages: (i) potential inefﬁciencies responding to true and false respectively) by y1:t.
with regard to the number of tests required to learn the struc-
ture due to the relatively rigid (predeﬁned) order in which 3.1 Generative Model over Structures and Tests
tests are performed, and (ii) potential instability due to cas- For an input data set D, our approach uses the posterior prob-
cading effects of errors in the test results. Instead, the present ability over structures Pr(X |D),X ∈Xto learn the struc-
paper takes a Bayesian approach that maintains the posterior ture of the underlying model as explained in more detail in the
probability distribution over the space of structures given the next section. For that probability to be calculated in a princi-
tests performed so far. We avoid the inefﬁciencies of previous pled way, we need a generative model that involves variables
approaches by greedily selecting, at each step, the optimally X, Y1,Y2,...,Yt and D that reﬂects any independence con-
informative tests according to information gain (decrease in straints among these variables. Our only constraint here is
entropy of the posterior distribution). As such the approach the assumption that the tests are the sufﬁcient statistics for the
can be seen as an instance of active learning [Tong and Koller, structure i.e., there is no information in the data set D beyond
2001]. In addition, our approach is more robust to errors in the value of the tests as far as structure is concerned. This
the test outcomes by making use of the probability of inde- stems from the fact that the structure X faithfully encodes
pendence instead of making deﬁnite decisions (corresponding the independencies in the domain. Note that this assump-
to probability 0 or 1). In this way an error in an independence tion is not particular to our approach, but is implicit in any
test does not permanently cause the correct structure to be independence-based approach. Our generative model only
excluded but only lowers its posterior probability.   formalizes it and makes it explicit.
  The rest of the paper is organized as follows: In the next The generative model that encodes this constraint is shown
section we present our notation, followed by a description of in Fig. 2 (left), where X and D are d-separated by the tests
our approach in detail. Following that, we present experimen- Y1,...,Yt. However, the posterior over structures Pr(X |
tal results and conclude with a summary of our approach. D) cannot be computed from this model because, accord-
                                                                         X   |D               X  | Y1:t
                                                      ing to the model, Pr(     )=     y1:t Pr(        =
3  Notation and Preliminaries                         y1:t, D)Pr(Y1:t = y1:t |D), which requires the computa-
A Markov network of a domain V is an undirected model tion of Pr(Y1:t = y1:t |D), currently an unsolved problem.
that can be used to represent the set of conditional indepen- We therefore assume the model shown in Fig. 2 (right), which
dencies in the domain. The application domain is a set of contains multiple data sets D1,...,Dt (abbreviated D1:t). In
random variables of size n = |V|. In this work we use cap- this model, it can be shown that the tests Y1 through Yt are
         A,B,...                                      independent given data sets D1:t. This allows the model to be
ital letters     to denote domain random variables and                                      t
bold letters for sets of variables (e.g., S). The space of all solved because now Pr(Y1:t = y1:t |D)= i=1 Pr(Yi =
structures (given V) is denoted by X and the space of all con- yi |Di), where the factors Pr(Yi = yi | Di) can be com-
ditional independence (CI)testsbyY. Conditional indepen- puted by known procedures such as the discrete version of
dence of A and B given S is denoted by (A⊥⊥B | S).The the Bayesian test of [Margaritis, 2005]. In practice we do not
set of conditional independencies implied by the structure of have more than one data set, and therefore we use the same
a MN are at least the ones that are implied by vertex sepa- data set for all tests i.e., Di = Dj, i = j. Thus, the model
ration i.e., (A⊥⊥B | S) if A and B are separated in the MN depicted on Fig. 2 (right) is used only as an approximation to
graph after removing all nodes in S (and all edges adjacent overcome the lack of an exact solution described above. As
to them). For example, in Fig. 1, (0⊥⊥4 |{2, 7}).Ifexactly we will show in the experiments section, this approximation
those independencies hold in the actual probability distribu- works well in both artiﬁcial and real world data sets.
tion of the domain, we say that the domain and the graph are Under this modiﬁed model, the posterior probability over
faithful to one another. Faithfulness excludes certain distri- structures must now be computed given data sets D1:t, i.e.,
butions that are unlikely to happen in practice, and is needed Pr(X |D1:t), which we abbreviate as Prt(X). In our cal-

                                                IJCAI-07
                                                  2432culations below we also use the conditional entropy H(X | Algorithm 1 Particle Filter Markov Network (PFMN) algo-
                                                                                 
Y1:t, D1:t) of X given the set of tests Y1:t and the data sets rithm. x = PFMN (N,M,q(X | X)).
D                        H  X |Y
 1:t, similarly abbreviated as t( 1:t).                1: X0 ←− sample N independent structure particles uniformly
  Finally, the other parameters of the model are as follows: distributed among all edge-set sizes (see text).
the prior Pr(X) is assumed uniform, and each test Yi is com- 2: t ←− 0
pletely determined given X = x i.e., Pr(Yi = true | X = 3: loop
                                                            Y   ←−                     score A, B | S
x) ∈{0, 1} (this is a direct consequence of the Faithfulness 4: t+1 arg max(A,B) arg maxS  t(      )
assumption).                                           5:   Y1:t+1 ←− Y1:t ∪{Yt+1}
                                                       6:   pT ←− Pr(Dt+1 | Yt+1 = t) /* Perform test on data. */
4  Approach                                            7:   pF ←− Pr(Dt+1 | Yt+1 = f) /* Perform test on data. */
                                                       8:   Update Prt+1(X) from pT and pF using Eq. (5).
To learn the MN structure of a domain from data, we em-     X    ←− PF  X ,M,      X ,q X | X
ploy a Bayesian approach, calculating the posterior proba- 9: t+1      ( t    Prt+1( ) (      ))
                                                      10:   if Ht+1(X | Y1:t+1)=0then
bility Prt(x) of a structure x ∈Xgiven data sets D1:t.The
                                                      11:        return unique structure x such that Prt(x)=1
problem of learning a structure in this framework can be sum- 12: t ←− t +1
marized in the following two steps: (i) ﬁnding a sequence of
tests Y1:t of minimum cost such that H(X | Y1:t, D1:t)=0, At each time t during the main loop of the algorithm (lines
                                x            X                                    
and (ii) ﬁnding the (unique) structure such that Pr( =             Yt+1     A ,B   | S  ∈Y
                                                     3–12), the test   =(            )      that optimizes
x  |D1:t)=1. (This is always possible due to our assump- a score function is selected (the score function is described
tion of Faithfulness, which guarantees the existence of a sin- below). Since for each pair of variables (A, B) ∈V×Vthe
gle structure consistent with the results of all possible tests in space of possible conditioning sets is exponential (equaling
the domain.)                                          the power set of V−{A, B}), this optimization is performed
  In practice, the above procedure presents considerable dif- by heuristic search; in particular, ﬁrst-choice hill-climbing is
ﬁculties because:                                     used. During this procedure, the neighbors of a current point
  • The space of structures X is super-exponential: |X | = S are all possible additions to S of a non-member, all possible
      n
      2                                               removals from S of a member, and all possible replacements
    2( ). Thus, the exact computation of the entropy Ht(X |
                                                      of a member of S by a non-member. The score of test Yt+1 is
    Y1:t), a sum over all x ∈X, is intractable.
                                                      deﬁned as follows:
  • The space of candidate  tests Y is also at least exponential           Ht(X  | Y1:t,Yt+1)
                   n  n−2                                      scoret(Yt+1)=−                         (1)
    in size: there are 2 m tests (A⊥⊥B | S) with |S| =                             W  (Yt+1)
    m,andm   ranges from 0 to n −2. Moreover, for a given         W  Y                  Y
                               |Y|                    where the factor ( ) denotes the cost of ,whichwetake
    number of tests t,thereexist t possible candidate to be proportional to the number of variables involved in the
    test sequences Y1:t to consider.                  test. This factor is used to discourage expensive tests. Ht(X |
We address the ﬁrst issue using a particle ﬁltering approach Y1:t,Yt+1) is the entropy given the (not yet performed) test
(discussed in section 4.2). At each step t, we maintain a pop- Yt+1, and is equal to:
                             X
ulation of candidate MN structures t for the purpose of rep- Ht(X | Y1:t,Yt+1)=Ht(X | Y1:t) − IG t(Yt+1). (2)
resenting the posterior probability distribution over structures The derivation of Eq. (2) makes use of our generative model
given the outcomes of tests performed so far. In this way, all (Fig. 2) and is simple and omitted due to lack of space. The
required quantities, such as posterior probability Prt(x) or
                                                      term IG t(Yt+1) denotes the information gain of candidate
conditional entropy Ht(X | Y1:t), can be estimated by sim- Y                           t             D
                                    X                 test t+1 given all information at time , i.e., data sets 1:t,
ple averaging over the particle population t. The second and is equal to:
issue (choosing the next test to perform) is addressed using a           
                                                            IG  Y      −         x        yx  | yx
greedy approach. At each step of our algorithm, we choose     t( t+1)=       Prt( )logPrt( t+1   1:t) (3)
                                    Y                                    x∈X
as the next test to perform the member of that minimizes        x
the expected entropy, penalized by a factor proportional to where by y1:t we denote the value of tests Y1:t in structure x
its cost. Since Y is exponential in size, the minimization is (each test is either true or false). The above expression
performed through a heuristic search approach.        follows from Bayes’ rule and our generative model which im-
  The next section explains our algorithm in detail.  plies that given a MN structure x,anytestY is completely
                                                      determined using vertex separation (assuming Faithfulness).
                                                                                                x
4.1  The PFMN Algorithm                               This implies that Prt(Y1:t = y1:t | x)=δ(y1:t,y1:t),where
  Our algorithm is called PFMN (Particle Filter Markov Net- δ(a, b) is the Kronecker delta function that equals 1 iff a is
work structure learner), and is shown in Algorithm 1. At each equal to b. We say that structure x is consistent with assign-
                                                                   x                         x      x
time step t, the algorithm maintains a set Xt containing N ment y1:t iff y1:t = y1:t. The quantity Prt(yt+1 | y1:t) of
                                                      Eq. (3) can be calculated as
structure particles.                                                          
                       X0                                                                       
  Initially, each structure in is generated by randomly  and                  x∈{yx ,yx } Prt(x )
                         m                  n                    yx   | yx         t+1 1:t
uniformly selecting a number of edges from 0 to 2 ,and       Prt( t+1   1:t)=                        (4)
                                    m                                                x  Prt(x )
then randomly and uniformly selecting the edges by pick-                          x ∈{y1:t}
          m
ing the ﬁrst pairs in a random permutation of all possible where by {y1:t} we denote the equivalence class of structures
                                                       
pairs. This ensures that all edge-set sizes have equal proba- x ∈Xthat are consistent with assignment y1:t. Using this
bility of being represented in X0.

                                                IJCAI-07
                                                  2433          x                                                                                        
notation, {y1:t} represents the set of all structures that imply Algorithm 2 Particle ﬁlter algorithm. X =
thesamevaluesfortheCItestsY1:t as structure x does. If x PF (X ,M,f,q).
    
and x are in the same class {y1:t}, we say they are equivalent X ←− f X
                                                      1: Prt( )     ( )
w.r.t. tests Y1:t and we denote this by x ∼t x .       2: for all particles x ∈Xdo
  Eq. (4) has an interesting and intuitive interpretation: at w x ←− Prt(x)
                                                       3:     ( )    Prt−1(x)          /* Compute weights. */
each time t, the posterior probability of a test Y being true        x ∈X
                           y         Y                 4: for all particles do
(false), given some assignment 1:t of tests 1:t, equals the we x ←−  P  w(x)
                                                       5:     ( )         w(x)       /* Normalize weights. */
total posterior probability mass of the structures in the equiv-      x∈X
                                                       6: /* Resample particles in X using we as the sampling probabili-
alence class {y1:t} that are consistent with Y = true
Y    false                                               ties. */
(  =       ), normalized by the posterior probability mass X  ←−      X , we
          {y  }                                        7:       resample(  )
of the class 1:t .                                     8: /* Move each particle in X  M times using Metropolis-Hastings
  To compute the information gain in Eq. (2) we also need and distribution Prt(X).*/
the posterior Prt(x)=Pr(x |D1:t) of a structure x ∈Xfor 9: X  ←− ∅
Eq. (3). As explained in section 3.1, we use the Bayesian 10: for all x ∈X do
                                                                   
test described in [Margaritis, 2005]). This test calculates 11: X ←− X ∪ M-H(x, M, Prt,q).
                                                                 
and compares the likelihoods of two competing multino- 12: return X
mial models (with different numbers of parameters), namely                                         x
   D  | Y    y  y  ∈{true,  false}                    Algorithm  3  Metropolis-Hastings algorithm.     =
Pr( t    t =  t), t                . Since according  M  − H (x, M, p, q).
to our generative model Pr(Di | y1:t,x)=Pr(Di | yi) and
               x                                         x(0) ←− x
Pr(y1:t | x)=δ(y1:t,y1:t), by Bayes’ law we have:      1:
                                     t                2: for i =0to M − 1 do
                                                x
Pr(x |D1:t) ∝ Pr(x)Pr(D1:t | x)=Pr(x)   Pr(Di | yi ).  3:   u ∼U[0,1].
                                                                     (i)
                                     i=1               4:   x  ∼ q(x | x ).
                                                                                ˘       (i)  ¯
                                                (5)           u<A   x(i),x       , p(x )q(x |x )
                                          x            5:   if     (     )=min   1  p(xi)q(x|x(i)) then
The constant of proportionality is independent of and thus       (i+1)    
can be calculated by a sum over all structures in X .Asex- 6:   x    ←−  x
plained above, this, like all equations in this section that in- 7: else
                                                       8:      x(i+1) ←− x(i)
volve summations over the space of structures X , is approx-   (M)
                                                       9: x ←− x
imated by a summation over Xt, the population of particles      
                                                      10: return x
from the previous iteration of the algorithm.
  This completes the description of test score function of Prt+1(X) is reﬂected in a “weight.” This weight is used as
Eq. (1). Returning to our description of Alg. 1, lines 6 and 7 a probability in the resampling step (line 7) to bias selection
                                        Y
calculate the data likelihoods of the optimal test t+1,which (with replacement) among the particles in Xt.Intheﬁnal
are used to update the posterior over structures and obtain step, all particles are “moved” (lines 9–11) through M pairs
      X
Prt+1( ) using Eq. (5). With this updated distribution, the of proposal/acceptance steps within the Metropolis-Hastings
                X
new set of particles t+1 is computed in line 9 using the par- (M-H) algorithm, shown in Alg. 3 [Andrieu et al., 2003]
ticle ﬁlter algorithm, described in the next section. (in our experiments we use M =16). This algorithm re-
                                            H  X  |                                              
  The PFMN algorithm terminates when the entropy t(   quires that Prt(X) and a proposal distribution q(X | X) be
Y                              X
 1:t) (estimated over the population t) becomes 0, return- provided as parameters. Prt(X) has already been addressed
ing the unique structure particle whose posterior probability above in Eq. (5). As in many particle ﬁltering applications,
equals 1.                                             the proposal distribution is a key factor for the success of
4.2  Particle ﬁlter for structures                    PFMN, and is discussed in detail in the next section.
A particle ﬁlter [Andrieu et al., 2003] is a sequential Markov- 4.3 Proposal distribution for structures
Chain Monte-Carlo (MCMC) method that uses a set of sam-
                                                      As discussed previously, to use the Metropolis-Hastings al-
ples, called particles, to represent a probability distribution
                                                      gorithm one must provide a proposal distribution. Here, we
that may change after each observation in a sequence of ob-
                                                      use a mixture proposal q = paqa +(1− pa)qb, consisting of
servations. At each step, an observation is performed and a
                                                      a global component qa and a local component qb.(Weused
new set of particles is obtained from the set of particles at the
                                                      pa =0.05  in our experiments). The components qa and qb
previous step. The new set represent the new (posterior) dis- are as follows:
tribution given the sequence of observations so far. One of
                                                        •                q                 q  x | x
the advantages of this sequential approach is the often drastic Global proposal a (Random walk): a( ) gen-
                                                                        x     x
reduction of the cost of sampling from the new distribution, erates a sample from by iteratively inverting each
                                                                 x               α
relative to alternative (non-sequential) sampling approaches. edge of with probability . (In our experiments, we
                                                              α     .           h
In our case, the domain is X and particles represent struc- use =005). Thus, if   denotes the Hamming dis-
                                                                               x    x     m   n n−   /
tures. Observations correspond to the evaluation of a single tance between structures and ,and = (  1) 2,
                                                                n
CI test on data.                                          where   is the number of nodes (same for both struc-
                                                                           q  x | x  αm    − α m−h
  The particle ﬁlter algorithm, shown in Algorithm 2, is  tures),wehavethat a(     )=    (1    )    .
                                                                                                       
used in line 9 of PFMN to transform population Xt to Xt+1. • Local proposal qb (Equivalence-class moves): qb(x |
                                                                                                     −
The change in the probability distribution from Prt(X) to x) chooses to either (a) remove an edge from set A (x)

                                                IJCAI-07
                                                  2434      Weighted cost of PFMN vs GSIMN           Accuracy of PFMN           Number of tests of GSIMN and PFMN
                                                                          600
    1.2         16 variables Connectivity  1.2     16 variables Connectivity   N=100, Connectivity = 0.5
    1                          0.1     1                         0.1      500   n.(n-1)/2
    0.8                        0.3    0.8                        0.3
                               0.5                               0.5            Number of tests of GSIMN
    0.6                        0.7    0.6                        0.7      400   Number of tests for PFMN
    0.4                        0.9    0.4                        0.9
    0.2                               0.2
                                                                          300
    1.2         20 variables          1.2          20 variables
    1                                  1                                  200
    0.8                             Accuracy  0.8
    0.6                               0.6                                 100
    0.4                               0.4                              Number  of tests (t)

  w_cost(PF)  / w_cost(GS)  0.2       0.2                                  0
    0                                  0
          40  80  120  160  200  240        40   80  120  160  200  240     8     12     16     20     24
      Number of structure particles (N)  Number of structure particles (N)    Number of variables in the domain (n)

Figure 3: Performance comparison of PFMN vs. GSIMN for artiﬁcial domains. Left: Ratio of weighted cost for n =16, 20 and τ =
0.1, 0.3, 0.5, 0.7, 0.9.`Middle:´ Accuracy for n =16, 20 and τ =0.1, 0.3, 0.5, 0.7, 0.9. Right: Number of tests of conducted by PFMN and
                 n
GSIMN compared to 2 .

    (described below), (b) add an edge from set A+(x),or of the recovered network, we measure accuracy as the frac-
    (c) remove an edge e and add e where (e, e) ∈ A±(x). tion of unseen CI tests that are correct i.e., we compare the
    Each of (a), (b), or (c) is chosen with probability 1/3. result of each test on the resulting structure (using vertex sep-
    Sets A+(x), A−(x) ,andA±(x) are deﬁned as:        aration) with the true value of the test (calculated using vertex
                                                      separation in the true model for artiﬁcial domains or statisti-
    A−  x   {e ∈ E x  | x   V,E x  −{e}  ,x ∼ x}
       ( )=        ( )    =(     ( )     )     t      cal tests on the data for real-world domains). The purpose
      +                                    
    A  (x)=  {e ∈ E¯(x) | x =(V,E(x) ∪{e}),x ∼t x}    of this procedure is to measure how well the output network
      ±                      
    A  (x)=  {(e, e ) | e ∈ E(x),e ∈ E(x) ∪{e},       generalizes to unseen tests.
                                        
              x =(V,  (E(x) −{e}) ∪{e }),x ∼t x}      5.1  Artiﬁcial Experiments
    where E(x) is the set of edges of structure x and E¯(x) We ﬁrst evaluated our algorithm in artiﬁcial domains in which
    is its complement. The set A+(x) (A−(x))isthesetof the structure of the underlying model, called true network,is
    all edges that are missing (existing) in x that if added known. This allowed (i) a systematic study of its behavior
    to (removed from) x, produce a structure that will be under varying conditions of domain sizes n and amount of
                             x           ±                                                   m
    equivalent to x i.e., in class {y1:t}.ThesetA (x) is the dependencies (reﬂect in the number of edges in the true
    set of edge pairs (e, e) such that if e is removed and e network), and, (ii) a better evaluation of quality of the output
    added to x the resulting structure will also be equivalent. networks because the true model is known. True networks 
                                                                                               m     τ n
    Therefore the result of any move under this proposal is were generated randomly by selecting the ﬁrst = 2
    a structure that is guaranteed to be equivalent to x, with pairs in a random permutation of all possible edges, τ being
    Hamming distance from x of at most 1.             a connectivity parameter.
                                                        Fig. 3 shows the results of experiments for which CI tests
  The local proposal was designed to maximize accep-
                                                      were conducted directly on the true network through vertex-
tance of proposed moves (i.e., line 5 of the M-H algorithm
                                                      separation, i.e., outcomes of tests are always correct. Fig. 3
evaluating to true) by proposing moves to independence-
                                                      (left) shows the ratio of the weighted cost of CI tests con-
equivalent structures, which have provably the same posterior
                                                     ducted by PFMN compared to GSIMN for two domain sizes
probability. A well-designed proposal q(X | X) i.e., one
                                                      n =16(above) and n =20(below), a number of connectiv-
that ensures convergence to the posterior distribution Prt(X),
                                                      ities (τ =0.1, 0.3, 0.5, 0.7, and 0.9), and an increasing num-
must also satisfy the requirements of aperiodicity and irre-
                                                      ber of structure particles N. We can see that weighted cost of
ducibility [Andrieu et al., 2003]. The above proposal q satis-
                                                      PFMN is lower than that of GSIMN for small (τ =0.1, 0.3)
ﬁes both requirements: it is aperiodic since it always allows
                                                      and large (τ =0.7, 0.9) connectivities, reaching savings of
for rejection, and irreducible since its support is the entire set
                                                      up to 78% for n =20and τ =0.1. The case of τ =0.5 is
of structures X .
                                                      inconclusive—PFMN does better in some, but not all cases.
5  Experiments                                        This can be explained by the fact that the number of structures
                                                      with connectivity τ grows exponentially as τ approaches 0.5
We experimentally compare PFMN  with the GSIMN  al-   (from either side), which clearly makes the search task in
gorithm of [Bromberg et al., 2006] on both artiﬁcial and PFMN more difﬁcult. Fig. 3 (middle) shows that even though
real-world data sets. GSIMN is a state-of-the-art exact PFMN is an approximate algorithm, its accuracy for large
independence-based algorithm that uses Pearl’s axioms of CI enough number of particles N is exactly 1 in all but a few
tests to infer certain tests without actually conducting them. cases (e.g., N = 160,n =16,τ =0.5). GSIMN is an exact
We report: (a) weighted number of tests,and(b)accuracy. algorithm, i.e., its accuracy is always 1 when independence
The weight of each test is used to account for the execu- tests are correct, which is the case for the above experiments.
tion times of tests with different conditioning set sizes, and Its accuracy is thus omitted from the ﬁgures. Fig. 3 (right)
is taken to be the number of variables involved in each test— shows that PFMN does much fewer (non-weighted) test than
see [Bromberg et al., 2006] for details. To obtain the quality GSIMN, and very close to the optimal number of tests which

                                                IJCAI-07
                                                  2435