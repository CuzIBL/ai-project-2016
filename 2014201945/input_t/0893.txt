           Attribution of Knowledge to Artificial Agents and their Principals∗

                   Samir Chopra                                       Laurence White
 Department of Computer and Information Science                       Rue Franklin 157
                 Brooklyn College                                     Brussels B-1000
                Brooklyn, NY 11210                                        Belgium
          schopra@sci.brooklyn.cuny.edu                          laurencefwhite@gmail.com

                    Abstract
    We consider the problem of attribution of knowl-  2   Agents’ knowledge: a pragmatic analysis
    edge to artificial agents and their legal principals. Consider the following knowledge claim: X knows p. Phi-
    When can we say that an artificial agent X knows p losophers have long considered the conditions under which
    and that its principal can be attributed the knowl- such a claim could be made going back to Plato’s Theatetus,
    edge of p? We offer a pragmatic analysis of knowl- which analyzed knowledge as justified true belief: i.e., X
    edge attribution and apply it to the legal theory of knows p iff:
    artificial agents and their principals.
                                                          1.  X believes p
1   Introduction                                          2.  p is true
An agent’s principal is its employer, or any other legal per- 3. X is justified in believing p
son engaging the agent to carry on transactions on its behalf.
A problem commonly faced by courts is deciding when to [Gettier, 1963] has shown by a series of counterexamples
attribute the knowledge in possession of an agent to its prin- that this analysis is flawed. Despite considerable effort, no
cipal. If the agent in question is an artificial one, how satisfactory analysis of knowledge has emerged that does
should the courts decide that a) the agent knows the propo- justice to these or newer counterexamples (largely due to
sition in question and b) that this knowledge can be attrib- difficulties in defining a satisfactory notion of justification).
uted to the agents’ principal? We need a philosophical ac- Knowledge attribution has long been recognized as ripe for
count of knowledge attribution that does justice to the first a treatment grounded in a more pragmatic understanding.
question, and thereby aids in the resolution of the second –
legal –  problem. Conversely, legal resolutions of these is- In our analysis of knowledge attribution, X knows p iff:
sues will aid us in a solution of the philosophical problem –
just as legal findings on personhood can clarify philosophi- 1. X has ready access to p
cal debates over the nature of personal identity. As the dele-
                                                          2.  p is true
gation of tasks to artificial agents increases, so will cases
                                                          3.  X can make use of the informational content of p
that encounter the need for decisions hinging on these de-
                                                              (equivalently, X can exercise certain capacities de-
bates, thus rendering more urgent the need for a solution. 
                                                              pendent on its knowing p)
The plan of this paper is as follows. In Section 2, we present
our analysis of knowledge attribution. In Section 3, (within We retain the truth condition of the original analysis and
limitations of space) we illustrate its plausibility by exam- introduce two new conditions. Condition 1 uses the notion
ples and arguments. In Section 4 we describe the legal of access to, or easy availability of, the proposition p. Con-
problem of attributing knowledge held by agents to their dition 3 – which replaces the notion of justification – im-
principals (while critically examining current legal doctrine plies counterfactuals such as “If X did not know p, then Y” –
in this area) and show how our analysis is of help. While we where Y is a statement like “X is not able to exercise capac-
concentrate on British Commonwealth and US law, this  ity C”. Since X is able to exercise capacity C, it knows p.
paper does not purport to be a complete survey of the rele- (See dispositional [Levi and Morgenbesser, 1964] or func-
vant law in the jurisdictions dealt with. When citing non- tionalist accounts [Armstrong, 1980] for similar notions.)
academic sources such as case reports, we use the legal
mode of citation.
                                                  
   ∗ We thank Rohit Parikh, John Sutton and the referees for help-
ful comments.3   The Case for the Pragmatic Analysis                       as I hurry towards the next train. Here, Gricean
                                                              semantics [Grice, 1975] is at play: if I said I did not
The following examples illustrate the intuitions underlying   know the meeting’s location, I would be misleading
our analysis:                                                 my questioner. This example is crucial in showing
                                                              that knowledge claims are connected to the prag-
    1. As I walk down the street, I am asked by a pas-        matics of speech. To deny a valid knowledge attri-
       serby, “Excuse me, do you know the time?” I an-        bution in this case would be to say something mis-
       swer “Yes” as I reach for my cellphone to check        leading or something whose semantic value is con-
       and inform him what time it is. This example, due      siderably less than the knowledge claim.
       to Andy Clark [2003], shows that we take our-
       selves to know those items of information that are
                     1                                In the Amazon example the agent is able to use my address
       easily accessible  and can be easily used. Clark’s to fulfill its functions. An alternative locution would be to
       example is part of an extended argument for dis- say, “Amazon has my address”, but what purpose would be
       tributed cognition through external tools and mem- served other than an avoidance of intentional vocabulary? If
       ory stores not confined to the inside of our crani- it did not ship to the correct address, Amazon could not use
       ums. For our analysis we note that information at as a defense the claim that it did not know (or ‘have access
       hand can be described as information that we   to’) my address. It was stored in their database and had been
       ‘know’ (I could not claim to know my friends’  used successfully in the past.  Amazon is capable of in-
       telephone numbers if, on being asked, I were to re- forming a potential customer that it is unable to ship goods
       ply, “I can’t remember”).                      since it does not ‘know’ the recipient’s address (or credit
                                                      card number). If Amazon has access to my address but it has
    2. A friend wants to buy me a book as a gift. He asks changed in the meantime, then it is natural to say that Ama-
       me for my shipping address so that he can send me zon does not know my address since it would not be able to
       the book. I direct him to my wish-list at Ama- perform the function of shipping books to me.
       zon.com saying, “Amazon knows my shipping ad-
       dress”. Indeed, the shopping cart software on that When we say, “Amazon.com knows my shipping address”,
       site does. When my friend has decided which    our analysis implies that:
       books he wants to buy, he pays and picks the ship-
       ping option. Amazon generates a shipping invoice   1.  Amazon has ready access to my shipping address
       complete with shipping address. The shopping cart      in its databases.
       software is able to discharge its functions using  2.  The shipping address is correct.
       that piece of information. I had stored the shipping 3. Amazon is able to perform capacities dependent
       address on Amazon precisely for such future use. I     upon its knowing my address (it is able to make
       could store an alternate shipping address and forget   use of the informational content of the address).
       its details, since Amazon ‘knows’ it and will ship
       to it if anyone decides to send me a book at that Furthermore, the relevant counterfactuals are true: if it did
       address. Note that parts of my address could be not know my address, Amazon’s core functionality with
       obtained from Amazon’s database (by Ama-       respect to its interactions with me would not be achievable;
       zon.com programmers) by writing specific queries if Amazon did not know my shipping address, it would not
       (e.g., “What street does customer X live on?”). In a be able to send books to me; if Amazon did not know my
       weaker sense, then, Amazon then also knows     address, it would not be able to send me a bill; but it is able
       which of its customers lives in postal code 11205. to do so; hence it knows my address. This kind of analysis is
                                                      readily extended to other kinds of agents that take actions
    3. I have to attend a meeting at the university campus based on information at their disposal. An artificial agent’s
       branch located in the city center. With directions actions could be described in much the same way as a hu-
       for the meeting written down on a piece of paper man agent’s – “The pricebot sent me a quote because it
       that I keep in my pocket, I head out the office door. knew my preferences”.
       As I do so, my office-mate asks me “Do you know
       where the meeting will be held?” I answer, “Yes”  Our analysis may be fruitfully contrasted with conven-
                                                      tional formal analysis, in which an agent’s belief corpus is
   1 [Parikh, 1994] has described knowledge as statements that the taken to be the set of propositions that the agent is commit-
agent is capable of deriving (from a given set of premises) within ted to (the agent answers “Yes” when asked “Do you be-
some reasonably tractable bounds as opposed to those implied by lieve  p?”). Formally, p is derivable using the inference ma-
epistemic closure (if X knows p, and X knows that p implies q then chinery built into that agent’s architecture. Thus I could say
X knows q). Parikh’s account limits knowledge to those q’s that of an artificial agent that it knows or believes p if p is deriv-
meet the tractability condition (thus providing an analysis of
knowledge in terms of the abilities of the agent as we do). able from its knowledge base. But to limit knowledge attri-bution to those agents that are capable of deriving the has been treated as a reliable source of information with
proposition p using a formally specifiable inference mecha- regards to the address – and thus humans might accurately
nism would be to put the proverbial cart before the horse. claim that they learned a proposition from a software agent.
We feel comfortable making the claim that the cat knows a When a book is purchased, my address has been used by
mouse is behind the door though we do not have the foggi- Amazon.com without any human knowing it. What sense
est idea of what kind of inferential mechanism is at hand. would it make to say that Amazon did not know the ad-
The cat reveals its knowledge through its actions. Whatever dress? Alternative locutions for describing this functionality
kind of retrieval or inference mechanism is at work, it en- of Amazon’s would be artificial. What could we say – that
ables the cat to go about its tasks. Similarly for an artificial Amazon has access to this true information, and can use it?
agent –  it reveals its knowledge of p through the ready The parallels with knowledge attributions to human agents
availability of the proposition in facilitating the agent’s should be clear. For human agents, on our analysis, are said
functionality. We do not discount a hybrid architecture that to know a proposition p when we can make such a claim. If
employs a deductive database that can infer further infor- I know that 619 times 3 is 1857 but cannot open a safe with
mation applying rules to a set of stored facts. In that case we this combination, then I do not know the combination to the
would say that the agent in question knows all the facts de- safe since I cannot open it but I do know the product of 619
rivable from its database as well – subject to tractability and 3.
conditions as in [Parikh, 1994].
                                                      In making knowledge attributions, there is a parallel be-
Where does an agent’s epistemology come into the picture? tween humans and artificial agents. The ease with which we
If an agent elicits information from humans then the respon- slip into the intentional attribution when it comes to Ama-
sibility of ensuring the accuracy of the information is the zon.com is an indication of this similarity. The intentional
user’s. If the user inputs incorrectly, the agent is in posses- stance is used when it is possible to give the best explana-
sion of false information and we do not make the knowledge tions of behavior using it. What kind of behavior would we
attribution. Thus, the software artifact inherits its epistemol- able to predict? We could predict Amazon’s responses to
ogy from the humans that supply it information and carry certain queries. For instance, we could say that Amazon
out data entry. This should not lead us to think that artificial knows the ISBN number for How Green Was My Valley
agents do not have an independent epistemology. Pricebots since it would be able to produce that number on request.
that read price information on remote web pages acquire But as we have argued, we could also predict Amazon’s
knowledge autonomously, by using their file-reading   success in certain tasks – Amazon could demonstrate its
mechanisms, presumably equipped with error checking and knowledge of the ISBN number of How Green Was My
validation routines that guarantee it will not read in garbage Valley by shipping me that book and none other.
(the software equivalent of a reliable sensor). The accuracy
with which these agents acquire information is a function of Condition 2 of our analysis is crucial (as in most analyses of
their design and the code that runs on them – very similar to knowledge) since if the shipping address in question were
human agents, the accuracy of whose beliefs is a function of incorrect we would not say that Amazon knows the shipping
how well their senses work in conjunction with background address.  The locution we would employ if Amazon were to
knowledge and their reasoning powers.                 use the incorrect shipping address would be “Amazon
                                                      shipped my books to what it thought (or believed) was my
We would not want to say that an artificial agent knows a correct address”. We would not make the claim that Ama-
proposition if the proposition is simply stored in the agent’s zon knows my address if in fact, the address is false (since it
knowledge base but is not accessible for use by the agent. In is possible to have a false belief).
that case, we would say that the information in question is
stored in the agent but the agent does not know it, since it is One way to deny that artificial agents can know propositions
unable to access or use it. Note that when files are deleted would be to ask, “Who does the knowing in the case of the
from a computer the information ordinarily does not vanish, artificial agent?” Our response would be that the same could
it simply becomes a target for over-writing. The information be asked of humans, and in the absence of any philosophi-
is not accessible any more without elaborate recovery meth- cally satisfactory analysis of personal identity there is no
ods, and hence the computer’s operating system is reasona- reason to believe that a stronger condition should be placed
bly enough said not to have access to it any more.    on artificial agents.  Below, we suggest that the correct legal
                                                      treatment of artificial agents is to assimilate them to human
In the case of Amazon, it is possible that not a single human agents, but without the personhood possessed by human
being employed by Amazon knows my address. It is con- agents. If the same view is adopted on the philosophical
ceivable that when the shipping invoice is printed out by the perspective, it becomes otiose to ask who does the knowing
software, a human clerk will pick it up and attach it to the in the case of an artificial agent – other than the agent itself,
box of books in question without bothering to check any of course.
further whether the address is correct or not. The software4   The legal doctrine of attributed knowledge        knowledge of the agent.2 Under this approach, the doctrine
                                                      operates on a pre-existing duty to convey information to
This inclusion of the ready-to-hand in the knowledge of an deem that the duty has been discharged.
agent has close and instructive parallels in the legal doctrine
of attributed knowledge. Under this doctrine, the law may In the US, the common law of agency does not require as a
impute to a principal knowledge – relating to the subject precondition an existing duty to communicate the informa-
matter of the agency – which the agent acquires while acting tion to the principal [DeMott, 2003]. As Langevoort [2003]
on behalf of its principal within the scope of its authority. points out, the courts’ description of attribution as the pre-
The scope of the agent’s authority refers to those transac- sumption that the agent has fulfilled its duty of candor in
tions that the principal has authorized the agent to conduct. conveying information is not correct, since attribution ap-
In some circumstances, knowledge gained by the agent out- plies even where interaction between principal and agent
side the scope of the agency can also be attributed to the creates enough scope of discretion that no transmission of
agent’s principal.                                    information is expected (or occurs).

Once knowledge is attributed to the principal, it is deemed In England the “duty to communicate” has been abandoned
                                                                                                 3
to be known by the principal and it is no defense for the as the explanatory basis of attribution of knowledge.  Simi-
principal to claim that he did not know the information in larly, Australian courts have inferred attribution of knowl-
question, for example, because the agent failed in its duty to edge in the absence of a duty to communicate information in
convey the information to the principal.              cases where the task assigned to the agent included making
                                                      appropriate disclosures. 4
The doctrine of attributed knowledge has many applications,
and is used generally in civil law contexts in cases where the We believe the rejection of the duty to communicate is cor-
knowledge of the principal is relevant. For instance, legal rect on policy grounds To require such a duty in order to
consequences attach to principals knowingly receiving trust attribute knowledge held by agents to their principals would
funds, or having notice of claims of third parties to property encourage principals to ask agents to shield them from in-
received, or knowingly making false statements.       convenient information, and would put principals acting
                                                      through agents in a better position than principals acting
The doctrine has close parallels with our analysis above, directly. Such an approach is also incompatible with modern
which extends the concept of knowledge to include the in- information management practices within companies, and
formation that we retain in storage devices – including we discuss why below.
written documents – that are ready-to-hand. From this per-
spective, a human agent is akin to a knowledge storage de- However, the fact that agents are capable of communication
vice under the control of a principal. Below, we suggest that is important to the attribution of knowledge. In terms of our
artificial agents can be thought of similarly. But first, we analysis of knowledge, a lack of capacity to communicate
explore the basis of the doctrine and its application to the information would render the first and/or third conditions
modern company.                                       unfulfilled – i.e., that the principal has ready access to the
                                                      knowledge held by the agent, or that the principal can make
4.1 A duty to communicate?                            use of its informational content. The capacity to communi-
                                                      cate therefore plays an explanatory role when thinking
While the doctrine of attributed knowledge is pervasive in about how artificial agents fit within this legal schema.
the legal systems under discussion, its precise doctrinal ba-
sis is still a matter of some dispute [DeMott, 2003]. 4.2 Attribution of knowledge to companies

One explanation of the doctrine relies on the supposed iden- A company is a special kind of organization that, in modern
tity of principal and agent, whereby the law sees them as legal systems, is recognized as a legal person in its own
one person for some purposes. However, this theory lacks right. How, then, does a company gain knowledge in the
explanatory power, and does not explain the public policy eyes of the law? Apart, possibly, from knowledge gained
justification for the rule.                           “directly” by the Board or general meeting of a company,
                                                      only through the attribution to it of knowledge gained by its
Another explanation put forward for attributed knowledge is agents (i.e., its directors, employees or contractors). By the
that the law presumes that agents will carry out their duties
to communicate information to their principals. For exam-                                                   
                                                         2 Vol. 2(1) (Fourth Edition Reissue) Agency, para 164.
ple, in the standard practitioner’s text Halsbury’s Laws of 3 El Ajou v Dollar Land Holdings plc & Anor [1994] 2 All ER
England, the scope of an agent’s duty to communicate de- 685, at 703–4 per Hoffmann L.J. See also [Reynolds, 2001], Arti-
termines the existence and the timing of any attributed cle 97(1) at paragraph 8-207.
                                                         4 Permanent Trustee Australia Limited v FAI General Insur-
                                                      ance Company Ltd (in Liq) [2003] HCA 25 at paragraph 87.doctrine of attribution, the company is deemed to gain the Our analysis could be utilized in a legal context for the task
knowledge that is gained by the natural persons (i.e., hu- of determining what is known by the artificial agent in
mans) engaged by it. 5                                question.6

The large modern company illustrates why the “duty to The scope of the agency would be those transactions that the
communicate” cannot found the attribution of knowledge. artificial agent has been deployed to conduct. Not all the
Given the company is an abstract entity, the only way to agent’s knowledge would necessarily be attributed to the
make sense of such a duty would be in terms of communi- principal. For example, an agent could conceivably act for
cation to other agents (such as immediate superiors), who two principals and in accordance with the law of agency,
are required to communicate it “directly” to the company as knowledge gained in the course of one agency is not always
embodied by the Board of directors (or general meeting). attributed to the other principal.7 A natural person could
Since in modern corporations authority to enter and admin- deploy an artificial agent, and in that instance the agent’s
ister contracts is usually delegated to relatively junior staff knowledge would be attributed to the principal in the same
members, it would be absurd if all the knowledge that had circumstances.
legal consequences for a company had to be communicated
upwards in this way. Instead, most information within the Surprisingly, there is a paucity of judicial pronouncements
modern corporation remains with lower-level officers, and on the possibility of attribution of knowledge held by artifi-
is only passed upwards in summary terms – or when there is cial agents. Some tangential judicial support for such a
some exceptional reason to do so, such as a dispute with treatment of artificial agents was given recently, but it was
outside parties. Abandoning the “duty to communicate” made clear that it did not (yet) represent the law.  In the
                                                                                                8
allows the legal system to acknowledge information man- Australian case Commercial Union v Beard & Ors , the
aged in accordance with modern decentralized practices. issue arose whether a fact contained in a news clipping, filed
                                                      in a company paper file, was “known” to an insurer for the
Today the most common way for information to be stored purposes of the relevant statute. If it was known to the in-
and controlled by low-level officers is by inputting it into surer, the party taking out insurance was relieved of the ob-
the company’s information systems. Some of these systems ligation of making disclosure of the fact to the insurer.
can be queried by senior managers – but it has never, to our
knowledge, been suggested that this is essential to the attri- The majority found that a matter could be “known” by the
bution. To what extent could information systems – artifi- insurer company if it were contained in the “current formal
cial agents – themselves be treated by the legal system as records” of the company. This term appeared to include the
agents for the purposes of attribution of knowledge?  minutes of the company’s Board meetings. However, the
                                                      majority held that the extract of the news clipping in ques-
4.3 Artificial agents as agents for knowledge impu-   tion was “not a record of [the insurer] and it was not con-
                                                      tained in any file to which officers of [the insurer] were ex-
    tation purposes
                                                      pected to have recourse for the purposes of the subject in-
                                                              9
In [Chopra and White, 2004], following [Kerr, 1999], it was surance.”  As a result, they found that the contents were not
argued that the legal system should and could extend the “known” to the company for the purposes of the statute.
legal treatment of human agents to artificial agents, with
appropriate modifications. Artificial agents, on this ap- The minority judge, however, seemed to discount that any-
proach, would have a legal status akin to slaves in Roman thing could be “known” to the company merely by being
law – that is, with capacity to enter contracts on behalf of contained in a record, while acknowledging that such a view
their principals, but without contracting capacity or legal had its attractions [emphasis added]:
personhood in their own right.                                We were not referred to any authority for the proposition that, in
                                                              the absence of actual knowledge on the part of relevant officers
We believe a similar move can and should be made with         of a company, the company may, nevertheless, “know” a matter,
                                                              where the relevant information is contained in a company file. I
respect to the imputation of knowledge. On this approach,
knowledge gained by artificial agents employed by corpora-                                                   
                                                         6
tions could be attributed to the corporations themselves,  The ‘actual knowledge’ of human agents is treated by the
where that knowledge would be attributed to the corporation Courts as self-evident and not needing further analysis: see the
                                                      five-fold categorization of knowledge in Baden v. Société Gé-
in the case of a human agent.                         nérale, [1993] 1 W.L.R. 509, 575-76.
                                                         7 On these cases, English and US law take divergent and
                                                      sometimes confusing approaches: see [DeMott, 2003]; [Reynolds,
                                                      2001], at paragraph 8-210; Permanent Trustee Australia Co Ltd v
                                                      FAI General Insurance Co Ltd (2001) 50 NSWLR 679 at 697 per
                                                      Handley JA.
   5 See Halsbury’s Laws of England, Companies, 7(1) (2004 8 [1999] NSWCA 422
Reissue), paragraph 441: How a company may act.          9 per Davies AJA at paragraph 63