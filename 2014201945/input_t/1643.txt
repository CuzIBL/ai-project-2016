          A Two-Stage Method for Active Learning of Statistical Grammars

                     Markus Becker                                 Miles Osborne
                   School of Informatics                        School of Informatics
                  University of Edinburgh                      University of Edinburgh
                 m.becker@ed.ac.uk                           miles@inf.ed.ac.uk


                    Abstract                          may depend on a probability which has been unreliably es-
                                                      timated from an as yet rarely observed event. In this case,
    Active learning reduces the amount of manually an- the model would indicate certainty about a particular analysis
    notated sentences necessary when training state-of- where indeed it is not conﬁdent. In general, we would like to
    the-art statistical parsers. One popular method, un- place less conﬁdence in selection decisions based on entropy
    certainty sampling, selects sentences for which the over probability distributions involving low frequency events.
    parser exhibits low certainty. However, this method However, sentences whose predicted parse was selected on
    does not quantify conﬁdence about the current sta- the basis of infrequent events may well be informative. Since
    tistical model itself. In particular, we should be entropy will not in itself always allow us to reliably select
    less conﬁdent about selection decisions based on  such examples for labelling, we need to consider other mech-
    low frequency events. We present a novel two-     anisms.
    stage method which ﬁrst targets sentences which     We propose a novel two-stage method which ﬁrst selects
    cannot be reliably selected using uncertainty sam- unparsable sentences according to a bagged parser, and ap-
    pling, and then applies standard uncertainty sam- plies uncertainty sampling to the remaining sentences using a
    pling to the remaining sentences. An evaluation   fully trained parser. Evaluation shows that this method per-
    shows that this method performs better than pure  forms better than single parser uncertainty sampling, and bet-
    uncertainty sampling, and better than an ensemble ter than an ensemble method with bagged ensemble members.
    method based on bagged ensemble members only.       To explain our results, we show empirically that entropy
                                                      and f-measure are negatively correlated. Thus, selection ac-
                                                      cording to entropy tends to acquire annotations of sentences
1  Introduction                                       with low f-measure under the current model. An oracle-
State-of-the-art parsers [Collins, 1997; Charniak, 2000] re- based experiment demonstrates that preferably selecting low
quire large amounts of manually annotated training mate- f-measure sentences is indeed beneﬁcial and explains why un-
rial, such as the Penn Treebank [Marcus et al., 1993], to certainty sampling is successful in general. Furthermore, we
achieve high performance levels. However, creating such la- ﬁnd that exactly those sentences which our proposed meth-
belled data sets is costly and time-consuming. Active learning ods targets show no such correlation between entropy and f-
promises to reduce this cost by requesting only highly infor- measure. In other words, entropy will not reliably identify in-
mative examples for human annotation. Methods have been formative examples from this subset, even though these sen-
proposed that estimate example informativity by the degree tences have below average f-measure and should be particu-
of uncertainty of a single learner as to the correct label of an larly useful. These ﬁndings help to explain why the proposed
example [Lewis and Gale, 1994] or by the disagreement of a method is a successful strategy.
committee of learners [Seung et al., 1992]. This paper is con-
cerned with reducing the manual annotation effort necessary 2 Active Learning Methods
to train a state-of-the-art lexicalised parser [Collins, 1997]. Popular methods for active learning estimate example infor-
  Uncertainty-based sampling has been successfully applied mativity with the uncertainty of a single classiﬁer or the dis-
to the same problem problem [Hwa, 2000]. Here, sentences agreement of an ensemble of classiﬁers.
are selected for manual annotation when the entropy over the Uncertainty-based sampling (or tree entropy) chooses ex-
probability distribution of competing analyses is high. En- amples with high entropy of the probability distribution for a
tropy quantiﬁes the degree of uncertainty as to the correct single parser [Hwa, 2000]:
analysis of a sentence.
                                                                te         X
  A problem with active learning methods such as uncer-        fM (s, τ) = −  PM  (t|s) log PM (t|s)  (1)
tainty sampling is that they have no method for dealing with               t∈τ
the consequences of low counts. For example, the parse tree where τ is the set of parse trees assigned to sentence s by
probability of the most likely reading in a peaked distribution a stochastic parser with parameter model M. Less spikeddistributions have a higher entropy and indicate uncertainty 4 Experimental Setup
of the parse model as to the correct analysis. Thus, it will be For our experiment, we employ a state-of-the-art lexicalised
useful to know their true parse tree.                 parser [Collins, 1997].1 We employ default settings without
  Ensemble-based methods for active learning select exam- expending any effort to optimise parameters towards the con-
ples for which an ensemble of classiﬁers shows a high de- siderably smaller training sets involved in active learning.
gree of disagreement. Kullback-Leibler divergence to the In common with almost all active learning research, we
mean quantiﬁes ensemble disagreement [Pereira et al., 1993; compare the efﬁcacy of different selection methods by per-
McCallum and Nigam, 1998]. It is the average Kullback- forming simulation experiments. We label sentences of sec-
Leibler divergence between each distribution and the mean tions 02 - 22 from the Penn WSJ treebank [Marcus et al.,
of all distributions:                                 1993], ignoring sentences longer than 40 words.
                                                        We report the average over a 5-fold cross-validation to en-
                     1  X                             sure statistical signiﬁcance of the results. In a single fold,
          f kl (s, τ) =     D(P   ||P  )        (2)
           M         k          M   avg               we randomly sample (without replacement) an initial labelled
                       M∈M                            training set of a ﬁxed size – 500 or 2,000 sentences, depend-
                                                      ing on the experiment – and a test set of 1,000 sentences. The
where M   denotes the set of k ensemble models, P
                                                avg   remaining sentences constitute the global pool of unlabelled
is the mean distribution over ensemble members in M,
                                                      sentences (ca. 37,000 sentences). For a realistic experiment,
P    = P    P  (t|s)/k, and D(·||·) is KL-divergence, an
 avg      M  M                                        we tag the test set with the TnT part-of-speech tagger as in-
information-theoretic measure of the difference between two
                                                      put for the parser [Brants, 2000]. We train TnT on 30,000
distributions. It will be useful to acquire the manual annota-
                                                      sentences in the global resource. In a 5-fold cross-validation,
tion of sentences with a high KL-divergence to the mean. This
                                                      the parser has 88.8% labelled precision and 88.6% labelled
metric has been applied for active learning in the context of
                                                      recall, when trained on 37k sentences and applied to test sets
text classiﬁcation [McCallum and Nigam, 1998].
                                                      of 1,000 sentences.2
                                                        We randomly sample (without replacement) a subset of
3  A Novel Two-Stage Selection Method                 1,000 sentences from the global pool in each iteration. From
                                                      this subset, 100 sentences are selected for manual annotation
Acquiring the correct analysis of a sentence of which the pre- according to the current sample selection method. Then, an-
dicted analysis was selected on the basis of infrequent events notated sentences are added to the training set.
may well be informative. Since entropy itself will not allow For consistent comparison across methods, we evaluate test
us to reliably select such examples for labelling, we need to set performance of a single parser trained on the entirety of
consider other mechanisms. A simple, but effective method is the labeled training data at each step, regardless of the selec-
to eliminate some infrequent events from the parsing model. tion method being a single or an ensemble method.
Simply bagging the current training set, and retraining the
parser on this set allows to identify such examples for la- Length balanced sampling For situations such as active
belling.                                              learning for parsing, the sentences in question need a variable
  Bagging is a general machine learning technique that re- number of labelling decisions. This may confound sample
duces variance of the underlying training methods [Breiman, selection metrics and it is therefore necessary to normalise.
1996]. It aggregates estimates from classiﬁers trained on For example, tree entropy may be directly normalised by sen-
bootstrap replicates (bags) of the original training data. Cre- tence length [Hwa, 2000], or by the binary logarithm of the
ating a bootstrap replicate entails sampling with replacement number of parser readings [Hwa, 2001].
n examples from a training set of n examples. A bootstrap We use the following method to control for sentence length
replicate will not only perturb all event counts to some de- in sample selection: Given a batch size b, we randomly sam-
gree, but will inevitably eliminate some of the low frequency ple b sentences from the pool and record the number el of
event types.                                          selected examples for sentence length l. Then, for all lengths
  The proposed method operates in two stages. We ﬁrst l = 1, 2, . . . 40, we select from all sentences in the pool of
select sentences which are unparsable according to a single length l the el examples with the highest score according to
bagged version of the parser, but (possibly) parsable under the our sample selection metric. Of course, the union of these
current fully trained model. From the remaining sentences, sets will have b examples again. Since we randomly sampled
we select those with the highest entropy as determined by the the batch from the pool, we may assume that the batch dis-
fully trained model. We can express this formally as follows: tribution reﬂects the pool distribution, in particular wrt. the
                                                      distribution of sentence lengths.

      two               te                 0             1We use the ﬂexible reimplementation of the parser by Dan
     f    0 (s, τ) = max(f (s, τM ), failure(s, M )) (3)
      M,M              M                              Bikel, developed at the University of Pennsylvania [Bikel, 2004]. It
                                                      can be obtained at http://www.cis.upenn.edu/˜dbikel/
       te
where fM is tree entropy according to a fully trained model 2It would be desirable for methodological reasons to automati-
                                      0
M, deﬁned in (1). The function failure(s, M ) returns inﬁn- cally tag the global resource, too. However, our corpus split scheme
ity when sentence s is parsable given bagged parser model does not leave enough disjoint training material for the tagger, so we
M 0, and 0 otherwise.                                 use the gold standard tags for the pool sentences.                                                            84
    100

     90                                                     82

     80
                                                            80
     70
                                                            78
     60
                                                        f-measure  in %
  performance  in % 50                                      76

     40
                             coverage                       74               entropy/unparsed
                            precision                                              unparsed
     30                     f-measure                                               random
                               recall                                               entropy
                                                            72
       1k         10k         100k      700k                       10k        15k      20k   25k
                 number of constituents                                number of constituents

Figure 1: A random sampling learning curve. The maximal Figure 2: Comparison of f-measure learning curves. Number
training set has 37k sentences (630k constituents). The num- of constituents is given on a log scale.
ber of constituents is given on a log scale.
                                                              Method              Cost  Reduction
                                                              random            23200        N/A
  This method effectively reproduces the sentence length      entropy/unparsed  16100      30.6%
proﬁle of the original corpus by construction and therefore   unparsed          16400      29.4%
guards against the selection of sentence length biased sub-   entropy           24500      -5.7%
sets. Furthermore, it is equally applicable for all metrics and
allows a direct comparison between metrics. Note that this Table 1: Annotation cost to reach 80% f-measure, and reduc-
method is applicable in general for the sample selection of tion over random sampling.
sequential data where one may expect to ﬁnd correlation be-
tween sample length and score.
                                                      Including/excluding unparsed sentences In this experi-
                                                      ment, we compare methods which do or do not include un-
Relevant evaluation measures Active learning for parsing parsed sentences in the batch of selected examples. Acquiring
is typically evaluated in terms of achieving a given f-measure the correct parse tree of an unparsable sentence increases the
for some amount of labelling expenditure. The cost of acquir- size of the model structure of the grammar and, presumably,
ing manually annotated training material is given in terms of helps to increase coverage in the test set.
the number of constituents. F-measure itself is a composite
                                                        A very simple method, unparsed, preferably includes un-
term, being composed of precision and recall [Black et al.,
                                                      parsed pool sentences in the batch. Should the number of
1991]. Fig. 1 shows a learning curve for a random sampling
                                                      unparsed sentences fall short of the batch size, we randomly
experiment. We see that precision and recall do not increase
                                                      sample parsable sentences from the pool to ﬁll the batch. By
at the same rate. For this reason, it may well be advantageous
                                                      contrast, entropy only selects parsable sentences with high
to aggressively increase recall with minimal impact on pre-
                                                      entropy. The method entropy/unparsed preferably selects
cision (formulate stronger: still want to increase precision).
                                                      unparsed pool sentences, and ﬁlls the batch with high entropy
One way of achieving this is to pursue sentences which can-
                                                      examples. We may view this method as being composed of a
not be parsed.
                                                      binary parsability component, and a gradual uncertainty com-
                                                      ponent. The baseline is random, a parser trained on ran-
5  Results                                            domly sampled training sets of different sizes.
                                                        We start with an initial training set of 500 randomly sam-
The experiments in this section address the following ques- pled sentences, containing 8,400 labeled constituents and
tions. Is it generally useful to select unparsable sentences for continue for 10 rounds until 1,500 sentences have been sam-
manual annotation? What is the gain of using the novel two- pled (ca. 26k constituents). Here, as in all subsequent experi-
stage method over a state-of-the-art uncertainty-based sam- ments we employ length balanced sampling, cf. Sec. 4.
ple selection method? Given that the two-stage method has Methods unparsed and entropy/unparsed perform con-
a bagged component, how does it compare against a state- sistently better than random (Fig. 2). Note that their per-
of-the-art ensemble-based method which employs bagged en- formance is nearly identical until more than 20k constituents
semble members?                                       have been labelled. Method entropy performs consistently      86                                                  86

    85.5

      85                                                  85


      84                                                  84
  f-measure  in %                                       f-measure  in %
      83                                                  83


                             two_stage                                           two_stage
      82                entropy/unparsed                  82                  kl-div/unparsed
                               random                                              random
              40k       60k     80k  100k 120k                       40k         60k       80k
                  number of constituents                              number of constituents

Figure 3: The new two-stage method versus state-of-the-art Figure 4: The new two-stage method versus a state-of-the-art
uncertainty sampling.                                 ensemble-based method.

     Method            Cov   Prec   Rec   Fm                 Method               Cost  Reduction
     random            95.8  82.0  78.6  80.3                random             115600       N/A
     entropy/unparsed  98.3  82.9  81.2  82.1                two stage           77900      32.6%
     unparsed          98.5  82.4  80.9  81.7                entropy/unparsed    86700      25.0%
     entropy           94.4  82.8  77.6  80.1
                                                      Table 3: Annotation cost to reach 85.5% f-measure, and re-
Table 2: Parseval values for different metrics after 25,000 duction over random sampling.
constituents have been annotated.

                                                      A two-stage selection method The novel method addresses
worse than random.                                    problems with sentences which cannot be reliably selected
  Methods unparsed  and entropy/unparsed reduce the   with popular active learning methods. Therefore, we expect a
amount of labeled constituents necessary to achieve 80% f- gain in performance. Method two stage preferably includes
measure by around 30% as compared to random, while en- sentences which are unparsable according to a parser trained
tropy actually increases the cost by 5.7% (Tab. 1).   on a bagged version of the current training set. Then, the
  We also compare performance across methods for the same batch is ﬁlled with (parsable) sentences which have high en-
amount of annotation effort. Tab. 2 shows precision, recall, tropy according to a second, fully trained parser.
and f-measure after labelling 25k constituents. Methods un-
                                                        Given that composite methods which preferably select un-
parsed and entropy/unparsed have considerably higher cov-
                                                      parsed sentences perform nearly uniformly well, we will now
erage than random, entropy has lower coverage. While all
                                                      use a considerably larger initial training set in order to be
methods show comparable values for precision, they differ
                                                      able to observe differences between these methods. We start
decidedly in their recall values. The two methods which
                                                      with 2,000 sentences (34k constituents), and continue for 30
aggressively pursue unparsed sentences, unparsed and en-
                                                      rounds until a total of 5,000 sentences has been sampled
tropy/unparsed, have more than 3% points higher recall than
                                                      (ca. 87k constituents).
entropy, and accordingly higher f-measures than random
and entropy.                                            Method two stage performs consistently better than both
  These results conﬁrm the importance of including unparsed random and entropy/unparsed (Fig. 3). It reduces the
sentences. Doing so helps achieving better coverage and a amount of labelled data necessary to reach 85.5% f-measure
higher recall value which directly translates into higher f- by 32.6% as compared to random (Tab. 3). The central re-
measure. Accordingly, all of the following experiments will sult of this paper is that, to reach this level, two stage reduces
include unparsed sentences in the batch. The negative re- the number of constituents by 8,800 constituents against the
sult for the purely entropy-based method shows clearly that state-of-the-art method entropy/unparsed: a reduction by a
a naive application of uncertainty sampling may have adverse further 10.1%. Also, it has consistently higher precision and
consequences. It is extremely important to consider which recall than entropy/unparsed after the labelling of 80k con-
phenomena a selection method is targeting.            stituents (Tab. 4).                                                                       Sentences  F-measure  Pearson
      86
                                                           parsable       4,919      83.9%    −0.37
    85.5                                                   unreliable       112      71.5%      0.05

      85                                              Table 5: Average f-measure and correlation coefﬁcients be-
                                                      tween entropy and f-measure

      84
                                                      periment to test this claim. Method oracle selects sentences
                                                      whose preferred parse tree (according to the current gram-


  f-measure  in %                                     mar) has low f-measure as determined against a gold-standard
      83                                              tree. Fig. 5 shows that method oracle performs consistently
                                                      better than our best result, the new two stage method. This
                                                      suggests that a selection method which successfully targets
                                oracle                difﬁcult sentences (low f-measure) will perform well.
      82                     two_stage
                               random                   In another experiment, we train the parser on a randomly
                                                      sampled training set of 2,000 sentences, and apply it to a test
              40k       60k     80k  100k 120k        set of 5,000 sentences. We are interested in the degree of
                  number of constituents              correlation between the variables f-measure (preferred tree
                                                      against gold-standard tree) and tree entropy. A correlation
Figure 5: Comparison of f-measure learning curves. Number analysis over all 4,919 parsable sentences shows that the two
of constituents is given on a log scale.              variables are indeed (negatively) correlated, cf. Tab. 5. Pear-
                                                      son’s coefﬁcient is −0.37. Given the size of the considered
     Method            Cov   Prec   Rec   Fm          data set, correlation is highly signiﬁcant (p = 0.01). Selec-
     random            99.0  85.1  84.0  84.5         tion according to entropy will thus tend to pick low f-measure
     two stage         99.7  86.0  85.2  85.6         sentences. Given the observation from the oracle experiment
     entropy/unparsed  99.6  85.7  84.9  85.3         that it is beneﬁcial to target low f-measure sentences, this
                                                      ﬁnding explains why entropy is a useful selection method.
Table 4: Parseval values for different metrics after 80,000 If we now apply a bagged version of the parser to the same
constituents have been annotated.                     test set, more sentences become unparsable since we elimi-
                                                      nate some infrequent parse events. Focusing on the 112 un-
                                                      reliable sentences which are parsable under a fully trained
An ensemble method   We now compare performance of    model, but not under a bagged model, we ﬁnd a Pearson co-
the new method against a state-of-the-art ensemble method, efﬁcient between entropy and f-measure close to 0 (Tab. 5).
namely the KL-divergence to the mean for an ensemble of n In other words, entropy and f-measure are uncorrelated, and
bagged parsers, cf. Subsec. 2. The method kl-div/unparsed entropy cannot reliably select difﬁcult examples within this
preferably selects unparsed pool sentences, and ﬁlls the batch class of sentences. What is more, the average f-measure
with sentences having a high mean KL-divergence. (We con- within these unreliable sentences is more than 12 percent-
sider a sentence as unparsed when at least one of the ensemble age points below average, indicating that acquiring their true
members fails to deliver an analysis.) As in the previous ex- parse trees will be particularly useful.
periment, we start with 2,000 sentences and continue for 30 Note that the ﬁrst stage of our new method targets exactly
rounds. We set the ensemble size to n = 5.            these kind of unreliable sentences. The above experiments
  Fig. 4 shows ensemble method kl-div/unparsed to per- demonstrate why the new method is indeed successful.
form better than two stage until 57k constituents. Af-
ter this point, two stage performs clearly better than kl-
div/unparsed. This is actually a surprising result, given that 7 Related Work
both methods perform similar jobs: They select unparsable Preferably selecting high entropy examples has been shown
sentences according to one or more bagged parsers, and then to be an effective method for parsing [Hwa, 2000]. Selecting
apply an information-theoretic measure, either entropy or unparsed sentences has been previously suggested because of
KL-divergence. We conjecture that, after having ﬁltered the their high uncertainty, e.g. in [Thompson et al., 1999]. How-
difﬁcult examples in the ﬁrst stage, the second stage should ever, to the best of our knowledge, this effect has not been
make use of the information to be had from a fully trained quantiﬁed before. Bagging ensemble members (or alterna-
parser. At any rate, our proposed method is conceptually tively random perturbation of event counts) has been explored
simpler and also quicker to compute than an ensemble-based in the context of active learning by [Argamon-Engelson and
method.                                               Dagan, 1999; McCallum  and Nigam, 1998].  In particu-
                                                      lar, Argamon-Engelson and Dagan have indicated that these
6  Understanding the New Method                       methods target low frequency events. Bagging (and boost-
                                                      ing) a parser ensemble has been employed to increase parser
Acquiring the annotation of objectively difﬁcult sentences performance [Henderson and Brill, 2000]. However, the ap-
should improve the parser. We employ an oracle-based ex- plication of a bagged parser ensemble to active learning is