                    Generalized Additive Bayesian Network Classiﬁers

             Jianguo Li†‡   and  Changshui Zhang‡     and  Tao Wang   † and  Yimin Zhang†
                              †Intel China Research Center, Beijing, China
                        ‡Department of Automation, Tsinghua University, China
               {jianguo.li, tao.wang, yimin.zhang}@intel.com, zcs@mail.tsinghua.edu.cn


                    Abstract                          called k-dependence Bayesian network (kdB). [Friedman et
                                                      al., 1997] proposed tree augmented Naive Bayes (TAN), a
    Bayesian network classiﬁers (BNC) have received   structure learning algorithm which learns a maximum span-
    considerable attention in machine learning ﬁeld.  ning tree (MST) from the attributes. Both TAN and kdB have
    Some special structure BNCs have been proposed    tree-structure graph. K2 is an algorithm which learns general
    and demonstrate promise performance. However,     BN for classiﬁcation purpose [Cooper and Herskovits, 1992].
    recent researches show that structure learning in
                                                        The key diﬀerences between these BNCs are their structure
    BNs may lead to a non-negligible posterior prob-
                                                      learning algorithms. Structure learning is the task of ﬁnding
    lem, i.e, there might be many structures have sim-
                                                      out one graph structure that best characterizes the true den-
    ilar posterior scores. In this paper, we propose a
                                                      sity of given data. Many criteria, such as Bayesian scoring
    generalized additive Bayesian network classiﬁers,
                                                      function, minimal description length (MDL) and conditional
    which transfers the structure learning problem to a
                                                      independence test [Cheng et al., 2002], have been proposed
    generalized additive models (GAM) learning prob-
                                                      for this purpose. However, it is inevitable to encounter such a
    lem. We ﬁrst generate a series of very simple BNs,
                                                      situation: several candidate graph structures have very close
    and put them in the framework of GAM, then adopt
                                                      score value, and are non-negligible in the posterior sense.
    a gradient-based algorithm to learn the combin-
                                                      This problem has been pointed out and presented theoretic
    ing parameters, and thus construct a more power-
                                                      analysis by [Friedman and Koller, 2003]. Since candidate
    ful classiﬁer. On a large suite of benchmark data
                                                      BNs are all approximations of the true joint distribution, it
    sets, the proposed approach outperforms many tra-
                                                      is natural to consider aggregating them together to yield a
    ditional BNCs, such as naive Bayes, TAN, etc, and
                                                      much more accurate distribution estimation. Several works
    achieves comparable or better performance in com-
                                                      have been done in this manner. For example, [Thiesson et
    parison to boosted Bayesian network classiﬁers.
                                                      al., 1998] proposed mixture of DAG, and [Jing et al., 2005]
                                                      proposed boosted Bayesian network classiﬁers.
1  Introduction                                         In this paper, a new solution is proposed to aggregate can-
Bayesian networks (BN), also known as probabilistic graph- didate BNs. We put a series of simple BNs into the framework
ical models, graphically represent the joint probability distri- of generalized additive models [Hastie and Tibshirani, 1990],
bution of a set of random variables, which exploit the con- and adopt a gradient-based algorithm to learn the combining
ditional independence among variables to describe them in parameters, and thus construct a more powerful learning ma-
a compact manner. Generally, a BN is associated with a di- chine. Experiments on a large suite of benchmark data sets
                                                                     ﬀ
rected acyclic graph (DAG), in which the nodes correspond to demonstrate the e ectiveness of the proposed approach.
the variables in the domain and the edges correspond to direct The rest of this paper is organized as follows. In Section
probabilistic dependencies between them [Pearl, 1988]. 2, we brieﬂy introduce some typical BNCs, and point out the
  Bayesian network classiﬁers (BNC) characterize the con- non-negligible problem in structure learning. In Section 3,
ditional distribution of the class variables given the attributes, we propose the generalized additive Bayesian network classi-
and predict the class label with the highest conditional prob- ﬁers. To evaluate the eﬀectiveness of the proposed approach,
ability. BNCs have been successfully applied in many ar- extensive experiments are conducted in Section 4. Finally,
eas. Naive Bayesian (NB) [Langley et al., 1992] is the sim- concluding remarks are given in Section 5.
plest BN, which only consider the dependence between each
feature xi and the class variable y. Since it ignores the de- 2 Bayesian Network Classiﬁers
pendence between diﬀerent features, NB may perform not
well on data sets which violate the independence assump- A Bayesian network B is a directed acyclic graph that en-
tion. Many BNCs have been proposed to overcome NB’s   codes the joint probability distribution over a set of random
                                                                            T
limitation. [Sahami, 1996] proposed a general framework variables x = [x1, ··· , xd] . Denote the parent nodes of xi
to describe the limited dependence among feature variables, by Pa(xi), the joint distribution PB(x) can be represented by

                                                IJCAI-07
                                                   913factors over the network structures as follows:       MST. Many experiments show that TAN signiﬁcantly outper-
                     
                        d                             forms NB. Figure 1(c) illustrates one possible graph structure
              PB(x) =     P(xi|Pa(xi)).
                        i=1                           of TAN.
                                                                                                  [
                  = { ,  }                              Both kdB and TAN generate tree-structure graph. Cooper
  Given data set D   (x y) in which y is the class vari- and Herskovits, 1992] proposed the K2 algorithm, which
able, BNCs characterize D by the joint distribution P(x, y),
                                    |                 adopts the K2 score measure and exhaustive search to learn
and convert it to conditional distribution P(y x) for predicting general BN structures.
the class label.
                                                      2.2  The structure learning problem
2.1  Several typical Bayesian network classiﬁers      Given training data D, structure learning is the task of ﬁnd-
The Naive Bayesian (NB) network assumes that each attribute ing a set of directed edges G that best characterizes the true
variable only depends on the class variable, i.e,     density of the data. Generally, structure learning can be cat-
                               
                                  d                   egorized into two levels: macro-level and micro-level. In the
        P(x, y) = P(y)P(x|y) = P(y) P(xi|y).
                                  i=1                 macro-level, several candidate graph structures are known,
                                                      and we need choosing the best one out. In order to avoid
Figure 1(a) illustrates the graph structure of NB.
                                         ﬀ            overﬁtting, people often use model selection methods, such
  Since NB ignores the dependencies among di erent fea- as Bayesian scoring function, minimum descriptive length
tures, it may perform not well on data sets which violate (MDL), etc [Friedman et al., 1997]. In the micro-level,
the attribute independence assumption. Many BNCs have structure learning cares about whether one edge in the graph
been proposed to consider the dependence among features. should be existed or not. In this case, people usually em-
[            ]
Sahami, 1996  presented a more general framework for  ploy the conditional independence test to determine the im-
limited dependence Bayesian networks, called k-dependence portance of edges [Cheng et al., 2002].
Bayesian classiﬁers (kdB).                              However, in both cases, people may face such a situa-
Deﬁnition 1: A k-dependence Bayesian classiﬁer is a   tion that several candidates (graphs or edges) have very close
Bayesian network which allows each feature xi to have a max- scores. For instance, suppose MDL is used as the crite-
imum of k feature variables as parents, i.e., the number of rion, people may encounter a situation that two candidate BN
                         +   +
variables in Pa(xi) equals to k 1(‘ 1’ means that k does not structure G1 and G2 have MDL score 0.899 and 0.900, re-
count the class variable y).                          spectively. Which one should be chosen? Someone may say
  According to the deﬁnition, NB is a 0-dependence BN. The that it is natural to select G1 out since it has a bit smaller MDL
kdB [Sahami, 1996] algorithm adopts mutual information score, but practice may show that G1 and G2 have similar per-
                                                      formance, and G may perform even better in some cases. In
I(xi; y) to measure the dependence between the ith feature          2
                                                      fact, both of them are non-negligible in the posterior sense.
variable xi and the class variable y, and conditional mutual in-
                                                        This problem has been pointed out and presented theoretic
formation I(xi, x j|y) to measure the dependence between two
                                                      analysis by [Friedman and Koller, 2003]. It shows that when
feature variables xi and x j.ThenkdB employs a heuristic rule
to construct the network structure via these two measures. there are many models that can explain the data reasonably
  kdB does not maximize any optimal criterion in structure well, model selection makes a somewhat arbitrary choice be-
learning. Hence, it yields limited performance improvement tween these models. Besides, the number of possible struc-
over NB. [Keogh and Pazzani, 1999] proposed super-parent tures grows super-exponentially with the number of random
Bayesian networks (SPBN), which assumes that there is an variables. For these two reasons, we don’t want to do struc-
attribute acting as public parent (called super-parent) for all ture learning directly. We hope aggregating a series of sim-
                                                      pler and weaker BNs together to obtain a much more accurate
the other attributes. Suppose xi is the super parent, denote the
                                                      distribution estimation of the underlying process.
corresponding BN as Pi(x, y), we have
                                                        We note that several researchers have proposed some
          ,   =          |     | ,
      Pi(x y)     P(y)P(xi y)P(x\i xi y)             schemes for this purpose, for examples, learning mixtures of
                              n                       DAG  [Thiesson et al., 1998], or ensembles of Bayesian net-
              =   P(y)P(xi|y)      P(x j|xi, y). (1)
                              j=1, ji                works by model averaging [Rosset and Segal, 2002; Webb et
It is obvious that SPBN structure is a special case of kdB al., 2005]. We brieﬂy introduce them in the following.
 =
(k 1). Figure 1(b) illustrates the graph structure of SPBN. 2.3 Model averaging for Bayesian networks
The SPBN algorithm adopts classiﬁcation accuracies as the
                                                      Since candidate BNs are all approximations of the true distri-
criterion to select out the best network structure.
                                                      bution, model averaging is a natural way to combine candi-
  [Friedman et al., 1997] proposed tree augmented Naive
                                                      dates together for a more accurate distribution estimation.
Bayes (TAN), which is also a special case of kdB (k=1). TAN
attempts to add edges to the Naive Bayesian network in order Mixture of DAG (MDAG)
to improve the posterior estimation. In detail, TAN ﬁrst com- Deﬁnition 2: If P(x| θc, Gc) is a DAG model, the following
                                      ,  |
putes the conditional mutual information I(xi x j y) between equation deﬁnes a mixture of DAG model
any two feature variables xi and x j, and thus obtain a full
                                                                    P(x|θs) =   πcP(x|θc, Gc),
adjacency matrix. Then TAN employs the minimum span-                           c
                                                            π                                      π ≥
ning tree algorithm (MST) on the adjacency matrix to obtain where c is the prior for the c-th DAG model Gc,and c 0,
                                                         π =   θ
a tree-structure BN. Therefore, TAN is optimal in the sense of c c 1, c is the parameter for graph Gc.

                                                IJCAI-07
                                                   914                                               y                                   y
                y

                                                       ...            x     x     x      x  ...  x
                                      xi    xj    xj+1      xd         1     2     3     4       d
                       ...
       x1    x2    x3       xd

           (a) Naive Bayesian          (b) Super-parent kdB (k=1)      (c) One possible structure of TAN

                                 Figure 1: Typical Bayesian network structures

  MDAG   learns the mixture models via maximizing the   GABN is an extensible framework since many diﬀerent
posterior likelihood of given data set. In detail, MDAG link functions can be considered. In this paper, we study a
combining uses the Chesseman-Stutz approximation and the special link function: fi(·) = log(·). Deﬁning z = (x, y)and
Expectation-Maximization algorithm for both the mixture taking exponent on both sides of the above equation, we have
components structure learning and the parameter learning.                          
                                                                             n           n  λ
  [               ]                                                 =         λ     =       i  .
   Webb et al., 2005 presented a special and simple case   exp[F(z)]  exp      i fi(z)    Pi (z)
of MDAG for classiﬁcation purpose, called the average one                    i           i
dependence estimation (AODE). AODE adopts a series of ﬁx- This is in fact a potential function. It can also be written as a
structure simple BNs as the mixture components, and directly probabilistic distribution when given a normalization factor,
assumes that all mixture components in MDAG have equal
                                                                                 n
mixture coeﬃcient. Practices show that AODE outperforms                      1       λ
                                                                      P(z) =        P i (z),          (5)
Naive Bayes and TAN.                                                        S λ(z)   i
                                                                                  i
Boosted Bayesian networks
Boosting is another commonly used technique for combin- where S λ(z) is the normalization factor:
ing simple BNs. [Rosset and Segal, 2002] employed the gra-         n             n         
                                                                        λ
dient Boosting algorithm [Friedman, 2001] to combine BNs S λ(z) =      P i (z) =  exp    λ log P (z) . (6)
                    [             ]                                     i                  i    i
for density estimation. Jing et al., 2005 proposed boosted       z   i          z      i=1
Bayesian network classiﬁers (BBN), and adopted general Ad-
aBoost algorithm to learn the weight coeﬃcients.        The likelihood of P(z) is called quasi-likelihood:
  Given a series of simple BNs: Pi(x, y), i = 1, ··· , n,            
                                                                        N
BBN aims to construct the ﬁnal approximation by linear ad- L(λ)  =       log P(zk)
ditive models: P(x, y) = n α P (x, y), where α ≥ 0are                   k=1
                      i=1 i i            i                               n                     
         ﬃ              α  =                                            N
weight coe cients, and i i    1. More generally, the
                                                                  =            λi log Pi(zk) − log S λ(zk)
constraint on α can be relaxed, but only α ≥ 0iskept:                   k=1
            i                        i                                     =
F(x, y) = n  α P (x, y). In this case, the posterior can be                i 1
          i=1 i i                                                       N                  
deﬁned as follows                                                 =        λ ·    −     λ    ,
                                                                        =     f(zk) log S (zk)        (7)
                        exp{F(x, y)}                                    k 1
              P(y|x) =             .           (2)
                            {   ,  }                       λ = λ , ··· ,λ T    =      , ··· ,   T
                       y exp F(x y )                 where    [ 1      n] , f(zk) [ f1(zk) fn(zk)] .
For general binary classiﬁcation problem y ∈{-1,1}, this prob-
                                                      3.1  The Quasi-likelihood optimization problem
lem can be solved by the exponent loss function
             L(α) =     exp{−yF(xk, y)}         (3)   Maximizing the quasi-likelihood, we can obtain the solution
                       k                              of the additive parameters. To make the GAM model mean-
via the AdaBoost algorithm [Friedman et al., 2000].   ingful and tractable, we add some constraints to the parame-
                                                      ters. The ﬁnal optimization problem turns to be:
3  Generalized additive   Bayesian networks
                                                                            L  λ
In this section, we present a novel scheme that can aggregate          max    ( )
                                                                         . .     ≤ λ ≤
a series of simple BNs to a more accurate density estima-               s t (1) 0  i  1              (8)
                                                                            (2)   λ = 1
tion of the true process. Suppose Pi(x, y), i = 1, ··· , n are the               i i
given simple BNs, we consider putting them in the framework
of generalized additive models (GAM) [Hastie and Tibshi- For equation constraint, the Lagrange multiplier can be
rani, 1990]. The new algorithm is called generalized additive adopted to transfer the problem into an unconstraint one;
Bayesian network classiﬁers (GABN).                   while for inequation constraints, classical interior point
  In the GAM framework, P (x, y) are considered to be linear method (IPM) can be employed. In detail, the IPM utilizes
                        i                             barrier functions to transfer inequation constraints into a se-
additive variables in the link function space:
                        n                             ries of unconstraint optimization problems [Boyd and Van-
              F(x, y) =   λi fi[Pi(x, y)].      (4)   denberghe, 2004].
                        i

                                                IJCAI-07
                                                   915  Here, we adopt the most used logarithmic barrier function,
and obtain the following unconstraint optimization problem:   Table 1: The training algorithm for GABN
                                                                                     N
                                                      Input: Given training set D = {(xi, yi)} =
                     n              n                                                i 1
  L(λ, rk,α) =   rk     log(λi) + rk   log(1 − λi)      Training Algorithm
                     i=1           i=1
                        n                                S0: set convergence precision >0, and the maximal step M;
             +   α(1 −     λi) + L(λ)                                             λ = λ , ··· ,λ T λ = /
                        i=1         
                    S1: initialize the interior point as [ 1 n] , i 1 n;
                                                                                        ,    = , ··· ,
             =   rk log(λ) + log(1n − λ) · 1n            S2: generate a series of simple BNs: Pi(x y), i 1 n;
                                                         S3:for k = 1:M
             +   α(1 − λ · 1n) + L(λ),          (9)
                                                             S4: select rk > 0andrk < rk−1,
                                                                                               L λ, ,α
where 1n indicates a n-dimensional vector with all elements     obtain the kth step optimization problem ( rk );
equal to 1, rk is the barrier factor in the kth step of the IPM S5: calculate gλ and the quasi-likelihood L(λ);
           α
iteration, and is the Lagrange multiplier.                   S6: employ L-BFGS procedure to solve: max L(λ, rk,α);
  Therefore, in the kth IPM iteration step, we need to max-  S7: test of the barrier term
imize an unconstraint problem L(λ, r ,α). Quasi-Newton                                  

                                k                                    =      λ +      − λ ·
method is adopted for this purpose.                                ak  rk log( ) log(1n ) 1n;

                                                             S8:ifak <jump to S9, else continue the loop;
3.2  Quasi-Newton method for the unconstraint            S9: Output the optimal parameter λ∗,
     optimization problem                                   and obtain the ﬁnal generalized models P(z; λ∗).

To solve the unconstraint problem: max L(λ, rk,α), we must
                 L     λ
have the gradient of w.r.t .                          3.4  A series of ﬁx-structure Bayesian networks
                        L  λ, ,α      λ
Theorem 1: The gradient of ( rk ) w.r.t is            There are one unresolved problem in the algorithm listed in
                                                   Table 1, which is in the S2 step, i.e, how to generate a series
    ∂L(λ, rk,α)            N
                =   gλ =      f(zk) − EP(z)[f(zk)]    of simple BNs as the weak learner. There are many methods
        ∂λ                 k=1
                               
                      for this purpose. In our experiments, we take super parent
                +     1 −   1    ·  − α  .            BN as the weak learner. Readers may consider other possible
                    rk λ    − λ   1n   1n      (10)
                          1n                          strategies to generate simple BNs.
                                                                                                ﬀ
Proof: In Equation (10), it is easy to obtain the gradient of For a d-dimensional data set, when setting di erent at-
                                                      tribute as the public parent node according to Equation (1),
the ﬁrst summation term and non-summation terms. Here, we               ﬀ
only present the gradient solution of the second summation it can generate d di erent ﬁx-structure super-parent BNs:
                                                      P (x, y), i = 1, ··· , d. Figure 1(b) depicts one example of
term, i.e., log S λ(z )inL(λ).                         i
               k                                      this kind of simple BNs. To improve performance, mutual in-
              ∂               ∂                       formation I(x , y) is computed for removing several BNs with
               log S λ(z) = 1   S λ(z)                           i
                 ∂λ       S λ(z) ∂λ                   lowest mutual information score. In this way, we obtain n
                                                   very simple BNs, and adopt them as weak learners in GABN.
                                                        Parameters (conditional probabilistic table) learning in
              ∵ S λ(z) =  exp λ · f(z)
                         z                            BNs is common, and thus details are omitted here. Note that
                                                   for robust parameter estimation, Laplacian correction and m-
           ∂S λ(z)
                  =     f(zk)exp λ · f(zk)            estimate [Cestnik, 1990] are adopted.
             ∂λ       zk
                                                   3.5  Discussions
       ∂log S λ(z)     1
     ∴            =          f(zk)exp λ · f(zk)       GABN has several advantages over the typical linear additive
          ∂λ         S λ(z)
                          zk                         BN models: Boosted BN (BBN). First, GABN is much more
                                            
         computational eﬃcient than BBN. Given d-dimensional and
                  =      P(zk)f(zk) = EP(z) f(zk) . 
                        zk                            N samples training set, it is not hard to prove that the com-
                                                      putational complexity of GABN is O(Nd2 + MNd), where M
  For computational cost consideration, we did not further
                                    L λ, ,α           is the IPM iteration steps. On the contrary, BBN requires se-
compute the second order derivative of ( rk ), while  quentially learning BN structures in each boosting step. This
                              [           ]
adopted the quasi-Newton method Bishop, 1995 to solve leads to a complexity of O(KNd2), where K is the boosting
the problem. In this paper, the L-BFGS procedure provided step, which is usually very large (in 102 magnitude). There-
  [                   ]
by Liu and Nocedal, 1989 is employed for this task.   fore, GABN dominates BBN on scalable learning task. Prac-
                                                      tice also demonstrates this point.
3.3  The IPM based training algorithm                   Furthermore, GABN presents a new direction for combin-
The interior point method starts from a point in the feasible ing weaker learners since it is a highly extensible framework.
region, sequentially adjusts the barrier factor rk in each itera- We present a solution for logarithmic link function. It is not
tion, and solves a series unconstraint problem L(λ, rk,α), k = hard to adopt other link functions under the GAM framework,
1, 2, ···. The detailed training algorithm is shown in Table 1. and thus propose new algorithms. Many existing GAM prop-
                                                      erties, optimization methods can be seamlessly adopted to ag-

                                                IJCAI-07
                                                   916gregate simple BNs for more powerful learning machines.       [Cheng et al., 2002] J. Cheng, D. Bell, and W. Liu. Learning be-
                                                                 lief networks from data: An information theory based approach.
4    Experiments                                                 Artiﬁcial Intelligence, 137:43–90, 2002.
                                                               [Cooper and Herskovits, 1992] G. Cooper and E. Herskovits. A
This section evaluates the performance of the proposed algo-     Bayesian method for the induction of probabilistic networks from
rithm, compared it with other BNCs such as NB, TAN, K2,          data. Machine Learning, 9:309–347, 1992.
kdB, SPBN; model averaging methods such as AODE, BBN;
                                                               [                  ]
and decision tree algorithm CART  [Breiman et al., 1984].      Dougherty et al., 1995 J. Dougherty, R. Kohavi, and M. Sahami.
                                                                 Supervised and unsupervised discretization of continuous fea-
   The benchmark platform was 30 data sets from the UCI          tures. In the 12th Intl. Conf. Machine Learing (ICML), San Fran-
machine learning repository  [Newman   et al., 1998].One         cisco, 1995. Morgan Kaufmann.
point should be indicated here: for BNCs, when data sets have
                                                               [                      ]
continuous features, we ﬁrst adopted discretization method to  Friedman and Koller, 2003 N. Friedman and D. Koller. Being
                                                                 Bayesian about network structure: a Bayesian approach to struc-
transfer them into discrete features [Dougherty et al., 1995].   ture discovery in Bayesian networks. Machine Learning, 50:95–
We employed 5-fold cross-validation for the error estimation,    126, 2003.
and kept all compared algorithms having the same fold split.
                                                               [Friedman et al., 1997] N. Friedman, D. Geiger, and M. Gold-
The ﬁnal results are shown in Table 2, in which the results      szmidt.  Bayesian network classiﬁers. Machine Learning,
by TAN and K2 are obtained by the Java machine learning          29(2):131–163, 1997.
toolbox Weka  [Witten and Frank, 2000].
                                                               [                  ]
   To present statistical meaningful evaluation, we conducted  Friedman et al., 2000 J. Friedman, T. Hastie, and R. Tibshirani.
                                                                 Additive logistic regression: a statistical view of boosting. Annals
the paired t-test to compare GABN with others. The last row      of Statistics, 28(337-407), 2000.
of Table 2 shows the win/tie/lose summary in 10% signiﬁ-
cance level of the test. In addition, Figure 2 illustrates the [Friedman, 2001] J. Friedman. Greedy function approximation: a
scatter plot of the comparison results between GABN and          gradient boosting machine. Annals of Statistics, 29(5), 2001.
other classiﬁers. We can see that GABN outperforms most        [Hastie and Tibshirani, 1990] T. Hastie and R. Tibshirani. General-
other BNCs, and achieves comparable performance to BBN.          ized Additive Models. Chapman & Hall, 1990.
Specially note, the SPBN column shows results by the best      [Jing et al., 2005] Y. Jing, V. Pavlovi´c, and J. Rehg. Eﬃcient dis-
individual super-parent BN, which are signiﬁcant worse than      criminative learning of Bayesian network classiﬁers via boosted
GABN. This demonstrates that it is eﬀective and meaningful       augmented naive Bayes. In the 22nd Intl. Conf. Machine Learn-
to use GAM for aggregating simple BNs.                           ing (ICML), pages 369–376, 2005.
                                                              [Keogh and Pazzani, 1999] E. Keogh and M. Pazzani. Learning
5    Conclusions                                                 augmented Bayesian classiﬁers: A comparison of distribution-
                                                                 based and classiﬁcation-based approaches. In 7th Intl. Workshop
In this paper, we propose a generalized additive Bayesian        Artiﬁcial Intelligence and Statistics, pages 225–230, 1999.
network classiﬁers (GABN). GABN aims to avoid the non-        [Langley et al., 1992] P. Langley, W. Iba, and K. Thompson. An
negligible posterior problem in Bayesian network structure       analysis of Bayesian classiﬁers. In the 10th National Conf. Arti-
learning. In detail, we transfer the structure learning problem  ﬁcial Intelligenc (AAAI), pages 223–228, 1992.
to a generalized additive models (GAM) learning problem.
                                                              [Liu and Nocedal, 1989] D. Liu and J. Nocedal. On the limited
We ﬁrst generate a series of very simple Bayesian networks       memory BFGS method for large-scale optimization. Mathemati-
(BN), and put them in the framework of GAM, then adopt           cal Programming, 45:503–528, 1989.
a gradient-based learning algorithm to combine those sim-
                                                              [                  ]
ple BNs together, and thus construct a more powerful clas-     Newman  et al., 1998 D. Newman, S. Hettich, C. Blake, and
                                                                 C. Merz. UCI repository of machine learning databases, 1998.
siﬁers. Experiments on a large suite of benchmark data sets
demonstrate that the proposed approach outperforms many       [Pearl, 1988] J. Pearl. Probabilistic Reasoning in Intelligent Sys-
traditional BNCs such as naive Bayes, TAN, etc, and achieves     tems: Networks of Plausible Inference. Morgan Kaufmann, 1988.
comparable or better performance in comparison to boosted     [Rosset and Segal, 2002] S. Rosset and E. Segal. Boosting density
Bayesian network classiﬁers. Future work will focus on other     estimation. In Advances in Neural Information Processing Sys-
possible extensions within the GABN framework.                   tem (NIPS), 2002.
                                                              [Sahami, 1996] M. Sahami. Learning limited dependence Bayesian
References                                                       classiﬁers. In the 2nd Intl. Conf. Knowledge Discovery and Data
                                                                 Mining (KDD), pages 335–338. AAAI Press, 1996.
[Bishop, 1995] C. M. Bishop. Neural Networks for Pattern Recog-
   nition. Oxford University Press, London, 1995.             [Thiesson et al., 1998] B. Thiesson, C. Meek, D. Heckerman, and
                                                                 et al. Learning mixtures of DAG models. In Conf. Uncertainty
[Boyd and Vandenberghe, 2004] S. Boyd and L. Vandenberghe.       in Artiﬁcial Intelligence (UAI), pages 504–513, 1998.
   Convex Optimization. Cambridge University Press, 2004.
                                                              [Webb  et al., 2005] G. Webb, J. R. Boughton, and Zhihai Wang.
[Breiman et al., 1984] L. Breiman, J. Friedman, R. Olshen, and   Not so naive Bayes: aggregating one-dependence estimators.
   C. Stone. Classiﬁcation And Regression Trees.WadsworthIn-     Machine Learning, 58(1):5–24, 2005.
   ternational Group, 1984.
                                                              [Witten and Frank, 2000] I. Witten and E. Frank. Data Mining:
[Cestnik, 1990] B. Cestnik. Estimating probabilities: a crucial task Practical Machine Learning Tools and Techniques with Java Im-
   in machine learning. In the 9th European Conf. Artiﬁcial Intelli- plementations. Morgan Kaufmann Publishers, 2000.
   gence (ECAI), pages 147–149, 1990.

                                                        IJCAI-07
                                                          917