                   A Learning Algorithm for Web Page Scoring Systems 

                         Michelangelo Diligenti, Marco Gori, Marco Maggini 
                 Dipartimento di Ingegneria dell'Informazione, Universita di Siena, Italy 
                                  {michi, marco, maggini}@dii.unisi.it 


                     Abstract                          to manage both positive examples (relevant pages) and neg•
                                                       ative examples (not relevant pages). The ranking function is 
    Hyperlink analysis is a successful approach to de•
                                                       based on the Web surfer model [Diligenti et al. 2002]. The 
    fine algorithms which compute the relevance of a 
                                                       relevance of a page is defined by the probability of the surfer 
    document on the basis of the citation graph. In this 
                                                       to visit the page when considering the stationary probability 
    paper we propose a technique to learn the parame•
                                                       distribution of the Markov chain which describes the surfer 
    ters of the page ranking model using a set of pages 
                                                       behavior. Basically the learning algorithm computes the pa•
    labeled as relevant or not relevant by a supervisor. 
                                                       rameters which define the surfer model in order to reduce the 
    In particular we describe a learning algorithm ap•
                                                       probability of visiting a negative example and to increase the 
    plied to a scheme similar to PageRank. The rank•
                                                       probability of being in a positive example. In order to reduce 
    ing algorithm is based on a probabilistic Web surfer 
                                                       the number of free parameters, the surfer model is simplified 
    model and its parameters are optimized in order to 
                                                       by dividing the pages among a set of categories and assuming 
    increase the probability of the surfer to visit a page 
                                                       that the surfer behavior depends only on the category of the 
    labeled as relevant and to reduce it for the pages 
                                                       page where it is located. In particular, the set of categories 
    labeled as not relevant. The experimental results 
                                                       does not need to be predefined but it is determined by a clus•
    show the effectiveness of the proposed technique in 
                                                       tering algorithm based on the document contents. Moreover, 
    reorganizing the page ordering in the ranking list 
                                                       whereas the focused versions of the PageRank simply direct 
    accordingly to the examples provided in the learn•
                                                       the Web surfer to the most promising page (i.e. the page 
    ing set. 
                                                       whose content is the most similar to the content of a target 
                                                       page), the proposed algorithm can exploit hierarchies of top•
1   Introduction                                       ics, allowing the surfer to follow complex paths, composed 
                                                       by many steps, leading to relevant pages. 
PageRank [Page et al, 1998] is the most popular algorithm 
                                                         The paper is organized as follows. In the next section the 
to compute the page relevance in a collection of hyper-
                                                       web surfer model is reviewed. Then, in section 3 the learning 
linked documents, like the Web. This algorithm exploits the 
                                                       algorithm is described. Section 4 reports some experiments 
topology of the citation graph in order to determine the au•
                                                       which show the effectiveness of the proposed learning algo•
thority of each page using the intuitive idea that authorita•
                                                       rithm. Finally, in section 5 the conclusions are drawn. 
tive documents are linked by many authoritative documents. 
Thus, the original algorithm computes a ranking which de•
pends only on the graph connectivity without considering 2 Random Walks on the Web graph 
the page contents. However, focused versions of PageR• Random walk theory has been widely used to compute the 
ank have been proposed in the literature [Haveliwala, 2002; absolute relevance of a page in the Web [Page et al, 1998; 
Richardson and Domingos, 2002; Diligenti et al, 2002] in or- Lempel and Moran, 2000]. The Web is represented as a graph 
der to define more selective rankings for topic-specific search G, where each Web page is a node and a link between two 
engines. These approaches allow a limited degree of adapta• nodes represents a hyperlink between the associated pages. 
tion related to the choice of the specific topic. A text classifier A common assumption is that the relevance Ip of page p is 
is trained using examples of pages on the topic of interest but represented by the probability of ending up in that page dur•
no further level of learning is considered in the ranking mod• ing a walk on this graph. 
els. In particular, the classifier is trained independently on the We consider the model of a Web surfer who can perform 
ranking algorithm.                                     one out of two atomic actions while visiting the Web graph: 
  We propose a more general approach in which the learning jumping to a node of the graph (action J) or following a 
algorithm is defined directly on the ranking model. In this hyperlink from the current page (action I). In general, the 
case there is not a predefined topic, but a supervisor provides action taken by the surfer will depend on the page contents 
examples of pages to be promoted and pages which are con• and the links it contains. We can model the user's behavior 
sidered as not relevant. Thus, the learning algorithm is able by a set of probabilities which depend on the current page: 


LEARNING                                                                                               575         is the probability of following one hyperlink from 
 page q, and is the probability of jumping from page 
 q. These values must satisfy the normalization constraint, 

   The two actions need to specify their targets. Assum•
 ing that surfer behavior is time-invariant, then the targets are 
 specified by the probability of jumping from page 
 q to page p, and the probability of selecting a hy•
 perlink from page q to page p. is not null only 
 for the pages p linked directly by page 
 being ch(q) the set of the children of node q in the graph 
 G. These sets of values must satisfy the probability nor•
 malization constraints and 

   The model considers a temporal sequence of actions per•
 formed by the surfer and it can be used to compute the proba•
 bility that the surfer is located in page p at time The 
 probability distribution on the pages of the Web is updated by 
 taking into account the actions at time using the follow•
 ing equation 
                                                               Figure 1: The context graph built by following the links back•
                                                               wards from an example page. 

                                                               where is the cluster which page q belongs to, is the 
                                                               number of pages in cluster the parameter 
 where is the set of the parents of node p. The relevance      is the tendency of the surfer to follow a hyperlink from a 
 of a page p, is computed using the stationary distribution    page in cluster to a page in cluster and the parameter 
 of the Markov Chain of equation (1). This is a simplified                is the tendency of the surfer to jump from a page 
 version of the model proposed in [Diligenti et al, 2002].     of cluster to a page of cluster The values 
                                                               are initialized to 1, such that all links are equally likely to be 
                                                               followed. Because of the model assumptions, the stronger is 
 3 Learning the page rank                                      the tendency of the surfer to move to a cluster, the higher are 
 The random surfer model which is used to compute the page     the scores associated to the pages in that cluster. 
 relevance scores depends on the values assigned to the proba•
 bilities and In most of the                                   3.1 The learning set 
 approaches proposed in the literature, these parameters are   We assume that a supervisor provides a set pages labeled as 
 predefined or computed from some features describing the      relevant and not relevant. These examples are used to define 
 page p and/or the page q. For example, in the original PageR- a specific view of the collection of documents contained in a 
 ank scheme and search engine. In particular the supervisor provides: 
                                                                    a set of pages which should get a high rank; 
   However, in the general case, it would be infeasible to es•
timate the parameters without any assumption to reduce their        a set of pages which should get a low rank. 
number. In particular, we assume that the pages are grouped    In the first case we refer to "positive" or "good" pages, 
 into n clusters according to their content and that the surfer's whereas in the latter case to "negative" or "bad" pages. 
behavior for any page p is dependent only on the cluster         Each example page is associated to its Context Graph [Dili•
which p belongs to. In our experiments we used classical       genti et al, 2000], representing how the page can be reached 
clustering techniques (e.g. k-means) on the bag-of-words rep•  from the Web. The graph is organized into layers as shown 
resentation of the pages [Kobayashi and Takeda, 2000]. By      in figure 1. The example page is the only node in layer 0. 
this hypothesis, the parameters of the model are computed as:  Then, each node in layer links only nodes in layer  
                                                               Thus, a page belongs to layer i if it is at least i links away 
                                                        (2)    from the example page. The depth K of the context graph is 
                                                               defined as the number of layers in the graph including level 0. 
                                                               For a given example, the context graph can be built by back-
                                                               crawling the Web adding to layer the pages which link 
                                                               the pages in layer i, up to a maximum level K. Moreover, all 
                                                               the pages that are linked by at least a page of the graph are 
                                                               also considered. 


576                                                                                                           LEARNING    The learning set consists of the context graphs built from    Similar relationships can be derived for the negative set. 
the examples in and is the set of all the pages                  The cost function (3) can be optimized by updating the pa•
contained in the learning set; is the union of the             rameters in using gradient ascent. The gradient with re•
pages in the layer k of the context graphs of the positive (neg• spect to the transition parameters can be 
ative) examples.                                               computed using equation (4), yielding 
3.2   Learning the transition parameters 
We define the set of transition probabilities  
                                which represent the proba•
bilities of the surfer to select a hyperlink from a page in the 
cluster to a page in the cluster The rank of the pages in 
the set is increased, whereas the rank of the pages in 
is decreased if we increase the probability of the paths leading where we neglected that and  
to pages in and we reduce the probability of paths leading                    actually depend on the transition parameters. 
to pages in . This effect can be obtained by maximizing        Since the model parameters are essentially a not 
the following cost function                                    normalized version of the transition parameters 
                                                                    we can update them directly as  

where is a discount factor which is used to reduce 
the impact on the cost function of the links too far from the 
target page.                                                   where 77 is the learning rate. 
   The probability of moving one                                 Once the model parameters are updated, a new score dis•
level in the context graph towards a good page, can be rewrit• tribution can be computed, thus updating the estimate of the 
ten as                                                         variables using equation (6). There•
                                                               fore, the function optimization and the score estimation can 
                                                               be iterated, using an EM style algorithm [Dempster et 0/., 
                                                               1977], till a termination criterion is satisfied. 

                                                               3.3  Learning the jump parameters 
                                                               The transition parameters for a jump can be optimized to in•
                                                               crease the probability of jumping to a "good" page and not to 
                                                               a "bad" page. This effect is obtained by maximizing the cost 
                                                               function 

where reflects the surfer model assump•                        where                                            is the set 
tion that the links are selected depending only on the clusters of the jump parameters. 
   and . The probability                                         We start observing that, 

      of following a link to a page in the cluster c3 of layer A: 
from a page in the cluster of layer of a context graph 
can be estimated as 


where is the number of links from the pages in 
the set A to the pages in the set B. 
   On the other hand the term repre•
sents the probability that the surfer is located in a page of the 
cluster given that the page belongs to the layer of 
a positive context graph. This probability can be estimated 
using the scores of the pages in cluster and in com•
puted using the current values of the parameters, as          Moreover, the value is computed at each iteration 
                                                              as 


LEARNING                                                                                                             577  Similar relationships can be derived for the negative set. 
   The cost function (9) is maximized by gradient ascent. The 
 gradient can be computed using equation (10) and the equiv•
 alent equation for the negative set, yielding 
                                                               where we assumed that all links out of a page are equally 
                                                               likely to be followed. The term is the 
                                                               probability of jumping to the layer fc of a positive context 
where we neglected the dependence of x(q d \ J) on the pa•     graph from a page of cluster i, and can be computed as 
rameters. At each iteration, the parameters are updated using 
the direction of the gradient as,                                                                                    (19) 

                                                               Finally, can be computed at each iteration as in 
being r; the learning rate, and then normalized to meet the    equation (12). Similar relationships hold for the negative set. 
probabilistic constraints                                        The gradient of the cost function (16) can be computed us•
                                                               ing equation (17) and the corresponding relationship for the 
                                                               negative set, yielding 

   The adapted surfer model can re-surf the Web graph yield•
ing a new estimate of equation (12). Thus, the new gradient 
estimate can be used to update again the parameters, till the  neglecting the dependence of 
stop criterion is satisfied.                                     I he parameters are updated by gradient ascent as 
3.4   Learning the surfer action bias 
The surfer action bias is represented by the value of  
    which is the probability of following a link from the cur•
rent page rather than jumping to another page                  where the learning rate must be chosen small enough in order 
                 In order to favor the action leading to good  to preserve the stochastic model of the surfer. 
pages, we maximize the following cost function which rep•        The adapted surfer model can re-surf the Web graph yield-
resents a weighted average of the probabilities of ending in   ing a new estimate of equation (12). Thus, the new gradient 
the layer k of a positive context graph and not of a negative  estimate can be used to update again the parameters and the 
context graph,                                                 procedure is iterated till the stop criterion is satisfied. 

                                                               4   Experimental results 
                                                               In the following experiments, the learning algorithm was ap•
                                                      (16)    plied only to the transition parameters of the random surfer 
                                                               model. The experiments were performed on two datasets, 
                                                              each containing 1.000.000 documents, which were collected 
                                                              by focus crawling the Web on the topics "wine" and "playsta-
                                                              tion", respectively. The documents were clustered using a 
                                                              hierarchical version of the k-means algorithm [Fukunaga, 
                                                               1990]. We considered different numbers of clusters generated 
                                                              by the k-means algorithm. In particular, in two different runs, 
                                                              we clustered the pages from the dataset on topic "playstation" 
                                                              into 16 and 25 sets, whereas the dataset on topic "wine" was 
                                                              clustered into 25 and 100 sets. 
                                                                 Two topic-specific search engines were build creating the 
                                                               inverted lists containing the terms of all the documents. Then, 
                                                              the rank of each page in the dataset was computed using the 
                                                              model described in section 2 by setting its parameters to re•
                                                      (17)    produce the original PageRank ranking. We tested the quality 
                                                              of the ranking function by submitting a set of queries to this 
                                                              focused search engine. The documents matching the query 
                                                              were sorted by the scores assigned by the random surfer. By 
                                                              interacting with the search engine, wc found pages which 
                                                              were authorities on the specific topic but were (incorrectly) 
                                                      (18)    not inserted in the top positions of the result list. Such pages 
                                                              were selected as examples of pages which should get a higher 


578                                                                                                           LEARNING                                                                Figure 3: Plots of the probability of the surfer of being located 
                                                               in a page of a given cluster for the topic "wine". The proba•
                                                               bilities are normalized with respect to the number of pages in 
                                                               the cluster, (a) 25 clusters, (b) 100 clusters. 
 Figure 2: Plots of the values of the transition parameters for 
 each cluster, resulting from the training of the surfer, (a)  of that cluster. In particular we used the dataset on topic 
 "wine" dataset. (b) "playstation" dataset.                    "wine" clustered into 25 and 100 groups, respectively. The 
                                                               probabilities arc normalized with respect to the number of 
                                                               pages in the cluster. The learning algorithm is able to increase 
 rank (positive examples). On the other hand, we found pages 
                                                               the likelihood of the random surfer to visit pages belonging 
 having a high score which were not authorities on the topic. 
                                                               to a set of clusters, while decreasing its probability of visiting 
 Such pages were selected as negative examples. The learn•
                                                               pages belonging to other clusters. 
 ing algorithm, as described in section 3, was applied to the 
                                                                 In figure 4, we report the rank of pages after the learning 
 datasets, using the selected examples. In the experimental 
                                                               session with respect of their rank before learning. In particu-
 setup only a small set of examples (3-15) was typically la•
                                                               lar, on the x axis the pages are sorted according to their page 
 belled as positive or negative. 
                                                               rank before the learning session. The closer a page to the 
                                                               origin, the more "relevant" the page is. On the y axis it is re•
 4.1  Analysis of the effects of learning 
                                                               ported the corresponding page rank after the learning session. 
 In a first group of experiments we aimed at demonstrating     The plot shows the generalization capability of the learning 
 that the surfer model as learnt by the proposed training algo• algorithm, which is able to change the ranks of a significant 
 rithm, could not be generated by a simpler (but less powerful) percentage of pages, even if a small set of example was pro•
 schema as the focused versions of PageRank, proposed in the   vided. 
 literature. 
   In figure 2-(a) 2-(b), we show the values of the transi•    4.2   Qualitative results 
tion parameters for each cluster, resulting from the train•    After training the surfer and computing the scores for all 
 ing sessions for a surfer on the topic "wine" and "playsta•   pages in the dataset, we compared the rank before and after 
tion", respectively. The learnt parameters represent a com•    the training process. 
plex interaction of the resulting surfer with the page clus•     Figure 5 reports the pages which obtained the largest vari•
ters. This interaction could not be expressed by a simple      ation in their rank. The pages that obtained a negative rank 
 schema as, for example, the focused versions of PageRank      variation were either topic-generic authoritative pages (e.g. 
proposed in the literature [Richardson and Domingos, 2002;     www.netscapc.com, www.yahoo.com) or pages relevant for 
Diligenti et al., 2002]. This demonstrates that the model, in  different topics than "wine" (e.g. "www.forgottensoldier-
order to learn the users' feedback, needed to exploit hierar•  .org"). Only the page "www.yahoo.com" was explicitly pro•
chies of topics to model the complex path leading either to    vided to the system as a negative example. On the other hand, 
positive or negative examples.                                 the pages that obtained a positive rank variation were effec•
   In figure 3-(a) and 3-(b), we show for each cluster, the val• tively authoritative pages on the topic "wine" (e.g. "www-
ues of the probability of the surfer of being located in a page .wine-searcher.com" or "webwinery.com"). Among the most 


LEARNING                                                                                                             579 