            Web Page Clustering using Heuristic Search in the Web Graph

          Ron Bekkerman                   Shlomo Zilberstein                   James Allan
    University of Massachusetts       University of Massachusetts       University of Massachusetts
         Amherst MA, USA                   Amherst MA, USA                  Amherst MA, USA
        ronb@cs.umass.edu                shlomo@cs.umass.edu                allan@cs.umass.edu


                    Abstract                          group search results by various meanings of the query. Re-
                                                      cently, successful academic [Zeng et al., 2004] and indus-
    Effective representation of Web search results re- trial (vivisimo.com) attempts have made the clustering of
    mains an open problem in the Information Re-      search results plausible for many WWW users. However, it is
    trieval community. For ambiguous queries, a tra-  still not accurate enough to attract an average user. The main
    ditional approach is to organize search results into drawback of many Web page clustering methods is that they
    groups (clusters), one for each meaning of the    take into account only the topical similarity between docu-
    query. These groups are usually constructed ac-   ments in the ranked list.
    cording to the topical similarity of the retrieved
    documents, but it is possible for documents to be   Topical similarity metrics between Web pages would not
    totally dissimilar and still correspond to the same help solving the clustering problem in at least two cases: (a)
    meaning of the query. To overcome this prob-      when there is not enough contextual information on a page:
    lem, we exploit the thematic locality of the Web— for example, within 20 ﬁrst hits on a query jaguar one can
    relevant Web pages are often located close to each ﬁnd a Web site savethejaguar.com, which presents a
    other in the Web graph of hyperlinks. We estimate large picture of the wild cat on the background, but does not
    the level of relevance between each pair of retrieved contain enough topical words to automatically associate the
    pages by the length of a path between them. The   page to the correct group; (b) when Web sites are contextu-
    path is constructed using multi-agent beam search: ally different but actually refer to the same meaning of the
    each agent starts with one Web page and attempts  query. For instance, given a query Michel Decary´ , one
    to meet as many other agents as possible with some can retrieve Web pages of at least three individuals: a com-
    bounded resources. We test the system on two types puter scientist (www.zoominfo.com/MichelDecary),
    of queries: ambiguous English words and people    alawyer(www.stikeman.com/cgi-bin/profile.
    names. The Web appears to be tightly connected;   cfm?P  ID=366), and a chansonnier (www.decary.com).
    about 70% of the agents meet with each other af-  All three studied at the University of Montr´eal and at McGill
    ter only three iterations of exhaustive breadth-ﬁrst University in Canada. Are they three different people or ac-
    search. However, when heuristics are applied, the tually one person?
    search becomes more focused and the obtained re-    These problems can be resolved by exploiting the thematic
    sults are substantially more accurate. Combined   locality of the Web graph (the directed graph in which nodes
    with a content-driven Web page clustering tech-   are Web pages and edges are hyperlinks). Hypothetically, if
    nique, our heuristic search system signiﬁcantly im- page A hyperlinks page B, then the creator of page A inten-
    proves the clustering results.                    tionally raised the topic of page B in the context of page A
                                                      which indicates that pages A and B are semantically close.
                                                      Davison [2000] empirically justiﬁes this hypothesis. In an
1  Introduction                                       average case, if two static Web pages are located in a short
Clustering of Web search results has been in the focus of the proximity to each other in the Web graph, then they stand in a
information retrieval community since the early days of the (probably vague) semantic relation. For the two examples
Web [Hearst and Pedersen, 1996; Zamir and Etzioni, 1998]. presented above, the site savethejaguar.com hyper-
The reasons for clustering of search results are two-fold. The links wcs.org, the Wildlife Conservation Society, which re-
ﬁrst is that the IR research community has long recognized veals its topic; while the pages of Michel Decary´ the sci-
the validity of the cluster hypothesis [van Rijsbergen, 1971] entist and the chansonnier point to cogilex.com, Michel’s
in top-ranked documents, i.e. similar documents tend to be previous enterprise, which implies that two of the three indi-
relevant to the same requests. A second (related) reason is viduals are in fact the same person. Note that no language
that the ranked list is usually too large and contains many modeling method would resolve this dilemma because the
documents that are irrelevant to the particular meaning of the two pages are strictly different (they are even written in dif-
query the user had in mind. Thus, it would be beneﬁcial to ferent languages).

                                                IJCAI-07
                                                  2280  Link analysis has been successfully applied to various Web aged to meet within the given budget on resources are then
mining tasks. A related task is identiﬁcation of Web commu- placed in the same cluster. By this, we construct a set C of
nities (see, e.g., Gibson et al. 1998), which are deﬁned as k  n topological clusters (we consider only k largest clus-
heavily connected components of the Web graph. Research ters of those constructed). In parallel to that, we apply a tra-
methods for this task are primarily graph-theoretic (graph par- ditional topical clustering method that assigns each document
titioning, network ﬂow etc.). Unfortunately, these methods from the original ranked list into one cluster from a set C of
                                                       
are inapplicable to our task, because we cluster isolated, un- k >ktopical clusters. After that, for each cluster ci ∈ C we
                                                                                                    
connected Web pages retrieved by an arbitrary query. The ﬁnd its closest cluster cj from C : j =argmaxj |ci ∩ cj |.
                                                                                               
most relevant previous work is He et al. [2002], who build a Each cluster ci is then enriched with elements of cj that do not
Web page clustering system that exploits the hyperlink struc- appear in any cluster of C. By this, we construct larger clus-
ture of the Web: they consider two Web pages to be similar if ters ci that contain documents that are either topologically or
they are in parent/child or sibling relations in the Web graph. topically related. This technique shows excellent results both
We propose a more general framework that incorporates both in terms of precision and recall (see Section 4).
topical and topological closeness: Web pages belong to the Besides the clustering of Web search results, the proposed
same cluster if they are similar in content or close to each system can be applied to various information retrieval and
other in the Web graph.                               Web mining tasks, such as Web appearance disambiguation
  To approximate the distance between two pages in the Web [Bekkerman and McCallum, 2005], acronym disambigua-
graph, we apply the heuristic search paradigm [Pearl, 1984]. tion [Pakhomov et al., 2005], interactive information retrieval
To our knowledge, this paper is the ﬁrst work that applies [Leuski and Allan, 2004], Web search with Web pages pro-
heuristic search (speciﬁcally, beam search) in the domain of vided as queries [Dean and Henzinger, 1999],aswellasfor
the Web graph. Heuristics have been used for focused Web Homeland security analysis and other important problems.
crawling (e.g. [Davidov and Markovitch, 2002]), where the In this study, we test our system on two applications:
goal is to collect as much useful information as possible while search result clustering and Web appearance disambiguation.
crawling the Web, and the heuristics estimate the amount of In the latter, the goal is to identify Web appearances of par-
information available in a particular Web sub-graph. In con- ticular people with potentially ambiguous names. The prob-
trast, we use heuristics to estimate the utility of expanding the lem is solved given afewnames of people who are known to
current node in terms of leading to the target node.  belong to the same social network. Disambiguation of each
  Since the heuristic search can be computationally hard, we person’s name is allowed by the presence of other names that
perform bidirectional search: we start searching from both are likely to correlate with it. We represent Web appearance
source and target nodes and expand hyperlinked nodes un- disambiguation as a special case of the search result cluster-
til the two search frontiers meet at a common node or until ing task: given m queries of people names, we construct one
the resources are depleted. In this setting, the computational cluster of Web pages that mention the people of our interest,
complexity is no longer an issue: after only three search it- while disregarding pages that mention their unrelated name-
erations, we can construct paths of length up to 8,1 which sakes. We generalize our multi-agent heuristic search by con-
are long enough to potentially diminish any semantic relation structing m × n agents that search for each other in the Web.
between the starting nodes. Thus, since short searches are ac-
ceptable in our case and since the out-degree of Web pages 2 Multi-agent heuristic search
is on average just about 8 [Kleinberg et al., 1999],evenex-
haustive breadth-ﬁrst search methods are feasible. Moreover, We propose two multi-agent heuristic search algorithms for
modern search engines store the adjacency table of most of topologically clustering n pages (we call them source pages).
the Web, i.e. no Web crawling is required for the heuristic Algorithm 1 is called Sequential Heuristic Search (SHS). We
search. We use heuristics not to reduce the search time, but to start with n singleton clusters of the source pages. We create
improve the search accuracy. As we discuss below, the mod- a collection of n Web agents each of which is assigned one
ern Web is tightly interconnected, so heuristics are used as source page. Each agent maintains a search frontier: a list of
ﬁlters to prune branches of search trees that are likely to es- nodes (URLs) to be expanded (initially, the URL of its source
tablish undesired connections between unrelated Web pages. page). At any search iteration each agent obtains URLs hy-
  To distribute the heuristic search, we build a multi-agent perlinked from the nodes of its search frontier. It then ap-
system: given n Web pages in the ranked list, we construct plies heuristics to select potentially good URLs to become its
n collaborative Web agents each of which is assigned one new search frontier. After that, we intersect the sets of URLs
page of the initial dataset. Each agent then performs heuris- obtained by all agents. If a common URL is found for two
tic search to traverse the Web graph in order to meet as many source pages, we merge the clusters they belong to. The sys-
other agents as possible. If an agent reached a dead end and tem stops after a predeﬁned number of iterations (usually, a
cannot continue the search, it can move up in the hierarchy of small number of 2 or 3, as discussed in Section 1).
Web directories which would presumably lead to a more gen- The SHS algorithm, while being simple and intuitive, suf-
eral page that has more hyperlinks. Pages whose agents man- fers from one crucial drawback: there is no possibility to con-
                                                      trol the topology of the constructed clusters. In a worst case,
  1                                                       l                                         A
   Starting with nodes A0 and B0, after three search iterations the after search iterations, if a path is found between page and
following path of length 8 can be constructed: A0 → A1 → A2 → B, as well as between pages B and C, and between pages C
A3 → C ←  B3 ← B2 ← B1 ← B0.                          and D (while no other links are found), then pages A and D

                                                IJCAI-07
                                                  2281Input:                                                Input:
 S = {s1,s2,...,sn} – URLs of source pages             S = {s1,s2,...,sn} – URLs of source pages
 l – number of search iterations                       CC1,...,CCk – core clusters obtained at iteration 0 of SHS
Output:                                                l – number of search iterations
 Clusters C1,...,Ck                                   Output:
                                                       Enlarged core clusters CC1,...,CCk
 For each si ∈ S do
   Initialize agent ai’s search frontier F0(ai) ←{si}  For each si ∈ S do
   Initialize agent ai’s set of extracted URLs T0(ai) ←{si} Initialize agent ai’s frontier F1(ai) ← Extract URLs(si)
 For each j =0,...,ldo                                   Initialize ai’s set of extracted URLs T1(ai) ←{si}∪F1(ai)
   ———Distributed search phase:———                     For each j =1,...,ldo
   For each si ∈ S do                                    ———Distributed search phase:———
    Construct Fj+1(ai) ← Extract URLs(Fj (ai))           For each si ∈ S do
    Filter Fj+1(ai) using a set of heuristics              Construct Fj+1(ai) ← Extract URLs(Fj (ai))
    Update Tj+1(ai) ← Tj (ai) ∪ Fj+1(ai)                   Filter Fj+1(ai) using a set of heuristics
   ———Result collection phase:———                          Update Tj+1(ai) ← Tj (ai) ∪ Fj+1(ai)
   Construct all pairs (si,si ) s.t. Tj+1(ai) ∩ Tj+1(ai ) = ∅ ———Result collection phase:———
   Initialize singleton clusters Ci ←{si}                Construct all pairs (si,si ) s.t. (Tj+1(ai) ∩ Tj+1(ai ) = ∅)∧
                                                                             
   For each pair (si,si ) do                              (∃t : si ∈ CCt) ∧ (∀t : si ∈/ CCt )
    If (si ∈ Ct) ∧ (si ∈ Ct ) ∧ (Ct = Ct ) then      For each pair (si,si ) do
      Merge Ct and Ct                                     Add si to CCl
    Algorithm 1: Sequential Heuristic Search (SHS).       Algorithm 2: Incremental Heuristic Search (IHS).

will be placed in the same cluster despite that the semantic Our topology-driven heuristic is high-degree node elimina-
relation between them is probably weak, as their distance in tion (or, in short, high-degree heuristic): after each search it-
               2
the Web can be 6l, which is too long even if l =2. A method eration, from the search frontiers we remove high out-degree
for building tightly connected clusters should be proposed. and high in-degree URLs that often connect between seman-
  Solving the Web appearance disambiguation problem,  tically unrelated pages. For example, both macromedia.
Bekkerman and McCallum [2005] noticed that matching hy- com and historians.org  point to google.com,which
perlinks of the source pages leads to a small but clean cluster does not imply that there is a tight semantic relation between
of relevant pages (called the core cluster). We adopt this idea Macromedia Inc. and the American Historical Association.
and propose another multi-agent heuristic search algorithm, For the graphical interpretation of the high-degree heuristic,
called Incremental Heuristic Search (IHS)—see Algorithm 2. see Figure 1(a). To detect high out-degree URLs, we simply
In IHS, we start with a set of core clusters generated at itera- count the number of hyperlinks at each page. To detect high
            3
tion 0 of SHS. The distributed search phase of IHS is exactly in-degree URLs, we use Google’s link: operator.
the same as of SHS, but at the result-collection phase we now A successful content-driven heuristic is the person name
select only pairs where one member belongs to a core cluster heuristic. Figure 1(b) illustrates the idea. An agent has a
while the other does not, so we add it to the corresponding good chance to meet another agent, if it expands a page that
core cluster. We ignore pairs in which both members belong shares a person name with a page expanded by another agent.
to different core clusters. Proceeding incrementally, we keep To extract person names from expanded Web pages, we ﬁrst
track of the diameter of each constructed cluster, which is remove markup, and then apply NER (Wei Li’s named entity
now independent of the cluster’s size.                tagger, see McCallum and Li, 2003). We extract only entities
2.1  Useful heuristics                                tagged as PERSON and consider people names that consist of
                                                      two, three or four words. We exclude people names that are
Two types of heuristics can be proposed in the Web domain: too common (again, we use Google’s link: operator).4
topology-driven and content-driven. Topology-driven heuris- In analogy to the person name heuristic, we also propose
tics are based on the layout of the Web graph, while content- the anchor text heuristic that matches snippets of anchor text
driven heuristics are based on features extracted from the in- extracted from the Web pages. We ignore too common an-
terior of Web pages. In this section we propose one topology- chor texts, such as contact us or copyright.Eiron
driven and two content-driven heuristics, all of which are and McCurley [2003] perform a comprehensive analysis of
fairly straightforward, but still prove to be effective when anchor texts and show that they usually summarize the con-
used in our framework of heuristic search in the Web graph. tent of the hyperlinked Web pages. Such summarization can
In our future work, we will explore other heuristics as well. be very useful in our case, when we attempt to predict a possi-
  2Since our search is bidirectional, after l iterations a hyperlink ble beneﬁt of expanding pages from the search frontier. Note
path of length up to 2l can be constructed.           that in contrast to people names, anchor snippets can be easily
  3We do not attempt to solve the fundamental problem of inferring identiﬁed by shallow parsing of the pages’ markup language.
the correct number of clusters. Instead, we preset this number for
each particular task: for Web appearance disambiguation, only one 4In many cases, an entity tagged as a person name has millions
core cluster is needed, while for clustering Web search results, the of Google’s hits if it is a tagger error. Examples of such entities are
number of clusters equals the number of main meanings of the query. Price Range and Mac Os.

                                                IJCAI-07
                                                  2282                                                        Category    # of pages Category        # of pages
                                   share name
       1          1
                                 1                      Car            36      Cornell project    2
             2
                                           1            Mac OS         11      Metal Band         1
       1          1                                     Wild cat       23      Movie              1
                                      2
                                                        Biotech ﬁrm     2      Photo gallery      1
                                                        Youth org       1      Atari game         5
Figure 1: An illustration of applying heuristics, at the ﬁrst Maya culture 1   Guitar             1
search iteration. Black nodes are the source pages. (left) Resin models 1      TV channel         1
High-degree heuristic. Gray nodes are high in- or out-degree Web hosting 1     Web designer       2
pages, eliminated from search frontiers. (right) Person name Reef lodge 2      E-commerce ﬁrm     1
heuristic. A hyperlink path between two nodes is constructed Book       1      Game archieve      1
over which a person name is also shared.                Singer          2      Aircraft           1
                                                        Emulator        2
3  Datasets                                                   Table 1: Statistics on the Jaguar dataset.
We use two datasets for evaluation of our methods: one for
Web appearance disambiguation and another for clustering 56,287 pages on the third hop. At each iteration the dataset
Web search results.                                   grew on average by a factor of 8, which corresponds surpris-
                                                      ingly well to the growth of the Web appearance disambigua-
3.1  Web appearance disambiguation dataset            tion dataset and to the ﬁndings of Kleinberg et al. [1999].
We downloaded Bekkerman’s Web appearance disambigua-
tion dataset from www.cs.umass.edu/∼ronb. It con-     4   Results and discussion
sists of 1085 Web pages retrieved on 12 names of people
from Melinda Gervasio’s social network (mostly, SRI engi- 4.1 Web appearance disambiguation
neers and university professors). The dataset is labeled ac- First, we apply both sequential and incremental search algo-
cording to the person’s occupation. Two of the 12 people rithms on the Web appearance disambiguation data in an ex-
appear to be unique in the Web, while the rest have relatively haustive manner, i.e. without applying heuristics. Surpris-
common names. Some of the names are extremely ambigu- ingly, we discover that the dataset is heavily interconnected.
ous, e.g. given a query ‘‘Tom Mitchell’’, 37 different After each iteration of the sequential search, the connected
Tom Mitchells are found within the ﬁrst 100 Google hits. The pages compose one large cluster of size 208, 543, 728, and
dataset contains pages of 187 unique people overall, while 786 (72.5% of the entire dataset) respectively. Some con-
only 12 of them are relevant (mentioned at 420 pages). For nections are extremely weak: 10% of 66,561 hyperlink paths
the statistics on the dataset as well as for the preprocessing found at the last iteration go through www.adobe.com/
procedure, see Bekkerman and McCallum [2005].         products/acrobat/readstep2.html, a page with
  We crawled the Web starting with these 1085 pages (source over 600,000 Google hits on it.
pages). We retrieved all available pages hyperlinked from the On this data, we report on precision, recall and F-measure
source pages, as well as the pages located one level above the of constructing one cluster of documents that mention rele-
source pages in the hierarchy of Web directories. We con- vant people. Precision/recall curves in Figure 2 show that
tinued this process until all the pages within three hops of the exhaustive sequential and incremental algorithms do quite
the original dataset were retrieved. In order not to produce poorly on this data, with slight advantage to the incremental
a priori weak connections and to still preserve a reasonable approach. After four iterations, we end up with above 80%
size for our dataset, we did not retrieve pages located at ex- recall, but the precision is very low (under 50%). However,
tremely popular domains, such as amazon.com.Wealso    the performance is improved when we apply the high-degree
ignored pages of non-textual format. At each crawling it- heuristic. We set the threshold of in/out hyperlinks at 1000—
eration our dataset grew almost an order of magnitude: we all pages with more than 1000 Google hits and pages contain-
downloaded 7009 pages at the ﬁrst hop, 69,454 pages at the ing more than 1000 hyperlinks are ﬁltered out. We also tried
second hop and 592,299 pages at the third hop, resulting in other thresholds, such as 100 and 10,000, without any signiﬁ-
669,847 unique Web pages overall.                     cant change in the performance. Note that only short paths are
                                                      effective: the precision drops at the second and third hops of
3.2  Jaguar dataset                                   the source pages. The reason for such a drop is that the high-
We built a new dataset for the problem of clustering Web degree heuristic is topology-driven—it ignores the content of
search results. We retrieved and labeled 100 ﬁrst Google hits the pages, which introduces a lot of noise while moving far
obtained on the query jaguar. We found 23 different cat- away from the source pages.
egories within the 100 retrieved pages: the largest ones are The person name heuristic turned out to be more effec-
obviously the car, the wild cat and the Mac operating system tive. We notice that since we perform short searches (up to
(version 10.2). Table 1 presents statistics on this dataset. three hops from the source pages), there is no need in nar-
  Exactly as for the Web appearance disambiguation dataset, rowing the search beam with the heuristic. Moreover, such
we crawled three hops off the Jaguar source pages, retrieving narrowing may hurt the recall of our system. Instead, we
883 pages on the ﬁrst hop, 8548 pages on the second hop and apply the heuristic as a ﬁltering method: at each iteration

                                                IJCAI-07
                                                  2283              Web appearance disambiguation dataset       Method           Precision Recall  F-measure
           1
                                                                   Web appearance disambiguation
          0.9                                             Topical (A/CDC)    87.3%   71.3%    78.4%
                                                          IHS (iteration 1)  89.9%   57.1%    69.9%
          0.8                                             Hybrid (iteration 1) 84.5% 83.3%    83.9%

          0.7                                             IHS (iteration 2)  81.7%   66.0%    73.0%
                                                          Hybrid (iteration 2) 78.5% 86.2%    82.2%
         precision
          0.6
               SHS (no heur)                                       Clustering of Web search results
               IHS (no heur)                              Topical (A/CDC)    75.0%   64.3%    69.2%
          0.5
               IHS+h/d                                    IHS (iteration 1)  93.3%   40.0%    56.0%
               IHS+names
          0.4                                             Hybrid (iteration 1) 77.1% 77.1%    77.1%
           0.4   0.5   0.6   0.7   0.8
                        recall                            IHS (iteration 2)  78.6%   47.1%    58.9%
                                                          Hybrid (iteration 2) 72.7% 80.0%    76.2%
Figure 2: Precision/Recall curves for four algorithms on the
Web appearance disambiguation dataset. h/d means high- Table 2: Results of topical clustering (A/CDC), topological
degree heuristic, names means person name heuristic. Four clustering (IHS) and their hybrid, on two datasets. The IHS
nodes in each curve correspond to search iterations 0, 1, 2 clustering (and the hybrid) results are obtained after the ﬁrst
and 3. At iteration 0 (only original nodes expanded) the core and second iterations of heuristic search (hyperlink paths of
cluster is built (for the IHS algorithm).             lengthupto4andupto6respectively).

j, we ﬁrst use an incremental exhaustive search in order to 4.2 Clustering of Web search results
ﬁnd pages that are linked with the core cluster, and then we
                                                      In contrast to Web appearance disambiguation, the problem
apply our Information Extraction module that extracts peo-
                                                      of clustering Web search results is not a one-class problem.
ple names from pages expanded during the search. For each
                                                      We evaluate our system on k largest classes of the data. For
source page si we build two sets: Tj(si) of all URLs found
                                                      our Jaguar dataset we chose k =3, so we build three clusters
during the search and Nj(si) of all people names extracted
                                                      (of cars, Mac OS, and wild cats). Let CCi be one of these
from the search tree. For the core cluster CC we construct
                                                      clusters and Cli be its corresponding class. Let Corri be a
Tj(CC)=    s ∈CC Tj(si) and Nj(CC)=   s ∈CC Nj(si).
            i                          i              set of pages from Cli that have been correctly assigned into
We put page si into CC if there is a hyperlink path from
                                                      CCi  by our system. Then the micro-averaged precision and
si to CC and a common person name is found: (Tj(si) ∩
                                                      recall of the system are:
Tj(CC) = ∅) ∧ (Nj(si) ∩ Nj(CC) = ∅). Note that the only
difference from Algorithm 2 is that the common person name         k                  k
                                                                     i=1 |Corri|         i=1 |Corri|
                                             si            Prec                  Rec               .
may not be on the constructed hyperlink path between and        =  k          ;     =  k
CC. This method shows good results on our data (see Fig-              i=1 |CCi|           i=1 |Cli|
ure 2). The best F-measure (73%) is achieved at the second
iteration, while at the third one the precision drops by almost On the Jaguar dataset, the sequential exhaustive search
20%, which implies that two iterations are enough. We also fails: after three iterations, 70 of the 100 pages are all con-
tried to apply the high-degree and the person name heuristics nected together. However, the incremental algorithm shows
together, but did not see any improvement in precision, while better results (see Figure 3): at the ﬁrst iteration it obtains
hurting recall.                                       82.4% precision but then the precision drops. When apply-
  To compare our results with the ones reported by Bekker- ing the high-degree heuristic (with the threshold at 10000 hy-
man and McCallum  [2005], we use their topical clustering perlinks), the result is even better, especially after the ﬁrst
method called Agglomerative/Conglomerative Distributional iteration (93.3% precision). We use the three clusters con-
Clustering (A/CDC), which is a state-of-the-art information- structed at this iteration as the core clusters (instead of using
theoretic technique. The results are shown in Table 2 (A/CDC the core clusters constructed at the previous iteration—this
result is by Bekkerman and McCallum, 2005). We see that design choice leads to a higher recall), and add the anchor
after the ﬁrst iteration the heuristic search method is compet- text heuristic (which improves the precision). The resulting
itive with A/CDC in precision, but is inferior in recall. How- system demonstrates good performance, while the F-measure
ever, when combining the two methods, we obtain excellent is consistently improved from 56% to 59% and then to 62%
results in terms of both precision and recall. After the sec- at the third hop from the source pages. This is the only result
ond search iteration the precision trades off against the recall we could obtain that shows usefulness of expanding pages at
(more noise is added) and the F-measure slightly decreases. the third hop.
  Heuristic search allows addition of 49 previously undis- When comparing the heuristic search method with topi-
covered documents to the topical cluster, 32 of which re- cal clustering, we observe exactly the same trend as for the
fer to Adam Cheyer and Steve Hardt. Bekkerman and Mc- Web appearance disambiguation task (see Table 2). The best
Callum [2005] notice that these two researchers work in in- performance (77.1% F-measure) is obtained by the combina-
dustry so their pages use different vocabulary than most of tion of the two methods after the ﬁrst heuristic search itera-
other academic-style pages in the cluster. Our heuristic search tion, which is a strong result for an unsupervised method on
method is especially designed to overcome this problem. a multi-class task.

                                                IJCAI-07
                                                  2284