                   Coalitional Bargaining with Agent Type Uncertainty

                             Georgios Chalkiadakis     and Craig Boutilier
                                    Department of Computer Science
                                 University of Toronto, Toronto, Canada
                                    { gehalk, cebly }@cs.toronto.edu


                    Abstract                          in the coalitional bargaining game. Preliminary experiments
                                                      illustrate the performance of this heuristic approach.
    Coalition formation is a problem of great interest in Although there is a considerable body of work on coali-
    AI, allowing groups of autonomous, individually ratio-
    nal agents to form stable teams. Automating the nego- tional bargaining, no existing models deal with explicit type
    tiations underlying coalition formation is, naturally, of uncertainty. Okada [7] suggests a form of coalitional bar-
    special concern. However, research to date in both AI gaining where agreement can be reached in one bargaining
    and economics has largely ignored the potential presence round if the proposer is chosen randomly. Chatterjee et al.
    of uncertainty in coalitional bargaining. We present a [3] present a bargaining model with a ﬁxed proposer or-
    model of discounted coalitional bargaining where agents der, which results in a delay of agreement. Neither model
    are uncertain about the types (or capabilities) of potential deals with type uncertainty—instead, they focus on calculat-
    partners, and hence the value of a coalition. We cast the ing subgame-perfect equilibria (SPE). Suijs et al. [9] intro-
    problem as a Bayesian game in extensive form, and de- duce stochastic cooperative games (SCGs), comprising a set
    scribe its Perfect Bayesian Equilibria as the solutions to of agents, a set of coalitional actions, and a function assign-
    a polynomial program. We then present a heuristic algo-
    rithm using iterative coalition formation to approximate ing to each action a random variable with ﬁnite expectation,
    the optimal solution, and evaluate its performance. representing action-dependent coalition payoff. Though they
                                                      provide strong theoretical foundations for games with this
                                                      restricted form of action uncertainty, they do not model ex-
1  Introduction                                       plicitly a coalition formation process. Kraus et al. [4] model
Coalition formation, widely studied in game theory and eco- coalition formation under a restricted form of uncertainty re-
nomics [8], has attracted much attention in AI as means of garding coalitional values in a request for proposal domain.
dynamically forming partnerships or teams of cooperating However, type uncertainty is not captured; rather, the mean
agents. While most models of coalition formation (e.g., coali- value of coalitions is common knowledge, and a “manager”
tional bargaining processes) assume that agents have full handles proposals (they also focus on social welfare maxi-
knowledge of types of their potential partners, in most nat- mization rather than individual rationality).
                                                                               [ ]
ural settings this will not be the case. Generally, agents will Chalkiadakis and Boutilier 2 propose an explicit model of
be uncertain about various characteristics of others (e.g., their type uncertainty and show how this translates into coalitional
capabilities), which in turn imposes uncertainty on the value value uncertainty. We adopt their model in our paper. How-
of any coalition. This presents the opportunity to learn about ever, their results focus on stability concepts and how coali-
the types of others based on their behavior during negotiation tions evolve during repeated interaction, as agents gradually
and by observing their performance in settings where coali- learn more about each other’s capabilities (in reinforcement
tions form repeatedly. Agents must be able to form coalitions learning style). The actual coalition formation processes used
and divide the generated value even in such settings. are fairly simple and are not inﬂuenced by strategic consider-
  Here we present a model of discounted coalitional bar- ations, nor do agents update their beliefs about other agents’
gaining under agent type uncertainty. We formulate this as a types during bargaining. Our work analyzes the actual bar-
Bayesian extensive game with observable actions [8],where gaining process in more depth.
the actions correspond to proposing choices of potential part-
ners and a payoff allocation, or accepting or rejecting such 2 Bayesian Coalitional Bargaining
proposals. Our model generalizes related bargaining models We begin by describing the Bayesian coalition formation
by explicitly dealing with uncertainty about agent types (or model and then deﬁne our coalitional bargaining game.
capabilities) and coalitional values. We formulate the perfect We assume a set of agents N = {1,...,n}, and for each
Bayesian equilibrium (PBE) solution of this game as a de- agent i a ﬁnite set of possible types Ti. Each agent i has a
cidable polynomial program. The complexity of the program speciﬁc type t ∈ Ti.WeletT = ×i∈N Ti denote the set of
makes it intractable for all but trivial problems, so we propose type proﬁles. Each i knows its own type ti, but not those of
an alternative heuristic algorithm to ﬁnd good agent strategies other agents. Agent i’s beliefs μi comprise a joint distribution

                                                IJCAI-07
                                                  1227over T−i,whereμi(t−i) is the probability i assigns to other adopt a suitable behavioral strategy, associating with each
agents having type proﬁle t−i. Intuitively, i’s type reﬂects its node in the game tree at which it must make a decision a dis-
“abilities;” and its beliefs about the types of others capture its tribution over action choices for each of its possible types.
uncertainty about their abilities. For instance, if a carpenter Furthermore, since it is uncertain about the types of other
wants to ﬁnd a plumber and electrician with whom to build agents, its observed history of other agents’ proposals and re-
a house, her decision to propose (or join) such a partnership, sponses give it information about their types (assuming they
to engage in a speciﬁc type of project, and to accept a spe- are rational). Thus, the preferred solution concept is that of
ciﬁc share of the surplus generated should all depend on her a perfect Bayesian equilibrium (PBE) [8]. A PBE comprises
probabilistic assessment of their abilities.          a proﬁle of behavioral strategies for each agent as well a sys-
  A coalition C ⊆ N of members with actual types tC has a tem of beliefs dictating what each agent believes about the
value V (tC ), representing the value this group can achieve by types of its counterparts at each node in the game tree. The
acting optimally. However, this simple characteristic func- standard rationality requirements must also hold: the strategy
tion representation of the model [8] is insufﬁcient, since this for each agent maximizes its expected utility given its beliefs;
value is not common knowledge. An agent i can only assess and each agent’s beliefs are updated from stage to stage using
the expected value of such a coalition based on its beliefs: Bayes rule, given the speciﬁc strategies being played. In this
Vi C           μi tC V  tC
  ( )=    tC ∈TC (  )  (  ).                          section, we formulate the constraints that must hold on both
  A coalition structure CS partitions N into coalitions of strategies and beliefs in order to form a PBE.
agents. A payoff allocation P = xi, given the stochas- Let σi denote a behavioral strategy for i, mapping informa-
tic nature of payoffs in this setting, assigns to each agent in tion sets (or observable histories h) in the game tree at which
coalition Cits share of the value attained by C (and must be i must act into distributions over admissible actions A(h).If
such that i∈C xi =1for each  C  ∈ CS). Chalkiadakis   i is a proposer at h (at stage s), let A(h)=P, the ﬁnite set of
                                                                                  h,ti
and Boutilier [2] deﬁne the Bayesian core as a generalization proposals available at h.Thenσi (π) denotes the (behav-
of the standard core concept, capturing an intuitive notion of ioral strategy) probability that i makes proposal π ∈Pat h
stability in the Bayesian coalition formation game.                                              h,ti
                                                      given its type is ti.Ifi is a responder at h,thenσi (y) is
  While coalition structures and allocations can sometimes the probability with which i accepts the proposal on the table
be computed centrally, in many situations they emerge as the            h,ti         h,ti
                                                      (says yes)ath (and σ (n)=1−   σ   (y) is the probability
result of some bargaining process among the agents, who                 i            i
                                                      i            μ       i            μh,ti t        i
propose, accept and reject partnership agreements [3].We says no). Let i denote ’s beliefs with i ( −i) being ’s
                                                                                   h                  t
now deﬁne a (Bayesian) coalitional bargaining game for the beliefs about the types of others at given its own type is i.
model above as a Bayesian extensive game with observable We deﬁne the PBE constraints for the game by ﬁrst deﬁn-
                                                                                   i
actions. The game proceeds in stages, with a randomly cho- ing the values to (generic) agent at each node and infor-
sen agent proposing a coalition and allocation of payments to mation set in the game tree, given a ﬁxed strategy for other
partners, who then accept or reject the proposal.     agents, and the rationality constraints on his strategies and
  A ﬁnite set of bargaining actions is available to the agents. beliefs. We proceed in stages.
                                                               ξ                    i         h        s
A bargaining action corresponds to either some proposal (1) Let be a proposal node for at history at stage .
                                                      Since the only uncertainty in information set h involves the
π =  C, PC  to form a coalition C with a speciﬁc payoff
                                                      types of other agents, each ξ ∈ h corresponds to one such
allocation PC specifying payoff shares xi to each i ∈ C,or
                                                      type vector t−i ∈ T−i;leth(t−i) denote this node in h.The
to the acceptance or rejection of such a proposal. The ﬁnite- value to i of a proposal π = C, PC  at h(t−i) is:
                     S
horizon game proceeds in stages, and initially all agents are                        X
                                                       h(t ),t      h(t )
active. At the beginning of stage s ≤ S, one of the (say n) q −i i (π)=p −i (π)x V (t )+ ph(t−i)(π,r)qξ/π/r,ti
                                                 1     i            acc     i s C                  i
active agents i is chosen randomly with probability γ = n                             r
to make a proposal C, PC  (with i ∈ C). Each other j ∈ C
                                                               h(t−i)
simultaneously (without knowledge of other responses) either where: pacc (π) is the probability that all j ∈ C (other
accepts or rejects this proposal. If all j ∈ C accept, the than i) accept π (this is easily deﬁned in terms of their ﬁxed
agents in C are made inactive and removed from the game. strategies); xi is i’s payoff share in PC ; r ranges over re-
               s−1
Value Vs(tC )=δ   V (tC ) is realized by C at s, and split sponse vectors in which at least one j ∈ C refuses the
                                                                h(t )
according to PC ,whereδ ∈ (0, 1) is the discount factor.1 proposal; p −i (π, r) denotes the probability of such a re-
If any j ∈ C rejects the proposal, the agents remain active      ξ/π/r,ti
                                                      sponse; and qi    denotes the continuation payoff for i at
(no coalition is formed). At the end of a stage, the responses s          ξ/π/r          n              π
                                            S     i   stage  +1at the node      (following after proposal
are observed by all participants. At the end of stage ,any and responses r). This continuation payoff is deﬁned (recur-
not in any coalition receives its discounted reservation value
 S−1                                                  sively) below. The value of π at history h (as opposed to a
δ   V (ti) (discounted singleton coalition value).
                                                      node) is determined by taking the expectation w.r.t. possible
                                                             h,ti           h,ti    h(t−i),ti
                                                      types: q  (π)=      μ   (t−i)q      (π).
3  Perfect Bayesian Equilibrium                              i         t−i  i       i
                                                        (2) Suppose i is a responder at node ξ = h(t−i) in his-
The coalitional bargaining game described above is clearly an
                                                      tory h at stage s. As above, ξ corresponds to speciﬁc t−i in
extensive form Bayesian game. We assume each agent will h. W.l.o.g. we can assume i is the ﬁrst responder (since all
                                                                                   h(t−i)
  1Agents could have different δ’s. As long as these are common responses are simultaneous). Let pacc (π) denote the prob-
knowledge, our analysis holds with only trivial modiﬁcations. ability that all other responders accept π. We then deﬁne the

                                                IJCAI-07
                                                  1228value to i of accepting π at ξ as:                    Finally, we add the obvious constraints specifying the domain
                               
 h(t−i),ti                                    ξ/y/r,t of the various variables denoting strategies or beliefs (they
q       y    ph(t−i) π x V t      ph(t−i) π, r q   i
 i     ( )=   acc  ( ) i s( C )+        (   ) i       take values in [0, 1] and sum up to 1 as appropriate).
                                r                       This ends the formulation of the program describing the
where again r ranges over response vectors in which at least PBE. This is a polynomial constraint satisfaction problem:
                          h(t )
one j ∈ C, j = i, refuses π; p −i (π, r) is the probability ﬁnding a solution to this system of constraints is equivalent
                      ξ/y/r,ti
of such a response; and qi   is the continuation payoff to deciding whether a system of polynomial equations and
for i at stage s +1after responses r by its counterparts. The inequalities has a solution [1]. The problem is decidable,
value of accepting at h is given by the expectation over type but is intractable. For example, an algorithm for deciding
                        h,ti
vectors tC w.r.t. i’s beliefs μ as above.             this problem has been proposed with exponential complex-
                        i                                [ ]
  The value of rejecting π at ξ = h(t−i) is the expected ity 1 . Speciﬁcally, the complexity of deciding whether a
                                                               s                               d   k
continuation payoff at stage s +1:                    system of polynomials, each of degree at most in vari-
                                                                         sk+1dO(k)
         h(t ),t                    ξ/n/r,t           ables has a solution is      . In our case, assuming a
        q  −i  i n      ph(t−i) π, r q   i
         i     (  )=          (   ) i                 random choice of proposer at each of S rounds, we can show
                      r                               that if α is the number of pure strategies, N the number of
(where r ranges over all responses, including pure positive agents, T the number of types, then s = O(N S), d = NS
responses, of the others).                            and k = O(αNT  ). This is due to a variety of combinatorial
  (3) We have deﬁned the value for i taking a speciﬁc ac- interactions evident in the constraints above, creating as they
tion at any of its information sets. It is now straightforward do interdependencies between belief and strategy variables.
to deﬁne the value to i of reaching any other stage s node In summary, the formulation above characterizes the PBE
controlled by j = i or by nature (i.e., chance nodes where a solution of our coalitional bargaining game as a solution of a
random proposer is chosen).                           polynomial program. However, it does not seem possible that
  First we note that, by assuming i responds “ﬁrst” to any this solution can be efﬁciently computed in general. Never-
proposal, our deﬁnition above means that we need not com- theless, this PBE formulation may prove useful for the com-
pute the value to i at any response node (or information set) putation of a PBE in a bargaining setting with a limited num-
controlled by j. For an information set hj where j makes a ber of agents, types, proposals and bargaining stages.
proposal, consider a node ξ = hj(tj) where j is assumed to
                             hj ,tj
be of type tj. Then, j’s strategy σj speciﬁes a distribu- 4 Approximations
                                            hj ,tj    The calculation of the PBE solution is extremely complex due
tion over proposals π (determined given the values qj (π)
which can be calculated as above, and j’s type tj). Agent i’s to both the size of the strategy space (as a function of the size
      t ,h (t )                                       of the game tree, which grows exponentially with the problem
     q i j j
value i      at this node is given by the expectation (w.r.t. horizon), and the dependence between variables representing
this strategy distribution) of its accept or reject values (or if it strategies and beliefs, as explained above. We present an ap-
is not involved in a proposal, its expected continuation value proximation strategy that circumvents these issues to some
      s                                       h
at stage +1given the responses of others). Its value at j is degree by: (a) performing only a small lookahead in the game
      ti           hj ,ti  ti,hj (tj )        ti
    Q   hj        μ     tj q                Q   hi
then  i ( )=    tj i   ( ) i     .Wedeﬁne     i ( )   tree in order to decide on a action at any stage of the game;
(where i is the proposer) as in Case 1 above.         and (b) ﬁxing the beliefs of each agent during this process.
  Finally, i’s value at information set h that deﬁnes the stage This latter approach, in particular, allows us to solve the game
s continuation game (i.e., where nature chooses proposer) is tree by backward induction, essentially computing an equilib-
                                                     rium for this ﬁxed-beliefs game. Note that while beliefs are
                h,t   1       t
               q  i         Q  i h
                i   = m       i ( j)                  held ﬁxed during the lookahead (while computing an imme-
                        j≤m                           diate action), they do get updated once the action is selected
                                                      and executed, and thus do evolve based on the actions of oth-
where m is the number of active agents, and hj is the infor-
mation set following h in which j is the proposer.    ers (this is in the spirit of receding horizon control). Further-
  (4) We are now able to deﬁne the rationality constraints. more, we allow sampling of type vectors in the computation
We require that the payoff from the equilibrium behav- to further reduce the tree size.
ioral strategy σ exceeds the payoffs of using pure strategies. More precisely, at any stage of the game, with a particular
Speciﬁcally, in PBE, for all i, ti ∈ Ti,allh that correspond collection of active agents (each with their own beliefs), we
to one of i’s information sets, and all actions b ∈ A(h),we implement the following steps:
have:X    X                     X                       1. An agent (e.g., proposer) constructs a game tree consisting of
                     h(t ),t              h(t ),t                                                       2
    h          h,ti    −i i         h        −i i         the next d rounds of bargaining (for some small lookahead d).
   μi (t−i)   σi (a)qi     (a) ≥   μi (t−i)qi    (b)
                                                          All active agents are assumed to have ﬁxed beliefs at each node
t−i      a∈A(h)                 t−i
                                                          in this tree corresponding to their beliefs at the current stage.
  We also add constraints for the Bayesian update of belief The agent computes its optimal action for the current round us-
                                 κ                        ing backward induction to approximate an equilibrium (similar
variables for any agent i regarding type tj of agent j perform-
                                                          in nature to an SPE) of this limited depth game. (We elaborate
ing aj at any h (for all i, ti ∈ Ti,allh and all aj):
                                                          below.) Furthermore, they sample partners’ types when calcu-
                      h,tκ                h,tk
 h∪aj ,ti κ   h,ti κ    j           h,ti k    j           lating the values of coalitions and proposals.
μi     (tj )=μi  (tj )σj (aj)/     μi  (tj )σj (aj)
                               k                         2        d
                              tj ∈Tj                     If less than rounds remain, the tree is suitably truncated.

                                                IJCAI-07
                                                  1229 2. Each player executes its action computed for the current round However, to evaluate this acceptance condition, i would need
    of bargaining. If a coalition is formed, it breaks away, leaving to know the other responders’ strategies (which in turn de-
    the remaining players as active.                  pend on i’s strategy). Therefore, i will make the simplifying
                                                                                    j
 3. All active agents update their beliefs, given the observed ac- assumption that all other responders evaluate their response
                                                      to π by assuming that the rest of the agents (including i) will
    tions of others in the current round, using Bayesian updating.             j     t ∈ t              i
    Further, each agent keeps track of the belief updates that any accept the proposal. Thus, any with j −i is assumed by
    other agent of a speciﬁc type would perform at this point. to accept if he evaluates his expected payoff from acceptance
                                                      as being greater than his (discounted) reservation payoff:
 4. The next bargaining round is implemented by repeating these    X
    steps until a complete coalition structure is determined or the xj μj (t−j )Vd({tj ,t−j }) ≥ Vd(tj ) (2)
    maximum number of bargaining rounds is reached.               t−j ∈tC
  We stress that the algorithm above does not approximate                   i
the PBE solution; getting good bounds for a true PBE approx- With this assumption, is able to evaluate the accep-
imation would only be likely by assuming belief updating at tance condition in Eq. 1 above, and so calculate a speciﬁc
                                                       h(t−i),ti
every node of the game tree mentioned in Step 1. However, qi  (y) value. Note that the use of this assumption can
if our algorithmic assumptions are shared by all agents, each sometimes lead to an overestimate of the value of a node.
can determine their best responses to others’ (approximately) At node ξ = h(t−i), i can also evaluate his refusal value
optimal play, and thus their play approximates an equilibrium
                                                         qh(t−i),ti n  V  t
of the ﬁxed-beliefs game. Indeed, we can deﬁne a sequential as i ( )=   d( i) in this last round. Then, respon-
equilibrium under ﬁxed beliefs (SEFB) as an extension of the der i’s actual strategy at h can be evaluated as the strategy
                                                                                      h,ti
SPE and a restriction of the PBE for a ﬁxed-beliefs bargaining maximizing i’s expected value given μi :
game, and can show the following (stated informally here):                   
                                                           h,ti                   h,ti     h(t−i),ti
                                                          σi   =arg max   {      μi  (t−i)qi     (r)}
                                                                    r∈{y,n}
Theorem 1 If the Bayesian core (BC) of a Bayesian coalitional              t−i∈tC
game G [2] is non-empty, and so is the BC of each one of G’s
subgames, then—regardless of nature’s choice of proposers —there If i is a proposer of type ti deliberating at ξ = h(t−i),the
is an SEFB strategy proﬁle of the corresponding ﬁxed-beliefs dis- value of making proposal π is:
                                                                          j
counted Bayesian coalitional bargaining game that produces a BC                        h,t
                                            3                 h(t ),t                    j
element; and conversely, if there is an order independent SEFB  −i  i       xiVd(tC ) if σj = y, ∀j
                                                             qi     (π)=                               (3)
proﬁle for a Bayesian coalitional bargaining game, then it leads to         Vd(ti)  otherwise
a conﬁguration that is in the BC of the underlying G.
                                                       (i.e., i will get his reservation value unless all the respon-
This result describes some notion of equivalence between co- ders of the speciﬁc type conﬁguration agree to this proposal).
                                                                                   h,ti
operative and non-cooperative Bayesian coalition formation Furthermore, i’s expected value qi (π) from making pro-
                                                                                                     h,t
solution concepts, and is similar to results (e.g., Moldovanu posal π to coalition C at h can be determined given μ i .
    [ ]                                                                                              i
et al. 5 ) for non-stochastic environments. It also motivates Thus, the best proposal that i of type ti can make to coali-
further Step 1 of our heuristic algorithm, equating ﬁxed be-                                      C;h,ti
                                                      tion C is the one with maximum expected payoff: σ =
lief equilibrium computation with determination of (i’s part                                      i
                                                               qh,ti π                  qC;h,ti
of) the Bayesian core. We now elaborate on this process. arg maxπ i ( ) with expected payoff i .
                                                                 i                                   h
  We assume that the agents proceed to negotiations that However,  can also propose to other coalitions at as
                                                                                C∗         i
will last d rounds (corresponding to the algorithm’s looka- well. Therefore, the coalition to which should propose
head value d) under the assumption that all beliefs will remain is the one that guarantees him the maximum expected pay-
                                                            ∗              C;h,ti     ∗
ﬁxed to their present values throughout the (Step 1) process. off: C =argmaxC {qi }.IfP is the payoff alloca-
We will present the deliberations of agent i during negotia- tion associated with that proposal, then the optimal coalition-
tions. For ﬁxed types t−i of possible partners, drawn accord- allocation pair that ti can propose in this subgame (that starts
     μ  i                                                                    ∗;h,ti   ∗   ∗
ing to i, will reason about the game tree and assume ﬁxed with i proposing at h)is:σi = {C ,P } with maximum
                                                                      ∗
beliefs of other agents. (Agents will track of the updates of        C ;h,ti
                                                      expected payoff qi  . Finally, if there exist more than one
other agents’ beliefs after this stage of bargaining; see Step 2        i i
             i                                   t    optimal proposal for , randomly selects any of them (this is
above). Then, can calculate the optimal action of any j taken into account in agents’ deliberations accordingly).
agent (including himself) at any information set by taking ex- Of course, when the subgame starts an agent i does not
pectations over the corresponding tree nodes.                                                        i
                                  d                   know who the proposer in this subgame will be; and has
  We begin our analysis at the last stage of negotiations. In only probabilistic beliefs about the types of his potential part-
any node ξ after history h where i of type ti is a responder                                       d:ξ,ti
                                                      ners. Thus, i has to calculate his continuation payoff q at
to proposal π ∈Pand assumes a speciﬁc type vector for                                              i
                                                      stage d (that starts at node ξ) with m participants, in the way
partners, he expects a value for accepting that is different to
                                                      explained in the previous section. This is straightforward, as
his (discounted) reservation value only if all other responders
                                                      i can calculate his expected payoffs from participating in any
accept the proposal as well:
                                                      subgame where some j proposes, given that any i can calcu-
                 j                                    late the optimal strategies (and associated payoffs) for any j
      h(t−i),ti     xiVd(tC ) if all t−i ∈ tC accept
     qi     (y)=                                (1)              d
                    Vd(ti)  otherwise                 in this round subgame.
                                                        Now consider play in a subgame starting in period d − 1,
  3Astrategyproﬁleisorder independent iff when played it leads again with the participation of m agents. The analysis for
to a speciﬁc CS,P, independently of the choice of proposers. this round can be performed in a way completely similar to

                                                IJCAI-07
                                                  1230the one performed for the last round of negotiations. How- during each negotiation round. There are 388 proposals a
ever, there is one main difference: the payoffs in the case of BE agent considers when negotiating in a stage with all ﬁve
a rejection are now the continuation payoffs (for agents of agents present (fewer in other cases).
speciﬁc type) from the last round subgame. We have to in- Table 1(a) shows performance when each agent has a
corporate this difference in our calculations. Other than that, uniform prior regarding the types of others. The BE al-
we can employ a similar line of argument to the one used for gorithm consistently outperforms KST, even though KST
identifying the equilibrium strategies in the last period. Pro- promotes social welfare (i.e., is well-aligned with total re-
ceeding in this way, we deﬁne the continuation payoffs and ward criterion) rather than individual rationality. KST
players’ strategies for each prior round, and ﬁnally determine agents without RL always converge to the coalition struc-
the ﬁrst round actions for any proposer i of type ti or any ture {4, 3, 2, 0, 1}; this is due to the fact that they
responder j of type tj responding to any proposal.    are discouraged from cooperating due to the lack of infor-
                                                      mation about their counterparts. When KST agents learn
5  Experimental Evaluation                            from observed actions after each episode (KST-Uni-RL) they
                                                      form the coalitions {2, 3, 4, 0, 1} in the last episode
To evaluate our approach, we ﬁrst conducted experiments
                                                      in 16 of 30 runs. BE agents, in contrast, form coalitions
in two settings, each with 5 agents having 5 possible types.
                                                      based on evolving beliefs about others, and do not form
Agents repeatedly engage in episodes of coalition formation,
                                                      the optimal structure {1, 2, 3, 4, 0}.5 Rather they tend
each episode consisting of a number of negotiation rounds.
                                                      to form coalitions of 2 or 3 members which exclude agent
We compare our Bayesian equilibrium approximation method
                                                      0 from being their partner. In addition, payoff division for
(BE) with KST, an algorithm inspired by a method presented
                                                      BE agents is more aligned with individual rationality than it
by Kraus et al. [4]. Though their method is better tailored
                                                      is with KST. The shares of (averaged) total payoff of KST-
to other settings, focusing on social welfare maximization, it
                                                      Uni-RL agents 0–4 are 0.8%, 0.7%, 28.8%, 29.6%, 40.1%,
is a rare example of a successfully tested discounted coali-
                                                      respectively, while for BE-Uni-RL (SS:10, LA:2) they are
tional bargaining method under some restricted form of un-
                                                      1.3%, 13.4%, 18.8%, 29.5%, 37%; this more accurately re-
certainty, which combines heuristics with principled game
                                                      ﬂects the power [6] of the agents. BE results are reason-
theoretic techniques. It essentially calculates an approxima-
                                                      ably robust with changing sample size and lookahead value
tion of a kernel-stable allocation for coalitions that form in
                                                      (at least in this environment with 3125 possible type vectors
each negotiation round with agents intentionally compromis-
                                                      in a 5-agent coalition).
ing part of their payoff in order to form coalitions. Like [4],
our KST uses a compromise factor of 0.8, but we assume no We attribute the poor performance of KST agents to the
central authority, only one agent proposing per round, and fact that they make their proposals without in any way tak-
coalition values estimated given type uncertainty.    ing into consideration the changing beliefs of others. With
  During an episode, agents progressively build a coalition the beliefs of the agents varying, negotiations drag (up to the
structure and agree on a payment allocation. The action exe- maximum of 10 rounds) due to refusals, resulting in reduced
cuted by a coalition at the end of an episode (the coalitional payoffs. BE agents do not suffer from this problem, since they
action) results in one of three possible stochastic outcomes keep track of all possible partners’ updated beliefs, and use
                                                      them during negotiation. Thus, they typically form a coali-
o ∈ O  =  {0, 1, 2} each of differing value. Each agent’s
type determines its “quality” and the “quality” of a coalition tion structure within the ﬁrst four rounds of an episode.
is dictated by the sum of the quality of its members less a We also experimented with a second setting in which sin-
penalty for coalition size.4 Coalition quality then determines gleton coalitions receive a penalty of -2 quality points (rather
                                                                              q tC          q ti /|C|
the odds of realizing a speciﬁc outcome (higher quality coali- than -1 above), and where ( )= ti∈tC ( ) (as
tions have greater potential). Finally, the value of a coalition coalitions get bigger they get penalized to reﬂect coordina-
given member types is the expected value w.r.t. the distribu- tion difﬁculties). This setting makes the quality of coalitions
tion over outcomes.                                   more difﬁcult to distinguish. Here, a near-optimal conﬁgura-
                                                                             { , ,  , ,  }
  In our ﬁrst setting, singleton coalitions receive a penalty of tion contains the structure 4 3 2 1 0 .Weusethree
-1 quality points. We compare BE and KST under various different priors: uniform, misinformed (agents have an initial
                                                               .                     t         t
learning models by measuring average total reward garnered belief of 0 8 that an agent with type has type +2), and
                                                                     .
by all coalitions in 30 runs of 500 formation episodes each, informed (belief 0 8 in the true type of each other agent).
with a limit of 10 bargaining rounds per episode and a bar- The results (Table 1(b)) indicate that KST agents again
gaining discount factor of δ =0.9. We also compare average do not do very well, engaging in long negotiations due
reward to the reward that can be attained using the optimal, to unaccounted-for differences in beliefs among the vari-
ﬁxed “kernel-stable” coalition structure {1, 2, 3, 4, 0}. ous agents. KST-Uni-RL agents, for example, typically use
  We compared BE and KST using agents that update their all ten bargaining rounds; in contrast, BE-Uni-RL usually
prior over partner types after observing coalitional actions— form structures within 3 rounds. Even when KST uses in-
thus learning by reinforcement (RL) after each episode—and formed priors, the fact that the expected value of coalitions
those that do not (No RL). In all cases, BE agents update is not common knowledge takes its toll. BE agents, on
their beliefs after observing the bargaining actions of others the other hand, derive the true types of their partners with

  4We omit the details here. We only note that agent 0 (of type 0) 5Nor should they, given bargaining horizon and δ—the kernel
is detrimental to any coalition (in our 2 ﬁrst settings). and other stability concepts do not consider bargaining dynamics.

                                                IJCAI-07
                                                  1231