    A Learning Scheme for Generating Expressive Music Performances of Jazz
                                              Standards
                                 Rafael Ramirez and Amaury Hazan
                                       Music Technology Group
                                        Pompeu Fabra University
                                    Ocata 1, 08003 Barcelona, Spain
                                      {rafael,ahazan}@iua.upf.es

                    Abstract                          errors in classical music. The work of Lopez de Mantaras et
                                                      al. is similar to ours but they are unable to explain their pre-
    We describe our approach for generating expressive dictions. The deviations and changes we consider are on note
    music performances of monophonic Jazz melodies.   duration, note onset, note energy, and intra-note features (e.g.
    It consists of three components: (a) a melodic    attack, vibrato). The study of these variations is the basis of
    transcription component which extracts a set of   an inductive content-based transformation tool for generating
    acoustic features from monophonic recordings, (b) expressive performances of musical pieces. The tool can be
    a machine learning component which induces an     divided into three components: a melodic transcription com-
    expressive transformation model from the set of ex- ponent, a machine learning component, and a melody synthe-
    tracted acoustic features, and (c) a melody synthe- sis component. In the following, we brieﬂy describe each of
    sis component which generates expressive mono-    these components.
    phonic output (MIDI or audio) from inexpressive
    melody descriptions using the induced expressive
    transformation model. In this paper we concentrate 2  Melodic description
    on the machine learning component, in particular,
                                                      Sound analysis and synthesis techniques based on spectral
    on the learning scheme we use for generating ex-
                                                      models are used for extracting high-level symbolic features
    pressive audio from a score.
                                                      from the recordings. The sound spectral model analysis tech-
                                                      niques are based on decomposing the original signal into si-
1  Introduction                                       nusoids plus a spectral residual. From the sinusoids of a
Expressive performance is an important issue in music which monophonic signal it is possible to extract information on
has been studied from different perspectives [Gabrielsson, note pitch, onset, duration, attack and energy, among other
1999]. The main approaches to empirically study expres- high-level information. This information can be modiﬁed and
sive performance have been based on statistical analysis (e.g. the result added back to the spectral representation without
[Repp, 1992]), mathematical modelling (e.g. [Todd, 1992]), loss of quality. We use the software SMSTools which is an
and analysis-by-synthesis (e.g. [Friberg, 1995]). In all these ideal tool for preprocessing the signal and providing a high-
approaches, it is a person who is responsible for devising a level description of the audio recordings, as well as for gen-
theory or mathematical model which captures different as- erating an expressive audio according to the transformations
pects of musical expressive performance. Recently, there has obtained by machine learning methods.
been work on applying machine learning techniques to the The low-level descriptors used to characterize the melodic
study of expressive performance. Widmer [Widmer, 2002] features of our recordings are instantaneous energy and fun-
has focused on the task of discovering general rules of ex- damental frequency. The procedure for computing the de-
pressive classical piano and recognizing famous pianists from scriptors is ﬁrst to divide the audio signal into analysis frames
their playing style. Lopez de Mantaras et al. [Lopez de Man- and compute a set of low-level descriptors for each analysis
taras, 2002] reported on SaxEx, a case-based reasoning sys- frame. Then, a note segmentation is performed using low-
tem capable of inferring a set of expressive transformations level descriptor values. Once the note boundaries are known,
and applying them to a solo performance in Jazz. In this pa- the note descriptors are computed from the low-level and the
per we describe an approach to investigate musical expressive fundamental frequency values (see [Gomez et al., 2003] for
performance based on inductive machine learning. In particu- details about the algorithm).
lar, we are interested in monophonic Jazz melodies performed
by a saxophonist. Our work differentiates from that of Wid- 3 Expressive performance knowledge
mer in that, being focused on saxophone Jazz performances, induction
we are interested in intra-note variations (e.g. vibrato) absent
in piano, as well as melody alterations (e.g. onset deviations, Data set. The training data used in our experimental inves-
ornamentations) which are normally considered performance tigations are monophonic recordings of three Jazz standards(Body and Soul, Once I loved and Like Someone in Love) per- 4 Melody synthesis
formed by a professional musician (a saxophone player) at The melody synthesis component transforms an inexpressive
11 different tempos around the nominal tempo. The resulting melody input into an expressive melody following the in-
data set is composed of 1936 performed notes.         duced models. Given a melody score (i.e. an inexpressive
Descriptors. In this paper, we are concerned with note-level description of a melody), our tool can either generate an ex-
(in particular note duration, note onset and note energy) and pressive MIDI performance, or generate an expressive audio
intra-note-level (in particular intra-note pitch and amplitude performance. In the second case, in addition to using the du-
shape) expressive transformations. Each note in the training ration, onset and energy models for computing expressive de-
data is annotated with its corresponding deviation and a num- viations of these parameters, we apply the intra-note model
ber of attributes representing both properties of the note it- to obtain the set of notes to be used to construct the audio
self and some aspects of the local context in which the note expressive performance. In order to build the ﬁnal audio per-
appears. Information about intrinsic properties of the note in- formance we transform each of the obtained notes according
clude note duration, note metrical position, and note envelope to the computed duration, onset and energy deviations, and
information, while information about its context include the concatenate the transformed notes using an algorithm that op-
note Narmour group(s) [Narmour, 1990], duration of previ- timizes the transitions between notes.
ous and following notes, and extension and direction of the
intervals between the note and the previous and following Acknowledgments
notes.                                                This work is supported by the Spanish TIC project ProMusic
Machine learning techniques. In order to induce predictive (TIC 2003-07776-C02-01). We would like to thank Emilia
models for duration ratio, onset deviation and energy, varia- Gomez, Esteban Maestre and Maarten Grachten for process-
tion, we have applied machine learning techniques such as re- ing the data.
gression trees, model trees and support vector machines (for
a complete comparison of the accuracy of these techniques, References
see [Ramirez et al., 2005]). Among these techniques, model [Friberg, 1995] Friberg, A., A Quantitative Rule System for Musical
trees is the most accurate method, and thus, we have based the Performance. PhD Thesis, KTH, Sweden, 1995.
machine learning component of our tool on this method. We
have also induced rule-based models [Ramirez et al., 2004] to [Gabrielsson, 1999] Gabrielsson, A., The performance of Music. In
explain the predictions made by our tool. In order to induce D.Deutsch (Ed.), The Psychology of Music (2nd ed.) Academic
a predictive model for intra-note features we have devised a Press, 1999.
learning scheme roughly described as follows:         [Gomez et al., 2003] Gomez, E., Grachten, M., Amatriain, X., Ar-
 1. apply k-means clustering to all the notes in the data set. cos, J., ”Melodic characterization of monophonic recordings for ex-
    We decided to set the number of clusters to ﬁve. This de- pressive tempo transformations”, Proceedings of Stockholm Music
                                                      Acoustics Conference, Stockholm, August 2003.
    cision was taken after analyzing a large number of notes
    in our data set and considering that there were basically [Lopez de Mantaras, 2002] Lopez de Mantaras, R., and Arcos, J.
    ﬁve qualitatively different types of note shapes. We char- 2002. Ai and music from composition to expressive performance.
    acterize each note in the data set by its attack, sustain and AI Magazine 23(3).
    release.                                          [Narmour, 1990] Narmour, E., The Analysis and Cognition of Basic
 2. apply a classiﬁcation algorithm (i.e. classiﬁcation trees) Melodic Structures: The Implication Realization Model. University
    to predict the cluster to which the note belongs. In order of Chicago Press, 1990.
    to train our classiﬁer we used the descriptors described [Ramirez et al., 2004] Ramirez, R., Hazan, A., Gomez, E., Maestre,
    above.                                            E., Understanding expressive transformations in saxophone jazz
 3. given a note and its cluster, apply a nearest neighbor al- standards using inductive machine learning. Proceedings of Sound
    gorithm to determine the most similar note in the cluster. and Music Conference (SMC), Paris, October 2004.
    We use the pitch and duration of a note as the distance [Ramirez et al., 2005] Ramirez, R., Hazan, A., Modeling Expressive
    measure, i.e. given a note, we look in the predicted clus- Music Performance in Jazz, Clearwater Florida, May 2005.
    ter for the closest note in duration and in pitch. We are
    particularly interested in duration and pitch because we [Rapp, 1992] Repp, B.H., Diversity and Commonality in Music Per-
    want to minimize the loss in sound quality when trans- formance: an Analysis of Timing Microstructure in Schumann’s
                                                      ‘Traumerei’. Journal of the Acoustical Society of America 104.
    forming the selected note to a note with the required
    pitch and the computed duration.                  [Todd, 1992] Todd, N., The Dynamics of Dynamics: a Model of
                                                      Musical Expression. Journal of the Acoustical Society of America
  Once we obtain all the notes in a score by applying 91, 1992.
the learning scheme described above, we proceed to ’glue’
the obtained notes together. Finally, we apply an algo- [Widmer, 2002] Widmer, G., In Search of the Horowitz Factor: In-
rithm to obtain smooth note transitions. A sample of a terim Report on a Musical Discovery Project. Invited paper. In Pro-
melody produced by our learning scheme can be found at ceedings of the 5th International Conference on Discovery Science,
www.iua.upf.es/∼rramirez/promusic/demo.wav.           Lbeck, Springer-Verlag.