 Concept Sampling: Towards Systematic Selection in Large-Scale Mixed Concepts in 
                                        Machine Learning 

                                    Yi Zhang1, 2 and Xiaoming Jin1 
                               1School of Software, Tsinghua University 
                        2Department of Computer Science, Tsinghua University 
                                        Beijing, 100084, China 
                       zhang-yi@mails.tsinghua.edu.cn, xmjin@tsinghua.edu.cn 

                    Abstract                         accessible, such as macro economic environments and po-
                                                     litical events [Harries et al., 1998]. On this occasion, one has 
    This paper addresses the problem of concept sam-
                                                     to break training data into segments over short time intervals 
    pling. In many real-world applications, a large col- in order to extract stable concept from each interval, and then 
    lection of mixed concepts is available for decision decision making is made upon the resulting large collection 
    making. However, the collection is often so large 
                                                     of diverse concepts. This strategy is widely used in on-line 
    that it is difficult if not unrealistic to utilize those 
                                                     learning over data streams [Street et al., 2001; Wang et al. 
    concepts directly, due to the domain-specific limi- 2003] and result in unlimited numbers of mixed concepts. 
    tations of available space or time. This naturally Example 2. Consider the problem of detecting credit card 
    yields the need for concept reduction. In this paper, frauds. The characteristics of the fraud events depend on 
    we introduce the novel problem of concept sam-   some hidden contexts, such as the policies of the specific 
    pling: to find the optimal subset of a large collection bank branch, the economic conditions, and the new law in the 
    of mixed concepts in advance so that the perform- local area. Thus the concepts, i.e. the fraud patterns under-
    ance of future decision making can be best pre-  lying the data, from different branches or even from different 
    served by selectively combining the concepts re- periods of the same branch may differ. Therefore, it is highly 
    mained in the subset. The problem is formulized as desirable to systematically analyze all these records. How-
    an optimization process based on our derivation of a ever, for privacy preserving, different branches can not share 
    target function, which ties a clear connection be-
                                                     their records. In this case, one possible solution is that each 
    tween the composition of the concept subset and the branch periodically contributes the concept describing its 
    expected error of future decision making upon the recent records, and the models for decision making can be 
    subset. Then, based on this target function, a sam-
                                                     constructed upon the large collection of available concepts. 
    pling algorithm is developed and its effectiveness is   The typical way to utilize mixed concepts, when there are 
    discussed. Extensive empirical studies suggest that, some data to be classified, is to select some “suitable” con-
    the proposed concept sampling method well pre-
                                                     cept(s) and combine them by some strategies, which will 
    serves the performance of decision making while  involve searching within the large concept collection. 
    dramatically reduces the number of concepts      However, such solutions are usually inefficient or even in-
    maintained and thus justify its usefulness in han-
                                                     feasible, because firstly, sometimes the collection is too large 
    dling large-scale mixed concepts.                to be held in main memory, and secondly, the time com-
                                                     plexity of decision making among large numbers of concepts 
1 Introduction                                       is unacceptable for many efficiency-critical applications, 
In many real-world applications people in machine learning such as online prediction of network intrusion.  
community are confronted with large numbers of mixed   This yields the need for concept reduction, the main focus 
concepts1, upon which the final decision is made. The in- of this paper. But is it possible to remove a large part of 
herent reason leading to this situation is that, in many cases, concepts in the collection while preserve the performance of 
the concept underlying the data evolves due to the changes of decision making? The theory of ensemble learning (i.e. 
hidden contexts [Widmer et al., 1996].               combining multiple concepts) [Kuncheva et al., 2003; Ruta 
  Example 1. Concepts related to stock trading strategies are et al., 2002] suggests that a concept can be approximated by 
influenced by many time-related factors which are hardly an ensemble of similar concepts. Thus, the removed concepts 
                                                     can be estimated by selectively combining the remains. 
   1                                                   While some literatures addressed the problem that seems 
     In the field of machine learning, a concept corresponds to a similar to this topic, e.g. data sampling, to the best of our 
learned description of a subset of instances defined over a large set knowledge, concept sampling has not been explored. The 
[Mitchell, 1997]. More generally, concepts can be deemed as map-
pings from instance space to label space and represented by classi- main contributions of this paper are: (1) we propose the 
fiers.                                               problem of concept sampling: to find the optimal subset of a 


                                               IJCAI-07
                                                 1150large collection of mixed concepts so that the performance of the future. Essentially, S contains all the concepts so far 
future decision making can be preserved. (2) We formally observed, and we assume that for each future instance q Q, 
derive a target function that ties a clear connection between the correct concept cq can be found in S2. 
the composition of the subset and the expected error of the  However, S is often too large to be directly used due to the 
decision making upon this subset. Using this function, we limitations of resources. Thus only a subset R whose size is 
formulize the problem of concept sampling as an optimiza- much smaller is permitted. Since many concepts in S have to 
tion process. (3) We design a sampling procedure to deter- be removed, the right concepts cq for many potential q Q  
mine the concept subset. The empirical results suggest that are not in R, and thus the performance of decision making 
the proposed method well preserves the performance of de- declines. This calls for concept sampling, which is to reduce 
cision making while dramatically reduces the number of the number of concepts maintained but preserve the per-
concepts maintained, and thus justify its usefulness in han- formance of decision making. This idea is possible because 
dling large-scale mixed concepts.                     the theory of ensemble learning [Kuncheva et al., 2003; Ruta 
                                                      et al., 2002] suggests that a concept c can be approximated by 
2 Related Work                                        an ensemble composed of its similar concepts. 
                                                       The problem of concept sampling is formally defined as to 
  The existence of mixed concepts has long been accepted in find the optimal subset R with predefined size V  that satisfies 
machine learning, such as in the on-line learning problem                                   0
over changing hidden contexts [Widmer et al., 1996], or in       R   arg min(E(R,Q))               (1) 
the discussion of extracting stable concepts when hidden             R S ,|R| V0
contexts evolve [Harries et al., 1998]. Recently, on-line where the target function E(R,Q) is the expected error rates of 
learning over data streams with evolving concept becomes an using concept set R to classify instances in Q. Note that for 
active field. Many algorithms in this field extract stable many q in Q, the best fitted concepts cq in the original set S 
concepts from short time intervals and engage various for- have been removed, and we want to approximate these con-
getting policies to emphasize the recent concepts [Street et al., cepts by selectively combining the remained ones. Therefore, 
2001; Wang et al. 2003]. In fact, all this work is to deal with the target function E(R, Q) can be further defined as: 
the mixed concepts. In this paper, we propose concept sam-   E(R,Q)       e( (R, q), q)p(q)  
pling, which plays an active role in managing large-scale                                          (2) 
                                                                       q Q
mixed concepts. Even in data streams scenario, our method 
                                                      Here p(q) is the probability of observing q in Q,  is 
can act as an offline component to extract useful information                                   (R,q)
                                                      the set of concepts in R that are selected to classify q (i.e. the 
from huge collection of historical concepts and thus com-
                                                      ensemble for classifying q), and         is  the  ex-
plements the existing on-line learning styles.                                     e( (R,q),q)
                                                      pected error of this classification. 
  Existing sampling techniques in machine learning mainly 
focus on instance space, which is to reduce the number of 
instances. General sampling methods often engage some 4 Concept Sampling Method 
empirical criteria on data distribution, e.g. density conden- This section presents our concept sampling method. To 
sation [Mitra et al., 2002], entropy-based data reduction minimize the target function mentioned in (2), the following 
[Huang et al., 2006]. Similarly, specific methods, such as key problems should be solved. First, the objective function 
those for instance-based learning [Wilson et al., 2000], also (2) involves Q, the set of unlabeled instances in future, which 
rely on some empirical criteria, e.g. assuming that object with is unseen in the time of sampling. Thus, the relationship 
the same label as its k neighbors is redundant.       between the composition of the subset R and the expected 
  Different from data sampling, this paper proposes a new error defined in (2) is not clear. Second, sampling should be 
problem of concept sampling, which is to preserve the quality efficient to handle large concept collection. In section 4.1, we 
of decision making upon the reduced concept set. More im- derivate an equivalent target function of (2) to deal with the 
portantly, we derive a target function that ties a clear con- first problem. In section 4.2, we design an efficient method to 
nection between the composition of the reduced set and the optimize this target function and discuss its effectiveness. 
performance of decision making, and formulize the sampling 
as an optimization rather than relying on empirical criteria. 4.1 The Target Function 
  One of the foundations of our work is the theory of com- In this section, we derivate an equivalent target function for 
bining multiple concepts. Since different concepts exist, the (2), which is computable given the subset R and the entire set 
diversity between concepts [Kuncheva et al., 2003] must be S, and thus ties a clear connection between the composition 
considered when pursuing the consensus. Further, theoretical of R and the expected error of future decision making upon R. 
analysis about the performance of the ensemble classifiers 
                                                        To handle the problem that Q is unknown, we define Qc as 
[Ruta et al., 2002] is engaged in our paper.          the set of instances q in Q whose inherent concept cq is c in S: 
3  General Framework of Concept Sampling                                                               
                                                        2
  Consider a large collection of mixed concepts S = {c1,  In fact, handling the new concept never observed in S is in-
                                                      vestigated in the field of on-line learning with concept drift [Wid-
c2, …, cn}, where each concept ci is represented by a classifier, 
and Q denotes the unknown set of instances to be classified in mer et al., 1996; Street et al., 2001; Wang et al. 2003] and is beyond 
                                                      the scope of this paper. 

                                               IJCAI-07
                                                 1151                       q                               Third, for ( (R,c),c) , we start the discussion with the 
         Qc   {q   Q | c   c}, c  S           (3)    case that (R,c) returns a single concept c’. Thus, (c',c)  is 
                                       q             the estimated error of using c’ to approximate c and can be 
  Recall from section 3, the correct concept c  of an unla- obtained from empirical test. Given D, a set of instances that 
beled instance q in Q can be found in S. Thus,{Qc | c S}is represents the instance distribution (labels are ignored), we 
a partition of Q. Then it holds that:                use concept c to label the instances in D, and denote the 
                                                                      c
      E(R,Q)       e( (R,q),q)p(q)                   labeled dataset as D . Then, (c',c) is naturally defined as: 
                q Q                                                                 c
                                              (4)                 (c',c)  1    (c', D )            (8) 
                      e( (R,q),q)p(q)
                                                                  c
                cQS q c                              where  (c', D ) measures the consistency (e.g. accuracy) of 
                                                                   c
  According to the definition of Q , the inherent concept cq concept c’ on D . Note that D can be obtained by random 
                             c                       sampling on all the available instances, and can be incre-
for each instance q in Qc is the concept c in S. Thus, the 
ensemble of concepts that are selected by (R,q) for clas- mentally maintained in dynamic situations [Vitter, 1985]. 
sifying q should be the set of concepts that are chosen for  However, if (R,c)  returns a group of concepts C*, how 
approximating concept c. Moreover, the expected error of to compute the expected error (C*,c) ? In this paper we use 
classifying q, termed e( (R,q),q) , can be represented by the majority voting as the combination strategy since it is 
the expected error of the approximation on concept c. both theoretically sound and practically powerful [Kuncheva 
                                                     et al., 2003; Ruta et al., 2002]. It is true that (C*,c) can be 
Therefore, given that (R,c)  is the set of concepts in R that   c
are selected to approximate concept c, and ( (R,c),c) re- tested on D as in (8). But it is very time consuming: con-
fers to the expected error of this approximation, it holds that: sider when searching the optimal R that minimizes (5), large 
                                                     numbers of possible combinations of (R,c) will be exam-
     E(R,Q)           e( (R,q),q)p(q)                ined, and for each possible combination, we need com-
                cQS q c                              pute  ( (R,c),c) . Fortunately, if the empirical error of each 
                       (  (R,c),c) p(q)              single concept c’ on c has been calculated as (8), the ex-
                                                     haustive testing of (C*,c) for all possible C* can be re-
                cQS q c                              placed by theoretical estimation [Ruta et al., 2002]: consider 
                    ( (R,c),c)    p(q)        (5)    using an ensemble C* of M concepts to approximate concept 
                                                     c. And e  is the probability that the ith concept in C* gives the 
                cQS            q c                          i
                                                     incorrect label in each voting, which is obtained by (8). Ac-
                    ( (R,c),c) p(c)                  cording to [Ruta et al., 2002], the distribution of the nor-
                c S                                  malized incorrect rates of these M concepts in each voting, 
                E'(R, S)                             defined as the number of incorrect votes divided by M, can be 
where three notations need investigation: p(c) , (R,c) and approximated by the probability density function of the 
                                                     normal distribution f (x) whose mean and variance are: 
  ( (R,c),c) .                                                    1  M         1   M
  First, concepts in S are collected independently and as-   e         ei ,      2   ei (1 ei )    (9) 
sumed to be equally important. Thus, p(c) is calculated as:      M   i 1      M    i 1
                         1                           Based on the above notions, (C*,c) , the expected error of 
                 p(c)                         (6)    the ensemble C* via majority voting, is the probability that 
                        | S |                        more than half of the M votes are incorrect: 
 Second,   (R,c)  defines the ensemble of concepts that 
                                                                  (C*,c)        f (x)dx            (10)
can be used to approximate concept c. Since concept c can be                x 0.5
approximated by combining multiple concepts similar to c   Finally, by combining (5), (6), (7), (8) and (10), we get a 
given that these similar concepts make different mistakes computable target function which is equivalent to (2):  
[Kuncheva et al., 2003; Ruta et al., 2002], (R,c) is defined 
as the concepts in R that are, “by and large”, similar to c:                  1
                                                       E(R,Q)     E'(R, S)           ( (R,c),c)    (11)
                                                                            | S | c S
         (R,c)  {c'  R |  (c',c)  d0}         (7) 
                                                     where  (R,c) is defined as (7), ( (R,c),c)  is  computed 
Here threshold d0 controls the strictness level of allowing a as (8) when (R,c)  returns a single concept and is estimated 
concept to be used as the ensemble member for approxi- as (10) when (R,c) is an ensemble C*. 
mating c. It is set as 0.15 in our empirical studies. Also note Function (11) indicates that E(R,Q) can be computed 
that (c',c) , the expected error of using concept c’ to ap- given the entire concept set S and a subset R. In this sense, it 
proximate concept c, will be defined later. According to (7), ties a clear connection between the composition of R and the 
concepts obviously inconsistent with c will be excluded, performance of future decision making based on R. As a 
while concepts with slight deviation from c are permitted, in result, the sampling problem in (1) can be solved as an op-
order to ensure the diversity in the ensemble.       timization process. 

                                               IJCAI-07
                                                 1152 Input:   the concept collection S                     According to the discussion at the end of section 4.1, the 
             the desired size of the reduced set, denoted by K value of a concept r is determined by, firstly, whether it can 
             the empirical error between concepts as in (8) join many ensembles for concepts in S, and secondly, 
             the threshold d0 in (7)                 whether there exist many other concepts in R that can also 
 Output: the subset R                                join these ensembles. In this sense, the step 3.1-3.4 of the 
                                                     algorithm repeatedly insert the concepts that join many en-
 Algorithm:                                          sembles that are in short of members, and remove the con-
 1. R  randomly sampling K concepts from S           cepts in R that can enter few ensembles or that mainly enter 
 2. Compute  ( (R,c),c)  for each c in S             the ensembles that already have sufficient members. Thus, 
 3. Repeat until R is invariant                      the quality of R is continuously improved. Note that step 3.5 
   3.1 Find the concept c’ in S, that the reduction of (11) is is used to avoid being trapped in the local minima. 
         largest if inserting c’ into R                 Before formally analyzing the time complexity, we men-
   3.2 Insert c’ into R and update ( (R,c),c)  for each tion two points. Firstly, we assume that for each concept r, 
         c in S                                      the notion Nb(r) {c  S | (r,c) d0}  can  be  efficiently 
   3.3 Find the concept r in R that the increase of (11) is  accessed: it can be computed before the sampling, and thus 
         smallest if removing r from R               each time concept sampling is executed, Nb(r) can be ac-
   3.4 Remove r from R and update  ( (R,c),c) for    cessed directly. In fact, an equivalent notion of Nb(r) is 
      each c in S                                    that Nb(r) {c  S | r  (R,c)} : the concepts whose en-
   3.5 The removed r is labeled so that it will not be in- sembles admit r. Thus, when r is inserted into or removed 
      serted into R again.                           from R, only concepts in Nb(r) will have their ensemble 
        Figure 1: Concept sampling algorithm         changed. This will dramatically facilitate the evaluation of 
                                                     the change of (11) for possible insertions or deletions in R. 
  Intuitively speaking, which concepts should be in R? For   Secondly, the time-consuming integral computation in (10) 
each concept c in S, (C*,c) computed as (9) and (10) re- depends on two variables, mean and variance of normal 
veals the relationship between the approximation error on c distribution f (x) . Thus we can discretize these two vari-
and the size M of the ensemble C* to approximate c: if ables and then produce an integral table beforehand, from 
largely remains constant and is smaller than 0.5, the larger which (10) can be accessed directly. 
the M, the lower the , and thus the lower the approximation   Accordingly, time complexity of the proposed method is 
error in (10). In this sense, we want that each concept c in S O(nN), where n is number of iterations, and N is the size of S: 
has an ensemble C* with enough qualified members from R.  Step 1: O(N). 
  On the one hand, the size of R is limited. Thus, the more 
                                                          Step 2: O(N). Firstly, for each r in R, update and for 
ensembles that a concept r can join simultaneously, the more 
likely r should be in R. Note that whether a concept r can join concepts in Nb(r); then for each c in S, com-
the ensemble of a concept c is determined according to (7). pute ( (R,c),c) as (8) or (10); at last, compute (11). 
  On the other hand, based on (9) and (10), the smaller the So the time complexity is O(N). 
size M of the ensemble C* of a concept c, the more dramatic  Step 3.1: O(N). For each concept c’ in S, inserting it to R 
the decrease of  when inserting a new member to C*, and   only affects the concepts in Nb(c’), thus the reduction 
thus the more substantial the decrease of the approximation of (11) can be estimated quickly. 
error on c. Thus, the concept that can enter the ensembles that 
few other concepts can enter should be deemed valuable, and  Step 3.2: O(|Nb(c’)|).  
the concept that mainly joins the ensembles that many other  Step 3.3: O(K). For each r in R, the increase of (11) is 
concepts can also join is more or less trivial.           computed rapidly based upon Nb(r). K is the size of R. 
4.2 The Sampling Procedure                                Step 3.4: O(|Nb(r)|). 
Clearly, it is very difficult to directly solve the optimization  Step 3.5: O(1). 
problem in (11) due to its nature of combinational optimiza-
tion: there are almost infinite possible combinations of R 
given S. In this section, we proposed a feasible approach that 5 Empirical Results 
iteratively improves the subset R, which is similarly in gen- In this section, we present our empirical results. The goals of 
eral to the methods designed for similar optimization prob- our experiments are: (1) to demonstrate the ability of our 
lem, e.g., as in [Huang et al., 2006]. More specifically, this concept sampling method to preserve the performance of 
method begins with a randomly selected subset R, and then decision making while reducing the number of concepts 
successively improves it by firstly inserting into R an outside maintained. (2) To justify the superiority of the proposed 
concept whose insertion maximizes the reduction of (11), and method over the straightforward selection method in terms of 
secondly, removing from R an inside concept whose deletion both performance and stability. We compared three methods: 
minimize the increase of (11). This process is repeated until decision making upon the entire concept set S (ES), upon the 
the subset is invariant (e.g. invariant in 10 continuous itera- reduced set obtained by concept sampling (CS); and upon the 
tions). The detailed algorithm is shown in figure 1. reduced set from random sampling (RS). 

                                               IJCAI-07
                                                 1153    3         1                                         3                        1         

   2.5                                                 2.5
                            0.95                                                0.95
    2                                                   2


   y 1.5                    0.9                        y 1.5                     0.9

    1                                                   1
                                            ES                                                   ES
                           Classification Accuracy Classification 0.85 CS        Accuracy Classification 0.85 CS
   0.5                                      RS         0.5                                       RS


    0                       0.8                         0                        0.8 
    0  0.5 1 1.5 2 2.5 3     0  20  40 60  80 100       0 0.5 1 1.5 2 2.5 3       0  20 40  60 80  100
             x                      Subset Size                  x                       Subset Size
 Figure 2: Distinct concepts Figure 3: Accuracy in sce- Figure 4: Miscellaneous con- Figure 5: Accuracy in sce-
 scenario (scenario 1)    nario 1                   cepts scenario (scenario 2) nario 2 
  Given a concept set, the decision making strategy was: (1) resulted in concept subsets with 5, 10, 20, 50, 100 concepts, 
for each instance q to be classified, a few evaluation data DE respectively. The final results were averaged over 10 inde-
(50 instances in our experiments) corresponding to the in- pendent runs. 
herent concept cq was given3; (2) based on DE, the following Clearly, the ideal sampling algorithms should sensibly 
“suitable concepts” were selected: the concept c* that had the determine the number of remained concepts for each class. 
highest (c*, D c )  (see (8)) plus all the concepts c satisfy- The results are shown in figure 3. It can be observed that: (1) 
ing  (c, D E ) 0.9 (c*, D E ) . (3) The selected concepts CS method outperformed RS, on all the sampling rates, in 
were combined by majority voting. In our experiments, each term of the performance of decision making upon the reduced 
concept was represented by a C4.5 tree [Quinlan, 1993]. concept set. (2) The lower the sampling rate, the more ob-
  To comprehensively examine the performance of our   vious was the superiority of CS over RS. This is because 
concept sampling method in various applications, three when the number of concepts that can be retained is quite 
typical scenarios were included in our empirical studies: (1) limited, the effectiveness of the sampling strategy becomes 
“distinct concepts scenario” and (2) “miscellaneous concepts crucial: even the unsuitable allocation of one position will 
scenario” are two boundary cases. Many real-world large lead to remarkable performance degradation. (3) CS method 
collections of mixed concepts can be deemed as the “inter- with a sampling rate at 20% could provide largely the same 
polations” of these two synthetic cases. Then, in (3) classification accuracy as the original set S without sampling. 
“real-world scenario”, the real-world “Adult” dataset was Miscellaneous Concepts Scenario: As in figure 4, 500 
tested, in order to evaluate the effectiveness of the proposed concepts were generated. But different from figure 2, no 
method in real-life applications.                     distinct class existed and the 500 concepts were miscella-
  Distinct Concepts Scenario: As in figure 2, 25 circle cen- neous, each of which corresponded to a random center circle. 
ters were produced. A concept was generated based on one of The entire set S, reduced set R and R’ were tested on 500 
the centers: 2D points (x and y coordinates were both in [0, 3]) testing datasets, each containing 500 examples correspond-
that fell into the circle around the center (with radius 1) were ing to one concept. We compared the average classification 
positive examples and otherwise negative. “Distinct concepts accuracy over these 500 datasets. Also, different sampling 
scenario” means that large numbers of concepts in the entire rates were tested and the final results were averaged over 10 
concept set S could be divided into distinct classes: concepts independent runs. From figure 5 we can observe the similar 
in the same class were similar, while concepts in different results as those in figure 3 (i.e. in distinct concepts scenario). 
classes were distinct. Each of the 25 circle centers in figure 2 And this justifies the superiorities of CS method over RS 
indicated a distinct class. For each class, 20 concepts were when concepts in the complete set S are miscellaneous. 
produced, each of which was trained from 200 random 2D  Real-world Scenario: In this scenario, we tested the three 
points, and 5% noise on labels was added when training each methods on the real-world “Adult” dataset from UCI re-
concept. Thus, even concepts in the same class (i.e. deter- pository. We divided both the training and the testing data-
mined by the same circle center) would be slightly different. sets into eight groups, based on the value of “workclass” 
Finally, 500 concepts in 25 distinct classes were generated, attribute. Two groups with very few examples were omitted. 
which formed the complete concept set S.              We extracted concepts from each of the remained six groups 
  The entire set S, reduced sets R  (generated by CS) and R’ in the training dataset. Since each concept was trained from 
(generated by RS) were tested on 25 testing datasets, each 100 examples in a group, groups with more examples pro-
containing 500 2D points corresponding to one distinct con- duced more concepts. Totally 322 concepts were generated 
cept class. We focused on the average classification accura- from the six groups, and formed the complete concept set S. 
cies over these 25 datasets. For totally 500 concepts in S, Testing dataset without the two omitted groups was di-
diverse sampling rates were tested for both CS and RS, which rectly used for test. This is reasonable because the proportion 
                                                      among the size of six remained groups is similar between the 
                                                      training dataset and the testing dataset. Thus, groups gener-
   3 In real-life applications, cq is often estimated from evaluation ating more concepts would have more instances in the testing 
data: in data stream scenario, the concept cq of the current instance q dataset, which is consistent with the assumption that concepts 
is estimated from recent training examples. And in the credit card in the complete set  should have the same probability to be 
                     q                                                S
fraud scenario, the concept c  of query q is estimated from recent useful “in future” (i.e. in testing dataset). 
labeled records from the branch where q is generated. 

                                               IJCAI-07
                                                 1154