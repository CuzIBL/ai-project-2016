                 On Natural Language Processing and Plan Recognition
                                Christopher W. Geib and Mark Steedman

                                          School of Informatics
                                        University of Edinburgh
                                     Edinburgh EH8 9LW, Scotland
                                           cgeib@inf.ed.ac.uk

                    Abstract                          these are individual words or utterances. In both cases, the
                                                      observations are used to create a higher level structure. In
    The research areas of plan recognition and natu-  NLP these higher level structures may be parse trees [Collins,
    ral language parsing share many common features   1997] or logical forms [Bos et al., 2004]. In PR they are
    and even algorithms. However, the dialog between  usually a hierarchical plan structure[Kautz and Allen, 1986;
                                 ﬀ
    these two disciplines has not been e ective. Specif- Kaminka et al., 2001; Geib and Goldman, 2003] or at least a
    ically, signiﬁcant recent results in parsing mildly high level root goal[Horvitz et al., 1998]. In either case, both
    context sensitive grammars have not been lever-   NLP and PR construct a higher level knowledge structure that
    aged in the state of the art plan recognition sys- relates the meanings of each of the individual observations to
    tems. This paper will outline the relations between a meaning for the collection of observations as a whole.
    natural language processing(NLP) and plan recog-
                                      ﬀ                 For the purposes of this discussion it will aid us to abstract
    nition(PR), argue that each of them can e ectively away from the speciﬁc details of the higher level structure that
    inform the other, and then focus on key recent re- is built by this process. To simplify this discussion we will
    search results in NLP and argue for their applica- talk about these systems as if they were creating an hierarchi-
    bility to PR.                                     cal data structure that captures the meaning of the collection
                                                      of observations. We will use the PR terminology and call this
1  Introduction                                       structure an explanation and following the NLP terminology
                                                      call the process of producing a single explanation parsing.
Without performing a careful literature search one could eas- In order to parse a set of observations into an explanation
ily imagine that the ﬁelds of Plan Recognition(PR) and Nat- both PR and NLP must specify the patterns of observations
ural Language Processing(NLP) are two separate ﬁelds that they are willing to accept or the rules that govern how the ob-
have little in common. There are few papers in either disci- servations can be combined. In PR this speciﬁcation is done
pline that directly cite work done in the other. While there in the form of a library of plans, while in NLP this is done
            [
are exceptions, Carberry, 1990; Blaylock and Allen, 2003; through a grammar. In Section 3 we will argue that there is
                                    ]
Pynadath and Wellman, 2000; Vilain, 1991 , even these pa- no signiﬁcant distinction between PR plan libraries and NLP
pers often are only citing NLP in passing and not making use grammars. Therefore, in this paper we will call all such spec-
of recent research results.                           iﬁcations of the rules for acceptable combination of observa-
  Interestingly, many researchers do see these two areas as tions grammars.
very related, but are still not taking the recent lessons learned
                                                        With this terminology in place, we can now describe both
in one area and applying them to the other. In an eﬀort to
                                                      NLP and PR as taking in as inputs a set of observations and a
rectify this lack, this paper will outline the commonalities be-
                                                      grammar specifying the acceptable sets of observations. Both
tween PR and NLP, argue why the results from each of these
                                                      NLP and PR then parse these observations to produce expla-
research areas should be used to inform the other, and then
                                                      nations that organize the observations into a structured repre-
outline some recent research results that could inform a uni-
                                                      sentation of the meaning of the collection.
ﬁed view of these two tasks.
                                                        Given this level similarity, it is not surprising that gram-
                                                      mars in both NLP and PR can result in multiple explanations
2  Commonalities                                      for a given set of observations. However, it is of interest that
In this section we will sketch the similarities at the surface in both disciplines this ambiguity has been resolved using
and algorithmic levels between PR and NLP before more for- very similar probabilistic methods. In both areas, the state of
mally drawing their representations together in the Section 3. the art methods are based on weighted model counting. These
We will start this process by laying out some terminology so systems build the set of possible explanations and establish a
that we can see the common parts of NLP and PR.       probability distribution over the set in order to determine the
  Both PR and NLP take as input a set of observations.In most likely explanation.
PR these are observations of action executions and in NLP The work in NLP often uses probability models derived

                                                IJCAI-07
                                                  1612from an annotated corpus of text[Clark and Curran, 2004] higher level task to be performed. Finally, C represents a set
while the probability models from PR have been based on of ordering constraints that have to hold between the subtasks
Markov models of the world dynamics [Bui et al., 2002] or for the method to be eﬀective.
probabilistic models of plan execution [Geib and Goldman, We will draw a parallel between HTNs and context free
2003]. While space prohibits a full exposition of these very grammars (CFGs). Following Aho and Ullman[Aho and Ull-
diﬀerent probability models, it is still telling that a weighted man, 1992] we deﬁne a CFG, G, as a 4-tuple G = (N, Σ, P, S )
model counting method is the state of the art in both ﬁelds. where
  Beyond these surface and algorithmic similarities there are • N is a ﬁnite set of nonterminal symbols,
psycholinguistic reasons for believing that PR and NLP are • Σ
very closely tied process that should inform one another. For is a ﬁnite set of terminal symbols disjoint from N,
example, consider indirect speech acts like asking someone • P is a set of production rules that have the form n → ω
                                                                                   ∗
“Do you know what time it is?” To correctly understand and where n ∈ N and ω ∈ (Σ ∪ N) , and
respond to this question requires both NLP and PR.      • S is a distinguished S ∈ N that is the start symbol.
  Correctly responding requires not merely parsing the sen-
tence to understand that it is a request about ones ability to Given these deﬁnitions, we would like to map the plans
provide a piece of information. It also requires recognizing represented as an HTN into an equivalent CFG. We ﬁrst con-
that asking the question of someone else is the ﬁrst step in a sider the case of a collection of HTN plans that are totally
                                                      ordered. That is, we assume that for every method deﬁnition
two part plan for ﬁnding out a piece of information by asking                     , ...,
someone else. PR allows one to conclude that if someone is the constraints on the subtasks st0 stn deﬁne a total order-
following this plan they most likely have the goal of knowing ing over the subtasks. Without loss of generality, we assume
the piece of information (the current time in this case) and that the subtasks’ subscripts represent this ordering.
that providing the desired information will be more helpful To encode the HTN as a CFG, we ﬁrst consider the op-
than answering the literal question asked.            erators. The processing for these is quite simple. We iden-
  Given the similarities between the two areas, it seems rea- tify the names of each operator as a terminal symbols in our
sonable that work in one area should inform the other. How- new grammar, and attach the add and delete lists to the non-
ever important results in each area are not being leveraged in terminal as features. Next we consider mapping the method
the other community. In the next section we will more for- deﬁnitions into productions within the grammar.
mally specify the relation between these two areas to help Given a totally ordered method deﬁnition, we can add the
researchers take advantage of the results in both areas. task to be decomposed to the set of non-terminal symbols.
                                                      Then we deﬁne a new production rule with this task its left
                                                      hand side. We then deﬁne the right hand side of the rule
3  Plans as Grammars                                  as the ordered set of subtasks. Thus, the method deﬁnition
Our argument that PR and NLP should inform one another (name, T, {st0, ..., stn}, C) is rewritten as the CFG production
would be signiﬁcantly strengthened if we could show, as we rule: T → st0, ..., stn and T is added to the set of non-
have asserted above, that the plan libraries used by PR sys- terminals.
tems are equivalent to the grammars used by NLP systems. For example, consider the very simple HTN method, m1
In the following section we will show the parallels between for acquiring shoes:
these two constructs and a mapping between them.
  Almost all PR work has been done on traditional hierar-  (m1,  acquire(shoes),
          1
chical plans. While much of the work in plan recognition         {goto(store), choose(shoes), buy(shoes)},
has not provided formal speciﬁcations for their plan repre-      {(1 ≺ 2), (2 ≺ 3)})
sentations they can all generally be seen as special cases of
                                                                           ≺
Hierarchical Task Networks (HTN) as deﬁned in [Ghallab et where the constraints (1 2) indicates the task goto(store)
                                                                                             ≺
al., 2004].                                           must precede the task choose(shoes) and (2 3) indicates
  According to Ghallab the actions of an HTN domain are that choose(shoes) must precede buy(shoes). This is very
deﬁned as either operators or methods. An operator corre- easily captured with the CFG production:
sponds to an action that can be executed in the world. Fol-   acquire(shoes) →
lowing Ghallab we will deﬁne them as a triple (n, add −         goto(store), choose(shoes), buy(shoes)
list, delete−list) where n is the name of the operator, add−list
is a list of predicates that are made true or added to the world This process of converting each method deﬁnition into a pro-
by the operator, and delete − list is the set of predicates that duction rule and adding the task to be decomposed to the set
are made false or deleted from the world by the operator. of non-terminals is repeated for every method in the HTN to
  A method on the other hand represents a higher level action produce the CFG for the plans. Now we turn to the question
                                                      of partial ordering.
and is represented as a 4-tuple (name, T, {st0, ..., stn}, C) such
that name is a unique identiﬁer for the method, T names the Limited cases of partial orderness could be handled in
                                                      CFGs by expanding the grammar with a production rules for
higher level action this method decomposes, and {st0, ..., stn}
identiﬁes the set of sub-tasks that must be performed for the each possible ordering. However, as the NLP community
                                                      has realized this can result in an unacceptable increase in the
  1See [Bui et al., 2002] for an exception that works on hierarchical size of the grammar, and the related runtime of the parsing
Markov models                                         algorithm[Barton, 1985].

                                                IJCAI-07
                                                  1613  So instead, to address this, the NLP community has pro- clude IL/LP Grammars[Shieber, 1984], Tree Adjunction
duced a number of diﬀerent grammar formalisms that al- Grammars(TAG)[Joshi and Schabes, 1997], and Combina-
low the grammar to separately express decomposition and tory Catagorial Grammars(CCG)[Steedman, 2000; Hocken-
ordering. This includes the work of Shieber on ID/LP  maier, 2003; Clark and Curran, 2004]. These “mildly context
grammars [Shieber, 1984], Nederhof on poms-CFGs [Neder- sensitive grammars”(MCSGs) have a number of properties
hof et al., 2003], and Hoﬀman[Hoﬀman, 1995]    and    that make them attractive for NLP including greater expres-
Baldridge[Baldridge, 2002] on partial orderness in Combi- siveness than CFGs but still having polynomial algorithms
natory Catagorial Grammars. All of of these are attempts to for parsing. These properties also make them attractive for
include partial orderness within the grammar formalism (and adoption by the PR community.
parsing mechanism) without the exponential increase in the While these grammars are of scientiﬁc interest, we should
grammar size and runtime. Since each of these formalisms justify their use, since it is not clear that PR requires gram-
use very diﬀerent representations, rather than presenting ex- mars that are more expressive than CFGs. Such a claim would
amples, we refer the reader to the cited papers. It suﬃces rest on the empirical need for plans that are not context free.
to say that these grammar formalisms introduce notational If nothing more than a CFG is needed for PR, then a well
additions to denote partial orderness within the production known parsing algorithm like CKY that has a cubic complex-
rules and to explicitly specify the ordering relations that are ity seems to be the obvious choices for application to PR.
required in each production. These formalisms can be used However, if there are PR problems that require recognizing
to capture HTN plan domains that require partial ordering. plans that are not within the class of CFG plans, this would
  It should be clear from this exposition that the grammar provide a convincing argument that PR requires a grammar
formalisms found in the NLP literature are suﬃcient to cover that is not context free. In the following we will provide
the method deﬁnitions found in most if not all of the PR liter- just such an example. While there are a number of diﬀerent
ature. However, to the best of our knowledge no one has used classes of MCSGs with diﬀerent expressiveness results, and
any of the relatively recent grammar formalisms and their as- the exploration of all of them may prove useful for PR re-
sociated parsing machinery for plan recognition. Making use search, we will focus on the subclass of MCSGs that includes
of these grammatical formalisms would also allow the use of CCGs and TAGs called Linear Index Grammars(LIG).
their associated formal complexity results as well, something Steedman[Steedman, 2000] has argued convincingly that
that has often been lacking in the work in PR.        CCGs and other LIGs are able to capture phenomena beyond
  Thus, we propose that NLP and PR could be uniﬁed by the CFGs that are essential to real world language use. Given
use of the same underlying grammatical formalisms for repre- the parallels we have already demonstrated between NLP
senting the constraints on observations, and using a common and PR, we argue that if this class is necessary for NLP it
parsing mechanism. In the case of probabilistic NLP and PR shouldn’t be surprising to us if this class of grammars cap-
systems, we believe these systems may need to retaining sep- tured essential phenomena in PR as well. In this light, Steed-
arate methods for computing their probability distributions, man shows that CCGs provide for crossing dependencies in
however the parsing of observations into explanations could NLP, a critical extension that context free grammars cannot
share a common framework. In the next section we will ad- capture. Likewise, if we ﬁnd that such crossing dependencies
vocate a speciﬁc class of grammars for this task.     are necessary for recognizing plans we would have a strong
                                                      argument that PR requires a grammar that is in the MCSG
4  New Grammar Formalisms for PR                      family.
                                                        While a full discussion of the handling of crossing depen-
Given that researchers in NLP have been working on the close dencies in CCGs is beyond the scope of this paper, it will be
relationship between grammars, parsers, and language ex- helpful to understand their basic structure in order to iden-
pressiveness it shouldn’t be a surprise that results from this tify them in PR contexts. Crossing dependencies occur when
work could inform the work in PR. There are some classes the words that make up a constituent (like a relative clause)
of grammars that are too computationally expensive to parse are interleaved in the sentence with the elements of a diﬀer-
for real world application. For example, the well known com- ent constituent. Steedman[Steedman, 2000] has argued that
plexity results for parsing context sensitive grammars (CSGs) a particularly strong example of the naturalness of these con-
have all but ruled them out for NLP work. Likewise we expect structs are Dutch verbs like proberen ‘to try’ which allow a
poor performance for applications of CSGs to PR. Unfortu- number of scrambled word orders that that are outside of the
nately, PR researchers have used these results as a motivation expressiveness of CFGs.
to build their own algorithms for parsing, often without even For example the translation of the phrase “... because I try
considering the limitations of the existing parsing algorithms. to teach Jan to sing the song.” has four possible acceptable
Examples include graph covering[Kautz and Allen, 1986] and orderings [1 - 4] and a ﬁfth that is more questionable.
Bayes nets[Bui et al., 2002], that trade one np-hard problem
for another. What has been largely ignored by the PR com- 1. . . . omdat ik1 Jan2 het lied3 probeer1 te leren2 zingen3.
munity is the NLP work in extending context free grammars . . . because I Jan the song try  to teach to sing.
         ﬃ
and their e cient parsing algorithms.                   2. . . . omdat ik1 probeer1 Jan2 het lied3 te leren2 zingen3.
  Recent work in NLP has expanded the language hierar-  3. . . . omdat ik1 probeer1 Jan2 te leren2 het lied3 te zingen3.
chy with grammars that have a complexity that falls be- 4. . . . omdat ik1 Jan2 probeer1 te leren2 het lied3 te zingen3.
tween context free and context sensitive. Examples, in- 5. ?. . . omdat ik1 Jan probeer1 het lied te leren zingen

                                                IJCAI-07
                                                  1614  The subscripts are included to show the correspondence of
the noun phrases to the verbs. For example in the ﬁrst or-
dering the noun phrases are all introduced ﬁrst followed by
their verbs in the same order as their nouns. This produces
the maximally crossed ordering for this sentence.
  The realization of these kinds of crossed dependencies in
a PR context is relatively straightforward. Its important to
keep in mind the mapping that we are using between tradi-
tional language grammars and planning grammars will mean Figure 2: An example plan with crossing dependency struc-
that dependencies in PR are not the same as in NLP. In NLP ture
dependencies are features like gender, number or tense that
must agree between diﬀerent words within the sentence. In 4.1 Why MCSGs?
the PR context, dependencies are equivalent to causal links Joshi[Joshi, 1985] ﬁrst formally deﬁned the class of MCSGs
                           [
in traditional nonlinear planning McAllester and Rosenblitt, as those grammars that share four properties that are relevant
    ]
1991 . That is, they are states of the world that are produced for NLP:
by one action and consumed by another. Therefore, a plan
with a crossing dependency would have the causal structure • The class of languages included covers all context free
shown in Figure 1 where in act1 is found to produce the pre- languages.
conditions for actions act2 and act3 which each produce a • The languages in the class are polynomially parsable.
precondition for act4. Such a structure requires that two dif-
                                                        •
ferent conditions be created and preserved across two diﬀer- The languages in the class only capture certain types
ent actions for their use. Note that while the actions are only of dependencies including nested (non-crossing) and
partially ordered, there is no linearizion of them that will re- crossed dependencies.
move the crossing dependency. That is, act2 and act3 can be • The languages in the class have the constant growth
reordered but this will not remove the crossing dependency. property which requires that if all of the sentences in the
                                                          language are sorted according to their length then any
                                                          two consecutive sentences do not diﬀer in their length
                                                          by more than a constant factor determined by the gram-
                                                          mar.
                                                      This set of properties are also relevant for deﬁning the class
                                                      of grammars that would work well for PR. We will argue for
                                                      each of them in order.
                                                        First, we have just demonstrated the need for grammars
                                                      that are more than context free for PR. Second, clearly poly-
                                                      nomial parsing is desirable for PR. In order to use these algo-
Figure 1: An abstract plan with a crossed dependency struc- rithms in real world applications they will need to be extended
ture                                                  to consider multiple possible interleaved goals and to handle
                                                      partially observable domains[Geib and Goldman, 2003].Ifa
                                                      single goal can’t be parsed in polynomial time what hope do
  The argument for the necessity of MCSGs for planning
                                                      we have for eﬃcient algorithms for the needed extensions?
rests on real world examples of plans with this structure. Be-
                                                      Further, PR is needed in a great many applications that will
ing able to describe what such a plan looks like is not com-
                                                      not tolerate algorithms with greater complexity. For example,
pelling if they never occur in PR problem domains. Fortu-
                                                      assistive systems are not useful if their advice comes to late.
nately, examples of plans with this structure are relatively
                                                        Third, Plans do have structure that is captured in depen-
common. Consider recognizing the activities of a bank rob-
                                                      dency structures. Therefore, it seems natural to try to restrict
ber that has both his gun and ski-mask in a duﬀel bag and
                                                      the grammar for plans to the kinds of dependency structures
his goal is to rob a bank. He must open the bag, put on the
                                                      that are actually used. Whether or not the dependency restric-
mask and pick up the gun and enter the bank. Figure 2 shows
                                                      tions that are consistent with NLP are the same set for PR is
this plan. This plan has exactly the same crossed dependency
                                                      largely an empirical question. We have already seen evidence
structure shown in Figure 1.
                                                      of crossing dependencies that required us to abandon CFGs in
  Note, that we could make this plan much more complex favor of MCSG’s. While nested and crossing dependencies in
with out eﬀecting the result. Actions could be added before the abstract can cover all the kinds of dependencies needed in
opening the bag, after entering the bank, and even between planning, diﬀerent MCSGs place diﬀerent restrictions on the
putting on the ski-mask and picking up the gun so long as the allowable depth of crossings and the number of nesting. This
critical causal links are not violated. The presence of plans will have a signiﬁcant impact on the expressiveness of a par-
with this structure and our desire to recognize such plans ticular MCSG and its applicability to PR.
gives us a strong reason to look at the grammars that fall this Fourth and ﬁnally, the requirement of the constant growth
class as a grammatical formalism for PR.              rate may be the hardest to understand. Intuitively in the PR

                                                IJCAI-07
                                                  1615domain this means that if there is a plan of length n then there References
is another plan of length at most n+K where K is a constant
                                                      [Aho and Ullman, 1992] Alfred V. Aho and Jeﬀrey D. Ull-
for the speciﬁc domain. For example this rules out that the
                                                         man.  Foundations of Computer Science. W.H. Free-
length of the next plan is a function of the length of the previ-
                                                         man/Computer Science Press, New York, NY, 1992.
ous plan or some other external feature. Note this says noth-
ing about the goals that are achieved by the plans. The plan [Baldridge, 2002] Jason Baldridge. Lexically Speciﬁed
of length n and length n+K may achieve very diﬀerent goals Derivational Control in Combinatory Categorial Gram-
but they are both acceptable plans within the grammar. This mar. PhD thesis, University of Edinburgh, 2002.
speaks to the intuition that given a plan one should be able to [Barton, 1985] G. Edward Barton. On the complexity of
add a small ﬁxed number of actions to the plan and get an- id/lp parsing. Computational Linguistics, 11(4):205–218,
other plan. Again this seems to be the kind of property one 1985.
expects to see in a PR domain and therefore in a plan gram-
mar.                                                  [Blaylock and Allen, 2003] Nate Blaylock and James Allen.
  Now, while we believe we have made a strong argument   Corpus-based statistical goal recognition. In Proceedings
for the use of MCSGs for PR, this is not the ﬁnal word on of the 18th International Joint Conference on Artiﬁcial In-
the question. While we have presented an argument that we telligence, pages 1303–1308, 2003.
need at least the expressiveness of LIGs, it may be the case [Bos et al., 2004] Johan Bos, Stephen Clark, Mark Steed-
that still more powerful grammar formalisms are needed. The man, James Curran, and Julia Hockenmaier. Wide-
most promising method for proving such a result would re- coverage semantic representations from a ccg parser. In
quire ﬁnding plans with dependency structures that are not in Proceedings of the 20th International Conference on Com-
MCSG that our PR systems need to recognize. Thus, deter- putational Linguistics (COLING ’04), 2004.
                     ﬃ
mining if MCSGs are su cient for PR is an open research [Bui et al., 2002] Hung H. Bui, Svetha Venkatesh, and Geoﬀ
question for the community.                              West. Policy recognition in the abstract hidden markov
  While there are well known languages that are not in   model. In Technical Report 4/2000 School of Computer
MCSG it is diﬃcult to see their relevance to planning do- Science, Curtin University of Technology, 2002.
mains. For example the language {a2n }, that is the language
                                                      [            ]
where in the length of any sentence of the language is a power Carberry, 1990 Sandra Carberry. Plan Recognition in Nat-
of two, is not MCSG as it fails the constant growth require- ural Language Dialogue. ACL-MIT Press Series in Natu-
ment. It is possible to imagine contrived examples where this ral Language Processing. MIT Press, 1990.
would be relevant for PR (Perhaps as part of some kind of [Clark and Curran, 2004] Stephen Clark and James Curran.
athletic training regime, we want to recognize cases where in Parsing the wsj using ccg and log-linear models. In Pro-
someone has run around the track a number of times that is ceedings of the 42nd Annual Meeting of the Association
a power of two.) However, this certainly seems anomalous for Computational Linguistics (ACL-04), 2004.
and most likely should be dealt with by reasoning that falls
                                                      [Collins, 1997] Michael Collins. Three generative, lexical-
outside of the grammar, like a counter and a simple test.
                                                         ized models for statistical parsing. In ACL ’97: Proceed-
                                                         ings of the 35th Annual Meeting of the Association for
                                                         Computational Linguistics, 1997.
5  Conclusions
                                                      [Geib and Goldman, 2003] Christopher W.  Geib   and
                                                                                        /
There are close ties between the process of natural language Robert P. Goldman. Recognizing plan goal abandonment.
processing and plan recognition. This relation should allow In Proceedings of IJCAI 2003, 2003.
these two processes to inform each other and allow the trans- [Ghallab et al., 2004] Malik Ghallab, Dana Nau, and Paolo
fer of research results from one area to the other. However, Traverso. Automated Planning: Theory and Practice.
much recent work in both ﬁelds has gone unnoticed by re- Morgan Kaufmann, 2004.
searchers in the other ﬁeld. This paper begins the process of
                                                      [Hockenmaier, 2003] Julia Hockenmaier. Data and Mod-
sharing these results, describing the isomorphism between the
                                                         els for Statistical Parsing with Combinatory Catagorial
grammatical formalisms from NLP and plan representations
                                                         Grammar. PhD thesis, University of Edinburgh, 2003.
for PR, arguing that like NLP, PR will require a grammatical
formalism in the mildly context sensitive family, and ﬁnally [Hoﬀman, 1995] Beryl Hoﬀman. Integrating ‘free’ word or-
that NLP and PR can form a common underlying task that   der syntax and information structure. In Proceedings of the
can usefully be explored together.                       1995 Conference of the European Chapter of Association
                                                         for Computational Linguistics, pages 245–252, 1995.
                                                      [Horvitz et al., 1998] Eric Horvitz, Jack Breese, David
Acknowledgments                                          Heckerman, David Hovel, and Koos Rommelse. The lu-
                                                         miere project: Bayesian user modeling for inferring the
The work described in this paper was conducted within the goals and needs of software users. In Proceedings of the
EU Cognitive Systems project PACO-PLUS (FP6-2004-IST-    14th Conference on Uncertainty in Artiﬁcial Intelligence,
4-027657) funded by the European Commission.             1998.

                                                IJCAI-07
                                                  1616