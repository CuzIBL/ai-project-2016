 A   Probabilistic Learning Method for XML Annotation of Documents

                   Boris Chidlovskii1                           J´erˆome Fuselier1,2
             1Xerox Research Centre Europe         2Universit´edeSavoie-LaboratoireSysCom
                6, chemin de Maupertuis                        Domaine Universitaire
                  38240 Meylan, France                   73376 Le Bourget-du-Lac, France


                    Abstract                          XML documents, like a conversion through a set of lo-
                                                      cal transformations [3], or entail the transformation to
    We consider the problem of semantic annota-       particular tasks, such as the semantic annotation of dy-
    tion of semi-structured documents according to    namically generated Web pages in news portals [11] or
    a target XML schema. The task is to annotate      the extraction of logical structure from page images [12].
    a document in a tree-like manner where the an-
    notation tree is an instance of a tree class de-    In this paper, we consider the general case of tree an-
    ﬁned by DTD or W3C XML Schema descrip-            notation of semi-structured documents. We make   no
    tions. In the probabilistic setting, we cope with assumptions about the structure of the source and tar-
    the tree annotation problem as a generalized      get documents or their possible similarity. We repre-
                                                      sent the document content as a sequence of observations
    probabilistic context-free parsing of an obser-        {         }
    vation sequence where each observation comes      x =   x1,...,xn , where each observation xi is a con-
    with a probability distribution over terminals    tent fragment. In the case of HTML documents, such
    supplied by a probabilistic classiﬁer associated  a fragment may be one or multiple leaves, often sur-
    with the content of documents. We determine       rounded with rich contextual information in the form of
    the most probable tree annotation by maximiz-     HTML tags, attributes, etc. The tree annotation of se-
    ing the joint probability of selecting a terminal quence x is given by a pair (y,d), where y and d refer to
                                                      leaves and internal nodes of the tree, respectively. The
    sequence for the observation sequence and the                  {         }
    most probable parse for the selected terminal     sequence y =  y1,...,yn  can be seen, on one side, as
    sequence.                                         labels for observations in x, and on the other side, as a
                                                      terminal sequence for tree d that deﬁnes the internal tree
                                                      structure over y according to the target XML schema.
1   Introduction                                        In supervised learning, the document annotation sys-
The future of the World Wide Web is often associated  tem  includes selecting the tree annotation model and
with the Semantic Web initiative which has as a tar-  training the model parameters from a training set S
get a wide-spread document reuse, re-purposing and ex- given by triples (x, y,d). We adopt a probabilistic set-
change, achieved by means of making document markup   ting, by which we estimate the probability of an anno-
and annotation more machine-readable. The success of  tation tree (y,d) for a given observation sequence x and
the Semantic Web initiative depends to a large extent on address the problem of ﬁnding the pair (y,d) of maximal
our capacity to move from rendering-oriented markup of likelihood.
documents, like PDF or HTML, to   semantic-oriented     We develop a modular architecture for the tree an-
document markup, like XML and RDF.                    notation of documents that includes two major compo-
  In this paper, we address the problem of semantic an- nents. The ﬁrst component is a probabilistic context-free
notation of HTML documents according to a target XML  grammar (PCFG) which is a probabilistic extension to
schema. A tree-like annotation of a document requires the corresponding (deterministic) XML schema deﬁni-
that the annotation tree be an instance of the target tion. The PCFG rules may be obtained by rewriting the
schema, described in a DTD, W3C XML Schema or an-     schema’s element declarations (in the case of a DTD)
other schema language. Annotation trees naturally gen- or element and type deﬁnitions (in the case of a W3C
eralize ﬂat annotations conventionally used in informa- XML Schema) and the rule probabilities are chosen by
tion extraction and wrapper induction for Web sites.  observing rule occurrences in the training set, similar
  The migration of documents from rendering-oriented  to learning rule probabilities from tree-bank corpora for
formats, like PDF and HTML, toward XML has recently   NLP tasks. PCFGs oﬀer the eﬃcient inside-outside al-
become an important issue in various research commu-  gorithm for ﬁnding the most probable parse for a given
nities [2; 3; 11; 12]. The majority of approaches either sequence y of terminals. The complexity of the algo-
make certain assumptions about the source and target  rithm is O(n3 ·|N|), where n is the length of sequence yand |N| is the number of non-terminals on the PCFG.   G  forms the set T (G) of unranked labeled rooted trees
  The second component is a probabilistic classiﬁer for constrained with schema G.
predicting the terminals y for the observations xi in x.
In the case of HTML documents, we use the maximum     2.1   Tree annotation problem
entropy framework [1], which proved its eﬃciency when When annotating HTML documents accordingly to a
combining content, layout and structural features ex- target XML schema, the main diﬃculty arises from the
tracted from HTML documents for making probabilistic  fact that the source documents are essentially layout-
predictions p(y)forxi.                                oriented, and the use of tags and attributes is not nec-
  With the terminal predictions supplied by the con-  essarily consistent with elements of the target schema.
tent classiﬁer, the tree annotation problem represents The irregular use of tags in documents, combined with
the generalized case of probabilistic parsing,whereeach complex relationships between elements in the target
position i in sequence y is deﬁned not with a speciﬁc schema, makes the manual writing of HTML-to-XML
terminal, but with a terminal probability p(y). Conse- transformation rules diﬃcult and cumbersome.
quently, we consider the sequential and joint evaluations In supervised learning, the content of source docu-
of the maximum likelihood tree annotation for observa- ments is presented as a sequence of observations x =
tion sequences. In the joint case, we develop a general- {x1,...,xn}, where any observation xi refers to a con-
ized version of the inside-outside algorithm that deter- tent fragment, surrounded by rich contextual informa-
mines the most probable annotation tree (y,d) according tion in the form of HTML tags, attributes, etc. The tree
to the PCFG and the distributions p(y) for all positions annotation model is deﬁned as a mapping X → (Y,D)
i in x. We show that the complexity of the generalized that maps the observation sequence x intoapair(y,d)
                            3
inside-outside algorithm is O(n ·|N|+n·|T |·|N|), where where y={y1,...,yn} is a terminal sequence and d is a
n is the length of x and y,andwhere|N| and |T | are the parse tree of y according to the target schema or equiv-
number of non-terminals and terminals in the PCFG.    alent PCFG  G, S⇒y. The training set  S for training
  We also show  that the proposed extension of the    the model parameters is given by a set of triples (x, y,d).
inside-outside algorithm imposes the conditional inde-  To determine the most probable tree annotation (y,d)
pendence requirement, similar to the Naive Bayes as-  for a sequence x, we maximize the joint probability
sumption, on estimating terminal probabilities. We test p(y,d|x,G), given the sequence x and PCFG G.Us-
our method on two collections and report an important ing the Bayes theorem and the independence of x and
advantage of the joint evaluation over the sequential one. G,wehave
                                                                      |          |     ·   |
2   XML annotation and schema                                    p(y,dx,G)=p(d   y,G)   p(y x),       (1)
                                                      where p(y|x) is the probability of terminal sequence y
                                                      for the observed sequence x,andp(d|y,G) is the proba-
  XML annotations of documents are trees where inner  bility of the parse d for y according the PCFG G.The
nodes determine the tree structure, and the leaf nodes most probable tree annotation for x is a pair (y,d)that
and tag attributes refer to the document content. XML maximizes the probability in (1),
annotations can be abstracted as the class T of unranked
labeled rooted trees deﬁned over an alphabet Σ of tag         (y,d)max = argmax  p(d|y,G) · p(y|x).   (2)
names [9]. The set of trees over Σ can be constrained                      (y,d)
by a schema D  that is deﬁned using DTD, W3C XML
                                                        In the following, we build a probabilistic model for
Schema or other schema languages.
                                                      tree annotation of source documents consisting of two
  DTDs and an important part of W3C XML Schema        components to get the two probability estimates in (2).
descriptions can be modeled as extended context-free  The ﬁrst component is a probabilistic extension of the
          [  ]
grammars  10 , where regular expressions over alpha-  target XML schema; for a given terminal sequence y,
bet Σ are constructed by using the two basic opera-                                    |
                    ·              |                  it ﬁnds the most probable parse p(d y,G) for sequences
tions of concatenation and disjunction andwithoccur-  according to the PCFG  G, where rule probabilities are
rence operators ∗ (Kleene closure), ? (a?=a|)and+
       · ∗                                            trained from the available training set. The second com-
(a+=a   a ). An extended context free grammar (ECFG)  ponent is a probabilistic content classiﬁer C,itestimates
is deﬁned by the 4-tuple G =(T,N,S,R), where T and
                                                      the conditional probabilities p(y|xi) for annotating ob-
N are disjoint sets of terminals and nonterminals in Σ,                           ∈
      ∪    S                                          servations xi with terminals y T . Finally, for a given
Σ=T     N;   is an initial nonterminal and R is a ﬁnite sequence of observations x, we develop two methods for
set of production rules of the form A → α for A ∈ N,
                                         ∪            ﬁnding a tree annotation (y,d) that maximizes the joint
where α is a regular expression over Σ = T N.The      probability p(y,d|x,G)in(1).
language L(G)deﬁnedbyanECFGG        is the set of ter-
minal strings derivable from the starting symbol S of
G. Formally, L(G)={w    ∈ Σ∗|S  ⇒ w},where⇒     de-   3   Probabilistic context-free grammars
notes the transitive closure of the derivability relation. PCFGs are probabilistic extensions of CFGs, where each
We represent as a parse tree d any sequential form that rule A → α in R is associated with a real number p in
reﬂects the derivational steps. The set of parse trees for the half-open interval (0; 1]. The values of p obey therestriction that for a given non-terminal A ∈ N, all rules The tree annotation model processes sequences of ob-
for A must have p values that sum to 1,               servations x =  {x1,...,xn} from the inﬁnite set X,
                                                     where the observations xi are not words in a language
            ∀A ∈ N :           p(r)=1.          (3)   (and therefore terminals in T ) but complex instances,
                    r=A→α,r∈R                         like HTML leaves or groups of leaves.
                                                        Content fragments are frequently targeted by vari-
  PCFGs have a normal form, called the Chomsky Nor-
                                                      ous probabilistic classiﬁers that produce probability es-
malForm(CNF),accordingtowhichanyruleinR          is
                                                      timates for labeling an observation with a terminal in
either A → BCor   A  ∈ b,whereA,  B and C  are non-                           
                                                      T , p(y|xi), y ∈ T ,where  p(y|xi)=1. Thetreean-
terminals and b is a terminal. The rewriting of XML                             y
annotations requires the binarization of source ranked notation problem can therefore be seen as a generalized
trees, often followed by an extension of the nonterminal version of probabilistic context-free parsing,wherethe
set and the underlying set of rules. This is a conse- input sequence is given by the probability distribution
quence of rewriting nodes with multiple children as a over a terminal set and the most probable annotation
sequence of binary nodes. The binarization rewrites any tree requires maximizing the joint probability in (2).
rule A → BCDas two rules    A →  BP  and P →  CD,       A similar generalization of probabilistic parsing takes
where P is a new non-terminal.                        place in speech recognition. In the presence of a noisy
  A PCFG deﬁnes a joint probability distribution over channel for speech streams, parsing from a sequence of
Y and D, random  variables over all possible sequences words is replaced by parsing from a word lattice, which
of terminals and all possible parses, respectively. Y and is a compact representation of a set of sequence hypothe-
D are clearly not independent, because a complete parse ses, given by conditional probabilities obtained by spe-
speciﬁes exactly one or few terminal sequences. We de- cial acoustic models from acoustic observations [4].
ﬁne the function p(y,d) of a given terminal sequence
y ∈ Y and a parse d ∈ D as the product of the p values 4  Content classiﬁer
for all of the rewriting rules R(y,d)usedinS⇒y.We     To produce terminal estimates for the observations xi,
also consider the case where d does not actually corre- we adopt the maximum entropy framework, according to
spond to y,                                           which the best model for estimating probability distribu-
                                                    tions from data is the one that is consistent with certain
                       p(r),  if d is a parse of y
   p(y,d)=      r∈R(y,d)                              constraints derived from the training set, but otherwise
              0,              otherwise.              makes the fewest possible assumptions [1]. The distri-
                                                      bution with the fewest possible assumptions is one with
  The values of p are in the closed interval [0; 1]. In
                                                      the highest entropy, and closest to the uniform distri-
the cases where d is a parse of y,allp(r) values in the
                                                      bution. Each constraint expresses some characteristic
product will lie in the half open interval (0; 1], and so
                                                      of the training set that should also be present in the
will the product. In the other case, 0 is in [0; 1] too.
                                                     learned distribution. The constraint is based on a binary
However, it is not always the case that p(y,d)=1.
                                     d,y              feature, it constrains the expected value of the feature
  The PCFG   training takes as evidence the corpus of in the model to be equal to its expected value in the
terminal sequences y with corresponding parses d from training set.
the training set S. It associates with each rule an ex- One important advantage of maximum entropy models
pected probability of using the rule in producing the cor- is their ﬂexibility, as they allow the extension of the rule
pus. In the presence of parses for all terminal sequences, system with additional syntactic, semantic and prag-
each rule probability is set to the expected count nor- matic features. Each feature f is binary and can depend
malized so that the PCFG constraints (3) are satisﬁed: on y ∈ T and on any properties of the input sequence
                        count(A →  α)                 x. In the case of tree annotation, we include the content
        p(A →  α)=                       .           features that express properties on content fragments,
                             count(A → β)
                      A→β∈R                           like f1(x, y)=“1ify is title and x’s length is less then
3.1   Generalized probabilistic parsing               20 characters, 0 otherwise”, as well as the structural and
                                                      layout features that capture the HTML context of the
PCFGs are used as probabilistic models for natural lan- observation x, like f2(x, y)=“1 if y is author and x’s
guages, as they naturally reﬂect the “deep structure” father is span,0otherwise”.
of language sentences rather than the linear sequences  With the constraints based on the selected features
of words. In a PCFG   language model, a ﬁnite set of  f(x, y), the maximum entropy method attempts to max-
words serve as a terminal set and production rules for imize the conditional likelihood of p(y|x) which is repre-
non-terminals express the full set of grammatical con- sented as an exponential model:
structions in the language. Basic algorithms for PCFGs                                        
that ﬁnd the most likely parse d for a given sequence y or              1        
choose rule probabilities that maximize the probability      p(y|x)=        exp     λα · fα(x, y) ,   (4)
                                                                      Zα(x)
of sentence in a training set, represent [6] eﬃcient ex-                          α
tensions of the Viterbi and Baum-Welsh algorithms for where Zα(x) is a normalizing factor to ensure that all
hidden Markov models.                                 the probabilities sum to 1,                                                        (0.3) Book → AU Section     (0.7) Book → AU SE
                                                             →                           →
                                                      (0.4) SE  Section Section   (0.6) SE  Section SE
                                                        (0.8) Section → TI ELS      (0.2) Section → TI EL
         Zα(x)=      exp     λαfα(x, y) .       (5)     (0.4) ELS → EL EL           (0.6) ELS → EL ELS
                  y        α                            (1.0) AU → author           (1.0) TI → title
                                                        (0.8) EL → para             (0.2) EL → footnote.
  For the iterative parameter estimation of the Max-
imum  Entropy exponential models, we use one of the     Assume now that we test the content classiﬁer C and
quasi Newton methods, namely the Limited Memory       PCFG   G on a sequence of ﬁve unlabeled observations
BFGS method, which is observed to be more eﬀective    x = {x1,...,x5}. Let the classiﬁer C estimate the prob-
than the Generalized Iterative Scaling (GIS) and Im-  ability for terminals in T as given in the following table:
proved Iterative Scaling (IIS) for NLP and IE tasks [7].               x1     x2    x3    x4     x5
                                                            author     0.3    0.2   0.1   0.1    0.2
                                                            title      0.4   0.4    0.3   0.3   0.3
5   Sequential tree annotation                              para       0.1    0.2   0.5   0.2    0.2
                                                            footnote   0.2    0.2   0.1   0.4    0.2
We use pairs (x, y) from triples (x, y,d) of the training
set S to train the content classiﬁer C and pairs (y,d)  According to the above probability distribution, the
to choose rule probabilities that maximize the likelihood most probable terminal sequence ymax is composed of
for the instances in the training set. C predicts the ter- the most probable terminals for all xi, i =1,...,5.
                                                           ’title title para footnote title’
minal probabilities p(y|x) for any observation x, while It is                                  with prob-
                                                                                |         ·    max|
the inside-outside algorithm can ﬁnd the parse d of the ability p(ymax)=p(ymax  x)=Πi       p(yi   xi)=
                                                         ·    ·    ·   ·
highest probability for a given terminal sequence y.  0.4  0.4 0.5  0.4 0.3=0.0096. However,    ymax  has
  By analogy with speech recognition, there exists a  no corresponding parse tree in G. Instead, there exist
naive, sequential method to combine the two compo-    two valid annotation trees for x,(y1,d1)and(y2,d2),
                                                      as shown in Figure 1.   In Figure 1.b, the terminal
nents C and G for computing a tree annotation for se-              ‘author title para title para’
quence x.First,fromC’s estimates  p(y|x), we deter-   sequence y2=                                   with
                                                      the parse d2=Book(AU SE(Section (TI EL) Section (TI
mine the (top k) most probable sequences ymax,j for x,                                        |
j =1,...,k. Second, we ﬁnd the most probable parses   EL))) maximizes the joint probability p(y,dx,G), with
                                                      p(y2)=0.3  · 0.4 · 0.5 · 0.3 · 0.2=0.0036, and
for all ymax,j, dmax,j = argmax p(d|ymax,j,G); and ﬁ-
                          d                           p(d2)=p(Book  → AU SE)  · p(AU →  author) ×
nally, we choose the pair (ymax,j,dmax,j)thatmaximizes      p(SE  → Section Section) · p(Section → TI EL) ×
                    ×                                                     2               2
the product p(ymax,j) p(dmax,j).                            p(TI →  title)  · p(EL → para)  ×
  The sequential method works well if the noise level       p(Section → TI EL)
is low (in speech recognition) or if the content classiﬁer =0.7 · 1.0 · 0.4 · 0.2 · 1.02 · 0.82 · 0.2=0.007172.
(in the tree annotation) is accurate enough in predicting                                 −5
                                                      Jointly, we have p(y2)×p(d2) ≈ 2.58·10 . Similarly, for
terminals y for xi. Unfortunately, it gives poor results
                                                      the annotation tree in Figure 1.a, we have p(y1)×p(d1)=
once the classiﬁer C is far from 100% accuracy in y pre-    ·          ≈     ·  −6
dictions, as it faces the impossibility of ﬁnding any parse 0.0048 0.0018432 8.85 10 .
for all the top k most probable sequences ymax,j.
                                                              Book           a)   b)      Book
                                                                  Section
                                                                                               SE
Example.    Consider an example target schema given                   ELS
by the following DTD:
                                                                          ELS             Section Section
                                                        d                         d
  <!ELEMENT     Book     (author, Section+)>             12
  <!ELEMENT     Section  (title, (para | footnote)+)>      AU  TI  EL   EL   EL      AUTI    EL  TI  EL
  <                                  >
   !ELEMENT     author   (#PCDATA)                      y author title para footnote para y author title para title para
  <!ELEMENT     title    (#PCDATA)>                      1                         2
  <                                  >
   !ELEMENT     para     (#PCDATA)                      x  x1  x2  x3   x4  x5    x  x1  x2  x3  x4  x5
  <!ELEMENT     footnote (#PCDATA)>
                                                       Figure 1: Tree annotations for the example sequence.
  The reduction  of the above schema deﬁnition   to
the Chomsky Normal Form    will introduce extra non-
terminals, so we get the PCFG    G  =(T,N,S,R),
where the terminal set is T ={author, title, para,
footnote}, the nonterminal set is N= {Book, Author,   6   The most probable annotation tree
SE, Section, TI, ELS, EL}, S=Book,andR     includes   As the sequential method fails to ﬁnd the most probable
twelve production rules.                              annotation tree, we try to couple the selection of termi-
  Assume that we have trained the content classiﬁer C nal sequence y for x with ﬁnding the most probable parse
and the PCFG  G and have obtained the following prob- d for y, such that (y,d) maximizes the probability prod-
abilities for the production rules in R:              uct p(d|y,G)· p(y|x) in (2). For this goal, we extend thebasic inside-outside algorithm for terminal PCFGs [6]. 7  Experimental results
We redeﬁne the inside probability as the most probable
joint probability of the subsequence of y beginning with We have tested our method for XML annotation on two
index i and ending with index j, and the most proba-  collections. One is the collection of 39 Shakespearean
                                             j                                                      1
ble partial parse tree spanning the subsequence yi and plays available in both HTML and XML format.    60
rooted at nonterminal A:                              scenes with 17 to 189 leaves were randomly selected for
                                                      the evaluation. The DTD fragment for scenes consists of
                                  j     j             4 terminals and 6 non-terminals. After the binarization,
      βA(i, j)=maxA,yj  p(Ai,j ⇒ yi ) · p(yi |x). (6)
                      i                               the PCFG in CNF contains 8 non-terminals and 18 rules.
                                                        The second collection, called TechDoc, includes 60
  The inside probability is calculated recursively, by tak-                                   2
ing the maximum over all possible ways that the nonter- technical documents from repair manuals. The tar-
minal A could be expanded in a parse,                 get documents have a ﬁne-grained semantic granularity
                                                      and are much deeper than in the Shakespeare collection;
                                                      the longest document has 218 leaves. The target schema
                                          q
 βA(i, j)=  maxi≤q≤j  p(A →  BC) · p(B ⇒ yi ) ×       is given by a complex DTD  with 27 terminals and 35
                           j        j                 nonterminals. The binarization increased the number
                   p(C ⇒  y   ) · p(y |x).
                           q+1      i                 of non-terminals to 53. For both collections, a content
  To proceed further, we make the  independence as-   observation refers to a PCDATA leaf in HTML.
sumption about p(y|x), meaning that for any q, i ≤ q ≤  To evaluate the annotation accuracy, we use two met-
             j         q       j
j, we have p(yi |x)=p(yi |x) · p(yq+1|x). Then, we can rics. The terminal error ratio (TER) is similar to the
rewrite the above as follows                          word error ratio used in natural language tasks; it mea-
                                                      sures the percentage of correctly determined terminals in
                                        q             test documents. The second metric is the non-terminal
βA(i, j)=maxi≤q≤j   p(A →  BC) · p(B ⇒ yi ) ×     (7) error ratio (NER) which is the percentage of correctly
                          j       q       j           annotated sub-trees.
                  p(C ⇒ yq+1) · p(yi |x) · p(yq+1|x)(8)
                                                        As content classiﬁers, we test with the maximum en-
         = maxi≤q≤j p(A → BC)  · βB(i, q) · βC (q +1,j)(9)
                                                      tropy (ME) classiﬁer. For the ME model, we extract 38
  The recursion is terminated at the βS (1,n)whichgives content features for each observation, such as the number
the probability of the most likely tree annotation (y,d), of words in the fragment, its length, POS tags, textual
                                                      separators, etc. Second, we extract 14 layout and struc-
                               n     n
        βS (1,n)=max p(S⇒y1     ) · p(y1 |x),         tural features include surrounding tags and all associated
                                                      attributes. Beyond the ME models, we use the maximum
where n is the length of both sequences x and y.      entropy Markov models (MEMM) which extends the ME
  The initialization step requires some extra work, as we with hidden Markov structure and terminal conditional
select among all terminals in T being candidates for yk: features [8]. The automaton structure used in MEMM
                                                      has one state per terminal.
        βA(k, k)=maxy   p(A →  yk) · p(yk|x).  (10)
                       k                                In all tests, a cross-validation with four folds is used.
  It can be shown that the redeﬁned inside function con- ME and MEMM   were ﬁrst tested alone on both col-
verges to a local maximum in the (Y,D) space. The extra lections. The corresponding TER values for the most
work during the initialization step takes O(n ·|T |·|N|) probable terminal sequences ymax serve a reference for
time which brings the total complexity of the extended methods coupling the classiﬁers with the PCFG. When
IO algorithm to O(n3 ·|N| + n ·|T |·|N|).             coupling the ME classiﬁer with the PCFG, we test both
  The independence assumption established above rep-  the sequential and joint methods. Additionally, we in-
resents the terminal conditional independence, p(y|x)= cluded a special case MEMM-PCFG where the content
n
  i=1 p(yi|x) and matches the Naive Bayes assumption. classiﬁer is MEMM  and therefore the terminal condi-
The assumption is frequent in text processing; it sim- tional independence is not respected.
pliﬁes the computation by ignoring the correlations be- The results of all the tests are collected in Table 1.
tween terminals. Here however it becomes a require-   The joint method shows an important advantage over
ment for the content classiﬁer. While PCFGs are as-   the sequential method, in particular in the TechDoc case,
sumed to capture all (short- and long- distance) re-  where the ME content classiﬁer alone achieves 86.23%
lations between terminals, the extended inside algo-  accuracy and the joint method reduces the errors in ter-
rithm (9)-(10) imposes the terminal conditional inde- minals by 1.36%. Instead, coupling MEMM    with the
pendence when building the probabilistic model. This  PCFG reports a decrease of TER values and a much less
impacts the feature selection for the maximum entropy important NER increase.
model, by disallowing features that include terminals of
neighbor observations yi−1, yi+1,etc,asinthemaxi-
mum entropy extension with HMM  and CRF models  [8;      1http://metalab.unc.edu/bosak/xml/eg/shaks200.zip.
5].                                                      2Available from authors on request.