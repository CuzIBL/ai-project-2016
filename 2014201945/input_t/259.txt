              Automated Generation of Understandable Contingency Plans 

                               Max Horstmann and Shlomo Zilberstein 
                                       University of Massachusetts 
                                    Department of Computer Science 
                                       Amherst, MA 01003, U.S.A. 
                                  {horstman, zilberstein} @cs.umass.edu 

                     Abstract                            We examine the relationship between policies and contin•
                                                       gency plans and define a precise measure of complexity of 
    Markov Decision Processes (MDPs) and contin•       contingency plans. Then, we introduce a technique for auto•
    gency planning (CP) are two widely used ap•
                                                       mated contingency plan generation and briefly describe our 
    proaches to planning under uncertainty. MDPs are 
                                                       experimental results. Finally, we conclude with a summary 
    attractive because the model is extremely general 
                                                       and future research directions. 
    and because many algorithms exist for deriving op•
    timal plans. In contrast, CP is normally performed 
    using heuristic techniques that do not guarantee op-
    timality, but the resulting plans are more compact 2 Formal Problem Description 
    and more understandable. The inability to present 
    MDP policies in a clear, intuitive way has limited 
                                                       A Markov Decision Process (MDP) with goal states is a tu•
    their applicability in some important domains. We 
                                                       ple (5, A, P, R, s , G), where S = S  x • • • x S  is a fac•
    introduce an anytime algorithm for deriving con•                  0               1          n
                                                       tored state space and S  is the (finite) domain of feature i. A 
    tingency plans that combines the advantages of the                     i
                                                       is a set of actions, P the transition probabilities, R the re•
    two approaches. 
                                                       ward expectations, so a known initial state and G a (possibly 
                                                       empty) set of absorbing terminal states. 
1 Introduction                                           A policy : S A is a mapping from states to ac•
Two closely related decision-theoretic planning paradigms tions. Following the notation in iLittman et al., 1998], a 
have emerged in the area of planning under uncertainty: Contingency Plan for an MDP is a tuple 
Markov Decision Processes (MDPs) have become a general where {V,E) is a directed graph with start state V. 
framework for planning and reinforcement learning. Contin•      associates and action with each node of the graph, 
gency Planning (CP) on the other hand emphasizes compact•        labels each edge with a set of states. 
ness and understandability of the solution. In both cases, an An interval label descriptor maps state sets to sets 
agent is manipulating an environment by performing actions of intervals over the feature domains defined as follows: 
with uncertain outcomes. In each move, the agent receives 
some reward and the overall goal is to maximize the cumula•
tive reward. 
  MDPs are solved by deriving a policy which maps domain The complexity of a contingency plan does not reflect 
states to actions, typically represented as a large table. Sev• the computational complexity of constructing it, but is a 
eral existing algorithms can construct optimal policies, but measure of how hard it is to understand. Formally: Com-
the results are not easy to visualize or understand. CP is an•                 • ABF • ALDS, where ABF is 
other widely used approach in stochastic domains which al• the average branching factor of the graph and ALDS is the 
lows plans to include branches that may depend on arbitrary average label descriptor size. The size of a single label de•
memory states.                                         scriptor is the number of constrained intervals. 
  It is clear why optimal plans are desirable, but the issue The value of a policy reflects the expected discounted re•
of understandability is less obvious. In fact, for some ap• turn when the agent selects action using n, starting from the 
plications, a plan represented as a large table mapping states initial state. It can be computed with standard algorithms such 
to actions may be perfectly suitable. However, the lack of as value iteration. Similarly, the value of a contingency plan 
clarity has limited the adoption of MDP planning in some ap• is the expected discounted return when the agent selects ac•
plication domains such as space exploration with unmanned tion using p, starting from the initial node. By considering the 
rovers. In this domain, communication with the agent is re• Markov chain induced by the state set S x V, the same meth•
stricted and due to high mission costs and associated risks, ods used to evaluate MDP policies can be used to evaluate a 
understanding and verifying plans are crucial.         contingency plan. 


1518                                                                                      POSTER PAPERS  3 Automated Contingency Plan Generation                       to this partition, perform a reachability analysis and merge 
 Our approach to automatically generate more understandable    the interval descriptors subsequently. We implemented a hill-
                                                               climbing algorithm that used some domain-specific heuristics 
 (less complex) contingency plans relies on solving first the 
                                                               for choosing a partition and performed random restarts, lead•
 underlying MDP, obtaining an optimal policy and creat•
                                                               ing to an interruptible anytime algorithm that performed well 
 ing an equivalent initial contingency plan from it: V =       on a simple set of test problems. 
                                                                  For instance, in a maze domain with stochastic move ac•
                                                               tions, the algorithm took advantage of bottlenecks, tunnels 
                                                               and alternative branches. In a simulated planetary rover do•
   By performing a depth-first search over the set of all reach- main with uncertain action durations, some hints for good 
 able edges between the state-node pairs in S x V, a reach•    split partitions lead to dramatically decreased complexity. 
 ability analysis can be applied to remove states from label   The resulting plan instructed the agent to perform certain ac•
 descriptors, edges from nodes and nodes from the graph.       tions until a resource (one feature of the factored state space) 
   Since a benefit of interval label descriptors is the ability drops below a certain threshold. This was very easy to under•
 to capture easily large sets of states and simplify the branch stand for humans unlike the optimal policy given in the form 
 conditions, n-dimensional boxes to describe them more com•    of a huge lookup-table. 
 pactly can be generated by merging intervals along the di•
 mensions of the state space. This is in particular very effec• 5 Conclusions 
 tive if the order of feature values corresponds to some natural 
 ordering.                                                     The main objective of this work has been to find solutions 
   By trading off optimality for clarity, interval descriptors for decision-theoretic planning problems that are optimal (or 
 can be merged even if there are a few states that make the    near-optimal) and also compact and understandable. We de•
 merge operator not admissible.                                fined a measure of the complexity of contingency plans, re•
   Most importantly, the node-split operator splits a node of  flecting their size, branching factor and the size of the label 
 the plan graph into two and adapts the incoming and outgoing  descriptors. The results we gained by experimenting with 
 edges according to a given partition of the state space. This several plan-transformation operators are encouraging and 
 gives contingency plans their real strength: The current node show the potential of automated generation of understandable 
 of the graph then represents partial information about the cur• contingency plans. 
 rent state ("context information"), so less details of the actual There are several interesting directions for future research. 
 state may be needed to make the branch decision on the next   First, a better measure of the plan complexity could be devel•
 step. For instance, with a state set S = {u, v, w, x, y, z) and oped. Second, additional operators could be added. Third, 
                                                               the language for representing edge labels could be enriched. 
 the partition S\ — {u,v,w}, S2 = {x,y,z}, the result of a 
 split operator is illustrated below:                          Finally, methods for finding good split partitions have to be 
                                                               found to create more reliable algorithms. The results reported 
                                                               in this paper provide a good framework for future exploration 
                                                               of these research directions. 

                                                               Acknowledgments 
                                                               Support for this work was provided in part by NASA under 
                                                               grant NAG-2-1463. Any opinions, findings, and conclusions 
                                                               or recommendations expressed in this material are those of 
                                                               the authors and do not reflect the views of NASA. 
 4 Experimental Results 
 It turns out that combining these operators can significantly References 
 reduce the complexity of a contingency plan. This leads to the [Bresina et al., 2002] J. Bresina, R. Dearden, N. Meulcau, S. Ra-
 following search problem: Given the initial contingency plan     makrishnan, D. Smith, and R. Washington. Planning Under Con•
 defined above, find a contingency plan with the lowest com•      tinuous Time and Resource Uncertainty: A Challenge for Al. 
 plexity, not giving up more that a certain fraction e of the op• Conference on Uncertainty in Artificial Intelligence, 2002. 
 timality of the policy n. Unfortunately, two major obstacles  [Feng and Hansen, 2002] Z. Feng and E.A. Hansen. Symbolic-
 make an exhaustive search intractable: The node-split oper•      Heuristic Search for Factored Markov Decision Processes. Eigh•
 ator is parameterized by a partition of the MDP state space,     teenth National Conference on Artificial Intelligence, 2002. 
 leading to a large number of possible splits and thus to a large 
                                                               [Hansen and Zilberstein, 2001] E.A. Hansen and S. Zilberstein. 
 branching factor. In addition, the computational complexity      LAO*:A Heuristic Search Algorithm that Finds Solutions with 
of reachability analysis is fairly high.                          Loops. Artificial Intelligence, 129(l-2):35-62, 2001. 
   As a result, we decided to relax the requirement of mini•
                                                               [Littman et al., 1998] M.L. Littman, J. Goldsmith, and M. Mund-
mizing complexity and developed a method to combine the 
                                                                  henk. The Computational Complexity of Probabilistic Planning. 
operators and automatically generate some understandable          Journal of Artificial Intelligence Research, 9:1-36, 1998. 
plans. Performance improvement can be obtained by choos•
ing a partition of the state space, split the nodes according 


 POSTER PAPERS                                                                                                      1519 