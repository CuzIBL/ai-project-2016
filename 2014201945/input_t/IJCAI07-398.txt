              The Value of Observation for Monitoring Dynamic Systems

           Eyal Even-Dar                   Sham M. Kakade                    Yishay Mansour∗
 Computer and Information Science    Toyota Technological Institute     School of Computer Science
     University of Pennsylvania            Chicago, IL, USA                 Tel Aviv University
    Philadelphia, PA 19104 USA               sham@tti-c.org                Tel Aviv, 69978, Israel
      evendar@seas.upenn.edu                                               mansour@cs.tau.ac.il

                    Abstract                          & Kanazawa, 1989] —  all of which share the Markov as-
                                                      sumption. Naturally, one would like to provide conditions
    We consider the fundamental problem of monitor-   as to when monitoring is possible when modelling errors are
    ing (i.e. tracking) the belief state in a dynamic sys- present. Such conditions can be made on either the dynam-
    tem, when the model is only approximately correct ics of the system (i.e. the transition between the states) or on
    and when the initial belief state might be unknown. the observability of the system. While the true model of the
    In this general setting where the model is (perhaps transition dynamics usually depends on the application itself,
    only slightly) mis-speciﬁed, monitoring (and con- the observations often depend on the user, e.g. one might be
    sequently planning) may be impossible as errors   able to obtain better observations by adding more sensors or
    might accumulate over time. We provide a new      just more accurate sensors. In this paper our main interest is
    characterization, the value of observation,which  in quantifying when the observations become useful and how
    allows us to bound the error accumulation.        it effects the monitoring problem.
    The value of observation is a parameter that gov-   Before we deﬁne our proposed measure, we give an illus-
    erns how much information the observation pro-    trative example of an HMM, where the value of information
    vides. For instance, in Partially Observable MDPs can vary in a parametric manner. Consider an HMM in which
    when it is 1 the POMDP is an MDP while for an     at every state the observation reveals the true state with prob-
    unobservable Markov Decision Process the param-   ability 1 −  and with probability  gives a random state. This
    eter is 0. Thus, the new parameter characterizes a can be thought of as having a noisy sensor. Intuitively, as
    spectrum from MDPs to unobservable MDPs de-       the parameter  varies from zero to one, the state monitoring
    pending on the amount of information conveyed in  become harder.
    the observations.                                   We introduce a parameter which characterizes how infor-
                                                      mative the observations are in helping to disambiguate what
                                                      the underlying hidden state is. We coin this parameter the
1  Introduction                                       value of observation. Our value of observation criterion tries
Many real world applications require estimation of the un- to quantify that different belief states should have different
                                                                                            L
known state given the past observations. The goal is to main- observation distributions. More formally, the 1 distance be-
tain (i.e. track) a belief state, a distribution over the states; tween any two belief states and their related observation dis-
in many applications this is the ﬁrst step towards even more tributions is maintained up to a multiplicative factor (which
challenging tasks such as learning and planning. Often the is at least the value of observation parameter).
dynamics of the system is not perfectly known but an approx- In this paper we use as an update rule a variant of the
imate model is available. When the model and initial state are Bayesian update. First we perform a Bayesian update given
perfectly known then state monitoring reduces to Bayesian our (inaccurate) model, and then we add some noise (in par-
inference. However, if there is modelling error (e.g. the tran- ticular, we mix the resulting belief state with the uniform dis-
sition model is slightly incorrect), then the belief states in the tribution). Adding noise is crucial to our algorithm as it en-
approximate model may diverge from the true (Bayesian) be- sures the beliefs do not become incorrectly overly conﬁdent,
lief state. The implications of such a divergence might be thus preventing the belief state from adapting fast enough to
dire.                                                 new informative information.
  The most popular dynamic models for monitoring some   Our main results show that if the model is only approxi-
unknown state are the Hidden Markov Model (HMM) [Ra-  mate then our modiﬁed Bayesian updates guarantee that the
biner & Juang, 1986] and extensions such as Partially Observ- true belief state and our belief state will not diverge — assum-
able Markov Decision Process [Puterman, 1994], Kalman Fil- ing the value of observation is not negligible. More speciﬁ-
ters [Kalman, 1960] and Dynamic Bayesian Networks [Dean cally, we show that if the initial state is approximately ac-
                                                      curate, then the expected KL-divergence between our belief
  ∗Supported in part by a grant from ISF and BSF.     state and the true belief state remains small. We also show

                                                IJCAI-07
                                                  2474that if we have an uninformative initial state (e.g., arbitrary the algorithm; Subsection 3.2 provides the main monitoring
initial belief state) we will converge to a belief state whose theorem; Subsection 3.3 proves the Theorem. In Section 4,
expected KL-divergence from the true belief state is small we show how to extend the results into the dynamic Bayesian
and will remain as such from then on. Finally, we extend our networks.
results to the setting considered in [Boyen & Koller, 1998],
where the goal is to compactly represent the belief state. The 2 Preliminaries
precision and rate of convergence depends on the value of
                                                      An Hidden Markov Model (HMM) is 4-tuple, (S, P, Ob, O),
observation.
                                                      where S is the set of states such that |S| = N, P is the
  One natural setting with an inaccurate model is when the
                                                      transition probability form every state to every state, Ob is
underlying environment is not precisely Markovian. For ex-
                                                      the observations set and O is the observation distribution in
ample, it might be that the transition model is slightly inﬂu-
                                                      every state. A belief state b is a distribution over the states
enced by some other extrinsic random variables. Given these
                                                      S such that b(i) is the probability of being at state si.The
extrinsic variables, the true transition model of the environ-
                                                      transition probabilities of the belief states are deﬁned accord-
ment is only a slightly different model each time step. This
                                                      ing to HMM  transition and observation probability, using a
is a case where we might like to model the environment as
                                                      Bayesian update.
Markovian, even at the cost of introducing some error, due to
                                                        For a belief state b(·), the probability of observing o is
the fact that transition model is not entirely Markovian. Our
                                                      O(o|b),where
results apply also to this setting. This is an encouraging re-               
sult, since in many cases the Markovian assumption is more           O(o|b)=    O(o|s)b(s).
of an abstraction of the environment, then the a precise de-                  s
scription.
Related Work. The work most closely related to ours is that After observing an observation o in belief state b(·), the up-
of Boyen and Koller (1998), where they considered monitor- dated belief state is:
ing in a Hidden Markov Model. In their setting, the environ-            O       O(o|s)b(s)
                                                                     (Uo b)(s)=
ment is (exactly) known and the agent wants to keep a com-                        O(o|b)
pact factored representation of the belief state (which may
                                                              O
not exactly have a factored form). Their main assumption is where Uo is deﬁned to be the observation update operator.
that the environment is mixing rapidly, i.e the error contract Also, we deﬁne the transition update operator T as,
by geometric factor after each time we apply the transition                  
                                                                     P                   
matrix operator. In contrast, we are interested in monitoring      (T b)(s)=     P (s ,s)b(s ).
when we have only an approximate environment model. Both                      s
our work and theirs assume some form of contraction where
                                                        We denote by bt the belief state at time t, where at time 0 it
beliefs tend to move closer to the truth under the Bayesian
                                                      is b . (We will discuss both the case that the initial belief state
updates — ours is through an assumption about the value of 0
                                                      is known and the case where it is unknown.) After observing
observation while their is through assumption about the tran-
                                                      observation ot ∈ Ob, the inductive computation of the belief
sition matrix. The main advantage of our method is that in
                                                      state for time t +1is:
many applications one can improve the quality of its observa-
                                                                        b      T P U Ob ,
tions, by adding more and better sensors. However, the mix-              t+1 =     ot t
ing assumption used by Boyan and Koller may not be alter-
able. Furthermore, in the ﬁnal Section, we explicitly consider where we ﬁrst update the belief state by the observation up-
                                                                                          o
their assumption in our setting and show how a belief state date operator according to the observation t andthenbythe
can be compactly maintained when both the model is approx- transition update operator. It is straightforward to consider
                                                                                     b
imate and when additional error accumulates from maintain- a different update order. Therefore, t+1 is the distribution
                                                                                      {o ,o ,...o }
ing a compact factored representation.                over states conditioned on observing 0 1  t  and on
                                                                             b
  Particle Filtering [Doucet, 1998] is a different monitoring the initial belief state being 0.
approach, in which one estimates the current belief state by
making a clever sampling, where in the limit one observes the 3 Approximate Monitoring
true belief state. The major drawback with this method is in We are interested in monitoring the belief state in the case
the case of a large variance where it requires many samples. where either our model is inaccurate or we do not have the
A combination of the former two methods was considered by correct initial belief state (or both). Let us assume that an
[Ng et al., 2002].                                    algorithm has access to a transition matrix P and an observa-
                       [                   ]
  Building on the work of Boyen & Koller, 1998 and the tion distribution O, which have error with respect to the true
              [               ]
trajectory tree of Kearns et al., 2002 , McAllester and Singh models. The algorithm’s goal is to accurately estimate the
(1999) provides an approximate planning algorithm. Similar            t                  ˆb
extensions using our algorithm may be possible.       belief state at time , which we denote by t.
                                                        For notational simplicity, we deﬁne Eo∼b = Eo∼O ·|b .
  Outline. The outline of the paper is as follows. In Sec-                                           ( )
                                                      When  P and P are clear from the context, we deﬁne T to
tion 2 we provide notation and deﬁnitions. Section 3 is the              b
main section of the paper and deals with monitoring and is be T P and T to be T P .WhenO and O are clear from the
                                                                               O            Ob
composed from several subsections; Subsection 3.1 describes context, we deﬁne Uo to be Uo and Uo to be Uo .

                                                IJCAI-07
                                                  2475  Our main interest is the behavior of                3.1  The Belief State Update
                              
                                                      Now we present the belief state update. The naive approach
                  E  KL   b ||ˆb
                         ( t t)                       is to just use the approximate transition matrix Pˆ and the ap-
                                                      proximate observation distribution Oˆ. The problem with this
where the expectation is taken with respect to observation se-
                                                      approach is that the approximate belief state might place neg-
quences {o ,o ,...ot− } drawn according to the true model,
         0  1      1                                  ligible probability on a possible state and thus a mistake may
   b     ˆb                         t
and t and t are the belief states at time , with respect to be irreversible.
these observation sequences.                            Consider the following update operator T˜. For each states
  In order to quantify the accuracy of our state monitoring, s ∈ S,
we must assume some accuracy conditions on our approxi-
                                                               (T˜)(s)=(1−  U )(Tˆ)(s)+U Uni(s),
mate model. The KL-distance is the natural error measure.   Uni
The assumptions that we make now on the accuracy of the where   is the uniform distribution. Intuitively, this update
                                                              U˜
model will later be reﬂected in the quality of the monitoring. operator mixes with the uniform distribution, with weight
                                                      U , and thus always keeps the probability of being in any state
Assumption 3.1 (Accuracy) For a  given HMM   model    bounded away from zero. Unfortunately, the mixture with
(S, P, Ob, O),an(T ,O) accurate model is an HMM     the uniform distribution is an additional source of inaccuracy
(S, P, Ob, O), such that for all states s ∈ S,        in the belief state, which our analysis would latter have to
                                                      account for.
                       
            KL(P  (·|s)||P (·|s)) ≤ T                  The belief state update is as follows.
            KL  O ·|s ||O ·|s ≤     .                                  ˆb    T˜Uˆ ˆb ,
               (  ( )   (  ))      O                                      t+1 =   ot t                (1)
                                                      where ˆbt is our previous belief state.
  Next we deﬁne the value of observation parameter.
Deﬁnition 3.1 Given an observation distribution O,letM O 3.2 Monitoring the belief state
be the matrix such that its (o, s) entry is O(o|s).TheValue In this subsection we present our main theorem, which relates
             γ                       Mx             the accuracy of the belief state to our main parameters: the
of Observation, , is deﬁned as infx:x1=1 1 and it is
in [0, 1].                                            quality of the approximate model, the value of observation,
                                                      and the weight on the uniform distribution.
  Note that if the value of observation is γ, then for any two
                                                      Theorem 3.2 At time t let ˆbt be the belief state updated ac-
belief states b1 and b2,
                                                      cording to equation (1), bt be the true belief state, Zt =
   b − b   ≥O  ·|b  − O ·|b   ≥ γb − b   .
     1   2 1      (  1)   (  2) 1      1   2 1        E  KL(bt||ˆbt) , and γ be the value of observation. Then
                                                                                        2
where the ﬁrst inequality follows from simple algebra.                Zt+1 ≤ Zt +  − αZt ,
  The parameter γ plays a critical rule in our analysis. At                     √              γ2
                                                      where   T   U    N    γ  O and α      2 N .
                γ            b − b     O ·|b  −           =    +    log  +3            =  2log
the extreme, when =1we have    1   2 1 =    (  1)                                               U
O ·|b                                                                 ˆ            
 (  2) 1. Note that this deﬁnition is very similar to def- Furthermore, if b0 − b01 ≤ α then for all times t:
inition of the Dobrushin coefﬁcient, supb ,b P (b , ·) −                 
                                     1 2     1                                  log N √
P b , ·                                                                            U
 ( 2  ) 1 and it is widely used in the ﬁltering literature           Zt ≤     =         2
[Moral, 2004]. We now consider some examples.                               α      γ
     γ                b
  Let  be 1 and consider 1 having support on one state and Also, for any initial belief states b and ˆb , and δ>0,there
b                           b − b                                               0      0
 2 on another state. In this case 1 2 1 =2and there-  exists a time τ(δ) ≥ 1, such that for any t ≥ τ(δ) we have
    O ·|b  − O ·|b                                                  
fore  (  1)    (  2) 1 =2, which implies that we have                              N √
                                                                                 log U
a different observations from the two states. Since this holds   Zt ≤      + δ =        2 + δ
for any two states, it implies that given an observation we can          α         γ
uniquely recover the state. To illustrate the value observation The following corollary now completely speciﬁes the algo-
characterization, in POMDP terminology for γ =1we have rithm by providing a choice for U , the weight of the uniform
a fully observable MDP as no observation can appear with distribution.                     
                                                                               ˆ              
positive probability in two states. At the other extreme, for an Corollary 3.3 Assume that b0 − b01 ≤ α and U =
unobservable MDP, we can not have a value of γ>0 since T                  t
                                                      log N . Then for all times ,
O(·|b1) − O(·|b2)1 =0for any two belief states b1 and b2.                  N  
                                                                                        √
                                                                         6log T
  Recall the example in the Introduction where at every state      Zt ≤           T + γ O
the observation reveals the true state with probability 1− and            γ
                                                                             
with probability gives a random state. Here, it is straight- Proof: With the choice of U ,wehave:
                 γ    −                                    √               √               √
forward to show that is 1 . Hence, as approaches 0,the                    γ    ≤        γ   .
value of observation approaches 1.                             2 =    4 T +6    O   3   T +     O
  We now show that having a value of γ bounded away from And,
                                                                    N       N  log N       N
zero is sufﬁcient to ensure some guarantee on the monitor-       log   =log         ≤  2log   ,
ing quality, which improves as γ increases. Throughout, the         U         T          T
paper we assume that γ>0.                             which completes the proof.                       

                                                IJCAI-07
                                                  2476                                                                  
3.3  The Analysis                                       Note that Zt is a positive super-martingale and therefore
                                                                                                  Z
We start by presenting two propositions useful in proving the converges with probability 1 to a random variable .The
                                                                  Z                      
theorem. These are proved later. The ﬁrst provides a bound expectation of cannot be larger than α , since whenever
                                                                      
on the error accumulation.                            Z  is larger than α its expectation in the next timestep is
                                                                                                    Z ≥
Proposition 3.4 (Error Accumulation) For every belief states strictly less than its expected value. Since by deﬁnition t
      ˆ                              ˆ         ˆ     Zt then, regardless the our initial knowledge on the belief
bt and bt and updates bt = TUot bt and bt = T˜Uot bt,
                     +1               +1              state, the monitoring will be accurate and results in error less
we have:                                                     
                                                    than   α .                                       
              ˆ                 ˆ
 Eot KL(bt   ||bt )   ≤  KL(bt||bt)+U  log N + O
           +1   +1                                    Error Accumulation Analysis
                                       
                         −KL(O(·|bt)||O(·|ˆbt))       In this subsection we present a series of lemmas which prove
                                                      Proposition 3.4. The lemmas bound the difference between
  The next proposition lower bounds the last term in Propo- the updates in the approximate and true model.
sition 3.4. This term, which depends in the value of obser- We start by proving the Lemma 3.8 provided at the begin-
vation, enables us to ensure that the two belief states will not ning of the Subsection
diverge.
                                                                                    b     b
Proposition 3.5 (Value of Observation) Let γ be the value of Lemma 3.6 For every belief states 1 and 2,
observation, b and b be belief states such that b (s) ≥ μ                
            1     2                       2                     KL(Tb   ||Tb ) ≤ KL(b ||b )+T
for all s.Then                                                         1   2         1  2
                                           
                                                       
                                             2          Proof: Let us deﬁne the joint distributions p1(s ,s)=
                             1  γKL(b1||b2)                                        
    KL  O  ·|b ||O ·|b  ≥                            P (s, s )b1(s) and p2(s ,s)=Pˆ(s, s )b2(s). Throughout the
       (  (  1)  (  2))                1
                             2     log μ              proof we speciﬁcally denote s to be the (random) ’ﬁrst’ state,
                                √                     and s to be the (random) ’next’ state. By the chain rule for
                            −3γ   O + O
                                                      relative entropy, we can write:
  Using these two propositions we can prove our main theo-
                                                                                                   
rem, Theorem 3.2.                                      KL(p1||p2)=KL(b1||b2)+Es∼b[KL(T        (·|s)||T(·|s)]
                                       U˜
  Proof of Theorem 3.2: Due to the fact that mixes with            ≤   KL(b1||b2)+T
the uniform distribution, we can take μ = U /N . Combining
Propositions 3.4 and 3.5, and recalling the deﬁnition of ,we where the last line follows by Assumption 3.1.
                                                        Let p (s|s) and p (s|s) denote the distributions of the
obtain that:                                               1          2
                                                      ﬁrst state given the next state, under p and p respectively.
        E    KL  b  ||ˆb  |o   , ..., o ≤                                              1     2
          ot    ( t+1  t+1) t−1    0                  Again, by the chain rule of conditional probabilities we have,
                                       
                                         2                                        
            KL(bt||ˆbt)+ − α KL(bt||ˆbt)                 KL(p1||p2)=KL(Tb1||Tab2)
                                                                                                  
                                                                          +Es∼Tb[KL(p1(s|s )||p2(s|s )]
  By taking expectation with respect to {o ,o ,...ot− },we
                                   0 1       1                        ≥   KL  Tb ||Tb ,
have:                                                                        (  1    2)
                                       
                                         2            where the last line follows from the positivity of the relative
       Zt    ≤   Zt +  − αE  KL(bt||ˆbt)
         +1                                           entropy. Putting these two results together leads to the claim.
                            2                          
             ≤   Zt +  − αZ ,
                            t                           The next lemma bounds the effect of mixing with uniform
where the last line follows since, by convexity,      distribution.
                                  
                     2                    2
                                                      Lemma 3.7  For every belief states b1 and b2
      E    KL(bt||ˆbt)  ≥  E  KL(bt||ˆbt)  ,

                                                       KL(Tb1||Tb˜ 2) ≤   (1 − U )KL(Tb1||Tbˆ 2)+U log N
which proves the ﬁrst claim in the theorem.
  We proceed with the case where the initial belief state is Proof: By convexity,
                    ˆ              
good in the sense that b0 − b01 ≤ α .Thenwehave
                                                           KL(Tb   ||Tb˜ ) ≤  (1 − U )KL(Tb ||Tbˆ )
   Z                                 Z  − αZ2                    1   2                     1   2
that t is always less than α . The function t t+                                KL   Tb ||Uni ·
             −   Z α                     Z  ≤                                 + U    (  1     ( ))
has derivative1 2 t , which is positive when t  α .
                                                                        ≤   (1 − U )KL(Tb ||Tbˆ )
Since Zt at   is mapped to   ,theneveryZt ≤      is                                          1   2
            α               α                  α                                     N,
mapped to a smaller value. Hence the Zt will always remain                    + U log
        
below   α .
  We conclude with the subtle case of unknown initial belief where the last line uses the fact that the relative entropy be-
state, and deﬁne the following random variable        tween any distribution and the uniform one is bounded by
                                                    log N.                                           
                     Z ,Z>        
                    t     t     α                     Combining these two lemmas we obtain the following
              Zt =      
                       α , Otherwise                  lemma on the transition model.

                                                IJCAI-07
                                                  2477Lemma 3.8  For every belief states b1 and b2,           Proof of Proposition 3.4: Using the deﬁnitions of updates
                                                      and the previous lemmas:
                                                                             
     KL(Tb1||Tb˜ 2) ≤ KL(b1||b2)+T + U log N
                                                         E       KL  b  ||ˆb
                                                           ot∼bt    ( t+1  t+1)
  After dealing with transition model, we are left to deal with                         
                                                                                     ˆ
the observation model. We provide an analog lemma with      =  Eot∼bt KL(TUot  bt||T˜Uot bt)
regards to the observation model.                                                    
                                                                                  ˆ
                                                            ≤  Eot∼bt KL(Uot  bt||Uot bt) + T + U log N
Lemma 3.9  For every belief states b1 and b2,
                                                                                          
                                                            ≤  KL(bt||ˆbt)+O − KL(O(·|bt)||O(·|ˆbt))
    E         KL  U  b ||U b
      o∼O ·|b1    ( o 1  o 2)
         (  )                                                  +T + U log N
                                       
        ≤ KL(b1||b2)+O  −  KL(O(·|b1)||O(·|b2))      where the ﬁrst inequality is by Lemma 3.8 and the second is
                                                      by Lemma 3.9 This completes the proof of Proposition 3.4.
  Proof: First let us ﬁx an observation o.Wehave:                                                      
                                 U b  s
KL  U b ||U b         U  b s      o 1( )             Value of Observation Proposition - The Analysis
   ( o 1   o 2)=         o 1( )log 
                     s            Uob2(s)             The following technical lemma is useful in the proof and it
                                                               L
                                  O o|s b s /O  o|b  relates the 1 norm to the KL divergence.
                        U b   s      (  ) 1( )  (  1)
                =      ( o 1)( )log                 Lemma 3.10  Assume that ˆb(s) >μfor all s and that μ< 1 .
                     s             O(o|s)b2(s)/O(o|b2)                                                 2
                                                      Then
                                  b  s      O  o|b
                        U b   s     1( ) −     (  1)                KL  b||ˆb ≤b − ˆb   1
                =      ( o 1)( )log b s   log                         (   )         1 log μ
                     s              2( )     O(o|b2)
                      
                                     O(o|s)             Proof: Let A be the set of states where b is greater than ˆb,
                    +    (Uob )(s)log
                             1                       i.e. A = {s|b(s) ≥ ˆb(s)}.So
                       s             O(o|s)
                                                                             b s             b s
                                                      KL  b||ˆb        b s     ( ) ≤    b s     ( )
where the last line uses the fact that O o|b and O o|b are ( )=        ( )log           ( )log
                                (  1)     (   2)                              ˆb(s)            ˆb(s)
constants (with respect to s).                                       s               s∈A
                                                                                           
  Now let us take expectations. For the ﬁrst term, we have:                          b(s)             b(s)
                                                                =      (b(s) − ˆb(s)) log +    ˆb(s)log
                                                                                   ˆb(s)            ˆb(s)
                         b s                                       s∈A                     s∈A
    E         U  b  s     1( )                                           
     o∼b1     ( o 1)( )log                                             1
                          b2(s)                                 ≤   log     (b(s) − ˆb(s))
            s                                                        μ
                                                                       s∈A
                          O(o|s)        b (s)                                                  

             O  o|b             b  s     1                                          b s − ˆb s
        =      (  1)     O  o|b  1( )logb  s                                          ( )   ( )
                           (  1)         2( )                       +    ˆb(s)log 1+
           o           s                                                                ˆb s
                          b  s                                       s∈A                ( )
             O  o|s b s     1( )   KL  b ||b                                             
        =      (  ) 1( )log     =     ( 1  2)                          1
                           b (s)                                ≤            b s − ˆb s      b s − ˆb s
           s,o              2                                       log μ   ( ( )   ( )) +   ( ( )  ( ))
                                                                       s∈A             s∈A
where the last step uses the fact that o O(o|s)=1. Simi-
                                                                    1      1     b − ˆb
larly, for the third term, it straightforward to show that:     =      log μ +1         1
                                                                  2
                          O o|s                      where we have used the concavity of the log function and
     E         U  b  s      (   )                         
      o∼b1     ( o 1)( )log                                                  1
                                                     that  s∈A(b(s)−ˆb(s)) = b−ˆb . The claim now follows
             s             O(o|s)                                            2      1
                                                                     μ<   1                            
                       O  o|s b s    O  o|s         using the fact that 2 .
               O o|b       (  ) 1( )     (  )           We are now ready to complete the proof. We will make
         =      (  1)      O o|b    log 
            o          s     (  1)     O(o|s)         use of Pinsker’s inequality, which relates the KL divergence
                                                         L                                       p    q
                                                     to the 1 norm. It states that for any two distributions and
         = Es∼b  KL(O(·|s)||O(·|s))
                                                                               1         2
                                                                    KL(p||q) ≥  (p − q1) .          (2)
  For the second term,                                                         2
                                                      Proof of Proposition 3.5: By Pinsker’s inequality and As-
                O  o|b
                  (  1)                              sumption 3.1, we have
    Eo∼b1  − log         = KL(O(·|b  )||O(·|b ))                           
                O o|b              1      2                                                      √
                  (  2)                                                                  
                                                        O(·|s) − O(·|s)1 ≤ 2KL(O(·|s)||O(·|s)) ≤  2O
directly from the deﬁnition of the relative entropy. The lemma for all states s ∈ S. Using the triangle inequality,
follows from Assumption 3.1.                     
                                                                                         
  Now we are ready to prove Proposition 3.4.          O(·|b)−O(·|ˆb)1 ≤O(·|b)−O(·|ˆb)1+O(·|ˆb)−O(·|ˆb)1 .

                                                IJCAI-07
                                                  2478