                 Inductive Learning in Less Than One Sequential Data Scan 

          Wei Fan, Haixun Wang, and Philip S. Yu                              Shaw-Hwa Lo 
                    IBM T.J.Watson Research                   Statistics Department, Columbia University 
                      Hawthorne, NY 10532                                 New York, NY 10027 
               {weifan,haixun,psyu}@us.ibm.com                           slo@stat.Columbia.edu 


                        Abstract                               dataset less than once and has been empirically shown to have 
                                                               higher accuracy than a single classifier. 
     Most recent research of scalable inductive learn•
                                                                 Based on averaging ensemble, we propose a statistically-
     ing on very large dataset, decision tree construc•
                                                               based multiple model inductive learning algorithm that scans 
     tion in particular, focuses on eliminating memory 
                                                               the dataset less than once. Previous research [Fan et al, 
     constraints and reducing the number of sequen•
                                                               2002bl on averaging ensemble has shown that it is more 
     tial data scans. However, state-of-the-art decision 
                                                               efficient and accurate than both bagging and meta-learning. 
     tree construction algorithms still require multiple 
                                                               In this paper, we apply Hoeffding inequality to estimate the 
     scans over the data set and use sophisticated control 
                                                               probability that the partial and complete models are equal in 
     mechanisms and data structures. We first discuss a 
                                                               accuracy. When the probability is higher than a threshold, 
     general inductive learning framework that scans the 
                                                               the algorithm stops model construction and returns the cur•
     dataset exactly once. Then, we propose an exten•
                                                               rent model, resulting in less than one sequential scan of the 
     sion based on Hoeffding's inequality that scans the 
                                                               dataset. Our objective is completely different from [Hulten 
     dataset less than once. Our frameworks are appli•
                                                               and Domingos, 2002] on determining whether to change the 
     cable to a wide range of inductive learners. 
                                                               shape of a decision tree. Unlike previous research [Hulten 
                                                               and Domingos, 2002; Gehrke et a/., 1999], our algorithm is 
1 Introduction                                                 not limited to decision tree, but is applicable to a wide range 
                                                               of inductive learners. When it is applied to decision tree 
Most recent research on scalable inductive learning over very 
                                                               learning, the accuracy is higher than a single decision tree. 
large dataset focuses on eliminating memory-constraints and 
                                                               Another advantage is that the ensemble reduces the asymp•
reducing the number of sequential data scans (or the total 
                                                               totic complexity of the algorithm besides scanning less data. 
number of times that the training file is accessed from sec•
                                                               One important distinction is that we are interested in reduc•
ondary storage), particularly for decision tree construction. 
                                                               ing sequential scan from secondary storage. Once the data 
State-of-the-art decision tree algorithms (SPRINT [Shafer 
                                                               can be held entirely in memory, the base learning algorithm 
et al, 1996], RainForest iGehrke et al 1998], and later 
                                                               is allowed to scan the data in memory multiple times. 
BOAT [Gchrke et a/., 1999] among others) still scan the data 
multiple times, and employ rather sophisticated mechanisms 
in implementation. Most recent work iHulten and Domingos,      2 One Scan 
20021 applies Hoeffding inequality to decision tree learning 
on streaming data in which a node is reconstructed iff it is sta• We first describe a strawman algorithm that scans the data 
tistically necessary. Besides decision tree, there hasn't been set exactly once, then propose the extension that scans the 
much research on reducing the number of data scans for other   data set less than once. The strawman algorithm is based on 
inductive learners. The focus of this paper is to propose a    probabilistic modeling. 
general approach for a wide range of inductive learning algo•
rithms to scan the dataset less than once from the secondary   2.1 Probabilistic Modeling 
storage. Our approach is applicable not only to decision trees Suppose is the probability that x is an instance of 
but also to other learners, e.g., rule and naive Bayes learners. class In addition, we have a benefit matrix that 
   Ensemble of classifiers has been studied as a general ap•   records the benefit received by predicting an example of class 
proach for scalable learning. Previously proposed meta-           to be an instance of class . For traditional accuracy-
learning [Chan, 1996] reduces the number of data scans to      based problems, and 
2. However, empirical studies have shown that the accuracy     For cost-sensitive application such as credit card fraud detec•
of the multiple model is sometimes lower than respective sin•  tion, assume that the overhead to investigate a fraud is $90 
gle model. Bagging [Breiman, 1996] and boosting iFreund        and is the transaction amount, then b[fraud, fraud\ — 
and Schapire, 1997] are not scalable since both methods scan               and fraud, fraud] = Using benefit 
the dataset multiple times. Our proposed method scans the      matrix and probability, the expected benefit received by pre-


LEARNING                                                                                                              595 dieting x to be an instance of class is 
                                                                Train(S, Sy, A", p) 
    Expected Benefit: — (1) Data : training set S, validation set partition 
                                                                          number A', confidence p 
                                                                Result : multiple model with size  
Based on optimal decision policy, the best decision is the la•
bel with the highest expected benefit:                          begin 
                                                                 I partition S into A' disjoint subsets of equal size 
                  max = argma* (2) 
   Assuming that is the true label of x, the accuracy of            train C\ from Si; 
the decision tree on a test data set is                             test C\ on SV; 
            Accuracy: A = (3) 
                                                                       test Ck on SV; 
For traditional accuracy-based problems, A is always normal•               compute Hoeffding error 
ized by dividing for cost-sensitive problems, A is usu•                confidence-satisfied true; 
ally represented in some measure of benefits such as dollar            for  
amount. For cost-sensitive problems, we sometimes use "to•
tal benefits" to mean accuracy.                                                     is the highest and is the 
                                                                           second highest; 
2.2 The Straw man Algorithm                                                if then 
The strawman algorithm is based on averaging ensemble iFan                    confidence-satisfied «- false; 
et al., 2002b]. Assume that a data set S is partitioned into K                break; 
disjoint subsets with equal size. A base level model                       end 
is trained from each Given an example x, each classi•                  end 
fier outputs individual expected benefit based on probability          if confidence satisfied then 
                                                                           return  
                                                                       else 
                                                       (4) 
                                                                       end 
The averaged expected benefit from all base classifiers is         end 
                                                                   return  
therefore 
                                                                end 
                                                       (5) 
                                                                       Algorithm 1: Less than one data scan 
We then predict the class label with the highest expected re•
turn as in Eq[2]. 
       Optimal Decision: (6) For finite population of size TV, the adjusted error is 
  The obvious advantage is that the strawman algorithm 
scans the dataset exactly once as compared to two scans by                                                          (8) 
meta-learning and multiple scans by bagging and boosting. 
In previous research [Fan et al, 2002b], the accuracy by 
the strawman algorithm is also significantly higher than both The range R of expected benefit for class label can be 
meta-learning and bagging. [Fan et al., 2002b] explains the  found from the index to the data, or predefined. When k 
statistical reason why the averaging ensemble is also more   base models are constructed, the Hoeffding error e^ can be 
likely to have higher accuracy than a single classifier trained computed by using Eq[8]. For data example x, assume than 
from the same dataset.                                                is the highest expected benefit and is the 
                                                             secondhighest, and are the Hoeffding errors. If 
3 Less Than One Scan 
                                                                             with confidence p, the prediction on x by 
The less-than-one-scan algorithm returns the current ensem•  the complete multiple model and the current multiple model 
ble with number of classifiers when the accuracy             is the same. Otherwise, more base models will be trained. 
of current ensemble is the same as the complete ensemble     The algorithm is summarized in Algorithm 1. 
with high confidence. For a random variable y in the range 
of with observed mean of Y after n observations,             Scan of validation set S V 
without any assumption about the distribution of y, Hoeffd-  If an example x satisfies the confidence p when k classifiers 
ing's inequality states that with probabilit> , the error of are computed, there is no utility to check its satisfaction when 
  to the true mean is at most                                more classifiers are computed. This is because that an ensem•
                                                             ble with more classifiers is likely to be a more accurate model. 
                                                      (7)    In practice, we can only read and keep one example x from 
                                                             the validation set in memory at one time. We only read a new 


596                                                                                                         LEARNING instance from the validation set if the current set of classi• To estimate the donation amount, we employed the multiple 
fiers satisfy the confidence test. In addition, we keep only the linear regression method. 
predictions on one example at any given time. This guaran•       The second data set is a credit card fraud detection prob•
tees that the algorithm scans the validation dataset once with lem. Assuming that there is an overhead $90 to dispute and 
nearly no memory requirement.                                 investigate a fraud and is the transaction amount, the fol•
Training Efficiency                                           lowing is the benefit matrix: 
The extra overhead of the Hoeffding-based less than one scan 
algorithm is the cost for the base classifiers to predict on the 
validation set and calculate the statistics. All these can be 
done in main memory. As discussed above, we can predict       As a cost-sensitive problem, the total benefit is the sum of 
on one example from the validation set at any given time. As• recovered frauds minus investigation costs. The data set was 
sume that we have k classifiers at the end and n is the size of sampled from a one year period and contained a total of 5M 
the validation set, the total number of predictions is approx• transaction records. We use data of the last month as test 
                                                              data (40038 examples) and data of previous months as train•
imately on average. The calculation of both mean and 
                                                              ing data (406009 examples). 
standard deviation can be done incrementally. We only need 
to keep and for just one example at anytime and                  The third data set is the adult data set from UC1 repository. 
calculate as follows:                                         For cost-sensitive studies, we artificially associate a benefit 
                                                              of $2 to class label F and a benefit of $1 to class label N, as 
                                                       (9)    summarized below: 

                                                      (10) 
The average number of arithmetic operation is approximately   We use the natural split of training and test sets, so the re•
                                                              sults can be easily replicated. The training set contains 32561 
  The problem that the proposed algorithm solves is one in    entries and the test set contains 16281 records. 
which the training set is very large and the I/O cost of data 4.2 Experimental Setup 
scan is the major overhead. When I/O cost is the bottleneck, 
                                                              We have selected three learning algorithms, decision tree 
the extra cost of prediction and statistical analysis is mini•
                                                              learner C4.5, rule builder RIPPER, and naive Bayes learner. 
mum. 
                                                              We have chosen a wide range of partitions, K 
4 Experiment                                                   {8,16,32,64,128,256}. The validation dataset SV is the 
                                                              complete training set. All reported accuracy results were run 
In empirical evaluation, we first compare the accuracy of the 
                                                              on the test dataset. 
complete multiple model (one scan as well as less than one 
scan) and the accuracy of the single model trained from the   4.3 Experimental Results 
same data set. We then evaluate the amount of data scan and   In Tables 1 and 2, we compare the results of the single classi•
accuracy of the less than one scan algorithm as compared to   fier (which is trained from the complete dataset as a whole), 
the one scan models. Additionally, we generate a dataset with one scan algorithm, and the less than one scan algorithm. We 
biased distribution and study the results of the less than one use the original "natural order" of the dataset. Later on in 
scan algorithm.                                               Section 4.4, we use a biased distribution. Each data set under 
4.1 Datasets                                                  study is treated both as a traditional and cost-sensitive prob•
                                                              lem. The less than one scan algorithm is run with confidence 
The first one is the well-known donation data set that first ap•
                                                              p = 99.7%. 
peared in KDDCUP'98 competition. Suppose that the cost 
of requesting a charitable donation from an individual x is   Accuracy Comparison 
$0.68, and the best estimate of the amount that x will donate The baseline traditional accuracy and total benefits of the sin•
is Y(x). Its benefit matrix is:                               gle model are shown in the two columns under "single" in 
                                                              Tables 1 and 2. These results are the baseline that the one 
                                                              scan and less than one scan algorithms should achieve. For 
                                                              the one scan and less than one scan algorithm, each reported 
As a cost-sensitive problem, the total benefit is the total   result is the average of different multiple models with K' rang-
amount of received charity minus the cost of mailing. The     ing from 2 to 256. In Tables 1 and 2, the results are shown in 
data has already been divided into a training set and a test  two columns under accuracy and benefit. As we compare the 
set. The training set consists of 95412 records for which it is respective results in Tables 1 and 2, the multiple model either 
known whether or not the person made a donation and how       significantly beat the accuracy of the single model or have 
much the donation was. The test set contains 96367 records    very similar results. The most significant increase in both ac-
for which similar donation information was not published un•  curacy and total benefits is for the credit card data set. The 
til after the KDD'98 competition. We used the standard train• total benefits have been increased by approximately $7,000 ~ 
ing/test set splits to compare with previous results. The fea• $10,000; the accuracy has been increased by approximately 
ture subsets were based on the KDD'98 winning submission.     1% ~ 3%. For the KDDCUP'98 donation data set, the total 


LEARNING                                                                                                             597                            C4.5                                                          C4.5 


                         RIPPER                                                        RIPPER 


Table 1: Comparison of single model, one scan ensemble, and   Table 2: Comparison of single model, one scan ensemble, 
less than one scan ensemble for accuracy-based problems       and less than one scan ensemble for cost-sensitive problems 

benefit has been increased by $1400 for C4.5 and $250 for     "batch" method is due to the fact that some examples satis•
NB.                                                           fied by previously learned classifiers have high probability, 
  We next study the trends of accuracy when the number of     but may not necessarily be satisfied by more base classifiers. 
partitions K increases. In Figure 1, we plot the accuracy and However, our empirical studies have shown that the differ•
total benefits for the credit card data sets, and the total benefits ence in how the validation set is handled doesn't significantly 
for the donation data set with increasing number of partitions influence the final model accuracy. 
K. C4.5 was the base learner for this study. As we can sec 
clearly that for the credit card data set, the multiple model 4.4 Biased Distribution 
consistently and significantly improve both the accuracy and 
                                                              When a data is biased in its distribution, the less than one 
total benefits over the single model by at least 1 % in accuracy 
                                                              scan algorithm need to scan more data than uniform distri•
and $40000 in total benefits for all choices of K. For the 
                                                              bution to produce an accurate model. With the same amount 
donation data set, the multiple model boosts the total benefits 
                                                              of datascan, it may not have the same accuracy as uniform 
by at least $1400. Nonetheless, when K increases, both the 
                                                              distribution. We have created a "trap" using the credit card 
accuracy and total benefits show a slow decreasing trend. It 
                                                              dataset. We sorted the training data with increasing transac•
would be expected that when K is extremely large, the results 
                                                              tion amount. The detailed results are shown in Table 4(a) and 
will eventually fall below the baseline. 
                                                              (b). The accuracy (and total benefits) in Table 4(a) are nearly 
  Another important observation is that the accuracy and to•  identical to the results of "natural distribution" as reported in 
tal benefit of the less than one scan algorithm are very close Tables 1 and 2. However, the amount of datascan by the less 
to the one scan algorithm. Their results are nearly identical. than one scan algorithm is over 0.9 as compared to approxi•
Data Scan                                                     mately 0.6 for natural distribution. As shown in Table 4(b), 
In both Tables 1 and 2, we show the amount of data scanned    when the datascan is less than 0.9 (the confidence is not satis•
for the less than one scan algorithm. It ranges from 40% (0.4) fied and less one scan will continue to compute more model), 
to about 70% (0.7). The adult dataset has the most amount     the total benefits are much lower. When distribution is biased, 
of data scanned since the training set is the smallest and it the variations in base classifiers' prediction are wider. It re•
requires more data partitions to compute an accurate model.   quires more data to compute an accurate model and the less 
C4.5 scans more data than both RIPPER and NB. This is be•     than one scan algorithm is performing in the correct way. 
cause we generate the completely unpruned tree for C4.5, and 
there are wide variations among different models.             4.5 Training Efficiency 
  In Table 3, we compare the differences in accuracy and      We recorded both the training time of the batch mode sin•
amount of training data when the validation set is either read gle model, and the training time of both the one scan al•
completely by every classifier (under "Batch") or sequentially gorithm and less than one scan algorithm plus the time to 
only by newly computed base classifiers (under "Seq") (as     classify the validation set multiple times and statistical es•
discussed in Section 3). Our empirical studies have found that timation. We then computed serial improvement, which is 
"Batch" mode usually scans approximately 1% to 2% more        the ratio that the one scan and less than one scan algo•
training data, and the models computed by both methods are    rithm are faster than training the single model. In Figure 2, 
nearly identical in accuracy. The extra training data from the we plot results for the credit card dataset using C4.5. Our 


598                                                                                                          LEARNING  Figure 1: Plots of accuracy and total benefits for credit card data sets, and plot of total benefits for donation data set with respect 
 to K when using C4.5 as the base learner 


                                C4.5 
                     1 Accuracy | 1 Data Scan                            Figure 2: Serial improvement for credit card dataset using 
                     1 Batch        Seq | 1 Batch      Seq               one scan and less than one scan 
          Donation     94.94%     94.94%      0.64     0.6l 

                       90.39%     90.41%      0.62     0.62 
            Adult       85.1%      85.0%      0.78     0.76 
                              RIPPER 


                                                                                                       Numbar at parthona 
                                 NB 


                                                                            (a). Performance for different classifier for biased distribution 

 Table 3: Comparison of accuracy and amount of datascan us•                (b). Performance of C4.5 with different amount of data scanned 
 ing the batch (or all in memory) and sequential method for                under the biased distribution 
 accuracy-based problems 
                                                                         Table 4: Performance of less than one scan under biased dis•
                                                                         tribution 
 training data can fit into the main memory of the machine. 
 Any single classifier algorithm that reduces the number of 
 data scan [Shafer et al 1996; Gehrke et al. 1998; 1999;                 SPRINT reads the attribute lists at this node and breaks them 
 Hulten and Domingos, 2002] will not have training time less             into r sets of attribute lists for r child nodes, for a total of 
 than this result. As shown in Figure 2, both one scan and less          / file read and r • / file writes. Later, RainForest [Gehrke 
 than one scan algorithm are significantly faster than the single        et ai, 1998] improves the performance of SPRINT by pro•
 classifier, and the less than one scan algorithm is faster than         jecting attribute list on each unique attribute value into an 
 the one scan algorithm.                                                 AVC-set. The complete AVC-sets of all attributes are called 
                                                                         AVC-group. When the AVC-group can be held in main mem•
5 Related Work                                                           ory, the selection of predictor attribute can be done efficiently. 
                                                                         To construct the initial AVC-group, it incurs the same cost as 
To scale up decision tree learning, SPRINT [Shafer et ai,                SPRINT to construct initial attribute lists plus one more scan 
 1996J generates multiple sorted attribute files. Decision tree          over the sorted lists to project into AVC-sets. Whenever a 
 is constructed by scanning and splitting attribute lists, which         split happens, RainForest has to access the data file again and 
eliminates the need for large main memory. Since each at•                reconstruct the AVC-group for child nodes. The exact num•
 tribute list is sorted, for a data file with / attributes and           ber of read and write is based on variations of RainForest that 
 N examples, the total cost to produce the sorted lists is               is chosen. In the best scenario where the AVC-group of ev•
 / 0(N - log(N)). External sort is used to avoid the need for            ery node in the tree fit in memory, the RF-read version still 
 main memory. Each attribute list has three columns-a unique             has to scan the whole data once at each level of the tree and 
record number, the attribute value and the class label; the to•          write it into r files. When this condition is not met, Rain•
 tal size of attribute lists is approximately three times the size       Forest solves the problem by multiple read and write. More 
of the original data set. When a split takes places at a node,           recently, BOAT [Gehrke et ai, 1999] constructs a "coarse" 


 LEARNING                                                                                                                               599 