     Skewing: An Efficient Alternative to Lookahead for Decision Tree Induction 

                           David Page                                         Soumya Ray 
                      page @ biostat. wise .edu                             sray@cs.wisc.edu 

            * Department of Biostatistics & Medical Informatics         Department of Computer Sciences 
                        University of Wisconsin                             University of Wisconsin 
                       Madison, Wisconsin 53706                            Madison, Wisconsin 53706 

                        Abstract 

     This paper presents a novel, promising approach 
     that allows greedy decision tree induction algo•
     rithms to handle problematic functions such as par•
     ity functions. Lookahead is the standard approach 
     to addressing difficult functions for greedy decision 
                                                               Table 1: Truth table for Drosophila (fruitfly) survival based 
     tree learners. Nevertheless, this approach is lim•
                                                               on gender and Sxl gene activity. 
     ited to very small problematic functions or subfunc-
     tions (2 or 3 variables), because the time complex•
     ity grows more than exponentially with the depth of 
                                                               a variety of data analysis approaches that employ an informa•
     lookahead. In contrast, the approach presented in 
                                                               tion gain or Kullback-Leibler divergence filter to do variable 
     this paper carries only a constant run-time penalty. 
                                                               selection or to control computation time, for example as in the 
     Experiments indicate that the approach is effective 
                                                               sparse candidate algorithm for learning Baycsian networks 
     with only modest amounts of data for problematic 
                                                               I Friedman et al, 1999]. The inability of such approaches to 
     functions or subfunctions of up to six or seven vari•
                                                               learn functions like 0 is frequently noted, for example, in the 
     ables, where the examples themselves may contain 
                                                               context of analyzing gene expression microarray data IFried•
     numerous other (irrelevant) variables as well. 
                                                               man etai, 1999; Szallasi, 2001]. 
                                                                 In TDIDT algorithms, the myopia of the search can be re•
    Introduction                                               duced at the cost of increased computation time. The stan•
                                                               dard approach is through depth-fc lookahead [Norton, 1989], 
Algorithms for the top-down induction of decision trees        where the default for TDIDT algorithms is depth-1 looka•
(TDIDT) are among the most widely used algorithms for          head. However, the time to perform a split grows exponen•
machine learning, data mining and statistical classifica•      tially with A:, and problematic functions remain no matter how 
tion. TDIDT implementations such as ID3 [Quinlan, 1983],       large a A: is chosen. 
C4.5 [Quinlan, 1997], C5.0 (www.rulequest.com) and CART          The purpose of this paper is to introduce an alternative ap•
iBreiman et al, 1984] are easy to use and (often) produce      proach to problematic functions such as parity, which does 
human-comprehensible models. Nevertheless, TDIDT algo•         not incur the high computational cost of lookahead. The ap•
rithms are well-known to be myopic because of their greedy     proach relies on the observation that these functions are not 
strategy for choosing split variables, or internal node labels. actually problematic if the distribution over the data is signif•
This myopia is at its worst when the data are labeled accord•  icantly different from uniform. In such cases other functions 
ing to parity functions such as exclusive-or (2-bit odd parity, may become problematic for TDIDT algorithms, but func•
denoted by ©). These and related other problematic functions   tions such as exclusive-or become relatively easy. Hence we 
naturally arise in real-world data. For example, in fruitfly ex• first observe that TDIDT algorithm performance on a data set 
periments, flies that are either male and have an active Sxl   can be improved if the algorithm has access to a second, sig•
gene or that are female and have an inactive Sxl gene survive, nificantly different distribution over the data. Of course such 
while other flies die; hence survival is an exclusive-or func• a second distribution often is unavailable. Therefore, we next 
tion of gender and Sxl activity (Table 1).                     show how the second distribution often can be simulated by 
  Of course, parity and parity-like functions are also prob•   skewing the data from the first distribution. We note that the 
lematic for other machine learning or statistical algorithms   present paper focuses entirely on classification, ignoring re•
that employ (explicitly or implicitly) a linear assumption in  gression trees, or trees that predict continuous outputs at their 
order to gain efficiency. Such models include perceptrons,     leaves. Furthermore, we limit our entire discussion to TDIDT 
logistic regression, linear support vector machines, Fischer's algorithms that perform only binary splits, although the data 
linear discriminant and naive Bayes. They also include any of  may use variables that can take more than two values. 


LEARNING                                                                                                              601  Table 2: For subfunctions at each of  
 and = 1, the fraction of assignments that are positive is , 
 as for / itself. Hence and have zero gain, while has 
 nonzero gain according to both entropy and Gini. 
                                                                       Figure 1: Two trees representing 
 2 Review of Lookahead for Hard Functions 
 We assume the reader is familiar with the approach of TDIDT 
algorithms. The assumed familiarity includes a knowledge of 
 the commonly-used functions to measure node purity, such as 
entropy [Quinlan, 1997] or Gini index iBreiman et al, 1984]. 
It also includes familiarity with the greedy heuristic of choos•
 ing to split on the variable that maximizes the improvement, 
or gain, in the node purity score. We now review the notions 
of problematic functions and lookahead. For ease of discus•
sion, this section limits itself to two-class problems, where 
the classes are positive and negative. 
   For the various node purity functions employed by differ•   Table 3: Six of the 12 functions over three variables that 
ent TDIDT algorithms, splitting on a variable can yield        are problematic even using depth-2 lookahead. The other six 
a non-zero gain only if the class distribution changes for at  problematic functions are the inverses of these. 
least one of the values (or ranges) that can take. If the 
distribution of classes is the same for every value of xt (or  cause labels have to be selected for three nodes from among n 
range) then x% will have zero gain according to any node pu•   variables. Furthermore, there are many functions that require 
rity measure in common usage. As an example, in Table 2        a higher lookahead. For example, suppose we have examples 
the variable x\ has non-zero gain according to either Gini or  constructed from variables and the target is one of 
entropy, whereas the variables and have zero gain.             the functions in Table 3 involving and . Even with 
   Consider a data set drawn from a uniform distribution over  depth-2 lookahead, TDIDT is highly likely to choose incor•
binary-valued variables labeled according to                   rect variables. These problems can be solved with depth-3 
the target function Even if we are fortunate                   lookahead, but the time to choose a split becomes 

enough to have a complete data set—one occurrence of each      and other problematic targets remain even then.1 
truth assignment over —it is clear that for every 
variable z;, the class distribution is exactly the same whether 3 Motivation for Skewing 
Xi is 0 or 1. So, regardless of how large a uniform sample 
we choose to draw, a variable will have non-zero gain only     Consider the first target function discussed in the previous 
because of chance. Thus, the probability that one of the cor•  section, , but now suppose the data are distributed 

rect variables (x99 or X'IOO) will have a higher gain than every differently from uniform. For example, we might introduce 
one of the incorrect variables is extremely low. Hence the     dependencies not present in the uniform distribution: for ev•
learning task is virtually impossible for a TDIDT algorithm.   ery odd number if X{ is 0 then has proba•
   With depth-2 lookahead the preceding task becomes triv•     bility 0.99 of being 1. Or we might suppose all variables are 
ial. A depth-2 lookahead from a given node chooses not only    independent as in the uniform distribution, but every variable 
the next split variable, but also the split variables at the next has probability only of taking the value 0. In either case, 
level. A TDIDT algorithm augmented in this way will con•       with a large enough sample we expect that the class distribu•
sider among the possible depth-2 trees the two shown in Fig•   tion among examples with will differ significantly 
ure 1, each of which will have the maximum possible gain.      from the class distribution among examples with  
For any reasonably large data set, with high probability all     To examine the preceding claim more closely, consider the 
other depth-2 trees will have gain only marginally different   second distribution, where each variable has probability of 
from zero. Hence we see that with depth-2 lookahead, © be•     being 0. If we draw a sample of 400 examples, we expect 
comes easy. Because depth-2 lookahead is repeated at every     roughly 100 of these to have and roughly 300 to 
step in tree construction, many other functions that have 2-
variable 0 as a subfunction become easy.                          'We can reduce this super-exponential growth of lookahead to 
   Of course, depth-2 lookahead comes with a price. Where n    "mere" exponential growth and still address parity-like functions if 
                                                               we require the tree to be "leveled"—all nodes at a level are labeled 
is the number of variables and m is the number of examples, 
                                                               by the same variable. But even this non-standard lookahead proce•
the time to choose the split goes from , be-                   dure imposes a high computational burden. 


602                                                                                                           LEARNING  have = 1. Of the 100 with we expect roughly                   weight of each example in which Xi takes the value , by 
   of these, or 75, to have and hence to belong to             multiplying the weight by a constant; for the sake of illustra•
 the positive class. Of the 300 with: we expect only           tion, suppose we double the weight. 
 of these, or 75 again, to belong to the positive class (to have At the end of this process, each example has a weight be•
           . Hence the fraction of positive examples is quite  tween 1 and . It is likely that each variable has a signif•
 different for the two values of of the examples with          icantly different weighted frequency distribution than previ•
     = 0 are positive, while of the examples with = 1          ously, as desired. But this is not guaranteed. For example, 
 are positive. As a result (and will have non-zero             suppose the original data set consists of 100 truth assignments 
 gain; for instance, information gain is roughly 0.19 (out of  over variables and . Suppose further that in half of these 
 maximum 1.0 possible) and Gini gain is roughly 0.06 (out      examples = 0 and = 1, and in the other half L = I 
 of maximum 0.25 possible). On the other hand, every vari•     and = 0. If the favored setting for each variable happens 
 able other than is likely to have nearly zero gain.           to be 1, then all examples get assigned weight 2, so the new 
 Hence unless a highly unlikely sample is drawn, a TDIDT al•   frequency distribution for each variable is the same as the 
gorithm will choose to split on either at which                original frequency distribution. In addition to this potential 
point the remainder of the learning task is trivial.           difficulty, a second difficulty is that this process can magnify 
   Notice that in the preceding discussion, moving to our sec• idiosyncrasies in the original data. For instance, suppose we 
ond distribution changed the marginal distribution for every   have a data set over , and (for simplicity) the favored 
variable, not just for those in the target. It would have re•  setting for each variable is 1. If we happen to have one ex•
vealed the correct variables if the target function had been   ample with many variables set to 1, it will get an inordinately 
          or even one of the problematic functions of three    high weight compared with other examples, potentially giv•
 variables in Table 3. Notice also that the important aspect   ing some insignificant variable a high gain. Can we mitigate 
of the second distribution was that it changed the frequency   these potential problems with the skewing procedure? 
distributions for the variables; the specific change for any     The difficulties in the preceding paragraph occur with some 
variable could have gone the other way—to probability | of     data sets combined with some choices of favored settings. 
taking value 0—and the second distribution still would have    Other selections of favored settings for the same data set may 
given non-zero gain to exactly the variables in the target.    leave other variables' frequencies unchanged, but it is rela•
   From the preceding discussion we conclude that if we have   tively unlikely they will leave the same variables' frequencies 
access to two distributions that are "different enough," then  unchanged. Furthermore, while other selections of favored 
choosing good variables to split on becomes relatively easy.   settings may magnify other idiosyncrasies in the data, it is 
However, in real-world problems we rarely have access to two   unlikely they will magnify the same idiosyncrasies. There•
different distributions over the data, or the ability to request fore, instead of using skewing to create only a second dis•
data according to a second distribution that we choose. In•    tribution, we use it to create some number T of additional 
stead, the next section discusses how in practice we can simu• distributions. The T different distributions come about from 
late a second distribution different from the first. We call this randomly (without replacement) selecting T different combi•
procedure skewing. The simulation approach tends to mag•       nations of favored settings for the n variables according to a 
nify idiosyncrasies in the data set, for example, introducing  uniform distribution. To ensure that tree construction is not 
some dependencies that were not present in the original dis•   thrown off course by any single bad distribution (either orig•
tribution. Nevertheless, our experiments indicate that, if the inal or skewed), the tree construction process is modified as 
data set is large enough, the magnification of idiosyncrasies  described in the following paragraph. 
is not a major problem.                                          Suppose we have weightings of the data (the origi•
                                                               nal data set plus T reweighted versions of this data set), and 
4 Skewing Algorithm                                            we are considering a split. We score each of the n variables 
                                                               against each of the weightings of the data. A variable 
The desired effect of the skewing procedure is that the skewed that is not part of the target function should have nearly zero 
data set should exhibit significantly different frequencies    gain on every weighting, although as already noted, it may 
from the original data set. Because we cannot draw new ex•     occur that on some weightings some of these variables can 
amples, we change the frequency distributions for variables    achieve high gain. But only variables that appear in the tar•
by attaching various weights to the existing examples. The     get should have significantly non-zero gain on most of the 
procedure initializes the weight of every example to 1. We     weightings (though not necessarily on all). Therefore, we 
next present the details of the re-weighting procedure for     set a gain threshold, and the variable that exceeds the gain 
binary-valued variables only. Nominal variables can be con•    threshold for the greatest number of weightings is selected as 
verted to binary variables. We discuss extensions to continu•  the split variable. Our expectation is that the selected variable 
ous variables in Section 7.                                    is highly likely to be correct in the sense that it actually is a 
   We may assume that every variable takes the value 0 in      part of the target function. Yet the time for choosing the split 
at least one example and takes the value 1 in at least one     remains 0{mn), in contrast to lookahead. We have increased 
example—otherwise, the variable carries no information and     the run-time only by a small constant. 
can be removed. For each variable we ran•                        Pseudocode for the algorithm is shown in Algorithm 1. 
domly, uniformly (independently for each variable) select a    Rather than actually doubling or tripling weights, the algo•
"favored setting" of either 0 or 1. We then increase the       rithm takes a parameter The weight of an ex-


LEARNING                                                                                                             603  Algorithm 1 Skewing Algorithm                                 following section describes the experiments designed to test 
 Input: A matrix D of m data points over n boolean             the preceding hypotheses. 
     variables, gain fraction G, number of trials T, 
     skew                                                      5 Experiments 
 Output: A variable Xi to split on, or —1 if no variable with 
     sufficient gain could be found                            In this section, we discuss experiments with synthetic and real 
  1: Entropy of class variable in D                            data, designed to test the hypotheses in the preceding para•
  2: Variable with max gain in D                               graph. In addition, the question arises of whether problematic 
  3: Gain of v in D                                            functions or subfunctions occur with high enough frequency 
  4: if then                                                   to justify the additional work of skewing. The experiments 
  5:                                                           in this section also address that question. We begin with a 
  6: for to n do                                               discussion of experiments using synthetic data, where target 
  7:                                                           functions as well as examples are drawn randomly and uni•
     {begin skewing loop}                                      formly, with replacement. In these experiments we compare 
  8: for t = 1 to T do                                         simple ID3 against ID3 with skewing. Wc selected ID3 to 
  9: for = 1 to n do                                           eliminate issues to do with more sophisticated pruning. The 
 10: Randomly chosen favored value for                         parameters input to the skewing algorithm (algorithm 1) were 
 11: for e — 1 to m do                                                                        These parameters were cho•
 12: W(e) = 1                                                  sen before the experiments and were held constant across all 
 13: fori = 1 to n do                                          experiments. Improved results could perhaps be obtained by 
 14: if/ lthen                                                 tuning s and G. 
 15: if Z?(e,i) = V(i) then                                      In the first set of experiments with synthetic data, exam•
 16: W(e) W(e) s                                               ples are generated according to a uniform distribution over 
 17: else                                                      30 binary variables. Target functions are drawn by randomly 
 18: W(e) W(e) (1 -s)                                          generating DNF formulae over subsets of 3 to 6 of the 30 
 19: N Entropy of class variable in D under W                  variables. The number of terms in each target is drawn ran•
20: for " = 1 to n do                                          domly, uniformly from between 1 and 25, and each term is 
21: E Gain of under distribution W                             drawn by choosing for each variable whether it will appear 
22: ifE G N then                                               negated, unnegated, or not at all (all with equal probabilities). 
23: F(i) F + 1                                                 All targets are ensured to be satisfiable. Examples over the 
     {end skewing loop}                                        30 variables that satisfy the target are labeled positive, and all 
24: argmaxF(i)                                                 other examples are labeled negative. Figures 2-5 show learn•
25: itF(j) 0 O then                                            ing curves for different target sizes. Each point on each curve 
26:    return                                                  is the average over several runs, each with a different target 
27: else                                                       and with a different sample of the specified sample size. 
28: return _                                                     In general, these figures fit our expectations. Both algo•
                                                               rithms perform well but skewing provides slightly yet consis•
                                                               tently better results (wc note that the differences are not sta•

ample is multiplied by s if xi takes preferred value Vi in the tistically significant). Probably the difference is skewed ID3 
example, and is multiplied by 1 — s otherwise. Hence for il•   is less likely than ordinary ID3 to include irrelevant variables, 

lustration, if s is |, the weight of every example in which Xi particularly when faced with problematic functions. One sur•
takes value Vi is effectively doubled relative to examples in  prise is that the figures indicate that an ordinary TDIDT al•
which Xi does not take value i;,-.                             gorithm outperforms skewing on average when the sample 
   Our hypothesis is that in practical experiments the new al• size is small relative to target size. As sample size grows, a 
gorithm will run somewhat slower than an ordinary TDIDT        crossover point is reached after which skewing consistently 
algorithm, only by a constant factor. It will rarely produce   outperforms the ordinary TDIDT algorithm. Furthermore, 
trees with lower accuracy than those of an ordinary TDIDT      the sample size required for effective skewing grows with the 
algorithm. It will often produce trees with slightly to moder• number of variables in the target, although these results alone 
ately higher accuracy—when the target contains one or more     do not make clear the order of this growth. It may well be ex•
problematic subfunctions. And it will sometimes produce        ponential, because target complexity can grow exponentially 
trees with much higher accuracy—when the target is itself a    with the number of variables in the target. Further experi•
problematic function. When the target is a problematic func•   ments are needed to explore the order of this growth. This ob•
tion over many variables, even after skewing the gain of any   servation implies a limitation of skewing—that skewing may 
individual variable in the target is likely to be small. There• be undesirable for learning tasks with small samples or target 
fore, we also hypothesize that unless the data set is large, the concepts that potentially employ many variables. 
benefits of the skewing approach will not apply to problem•      The next set of experiments focuses on the problematic 
atic target functions of five or more variables. Note that while functions alone. The methodology is the same as before, 
a large number of variables in the target reduces the potential with the following exception. Targets are drawn randomly 
gain, the number of variables in the examples does not. The    from functions that can be described entirely by variable co-


604                                                                                                           LEARNING                                                                          Figure 6: Three-Variable Hard Targets 


              Figure 3: Four-Variable Targets                             Figure 7: Four-Variable Hard Targets 


              Figure 4: Five-Variable Targets                             Figure 8: Five-Variable Hard Targets 

             —i 1 1 


               Figure 5: Six-Variable Targets                             Figure 9: Six-Variable Hard Targets 

LEARNING                                                                                                             605 