              Acquiring a Robust Case Base for the Robot Soccer Domain

                                    Raquel Ros, Josep Llu´ıs Arcos
                             IIIA - Artiﬁcial Intelligence Research Institute
                             CSIC - Spanish Council for Scientiﬁc Research
                                Campus UAB, 08193 Barcelona, Spain      ∗
                                        {ros,arcos}@iiia.csic.es

                    Abstract
                                                                           y
    This paper presents a mechanism for acquiring                                   x
    a case base for a CBR system that has to deal
    with a limited perception of the environment. The
    construction of case bases in these domains is
    very complex and requires mechanisms for au-
    tonomously adjusting the scope of the existing
    cases and for acquiring new cases. The work pre-
    sented in this paper addresses these two goals: to
                                                      Figure 1: Snapshot of the Four-Legged League (image ex-
    ﬁnd out the “right” scope of existing cases and to
                                                      tracted from the Ofﬁcial Robocup Rule Book).
    introduce new cases when no appropriate solution
    is found. We have tested the mechanism in the     acquiring case adaptation knowledge combining rule-based
    robot soccer domain performing experiments, both  methods with user guidance [Leake et al., 1996].Inthe
    under simulation and with real robots.            AQUA system, [Ram, 1993] presents an approach for incre-
                                                      mental learning based on the revision of existing cases and the
1  Introduction                                       incorporation of new ones when no solution was found. His
                                                      main challenges were: (i) novel situations; (ii) mis-indexed
Working with robots that interact with their environment is a cases; and (iii) incorrect or incomplete cases. In our ap-
complex task due to the degree of uncertainty in the robot’s proach, we attempt to cover (i) and (iii) in our domain.
perception of the world. But even if we achieve a very accu- This paper is the continuation of previous work [Ros et
rate perception mechanism, and as consequence, have a very al., 2006], where we designed a Case-Based Reasoning sys-
low degree of uncertainty, we still have to consider that the tem for action selection in the robot soccer domain. Before
environment is not fully controllable and unpredictable situa- introducing the problem we address, we brieﬂy describe the
tions can occur. For these reasons, the reasoning system has domain and the most relevant features of the system.
to introduce some mechanism that, on the one hand, allows We focus our work on the Four-Legged League of the
to adapt the a priori knowledge given by an expert to the real RoboCup Soccer Competition. In this league teams consist
perception of the robot and, on the other hand, automatically of four Sony AIBO robots. The robots operate fully au-
incorporates new knowledge when an unexpected situation tonomously, i.e. there is no external control, neither by hu-
occurs.                                               mans nor by computers. The ﬁeld is 6 m long and 4 m wide.
  Researchers address the case base acquisition in different There are two goals (cyan and yellow) and four colored mark-
ways based on their domains. In the textual case-based rea- ers the robots use to localize themselves in the ﬁeld. There are
soning ﬁeld, cases are extracted from textual sources as for two teams in a game: a red team and a blue team. Figure 1
example e-mails, manuals and scientiﬁc papers [Minor and shows a snapshot of the ﬁeld. For details on the ofﬁcial rules
Biermann, 2005], or medical records [Abidi and Manickam, refer to the Ofﬁcial Robocup Rule Book.
2002]. In robotics, log ﬁles created during the execution of
the tasks are used as inputs for case generation [Gabel and System Description
           ]
Veloso, 2001 . Interactive approaches have been studied to The goal of the CBR system is to determine the actions the
complete the missing knowledge of the system with a user robots should execute given a state of the game (a snapshot
through concept mapping [Leake and Wilson, 1999] or even
                                                      of the game at time t). Instead of considering single actions
  ∗Partial funding by the Spanish Ministry of Education and Sci- for each state, we deﬁne sequences of actions which we call
ence projects QUALNAVEX (DPI2003-05193-C02-02) and MID- game plays. Some examples are get near the ball and shoot
CBR (TIN2006-15140-C03-01). Raquel Ros holds a scholarship or get the ball, grab it, turn and shoot. Hence, a case con-
from the Generalitat de Catalunya Government.         sists of the description of the environment (problem descrip-

                                                IJCAI-07
                                                  1029tion) and the game play the robots performed for that state or because the parameters have high values (modelling large
(solution description). The problem description features are: regions) or low values (modelling small regions).
robots positions, ball position, opponents positions, defend- We are also interested in creating case bases with “essen-
ing goal, time of the game and score difference. The solution tial” cases, meaning that we expect to include cases that are
description is deﬁned as an ordered list of actions.  general enough to cover basic situations, but also include
  Henceforward in the paper we will focus on a simpliﬁed cases that represent speciﬁc situations: the ﬁrst ones are
version of the system: We consider only one robot, the ball represented with larger scopes, while the second ones, with
and the defending goal as the description of the problem (al- smaller scopes. Having these two types of cases allows us to
though the ideas presented are extendable to the other features reduce the number of cases to the most relevant ones, which
described above). The solution description remains the same: is a desirable property in any real-time response domain.
               case =((R, B, G),A)                      Since we cannot improve the robot’s perception (because
                                                      of hardware limitations and the need of real time response)
where R =(x, y, θ) , B =(x, y), G = {cyan, yellow},and and ﬁnding by hand the right parameters for the reasoning
A =[a1,a2, ..., an] (θ corresponds to the robot’s angle with module is very hard, we propose: ﬁrst, to include in the
respect to the x axis of the ﬁeld).                   reasoning model a mechanism to automatically compute the
  In order to retrieve the most similar case, we model the scope of existing cases based on the actual perception of the
similarity function1 based on the ball position with a 2D robot; and second, to introduce an additional mechanism in
Gaussian function. We consider two points in the Cartesian combination with the former to include new cases in the sys-
plane are similar if their distance is below a given threshold. tem if no case is retrieved. With the former we ensure that
Using a Gaussian allows us to model the maximum distances the regions the cases cover are adapted to the robots believes
in the x and y axis we consider two values to be similar, as of the world, and therefore, they will respond according to its
well as different degrees of similarities based on the propor- perception. And with the latter, the robot has the ability to
tional distances of the points. It is deﬁned as follows: be “creative” and to act with a new behavior when the system
                                                      does not return any possible solution.
                               2     2
                         −( (x) + (y ) )              Brieﬂy, our approach is focused on creating an initial case
                            2τ2   2τ2
            G(x, y)=e       x     y                 base with partial information so the system itself can com-
                                                      plete it either by modifying the scope of the cases, in order to
where x, y are the distances between the points in the x
                                                      cover the problem space with the minimum number of cases,
and y axis respectively, and τx,τy, the maximum distances
                                                      or introducing new cases when needed. Both processes are
for each axis. These parameters represent the scope of the
                                                      guided by a human trainer.
case, i.e. the region where the points are considered similar
(represented by an ellipse on a 2D plane). Each case may
have different region sizes, hence, for each case we also store 2 Learning the Scope of Existing Cases
this additional information as part of the system knowledge. The initial case base of our system is manually created and
  When a new problem is presented to the system, we ﬁrst represents the a priori expert’s knowledge. The idea is to pro-
ﬁlter the cases based on the defending goal G: the problem vide the system a set of cases which must be adapted to the
and the case must have the same defending goal. Otherwise, robot’s actual perception. The expert knows several generic
they cannot be considered similar at all. Next, we compute situations (prototypical cases) and their corresponding solu-
the similarity between the position of the balls. If the simi- tions, but cannot predict the real scope of the cases from the
larity exceeds a given threshold then we consider the case as robot’s point of view. Thus, we propose to create an initial
a potential solution to the problem. We select the case with case base composed of prototypical cases with default scopes,
                                                       0     0
higher similarity degree for the reusing step.        τx and τy , and then let the robot learn them. To this end, the
  The problem we address in this paper is how to ensure expert should tell the robot if the case retrieved at each time
the robustness of the system given the high uncertainty of is correct or not. We refer to this process as the training step.
the robot’s perception. In a CBR system, the correctness of As mentioned in the previous section, we refer to the case’s
the case base is one of the main issues that determines the scope as the region of the ﬁeld where that case should be re-
accuracy of the system performance, since it represents its trieved. These regions are modelled as ellipses, which corre-
knowledge. Given a new problem, its solution is obtained by spond to Gaussians’ projections on a plane (from now on, we
comparing the description of the new problem with the cases will refer directly to the ellipse). In order to learn the scope
in the case base. In our approach the position of the ball in of the cases, we propose to modify the size of these ellipses
                                   τ      τ
the ﬁeld (perception) and the parameters x and y used in varying the Gaussian parameters τx and τy. To this end, we
the similarity function (knowledge) are crucial for the correct must deﬁne some policy to determine when and how these
performance of the retrieval step. Otherwise, wrong cases parameters should be adjusted.
might be returned, or even no case at all. In both situations,
it could be either because the ball is not correctly positioned 2.1 When to adjust the values
  1In the complete system we take into account other features to The goal of increasing or decreasing the size of the ellipse is
compute the similarity between cases. The robot position is not in- to reach the “ideal” region that a case should cover. The cen-
cluded in the retrieval step. We will not go into details, since it is not ter of the ellipse represents the exact position of the ball on the
relevant for the work we present here.                ﬁeld speciﬁed in that case. As we move towards the boundary

                                                IJCAI-07
                                                  1030                      τy
                                                                                        i
                                                                   0                   τy
                      γy                                          τy
                                  τ
                                  x                                    0                       i
                             γx                                       τx                      τx


                                                                  t−1                   t
Figure 2: Example of a security region (gray region) and a       τy                    τy
risk region (white region) deﬁned by γx =0.5 and γy =0.75.

                                                                             t−1                  t
of the ellipse, the uncertainty about whether the points belong              τx                  τx
to the scope of the case increases. We call the set of points
next to the center of the ellipse the security region, and those
near the boundary of the ellipse the risk region.Wedeﬁneγx
                                                            Figure 3: Case scope evolution. γx = γy =0.8
and γy as the relative size of the security region with respect
to the size of the ellipse (each value corresponds to a radius increasing policies (we only show the equations for axis x;
percentage). Figure 2 shows an example of these regions. the same equations are used for axis y):
  When the system proposes a solution for a new problem, •
we use the expert’s feedback for tuning the parameters of the ﬁxed: the increasing amount is a ﬁxed value. Thus, we
                                                          deﬁne a step function:
retrieved case. If the proposed solution succeeded, the scope             
of the case is increased. Otherwise, it is decreased.                        ˆ
                                                                             δx  if x ≥ γxτx
                                                                     δx =
  We will ﬁrst focus on the increasing process. If the prob-                 0   otherwise
lem is located inside the security region (the position of the
ball is in this region) the system cannot introduce new knowl- • linear: we compute the increasing value based on a lin-
edge to the case. Its current information is robust enough to ear function:
determine that the problem corresponds to the scope of that          
                                                                        x−γxτx  ˆ
                                                                        τ −γ τ  · δx  if x ≥ γxτx
case. On the contrary, if the problem is within the risk region  δx =    x  x x
the system can conﬁrm the current scope of the case increas-            0             otherwise
ing the size of the ellipse. Expanding the ellipse results in
                                                        •
expanding the security region as well.                    polynomial: we compute the increasing value based on
  Problems are incorrectly solved using a case due to scope a polynomial function:
                                                                    
overestimation. Hence, we have to reduce the size of the el-                    5
                                                                       (x−γxτx)  ˆ
                                                                               5 · δx  if x ≥ γxτx
lipse. If the ball is inside the security region, we do not de- δx =   (τx−γxτx)
crease the parameters since it corresponds to a robust region.         0               otherwise
If the problem is within this region and the feedback is neg-
ative, we assume that the error is caused by something else After computing the increasing values, we update τx and τy
(wrong localization) and not because of a wrong knowledge adding the computed δx and δy respectively.
of the system. As an illustration, imagine the following sit- The motivation for decreasing the parameters is to reduce
uation: the robot is not well localized and as a consequence, the ellipse size so the new problem solved is not considered
it perceives the position of the ball incorrectly. It could hap- inside the region anymore. To this end, we equal τx and τy
pen that it retrieves the right case given its own perception. to the values in the problem, only if they are higher than the
But from the external observer perception, the case used to radius of the security region:
                                                                       
solve that problem is not the right one. Therefore, the feed-                      ≥ γ  τ
                                                                  τ t =    x    if  x    x x
back given to the robot is negative. If the system reduces the     x     τ t−1
ellipse, it could radically reduce the scope of the case, not             x     otherwise
because it was overestimated, but because of the high impre- We update τy in the same way.
cision. However, when the problem is inside the risk region, Updating both values separately and only when the prob-
the system does reduce the scope of the case, since the scope lem is within the risk region prevents from radically reducing
overestimation might be the cause of the negative feedback. the scope of the case. Below we describe a simple example
  In summary, the system enlarges or reduces the scope of to illustrate the approach.
a case, i.e. modiﬁes its knowledge, when the problem pre-
sented is correctly or incorrectly solved and it is within the 2.3 Example
case’s risk region.                                   Figure 3 depicts four steps of the training process. The gray
2.2  How to adjust the values                         region represents the security region, while the dashed ellipse
                                                      corresponds to the “ideal” scope of the case (deﬁned by the
The ﬁrst problem is to determine the increasing values for human trainer) we attempt to reach. Any problem located
each parameter (τx,τy), i.e. how much should the system within this ideal area produces a positive feedback by the ex-
increase the size of the ellipse with respect to each axis. We pert. The black dot represents a new solved problem (ball
                                     ˆ     ˆ
deﬁne δx and δy as the increasing value, and δx and δy as the position with respect to the case). Figure 3 (a) shows the ini-
maximum increasing value for each axis. We propose three tial stage at time 0, where the scope of the case is minimum

                                                IJCAI-07
                                                  1031             y                                        appear between them (white regions). They represent the re-
                                                      gions where the robot will have to acquire new information
                                                      to increase its knowledge.
                                                        A new case is created using the description of the envi-
                                                      ronment (problem description), and a generated game play
                                                      (solution of the new case). To create a game play, we pro-
                                x                     vide the system a set of possible actions the robot can per-
 Figure 4: “Ideal” case base based on the expert knowledge. form. The combination of these actions correspond to poten-
τ 0,τ0                                                tial game plays. Given a new problem to solve, if it does not
( x y ). Since the new solved problem is within the risk re- retrieve any case (either due to imprecision problems or be-
gion and the feedback is positive, we proceed to enlarge the cause the problem is actually in a gap) the system generates
size of the ellipse using one of the policies deﬁned.                  2
         i                                            a random game play . The robot executes the suggested ac-
  At time , Figure 3 (b), we can observe that the ellipse has tion and the expert evaluates the correctness of the solution
increased, but still has not reached the expected size. Hence, proposed. Only if it succeeds, the new case is created.
we continue to enlarge the scope by solving new problems as When a new case is inserted into the system, it is cre-
long as the expert feedback is still positive.
                 t−1                                  ated with a minimum scope (a small ellipse). From that mo-
  Figure 3 (c), time , depicts a situation where the ellipse ment on, the evolution of the new case depends on how often
generated is bigger than the expected size. From now on, the the robot reuses it, enlarging or reducing its scope using the
feedback may be positive or negative. If a new problem is mechanism presented previously. The idea is that at the be-
within the risk region and the feedback is positive, then we ginning, the new case could seem to be a good solution for
would proceed to increase the ellipse. But, if the feedback that concrete situation, but its actual effectiveness has to be
is negative, then the decreasing process is used to reduce the evaluated when the robot reuses it. As time passes, if the
ellipse. The ﬁgure shows an example of this situation. As scope of the case does not increase, and instead, it is reduced,
we can see, the new problem is located in the risk region,but we can deduce that the case is not useful for the robot’s per-
out of the ideal scope. Thus, the current scope is reduced, but
              τ         <γ  τ                        formance. On the contrary, if its scope increases, or at least,
only by updating x since y  y y.                      it remains stable, then we consider that the case contributes
  Figure 3 (d) shows the updated scope, where the problem to the system’s knowledge.
remains outside the scope of the case. As more problems are
solved, the scope of the case will converge to the ideal scope.
  In conclusion, we distinguish two phases in the training 4 Experimentation
process: growing the scope of the case and converging to the This section describes the experiments performed in order to
ideal scope. During the ﬁrst phase, the feedback is always test the approach introduced in the paper. We divide the ex-
positive and the scope is always being expanded. The second perimentation in two stages: simulation and real robots.
phase occurs once the expected scope is exceeded. Then, the
feedback could either be positive or negative. The goal of the 4.1 Simulation
positive feedback is to enlarge the scope, while the goal of The goal of this ﬁrst phase is to examine the behavior of the
negative feedback is to converge to the ideal scope the human policies using different values for the parameters presented in
trainer expects.                                      Section 2.1. Since we had to test different combinations of
                                                      values, simulation was the fastest way to obtain orientative
3  Introducing New Cases in the System                results. The most relevant were selected for the experimenta-
Finding out all possible situations a robot can encounter dur- tion with real robots.
ing its performance is unfeasible. Even more so in a domain We based the experiments on a single case to observe how
with high uncertainty, such as the domain we are working the different values of the variables affect the evolution of
                                                      its scope, i.e. the resulting size of the ellipse for the case.
with. Because of the domain (a real time game), we cannot                                       τ  =  100
afford the robot to stop during the game just because it does The initial case was deﬁned with a small scope, x
                                                      and τy = 100. The expected outcome was τx = 450 and
not “know” what to do in that situation. Somehow, the robot τ = 250
must always execute an action at every time step of the game. y . We randomly created 5000 problems. Every time
  After the training step, the knowledge of the system might the case was retrieved, we used the different policies to mod-
present some “gaps”, i.e. the scope of the cases may not cover ify the scope of the case. The experiment was repeated com-
the whole ﬁeld. Of course, this depends on the number of bining the following values:
cases used during the training. But as we mentioned, on the • security region size (expressed as percentage):
one hand, the expert cannot deﬁne all possible cases. And on γx = {0.5, 0.6, 0.7, 0.8, 0.9}
the other hand, we focus our approach on initially deﬁning • maximum increasing value (expressed in mm):
a set of generic situations, allowing the robot to create new ˆ
                                                          δx = {10, 20, 30, 40, 50, 60, 70, 80, 90, 100}
ones based on its own experience afterwards.
  Figure 4 shows an example of a hand coded case base (only 2In this paper, the number of available actions is low. Hence,
4 cases). For simplicity, we only show a quarter of the ﬁeld we considered that generating random game plays is feasible. We
(the positive-positive quadrant). Each ellipse represents a pre- believe that for more complex situations other mechanisms should
deﬁned case (given knowledge). As we can see, several gaps be used since the random generation would not be scalable.

                                                IJCAI-07
                                                  1032   750                               750                                750
     0.5                               0.5                                0.5
     0.6                               0.6                                0.6
     0.7                               0.7                                0.7
     0.8                               0.8                                0.8
   700 0.9                           700 0.9                            700 0.9

   650                               650                                650

   600                               600                                600
  x  radius                         x  radius                          x  radius
   550                               550                                550

   500                               500                                500

   450                               450                                450
     10  20  30  40  50  60  70  80  90  100  10  20  30  40  50  60  70  80  90  100  10  20  30  40  50  60  70  80  90  100
             maximum increment                 maximum increment                  maximum increment
               (a)                                 (b)                                (c)
           Figure 5: Resulting τx using the policies: (a) Fixed policy. (b) Linear policy. (c) Polynomial policy.
                                ˆ
The same values were used for γy and δy.              and policy to use in the system. The second one consists in
  For each combination of values, we ran 10 experiments. evaluating the convergence of the cases in a given case base.
Figure 5 shows the average of the obtained results. On the Finally, the goal of the third one is to observe if the system is
         ˆ      ˆ
one hand, δx and δy deﬁne how much the ellipse may in- able to acquire new knowledge when no solution is found.
crease at each time. Hence, the higher their values, the bigger
the resulting scope of the case. On the other hand, γx and γy Testing the parameters and policies As we mentioned, we
determine the size of the security region and the risk region can divide the training process in two steps: growing the
(low values represent small security regions and large risk re- scope of the case and converging to the ideal scope. We
gions). The risk region determines when the scope of the case are interested in rapidly enlarging the size of the ellipse until
has to be modiﬁed. As this region increases, there are more reaching the ideal one, and then opt for a more conservative
chances of modifying the scope as well. Thus, for all three behavior to adjust it. We switch from one strategy to the other
policies, the curves tend to increase from left to right. when the size of the ellipse is decreased.
  With respect to the policies, the ﬁxed policy obtains the We can achieve this strategy either by combining the poli-
highest τx and τy values, while the polynomial obtains the cies or by modifying the values of the parameters through the
lowest ones. The former function has a more aggressive be- process. Regarding the policies, we have included two addi-
havior, radically increasing the size of the ellipse. The latter tional strategies: ﬁxed or linear policy for the growing step,
function has a more conservative behavior, computing ﬁrst and the polynomial for the convergence step. With respect to
small increments and then increasing as we reach the bound- the parameters, for the ﬁrst step we deﬁne large risk regions
ary of the ellipse. As a consequence, the ﬁxed policy sig- and high increasing values, and the opposite for the second
niﬁcantly varies between the different parameters, whereas step.
the behavior of the polynomial policy remains more stable al- The experimentation is similar to the simulation stage,
though the values of the parameters change. Regarding the where the experiments are based on a single case. The ex-
linear policy, it has an intermediate behavior with respect to pected scope of the case is τx = 900 and τy = 600.We
the other two (tending more to the ﬁxed policy).      generated 100 new problems, manually positioning the ball
  After the experimentation, we can conﬁrm that a more con- in the ﬁeld. Each experiment combined the ﬁve policies with
servative policy, low increasing values and small risk regions                                ˆ
                                                      three different sets of parameters: (i) γx =0.5, δx = 100; (ii)
is the most appropriate combination to obtain the desired γ =0.7 δˆ =50    γ  =0.5  δˆ = 100    γ  =0.7
scope of the cases. The conclusion is obvious since we are x  , x     ; (iii) x    , x       and x      ,
                                                      δˆ =50
establishing ideal conditions in order to gradually increase x (the former are for the growing process, and the lat-
                                                      ter, for the convergence process). The same values were used
the scope of the cases. But two problems arise when extend-    ˆ
ing the experiments to the real world: time and uncertainty. for γy and δy. Each parameter varies separately depending on
First, the number of iterations needed to reach the expected the τ altered (modifying τx does not imply modifying τy as
result is unfeasible when working with real robots; and sec- well). We performed 10 trials per set.
ond, a noise-free environment is only available under simu- Comparing the results with respect to the expected scope,
lation. Although we have observed different behaviors in the we verify that: (i) the ﬁxed and linear policies generate the
graphics obtained when gradually modifying the parameters, highest τx values exceeding it; (ii) the polynomial policy does
these differences are not so obvious in a real environment be- not even reach the ideal scope because of the low increas-
cause other issues modify the expected result. Therefore, the ing speed; (iii) both ﬁxed-polynomial and linear-polynomial
next stage is to experiment in the real world with the most strategies obtain the closest scopes to the expected ones, since
relevant parameters (understanding relevant as the ones that they combine the advantages of both policies.
show more contrasting behaviors) to determine the effective- Regarding the values of the parameters, we conﬁrmed the
ness of the approach presented.                       conclusions drawn from simulation: combining low increas-
                                                      ing values and small risk regions ensures reaching the ex-
4.2  Real robots                                      pected result. The problem once again is the number of steps
Three types of experiments were performed with real robots. for achieving this goal. Hence, combining the values of the
The ﬁrst one aims to ﬁnd out the most appropriate parameters parameters –ﬁrst high increasing values and large risk regions

                                                IJCAI-07
                                                  1033