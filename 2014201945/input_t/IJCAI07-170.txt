               Reinforcement Learning of Local Shape in the Game of Go

                          David Silver, Richard Sutton, and Martin Muller¨
                                   Department of Computing Science
                                          University of Alberta
                                      Edmonton, Canada T6G 2E8
                               {silver, sutton, mmueller}@cs.ualberta.ca

                    Abstract                          effective. They are fast to compute; easy to interpret, modify
                                                      and debug; and they have good convergence properties.
    We explore an application to the game of Go of      Secondly, weights are trained by temporal difference learn-
    a reinforcement learning approach based on a lin- ing and self-play. The world champion Checkers program
    ear evaluation function and large numbers of bi-  Chinook was hand-tuned by expert players over 5 years.
    nary features. This strategy has proved effective When weights were trained instead by self-play using a tem-
    in game playing programs and other reinforcement  poral difference learning algorithm, the program equalled
    learning applications. We apply this strategy to Go the performance of the original version [7]. A similar ap-
    by creating over a million features based on tem- proach attained master level play in Chess [1]. TD-Gammon
    plates for small fragments of the board, and then achieved world class Backgammon performance after train-
    use temporal difference learning and self-play. This ingbyTD(0)andself-play[13]. A  program trained by
    method identiﬁes hundreds of low level shapes with TD(λ) and self-play outperformed an expert, hand-tuned ver-
    recognisable signiﬁcance to expert Go players, and sion at the card game Hearts [11]. Experience generated
    provides quantitive estimates of their values. We by self-play was also used to train the weights of the world
    analyse the relative contributions to performance of champion Othello and Scrabble programs, using least squares
    templates of different types and sizes. Our results regression and a domain speciﬁc solution respectively [2;
    show that small, translation-invariant templates are 9].
    surprisingly effective. We assess the performance   Finally, a linear evaluation function is combined with
    of our program by playing against the Average Lib- a suitable search algorithm to produce a high-performance
    erty Player and a variety of computer opponents on
       9×9                                            game playing program. Minimax search variants are particu-
    the    Computer Go Server. Our linear evaluation  larly effective in Chess, Checkers, Othello and Backgammon
    function appears to outperform all other static eval- [3; 7; 2; 13], whereas Monte-Carlo simulation has proven
    uation functions that do not incorporate substantial most successful in Scrabble [9] and Hearts [11].
    domain knowledge.                                   In contrast to these games, the ancient oriental game of
                                                      Go has proven to be particularly challenging. The strongest
1  Introduction                                       programs currently play at the level of human beginners, due
A number of notable successes in artiﬁcial intelligence can be to the difﬁculty in constructing a suitable evaluation function
                                                      [ ]
attributed to a straightforward strategy: linear evaluation of 6 . It has often been speculated that Go is uniquely difﬁcult
many simple features, trained by temporal difference learn- for computers because of its intuitive nature, and requires an
ing, and combined with a suitable search algorithm. Games altogether different approach to other games. Accordingly,
provide interesting case studies for this approach. In games as many new approaches have been tried, with limited success.
varied as Chess, Checkers, Othello, Backgammon and Scrab- In this paper, we return to the strategy that has been so
ble, computers have exceeded human levels of performance. successful in other domains, and apply it to Go. We develop
Despite the diversity of these domains, many of the best pro- a systematic approach for representing intuitive Go knowl-
grams share this simple strategy.                     edge using local shape features. We evaluate positions using
  First, positions are evaluated by a linear combination of a linear combination of these features, and learn weights by
many features. In each game, the position is broken down into temporal difference learning and self-play. Finally, we incor-
small, local components: material, pawn structure and king porate a simple alpha-beta search algorithm.
safety in Chess [3]; material and mobility terms in Check-
ers [7]; conﬁgurations of discs in Othello [2]; checker counts 2 The Game of Go
in Backgammon [13]; single, duplicate and triplicate letter The main rules of Go are simple. Black and white players
rack leaves in Scrabble [9]; and one to four card combina- take turns to place a single stone onto an intersection of the
tions in Hearts [11]. In each case, with the notable exception Go board. Stones cannot be moved once played, but may be
of Backgammon, a linear evaluation function has proven most captured. Sets of adjacent, connected stones of one colour

                                                IJCAI-07
                                                  1053                                                      achieved in predicting expert moves, this approach has not
                         A                     A      led to strong play in practice. This may be due to its focus
        B                                             on mimicking rather than evaluating and understanding the
              A          C                            shapes encountered.
                                          B             A second approach has been to train a multi-layer per-
                       B                              ceptron, using temporal difference learning by self-play [8;
                                                      4]. The networks implicitly contain some representation of
       (a)               (b)              (c)         local shape, and utilise weight sharing to exploit the natu-
                                                      ral symmetries of the Go board. This approach has led to
Figure 1: (a) If black plays at A he captures two white stones stronger Go playing programs, such as Enzenberger’s Neu-
on the right. Playing at B is a common tesuji to capture the roGo III [4], that are competitive with the top commercial
white stone at the top. The stones in the bottom-left form a programs on 9 × 9 boards. However, the capacity for shape
common joseki from the marked stone. (b) Black can play knowledge is limited by the network architecture, and the
according to one of three proverbs: A is the one-point jump; knowledge learned cannot be directly interpreted or modiﬁed
B is the ponnuki; and C is a hane at the head of two stones. in the manner of pattern databases.
(c) The safety of the marked black stone depends on context:
it is safe in the top-left; should be captured by white A in the 4 Local shape representation
top-right; but should be safe from white B in the bottom-left.
                                                      We represent local shape by a template of features on the Go
                                                      board. The shape type is deﬁned by the template size, the fea-
are known as blocks. The empty intersections adjacent to a tures used in the template, and the weight sharing technique
block are called its liberties. If a block is reduced to zero used.
liberties by the opponent, it is captured and removed from the A template is a conﬁguration of features for a rectangu-
board (Figure 1a). At the end of the game, each player’s score lar region of the board. A basic template speciﬁes a colour
is equal to the number of stones they have captured plus the (black, white or empty) for each intersection within the rect-
total number of empty intersections, known as territory,that angle. The template is matched in a given position if the rect-
they have surrounded.                                 angle on the board contains exactly the same conﬁguration as
                                                      the template. A local shape feature simply returns a binary
3  Shape Knowledge in Go                              value indicating whether the template matches the current po-
The concept of shape is extremely important in Go. A good sition.
shape uses local stones efﬁciently to maximise tactical advan- We use weight sharing to exploit several symmetries of the
tage. Professional players analyse positions using a large vo- Go board [8]. All rotationally and reﬂectionally symmetric
cabulary of shapes, such as joseki (corner patterns) and tesuji shapes share the same weights. Colour symmetry is repre-
(tactical patterns). These may occur at a variety of different sented by inverting the colour of all stones when evaluating
scales, and may be speciﬁc to one location on the board or a white move. These invariances deﬁne the class of location
equally applicable across the whole board (Figure 1). For ex- dependent shapes. A second class of location independent
ample, the joseki at the bottom left of Figure 1a is speciﬁc to shapes also incorporates translation invariance. Weights are
the marked black stone on the 3-4 point, whereas the tesuji at shared between all local shape features that have the same
the top could be used at any location. Many Go proverbs exist template, regardless of its location on the board. Figure 2
to describe shape knowledge, for example “ponnuki is worth shows some examples of weight sharing for both classes of
30 points”, “the one-point jump is never bad” and “hane at shape.
the head of two stones” (Figure 1b).                    For each type of shape, all possible templates are exhaus-
  Commercial Computer Go programs rely heavily on the tively enumerated to give a shape set. For template sizes up
use of pattern databases to represent shape knowledge [6]. to 3 × 3, weights can be stored in memory for all shapes
Many years are devoted to hand-encoding professional ex- in the set. For template sizes of 4 × 3 and larger, storage
pertise in the form of local pattern rules. Each pattern rec- of all weights in memory becomes impractical. Instead, we
ommends a move to be played whenever a speciﬁc conﬁgura- utilise hashed weight sharing. A unique Zobrist hash [15] is
tion of stones is encountered on the board. The conﬁguration computed for each location dependent or location indepen-
can also include additional features, such as requirements on dent shape, and h bins are created according to the available
the liberties or strength of a particular stone. Unfortunately, memory. Weights are shared between those shapes falling
pattern databases suffer from the knowledge acquisition bot- into the same hash bin, resulting in pseudo-random weight
tleneck: expert shape knowledge is hard to quantify and en- sharing. Because the distribution of shapes is highly skewed,
code, and the interactions between different patterns may lead we hope that updates to each hash bin will be dominated by
to unpredictable behaviour. If pattern databases were instead the most frequently occurring shape. This idea is similar to
learned purely from experience, it could signiﬁcantly boost the hashing methods used by tile coding [5].
the robustness and overall performance of the top programs. There is a partial order between shape sets. We deﬁne the
  Prior work on learning shape knowledge has focussed on predicate Gi,j to be 1 if shape set Si is more general than
predicting expert moves by supervised learning of local shape shape set Sj,and0 otherwise. Shape sets with smaller tem-
[10; 14]. Although success rates of around 40% have been plates are strictly more general than larger ones, and location

                                                IJCAI-07
                                                  1054                                                                  ear combination of all local shape features φj,k from all shape
                                                                  sets j, with their corresponding weights θj,k. The sigmoid
                                                                  function σ squashes the output to the desired range [0, 1].
                                                                                      ⎛                 ⎞
                                                                                        m nj
                                                                                      ⎝                 ⎠
                                                                             V (s)=σ           φj,k(s)θj,k        (1)
                                                                                        j=1 k=1
                                                                  .
                                                                    All weights θ are initialised to zero. The agent selects
                                                                  moves by a -greedy single-ply search over the value func-
                                                                  tion, breaking ties randomly. After the agent plays a move,
                                                                  the weights are updated by the TD(0) algorithm [12].The
                                                                  step-size is the same for all local shape features within a set,
                                                                  and is deﬁned to give each set an equivalent proportion of
                                                                  credit, by normalising by the number of parameters updated
                                                                  on each time-step.
                                                                                δ = r + V (s  ) − V (s )
            Figure 2: Examples of location dependent and location inde-                    t+1       t            (2)
            pendent weight sharing, on a 5 × 5 board.                                α
                                                                            Δθj,k =     δφj,kV (st)(1 − V (st))   (3)
                                                                                   mnj
            independent shape sets are strictly more general than location Because the local shape features are binary, only a sub-
            dependent. A more general shape set provides no additional
                                                                  set of features need be evaluated and updated. This leads to
            information over a more speciﬁc shape set, but may provide         O(   m  n )
                                                                  an an efﬁcient   j=1 j  implementation rather than the
            a useful abstraction for rapid learning.              O(   m  N  )
              The frequency with which shapes occur varies by several  j=1  j time that would be required to evaluate or up-
            orders of magnitude. For each of the m shape sets Sj,the date all weights.
            total number of weights in the set is denoted by Nj,andthe
            total number of local shape features in the set that are matched 6 Experiments with learning shape in 5 × 5
            in any position is a constant value nj. Figure 3 shows the Go
            breakdown of total number and frequency of shape sets on a
                                                                  We trained two Go-playing agents by coadaptive self-play,
            5 × 5 board.
                                                                  each adapting its own set of weights so as to defeat the other.
                                                                  This approach offers some advantages over self-play with a
            5  Learning algorithm                                 single agent: it utilises two different gradients to avoid local
            In principle, a set of shape weights can be learned for any minima; and the two learned policies provide a robust per-
            purpose, for example connecting, capturing, or surrounding formance comparison. To prevent games from continuing for
            territory. In our case, weights are learned that directly con- excessive numbers of moves, agents were not allowed to play
            tribute to winning the game of 9 × 9 Go. At the end of each within their own single-point eyes [4].
            game, the agent is given a reward of r =1for a win and  To test the performance of an agent, we used a simple tac-
            r =0for a loss.                                       tical algorithm for the opponent: ALP, the average liberty
              The value function V π(s) is deﬁned to be the expected player. To evaluate a position, the average number of liberties
            reward from board position s when following policy π,or of all opponent blocks is subtracted from the average over the
            equivalently the probability of winning the game. We form player’s blocks. Any ties are broken randomly. For every 500
            an approximation V (s) to the value function by taking a lin- games of self-play, 100 test games were played between each
                                                                  agent in the pair and ALP.
                                                                    We performed several experiments, each consisting of 50
                      Template   Location   Location
                      size     independent  dependent             separate runs of 100,000 training games. At the end of train-
                               n    N      n   N                  ing, a ﬁnal 1,000 test games were played between each agent
                                 i   i      i    i                                              5 × 5
                      1 × 1    25   3      25  27                 and ALP. All games were played on  boards. The learn-
                      2 × 1    40   9      40  54                 ing rate in each experiment was measured by the average
                      2 × 2    16   81     16  324                number of training games required for both agents to exceed
                      3 × 2    24   729    24  2916               a 50% win rate against ALP, during ongoing testing. Over-
                      3 × 3    9    19683  9   78732              all performance was estimated by the average percentage of
                      4 × 3    12   h      12  h                  wins of both agents during ﬁnal testing. All experiments were
                      4 × 4    4    h      4   h                  run with α =0.1 and an exploration rate of  =0.1 during
                      5 × 4    4    h      4   h                  training and  =0during testing. For shape sets using hashed
                      5 × 5    1    h      1   h                  weight sharing, the number of bins was set to h = 100, 000.
                                                                    In the ﬁrst set of experiments, agents were trained using
               Figure 3: Number of shapes in each set for 5 × 5 Go. just one set of shapes, with one experiment each for the 1 × 1

                                                            IJCAI-07
                                                              1055            location independent shape set, up to the 5 × 5 location de- To approximate a canonical solution, we introduce the on-
            pendent shape set. The remaining experiments measured the line cascade algorithm. This calculates multiple approxima-
            effect of combining shape sets of various degrees of general- tions to the value function, each one based on a different sub-
            ity. For each shape set Si, an experiment was run using all set of all features. For each shape set Si, a TD-error δi is cal-
            shape sets as or more general, {Sj : Gj,i}.           culated, based on the evaluation of all shape sets as or more
              Amongst individual shape sets, the 2 × 2 shapes perform general than Si, and ignoring any less general shape sets. The
            best (Figure 4), achieving a 25% win rate within just 1000 corresponding value function approximation is then updated
            games, and surpassing an 80% win rate after further train- according to this error,
            ing. Smaller shapes lack sufﬁcient representational power,              ⎛                     ⎞
            and larger shapes are too numerous and speciﬁc to be learned              m  nj
                                                                                    ⎝                     ⎠
            effectively within the training time. Location independent     Vi(s)=σ           Gj,iφj,k(s)θj,k      (4)
            sets appear to outperform location dependent sets, and learn              j=1 k=1
            considerably faster when using small shapes. If training runs       δ = r + V (s  ) − V (s )
            were longer, it seems likely that larger, location dependent         i       i  t+1    i t            (5)
            shapes would become more effective.                                     α
              When several shape sets are combined together, the per-      Δθj,k =     δjφj,kVj (st)(1 − Vj(st))  (6)
                                                                                   mnj
            formance surpasses a 90% win rate. Location independent
            sets again learn faster and more effectively. However, a mix- By calculating separate TD-errors, the weights of the most
            ture of different shapes appears to be a robust and successful general features will approximate the best representation pos-
            approach. Learning time slows down approximately linearly sible from just those features. Simultaneously, the speciﬁc
            with the number of shape sets, but may provide a small in- features will learn any weights required to locally correct the
            crease in performance for medium sized shapes.        abstract representation. This prevents the speciﬁc features
                                                                  from accumulating any knowledge that can be represented
                                                                  at a more general level, and leads to a canonical and easily
            7  Board growing                                      interpreted weight vector.
            The training time for an agent performing one-ply search is
            O(k4) for k × k Go, because both the number of moves and 9 Generalised shape features
            the length of the game increase quadratically with the board The local shape features used so far specify whether each in-
            size. Training on small boards can lead to signiﬁcantly faster tersection is empty, black or white. However, the templates
            learning, if the knowledge learned can be transferred appro- can be extended to specify the value of additional features at
            priately to larger boards.                            each intersection. This provides a simple mechanism for in-
              Of course, knowledge can only be transferred when equiv- corporating global knowledge, and to increase the expressive
            alent features exist on different board sizes, and when the ef- power of the representation.
            fect of those features remains similar. Local shape features One natural extension is to use liberty templates,which
            satisfy both of these requirements. A local shape feature on incorporate an external liberty count at each stone, in addi-
            a small board can be aligned to an equivalent location on a tion to its colour. An external liberty is a liberty of a block
            larger board, relative to the corner position. The weights for that lies outside of the template. The corresponding count
            each local shape feature are initialised to the values learned measures whether there is zero, one, or more than one exter-
            on the smaller board. Some new local shape features are in- nal liberty for a particular stone. This provides a primitive
            troduced in the centre of the larger board; these do not align measure of a stone’s strength beyond the local shape. A lo-
            with any shape in the smaller board and are initialised with cal liberty feature returns a binary value indicating whether
            zero weights.                                         its liberty template matches the current position. There are a
              Using this procedure, we started learning on a 5 × 5 board, large number of local liberty features, and so we use hashed
            and incremented the board size whenever a win rate of 90% weight sharing for these shapes.
            against ALP was achieved by both agents.
                                                                  10   Results
            8  Online cascade                                     To conclude our study of shape knowledge, we evaluated the
                                                                  performance of our shape-based agents at 9 × 9 Go against a
            The local shape features in our representation are linearly de- variety of established Computer Go programs. We trained a
            pendent; the inclusion of more general shapes in the partial ﬁnal pair of agents by coadaptive self-play, using the online
            order introduces much redundancy. There is no unique solu- cascade technique and the board growing method described
            tion giving the deﬁnitive value of each shape; instead there is above. The agents used all shapes from 1 × 1 up to 3 × 3,
            a large subspace of optimal weight vectors.           including both location independent and location dependent
              Deﬁning a canonical optimal solution is desirable for two shapes based on both local shape features and local liberty
            reasons. Firstly, we would like the goodness of each shape to features, for a total of around 1.5 million weights.
            be interpretable by humans. Secondly, a canonical weight To evaluate each agent’s performance, we connected it to
            vector provides each weight with an independent meaning the Computer Go Server1 and played around 50 games of
            which is preserved between different board sizes, for example
            when using the board growing procedure.                  1http://cgos.boardspace.net/9x9.html

                                                            IJCAI-07
                                                              1056Figure 4: (Top left) Percentage test wins against ALP after training with an individual shape set for 100,000 games. (Top right)
Number of training games with an individual shape set required to achieve 50% test wins against ALP. (Bottom left) Percentage
test wins using all shape sets as or more general. (Bottom right) Training games required for 50% wins, using all shape sets as
or more general.

9 × 9 Go with a 10 minute time control. At the time of writ- CGOS name Program description Games Elo rating
ing, this server includes over ﬁfty widely varying Computer Linear-B Shape features      71      +1070
Go programs. Each program is assigned an Elo rating ac- Linear-L   Shape and liberty features 28 +1140
cording to its performance on the server, currently ranging Linear-S Shape features and search 43 +1210
from around -170 for the random player up to +1860 for the
                                                            Figure 5: CGOS ratings attained by trained agents.
strongest current programs. Programs based on a static evalu-
ation function with little prior knowledge include ALP (+850)
and the Inﬂuence Player (+700). All of the stronger programs
on the server incorporate sophisticated search algorithms or stones in the corner is bad, but that surrounding the corner
                                                                                                    3 × 2
complex, hand-encoded Go knowledge.                   is good; and that connected stones are powerful. The
                                                      shapes show the value of cutting the opponent stones into sep-
  The agent trained with local shape features attains a rating          3 × 3
of +1070, signiﬁcantly outperforming the other simple, static arate groups, and the shapes demonstrate three different
evaluators on the server. When local liberty features are used ways to form two eyes in the corner. Each specialisation of
as well, the agent’s rating increases to +1140 (Figure 5). Fi- shape adds more detail; for example playing one stone in the
nally, when the basic evaluation function is combined with a corner is bad, but playing two connected stones in the corner
full-width, iterative-deepening alpha-beta search, the agent’s is twice as bad.
performance increases to +1210.                         However, the whole is greater than the sum of its parts.
                                                      Weights are learned for over a million shapes, and the agent’s
11   Discussion                                       play exhibits global behaviours beyond the scope of any sin-
                                                      gle shape, such as territory building and control of the cor-
The shape knowledge learned by the agent (Figure 6) repre- ners. Its principle weakness is its local view of the board;
sents a broad library of common-sense Go intuitions. The the agent will frequently play moves that look beneﬁcial lo-
1 × 1 shapes encode the basic value of a stone, and the value cally but miss the overall direction of the game, for example
of each intersection. The 2 × 1 shapes show that playing adding stones to a group that has no hope of survival. Our

                                                IJCAI-07
                                                  1057