               The Effect of Restarts on the Efﬁciency of Clause Learning

                                             Jinbo Huang
                                    Logic and Computation Program
                                         National ICT Australia
                                     Canberra, ACT 0200 Australia
                                       jinbo.huang@nicta.com.au

                    Abstract                          rithm known as DPLL [Davis et al., 1962], which solves SAT
                                                      by selecting a variable and determining, recursively, whether
    Given the common use of restarts in today’s clause the formula can be satisﬁed by setting that variable to either
    learning SAT solvers, the task of choosing a good value. In fact, the initial intuition for learning clauses and
    restart policy appears to have attracted remarkably appending them to the CNF formula was to help prune the
    little interest. On the other hand, results have  DPLL search, as discussed in earlier work [Marques-Silva
    been reported on the use of different restart poli- and Sakallah, 1996].
    cies for combinatorial search algorithms. Such re-
    sults are not directly applicable to clause learn-  It has been shown, however, that clause learning as prac-
    ing SAT solvers, as the latter are now under-     ticed in today’s SAT solvers, assuming unlimited restarts, cor-
    stood as performing a form of resolution, some-   responds to a proof system exponentially more powerful than
                                                                  [                ]
    thing fundamentally different from search (in the that of DPLL Beame et al., 2004 . Speciﬁcally, each learn-
    sense of backtracking search for satisfying assign- ing step is in fact a sequence of resolution steps, of which
    ments). In this paper we provide strong evidence  the learned clause is the ﬁnal resolvent; conversely, a reso-
    that a clause learning SAT solver could beneﬁt sub- lution proof can be simulated in polynomial time by repeat-
    stantially from a carefully designed restart policy edly (i) learning each of the resolvents in the proof and (ii)
    (which may not yet be available). We begin by     restarting (this assumes a deviation from standard practice:
    pointing out that the restart policy works together the freedom to ignore assignments made by unit propagation).
    with other aspects of a SAT solver in determining Clause learning can hence be as powerful as general resolu-
    the sequence of resolution steps performed by the tion, while DPLL has been known to correspond to the expo-
                                                                                     [               ]
    solver, and hence its efﬁciency. In this spirit we nentially weaker tree-like resolution Beame et al., 2004 .
    implement a prototype clause learning SAT solver    Despite the dependence of this theoretical result on the as-
    that facilitates restarts at arbitrary points, and con- sumption of unlimited restarts, remarkably little has been said
    duct experiments on an extensive set of industrial in the literature on the importance of choosing a good restart
    benchmarks using various restart policies, includ- policy in practice. This is in stark contrast, for example, to the
    ing those used by well-known SAT solvers as well  sustained quest by researchers for better decision heuristics.
    as a universal policy proposed in 1993 by Luby    We argue that this imbalance of attention is doing a disservice
    et al. The results indicate a substantial impact of to the SAT community, because with the modern understand-
    the restart policy on the efﬁciency of the solver, and ing of clause learning, it can be seen that the decision heuris-
    provide motivation for the design of better restart tic, together with the restart policy and other components of
    policies, particularly dynamic ones.              the solver, determines the sequence of resolution steps per-
                                                      formed by the solver and hence its efﬁciency.
                                                        In this paper we would like to take a step toward study-
1  Introduction                                       ing the importance of restart policies in clause learning SAT
Propositional satisﬁability (SAT) is the problem of determin- solvers, with a view to motivating further work on designing
ing whether a propositional formula, traditionally in conjunc- more effective policies. To this end we have created a small
tive normal form (CNF), has a satisfying assignment—an as- prototype SAT solver, called TINISAT, which implements the
signment of truth values to its variables making it evaluate to essentials of a modern clause learning solver and is designed
true. In this paper we focus on a class of algorithms for SAT to facilitate adoption of arbitrary restart policies. After choos-
that has become known as conﬂict-driven clause learning,or ing and ﬁxing a reasonably effective decision heuristic, we
clause learning for short. These algorithms are currently the conducted experiments using an extensive set of large indus-
best for large SAT instances that arise from industrial appli- trial benchmarks, on which we ran versions of TINISAT using
cations, such as formal veriﬁcation [Berre and Simon, 2005]. different restart policies including those used by well-known
  Clause learning SAT solvers have grown out of their prede- SAT solvers and particularly one proposed in [Luby et al.,
cessors that implemented variants of a systematic search algo- 1993] based on a sequence of run lengths of the following

                                                IJCAI-07
                                                  2318form: 1, 1, 2, 1, 1, 2, 4, 1, 1, 2, 1, 1, 2, 4, 8,...The results we tions have deterministic behavior,2 and (iv) the ﬁrst decision
have obtained indicate a substantial impact of the restart pol- is made in decision level 2:level1 is reserved for literals
icy on the efﬁciency of the solver. Speciﬁcally, all nontrivial found to be implied by the CNF formula, and level 0 to signal
restart policies we experimented with did signiﬁcantly better derivation of the empty clause. The functions involved have
than if restarts were disabled, and exhibited considerably dif- the following semantics:
ferent performance among themselves. More interestingly, • selectLiteral uses some decision heuristic to select a
this difference in performance appears more marked when   free variable and then select one of its two literals, and
one looks at individual benchmarks or benchmark families, returns it, or returns nil if no free variables exist.
as opposed to the whole set in aggregate, which suggests that
                                                        • decide
substantial performance gains may be possible by using ap-       increments the decision level, sets the given lit-
propriate dynamic restart policies (all policies compared in eral to true, and performs unit propagation; it returns true
this paper are static ones).                              iff no empty clause is derived.
  The rest of the paper is organized as follows: We present • learnClause performs 1-UIP learning to derive an im-
the simple design of TINISAT and use it as the basis for dis- plicate of the CNF formula, and sets the assertion level
cussing the semantics of modern clause learning, leading to (i) to 0 if the empty clause is derived, (ii) to 1 if a unit
an analytical explanation why restart policies are important. clause is derived, and otherwise (iii) to the second high-
We then describe our experimental setup including the vari- est decision level among literals of the derived clause.
ous restart policies we shall use and our attempts to identify a
                                                        • assertionLevel returns the assertion level, which has
reasonable decision heuristic so that all policies can be tested
                                                          been set by the last call to learnClause.
on competitive ground. We then report the results obtained
and make a number of important observations. Finally, we • restartPoint returns true iff the solver is to restart now
discuss related work and present our conclusions.         according to some restart policy.
                                                        • backtrack(k) undoes all variable assignments in deci-
2  Essentials of Clause Learning                          sion levels >k, and sets the decision level to k.
We start by presenting the design of a simple SAT solver, • assertLearnedClause adds the learned clause to the
TINISAT, that (i) boasts the essentials of modern clause learn- clause pool, performs unit propagation if the current de-
ing technology, and (ii) provides a basis for our discussion of cision level equals the assertion level (this is the condi-
the importance of restart policies later in the section. The tion under which the learned clause becomes unit), and
top-level procedure of TINISAT, implemented in under 800  returns true iff no empty clause is derived.
lines of C++, is given in Algorithm 1, which operates on an
implicit CNF formula whose satisﬁability is in question. 2.1 Restarts and Backtracks Uniﬁed
                                                      Note that under this design all that is needed to adopt a given
Algorithm 1 TINISAT                                   restart policy is to implement restartPoint accordingly. It is
 1: loop                                              also interesting to note that a normal backtrack after learn-
 2:  if (literal = selectLiteral()) == nil then       ing (Line 12) and a complete restart (Line 10) can both be
 3:    return SATISFIABLE                             regarded as special cases of a more general scheme [Lynce
 4:  if !decide(literal) then                         and Silva, 2002] where the solver can backtrack to any level
 5:    repeat                                         between 0 and the current decision level (exclusive). What
 6:      learnClause()
 7:      if assertionLevel() == 0 then                we would like to stress here, however, is that the partic-
 8:       return UNSATISFIABLE                        ular scheme used by most clause learning SAT solvers to-
 9:      if restartPoint() then                       day, namely backtracking to the assertion level (except when
10:        backtrack(1)                               restarting), has obscured their original characteristics (inher-
11:      else                                         ited from DPLL) as systematic search algorithms. In particu-
12:        backtrack(assertionLevel())                lar, these solvers do not perform branching anymore: The set-
13:    until assertLearnedClause()                    ting of a literal occurs on Line 4, but the setting of its negation
                                                      is never explicitly tried (and possibly never tried at all even
  The following components of a modern clause learn-  implicitly).
ing SAT solver can be identiﬁed in Algorithm 1: deci-   For example, suppose assignments {A, B, C,D} have
sion heuristic (selectLiteral), unit propagation (decide, as- been made in decision levels 2, 3, 4, 5, respectively, before the
sertLearnedClause), clause learning (learnClause, back- empty clause is derived in level 5, and suppose the following
track), restarts (restartPoint, backtrack). We assume famil- clause is learned: A ∨ D. This clause says that the two deci-
iarity with the common terminology for DPLL and clause sions made in levels 2 and 5 alone are responsible for the con-
learning algorithms, and assume that (i) 1-UIP [Zhang et ﬂict. Now, despite the fact that simply ﬂipping the value of
al., 2001] is used as the learning scheme, (ii) no clauses are variable D in level 5 could well result in a satisﬁable subprob-
ever deleted (hence the algorithm is complete1), (iii) all func- lem, Line 12 insists on taking the solver back to the assertion
  1It can be shown that each learned clause is subsumed by no 2The learning scheme, clause deletion policy, and use of ran-
existing clause [Zhang, 2005]. Since the algorithm keeps learning domness are all important factors that affect the efﬁciency of clause
distinct clauses it will always terminate.            learning SAT solvers, but are beyond the scope of this paper.

                                                IJCAI-07
                                                  2319level, which is 2, erasing assignments {D, C, B} on the way. efﬁciency of clause learning SAT solvers even when no ran-
The learned clause then gets asserted in level 2 (Line 13), domness is present (which will be the case in our experiments
and implies D (because the assignment A is present, making with TINISAT in Section 4).
the clause unit), triggering a round of unit propagation. No- In this section we have provided a new understanding of
tice that the branches {A, B, C} and {A, B}, which can well modern clause learning through the design of TINISAT, a con-
contain solutions, have been skipped over without ever being crete and simple clause learning SAT solver, leading to an an-
explored.                                             alytical argument that the restart policy matters. We now pro-
  It should be emphasized here, as we already alluded to, ceed to support this argument with an empirical study of con-
that this behavior is not a peculiarity of TINISAT, but is the crete restart policies using real-world SAT benchmarks and
common practice of most current clause learning SAT solvers the TINISAT solver.
we have seen, including Chaff, BerkMin, MiniSat, and Siege.
(The earlier solver GRASP, though, used a different back- 3 Experimental Setup
tracking scheme such that no branch was skipped over unless
proven to contain no solutions.)                      We describe in this section the restart policies we shall ex-
                                                      periment with, the decision heuristic to be used with these
2.2  Importance of Restarts                           policies in the experiments, and our choice of benchmarks.
This shift of paradigm in backtracking, as we discussed, ob- 3.1 Restart Policies
scures the characteristics of clause learning SAT solvers as In choosing the set of restart policies for our empirical study,
systematic search algorithms. For this reason we propose to we have aimed to include those that are currently used by
view the modern practice of clause learning not as a version well-known SAT solvers as well as some less conventional
                                   3
of DPLL search enhanced with resolution, but as a pure res- ones that may have escaped the attention of the clause learn-
olution algorithm in its own right. In fact, what Algorithm 1 ing community. Speciﬁcally, we shall experiment with the
does is nothing other than the following 3-step cycle: following seven restart policies:
    (1) set variables till hitting a conﬂict;           • N: a policy calling for no restarts at all.
    (2) derive a clause by resolution;                  • M: a geometric policy used in MiniSat v1.14 [E´en and
                                                          S¨orensson, 2005] with an initial restart interval of 100
    (3) unset some variables and go back to (1).
                                                          conﬂicts, which increases by a factor of 1.5 after each
For unsatisﬁable formulas, this loop terminates on deriving restart. We will denote it by (100, 1.5).
the empty clause in (2); for satisﬁable formulas, it terminates • Z: a ﬁxed-interval policy used in Chaff II, also known
when (1) “happens” to exhaust all variables without conﬂict. as the 2004 version of zChaff [Moskewicz et al., 2001],
  In this context the importance of the restart policy be- with a restart interval of 700 conﬂicts, denoted (700, 1).
comes prominent: Together with the existing backtracking
                                                        • B: a ﬁxed-interval policy used in BerkMin [Goldberg
scheme, it dictates the set of assignments to undo in (3),
                                                          and Novikov, 2002] with a restart interval of 550 con-
which, together with the decision heuristic, ultimately de-
                                                          ﬂicts, denoted (550, 1).
termines the entire sequence of resolution steps performed
in (2). In other words, the decision heuristic, backtracking • G: a geometric policy (32, 1.1), which we have added to
scheme, and restart policy can all be understood as serving a improve the balance between ﬁxed-interval and geomet-
single purpose, that of guiding the resolution process.   ric policies we consider.
  Consider for example a clause learning SAT solver that has • S: a ﬁxed-interval policy used in Siege [Ryan, 2004]
run on a hard instance for a period of time without restarts. with a restart interval of 16000 conﬂicts, denoted
The solver has now accumulated a considerable number of   (16000, 1).
learned clauses, which have helped update the variable and
                                                        •
literal scores as are maintained by decision heuristics typical L: a class of policies proposed in [Luby et al., 1993] for
in clause learning SAT solvers. These new scores represent, randomized algorithms based on the following sequence
                                                                         1, 1, 2, 1, 1, 2, 4, 1, 1, 2, 1, 1, 2, 4, 8,...
in a way, the solver’s current state of belief about the order in of run lengths:
which future decisions should be made, having taken into ac- (deﬁned below). In our experiments we take a
count all the conﬂicts discovered so far. Without the freedom “unit run” in this sequence to be 32 conﬂicts (we
of restarts, however, the solver would not be able to fully ex- have experimented with other units as well; see
ecute its belief because it is bound by the decisions that have http://rsise.anu.edu.au/˜jinbo/tinisat/). Hence the actual
                                                                             32, 32, 64, 32, 32, 64, 128,...
been made earlier. In particular, note that these early deci- restart intervals are:                  We
sions were made without the beneﬁt of the new knowledge in denote this policy by (Luby’s, unit=32).
the form of all the conﬂicts discovered since. This, we be- The ﬁrst six of these policies are straightforward, while
lieve, is the main reason why restarts can help improve the Luby’s policy can be formally deﬁned as the sequence
                                                      t1,t2,t3,...such that:
  3Recall that each learning step is a sequence of resolution steps 
                                                                    k−1              k
the ﬁnal resolvent of which is recorded as the “learned clause”    2   ,ifi       =2   − 1;
                                                            ti =                  k−1       k
[Beame et al., 2004].                                              ti−2k−1+1,if2     ≤ i<2   −  1.

                                                IJCAI-07
                                                  2320  We have chosen Luby’s policy because of an interesting solvers, allowing us to complete all the experiments in about
property it has: In the context of a particular class of ran- 80 CPU days.
domized algorithms, known as Las Vegas algorithms, [Luby The overall results are shown in Table 1. In the second
et al., 1993] proved that this policy is universally optimal in and third columns we report for each benchmark family the
the sense that (i) it achieves an expected running time that is number of instances and their total size (in megabytes). In the
only a logarithmic factor slower than the true optimal policy, remaining columns we report the number of instances solved
which is determined by the speciﬁc running time distribution by each solver for each benchmark family. The total number
of the algorithm on a speciﬁc problem instance, and (ii) no of instances solved by each solver and the total time it spent
other universal policy can do better by more than a constant on all instances (including the 2 hours in case it did not solve
factor. (The theoretical relevance of this property to clause the instance) are reported in the two bottom rows.
learning remains an interesting question though.)       The ﬁrst observation we make from these results is that
                                                      restarts deﬁnitely helped: All the six nontrivial policies did
3.2  Decision Heuristic                               signiﬁcantly better than the no-restarts policy. Even the least
To make our comparison of restart policies more meaning- effective of them allowed TINISAT to ﬁnish 2.43 days sooner
ful, we have taken steps to ensure that other components and solve 37 more instances than the no-restarts policy.
of the SAT solver are tuned toward their best performance. Our second observation is that Luby’s universal policy ap-
Rather than low-level optimizations, however, we focused pears to outperform all the rest on this particular set of bench-
on choosing a reasonably effective decision heuristic. Based marks (albeit by only a small margin in some cases). Given
on experiments using a subset of our full benchmark suite that the optimality of Luby’s policy was originally proved for
(described below), we found that a version of the VSIDS Las Vegas algorithms, this empirical result provides motiva-
heuristic [Moskewicz et al., 2001] combined with BerkMin’s tion for extending the theoretical study of Luby’s policy to
practice of choosing literals from recent unsatisﬁed conﬂict clause learning algorithms.
clauses [Goldberg and Novikov, 2002] tended to work well. To give a more concrete picture of the impact of the restart
  Speciﬁcally, for each literal we keep a score that is ini- policy on the efﬁciency of the solver, we present detailed re-
tially the number of its occurrences in the original clauses. On sults in Tables 2 and 3 for two of the benchmark families
learning a clause, we increment the score of every literal by 1 where TINISAT solved all instances using every policy. For
for each of its occurrences in clauses that are involved in the space constraints we only include three policies in each ta-
resolution process. The scores of all literals are halved once ble: the no-restarts policy and the worst and best of the rest.
every 128 conﬂicts. When a decision is called for (Line 2 of Results on Siege are also included as a reference point.
Algorithm 1), we pick a (free) literal with the highest score An interesting observation to make from Tables 2 and 3 is
from the most recently learned clause that has not been sat- that the difference in performance between the restart poli-
isﬁed, and set it to true; if no such clause exists we pick any cies becomes more substantial now that we look at individual
(free) literal with the highest score.                benchmark families, as opposed to the whole set in aggregate
                                                      (bottom two rows of Table 1). For example, policy L out-
3.3  Benchmarks                                       performs policy M in running time by only a factor of 1.2
We use the entire set of industrial benchmarks distributed in Table 1, but a factor of 2.5 in Table 2. In fact we can
by Miroslav Velev  of Carnegie Mellon  University at  also see in Table 1 that none of the policies is consistently
http://www.ece.cmu.edu/˜mvelev/, except sss.1.0, sss.1.0a, best across all benchmark families (in terms of the number
sss-sat-1.0, vliw-sat-1.0, and vliw-sat-1.1 as they are too of solved instances). While this explains the decreased dif-
easy,4 and dlx-iq-unsat-2.0 as the download appeared to be ference between policies when results are aggregated, it also
incomplete. This gives us 22 benchmark families with 251 suggests that substantial performance gains would be possible
instances totaling about 25GB in size—hence the CNF for- if one were to use an appropriate dynamic policy that could
mulas have an average size of about 100MB.            adapt to a given benchmark or benchmark family.
                                                        We would like to remind the reader here that all these ex-
                                                      periments with TINISAT were conducted with a ﬁxed deci-
4  Results                                            sion heuristic, learning method, and backtracking scheme. As
Our experiments consist of running TINISAT with each of we have discussed, all these components work in combina-
the seven restart policies on the entire set of benchmarks. tion with the restart policy to determine the efﬁciency of the
For additional reference points, we have also run MiniSat solver. Hence we view the design of better restart policies as
v1.14 [E´en and S¨orensson, 2005] and Siege v4 [Ryan, 2004] an opportunity among possibly many others of bringing about
(given seed 123456789 for its random number generator) on a new generation of clause learning SAT solvers. Finally,
the same set of benchmarks. All our experiments were con- we note that detailed results of the experiments described in
ducted on a cluster of 16 AMD Athlon 64 processors running this section, as well as TINISAT, can be downloaded from
at 2GHz with 2GB of RAM  under SuSE Linux 9.3 Profes- http://rsise.anu.edu.au/˜jinbo/tinisat/.
sional. A time limit of 2 hours was imposed on all runs of the
                                                      5   Related Work
  4These ﬁve families contain a total of 357 instances, which were
solved by Siege v4 in 428 seconds, TINISAT-L in 506 seconds, Min- While in this work we have focused on restart policies for
iSat v1.14 in 661 seconds, and Chaff II in 1971 seconds. clause learning, in previous work researchers have studied

                                                IJCAI-07
                                                  2321 Table 1: Overall results on running TINISAT                  with seven restart policies: N = No restarts; M = (100, 1.5); Z = (700, 1); B =
(550, 1); G = (32, 1.1); S = (16000, 1); L = (Luby’s, unit=32). Cutoff was 2 hours.
                        Benchmark             Number of      Size                             Number of Instances Solved
                        Family                 Instances    (MB)        N        M       Z       B       G       S       L      MiniSat     Siege
                        dlx-iq-unsat-1.0          32         4543       10       32      32      32      32      32      32        23        17
                        engine-unsat-1.0          10          62         7       7       8       7       7       7       7         7          7
                        fvp-sat.3.0               20         414         1       16      9       9       18      17      14        11        20
                        fvp-unsat.1.0             4           4          4       4       4       4       4       4       4         4          4
                        fvp-unsat.2.0             22          74        22       22      22      22      22      22      22        20        22
                        fvp-unsat.3.0             6          243         0       0       0       0       0       1       0         0          5
                        liveness-sat-1.0          10         1264        6       5       8       9       6       4       7         7          6
                        liveness-unsat-1.0        12         1134        4       4       4       4       4       4       4         4          4
                        liveness-unsat-2.0        9          537         3       3       3       3       3       3       3         3          3
                        npe-1.0                   6          695         2       4       3       4       3       4       3         3          3
                        pipe-ooo-unsat-1.0        15         1452        6       7       7       7       7       7       7         3          8
                        pipe-ooo-unsat-1.1        14         775         7       8       8       8       8       8       8         3          9
                        pipe-sat-1.0              10         1662        9       6       9       10      5       9       10        6         10
                        pipe-sat-1.1              10         984         9       7       10      10      10      10      10        8         10
                        pipe-unsat-1.0            13         989         8       8       9       8       8       8       9         4         12
                        pipe-unsat-1.1            14         760         9       9       11      11      11      10      11        4         14
                        vliw-sat-2.0              9          1611        5       8       5       6       8       9       9         6          9
                        vliw-sat-2.1              10         3680        4       2       1       2       5       6       5         2          2
                        vliw-sat-4.0              10         3076       10       10      10      10      10      10      10        10         7
                        vliw-unsat-2.0            9          695         0       1       1       1       2       2       2         0          7
                        vliw-unsat-3.0            2          124         0       0       1       0       0       0       2         0          2
                        vliw-unsat-4.0            4          405         1       1       1       1       1       1       1         0          1
                        Total                    251        25180      127      164     166     168     174     178     180       128       182
                        Total Time on All Instances (days)             11.28    8.85    8.49    8.56    8.00    7.80    7.68     11.45      7.45

 Table 2: Detailed results for fvp-unsat-2.0, all unsatisﬁable except 7pipe                     bug. Abbreviations: Dec. (decisions), Con. (conﬂicts),
Res. (restarts). Times are in seconds. Three policies included: Policy N (no restarts) and the worst and best of the rest.

  Benchmark      Number of                  TINISAT-N                           TINISAT-M                           TINISAT-L                          Siege
                Vars   Clauses    Dec.      Con.    Res.  Time (s)    Dec.      Con.    Res.  Time (s)    Dec.      Con.   Res.   Time (s)    Dec.     Con.    Time (s)
  2pipe          892     6695       3014      1170     0      0.04      3174      1287     4      0.05      2938     1066     16     0.04      4155     1741      0.06
  2pipe 1 ooo    834     7026       2509      1089     0      0.04      3147      1387     5      0.06      2766     1210     19     0.05      4541     2423      0.10
  2pipe 2 ooo    925     8213       2895      1371     0      0.06      3085      1356     5      0.06      3133     1412     22     0.06      4552     2269      0.09
  3pipe         2468    27533      19265      6566     0      0.54     22157      8404     9      0.74     20805     7599     86     0.72     25197     8718      0.53
  3pipe 1 ooo   2223    26561      11702      5455     0      0.45     12629      5245     8      0.44     15806     6257     65     0.59     16073     7620      0.49
  3pipe 2 ooo   2400    29981      16688      7435     0      0.70     23334      9943     9      1.04     23500    10164   117      1.14     23858     9465      0.69
  3pipe 3 ooo   2577    33270      22712     10182     0      1.09     27407     12487    10      1.37     23423     9038   100      1.05     26745    11997      1.01
  4pipe         5237    80213      69238     24060     0      5.46     95579     37319    12      8.49     80798    25041   250      6.17     98531    31725      4.03
  4pipe 1 ooo   4647    74554      44644     18764     0      3.72     55785     23445    11      5.35     56859    22797   220      5.06     59881    27800      3.51
  4pipe 2 ooo   4941    82207      54429     22013     0      5.14     71958     29928    12      7.36     74383    26652   253      6.70     70755    30660      4.15
  4pipe 3 ooo   5233    89473      65850     22820     0      5.74     88936     34633    12      9.50     76519    23385   227      6.22     82797    33116      4.93
  4pipe 4 ooo   5525    96480      73743     30316     0      8.33    169434     81833    14     35.62     88729    29030   254      8.17     90161    36656      5.53
  5pipe         9471   195452     101530     18176     0      6.03    110712     14569    10      4.27    131655    14135   126      4.67    139115    15618      2.58
  5pipe 1 ooo   8441   187545      94343     30709     0     11.49    117321     37638    12     18.16    104141    31560   254     12.43    146601    58115     13.55
  5pipe 2 ooo   8851   201796      97195     33725     0     13.58    140816     48862    13     29.24    123345    33414   268     15.02    145892    55019     13.60
  5pipe 3 ooo   9267   215440     123283     33839     0     17.53    149626     48628    13     28.17    120410    30881   254     13.96    133506    47582     12.06
  5pipe 4 ooo   9764   221405     216620     78339     0     52.23    265416     87930    15     56.29    236519    64407   509     35.57    254647    90128     23.14
  5pipe 5 ooo  10113   240892     116861     37552     0     17.99    141425     42796    13     24.41    121361    30269   254     15.50    150600    47137     12.47
  6pipe        15800   394739     577706    208817     0    273.60    571806    184524    16    192.17    453483    88171   640     73.44    518038    91997     28.77
  6pipe 6 ooo  17064   545612     496762    124712     0    181.56    463974    125891    15    153.58    404756    85444   636     81.42    485418   146967     63.59
  7pipe        23910   751118   1188862     388233     0    764.32  1168826     318819    18    497.61    876368  130092    998    171.05    997777   131085     59.19
  7pipe bug    24065   731850     166382     17509     0     13.89    542511    143273    16    119.26    144616    14526   131     10.27    282794    12309      4.21
  Total                         3566233   1122852      0  1383.53   4249058   1300197    252  1193.24    3186313  686550   5699     469.3   3761634   900147    258.25

 Table 3: Detailed results for vliw-sat-4.0, all satisﬁable. Abbreviations: Dec. (decisions), Con. (conﬂicts), Res. (restarts).
 Times are in seconds. Three policies included: Policy N (no restarts) and the worst and best of the rest. Cutoff was 2 hours.

 Benchmark        Number of                   TINISAT-N                           TINISAT-S                          TINISAT-L                          Siege
               Vars    Clauses      Dec.      Con.    Res.  Time (s)     Dec.      Con.   Res.  Time (s)    Dec.      Con.   Res.  Time (s)    Dec.     Con.   Time (s)
 bug1         521188  13378641     9881446    604996     0   2199.98    8521585    49471     3   242.00    6064544    22613   220    236.31  4138878   26364     154.52
 bug2         521158  13378532     7680490    267252     0    712.04    6189255    17318     1   118.74    6274425    11959   125    174.57  3185190   16616     102.06
 bug3         521046  13376161     4447142      2098     0     57.72    4447142     2098     0    57.75    3327664     2781    36     76.90  4460466   24509     153.75
 bug4         520721  13348117     7670838    563100     0   2045.31    6246054    43329     2   229.49    5710694    16378   156    187.37  4092308   19896     132.98
 bug5         520770  13380350     7754522     86586     0    582.81    7459833    29041     1   190.47    4620905     7980    92    133.09      –       –        –
 bug6         521192  13378781     8808865    362411     0   1246.53    7811661    44263     2   219.33    5449328    13250   126    160.90  4138295   37005     199.82
 bug7         521147  13378010     7893893    595572     0   1603.23    6125501    19414     1   115.87    4070162     8026    92    120.43  4008315   15148     103.39
 bug8         521179  13378617     7793145    340079     0   1557.75    6166611    38239     2   216.89    4564989    10454   122    151.55      –       –        –
 bug9         521187  13378624     6045105     40574     0    289.25    6871992    46122     2   284.25    3547174     5713    62     97.72  4475278   30692     190.22
 bug10        521182  13378625     6935993     44236     0    252.06    8242265    56847     3   354.18    4681271    10683   123    150.14      –       –        –
 Total                            74911439   2906904     0  10546.68   68081899   346142    17  2028.97   48311156  109837   1154  1488.98                    11836.74


                                                                              IJCAI-07
                                                                                 2322