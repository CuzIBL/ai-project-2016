                           A Fast Analytical Algorithm for Solving
                Markov Decision Processes with Real-Valued Resources∗
                            Janusz Marecki     Sven Koenig     Milind Tambe
                                     Computer Science Department
                                    University of Southern California
                               941 W 37th Place, Los Angeles, CA 90089
                                  {marecki, skoenig, tambe}@usc.edu

                    Abstract                          its. One could model them with Markov decision processes
                                                      (MDPs) by encoding the available amounts of resources in
    Agents often have to construct plans that obey deadlines the states. The resulting continuous state MDP can then be
    or, more generally, resource limits for real-valued re- solved approximately by discretizing the state space (for ex-
    sources whose consumption can only be characterized
    by probability distributions, such as execution time or ample, by discretizing the probability distributions) and then
    battery power. These planning problems can be mod- applying standard dynamic programming algorithms, such as
    eled with continuous state Markov decision processes value iteration [Boyan and Littman, 2000]. A major draw-
    (MDPs) but existing solution methods are either inefﬁ- back of this approach is that discretizing the state space re-
    cient or provide no guarantee on the quality of the re- sults in a combinatorial explosion of the number of states
    sulting policy. We therefore present CPH, a novel so- as the discretization becomes more and more ﬁne-grained.
    lution method that solves the planning problems by ﬁrst In response, discretization-free approaches have been devel-
    approximating with any desired accuracy the probability oped, some of which do not provide strong guarantees on the
    distributions over the resource consumptions with phase- quality of the resulting policy [Lagoudakis and Parr, 2003;
    type distributions, which use exponential distributions as Nikovski and Brand, 2003; Hauskrecht and Kveton, 2003].
    building blocks. It then uses value iteration to solve the          [                  ]
    resulting MDPs by exploiting properties of exponential Lazy Approximation Li and Littman, 2005 is currently the
    distributions to calculate the necessary convolutions ac- leading discretization-free algorithm for solving continuous
    curately and efﬁciently while providing strong guaran- state MDPs with quality guarantees. It solves the planning
    tees on the quality of the resulting policy. Our experi- problems by approximating the probability distributions over
    mental feasibility study in a Mars rover domain demon- the resource consumptions with piecewise constant functions.
    strates a substantial speedup over Lazy Approximation, It then uses value iteration to solve the resulting MDPs by re-
    which is currently the leading algorithm for solving con- peatedly approximating the value functions with piecewise
    tinuous state MDPs with quality guarantees.       constant value functions. Lazy Approximation has two prob-
                                                      lems that increase its runtime: First, complex piecewise con-
1  Introduction                                       stant functions are often necessary to approximate probabil-
                                                      ity distributions closely. Second, the value functions need
Recent advances in robotics have made aerial, underwater
                                                      to be reapproximated after every iteration. We thus present
and terrestrial unmanned autonomous vehicles possible. In
                                                      CPH (Continuous state MDPs through PHase-type proba-
many domains for which such unmanned vehicles are con-
                                                      bility distributions), a novel solution method that solves the
structed, the vehicles have to construct plans that obey dead-
                                                      planning problems by ﬁrst approximating the probability dis-
lines or, more generally, resource limits for real-valued re-
                                                      tributions over the resource consumptions with phase-type
sources whose consumption can only be characterized by
                                                      distributions, which use exponential distributions as building
probability distributions, such as execution time or battery
                                                      blocks [Neuts, 1981]. It then uses value iteration to solve
power.  It is a major challenge for AI research to con-
                                                      the resulting MDPs by exploiting properties of exponential
struct good plans for these domains efﬁciently [Bresina et al.,
                                                      distributions to calculate the value functions after every itera-
2002]. One cannot model the resulting planning problems
                                                      tion accurately (= without reapproximations) and efﬁciently.
with generalized semi-Markov decision processes [Younes
                                                      CPH shares with Lazy Approximation that it provides strong
and Simmons, 2004] since they can model actions that con-
                                                      guarantees on the quality of the resulting policy but runs sub-
sume real-valued amounts of resources but not resource lim-
                                                      stantially faster than Lazy Approximation in our experimental
  ∗This material is based upon work supported by the  feasibility study in a Mars rover domain.
DARPA/IPTO COORDINATORS program and the Air Force Re-
search Laboratory under Contract No. FA875005C0030. The views
and conclusions contained in this document are those of the au- 2 The Planning Problem
thors and should not be interpreted as representing the ofﬁcial poli-
cies, either expressed or implied, of the Defense Advanced Research We use time as an example of a real-values resource and solve
Projects Agency or the U.S. Government.               planning problems that are similar to the ones studied in [Li

                                                IJCAI-07
                                                  2536                                                      expected total reward that the agent can obtain until execution
                  return      return
             start      base       site 3                                  s ∈  S                      ≤
                  to base    to base                  stops if it starts in state with time-to-deadline 0
                                                      t ≤ Δ. The agent can maximize its expected total reward by
               move  return return move               executing the action
               to site 1 to base to base to site 3                 X
                                                                          
                        move                                           P s |s, a
                  site 1     site 2                   arg max     (     (     )
                       to site 2                         a∈A(s)
                                                                   s∈S
                                                                      Z t
                                                                                         ∗          
            Figure 1: Mars rover domain.                                 ps,a(t )(R(s, a, s )+V (s )(t − t )) dt )
                                                                       0
                                                             s ∈ S                     ≤  t ≤
                ]                                      in state    with time-to-deadline 0   Δ, which can
and Littman, 2005 . They are modeled as MDPs with two                                           a ∈  A s
sources of uncertainty: action durations and action outcomes. be explained as follows: When it executes action ( )
                                                      in state s ∈ S, it incurs action duration t with probability
S denotes the ﬁnite set of states of the MDP and A(s) the                                     
                                                      ps,a(t ).If0 ≤ t ≤ t, then it transitions to state s ∈ S with
ﬁnite set of actions that can be executed in state s ∈ S.          
                            s ∈ S                     probability P (s |s, a) and obtains an expected total future re-
Assume that an agent is in state  with a deadline be-        R s, a, s V ∗ s t − t
ing t>0  time units away (= with time-to-deadline t), after ward of ( )+   ( )(    ).
which execution stops. It executes an action a ∈ A(s) of Value iteration calculates the values V n(s)(t) using the fol-
its choice. The execution of the action cannot be interrupted, lowing Bellman updates for all states s ∈ S, time-to-
and the action duration t is distributed according to a given deadlines 0 ≤ t ≤ Δ, and iterations n ≥ 0
                         
probability distribution ps,a(t ) that can depend on both the 0
                                                     V  (s)(t):=0
state s and the action a.Ift ≥ t, then the time-to-deadline       8
                                                                                                   t ≤
t − t ≤ 0 after the action execution is non-positive, which       < 0        P                    if   0
                                                        n+1                        P s|s, a
                                                      V    (s)(t):= maxa∈A(s)( s∈S  (    )      otherwise
means that the deadline is reached and execution stops. Oth-      :   R t
                                                                       p   t R s, a, s V n s t − t dt .
erwise, with probability P (s |s, a), the agent obtains reward         0 s,a( )( (    )+    ( )(    ))  )
                                 
R(s, a, s ) ≥ 0 and transitions to state s ∈ S with time-to-                     n           ∗
                                                      It then holds that limn→∞ V (s)(t)=V  (s)(t) for all
deadline t − t and repeats the process. The time-to-deadline
                                                      states s ∈ S and times-to-deadline 0 ≤ t ≤ Δ. Unfortu-
at the beginning of execution is > . The objective of the
                           Δ   0                      nately, value iteration cannot be implemented as stated since
agent is to maximize its expected total reward until execution             n
                                                      the number of values V (s)(t) is inﬁnite for each iteration
stops.
                                                      n since the time-to-deadline t is a real-valued variable. We
                                                                                        n
We use a Mars rover domain (Figure 1) that is similar to one remedy this situation by viewing the V (s) as value func-
      [                 ]                             tions that map times-to-deadline t to the corresponding values
used in Bresina et al., 2002 as an example of our planning n
problems. A rover has to maximize its expected total reward V (s)(t), similar to [Liu and Koenig, 2005]. In this context,
with time-to-deadline Δ=4. The states of the MDP cor- we make the following contributions: First, we show after
respond to the locations of the rover: its start location, site how many iterations n value iteration can stop so that the ap-
                                                                                  |V ∗ s t −V n s t |
1, site 2, site 3 and its base station. The rover can perform proximation error maxs∈S,0≤t≤Δ ( )( ) ( )( ) is no
                                                      larger than a given constant >0. Second, we show that each
two actions with deterministic action outcomes outside of its       n
base station: (1) It can move to the next site and collect a value function V (s) can be represented exactly with a vec-
rock probe from there. It receives rewards 4, 2 and 1 for col- tor of a small number of real values each. Finally, we show
lecting rock probes from sites 1, 2 and 3, respectively. (2) how the Bellman updates can efﬁciently transform the vec-
                                                      tors of the value functions V n(s) to the vectors of the value
It can also move back to its base station to perform a com-     n
munication task, which drains its energy completely and thus functions V +1(s). We present these three contributions in
makes it impossible for the rover to perform additional ac- the following as three steps of CPH.
tions. It receives reward 6 for performing the communication
task. All action durations are characterized by the exponen-
                       −t                           4   CPH: Step 1
tial distribution ps,a(t )=e = E(1). We also consider
the case where all action durations are characterized by ei-
                                                      CPH ﬁrst approximates the probability distributions of ac-
ther Weibull or Normal distributions. The Mars rover domain
                                                      tion durations that are not exponential with phase-type dis-
might appear small with only 5 discrete states. However, if
                                                      tributions, resulting in chains of exponential distributions
one discretizes the time-to-deadline into 1/100 time units and λe−λt E λ                          λ>
encodes it in the states, then there are already 5 ∗ 400 = 2000 = ( ) with potentially different constants 0
states.                                               [Neuts, 1981]. CPH then uses uniformization (i.e., creates
                                                      self-transitions) to make all constants λ identical without
                                                      changing the underlying stochastic process. We do not give
3  General Approach                                   details on how to implement these two well-known steps (see
                                                      [Neuts, 1981; Younes and Simmons, 2004] for details) but
We can solve our planning problems by encoding the time- rather provide an example. Figure 2 shows how an action with
to-deadline in the states, resulting in a continuous state MDP a deterministic action outcome and an action duration that is
that can be solved with a version of value iteration [Boyan and characterized by a Normal distribution N(μ =2,σ =1)is
Littman, 2000] as follows: Let V ∗(s)(t) denote the largest ﬁrst approximated with a phase-type distribution with three

                                                IJCAI-07
                                                  2537    Initial probability distribution

                     p=N(2,1)
     start                                base
                       P=1

    After transformation to phase-type distribution

          p=E(1.45)    p=E(1.42)    p=E(1.43)
     start                                base
            P=1        P=0.97        P=1

                             p=E(1.42)
                              P=0.03

                 p=E(1.45)   p=E(1.45)
    After uniformization P=0.02 P=0.01

          p=E(1.45)    p=E(1.45)   p=E(1.45)
     start                                base
            P=1         P=0.95      P=0.99

                             p=E(1.45)                                                     ∗
                              P=0.03                  Figure   3:      Value   function   V (start)  con-
                                                                                             ∗
Figure 2: Transformation of a Normal distribution to a chain sists of four gamma functions: V (start)=
                                                        V ∗     ,  . V ∗     , .  V ∗     , . V ∗
of exponential distributions E(1.45). p and P denote the ac- [0: 0 (start) 0 8: 1 (start) 1 8: 2 (start) 3 2: 3 (start)] =
tion duration and action outcome probabilities, respectively. [0:[6, 6], 0.8:[10, 10, 6], 1.8:[12, 8.73, 8, 6],
                                                      3.2:[13, 27.1, −1.92, 7, 6]].
phases and then uniformized to yield the exponential distri-
butions E(1.45). From now on, we can therefore assume 6   CPH: Step 3
without loss of generality that the action durations t of all
                                                      We now explain how CPH uses value iteration. For n =0,
actions are distributed according to exponential distributions           n               n
                                                    the value functions V (s)=0satisfy V (s)=[0:[0]] and
ps,a(t )=p(t )=E(λ) with the same constant λ>0. Value
                                         ∗            thus are (piecewise) gamma. We show by induction that all
iteration cannot determine the value functions V (s) with a           n
                                                      value functions V +1(s) are piecewise gamma if all value
ﬁnite number of iterations since the uniformized MDPs have      n
                                                      functions V s  are piecewise gamma. We also show how
self-transitions. However, Theorem 1 in the appendix shows       ( )
                                                      the Bellman updates can efﬁciently transform the vectors of
that the approximation error is no larger than a given constant         n
                                                      the value functions V (s) to the vectors of the value functions
>0  if value iteration stops after at least            n
                                                      V  +1(s). Value iteration calculates
                                                                                         t
               λΔ                                                           
            log e −1       λΔ                           n+1                                           
                eλΔ Rmax(e    − 1)                    V    (s)(t):=    max      P (s |s, a) p(t )(R(s, a, s )
                                                                      a∈A s
                                                                          ( )            0
                                                                           s ∈S
                R                       R  s, a, s
iterations, where max := maxs∈S,a∈A(s),s ∈S (    ).                       V n s t − t dt.
Running time of Step 1 of CPH is negligible since analytic              +    ( )(    ))
phase-type approximation methods run in time O(1).    We break this calculation down into four stages: First,
                                                        n
                                                      V  (s)(t − t):=R(s, a, s)+V   n(s)(t − t). Sec-
                                                                           t     n
                                                      ond, V n(s)(t):=    p(t)V (s)(t − t) dt. Third,
                                                                          0
5  CPH: Step 2                                         n                             n 
                                                      V  (s, a)(t):=     s∈S P (s |s, a)V (s )(t). Finally,
                                                        n+1                   n       1
It follows directly from the considerations in the next section V (s)(t):=maxa∈A s V (s, a)(t).
                                 t   <t    < ... <                        ( )
that there exist times-to-deadline 0= s,0 s,1                              n
t                    V n s t     V n s t       t ∈                       V   s t −  t      R  s, a, s
s,ms+1 =Δsuch that      ( )( )=   i ( )( ) for all    Stage 1:  Calculate   (  )(     ):=      (     )+
                                                        n       
[ts,i,ts,i ), where                                   V  (s )(t − t ). Our induction assumption is that all value
       +1                                                        n                              n  
                    „                        «        functions V (s ) are piecewise gamma, i.e., V (s )=
                                          n−1                       m                            n
 n               −λt                  (λt)                      n+1   s                            
V  s t   c    − e    c     ...  c                     [ts,i:[cs,i,j]j ]i . In Stage 1, CPH calculates V (s )(t −
 i ( )( )= s,i,1      s,i,2 + +  s,i,n+1 n −    (1)              =1  =0
                                      (    1)!        t):=R(s, a, s)+V n(s)(t−t), which is the same as calcu-
                                                             n                    n                 
 for the parameters cs,i,j for all s ∈ S, 0 ≤ i ≤ ms and lating V (s )(t):=R(s, a, s )+V (s )(t) since R(s, a, s )
1 ≤ j ≤  n +1. We refer to the times-to-deadline ts,i as is constant. Then,
                                                             n                   n  
breakpoints, the [ts,i,ts,i+1) as intervals, the expressions for V s   R s, a, s  V  s
                 V n s                                        ( )=      (     )+     ( )
the value functions i ( ) as gamma functions (which is a                                    n   ms
                                                                       R s, a, s  t    c    +1
simpliﬁcation since they are actually linear combinations of       =    (     )+[  s ,i :[ s ,i,j]j=1 ]i=0
                                                                                 n+1 ms
Euler’s incomplete gamma functions), and the expressions           =[ts,i :[c  ]   ]   .
                     V n s                                                    s ,i,j j=1 i=0
for the value functions ( ) as piecewise gamma func-                                  
                                     n                      c      R  s, a, s c       c       c 
tions. We represent each gamma function Vi (s) as a vector where s,i,1 = ( )+ s ,i,1 and s,i,j = s ,i,j for all
c    ,...,c         c   n+1                           0 ≤ i ≤ ms and 2 ≤ j ≤ n +1.
[ s,i,1   s,i,n+1]=[ s,i,j]j=1 and each piecewise gamma
         n                              n   ms                              n
        V  s                       t   V  s              1                                  e n    
function  ( ) as a vector of vectors [ s,i: i ( )]i=0 =                   V   s, a, s t − t  V   s, a, s t
         n   m                                           We should really use (    )(    ) and  (     )( )
t   c     +1  s                                                n          e n 
[ s,i:[ s,i,j]j=1 ]i=0. Figure 3, for example, shows the value instead of V (s )(t−t ) and V (s )(t), respectively, but this would
         ∗
function V (start) for our Mars rover domain.         make our notation rather cumbersome.

                                                IJCAI-07
                                                  2538                            
                V n s t     t p t V n s t − t dt
Stage 2: Calculate ( )( ):=  0  ( )   ( )(    )   ,   Then,
                           n                 n                      
which is a convolution of p and V (s ) denoted as p∗V (s ). V n s, a     P s|s, a V n s
                          n                                (  )=           (    )   ( )
Consider the value functions V (s ) deﬁned in Stage 1. We             s∈S
                                                                      
now show by induction that                                                                        m
                                                                          P s|s, a t    c  n+2  s,a
                                                                  =         (    )[ s,a,i :[ s,i,j]j=1 ]i=0
     n                                                              s∈S
    Vi (s )(t)                                                               
                                                                                           n+2 ms,a
                            i                                          ts,a,i    P s |s, a c 
            n                           n                        =[       :[      (    ) s ,i,j]j=1 ]i=0
                      −λt     λts,i                                       
    =(p ∗ V i (s ))(t) − e (  e     (p ∗ V i (s ))(ts,i )                 s ∈S
                                                                              n   ms,a
                          i =1                                         t     c      +2     .
                                                                  =[s,a,i  :[ s,a,i,j]j=1 ]i=0
          i−1
                         n
              λt           
       −     e  s ,i +1 p ∗ V s t  
                     (    i ( ))( s ,i +1))     (2)                   V n+1 s t              V n s, a t
                                                      Stage 4: Calculate    ( )( ):=maxa∈A(s)    (   )( ).
          i=0
                                                      Consider the value functions V n(s, a) deﬁned in Stage 3.
                                                                                                  ms,a
                                                                                          {ts,a,i}
for all t ∈ [ts ,i,ts ,i+1). Equation (2) holds for i =0since Since they might have different breakpoints i=0 for
 n             n                                                  a ∈ A  s
V s   t    p ∗ V s   t       t ∈ t  ,t             different actions  ( ), CPH introduces common break-
   ( )( )=(     0 ( ))( ) for all [ s ,0 s ,1). Assume            {t    }ms,a
now that Equation (2) holds for some i. It then also holds for points a∈A(s) s,a,i i=0 without changing the value func-
                                                            n
i +1as shown in Figure 4. We use the following lemma to tions V (s, a). CPH then introduces additional dominance
         n                                          breakpoints at the intersections of the value functions to en-
show that Vi (s ) is a gamma function and convert it to vector
                                                      sure that, over each interval, one of the value functions dom-
notation.                                                                          m
                                                                               {t } s
Lemma 1.  Let p(t)=E(λ). Then,  p ∗ [k1,k2,...,kn]=   inates the other ones. Let s,i i=0 be the set of break-
k ,k ,k ,...,k                                                                                    n   m
[ 1 1  2      n].                                                            V n+1 s      t c +2 s
                                                      points afterwards. Then,    ( )=[s,i:[  s,i,j]j=1 ]i=0
                                                                
                                                            c       c                 a   ∈ A  s
                                                      where  s,i,j = s,as,i,i ,j for actions s,i ( ) and the
                                                                      
                                                            i      t  ,t      ⊆  t     ,t    
                                               n      unique  with [ s,i s,i )  [ s,as,i,i s,as,i,i ) and, for
Proof. By symbolic integration, p ∗ k n      k  +1                        +1                  +1
                                  [ ]j=1 =[]j=1          t ∈ t ,t     V n s, a t ≤ V n s, a t
                k          p ∗  k ,...,k       p ∗    all    [ s,i s,i+1), (  )( )     (   s,i)( ). Further-
for any constant .  Then,     [ 1     n]=                      a
   n−1          i         n          n−1              more, action s,i should be executed according to the value
       k −  k          k                 p ∗  k  −              n
(  i=1 [ i   i+1]j=1 +[ n]j=1)=     i=1 (   [ i      function V +1(s) if the current state is s and the time-to-
    i           n       n−1         i+1      n+1                       
ki        p ∗ kn            ki − ki       kn                    t ∈ t ,t
 +1]j=1)+    [  ]j=1 =  i=1 [     +1]j=1 +[ ]j=1 =    deadline is  [ s,i s,i+1).
[k1,k1,...,kn].
                                                      To summarize, the value functions V n+1(s) are piecewise
                                                      gamma, and the vectors of the value functions V n(s) can be
                                                      transformed automatically to the vectors of the value func-
We now transform Equation (2) to vector notation.     tions V n+1(s). The lengths of the vectors increase linearly in
                                                      the number of iterations and, although the number of break-
 V n s     c   n+2
  i ( )=[s,i,j]j=1                                   points (that are placed automatically during the transforma-
                                 
             c      c      c      c      z       tions) can increase exponentially, in practice it stays small
  where       s ,i,1 := s ,i,1 s ,i,2 := s ,i,1 + s ,i
             c       c    ∀                        since one can merge small intervals after each iteration of
              s,i,j+1 := s,i,j j=2,3,...,n+1        value iteration to reduce the number of breakpoints The trans-
                     i
                                 n                   formations require only simple vector manipulations and a
                        λts,i      
   with      zs,i :=  e     (p ∗ V i (s ))(ts,i ) numerical method that determines the dominance breakpoints
                    i=1                              approximately, for which CPH uses a bisection method. We
                   i−1                                now show experimentally that the transformations are both
                                   n
                        λts,i+1      
                 −     e      (p ∗ V i (s ))(ts,i+1). efﬁcient and accurate.
                   i=0

              n             n   ms                7   Experiments
            V  s    t   c     +2
Consequently,  ( )=[s  ,i:[ s,i,j]j=1 ]i=0 .
                                                      We performed an experimental feasibility study in our Mars
                               V n s, a t
Stage   3:        Calculate      (  )( ):=rover domain that compares CPH            and Lazy Approxima-
               n 
  s∈S P (s |s, a)V (s )(t). Consider the value func- tion (LA) [Li and Littman, 2005], currently the leading
tions V n(s) deﬁned in Stage 2. Since they  might   algorithm for solving continuous state MDPs with qual-
                              ms
                        {t  }                        ity guarantees (Figure 5). We always plot the error
have different breakpoints s ,i i=0 for different states        |V ∗     t  − V       t |
s ∈ S with P (s|s, a) > 0, CPH introduces the common max0≤t≤Δ    (start)( )   (start)( ) of the calculated
                                                    value function V (start) on the x-axis and the correspond-
                 ms,a                           ms
           {t   }                         {t  }
breakpoints  s,a,i i=0 =     s∈S:P (s|s,a)>0 s ,i i=0 ing runtime (measured in milliseconds) in log scale on the
without changing the value functions V n(s). Afterwards, y-axis.
V n s    t    c n+2 ms,a       c     c
   ( )=[s,a,i:[  s,i,j]j=1 ]i=0 where s,i,j = s,i,j Experiment 1: We determined how CPH and Lazy Approx-
              
for the unique i with [ts,a,i,ts,a,i+1) ⊆ [ts,i ,ts,i+1). imation trade off between runtime and error for our Mars

                                                IJCAI-07
                                                  2539               Z t                                Z t
    e n              n            e n                 n      
    Vi+1(s )(t)=  p(t )V (s )(t − t ) dt = Vi+1(s )(t)= p(t − t )V (s )(t ) dt
                0                                  0
                                                                      n        n     
       We split the integral into two ranges. For ts,i+1 ≤ t ≤ t<ts,i+2 we know that V (s )(t )=V i+1(s )(t ).
      Z                           Z
        t                           t 
                   n            s ,i+1      n      
    =       p(t − t )V i+1(s )(t ) dt +  p(t − t )V (s )(t ) dt
       ts,i+1                     0
                                                          
                           −λ(t−t )  −λ(t−t  )  −λ(t  −t )   −λ(t−t  )          
       It holds that p(t − t )=λe  = e     s ,i+1 λe s ,i+1  = e     s ,i+1 p(ts,i+1 − t )
      Z t                     Z t                                     Z t
                 n               s,i+1       n                          s,i+1           n
                                                    −λ(t−ts,i+1)                       
    =    p(t − t )V i+1(s )(t ) dt − p(t − t )V i+1(s )(t ) dt + e           p(ts,i+1 − t )V (s )(t ) dt
       0                       0                                       0
      Z t                                 Z t
                 n                          s,i+1           n
                           −λ(t−ts,i+1)                           −λ(t−ts,i+1) e n 
    =    p(t − t )V i+1(s )(t ) dt − e           p(ts,i+1 − t )V i+1(s )(t ) dt + e Vi (s )(ts,i+1)
       0                                   0
           n                          n
                      −λ(t−ts,i+1)                 −λ(t−ts,i+1) e n 
    =(p ∗ V i+1(s ))(t) − e      (p ∗ V i+1(s ))(ts,i+1)+e     Vi (s )(ts,i+1)
       We ﬁnally unroll the recursion by using the induction assumption.
           n                          n                               n
                      −λ(t−ts,i+1)                 −λ(t−ts,i+1)     
    =(p ∗ V i+1(s ))(t) − e      (p ∗ V i+1(s ))(ts,i+1)+e     ((p ∗ V i (s ))(ts,i+1)
                                                                         !
                    i                         i−1
                   X             n           X               n
          −λts,i+1    λts,i                   λts,i+1     
       − e            e     (p ∗ V i (s ))(ts,i ) − e (p ∗ V i (s ))(ts,i+1) )
                                             
                   i =1                      i =0                                !
                            i+1                        i
           n               X             n            X              n
                      −λt      λts,i                  λts,i+1     
    =(p ∗ V i+1(s ))(t) − e    e    (p ∗ V i (s ))(ts,i ) − e (p ∗ V i (s ))(ts,i+1)
                           i=1                       i=0
                                 Figure 4: Proof by induction of Equation (2).


Figure 5: Empirical evaluation of CPH and Lazy Approximation (Experiments 1,2,3) and the tightness of the theoretical
planning horizon calculated from Theorem 1.

rover domain. Since the action durations in the Mars rover by either Weibull distributions W eibull(α =1,β =2)(Ex-
domain are already distributed exponentially and thus phase- periment 2) or Normal distributions N(μ =2,σ =1)(Ex-
type with one phase, there are no errors introduced by approx- periment 3), and both methods thus need to approximate the
imating the probability distributions over the action durations value functions. We ﬁxed for CPH the accuracy of the bisec-
with phase-type distributions. Also, since these distributions tion method but varied the the number of phases used to ap-
are equal, uniformization will not introduce self-transitions proximate the probability distributions. Our results show that
and value iteration can run for a ﬁnite number of iterations. CPH is still faster than Lazy Approximation with the same
We therefore varied for CPH the accuracy of the bisection error, by more than one order of magnitude for small errors.
method that determines the dominance breakpoints and for For example, CPH needed 149ms (with ﬁve phases) and Lazy
Lazy Approximation the accuracy of the piecewise constant Approximation needed 4471ms to compute a policy that is
approximations of the probability distributions over the ac- less than 1% off optimal for the Weibull distributions. In ad-
tion durations and value functions. Our results show that dition, we observed that CPH converged much faster than the
CPH is faster than Lazy Approximation with the same error, theoretical planning horizon calculated from Theorem 1 sug-
by three orders of magnitude for small errors. For example, gests.
CPH needed 2ms and Lazy Approximation needed 1000ms to
compute a policy that is less than 1% off optimal, which cor-
responds to an error of 0.13 in the Mars rover domain. 8  Conclusions
                                                      Planning with real-valued and limited resources can be mod-
Experiments 2 and 3: We then determined how CPH and Lazy eled with continuous state MDPs. Despite recent advances
Approximation trade off between runtime and error when all in solving continuous state MDPs, state-of-the art solution
action durations in our Mars rover domain are characterized methods are still either inefﬁcient [Li and Littman, 2005]

                                                IJCAI-07
                                                  2540