          Natural Language Query Recommendation in Conversation Systems 

                      Shimei Pan                                        James Shaw* 
           IBM T.J. Watson Research Center                                Ask.com 
                   19 Skyline Drive                             343 Thornall Street, 7th floor 
                 Hawthorne, NY 10532                                  Edison, NJ 08837 
                  shimei@us.ibm.com                                James.shaw@ask.com 
                             
                    Abstract                          This simple dialog demonstrates two typical problems in 
                                                      dialog systems: limited input understanding capability and a 
    In this paper, we address a critical problem in con- lack of proper guidance when errors occur. Limited input 
    versation systems: limited input interpretation ca-
                                                      understanding capability is what got the user into trouble 
    pabilities. When an interpretation error occurs, us-
                                                      originally. In U1, the system was not able to understand 
    ers often get stuck and cannot recover due to a lack “mean”. Moreover, from U2-U3, without any guidance from 
    of guidance from the system. To solve this prob-
                                                      the system, the user tried twice to recover from this problem 
    lem, we present a hybrid natural language query 
                                                      without much success. Instead, if the system had responded 
    recommendation framework that combines natural    with S1’, it would be much easier for the user to rephrase 
    language generation with query retrieval. When re-
                                                      successfully.     
    ceiving a problematic user query, our system dy-
                                                        To enhance a user’s experience and improve the robust-
    namically recommends valid queries that are most  ness of human-computer interaction, most existing ap-
    relevant to the current user request so that the user 
                                                      proaches focus on improving a system’s interpretation ca-
    can revise his request accordingly. Compared with 
                                                      pability directly. Nonetheless, such improvements may still 
    existing methods, our approach offers two main    be limited since they would never cover the entire range of 
    contributions: first, improving query recommenda-
                                                      user expressions for any non-trivial application. Alterna-
    tion quality by combining query generation with 
                                                      tively, we propose to improve the robustness of system-user 
    query retrieval; second, adapting generated rec-  interaction by providing proper guidance on what the sys-
    ommendations dynamically so that they are syntac-
                                                      tem can do so that users know how to adapt their queries to 
    tically and lexically consistent with the original 
                                                      those within the system’s capability. Figure 2 summarizes 
    user input. Our evaluation results demonstrate the our approach. Assume U contains all the possible user ex-
    effectiveness of this approach.   
                                                      pressions in an application and S has all the expressions a 
     
                                                      system understands. Since S is usually a subset of U, when a 
1 Introduction                                        user query u1 is outside of S, it will cause interpretation 
Mixed initiative conversation systems allow users to interact errors. When this occurs, our system will provide proper 
with computers using speech or natural language. However, guidance on what the system can do so that users know how 
these systems have not been used widely in practice mainly to revise u1 to s1.  
due to their limited input understanding capabilities. When  
interpretation errors occur, a user frequently gets stuck and  
cannot recover. Figure 1 shows an example in which the  
                                                                    u1
user cannot recover from repeated interpretation errors.                      s1
                                                                               s2

  U1: Show the mean home price and household income of Pleasantville.  u2   Valid System
  S1: Sorry, I don’t understand you. Please rephrase your request.          Expressions (S)

  (S1’: Sorry, I don’t understand. Do you mean “Show the average home price  User Expressions (U)
  and household income of Pleasantville”)              
  U2: Show the median house price and household income of Pleasantville.  Figure 2: Query recommendation for User query adaptation
  S2: Sorry. I don’t understand. Please rephrase your question.   
  U3 :Show the mean home price and the mean household income of Pleas-  To make a system’s interpretation capability apparent to 
  antville                                            a user in context, we propose a hybrid query recommenda-
  S3: Sorry, I still don’t understand ….              tion framework in which we combine a retrieval-based ap-
                    :  An Example                     proach with dynamic query generation. As a result, the hy-
                                                      brid system not only recommends queries based on the ex-
                                                      amples from a query corpus. It also dynamically composes 
                                                      new queries so that it can recommend queries that are close 
   * This work was conducted when the author was at IBM 


                                                IJCAI-07
                                                  1701to the user’s original intent, which might be beyond the In the following, we first explain the hybrid query rec-
scope of pre-stored examples.                         ommendation approach, and then provide evaluation results. 
     Our  approach  is  embodied  in  an  intelligent  multimodal  
conversation system supporting four different applications 3  Hybrid Query Recommendation    
including a real estate application that helps potential buyers To achieve robust, scalable, fine-grained query recommen-
to search for residential properties (Zhou et al., 2006). In dation, we extend our previous work on retrieval-based 
this system, users may enter their queries using multiple query recommendation (Pan et al., 2005) with dynamic 
input modalities such as speech, text, and GUI.  In this query generation. There are two independent recommenders 
work, we focus on queries typed in as text. To respond to a in our system: a retrieval-based recommender and a genera-
user’s request, the system dynamically generates a multime- tion-based recommender. The retrieval-based recommender 
dia presentation that includes automatically generated spo- produces recommendations by selecting from pre-stored 
ken utterances and graphics. When interpretation errors oc- examples while the generation-based recommender dynami-
cur, the system dynamically generates several recommenda- cally constructs recommendations based on the current user 
tions so that the user can select one of them to replace the input. After combining the results from both, the hybrid 
original query or make further revisions if desired.  recommender is able to recommend queries beyond the 
  In the rest of the paper, after a brief discussion of related scope of the pre-stored examples. In the following, to ex-
work, we explain the technical details of our approach. We plain the hybrid approach, first we summarize the retrieval-
then present our evaluation results.                  based approach, and then we focus on the new generation-
                                                      based method. 
2 Related Work 
 Previously, many approaches have been proposed to make a 3.1 Retrieval-based Recommendation 
conversation system’s capability apparent to users, such as The main knowledge in retrieval-based recommendation is a 
tutorials or context-sensitive help. Tutorials (Kamm et al., query corpus containing typical query examples a system 
1998) are often too brief to cover all valid queries. Even if can understand. For each example in the corpus, the system 
they manage to do so, users may not be able to remember stores not only the query itself, but also its semantics pro-
and apply them to the current task. Alternatively, context- duced by a semantic interpreter as well as contextual fea-
sensitive help systems (Hastie et al., 2002, Stein, 1998) pre- tures derived from the conversation history. When a prob-
sent relevant examples in context. However, existing ap- lematic user query is encountered, the system searches for 
proaches for context-sensitive help have limitations. For the most relevant examples from the corpus. The relevance 
example, finite state machine-based approaches (Walker et of an example to a user query is defined as the weighted 
al, 1998) do not scale well. Depending on the granularity of combination of three similarity scores: the string similarity 
the predicted classes, a decision tree-based help (Hastie et score, the semantic similarity score and the context similar-
al., 2002) may be too coarse to provide guidance on the ex- ity score. Based on the total relevance score for each exam-
act wording. Moreover, retrieval-based recommendation has ple in the corpus, the system will recommend the top N que-
been used in applications like spelling recommendations in ries to the user. 
Google spell check (Google, 2006). We have also used re-   This approach works effectively when the system stores 
trieval-based query recommendation in conversation appli- almost all the valid query combinations in the query corpus. 
cations (Pan et al., 2005). However, depending on the cov- However, in any non-trivial conversation system, the num-
erage of the examples in a system, the most relevant pre- ber of queries a system can understand is huge due to com-
stored example may not be close enough to be helpful. In binatorial explosion. For example, in a real estate applica-
contrast, the approach we propose here combines retrieval- tion in which there are 40 attributes associated with a house, 
based approach with dynamic query generation to provide a system would need to store at least 240 examples just to 
scalable, fine-grained context-sensitive help based on the cover queries related to searching for a house using any 
current user query.                                   combination of the attributes as search constraints. Thus, it 
  Our work on dynamic recommendation generation also  is often impossible to collect and pre-store all the queries 
offers several contributions to the area of natural language ahead of time. To solve this problem, we propose to dy-
generation. For example, traditionally, content selection is namically compose new queries based on the current user 
done either based on domain heuristics (McKeown et al., request so that the system can recommend queries beyond 
1997) or content models learned from a corpus (Duboue and the scope of pre-stored examples. 
McKeown. 2003, Barzilay and Lapata, 2005). In contrast, 
our content selection is done based on the analysis of the 3.2 Generation-based Recommendation 
current user query as well as the categorization of an inter- Dynamic query generation offers a significant advantage 
pretation error. For sentence generation, in addition to the over the retrieval-based approach because the system is able 
grammaticality of generated sentences (Shaw. 1998, Lang- to generate a large number of queries based on a small set of 
kilde. 2000, Walker et al., 2002) we also focus on using a query examples.  In general, it is difficult to cover all the 
cascade model to minimize the unnecessary difference be- combinations of concepts and attributes in a corpus. It is not 
tween the original user query and system’s recommenda- difficult however, to cover each concept or attribute in at 
tions.                                                least one of the examples. In the following example, assume 


                                                IJCAI-07
                                                  1702there are seven concepts {A, B, C, D, E, F, G} and four interpretation problems and to derive proper content selec-
query examples {ABE, CD, EFG, DFG}. When receiving a  tion operators. Since most problems that an interpreter can 
new user inquiry “ABDF ” in which “ ” is unknown to   reliably identify involve unknown words, in this study, we 
the system, a retrieval-based recommender may not be able focus on unknown word classification.    
to produce any good recommendations because none of the In the next section, we describe how to categorize each 
existing examples is close enough to the user query. Using unknown word using a set of semantic and syntactic fea-
generation-based recommendation, however, the system can tures.  Later we explain how to build multiple classifiers to 
produce a query more similar to the user’s problematic select appropriate content selection operators.  
query.  For example, based on the partially understood re-  
sults and the system’s guess that the meaning of “ ” is “G”, 
the system may decide to generate a sentence conveying  No Name        Definition                Example 
“ABDFG” which is closer to the user query than any of the 1 EleSynRel The relationship between the problem Modifier-
existing examples.                                                     element and the element anchor Head 
                                                        2    PrElePOS  The Part-of-Speech (POS) of the Adjective 
                                                                       problem element 
                                                        3    PrEleRole  The role played by the problem ele- Modifier 
                               Recommendations                         ment in its relationship with its anchor 
                                                        4 PrEle        Further classification for  the unknown NA 
                                                             Classification word  
                                                        5    AncElePOS  The POS for the anchor element  NP 
                                                        6    AncEleRole  The role played by the anchor element Head 
                                                                       in its relationship with  the problem 
                                                                       element 
                                                        7    AncEleType  The semantic type associated with the Known-
                                                                       anchor element            Concept 
                                                        8 PrSegSyn-    The syntactic relation between the Modifier-
                                                             Rel       problem and the anchor segment Head 
                                                               
                                                        9    PrSegPOS  The POS for the problem  segment  NP 
                                                        10   PrSegRole  The role played by the problem seg- Modifier 
                                                                       ment in its relationship with its anchor 
                                                        11   AncSegPOS  The POS for the Anchor Segment  Noun 
                                                        12   AncSegRole  The role played by the anchor segment Head 
                                                                       in its relationship with the problem 
  To explain the generation-based recommendation method                segment 
in detail, let’s follow the flow in Figure 3. When a problem- 13  AncSegType  The semantic type associated with the Known-
                                                                       anchor segment            Concept 
atic user query, e.g. with unknown words, is received, the 
interpreter produces an error code plus partial interpretation 3.2.1.1 Classification Features 
results in the form of an incomplete semantic graph. From To categorize the problem associated with each unknown 
the user query and the partial interpretation results, the fea- word and to derive proper content selection operators for 
ture extractor automatically derives a set of syntactic and recommendation, we extract a feature vector containing 
semantic features characterizing the unknown word in the thirteen semantic and syntactic features for each unknown 
user query. Based on the extracted features and annotated word. The rationale for selecting these features is to estab-
examples in the problematic query corpus, the classifier lish a connection between the unknown words and their 
selects one or more content selection operators.  Then the context. If the system can understand some words that have 
content selector revises the partial semantic graph using the direct relationships with the unknown word, the system may 
selected operators to formulate the content of a recommen- be able to infer the semantics of the unknown word. Most of 
dation. Finally, the sentence generator takes the revised these features are defined based on the following four con-
semantic graphs and produces grammatical sentences as the cepts: problem element, anchor element, problem segment 
recommendations. The following section explains this com- and anchor segment. A problem element is the basic token 
ponent in detail, starting with classification-based content that contains the unknown word. An anchor element is a 
selection.                                            syntactic constituent that has a direct relationship with the 
                                                      problem element in a syntactic tree. For example, if the 
3.2.1 Classification-based Content Selection 
                                                      problem element is a modifier in a noun phrase, the anchor 
Content selection is critical in recommendation generation.  element will be the head of that noun phrase; or if the prob-
It is an attempt for the system to restore the semantics of lem element is a syntactic object, the anchor element will be 
problematic user inputs based on the system’s knowledge the verb of that object. Similarly, a problem segment is the 
about the application domain, the current user input, and the most specific syntactic constituent that contains the problem 
conversation context. Depending on the type of interpreta- element. The anchor segment is the closest syntactic 
tion errors, the system may add new content or remove ex- constituent of the problem segment. We generally ignore 
isting content to produce a recommendation. Currently, we function words when defining these concepts.  
employ a classification-based approach to categorize the 


                                                IJCAI-07
                                                  1703  In the following example, “show houses in unified school formance of majority-based classifiers in which the classifi-
district”, we assume “unified” is the unknown word. Based ers always predict “no”. The results indicate that content 
on our definition, the problem element is “unified”, element selection can quite reliably help the system in recovering the 
anchor is “school district”; problem segment is “in unified semantics of unknown words. 
school district”; and anchor segment is “houses”.  Table 1 
                                                      3.2.1.4 Applying Content Selection Operators 
summarizes the definitions of the classification features.  
                                                      If a content selection operator is chosen, it is used to revise 
3.2.1.2 Content Selection Operators                   the semantic graph of the original user query. Three knowl-
In the classification training corpus, after a feature vector is edge resources are used in this process: the domain ontol-
extracted for each unknown word in a sentence, we also ogy, the query corpus and the response corpus. For example, 
assign one or more content revision operators for each vec- if the operator OpAttributeOntology is selected to revise the 
tor, indicating proper ways to revise the partial understand- unknown word in “Show the xxx of the houses in Pleasant-
ing results to formulate the semantics of a recommendation. ville”, the system will retrieve all the attributes associated 
To define the set of content selection operators, we analyzed with the anchor concept “House” from the ontology.  For 
a set of problematic user queries. Table 2 listed four com- each retrieved attribute, the system generates one semantic 
monly used content revision operators in structured infor- graph, resulting in many possible recommendations.  In ad-
mation seeking applications like real estate database search. dition to the ontology, both the query corpus and the re-
                                                      sponse corpus are also used when applying the other content 
                                                      selection operators. In the next example, if OpConstraintAt-
 Name Description          Example                    tribute is chosen to revise the user query “show houses with 
 Op-      Replace an unknown Replace “unified” in “unified fair tax” in which “fair” is the unknown word, the system 
 Con-     word with constraints school district” with all the will search both the query corpus and the response corpus to 
 straint- that are  compatible school district constraints, e.g.  find all distinct query constraints that use the house attribute 
 Ontology with the word and the Pleasantville school district. “annual tax”, such as “houses with annual tax below 
          ontology                                    $10000”. Similarly, for each distinct constraint retrieved, 
 Op-      Replace an unknown Replace “fair” in “Houses with the system generates one semantic graph.  
 Con-     word with constraints fair tax” with a constraint on the   After applying content selection operators, the results 
 straint- that are compatible house attribute: AnnualTax, e.g. contain a set of semantic graphs, each representing the con-
 Attribute with the current tax less than $100000     tent of a recommendation. In the following section, we ex-
          attribute                                   plain how to generate a sentence that not only conveys the 
 Op-      Replace an unknown Replace “dimension” in “the semantics in the semantic graph faithfully but also is syntac-
 Attribute- word with attributes  dimension of the house” with an 
                                                      tically and lexically consistent with the original user query.  
 Ontology that are compatible attribute of house  based on the 
          with the ontology ontology, e.g. square footage. 3.2.2 Cascade model for Sentence Generation 
 Op-      Replace unknown  Replace “preserve” in “preserve  Once the content of a recommendation is determined, it is 
 Operator- word with a known the houses” with  a known op- sent to the sentence generator to be realized as grammatical 
 Ontology operator         erator like “save”         sentences. Here we implemented an instance-based sentence 
3.2.1.3 Classification Results                        generator that selects and dynamically composes new sen-
Our training examples for classification are collected from a tences from examples in an instance corpus.  
wizard-of-oz (WOZ) user study. In total, we have collected One critical issue in recommendation generation is to 
36 conversation segments and ~500 user requests. Among adapt a sentence’s surface form to be as similar to the ex-
these requests, our system detected 187 unknown words. pressions in the original user input as possible so that it is 
For each unknown word, a feature vector (described in Ta- easier for a user to identify the changes between them. For 
ble 1) was extracted automatically from the interpretation example, when the original user query “Show xxx houses in 
results. In addition, for each feature vector, we manually Pleasantville with 2000 sq ft.” can not be understood by the 
assigned a yes or no tag for every content selection operator interpreter, it is more desirable if the system recommends 
defined in table 2.  In total, we have trained four content “Show 4 bedroom houses in Pleasantville with 2000 sq ft” 
selection classifiers. Currently, we use JRip (Witten and than “Show 2000 sq ft houses with 4 bedrooms in Pleasant-
Frank, 2005), a Java implementation of the RIPPER classi- ville” even though both convey the same semantics. 
fier (Cohen, 1998) in our experiment.                   In the following, we describe an instance-based sentence 
                                                      generation approach that matches and selects words and 
                                                      phrases from a cascade of instance corpora so that the sys-
  Classifier Accuracy Majority classification tem can reuse as many expressions similar to the user’s as 
  OpConstraintOntology 98.4%   81.7%                  possible. We start with a brief introduction on instance-
  OpConstraintAttribute 93.6% 83.9%                   based sentence generation. 
  OpAttributeOntology 91.4% 81.8%                      3.2.2.1 Instance-based sentence generation 
  OpOperatorOntology 94.6% 78.6%                      In instance-based sentence generation, all the semantic and 
  Table 3 shows the performance of each classifier based syntactic knowledge needed for sentence generation is en-
on ten-fold cross validation. We compare it with the per- coded in an instance corpus.  Each instance in the corpus is 


                                                IJCAI-07
                                                  1704represented as a pair of semantic graph and realization tree. realization order, the realization form and the presence of 
The semantic graph represents the concepts and relations discourse cue phrases. Table 5 shows the effects of adapta-
conveyed in a sentence and the realization tree represents tion using these features. In the first example, based on the 
how the concepts and relations are realized using words and interpretation results of Q1, the system knows that in R1 the 
phrases. During generation, the input semantic graph from house bedroom constraint should come before the bathroom 
the content planner is matched against all the semantic and the city constraints. It is also aware that the bedroom 
graphs in the corpus and the closest matching one is re- and bathroom constraints should be realized as pre-
trieved with its associated realization tree. Moreover, if the modifiers instead of post-modifiers. Without taking input 
matching is not perfect, based on the difference, a set of order and form into consideration, however, the system 
adaptation operators are derived so that new sentences can might generate a recommendation like R2 which is consid-
be generated by deleting unneeded content from, or adding ered worse than R1. In the next example, assume Q2 and Q3 
new content to the associated realization tree. More details are two consecutive user queries. Since the system interprets 
on instance-based sentence generation can be found in (Pan user query in context, the interpretation results of Q3 is 
and Shaw 2004, Pan and Shaw 2005). In the following, we equivalent to Q3’. Without discourse cue phrase adaptation, 
focus on the new cascade model which is designed to maxi- the system will recommend R3 which is less coherent and 
mize the syntactic and lexical consistency between the context appropriate than R4.  
original user query and the generated recommendations.   

3.2.2.2 Cascade model for instance selection             : Show 3 bedroom 2 bathroom houses in xxx cities. 
The essence of instance-based sentence generation is to re- : Show 3 bedroom 2 bathroom houses in cities with less than 1000 
use as many words, phrases or even the entire sentences in people. 
the instance corpus to convey desired semantics. As a result, In cities with less than 1000 people, show 2 bathroom houses with 3 
the size and style of the instance corpus can significantly bedrooms. 
impact the generation quality. Sufficient coverage ensures  Show 3 bedroom houses  
that there always exist proper words or phrases in the in- :Just those in xxx cities 
stance corpus to convey any given semantics. Using in- Q3’ Show 3 bedroom houses in xxx cities 
stances similar in style maximizes the chance for reusing :Show 3 bedroom houses in cities with less than 1000 people  
large chunks of the corpus material in a new sentence. The : Just those in cities with less than 1000 people 
larger the reused chunks are, the fewer adaptations are re-
quired, and the better the generation quality is.     3.3 Merging Recommendations 
  To balance the needs for good coverage and similar style, After the retrieval-based and the generation-based recom-
we use three types of instances available in our system: the mender produce two sets of recommendations independ-
current user input, the examples in a query corpus and the ently, we merge the results. Currently given the maximum 
examples in a response corpus. The current user input only number of recommendations to display on the recommenda-
contains one pair of semantic graph and realization tree tion panel, we take the number of results proportionally 
automatically derived from the partial interpretation results. from each recommender. For example, if the maximum 
Since the system may make mistakes, it can be incorrect. number of recommendations allowed is five, the generation-
However, it is the only source containing the content and based recommender produces eight results, and the re-
form of the current user query. The query corpus contains trieval-based recommender produces two results, the final 
typical queries our system understands. Each request in the five recommendations contain four recommendations from 
query corpus is manually annotated with a semantic graph the generation-based system, one result from the retrieval-
and a realization tree. Style-wise, it is similar to the recom- based system.   
mendations to be generated. However, since the size of the  
query corpus is small†, to improve the coverage, we also use 
the response corpus. The response corpus is designed ini- 4 Evaluations 
tially for response generation. It is clean but with a some- We perform an evaluation to verify the usefulness of the 
what different style. For example, query corpus contains proposed approach. Through this evaluation, we want to 
users’ requests for data, while instances in the response cor- gather two results. First, we want to see whether the hybrid 
pus are descriptions of query results. Nonetheless, they still approach can improve recommendation quality over a base-
share a significant amount of vocabulary.             line system. Second, we want to verify that recommenda-
  In the cascade model, to ensure that the output sentences tions dynamically generated by our approach are valid que-
convey the desired semantics, instances are only selected ries that can be understood by our system. Otherwise, users 
from the two clean sources: the query corpus first, followed may be frustrated due to subsequent rejections of the rec-
by the response corpus.  But, to generate recommendations ommendations by the interpreter.  
as close to the original user query as possible, we adapt the In the first evaluation, we use the retrieval-based recom-
generation results based on the features extracted from the mender as the baseline. Based on our previous study (Pan et 
user query.  Overall, three input features are extracted: the al., 2005), systems aided by retrieval-based recommenda-
                                                      tion were more effective than the same system without any 
                                                      recommendations. Overall, users achieved higher task suc-
   † There are 330 examples in the query corpus.    


                                                IJCAI-07
                                                  1705