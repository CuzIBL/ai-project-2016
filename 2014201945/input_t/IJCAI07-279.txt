                               Word Sense Disambiguation with
                Spreading Activation Networks Generated from Thesauri

            George Tsatsaronis1∗, Michalis Vazirgiannis1,2†     and  Ion Androutsopoulos1
           1Department of Informatics, Athens University of Economics and Business, Greece
                                2GEMO Team, INRIA/FUTURS, France


                    Abstract                          2003] a larger subset of semantic relations compared to pre-
                                                      vious approaches was used, but antonymy, domain/domain
    Most word sense disambiguation (WSD) meth-        terms and all inter-POS relations were not considered.
    ods require large quantities of manually annotated  In this paper we propose a new WSD algorithm, which
    training data and/or do not exploit fully the se- does not require any training, in order to explore the afore-
    mantic relations of thesauri. We propose a new    mentioned potential. A word thesaurus is used to construct
    unsupervised WSD algorithm, which is based on     Spreading Activation Networks (SANs), initially introduced
    generating Spreading Activation Networks (SANs)   in WSD by Quillian [1969]. In our experiments we used
    from the senses of a thesaurus and the relations be- WordNet [Fellbaum, 1998], though other thesauri can also
    tween them. A new method of assigning weights to  be employed. The innovative points of this new WSD algo-
    the networks’ links is also proposed. Experiments rithm are: (a) it explores all types of semantic links, as pro-
    show that the algorithm outperforms previous un-  vided by the thesaurus, even links that cross parts of speech,
    supervised approaches to WSD.                     unlike previous knowledge-based approaches, which made
                                                      use of mainly the “is-a” and “has-part” relations; (b) it in-
                                                      troduces a new method for constructing SANs for the WSD
1  Introduction                                       task; and (c) it introduces an innovative weighting scheme for
                                                      the networks’ edges, taking into account the importance of
Word Sense Disambiguation (WSD) aims to assign to every
                                                      each edge type with respect to the whole network. We show
word of a document the most appropriate meaning (sense)
                                                      experimentally that our method achieves the best reported ac-
among those offered by a lexicon or a thesaurus. WSD is im-
                                                      curacy taking into account all parts of speech on a standard
portant in natural language processing and text mining tasks,
                                                      benchmark WSD data set, Senseval 2 [Palmer et al., 2001].
such as machine translation, speech processing, information
retrieval, and document classiﬁcation. A wide range of WSD The rest of the paper is organized as follows. Section 2 pro-
algorithms and techniques has been developed, utilizing ma- vides preliminary information concerning the use of semantic
chine readable dictionaries, statistical and machine learning relations and SANs in the WSD task. Section 3 discusses our
methods, even parallel corpora. In [Ide and Veronis, 1998] SAN construction method and edge weighting scheme. Sec-
several approaches are surveyed; they address WSD either in tion 4 presents our full WSD method, and Section 5 experi-
a supervised manner, utilizing existing manually-tagged cor- mental results and analysis. Section 6 discusses related WSD
pora, or with unsupervised methods, which sidestep the te- approaches, and Section 7 concludes.
dious stage of constructing manually-tagged corpora.
  The expansion of existing, and the development of new 2 Background
word thesauri has offered powerful knowledge to many text 2.1 Semantic Relations in WordNet
processing tasks. For example, exploiting is-a relations (e.g.
hypernyms/hyponyms) between word senses leads to im-  WordNet is a lexical database containing English nouns,
proved performance both in text classiﬁcation [Mavroeidis et verbs, adjectives and adverbs, organized in synonym sets
al., 2005] and retrieval [Voorhees, 1993]. Semantic links be- (synsets). Hereafter, the terms senses and synsets are
tween senses, as derived from a thesaurus, are thus impor- used interchangeably. Synsets are connected with vari-
tant, and they have been utilized in many previous WSD ap- ous edges, representing semantic relations among them,
proaches, to be discussed below. Previous WSD approaches, and the latest WordNet versions offer a rich set of
however, have not considered the full range of semantic links such links: hypernymy/hyponymy, meronymy/holonymy,
between senses in a thesaurus. In [Banerjee and Pedersen, synonymy/antonymy, entailment/causality, troponymy, do-
                                                      main/domain terms, derivationally related forms, coordinate
  ∗Partially funded by the PENED 2003 Programme of the EU and terms, attributes, and stem adjectives. Several relations cross
the Greek General Secretariat for Research and Technology. parts of speech, like the domain terms relation, which con-
  †Supported by the Marie Curie Intra-European Fellowship. nects senses pertaining to the same domain (e.g. light,asa

                                                IJCAI-07
                                                  1725noun meaning electromagnetic radiation producing a visual                             S.1.1  S.2.1
sensation, belongs to the domain of physics), and the at-
tributes relation, which connects a word with its possible val- Index: W1    W2    W1            W2
ues (e.g. light, as a noun, can be bright or dull). Our method = Word Node
                                                                                       S.1.2 S.2.2
                                                              = Sense Node
utilizes all of the aforementioned semantic relations.                 Initial Phase    Phase 1

                                                             = Activatory Link   GW    GW
2.2  WSD Methods that Exploit Semantic Relations                                 1.1.1 2.1.1
                                                                                  ...  ...
Several WSD approaches capitalize on the fact that thesauri   = Inhibitory Link S.1.1        S.2.1
                                                                                 GW    GW
like WordNet offer important vertical (“is-a”, “has-part”)                       1.1.n 2.1.n
and horizontal (synonym, antonym, coordinate terms) se-                W1                        W2
                                                                                 GW    GW
mantic relations. Some examples of these approaches are                          1.2.1 2.2.1
                                                                           S.1.2             S.2.2
[Sussna, 1993; Rigau et al., 1997; Leacock et al., 1998;                          ... ... 
Mihalcea et al., 2004; Mavroeidis et al., 2005; Montoyo et                       GW    GW
al., 2005]. Most of the existing WSD approaches, however,                        1.2.n 2.2.n
exploit only a few of the semantic relations that thesauri pro-                    Phase 2
vide, mostly synonyms and/or hypernyms/hyponyms. For ex-
ample Patwardhan et al. [2003] and Banerjee and Pedersen Figure 1: A previous method to generate SANs for WSD.
[2003] focus mostly on the “is-a” hierarchy of nouns, thus ig-
noring other intra- and all inter-POS relations. In contrast, our work according to a spreading strategy, ensuring that even-
method takes into account all semantic relations and requires tually only one sense node per initial word node will have a
no training data other than the thesaurus.            positive activation value, which is taken to be the sense the
                                                      algorithm assigns to the corresponding word. Note that this
2.3  Previous Use of SANs in WSD                      approach assumes that all occurrences of the same word in the
Spreading Activation Networks (SANs) have already been text fragment we apply the algorithm to have the same sense,
used in information retrieval [Crestani, 1997] and in text which is usually reasonable, at least for short fragments like
structure analysis [Kozima and Furugori, 1993]. Since their sentences or paragraphs.
ﬁrst use in WSD by Quillian [1969], several others [Cotrell We use a different method to construct the network, ex-
and Small, 1983; Bookman, 1987] have also used them in plained below. Note also that Veronis and Ide do not use
WSD, but those approaches required rather ad hoc hand- direct sense-to-sense relations; in contrast, we use all such
encoded sets of semantic features to compute semantic sim- relations, as in section 2.1, that exist in the thesaurus. More-
ilarities. The most recent attempt to use SANs in WSD that over they weigh all the activatory and inhibitory edges with 1
we are aware of, overcoming the aforementioned drawback, and −1, whereas we propose a new weighting scheme, taking
is [Veronis and Ide, 1990].                           into account the importance of each edge type in the network.
  Figure 1 illustrates how SANs were applied to WSD by
Veronis and Ide. Let W1 and W2 be two words that co-occur 3 SAN Creation
(e.g. in a sentence or text) and that we want to disambiguate.
                                                      In this section we propose a new method to construct SANs
They constitute the network nodes (word nodes) depicted in
                                                      for the WSD task, along with a new weighting scheme for
the initial phase of Figure 1; more generally, there would be
                                                      the edges. Our algorithm disambiguates a given text sentence
n word nodes, corresponding to the n words of the text frag-
                                                      by sentence. We only consider the words of each sentence
ment. Next, all relevant senses of W1 and W2 are retrieved
                                                      that are present in the thesaurus, in our case WordNet. We
from a machine readable dictionary (MRD), and are added
                                                      also assume that the words of the text have been tagged with
as nodes (sense nodes) to the network. Each word is con-
                                                      their parts of speech (POS). For each sentence, a SAN is con-
nected to all of its senses via edges with positive weights
                                                      structed as shown in Figure 2. For simplicity, in this exam-
(activatory edges). The senses of the same word (e.g. S11
                                                      ple we kept only the nouns of the input sentence, though the
and S12) are connected to each other with edges of negative
                                                      method disambiguates all parts of speech. The sentence is
weight (inhibitory edges). This is depicted as phase 1 in Fig-
                                                      from the d00 ﬁle in Senseval 2 data set:
ure 1. Next, the senses’ glosses are retrieved, tokenized, and
reduced to their lemmas (base forms of words). Stop-words “If both copies of a certain gene were knocked out, be-
are removed. Each gloss word (GW) is added as a node to   nign polyps would develop.”
the network, and is connected via activatory links to all sense To construct the SAN, initially the word nodes, in our case
nodes that contain it in their glosses (phase 2). The possible the nouns copies, gene and polyps, along with their senses are
senses of the gloss words are retrieved from the MRD and added to the network, as shown in the initial phase of Figure
added to the network. The network continues to grow in the 2. The activatory and inhibitory links are then added, as in the
same manner, until nodes that correspond to a large part or previous section, but after this point the SAN grows in a very
the whole of the thesaurus have been added. Note that each different manner compared to Veronis and Ide. First, all the
edge is bi-directional, and each direction can have a different senses of the thesaurus that are directly linked to the existing
weight.                                               senses of the SAN via any semantic relation are added to the
  Once the network is constructed, the initial word nodes SAN, along with the corresponding links, as shown in expan-
are activated, and their activation is spread through the net- sion round 1 of Figure 2. Every edge is bi-directional, since

                                                IJCAI-07
                                                  1726                                     ...              We use the function of equation 3, which incorporates fan-out
                          Synonym
          gene                       S.2.1                                                            τ
                                 ...                  and distance factors to constrain the activation spreading; is
                            S.1.1  Holonym   S.3.1
     S.1.1                              Attribute     a threshold value.
                S.3.1                                                 
          S.2.1             S.1.2 Hypernym        ...
     S.1.2               ...         ...    ...                         0           , if Aj(p) <τ
                            ...          ...                  O   p
copies ...           polyps    Antonym                          j ( )=   Fj · A p   ,                 (3)
                            S.1.4      ...   S.3.2                      p+1   j( )   otherwise
     S.1.4
                S.3.2
        Initial Phase            Expansion Round 1    Equation 3 prohibits the nodes with low activation levels from
    Index: = Word Node = Sense Node = Activatory Link = Inhibitory Link                         1
                                                      inﬂuencing their neighboring nodes. The factor p+1 dimin-
                                                      ishes the inﬂuence of a node to its neighbors as the iterations
        Figure 2: Our method to construct SANs.
                                                      progress (intuitively, as “pulses” travel further). Function Fj
                                                      is a fan-out factor, deﬁned in equation 4. It reduces the inﬂu-
the semantic relations, at least in WordNet, are bi-directional ence of nodes that connect to many neighbors.
(e.g. if S1 is a hypernym of S2, S2 is a hyponym of S1). In
                                                                                   Cj
the next expansion round, the same process continues for the             Fj =(1−     )                (4)
newly added sense nodes of the previous round. The net-                           CT
work ceases growing when there is a path between every pair CT is the total number of nodes, and Cj is the number of
of the initial word nodes. Then the network is considered as nodes directly connected to j via directed edges from j.
connected. If there are no more senses to be expanded and the
respective SAN is not connected, we cannot disambiguate the 3.2 Assigning Weights to Edges
words of that sentence, losing in coverage. Note that when In information retrieval, a common way to measure a token’s
adding synsets, we use breadth-ﬁrst search with a closed set, importance in a document is to multiply its term frequency
which guarantees we do not get trapped into cycles.   in the document (TF) with the inverse (or log-inverse) of its
3.1  The Spreading Activation Strategy                document frequency (IDF), i.e. with the number of docu-
                                                      ments the token occurs in. To apply the same principle to the
The procedure above leads to networks with tens of thousands weighting of SAN edges, we consider each node of a SAN
of nodes, and almost twice as many edges. Since each word as corresponding to a document, and each type of edge (each
is eventually assigned its most active sense, great care must kind of semantic relation) as corresponding to a token.
be taken in such large networks, so that the activation is efﬁ- Initially each edge of the SAN is assigned a weight of −1
ciently constrained, instead of spreading all over the network. if it is inhibitory (edges representing antonymy and compet-
  Our spreading activation strategy consists of iterations. ing senses of the same word), or 1 if it is activatory (all other
The nodes initially have an activation level 0, except for the edges). Once the network is constructed, we multiply the ini-
input word nodes, whose activation is 1. In each iteration,
                                                      tial weight wkj of every edge ekj with the following quantity:
every node propagates its activation to its neighbors, as a
function of its current activation value and the weights of the       ETF  (ekj) · INF(ekj )          (5)
edges that connect it with its neighbors. We adopt the acti-
vation strategy introduced by Berger et al. [2004], modifying ETF, deﬁned in equation 6, is the edge type frequency, the
                                                      equivalent of TF. It represents the percentage of the outgoing
it by inserting a new scheme to weigh the edges, which is                               e
discussed in section 3.2. More speciﬁcally, at each iteration edges of k that are of the same type as kj. When comput-
                                                      ing the edge weights, edges corresponding to hypernym and
p every network node j has an activation level Aj(p) and an
output Oj (p), which is a function of its activation level, as hyponym links are considered of the same type, since they
shown in equation 1.                                  are symmetric. The intuition behind ETF is to promote edges
                                                      whose type is frequent among the outgoing edges of node k,
                 O  p    f A  p
                   j( )=  (  j( ))              (1)   because nodes with many edges of the same type are more
The output of each node affects the next-iteration activation likely to be hubs for the semantic relation that corresponds to
level of any node k towards which node j has a directed edge. that type.
Thus, the activation level of each network node k at itera-              |{e |type e    type e  }|
tion p is a function of the output, at iteration p − 1,ofevery ETF e       ki    ( ki)=     ( kj)
                                                                 ( kj )=          |{e  }|             (6)
neighboring node j having a directed edge ejk,aswellasa                              ki
                        W
function of the edge weight jk, as shown in equation 2. The second factor in equation 5, deﬁned in equation 7, is the
Although this process is similar to the activation spreading of inverse node frequency (INF), inspired by IDF. It is the fre-
feed-forward neural networks, the reader should keep in mind
                                                      quency of ekj ’s type in the entire SAN.
that the edges of SANs are bi-directional (for each edge, there
                                                                                    N
exists a reciprocal edge). A further difference is that no train-   INF  e            +1
ingisinvolvedinthecaseofSANs.                                           ( kj)=logN                    (7)
                                                                                   type(ekj )
            Ak(p)=      Oj(p − 1) · Wjk         (2)                                             N
                                                      N is the total number of nodes in the SAN, and type(ekj )
                      j                               is the number of nodes that have outgoing edges of the same
Unless a function for the output O is chosen carefully, after a type as ekj .AsinIDF, the intuition behind INF is that we
number of iterations the activation ﬂoods the network nodes. want to promote edges of types that are rare in the SAN.

                                                IJCAI-07
                                                  17274  The WSD Algorithm                                  method is proposed and is evaluated on Senseval 2; it uses
Our WSD algorithm consists of four steps. Given a POS- thesauri-generated semantic networks, along with Pagerank
tagged text, a designated set of parts of speech to be disam- for their processing. We also report the accuracy of the best
biguated, and a word thesaurus:                       reported unsupervised method that participated in the Sen-
                                                                                               [
Step 1: Fragment the text into sentences, and select the words seval 2 “English all words” task, presented in Litkowski,
                                                          ]
having a part of speech from the designated set. For each sen- 2001 .
tence repeat steps 2 to 4.                            5.2  Performance of the Methods
Step 2: Build a SAN, according to section 3. If the SAN is not
connected, even after expanding all available synsets, abort Table 2 presents the accuracy of the six WSD methods, on the
the disambiguation of the sentence.                   three ﬁles of Senseval 2. The presented accuracy corresponds
Step 3: Spread the activation iteratively until all nodes are in- to full coverage, and hence recall and precision are both equal
active.1 For every word node, store the last active sense node to accuracy. The results in Table 2 suggest that our method
with the highest activation.2                         outperforms that of Veronis and Ide, the hybrid method, and
Step 4: Assign to each word the sense corresponding to the the random baseline. Moreover, our method achieved higher
sense node stored in the previous step.               accuracy than the best unsupervised method that participated
                                                      in Senseval 2, and overall slightly lower accuracy than the
                                                      reported results of [Mihalcea et al., 2004].
5  Experimental Evaluation

We evaluated our algorithm on a major benchmark WSD data      0.400
                                                                                                   0.387
set, namely Senseval 2 in the “English all words”task. The    0.380
                                                                                 0.368
data set is annotated with senses of WordNet 2. We exper-     0.360                                0.365
                                                                                0.346
imented with all parts of speech, to be compatible with all   0.340                                0.343
                          [               ]                                               0.329
published results of Senseval 2 Palmer et al., 2001 .Table1   0.320              0.325
shows the number of occurrences of polysemous and monose-                                 0.308
                                                              0.300
                                                           Accuracy
mous words of WordNet 2 in the data set we used, per POS,                                 0.287
                                                              0.280
as well as the average polysemy.                                       0.279
                                                              0.260    0.259
               Nouns   Verbs   Adj.  Adv.   Total             0.240    0.239
                                                              0.220
 Monosemous     260      33     80    91     464
                                                           Baseline   Synsets+Glosses Veronis & Ide Synsets
  Polysemous    813     502    352    172   1839
 Av. Polysemy     4      9      3      3      5

Table 1: Occurrences of polysemous and monosemous words Figure 3: Accuracy on polysemous words and the respective
of WordNet 2 in Senseval 2.                           0.95 conﬁdence intervals.
                                                        Figure 3 shows the corresponding overall results for the
5.1  Methods Compared                                 four methods we implemented, when accuracy is computed
In order to compare our WSD method to that of [Veronis and only on polysemous words, i.e. excluding trivial cases, along
Ide, 1990], we implemented the latter and evaluated it on Sen- with the corresponding 0.95 conﬁdence intervals. There is
seval 2, again using WordNet 2. We also included in the com- clearly a statistically signiﬁcant advantage of our method
parison the baseline for unsupervised WSD methods, i.e. the (Synsets) over both the baseline and the method of Vero-
assignment of a random sense to each word. For the baseline, nis and Ide. Adding WordNet’s glosses to our method
the mean average of 10 executions is reported. Moreover, in (Synsets+Glosses) does not lead to statistically signiﬁcant
order to evaluate the possibility of including glosses in our difference (overlapping conﬁdence intervals), and hence our
method, instead of only synset-to-synset relations, we imple- method without glosses is better, since it is simpler and re-
mented a hybrid method which utilizes both, by adding to our quires lower computational cost, as shown in section 5.3. The
SANs the gloss words of the synsets along with their senses, decrease in performance when adding glosses is justiﬁed by
similarly to the method of Veronis and Ide. For the purposes the fact that many of the glosses’ words are not relevant to
of this implementation, as well as for the implementation of the senses the glosses express, and thus the use of glosses
the original method of Veronis and Ide, we used the Extended introduces irrelevant links to the SANs.
WordNet [Moldovan and Rus, 2001], which provides the POS Figure 3 does not show the corresponding results of Mihal-
tags and lemmas of all WordNet 2 synset glosses. In the com- cea et al.’s method, due to the lack of corresponding published
parison, we also include the results presented in [Mihalcea results; the same applies to the best unsupervised method of
et al., 2004]. There, another unsupervised knowledge-based Senseval 2. We note that in the results presented by Mihalcea
                                                      et al., there is no allusion to the variance in the accuracy of
  1
   In equation 3, Fj · Aj (p) is bounded and, hence, as p increases, their method, which occurs by random assignment of senses
eventually all nodes become inactive.                 to words that could not be disambiguated, nor to the num-
  2If there is more than one sense node with this property per word, ber of these words. Thus no direct and clear statement can
we select randomly. This never occurred in our experiments. be made regarding their reported accuracy. In Figure 4 we

                                                IJCAI-07
                                                  1728                  Words       SAN      SAN Glosses        SAN         Baseline  Best Unsup.  Pagerank
                Mono   Poly  Synsets  Veronis and Ide Synsets+Glosses            Senseval 2  Mihalcea
    File 1 (d00) 103   552    0.4595      0.4076          0.4396       0.3651    unavailable  0.4394
    File 2 (d01) 232   724    0.4686      0.4592          0.4801       0.4211    unavailable  0.5446
    File 3 (d02) 129   563    0.5578      0.4682          0.5115       0.4303    unavailable  0.5428
     Overall    464   1839    0.4928      0.4472          0.4780       0.4079      0.4510     0.5089

                         Table 2: Overall and per ﬁle accuracy on the Senseval 2 data set.

compare the accuracy of our method against Mihalcea et al.’s nodes, and n the number of words to be disambiguated. Since
on each Senseval 2 ﬁle. In this case we included all words, we use breadth-ﬁrst search, the computational complexity of
monosemous and polysemous, because we do not have results constructing each SAN (network) is O(n · kl+1).Further-
for Mihalcea et al.’s method on polysemous words only; the more, considering the analysis of constrained spreading acti-
reader should keep in mind that these results are less infor- vation in [Rocha et al., 2004], the computational complexity
mative than the ones of Figure 3, because they do not exclude of spreading the activation is O(n2 · k2l+3). The same com-
monosemous words. There is an overlap between the two putational complexity ﬁgures apply to the method of Veronis
                                                      and Ide, as well as to the hybrid one, although k and l differ
                                                      across the three methods. These ﬁgures, however, are worst
                                           0.595
                                             0.586    case estimates, and in practice we measured much lower com-
    0.580                       0.579
                                          0.558       putational cost. In order to make the comparison of these
                                0.545        0.545
    0.530                                             three methods more concrete with respect to their actual com-
                                           0.521
                                0.510                 putational cost, Table 3 shows the average numbers of nodes,
                0.498        0.500           0.500
    0.480          0.484                              edges, and iterations per network (sentence) for each method.

 Accuracy                    0.469
                0.460                                 Moreover, the average CPU time per network is shown (in
                  0.439      0.437
    0.430                                             seconds), which includes both network construction and ac-
                0.421                                 tivation spreading. The average time for the SAN Synsets
                   0.395
    0.380                                             method to disambiguate a word was 1.37 seconds. Table 3

     Synsets File 1  Synsets File 2   Synsets File 3
                                                                     SAN      SAN Glosses      SAN
     Mihalcea et al. File 1 Mihalcea et al. File 2 Mihalcea et al. File 3
                                                                    Synsets  Veronis and Ide Synsets+Glosses
                                                       Nodes/Net.  10,643.74   6,575.13       9,406.04
Figure 4: Accuracy on all words and the respective 0.95 con- Edges/Net. 13,164.84 34,665.53  37,181.64
ﬁdence intervals.                                      Pulses/Net.  166.93      28.64          119.15
                                                        Sec./Net.   13.21        3.35          19.71
conﬁdence intervals for 2 out of 3 ﬁles, and thus the differ-
ence is not always statistically signiﬁcant.                 Table 3: Average actual computational cost.
  Regarding the best unsupervised method that participated
in Senseval 2, we do not have any further information apart shows that our method requires less CPU time than the hy-
from its overall accuracy, and therefore we rest on our advan- brid method, with which there is no statistically signiﬁcant
tage in accuracy reported in Table 2. Finally, we note that to difference in accuracy; hence, adding glosses to our method
evaluate the signiﬁcance of our weighting, we also executed clearly has no advantage. The method of Veronis and Ide has
experiments without taking it into account in the WSD pro- lower computational cost, but this comes at the expense of a
cess. The accuracy in this case drops by almost 1%, and the statistically signiﬁcant deterioration in performance, as dis-
difference in accuracy between the resulting version of our cussed in Section 5.2. Mihalcea et al. provide no comparable
method and the method of Veronis and Ide is no longer sta- measurements, and thus we cannot compare against them; the
tistically signiﬁcant, which illustrates the importance of our same applies to the best unsupervised method of Senseval 2.
weighting. We have also conducted experiments in Senseval
3, where similar results with statistically signiﬁcant differ- 6 Related Work
ences were obtained: our method achieved an overall accu- The majority of the WSD approaches proposed in the past
racy of 46% while Ide and Veronis achieved 39,7%. Space deal only with nouns, ignoring other parts of speech. Some
does not allow further discussion of our Senseval 3 experi- of those approaches [Yarowsky, 1995; Leacock et al., 1998;
ments.                                                Rigau et al., 1997] concentrate on a set of a few pre-selected
                                                      words, and in many cases perform supervised learning. In
5.3  Complexity and Actual Computational Cost         contrast, our algorithm requires no training, nor hand-tagged
Let k be the maximum branching factor (maximum number or parallel corpora, and it can disambiguate all the words in a
of edges per node) in a word thesaurus, l the maximum path text, sentence by sentence. Though WordNet was used in our
length, following any type of semantic link, between any two experiments, the method can also be applied using LDOCE

                                                IJCAI-07
                                                  1729