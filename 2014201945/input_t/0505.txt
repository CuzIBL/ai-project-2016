       Automatic     Evaluation     of Text   Coherence:     Models    and  Representations
                      Mirella Lapata                              Regina  Barzilay
                   School of Informatics       Computer  Science  and Artiﬁcial Intelligence Laboratory
                  University of Edinburgh               Massachusetts  Institute of Technology
                     2 Buccleuch  Place                        G-468,  the Stata Center
                 Edinburgh  EH8  9LW,  UK                      Cambridge,  MA   14853
                   mlap@inf.ed.ac.uk                          regina@csail.mit.edu

                    Abstract                           Newspapers reported Wednesday that three top Libyan of-
                                                       ﬁcials have been tried and jailed in the Lockerbie case.
    This paper investigates the automatic evaluation of Secretary-General Koﬁ Annan said Wednesday that he may
    text coherence for machine-generated texts. We in- travel to Libya next week in hopes of closing a deal. The sanc-
                                                       tions were imposed to force Libyan leader Moammar Gadhaﬁ
    troduce a fully-automatic, linguistically rich model to turn the men over. Louis Farrakhan, the leader of a U.S.
    of local coherence that correlates with human judg- Muslim group, met with Gadhaﬁ and congratulated him on
    ments. Our modeling approach relies on shallow     his recovery from a hip injury.
    text properties and is relatively inexpensive. We
    present experimental results that assess the pre-            Table 1: A low-coherence summary
    dictive power of various discourse representations
    proposed in the linguistic literature. Our results
    demonstrate that certain models capture comple-   quantiﬁed without recourse to elaborate world knowledge and
    mentary aspects of coherence and thus can be com- handcrafted rules.
    bined to improve performance.
                                                        The coherence models described in this paper fall into
                                                      two broad classes that capture orthogonal dimensions of en-
                                                      tity distribution in discourse. The ﬁrst class incorporates syn-
1  Introduction                                       tactic aspects of text coherence and characterizes how men-
The use of automatic methods for evaluating machine-  tions of the same entity in different syntactic positions are
generated text is quickly becoming mainstream in machine spread across adjacent sentences. Inspired by Centering The-
translation, text generation, and summarization. These meth- ory [Grosz et al., 1995], our algorithm abstracts a collection
ods target various dimensions of text quality, ranging from of coherent texts into a set of entity transition sequences, and
sentence grammaticality to content selection [Bangalore et deﬁnes a probabilistic model over their distribution. Given a
al., 2000; Papineni et al., 2002; Hovy and Lin, 2003]. How- new text, the model evaluates its coherence by computing the
ever, a fundamental aspect of text structure — its coherence probability of its entity transitions according to the training
— is still evaluated manually.                        data.
  Coherence is a property of well-written texts that makes The second, semantic, class of models quantiﬁes local co-
them easier to read and understand than a sequence of ran- herence as the degree of connectivity across text sentences.
domly strung sentences. Although the same information can This approach is motivated by the ﬁndings of Halliday and
be organized in multiple ways to create a coherent text, some Hasan [1976] who emphasized the role of lexical cohesion in
forms of text organization will be indisputably judged inco- text coherence. The key parameter of these models is the def-
herent. The automatically-generated summary in Table 1 is inition of semantic relatedness (as a proxy for connectivity)
an example of a badly organized text: the presence of themat- among text entities. We explore a wealth of similarity mea-
ically unrelated sentences and non-canonical time sequences sures, ranging from distributional to taxonomy-based, to ﬁnd
makes it nearly impossible to comprehend. We want to de- an optimal lexico-semantic representation for the coherence
velop an automatic method that can distinguish such a sum- assessment task.
mary from a text with no coherence violations.          We  employ our models to evaluate the coherence of mul-
  This paper focuses on local coherence, which captures text tidocument summaries produced by systems that participated
organization at the level of sentence to sentence transitions, in the Document Understanding Conference. We acquire co-
and is undoubtedly necessary for global coherence. This topic herence ratings for a large collection of machine-generated
has received much attention in the linguistic literature [Grosz summaries by eliciting judgments from human subjects, and
et al., 1995; Morris and Hirst, 1991; Halliday and Hasan, examine the extent to which the predictions of various mod-
1976] and a variety of qualitative models have been proposed. els correlate with human intuitions. Our experiments demon-
The emphasis of our work is on quantitative models of local strate that while several models exhibit a statistically signif-
coherence that can be efﬁciently computed from raw text, and icant agreement with human ratings, a model that fuses the
readily utilized for automatic evaluation. Our approach relies syntactic and the semantic views yields substantial improve-
on shallow text properties that can be easily identiﬁed and ment over any single model. Our contributions are twofold:Modeling    We  present a fully automatic, linguistically 1. [Former Chilean dictator Augusto Pinochet]o, was arrested in
rich model of local coherence that correlates with human  [London]x on [14 October]x 1998.
judgments. This model is the ﬁrst attempt, to our knowledge, 2. [Pinochet]s, 82, was recovering from [surgery]x.
                                                       3. The arrest was in response to an extradition warrant
to automatically evaluate the local coherence of machine- [       ]s     [      ]x   [                ]x
                                                          served by [a Spanish judge]s.
generated texts.                                       4. [Pinochet]o was charged with murdering [thousands]o, in-
Linguistic Analysis  We present experimental results that cluding many [Spaniards]o.
assess the predictive power of various knowledge sources and 5. [Pinochet]s is awaiting [a hearing]o, [his fate]x in [the
discourse representations proposed in the linguistic literature. balance]x.
In particular, we show that certain models capture comple- 6. [American scholars]s applauded the [arrest]o.
mentary aspects of coherence and thus can be combined to
improve performance.                                  Table 2: Summary augmented with syntactic annotations for
                                                      entity grid computation.
2  Related  Work
Most of the work on automatic coherence evaluation has been
done in the context of automatic essay scoring. Higgins et

   [   ]                                                                  gery


al. 2004 develop a system that assesses global aspects of                        arrant
                                                                                              ate

coherence in student essays. They use a manually annotated    Dictator Augusto Pinochet London October Sur Arrest Extradition W Judge Thousands Spaniards Hearing F Balance Scholars
corpus of essays to learn which types of discourse segments 1 O O O X X  – –  – – –  – –  – –  – –  1
can cause breakdowns in coherence. Other approaches focus 2  – –  S – –  X –  – – –  – –  – –  – –  2
on local coherence. Miltsakaki and Kukich [2004] manually 3  – –  – – –  – S X  X S  – –  – –  – –  3
annotate a corpus of student essays with entity transition in- 4 – – O – – – – – – – O O  – –  – –  4
formation, and show that the distribution of transition types 5 – – S – – – – – – –  – –  O X  X –  5
correlates with human grades. Foltz et al. [1998] propose a 6 – – – – –  – O  – – –  – –  – –  – S  6
model of local coherence that presupposes no manual cod-
ing. A text is considered coherent if it exhibits a high degree
of meaning overlap between adjacent sentences. They employ            Table 3: An entity grid
a vector-based representation of lexical meaning and assess
semantic relatedness by measuring the distance between sen- of discourse changes from sentence to sentence. The key as-
tence pairs. Foltz et al. report that the model correlates reli- sumption is that certain types of entity transitions are likely
ably with human judgments and can be used to analyze dis- to appear in locally coherent discourse. Centering also estab-
course structure. The success of this approach motivates our lishes constraints on the linguistic realization of focus, sug-
research on semantic association models of coherence. gesting that focused entities are likely to occupy prominent
  Previous work has primarily focused on human authored syntactic positions such as subject or object.
texts and has typically utilized a single source of informa-
tion for coherence assesment. In contrast, we concentrate on Discourse representation To expose entity transition
machine generated texts and assess which knowledge sources patterns characteristic of coherent texts, we represent a text
are appropriate for measuring local coherence. We introduce by an entity grid. The grid’s columns correspond to discourse
novel models but also assess whether previously proposed entities, while the rows correspond to utterances (see Table 3).
ones (e.g., Foltz et al. [1998]) generalize to automatically We follow Miltsakaki and Kukich [2004] in assuming that an
generated texts. As a byproduct of our main investigation, we utterance is a traditional sentence (i.e., a main clause with ac-
also examine whether humans can reliably rate texts in terms companying subordinate and adjunct clauses).
of coherence, thus undertaking a large-scale judgment elici- Grid columns record an entity’s presence or absence in a
tation study.                                         sequence of sentences (S1,...,Sn). More speciﬁcally, each
                                                      grid cell represents the role ri j of entity e j in a given sen-
3  Models   of Local Coherence                        tence Si. Grammatical roles reﬂect whether an entity is a sub-
                                                      ject (s), object (o), neither (x) or simply absent (–). Table 3
In this section we introduce two classes of coherence models, illustrates an entity grid constructed for the text in Table 2.
and describe how they can be used in automatic evaluation. Since the text contains six sentences, the grid columns are of
We motivate their construction, present a corresponding dis- length six. As an example consider the grid column for the
course representation, and an inference procedure.    entity arrest, [– – s – – o]. It records that arrest is present in
                                                      sentence 3 as a subject and in sentence 6 as an object, but is
3.1 The Syntactic View                                absent from the rest of the sentences.
Linguistic Motivation  Centering theory [Grosz et al.,  Ideally, each entity in the grid should represent an equiv-
1995; Walker et al., 1998] is one of the most inﬂuential frame- alence class of coreferent nouns (e.g., Former Chilean dicta-
works for modeling local coherence. Fundamental in Center- tor Augusto Pinochet and Pinochet refer to the same entity).
ing’s study of discourse is the way entities are introduced However, the automatic construction of such an entity grid
and discussed. The theory asserts that discourse segments requires a robust coreference tool, able to accurately process
in which successive utterances mention the same entities are texts with coherence violations. Since coreference tools are
more coherent than discourse segments in which multiple en- typically trained on coherent texts, this requirement is hard to
tities are discussed. Coherence analysis revolves around pat- satisfy. Instead, we employ a simpliﬁed representation: each
terns of local entity transitions which specify how the focus noun in a text corresponds to a different entity in the grid. Thesimpliﬁcation allows us to capture noun-coreference, albeit To compare texts with variable lengths and entities, the
in a shallow manner — only exact repetitions are considered probabilities for individual columns are normalized by col-
coreferent. In practice, this means that named entities and umn length (n) and the probability of the entire text is nor-
compound nouns will be treated as denoting more than one malized by the number of columns (m):
entity. For instance, the NP Former Chilean dictator Augusto              m  n
Pinochet will be mapped to three entities: dictator, Augusto,         1
                                                       Pcoherence(T) ≈   ∑  ∑ logP(ri, j|r(i−h), j ...r(i−1), j) (4)
and Pinochet. We further assume that each noun within an             m · n j=1 i=1
NP bears the same grammatical role as the NP head. Thus, all
three nouns in the above NP will be labeled as objects. When Once the model is trained on a corpus of coherent texts,
a noun is attested more than once with a different grammati- it can be used to assess the coherence of unseen texts. Texts
cal role in the same sentence, we default to the role with the that are given a high probability Pcoherence(T) will be deemed
highest grammatical ranking (i.e., s > o > x).        more coherent than those given low probability.
  Entity grids can be straightforwardly computed provided Notice that the model is unlexicalized, i.e., the estimation
that an accurate parser is available. In the experiments re- of P(e j;S1 ...Sn) is not entity-speciﬁc. In practice, this means
ported throughout this paper we employed Collins’ [1998] that entities with the same column topology (e.g., London,
state-of-the-art statistical parser to identify discourse entities October in Table 3) will be given the same probability.
and their grammatical roles. Entities involved in passive con-
structions were identiﬁed using a small set of patterns and 3.2 The Semantic View
their corresponding deep grammatical role was entered in the Linguistic motivation An important factor in text com-
grid (see the grid cell o for Pinochet, Sentence 1, Table 3). prehension is the degree to which sentences and phrases are
                                                      linked together. The observation dates back to Halliday and
Inference   A fundamental assumption underlying our in- Hasan [1976] who stressed the role of lexical cohesion in text
ference mechanism is that the distribution of entities in coher- coherence. A number of linguistic devices — entity repeti-
ent texts exhibit certain regularities reﬂected in the topology tion, synonymy, hyponymy, and meronymy — are considered
of grid columns. Grids of coherent texts are likely to have to contribute to the “continuity of lexical meaning” observed
some dense columns (i.e., columns with just a few gaps such in coherent text. Morris and Hirst [1991] represent lexical
as Pinochet in Table 3) and many sparse columns which cohesion via lexical chains, i.e., sequences of related words
will consist mostly of gaps (see London, judge in Table 3). spanning a topical text unit, thus formalizing the intuition that
One would further expect that entities corresponding to dense coherent units will have a high concentration of dense chains.
columns are more often subjects or objects. These character- They argue that the distribution of lexical chains is a surface
istics will be less pronounced in low-coherence texts. indicator of the structure of coherent discourse.
  We deﬁne the coherence of a text T (S1,...,Sn) with en-
                                                      Discourse representation  The key premise behind lex-
tities e1 ...em as a joint probability distribution that governs
how entities are distributed across document sentences: ical chains is that coherent texts will contain a high number
                                                      of semantically related words. This allows for a particularly
          Pcoherence(T) = P(e1 ...em;S1 ...Sn)  (1)   simple representation of discourse that does not take account
We further assume (somewhat simplistically) that an entity is of syntactic structure or even word order within a sentence.
selected into a document independently of other entities: Each sentence is thus represented as a bag of words. These
                         m                            can be all words in the document (excluding function words)
                                                      or selected grammatical categories (e.g., verbs or nouns). For
           Pcoherence(T) ≈ ∏ P(e j;S1 ...Sn)    (2)
                        j=1                           our models, we assume that each sentence is represented by a
                                                      set of nouns.
To give a concrete example, we will rate the coherence of the Central to this representation is the ability to measure se-
text in Table 3 by multiplying together the probabilities of the mantic similarity. Different coherence models can be there-
entities Dictator, Augusto, Pinochet, etc.            fore deﬁned according to the chosen similarity measure.
                 ...
  We deﬁne P(e j;S1 Sn) as a probability distribution over Inference Measuring local coherence amounts to quan-
transition sequences for entity e j across all n sentences of tifying the degree of semantic relatedness between sentences.
text T. P(e j;S1 ...Sn) is estimated from grid columns, as ob- Speciﬁcally, the coherence of text T is measured by taking
served in a corpus of coherent texts:                 the mean of all individual transitions.
      P(e j;S1 ...Sn) = P(r1, j ...rn, j)                                       n−1
                      n
                                                                                ∑  sim(Si,Si+1)
                   =  ∏ P(ri, j|r1, j ...r(i−1), j)                             i=1
                     i=1                        (3)              coherence(T ) =                      (5)
                      n                                                             n − 1
                   ≈  ∏ P(ri, j|r(i−h), j ...r(i−1), j)
                     i=1                              where sim(Si,Si+1) is a measure of similarity between sen-
                                                      tences S and S .
where r represents the grammatical role for entity e in sen- i    i+1
      i, j                                  j           We  have experimented with three broad classes of mod-
tence i (see the grid columns in Table 3). The estimates for
                                                      els which employ word-based, distributional, and taxonomy-
P r  r  ...r     are obtained using the standard Markov
 ( i, j| 1, j (i−1), j)                               based similarity measures.
assumption of independence, where h is the history size. In its simplest form, semantic similarity can be operational-
Assuming a ﬁrst-order Markov model, P(Pinochet;S1 ...S6) ized in terms of word overlap:
will be estimated by multiplying together P(O), P(S|O),
P(–|S), P(O|–), P(S|O), and P(–|S) (see the column for                     2|words(S1) ∩ words(S2)|
                                                              sim(S1,S2) =                            (6)
Pinochet in Table 3).                                                     (|words(S1)| + |words(S2)|)where words(Si) is the set of words in sentence i. The judgments for multi-document summaries produced by sys-
main drawback of this measure is that it will indicate low- tems and human writers for the Document Understanding
coherence for sentence pairs that have no words in common, Conference (DUC, 2003). Automatically generated sum-
even though they may be semantically related.         maries are prone to coherence violations [Mani, 2001] — sen-
  Measures of distributional similarity, however, go beyond tences are often extracted out of context and concatenated to
mere repetition. Words are considered similar if they occur form a text — and are therefore a good starting point for co-
within similar contexts. The semantic properties of words herence analysis.
are captured in a multi-dimensional space by vectors that are We randomly selected 16 input document clusters1 and in-
constructed from large bodies of text by observing the dis- cluded in our materials six summaries corresponding to each
tributional patterns of co-occurrence with their neighboring cluster. One summary was authored by a human, whereas
words. For modeling coherence, we want to be able to com- ﬁve were produced by automatic summarization systems that
pare the similarity of sentences rather than words (see (4)). participated in DUC. Thus the set of materials contained
Our computation of sentence similarity follows the method 6 · 16 = 96 summaries. From these, six summaries were re-
described in Foltz [1998]: each sentence is represented by the served as a development set, whereas the remaining 90 sum-
mean (centroid) of the vectors of its words, and the similarity maries were used for testing (see Section 5).
between two sentences is determined by the cosine of their
means.                                                Procedure and Subjects    Participants ﬁrst saw a set of
                                                      instructions that explained the task, deﬁned local coherence,
     sim(S1,S2) =   cos(µ(S~1),µ(S~2))                and provided several examples. Then the summaries were
                           n                          presented; a new random order of presentation was generated
                               ~     ~
                          ∑  µ j(S1)µ j(S2)           for each participant. Participants were asked to use a seven
                          j=1
                =                               (7)   point scale to rate how coherent the summaries were with-
                       n            n                 out having seen the source texts. The study was conducted
                            ~   2        ~  2
                       ∑ (µ j(S1)) ∑ (µ j(S2))        remotely over the Internet and was completed by 177 unpaid
                    s j=1        s j=1                volunteers (approximately 23 per summary), all native speak-
             1                                        ers of English.
where µ(~Si) = ∑    ~w, and ~w is the vector for word w.
            |Si| ~w∈Si                                  The judgments were averaged to provide a single rating per
  An alternative to inducing word similarity relationships summary and all further analyses were conducted on means.
from co-occurrence statistics in a corpus, is to employ a man- We report results on inter-subject agreement in Section 5.2.
ually crafted resource such as WordNet [Fellbaum, 1998].
WordNet-based similarity measures have been shown to cor- 5 Experiments
relate reliably with human similarity judgments and have
been used in a variety of applications ranging from the de- All our models were evaluated on the DUC summaries for
tection of malapropisms to word sense disambiguation (see which we elicited coherence judgments. We used correlation
Budanitsky and Hirst [2001] for an extensive survey). We em- analysis to assess the degree of linear relationship between
ployed ﬁve measures commonly cited in the literature. Two human ratings and coherence as estimated by our models. In
of these measures [Hirst and St-Onge, 1998; Lesk, 1986] de- this section we provide an overview of the parameters we ex-
ﬁne similarity solely in terms of the taxonomy, whereas the plored, and present and discuss our results.
other three are based on information theory [Resnik, 1995;
Jiang and Conrath, 1997; Lin, 1998] and combine taxonomic 5.1 Parameter Estimation
information with corpus counts. By considering a broad range Since the models we evaluated have different training require-
of these measures, we should be able to distill which ones are ments (e.g., some must be trained on a large corpus and
best suited for coherence assessment.                 others are not corpus speciﬁc), it was not possible to have
  All WordNet-based measures compute a similarity score at a uniform training corpus for all models. Here, we give an
the sense level. We deﬁne the similarity of two sentences S1 overview of the data requirements for our models and explain
and S2 as:                                            how these were met in our experiments.
                     ∑   argmax  sim(c ,c )             For the entity-based models, we need corpus data to se-
                                      1  2            lect an appropriate history size and to estimate the proba-
                   w1∈S1 c1∈senses(w1)
                   w2∈S2 c ∈senses(w )                bility of various entity transitions. History size was adjusted
        sim(S ,S ) =     2      2               (8)
            1  2            |S ||S |                  using the development corpus described above. Entity tran-
                             1   2                    sition probabilities were estimated from a different corpus:
where |Si| is the number of words in sentence i. Since the we used 88 human summaries that were produced by DUC
appropriate senses for words w1 and w2 are not known, our analysts. We assumed that human authored summaries were
measure selects the senses which maximize sim(c1,c2). coherent and therefore appropriate for obtaining reliable esti-
                                                      mates. The grid columns were augmented with the start and
4  Collecting  Coherence   Judgments                  end symbols, increasing the size of the grid cell categories
                                                      to six. We estimated the probabilities of individual columns
To comparatively evaluate the coherence models introduced
                                                      using n-gram models (see (3)) of variable length (i.e., 2–4)
above, we ﬁrst needed to establish an independent measure of
                                                      smoothed with Witten Bell discounting.
coherence by eliciting judgments from human participants.
Materials and Design    For optimizing and testing our   1Summaries with ungrammatical sentences were excluded from
models, we created a corpus representative of coherent and our materials, to avoid eliciting judgments on text properties that
incoherent texts. More speciﬁcally, we elicited coherence were not coherence-related.                      Humans     Egrid    Overlap    LSA      HStO     Lesk    JCon     Lin
            Egrid      .246*
            Overlap    .120     −.341**
            LSA        .230*     .042      .013
            HStO       .322**    .071      .093      .037
            Lesk       .125      .227     −.032      .098     .380**
            JCon      −.290**   −.392**    .485**    .035     .625**  .270*
            Lin        .173      .074     −.107      .053     .776**  .421**  .526**
            Resnik     .207     −.003      .052     −.063     .746**  .410**  .606**   .809**
                                   *p < .05 (2-tailed) **p < .01 (2-tailed)

Table 4: Correlation between human ratings and coherence models, measured by Pearson coefﬁcient. Stars indicate the level of
statistical signiﬁcance.

  Some of the models based on semantic relatedness pro- claims in the literature [Foltz et al., 1998] that distributional
vide a coherence score without presupposing access to cor- similarity is a signiﬁcant predictor of text coherence. The fact
pus data. These models are based on the measures proposed that JCon correlates3 with human judgments is not surprising
by Hirst and St-Onge [1998], Lesk [1986] and on word over- either; its performance has been superior to other WordNet-
lap. Since they require no prior training, they were directly based measures on two tasks that are particularly relevant
computed on a lemmatized version of the test set. Other se- for the problem considered here: modeling human similar-
mantic association models rely on corpus data to either auto- ity judgments and the automatic detection of malapropisms,
matically construct representations of lexical meaning [Foltz a phenomenon that often results in locally incoherent dis-
et al., 1998] or to populate WordNet’s representations with courses (see Budanitsky and Hirst [2001] for details). It is
frequency counts [Resnik, 1995; Jiang and Conrath, 1997; worth considering why HStO performs better than JCon. In
Lin, 1998]. We used a lemmatized version of the North Amer- quantifying semantic similarity, HStO uses all available re-
ican News Text Corpus for this purpose. The corpus contains lations in WordNet (e.g., antonymy, meronymy, hyponymy)
350 million words of text taken from a variety of news sources and is therefore better-suited to capture cohesive ties arising
and is therefore appropriate for modeling the coherence of from semantically related words than JCon which exploits
news summaries. Vector-based representations were created solely hyponymy. These results further suggest that the cho-
using a term-document matrix. We used singular value de- sen similarity measure plays an important role in modeling
composition [Berry et al., 1994] to reduce the vector space coherence.
to 100 dimensions and thus obtained a semantic space similar The correlations obtained by our models are substan-
to Latent Semantic Analysis (LSA, Foltz et al. [1998]). tially lower when compared with the inter-subject agreement
                                                      of .768. Our results indicate that there is no single method
5.2 Results                                           which captures all aspects of local coherence, although HStO
The evaluation of our coherence models was driven by two yields the highest correlation coefﬁcient in absolute terms. It
questions: (a) What is the contribution of various linguistic is worth noting that Egrid is competitive with LSA and the
sources to coherence modeling? and (b) Are the two modeling semantic association models based on WordNet, even though
frameworks complementary or redundant?                it is unlexicalized. We conjecture that Egrid makes up for the
Upper bound     We ﬁrst assessed human agreement on co- lack of lexicalization by taking syntactic information into ac-
herence judgments. Inter-subject agreement gives an upper count and having a more global perspective of discourse. This
bound for the task and allows us to interpret model perfor- is achieved by explicitly modeling entity transitions spanning
mance in relation to human performance. To measure agree- more than two consecutive sentences.
ment, we employed leave-one-out resampling [Weiss and Ku- As far as model intercorrelations are concerned, note that
likowski, 1991]. The technique correlates the ratings of each Egrid is not signiﬁcantly correlated with the LSA model, thus
participant with the mean ratings of all other participants. The indicating that the two models capture complementary co-
average agreement was r = .768.                       herence properties. Interestingly, LSA displays no correla-
                                                      tion with the WordNet-based models. Although both types
Model Performance     Table 4 displays the results of cor-
                             2                        of models rely on semantic relatedness, they employ dis-
relation analysis on 58 summaries from our test set. Sig- tinct representations of lexical meaning (word co-occurrence-
niﬁcant correlations are observed for the entity grid model based vs. taxonomy-based). Expectedly, the WordNet-based
(Egrid), the vector-based model (LSA), and two WordNet- coherence models are all intercorrelated (see Table 4).
based models, employing Hirst and St-Onge’s [1998] (HStO)
and Jiang and Conrath’s [1997] (JCon) similarity measures. Model Combination An obvious question is whether a
  The signiﬁcant correlation of the Egrid model with human better ﬁt with the experimental data can be obtained via model
judgments empirically validates Centering’s claims about the combination. A standard way to integrate different predictors
importance of entity transitions in establishing local coher- (i.e., models) is multiple linear regression. Since several of
ence. The performance of the LSA model conﬁrms previous
                                                         3The correlation in negative, because JCon is a measure of dis-
  2The rest of the test data — 32 summaries — was used for eval- similarity; the coefﬁcient therefore has the opposite sign in compar-
uating our combined model.                            ison with the other WordNet-based measures.