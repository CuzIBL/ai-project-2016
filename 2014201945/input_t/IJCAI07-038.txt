                                   Context-Driven Predictions

                                Marc G. Bellemare    and  Doina Precup
                                           McGill University
                                      School of Computer Science
                                    {marcgb,dprecup}@cs.mcgill.ca


  Keywords:  Prediction learning, associative memories, not observable by the agent for long periods of time. One
context-based model                                   may imagine, for example, an environment divided into re-
                    Abstract                          gions where certain observations only appear in certain re-
                                                      gions. Objects which evolve in the world also involve keep-
    Markov models have been a keystone in Artiﬁcial   ing track of additional state information about them. Recent
    Intelligence for many decades. However, they re-  work on predictive representations [Sutton and Tanner, 2005;
    main unsatisfactory when the environment mod-     Littman et al., 2002] aims to bridge this gap by explicitly con-
    elled is partially observable. There are pathological structing a state, which is a sufﬁcient statistic for predicting
    examples where no history of ﬁxed length is suf-  the future, from the experience of the agent.
    ﬁcient for accurate prediction or decision making.
    On the other hand, working with a hidden state (like The goal of our work is also to build a predictive represen-
    in Hidden Markov Models or Partially Observable   tation which is not based on a ﬁxed length of history and can
    Markov Decision Processes) has a high computa-    be learned incrementally from data. Speciﬁcally, our work is
    tional cost. In order to circumvent this problem, geared towards environments where observations are subjec-
    we suggest the use of a context-based model. Our  tive. In such systems, we can hope that the only state the agent
    approach replaces strict transition probabilities by needs to be concerned about is based on what is directly ac-
    inﬂuences on transitions. The method proposed     cessible to it. More formally, this would be observations that
    provides a trade-off between a fully and partially lead to better predictions of the immediate future. Our work
    observable model. We also discuss the capacity    is based on hetero-associative memories, such as Sparse Dis-
    of our framework to model hierarchical knowledge  tributed Memories [Kanerva, 1993], but puts the emphasis on
    and abstraction. Simple examples are given in or- predicting future events rather than as a way of palliating to
    der to show the advantages of the algorithm.      noisy inputs. Our model can also handle new observations
                                                      and can generalize to a certain extent, as there is no neces-
                                                      sity for model speciﬁcation in advance beyond deﬁning the
1  Introduction                                       observation features. In this paper we do not explicitly dis-
The ability to predict future events is a necessary compo- cuss planning and rewards, but assume that predictions can
nent of intelligent agents, as it facilitates accurate planning. be used to determine actions.
A standard approach is to predict the future solely through Similar problems have been explored in the connectionist
observations, for example using a ﬁxed-order model Markov literature. For example, [Elman, 1990] considers the notion
Chain. Unfortunately, a history of any ﬁxed length might of context in a neural network as being closely related to the
not be sufﬁcient to accurately predict the next observation. previous state of the system. Through simple tasks, he shows
Methods using a variable history window (e.g. McCallum, that a recurrent neural network can learn to base its predic-
1995) work well in practice but are largely heuristic. A tions on past inputs summarized in its hidden units’ activa-
different approach for making predictions through time is tion. Research on a symbol prediction task close to ours has
to introduce a notion of latent or hidden state of the en- been described in [Bose et al., 2005]. However, they take
vironment, as in Hidden Markov Models [Rabiner, 1989] a neural network approach to the problem and focus on a
and Partially Observable Markov Decision Processes [Kael- particular kind of network architecture and implementation.
bling et al., 1998]. Such models clearly separate observa- Recently, Boltzmann machines have been used for prediction
tions from hidden states, and keep track of the current state through time as well [Taylor et al., 2007]. Our proposed algo-
through a belief vector. However, assuming a hidden state rithm is signiﬁcantly different from those mentioned above:
requires knowledge about it, and often the state transition we focus on weighting past observations in a direct fashion
probabilities are assumed to be known. For learning agents, rather than through an internal state of the agent. A simi-
this approach appears restrictive: knowledge is necessarily lar prediction task is addressed in [Gopalratnam and Cook,
bounded by the state representation. Furthermore, the true 2007], in the context of text parsing. However, in their ap-
state of the system may involve many elements which are proach all data structures are geared towards dealing with

                                                IJCAI-07
                                                   250symbols. Our approach works both for discrete and contin- for prediction purposes.
uous observations (although our experiments contain discrete Other associative memory frameworks have been proposed
observations only).                                   before SDMs. Among those still used today, we ﬁnd Hop-
  The paper is organized as follows. In Section 2 we brieﬂy ﬁeld networks [Hopﬁeld, 1982] and Boltzmann Machines
review Sparse Distributed Memories, on which we based our [Fahlman et al., 1983]. Both rely on the idea of neuron-
ideas. We also discuss Hopﬁeld networks and Boltzmann Ma- like elements that share undirected connections with their
chines and the beneﬁts of Sparse Distributed Memories over neighbors. In the simplest version, each element has two
them. In Section 3, we formally deﬁne our framework. Fol- states. With each undirected connection is associated a
lowing this, we discuss a simple algorithm for learning from weight, which inﬂuences the corresponding neighbor into be-
observations in Section 4. We then give examples showing ing in a certain state, based on the weight sign. Usually, a
the predictive power of the algorithm in Section 5. Finally, positive weight indicates that both units should be in the same
Section 6 discusses the sort of environments for which our state, whereas a negative weight indicates they should take
method is suitable, its current failings compared to existing different states. The system then attempts to minimize its en-
models and how it may be used to obtain abstraction.  ergy. This energy increases when units of the system take
                                                      states that violate the inﬂuence of their connection weight.
2  Background                                           Inputs can be given to these algorithms by forcing certain
                                                      elements to stay in a given state, for example by explicitly
Sparse Distributed Memories (SDMs) were developed in or- deﬁning input units with ﬁxed values. Since both algorithms
der to model long-term memory [Kanerva, 1993]. Of interest are undirected by nature, obtaining a minimum energy state (a
is the capacity of such memories to both retrieve a noise-free solution) requires iterating over the unit states until the energy
version of an input vector (auto-association) and to retrieve an stagnates. Unfortunately, this process can be slow. Learning
associated output vector (hetero-association). As opposed to in such a model is also computationally expensive, although
other types of associative memories, SDMs do not require an Restricted Boltzmann Machines have recently been used to
iterative process in order to converge to the desired answer. In achieve good results [Hinton et al., 2006].
that respect, they resemble feed-forward neural networks, and
have been modelled as such. SDMs have been used in pre-
diction, for example for robot navigation [Rao and Fuentes, 3 Framework
1996].                                                Unfortunately, SDMs suffer from a big disadvantage. They
  A SDM  is divided into two parts: the hard locations and are, by nature, deterministically noise-correcting and do not
the output words. A hard location is simply a binary vector allow us to predict events that may occur. Before we discuss
of the same length as the input vector. To each hard location our proposed framework, we must make a few assumptions
corresponds an output word which is composed of an integer- regarding the environment that we are interested in modelling
valued vector, possibly of different length.          and deﬁne a few terms. First, let us deﬁne a percept as a
  When a vector is given as input to the memory, its distance real-valued vector representing an observation. Secondly, we
di to each hard location i is computed; in the original imple- assume that association is not done on a one-to-one basis.
mentation, di is simply a Hamming distance. The output of Rather, a given percept may be associated to many other per-
the system is then obtained in the following way: for each cepts, with different strengths of association for each. We
hard location with distance di ≤ δ, where δ is a threshold formalize this by proposing that an input percept p maps to a
value, its corresponding word is added to the output sum s. distribution over associated percepts.
The actual output o is found by thresholding the sum such We deﬁne the memory as a set of cells, C. Each of these
that oi =1if si > 0, and oi =0otherwise.              cells acts as a hard location, and therefore has a single per-
  Learning in SDMs is fairly straightforward. When we wish cept associated with it, which we denote Ci through nota-
to map an input v to a target word z, we determine which hard tional abuse. Each cell also has a saliency value si associated
locations are activated (di ≤ δ)byv. For each of these, we with it and a vector of output weights, Wi. Wi represents
add z to their corresponding output word.             directed associations between cells, and so Wi has the same
  If the hard locations are orthogonal, we obtain something size as C. In general, we denote the weight matrix W and the
very close to a linear approximator. Such systems have been saliency vector corresponding to C, s.
studied extensively, for example as associative search net- When the system is presented with an input percept p,it
works [Barto et al., 1981]. If the hard locations are not or- computes an activation vector α similar to the activation in
thogonal, however, the memory potentially gains in general- SDMs. Here however, α is a real-valued vector with elements
ization capacity by producing more complex combinations of taking values between 0 and 1, where αi =1indicates a
mappings. Various works on SDMs have experimented with perfect match between p and Ciand αi =0indicates no
extending the system to handle real values, computing its ca- match. Usually, we would like i αi =1. In our work,
pacity, and deciding how to craft the hard locations to bet- we use the simplest activation function, namely αi =1if
ter suit a given task. Presented as above, SDMs are interest- Ci = p, and αi =0otherwise. In effect, we are assuming
ing as they propose an intuitive coverage of the input space that percepts are actually symbolic and not subject to noise;
through actual potential inputs (work on this idea in rein- this need not be the case in general.
forcement learning was done in [Ratitch and Precup, 2004]). By themselves, SDMs do not allow any association based
Their hetero-associative capacity also makes them promising on past observations. To circumvent this, we use the saliency

                                                IJCAI-07
                                                   251value of cells as an activation trace. More formally, at every  1. Initialize hard locations
time step we set the saliency vector to                         2. W ← 0
                    s ← γs + α                                  3. For each episode do
                                                                4.  s ← 0
  This is similar to the cumulative eligibility trace in rein-  5.  Repeat until done
forcement learning [Sutton, 1988], where γ is a decay factor.   6.    Compute π from s
A restricted form of this type of encoding has also been used   7.    Observe the next percept p
for prediction in [Bose et al., 2005; Furber et al., 2004].If   8.    Update W based on π − α(p)
αi =1for exactly one percept and 0 everywhere else, s rep-      9.    s ← γs + α(p)
resents the past sequence of observations, provided no per-
cept has been observed twice. Note that for the purposes of Table 1: Our context-based prediction algorithm.
the system, s does not need to be an exponentially decaying
trace of observations; its goal is to serve as context to obtain
                                                      of cell i. Here Wi,j represents how strongly j inﬂuences the
better predictions. In Section 6 we will discuss one possible    i
way of improving the above equation.                  prediction of.                     
                                                            σ =    eτβi              β  =     W   s
  Our algorithm diverges from SDMs as it attempts to predict Let  i    , and recall that k   i  k,i i. First
                                                      note that:
percepts that match its hard locations, rather than separating                                          
them from the output words. We deﬁne secondary activation, ∂          ∂   eτβk   τeτβk     ∂
                                                            π   =             =         σ      β − eτβi s
          β                                                  k                     2            k       j
denoted by , as a vector representing a prediction weight ∂Wi,j     ∂Wi,j  σ      σ      ∂Wi,j
for each cell. We are interested in predicting which of the         
                                                                      τs π  (1 − π )   k = i
percepts (represented by cells) may be observed. We assume      =       j k      k   if
here that the hard locations represent actual percepts. We then       −τsjπkπi       otherwise
compute β as
                     β = W  s                         Let 
 be the vector of errors, with 
i = πi − αi. From the
                                                      above equation we obtain:
From this equation one can notice that the weight matrix in-
                                                                   
deed acts as a set of associations; experiencing a percept leads ∂               ∂
                                                            E  =      (πk − αk)      πk
to related percepts being predicted.                  ∂Wi,j                    ∂Wi,j
  At the very least the values of β should be in the correct        k                     
prediction order and signiﬁcantly different. Any function of   =   τsj(πi(1 − πi)(πi − αi) −  πiπk(πk − αk))
β which preserves this ordering and results in a valid proba-                              k=i
bility distribution may be used to predict the next time step.                      
                                                               =   τs π ((1 − π )
 −   π  
 )
We chose to use a simple Boltzmann distribution using β and          j i      i  i       k k
given by:                                                                           k=i
                             τβ
                            e  i                               =   τsjπi(
i − π · 
)
                 P (Ci|s)=
                              eτβi
                             i                        We can then modify the output weights through a standard
The distribution’s entropy is controlled by τ, a standard tem- update rule with learning rate c ∈ (0, 1):
perature parameter between 0 and ∞. In the experiments we
used τ =1but other values may yield better results.                                  ∂
                                                                     Wi,j ← Wi,j − c     E
                                                                                   ∂Wi,j
4  Learning
                                                        Usually, probability-producing systems are trained using a
After having discussed how the algorithm predicts events, likelihood-based gradient. Here, however, there are two rea-
we now describe how learning can be accomplished. Log- sons why we might prefer to use the sum of squared errors
                                 s
ically, since we use the saliency vector as contextual hints to compute the gradient. First, it explicitly allows us to train
which allow us to make a better prediction, we should modify the system to output a combination of hard locations, through
               s
weights based on .                                    the α vector. This can be interesting if many percepts acti-
  For now, assume that we already have a known set of hard vate two or three hard locations due to noise. Also, we are
            pt                           t
locations. Let be the percept observed at time , and sim- interested in good generalization capabilities. Experiments in
     Ct               W  t                   st
ilarly , the set of cells, the weight matrix and the  which we used maximum likelihood gave worse results. We
                                        pt   α(pt)
saliency vector. We denote the activation due to by . believe that is might due to the fact that there is no ’ground
                              pt                s
  Assuming that we want to predict in the future when is truth’ distribution which we are approximating; instead, we
                       W t
present, we should modify to produce a probability dis- are constructing an appropriate distribution through associa-
                α(pt)            π
tribution similar to . Formally, let be our probability tion.
            C                           E
distribution on ; we deﬁne the prediction error as:     There is a second learning problem, which we ignore here,
                     1 
                E =      (π − α )2                    but is of interest. The hard locations do not have to be pre-
                     2     i    i                     deﬁned, or ﬁxed. In a way, learning to recognize a percept can
                       i                              be just as hard as prediction. We discuss this issue further in
              th                t
where αi is the i component of α(p ). We then compute Section 6 below. The whole algorithm is presented in Table
                                          th
the gradient of the error with respect to Wi,j , the j weight 1.

                                                IJCAI-07
                                                   252      Episodes          40    80    400   800              Episodes            50    100   200   500
      P(1)=0.5   P(1)  0.41  0.43  0.45  0.45            Context 1,2  P(3|12) 0.46  0.61   0.74  0.85
                 P(2)  0.46  0.48  0.51  0.51                         P(4|12) 0.24  0.19   0.13  0.075
      P(1)=0.75  P(1)  0.69  0.72  0.73  0.73            Context 2,1  P(3|21) 0.21  0.17   0.12  0.07
                 P(2)  0.20  0.20  0.23  0.24                         P(4|21) 0.50  0.64   0.75  0.855
     P(1)=0.875  P(1)  0.79  0.84  0.86  0.86
                 P(2)  0.12  0.10  0.10  0.11         Table 3: Predicted observation based on order of past obser-
                                                      vations.
Table 2: Predicted observation frequencies based on the num-
ber of training episodes.                                    Episodes   100   200   500    1000   2000
                                                        Sequence pairs
                                                                  618  0.77  0.86   0.92   0.95   0.96
5  Examples                                                       719  0.78  0.86   0.92   0.95   0.96
In this section we give examples in order to show that our       6128  0.59  0.74   0.86   0.90   0.935
proposed algorithm can indeed perform prediction in a simi-      7129  0.62  0.75   0.86   0.905  0.935
lar fashion to that of a strict Markovian Model, without being  61238  0.44  0.55   0.71   0.80   0.87
restricted by a ﬁxed history length or states. For all of the ex- 71239 0.50 0.60   0.74   0.82   0.88
amples, we use sequences of numbers as observations. Each
number is encoded as a n-sized vector with the corresponding   612348  0.40  0.44   0.53   0.63   0.73
bit set to 1 and all others set to 0; n is the maximum number of 712349 0.46 0.51   0.60   0.68   0.75
observations. Note that in a more natural task a percept vec- 6123458  0.39  0.42   0.45   0.49   0.55
tor may represent sensory input, and so its structure would be 7123459 0.45  0.48   0.525  0.57   0.62
well-deﬁned. To simplify matters, we assume that the agent
always receives the percept 0 (ﬁrst bit set to 1) at the begin- Table 4: Predicted observation based on long-term context.
ning of each episode, and never receives it during the episode. Values in italic show when the actual observation was not pre-
This is similar to deﬁning a special start symbol when learn- dicted with the highest probability.
ing a kth order Markov Model, and we would expect this to
have little impact in a system that learns continuously. Here
                                                      are also predicted with probabilities that decrease as training
we are using a learning rate of 0.5, which gave sufﬁciently
                                                      increases. If we were only interested in predicting the end
stable results. The decay factor γ was also arbitrarily set to
                                                      symbol, we could increase τ to obtain higher probabilities.
0.5.
                                                      With our choice of parameters, the initial symbol (1 or 2) is
  The ﬁrst experiment is the simplest one, and shows that predicted to occur with roughly 50% chance, as in the ﬁrst
the system can approximate symbol frequencies. We simply experiment.
produce two separate one-observation episodes with certain
                                                        The third set of sequences that we looked at shows that the
frequencies. One of these episodes produces a 1, while the
                                                      system can learn to use context from any point in the past to
other produces a 2. In order to avoid variance in the results,
                                                      differentiate two predictions. More speciﬁcally, we have two
we chose to use a ﬁxed sequence of episodes. These always
                                                      target symbols (8 and 9) which can only be predicted based
start with 1, end in 2 and episodes containing 2’s are experi-
                                                      on the ﬁrst symbol that occurs, which is either a 6 or a 7. The
enced at regular intervals in-between. Estimated frequencies
                                                      training strings and the predicted probability of the correct
are given in Table 2, where we show the probability of each
                                                      symbol are reported in Table 4.
event after a certain number of training episodes. Actual fre-
                                                        As can be seen from this table, the system follows a slow
quencies are given on the left-hand side. Note that the given
                                                      degradation in predicting the correct event as the differenti-
probabilities do not sum to one due to the Boltzmann distribu-
                                                      ating observation becomes more remote. The fact that 9 is
tion: it assigns a non-zero probability to all events, and here
                                                      systematically predicted with higher probability than 8 is an
0 is predicted as unlikely but not impossible. We can see that
                                                      artefact of our experiment, due to the sequence containing
with sufﬁcient training, all estimates converge towards their
                                                      9 being presented after the sequence containing 8. It is in-
true values. The learning rate here prevents us from obtaining
                                                      teresting to note that for the longest example given here, the
the exact probabilities as the output weights oscillate between                                       −5
                                                      saliency of the context percept at the time of prediction is 2 .
episodes.
                                                      Yet it can be seen that as the number of iterations increases,
  The second experiment is aimed at showing that the sys- the algorithm learns to correctly distinguish between the two
tem, despite having no explicit indication of ordering, can in events.
fact discriminate based on recency. The example strings that
we use are respectively 123 and 214. The goal here is to show
that we can predict with high probability which of the two end 6 Discussion
symbols will appear based on ordering. Results are given in The framework presented seems to provide us with a way of
Table 3.                                              predicting events which, if not perfectly accurate, is not sub-
  Clearly as the number of training iterations increases the ject to history length constraints and does not require explicit
system becomes able to more precisely predict the true sym- state knowledge. Of chief interest is the fact that the algo-
bol based on context. Again here, the symbols 0, 1 and 2 rithm, as shown above, can handle temporal ordering, which

                                                IJCAI-07
                                                   253can be key to many predictions. However, it can also han- when this happens. This may be seen as a failure of the frame-
dle predicting observations from unordered sequences. As a work, and indeed it is not suited for environments where there
brief example, we can imagine an environment where we ob- are very few observations. However, if we consider that the
tain clues about the ’state’ of the world, which is represented algorithm builds a causality link from a percept to another,
by a separate percept (possibly one that occurs often while then the problem may be solved in the following way. The
in that ’state’). Markov Models relying on a strict order of repeated occurrence of a percept does not provide additional
observations will need many samples to produce clear pre- information; it instead becomes a question of the duration of
dictions. Our algorithm, on the other hand, can infer from the observation. Our framework implicitly handles duration
many weak clues a more general percept.               if we do not allow non-zero output weights from a cell to it-
  Of a technical nature, both the saliency vector and the self. Indeed, the output weights to the context should become
weight updating scheme are ﬂexible and may be modiﬁed to larger in order to accurately predict an event which occurs
ﬁt different situations. For example, we experimented with for a longer period of time, and therefore such events should
modifying the saliency vector to keep relevant percepts in be predicted even when the relevant contextual information is
context and remove more quickly irrelevant ones. This can further in the past.
be easily done by considering the gradient of the error on the
                                                        We purposefully left out the recognition problem in our
current prediction with respect to the saliency of each cell; the
                                                      presentation of the algorithm. Constructing suitable hard lo-
update rule is then very similar to the one used for the output
                                                      cations might not be simple. However, our framework im-
weights. The weight updating scheme may also be improved
                                                      plicitly proposes a different approach to recognition. A set
by preventing negative gradients. Currently, our weight up-
                                                      of temporally or spatially related percepts may all be associ-
date rule reduces the probability of all the events that did not
                                                      ated together to form a single, more abstract object. This can
occur (αi =0) at the same time as it increases the probability
                                                      be the case in Computer Vision for example, where multiple
of the actual observation. This might hinder later learning; a
                                                      poses are mapped to be same object.
purely additive approach, closer to a frequency-based model,
might perform better.                                   In a similar fashion, the algorithm is not restricted to a sin-
  Obviously, being able to predict events more or less accu- gle modality. We have extended it to include multiple modal-
rately is not sufﬁcient for an agent; we also need correct con- ities which are associated together. Although a separate pre-
trol. From a reinforcement learning point of view, we can nat- diction is made for each modality, context is constituted of all
urally incorporate reward into our framework in three speciﬁc modalities. Such a capacity opens many opportunities, such
ways. First, we can explicitly attempt to assign a value to each as building knowledge through associations across modali-
observation, and compute the predicted expected value based ties. Among the most striking is the possibility to link per-
on context. A similar idea was developed in [Ratitch and ceptual knowledge to actions if both are represented in the
Precup, 2004], but their algorithm did not explicitly model associative framework proposed.
observation prediction, and was done in a fully observable
framework. We can also modify the weight update rule to use The discussion above suggests the use of actual percepts as
the magnitude of rewards to focus learning on important parts abstraction, ie. strong contextual clues. An abstract percept
of the system. More interestingly, though, we can modify the may be one that activates its instances, for example the idea
saliency vector based on reward information, so that a percept of a tree may simply be a cartoon tree, which then maps to
which is known to be associated with high reward or penalty all other kinds of trees. What is interesting here is that con-
might be kept in context longer.                      sidering an abstract object as a regular percept allows us to
                                                      manipulate it atomically, without regard for its abstract na-
  In the case of a truly stochastic event, similar to what we
                                                      ture. Having atomic abstractions in turn allows us to abstract
presented in the ﬁrst experiment of Section 5, our predic-
                                                      them, and gives us the capacity to build hierarchical knowl-
tion will, by deﬁnition, never be accurate. This is in some
                                                      edge without any additional apparatus.
sense a drawback of the algorithm: transitions are expected
be fully determined by some past event. There are many ways Past research on association and prediction has mainly fo-
to address this problem. First, we can assume that no event cused on obtaining averages of noisy observations. The nov-
is truly stochastic in nature, and that stochasticity only ap- elty in our approach comes from the fact that we put the em-
pears through lack of contextual information. In such a case, phasis on the importance of associations between incomplete
we could try inferring missing past events by increasing the observations. As such, our framework takes a radically differ-
saliency of percepts which usually cause the current observa- ent view of perceptions: noise can be overcome through the
tion. Another approach would be to explicitly consider the association of noisy signals. All the associative memories dis-
variance of a prediction. Qualitatively, large output weights cussed above share the fault that they are geared towards pro-
suggest strong evidence towards a prediction. If more than ducing an ideal output. Rather, we hope that our framework
one event is strongly activated by context, then there is reason can build many (possibly noisy) associations which, when
to believe that stochasticity will appear in the observations. combined, yield a correct answer. This is something that can
  One question that has been largely ignored so far in this pa- only be done if we can produce a probability distribution over
per is that of handling percepts which occur more than once associations; we also need to be able to use events from any
in a short interval of time. Since the algorithm has no way of time in the past. Our algorithm, by achieving both of these,
specifying that an observation has occurred twice, we lose the seems promising for predictive and knowledge-building pur-
capacity of a kth order model to make a separate prediction poses.

                                                IJCAI-07
                                                   254