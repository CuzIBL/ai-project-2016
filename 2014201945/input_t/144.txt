              Generalizing Plans to New Environments in Relational MDPs 
                 Carlos Guestrin Daphne Koller Chris Gearhart Neal Kanodia 
                              Computer Science Department, Stanford University 
                               {guestrin, koller, cmg33, nkanodia}@cs.stanford.edu 

                      Abstract 
   A longstanding goal in planning research is the ability to gen•
   eralize plans developed for some set of environments to a 
   new but similar environment, with minimal or no replanning. 
   Such generalization can both reduce planning time and al•
   low us to tackle larger domains than the ones tractable for 
   direct planning. In this paper, we present an approach to 
   the generalization problem based on a new framework of re•
   lational Markov Decision Processes (RMDPs). An RMDP 
   can model a set of similar environments by representing ob•
                                                       Figure 1: Freecraft strategic domain with 9 peasants, a barrack, a 
   jects as instances of different classes. In order to generalize 
                                                       castle, a forest, a gold mine, 3 footmen, and an enemy, executing the 
   plans to multiple environments, we define an approximate 
                                                       generalized policy computed by our algorithm. 
   value function specified in terms of classes of objects and, in 
   a multiagent setting, by classes of agents. This class-based research 115; 161, and even earlier in traditional planning [5]. 
   approximate value function is optimized relative to a sam• This problem is a challenging one, because it is often unclear 
   pled subset of environments, and computed using an efficient how to translate the solution obtained for one domain to an•
   linear programming method. We prove that a polynomial other. MDP solutions assign values and/or actions to states. 
   number of sampled environments suffices to achieve perfor•
   mance close to the performance achievable when optimizing Two different MDPs (e.g., two Freecraft scenarios), are typ•
   over the entire space. Our experimental results show that our ically quite different, in that they have a different set (and 
   method generalizes plans successfully to new, significantly even number) of states and actions. In cases such as this, the 
   larger, environments, with minimal loss of performance rel• mapping of one solution to another is not well-defined. 
   ative to environment-specific planning. We demonstrate our Our approach is based on the insight that many domains 
   approach on a real strategic computer war game.     can be described in terms of objects and the relations between 
                                                       them. A particular domain will involve multiple objects from 
1 Introduction                                         several classes. Different tasks in the same domain will typ•
Most planning methods optimize the plan of an agent in a ically involve different sets of objects, related to each other 
fixed environment. However, in many real-world settings, an in different ways. For example, in Freecraft, different tasks 
agent will face multiple environments over its lifetime, and might involve different numbers of peasants, footmen, ene•
its experience with one environment should help it to perform mies, etc. We therefore define a notion of a relational MDP 
well in another, even with minimal or no replanning.   (RMDP), based on the probabilistic relational model (PRM) 
   Consider, for example, an agent designed to play a strate• framework 110]. An RMDP for a particular domain pro•
gic computer war game, such as the Freecraft game shown vides a general schema for an entire suite of environments, 
in Fig. 1 (an open source version of the popular Warcraft or worlds, in that domain. It specifies a set of classes, and 
game). In this game, the agent is faced with many scenar• how the dynamics and rewards of an object in a given class 
ios. In each scenario, it must control a set of agents (or units) depend on the state of that object and of related objects. 
with different skills in order to defeat an opponent. Most sce• We use the class structure of the RMDP to define a value 
narios share the same basic elements: resources, such as gold function that can be generalized from one domain to another. 
and wood; units, such as peasants, who collect resources and We begin with the assumption that the value function can 
build structures, and footmen, who fight with enemy units; be well-approximated as a sum of value subfunctions for the 
and structures, such as barracks, which are used to train foot• different objects in the domain. Thus, the value of a global 
men. Each scenario is composed of these same basic build• Freecraft state is approximated as a sum of terms correspond•
ing blocks, but they differ in terms of the map layout, types ing to the state of individual peasants, footmen, gold, etc. We 
of units available, amounts of resources, etc. We would like then assume that individual objects in the same class have 
the agent to learn from its experience with playing some sce• a very similar value function. Thus, we define the notion 
narios, enabling it to tackle new scenarios without significant of a class-based value function, where each class is associ•
amounts of replanning. In particular, we would like the agent ated with a class subfunction. All objects in the same class 
to generalize from simple scenarios, allowing it to deal with have the value subfunction of their class, and the overall value 
other scenarios that are too complex for any effective planner. function for a particular environment is the sum of value sub-
   The idea of generalization has been a longstanding goal in functions for the individual objects in the domain. 
Markov Decision Process (MDP) and reinforcement learning A set of value subfunctions for the different classes imme-


PROBABILISTIC PLANNING                                                                                1003 diately determines a value function for any new environment          specifies a set of objects de•
in the domain, and can be used for acting. Thus, we can com• noted o.L. For example, in a world containing 2 peasants, 
pute a set of class subfunctions based on a subset of environ• we would have 
ments, and apply them to another one without replanning. if Peasantl is building a barracks, we would have that 
   We provide an optimality criterion for evaluating a class- Peasant l.BuildTarget = Barrackl. 
based value function for a distribution over environments, and The dynamics and rewards of an RMDP are also de•
show how it can, in principle, be optimized using a linear pro• fined at the schema level. For each class, the schema 
gram. We can also "learn" a value function by optimizing specifies an action C.A, which can take on one of sev•
it relative to a sample of environments encountered by the eral values Doni For example, Doni-
agent. We prove that a polynomial number of sampled en• Wait, Mine, Harvest, Build Each class C is also associ•
vironments suffice to construct a class-based value function ated with a transition model , which specifies the proba•
which is close to the one obtainable for the entire distribution bility distribution over the next state of an object in class 
over environments. Finally, we show how we can improve C, given the current state of the action taken on , and the 
the quality of our approximation by automatically discover• states and actions of all of the objects linked to o: 
ing subclasses of objects that have "similar" value functions.                                         (1) 
   We present experiments for a computer systems admin• For example, the status of a barrack, Barrack.Sta/z*/, 
istration task and two Freecraft tasks. Our results show depends on its status in the previous time step, on 
that we can successfully generalize class-based value func• the task performed by any peasant that could build it 
tions. Importantly, our approach also obtains effective poli• (Barrack.BuiltBy.Task), on the amount of wood and gold, etc. 
cies for problems significantly larger than our planning algo•
                                                         The transition model is conditioned on the state of C.Lt, 
rithm could handle otherwise.                         which is, in general, an entire set of objects (e.g., the set of 
                                                      peasants linked to a barrack). Thus we must now provide 
2 Relational Markov Decision Processes                a compact specification of the transition model that can de•
A relational MDP defines the system dynamics and rewards pend on the state of an unbounded number of variables. We 
at the level of a template for a task domain. Given a particu• can deal with this issue using the idea of aggregation [10]. 
lar environment within that domain, it defines a specific MDP In Freecraft, our model uses the count aggregator where 
instantiated for that environment. As in the PRM framework the probability that Barrack.Status transitions from Unbuilt to 
of [10], the domain is defined via a schema, which speci• Built depends on Barrack.BuiltBy.Task - Built], the num•
fies a set of object classes Each class               ber of peasants in Barrack.BuilBy whose Task is Build. 
C is also associated with a set of state variables       Finally, we also define rewards at the class level. We as•
                 which describe the state of an object in sume for simplicity that rewards are associated only with the 
that class. Each state variable C.S has a domain of possible states of individual objects; adding more global dependencies 
values Doni We define Sc to be the set of possible    is possible, but complicates planning significantly. We define 
states for an object in the possible assignments to the a reward function which represents the con•
state variables of C.                                 tribution to the reward of any object in C. For example, we 
   For example, our Freecraft domain might            may have a reward function associated with the Enemy class, 
have classes such as Peasant, Footman, Gold;          which specifies a reward of 10 if the state of an enemy object 
the class Peasant may have a state variable           is Dead: (Enemy.State = Dead) = 10. We assume 
Task whose domain is Dom[Peasant.Task:] = that the reward for each object is bounded by 
{Waiting, Mining, Harvesting, Building}, and a state     Given a world, the RMDP uniquely defines a ground fac•
variable Health whose domain has three values. In this tored MDP whose transition model is specified (as usual) 
case, SPeasant would have 4 • 3 = 12 values, one for each as a dynamic Bayesian network (DBN) [3]. The random vari•
combination of values for Task and Health.            ables in this factored MDP are the state variables of the in•
  The schema also specifies a set of links = dividual objects o.S, for each and for each 
            for each class representing links between ob• S Thus, the state s of the system at a given point in 
jects in the domain. Each link C.L has a range        time is a vector defining the states of the individual objects in 
For example, Peasant objects might be linked to Barrack the world. For any subset of variables X in the model, we de•
objects [Peasant.BuildTarget] = Barrack, and to the   fine s[X] to be the part of the instantiation s that corresponds 
global Gold and Wood resource objects. In a more com• to the variables X. The ground DBN for the transition dy•
plex situation, a link may relate C to many instances of a namics specifies the dependence of the variables at time 
class C, which we denote by for example,              on the variables at time The parents of a variable _.S' are 
 Enemy.MyJFootmen indicates that an instance          the state variables of the objects that are linked to . In our 
of the enemy class may be related to many footman instances. example with the two peasants, we might have the random 
  A particular instance of the schema is defined via a variables Peasantl.Task, Peasant2.Task, Barrackl.Stato, 
world specifying the set of objects of each class; we use etc. The parents of the time variable Barrackl.Status' 
       to denote the objects in class C, and to de•   are the time variables Barrackl. Status', Peasant l.Task, 
note the total set of objects in The world also spec• Peasant2.Task, Goldl Amount and Woodl Amount. 
ifies the links between objects, which we take to be fixed The transition model is the same for all instances in the 
throughout time. Thus, for each link C.L, and for each same class, as in (1). Thus, all of the o.Status variables for 


1004                                                                             PROBABILISTIC PLANNING  Figure 2: Freecraft tactical domain: (a) Schema; (b) Resulting fac•
 tored MDP for a world with 2 footmen and 2 enemies. 
 barrack objects o share the same conditional probability dis•
 tribution. Note, however, that each specific barrack depends 
 on the particular peasants linked to it. Thus, the actual parents 
 in the DBN of the status variables for two different barrack 
 objects can be different. 
   The reward function is simply the sum of the reward func•
 tions for the individual objects: 


 Thus, for reward function for the Enemy class described 
 above, our overall reward function in a given state will be 
 10 times the number of dead enemies in that state. 
   It remains to specify the actions in the ground MDP The 
 RMDP specifies a set of possible actions for every object in 
 the world. In a setting where only a single action can be taken 
 at any time step, the agent must choose both an object to 
 act on, and which action to perform on that object. Here, 
 the set of actions in the ground MDP is simply the union 
     Dom[o.i4]. In a setting where multiple actions can be 
 performed in parallel (say, in a multiagent setting), it might 
 be possible to perform an action on every object in the domain 
 at every step. Here, the set of actions in the ground MDP is a In our setting, the state space is exponentially large, with 
 vector specifying an action for every object: Dom[o.i4]. one state for each joint assignment to the random variables 
 Intermediate cases, allowing degrees of parallelism, are also o.S of every object (e.g., exponential in the number of units in 
possible. For simplicity of presentation, we focus on the mul• the Freecraft scenario). In a multiagent problem, the number 
tiagent case, such as Freecraft, where, an action is an assign• of actions is also exponential in the number of agents. Thus 
ment to the action of every unit.                      this LP has both an exponential number of variables and an 
Example 2.1 (Freecraft tactical domain) Consider a sim• exponential number of constraints. Therefore the exact solu•
plified version of Freecraft, whose schema is illustrated tion to this linear program is infeasible. 
in Fig. where only two classes of units partici•          We address this issue using the assumption that the 
pate in the game: Both value function can be well-approximated as a sum of 
the footman and the enemy classes have only one state  local value subfunctions associated with the individual 
 variable each, Health, with domain Dom[Health]        objects in the model. (This approximation is a special 
 {Healthy, Wounded, The footman class contains         case of the factored linear value function approach used 
one single-valued link: [Footman.MyJEnemy] Enemy. in [6].) Thus we associate a value subfunction with 
Thus the transition model for a footman's health will  every object in w. Most simply, this local value function 
depend on the health of its enemy:                     can depend only on the state of the individual object Sa. 
                       i.e., if footman's enemy is     In our example, the local value subfunction for 
not dead, than the probability that a footman will be• enemy object Enemy 1 might associate a numeric value for 
come wounded, or die, is significantly higher. A foot• each assignment to the variable Enemy J.Health. A richer 
man can choose to attack any enemy. Thus each foot•    approximation might associate a value function with pairs, 
man is associated with an action Footman.A which se•   or even small subsets, of closely related objects. Thus, the 
lects the enemy it is attacking.1 As consequence, 
                                                       world requires a small extension of our basic representation. We 
    A model where an action can change the link structure in the omit details due to lack of space. 


PROBABILISTIC PLANNING                                                                                1005                                                        does not help us provide a value function for objects in other 
                                                       worlds, especially worlds with different sets of objects. 
                                                          To obtain generalization, we build on the intuition that dif•
                                                       ferent objects in the same class behave similarly: they share 
                                                       the transition model and reward function. Although they dif•
                                                       fer in their interactions with other objects, their local contri•
                                                       bution to the value function is often similar. For example, 
                                                       it may be reasonable to assume that different footmen have a 
                                                       similar long-term chance of killing enemies. Thus, we restrict 
                                                       our class of value functions by requiring that all of the objects 
                                                       in a given class share the same local value subfunction. 
                                                          Formally, we define a class-based local value subfunc•
                                                       tion for each class. We assume that the parameteriza•
                                                       tion of this value function is well-defined for every object 
                                                       o in C. This assumption holds trivially if the scope of 
                                                       is simply we simply have a parameter for each as•
   As for any linear approximation to the value function, the signment to Dom When the local value function can 
 LP approach can be adapted to use this value function rep• also depend on the states of neighboring objects, we must 
 resentation [14]. Our LP variables are now the local compo• define the parameterization accordingly; for example, we 
 nents of the individual local value functions:        might have a parameter for each possible joint state of a 
                                                       linked footman-enemy pair. Specifically rather than defin•
                                                 (3)   ing separate subfunctions we de•
 In our example, there will be one LP variable for each joint fine a class-based subfunction Now the contri•
 assignment of FI.Health and El.Health to represent the com• bution of Footmanl to the global value function will be 
 ponents of Similar LP variables will be included for         (F1.Health,El.Health). Similarly Footman! will 
 the components of                                     contribute (F2.Health, E2.Health). 
   As before, we have a constraint for each global state s and A class-based value function defines a specific value func•
 each global action                                    tion for each world w, as the sum of the class-based local 
                                                       value functions for the objects in u)\ 

                                                                                                       (5) 
 This transformation has the effect of reducing the number of 
 free variables in the LP to n (the number of objects) times the This value function depends both on the set of objects in the 
 number of parameters required to describe an object's local world and (when local value functions can involve related ob•
 value function. However, we still have a constraint for each jects) on the links between them. Importantly, although ob•
 global state and action, an exponentially large number. jects in the same class contribute the same function into the 
   Guestrin, Koller and Parr [6] (GKP hereafter) show that, summation of (5), the argument of the function for an object 
 in certain cases, this exponentially large LP can be solved is the state of that specific object (and perhaps its neighbors). 
 efficiently and exactly. In particular, this compact solution In any given state, the contributions of different objects of the 
 applies when the MDP is factored (i.e., represented as a same class can differ. Thus, every footman has the same local 
 DBN), and the approximate value function is decomposed value subfunction parameters, but a dead footman will have a 
 as a weighted linear combination of local basis functions, as lower contribution than one which is alive. 
 above. Under these assumptions, GKP present a decomposi•
 tion of the LP which grows exponentially only in the induced 5 Finding Generalized MDP Solutions 
 tree width of a graph determined by the complexity of the With a class-level value function, we can easily generalize 
 process dynamics and the locality of the basis function. from one or more worlds to another one. To do so, we as•
   This approach applies very easily here. The structure of sume that a single set of local class-based value functions 
 the DBN representing the process dynamics is highly fac• is a good approximation across a wide range of worlds As•
 tored, defined via local interactions between objects. Simi• suming we have such a set of value functions, we can act in 
 larly, the value functions are local, involving only single ob• any new world without replanning, as described in Step 3 
jects or groups of closely related objects. Often, the induced of Fig. 3. We simply define a world-specific value function as 
 width of the resulting graph in such problems is quite small, in (5), and use it to act. 
 allowing the techniques of GKP to be applied efficiently. We must now optimize in a way that maximizes the 
                                                       value over an entire set of worlds. To formalize this intuition, 
4 Generalizing Value Functions                         we assume that there is a probability distribution over 
Although this approach provides us with a principled way the worlds that the agent encounters. We want to find a sin•
 of decomposing a high-dimensional value function in certain gle set of class-based local value functions that is a 
 types of domains, it does not help us address the generaliza• good fit for this distribution over worlds. We view this task as 
 tion problem: A local value function for objects in a world one of optimizing for a single "meta-level" MDP where 


 1006  nature first chooses a world and the rest of the dynam• for these worlds only. The resulting class-based value func•
 ics are then determined by the . Precisely, the state tion can then be used for worlds that were not sampled. 
 space of The transi•                                     We will start by sampling a set of worlds according 
tion model is the obvious one: From the initial state nature to We can now define our LP in terms of the worlds 
chooses a world according to and an initial state in   in D, rather than all possible worlds. For each world in 
w according to the initial starting distribution over the our LP will contain a set of constraints of the form presented 
 states in The remaining evolution is then done according to in Eq. (4). Note that in all worlds these constraints share the 
    dynamics. In our example, nature will choose the number variables which represent our class-based value function. 
                                                       The complete LP is given by: 
 of footmen and enemies, and define the links between them, 
which then yields a well-defined MDP,e.g.,                Variables: 
5.1 LP Formulation                                        Minimize: 
The meta-MDP allows us to formalize the task of finding a 
generalized solution to an entire class of MDPs. Specifically, 
we wish to optimize the class-level parameters for not for Subject to: 
a single ground MDP but for the entire 
   We can address this problem using a similar LP solu•
tion to the one we used for a single world in Sec. 3. The 
variables are simply parameters of the local class-level value 
subfunctions : For 
the constraints, recall that our object-based LP formulation 
in (4) had a constraint for each state s and each action vector                                         (8) 
                In the generalized solution, the state space where is the marginalization of to the vari•
 is the union of the state spaces of all possible worlds. Our ables in For each world, the constraints have the same 
constraint set for will, therefore, be a union of constraint form as the ones in Sec. 3. Thus, once we have sampled 
sets, one for each world each with its own actions:    worlds, we can apply the same LP decomposition techniques 
                                                       of GKP to each world to solve this LP efficiently. Our gener•
                                                       alization algorithm is summarized in Step 2 of Fig. 3. 
                                                 (6)      The solution obtained by the LP with sampled worlds will, 
 where the value function for a world, (s), is defined at in general, not be equal to the one obtained if all worlds are 
the class level as in Eq. (5). In principle, we should have an considered simultaneously. However, we can show that the 
additional constraint for the state s0. However, with a natural quality of the two approximations is close, if a sufficient num•
choice of state relevance weights a, this constraint is elimi• ber of worlds are sampled. Specifically, with a polynomial 
nated and the objective function becomes:              number of sampled worlds, we can guarantee that, with high 
                                                       probability, the quality of the value function approximation 
    Minimize: (7) obtained when sampling worlds is close to the one obtained 
                                                       when considering all possible worlds. 
                                                       Theorem 5.1 Consider the following class-based value func•
if In some models, the potential number 
of objects may be infinite, which could make the objective tions (each with k parameters): obtained from the LP over 
function unbounded. To prevent this problem, we assume all possible worlds by minimizing Eq. (7) subject to the con•
that the goes to zero sufficiently fast, as the num•   straints in obtained from the LP with the sampled 
ber of objects tends to infinity. To understand this assump• worlds in (8); and the optimal value function of the meta-
tion, consider the following generative process for selecting MDP For a number of sampled worlds m polynomial in 
worlds: first, the number of objects is chosen according to                           the error is bounded by: 
      then, the classes and links of each object are cho•
sen according to Using this decomposition, we 
have that The intuitive assump•
                                                       with probability at least 1 
tion described above can be formalized as: 
                                                       where 
         for some Thus, the distribution               is the maximum per-object reward. 
over number of objects can be chosen arbitrarily, as long as it 
is bounded by some exponentially decaying function.    The proof, which is omitted for lack of space (see online ver•
                                                       sion of this paper), uses some of the techniques developed by 
5.2 Sampling worlds                                    de Farias and Van Roy [2] for analyzing constraint sampling 
The main problem with this formulation is that the size of in general MDPs. However, there are two important differ•
the LP — the size of the objective and the number of con• ences: First, our analysis includes the error introduced when 
straints — grows with the number of worlds, which, in most sampling the objective, which in our case is a sum only over 
situations, grows exponentially with the number of possible a subset of the worlds rather than over all of them as in the 
objects, or may even be infinite. A practical approach to ad• LP for the full meta-MDP. This issue was not previously ad•
dress this problem is to sample some reasonable number of dressed. Second, the algorithm of de Farias and Van Roy re•
worlds from the distribution and then to solve the LP  lies on the assumption that constraints are sampled according 


PROBABILISTIC PLANNING                                                                                1007 