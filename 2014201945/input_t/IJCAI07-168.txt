      Transfer Learning in Real-Time Strategy Games Using Hybrid CBR/RL

    Manu Sharma, Michael Holmes, Juan Santamaria, Arya Irani, Charles Isbell, Ashwin Ram
                                         College of Computing
                                    Georgia Institute of Technology
                        {manus, mph, jcs, arya, isbell, ashwin} @cc.gatech.edu


                    Abstract                            We introduce a mechanism for achieving transfer learning
                                                      in Real Time Strategy (RTS) games. Our technical contribu-
    The goal of transfer learning is to use the knowl- tions are:
    edge acquired in a set of source tasks to improve
                                                        • A novel multi-layered architecture named CAse Based
    performance in a related but previously unseen
                                                          Reinforcement Learner (CARL). It allows us to capture
    target task. In this paper, we present a multi-
                                                          and separate the strategic and tactical aspects of our do-
    layered architecture named CAse-Based Reinforce-
                                                          main, and to leverage the modular nature of transfer
    ment Learner (CARL). It uses a novel combina-
                                                          learning.
    tion of Case-Based Reasoning (CBR) and Rein-
    forcement Learning (RL) to achieve transfer while   • A novel combination of Case-Based Reasoning (CBR)
    playing against the Game AI across a variety of       and Reinforcement Learning (RL). In particular, we use
    scenarios in MadRTSTM, a commercial Real Time         CBR as an instance-based state function approximator
    Strategy game. Our experiments demonstrate that       for RL, and RL as a temporal-difference based revision
    CARL not only performs well on individual tasks       algorithm for CBR.
    but also exhibits signiﬁcant performance gains      In the next section, we brieﬂy introduce and motivate the
    when allowed to transfer knowledge from previous  choice of RTS games as an application domain. Section 3
    tasks.                                            details the hybrid CBR/RL approach for transfer learning.
                                                      We then provide detailed experimental analyses demonstrat-
                                                      ing validity of the proposed techniques in Section 4. Finally,
1  Transfer Learning                                  we describe related work before ending with a discussion of
Transferring knowledge from previous experiences to new recommendations for future research.
circumstances is a fundamental human capability. In general,
the AI community has focused its attention on generalization, 2 Real Time Strategy Games
the ability to learn from example instances from a task and RTS games are a family of computer games that involve sev-
then to interpolate or extrapolate to unseen instances of the eral players competing in real time. Typically, the goal is to
same task. Recently, there has been growing interest in what kill all the other players in the game or to dominate most of
has come to be known as transfer learning. Transfer learn- the territories of the game.
ing is somewhat like generalization across tasks; that is, after RTS games are very popular, and have been created by
seeing example instances from a function or task, the transfer non-AI developers for a non-AI community. Therefore, RTS
learning agent is able to show performance gains when learn- games have not been designed as synthetic domains where
ing from instances of a different task or function. For exam- AI techniques can easily exhibit superior behaviors. These
ple, one might imagine that learning how to play checkers games are interesting as they are characterized by enormous
should allow one to learn to play chess, a related but unseen state spaces, large decision spaces, and asynchronous inter-
game, faster.                                         actions [Aha et al., 2005]. RTS games also require reasoning
  One particular taxonomy of transfer learning has been pro- at several levels of granularity, production-economic facility
posed by DARPA [2005]. There are eleven “levels”, ranging (usually expressed as resource management and technologi-
from the simplest type of transfer (Level 0, or Memorization) cal development) and tactical skills necessary for combat con-
to the most complex form of transfer (Level 10, or Differ- frontation. RTS games are even used in real-life applications
ing). The levels measure complexity of the transfer problem such as military training systems.
in terms of speed, quality, reorganization, and scale. Of note The particular RTS game that we used in our research is
is that the levels emphasize characteristics of the types of MadRTSTM, a commercial RTS game being developed for
problems to which the transfer applies, rather than the rep- military simulations. Figure 1 shows a source task (left) and
resentations or semantic content of the knowledge needed for target task (right) for achieving Level 4 Transfer (“Extend-
solutions. See Table 1 for the ﬁrst ﬁve transfer levels. ing”) in MadRTS. The experimental results presented in this

                                                IJCAI-07
                                                  1041 Transfer Level                                          Description
 0. Memorization    New problem instances are identical to those previously encountered during training. The new
                    problems are solved more rapidly because learning has occurred.
 1. Parameterization New problem instances have identical constraints as Memorization, but have different parameter val-
                    ues chosen to ensure that the quantitative differences do not require qualitatively different solutions.
 2. Extrapolating   New problem instances have identical constraints as Memorization, but have different parameter
                    values that may cause qualitatively different solutions to arise.
 3. Restructuring   New problem instances involve the same sets of components but in different conﬁgurations from
                    those previously encountered during training.
 4. Extending       New problem instances involve a greater number of components than those encountered during
                    training, but are chosen from the same sets of components.

                      Table 1: Transfer Learning Levels 0-4 from a taxonomy of eleven levels.


Figure 1: A source task (left) and target task (right) for exhibiting Transfer Learning Level 4. This example is simpler than our
actual experiments; however, it demonstrates the same changes in scale that characterizes “Extending”. In our experiments,
both the number of territories and the number of troops is increased—from three and ﬁve, respectively, in the source task to ﬁve
and eleven in the target task—necessitating a qualitatively different set of tactical decisions.

paper were obtained with much larger maps and with higher in Figure 2. Each layer includes these three representative
numbers of troops, buildings and territories.         modules:
                                                        • The planner makes decisions about action selection at
3  Approach                                               that layer based on the current state.
Because we are particularly interested in applying transfer • The controller is a reactive module that provides percep-
learning to RTS games, we have designed an architecture   tion/action interfaces to the layer below.
suited to that purpose. In particular, we have implemented
a multi-layered architecture that allows us to learn at both • The learner modiﬁes the representation used by the
strategic and tactical levels of abstraction. In this section, we planner in situ, splitting and merging actions, updating
describe in detail that architecture as well as our approach to features about the actions and states, and so on.
case-based reasoning and reinforcement learning.        For our experiments, we have deﬁned three levels. The
                                                      top-most strategic level is currently rather simple (based
3.1  System Architecture                              on a hand-coded strategy). The middle tactical layer uses
CAse-Based Reinforcement Learner (CARL) is realized as a hybrid Case-Based Reasoner and Reinforcement Learner.
a multi-layered architecture similar in spirit to other popular Speciﬁcally, this layer makes tactical decisions over an ac-
multi-tiered architectures [Albus, 1991]. Upper layers rea- tion space of Attack, Explore, Retreat and Conquer Nearest
son about strategy while lower levels are concerned with ever Territory. The lowest layer incorporates a reactive planner,
ﬁner distinctions of tactics. The granularity and time scale of scripted to perform the tasks in a predeﬁned manner speciﬁc
the actions generally reduces in the lower layers. Actions for to MadRTSTM. The lower layer responds to the tasks del-
an upper layer Ai are treated as goals by the layer immedi- egated from the tactical layer as if they were goals for the
ately below. Each layer is conceptually identical, as shown reactive planner.

                                                IJCAI-07
                                                  1042       State Feature                                  Description
        AvgHealth    Average health of the agent’s troops that are currently alive
           %Sp       Agent’s alive troops as a percentage of their initial strength
           %So       Opponent’s troops killed as a percentage of their initial strength
           %Tp       Territories owned by the agent as a percentage of the maximum available in the scenario
           %To       Territories owned by the opponent as a percentage of the maximum available in the scenario

Table 2: Features representing state of the game at the Tactical layer of CARL. At any instance of time, a territory can be either
owned by the agent, the opponent or neither of the two playing sides. A side owns a territory if it sends a constant number of
troops to that territory and leaves behind a smaller ﬁxed number of troops for patrolling.

                                                      rithms use this experience to learn value functions (or Q-
                                                      values) that map state-action pairs to the maximal expected
                                                      sum of reward that can be achieved starting from that state-
                                                      action pair. The learned value function is used to choose ac-
                                                      tions stochastically, so that in each state, actions with higher
                                                      value are chosen with higher probability. In addition, many
                                                      RL algorithms use some form of function approximation (rep-
                                                      resentations of complex value functions) both to map state-
                                                      action features to their values and to map states to distribu-
                                                      tions over actions (i.e., the policy). See [Sutton and Barto,
                                                      1998] for an extensive introduction to RL.
                                                        Case-Based Reasoning (CBR) provides an approach to
                                                      tackling unseen but related problems based on past experi-
                                                      ence [Kolodner, 1993; Leake, 1996]. It has been formalized
                                                      into a four-step process: Retrieve, Reuse, Revise and Retain
                                                      [Aamodt and Plaza, 1994]. One view of this work is that it
                                                      uses CBR to perform online lazy learning of a value function
                                                      using RL techniques to solve temporal credit assignment.
                                                      Case Representation
                                                      Individual cases must represent information about a region in
                                                      the generalized state space and provide utilities for executing
                                                      different actions in that region of the state space. In general,
                                                      the ith case, Ci, is represented as a tuple of four objects
Figure 2: A representative layer in our Transfer Learning ar-         Ci =<Si,Ai,Qi,ei  >
chitecture CARL. The architecture contains several of these
levels that are conceptually similar; the planner, learner, and where
controller are instantiated in different ways, depending upon • State Si is a vector composed of features representing
the needs of that level.                                  game state that the system has already experienced.
                                                        • Action set Ai is a list of the possible actions the agent
3.2  Case-Based Reinforcement Learning                    can take at that level in the architecture.
                                                        • Utilities Qi is a vector of utilities for each action in Ai
Reinforcement Learning (RL) is one natural approach for
                                                          available for the situation described by Si.
building interactive agents. In RL, problems of decision-
making by agents interacting with uncertain environments • Eligibility ei is a real number reﬂecting the cumulative
are usually modeled as Markov Decision Processes (MDPs).  contribution of the case in previous time steps. Eligibil-
In the MDP framework, at each time step the agent senses  ities are used to support TD(λ) updates during revision.
the state of the environment, and chooses and executes an (0 ≤ ei ≤ 1)
action from the set of actions available to it in that state. It is important to note that the agent need not have per-
The agent’s action (and perhaps other uncontrolled external formed a particular action on a particular state for the corre-
events) causes a stochastic change in the state of the environ- sponding cases to provide suggestions for carrying out those
ment. The agent receives a (possibly zero) scalar reward from actions, i.e. ∀Ci ∈ Case Library, ∀aij ∈ Ai, ∃qij where j =
the environment. The agent’s goal is to develop an optimal 1 ...N and N = |Ai|.
policy that chooses actions so as to maximize the expected Table 2 enumerates the features used in the experiments re-
sum of rewards over some time horizon.                ported here. The features are chosen such that they represent
  Many RL algorithms have been developed for learning the temporal changes in the behavior of the different sides in
good approximations to an optimal policy from the agent’s the game. Note that each feature is represented as a numeric
                                                                              5
experience in its environment. At a high level, most algo- value, so in our case Ci ∈ R .

                                                IJCAI-07
                                                  1043Retrieval                                             4   Evaluation and Analysis
Because our features are numeric, we can use Euclidean
                                                      We evaluate our hybrid CBR/RL transfer learning agent in the
distance as our similarity metric. k-NN(Sq)={Ci  ∈
                                                      MadRTSTMgame domain. Our evaluation focuses on examin-
Case Library | di ≤ τk},whereSq is the queried state, di is
                                                      ing the hypothesis that the agent should do well with learning
the similarity (Euclidean distance) of the state knowledge of
                                                      a given task in MadRTS, and should also be able to learn bet-
every case Ci with queried state Sq and τk is a task-dependent
                                                      ter across tasks by transferring experience.
smoothing parameter. A Gaussian kernel function (K(di) =
      2  2                                              To quantify the learning that is being done by the agent on
exp(−di /τk )) determines the relative contribution of individ-
ual cases. The utilities for a queried state is the weighted a single task, we compute a performance metric at the end of
average of the utilities of the retrieved cases:      each completion of a single game on a particular map. The
                                                      metric is computed as a linear combination of ﬁve features:
                                                     average health of the remaining agent troops, time elapsed in
  ˆ                              K(di)
  Qt(Sq,aij )=                               uij     the trial, enemy troops killed, agent’s troops alive and terri-
                                          ( h)
                             ∀C ∈k  (Sq ) K d
              ∀Ci ∈ k-NN(Sq)   h  -NN                 tories owned by the agent as a percentage of their respective
                                                      initial strengths. Given the learning curve T, representing the
Reuse
                                                      performance of the agent when acting directly on the target
The planner chooses the action with the highest expected util- task and the transfer learning curve T/S, representing the per-
ity with a probability of 1 − . To encourage exploration,
                                                    formance of the agent after transferring the knowledge from
begins as a large value but decreases monotonically as a func- the source task, we use the following three metrics to deter-
tion of the game trials and the number of decisions made by mine the quality of transfer learning:
the case-based planner during those trials.
  The action is passed as a goal for the lower layer to achieve. • Jump Start - Difference in the mean value of the ﬁrst ten
As noted earlier, the lowest layer is a scripted reactive planner data points (trials) of T from the mean value of the ﬁrst
that commands a series of primitive actions that directly af- ten data points of T/S
fect the MadRTS game engine. For example, given the tactic • Asymptotic Gain - Difference in the mean value of the
explore, the lower planner would ﬁnd a subset of idle troops
                                                          last ten data points (trials) of T from the mean value of
and instruct them to carry out a set of exploratory actions as
                                                          the ﬁrst ten data points of T/S
deﬁned by the game engine’s instructions.
                                                        • Overall Gain - Difference in the mean value of the all
Revision
                                                          the data points (trials) of T from the mean value of all
Our revision phase uses RL-style updates to the utilities Qi the data points of T/S
for actions Ai chosen by the agent. In particular, a tempo-
ral difference (λ) [Sutton, 1988] update ensures that previous These, in fact, measure different parameters about the
errors are incorporated in future decisions.          transfer. For example, it is possible for two curves to have
                   ˆ      ˆ                           the same overall gain but for the initial advantage of transfer
Qij  ←  α(rt+1 + γQt+1 − Qt)ei ∀Ci  ∈ Case Library
                                                      (Jump Start) to be vastly different.
      ˆ        ˆ
where Qt+1 and Qt are the utilities of the actions chosen by
the decision cycle of the agent for state representation Si at 4.1 Experiments
time instances t +1and t, respectively; rt+1 is a reward; ei
                                                      In this section, we present the results of transfer at levels 3
is an eligibility trace; α is the gradient-descent based utility
                                                      and 4. All our experiments are carried out with large 64 × 64
update factor; and constant γ is the Q-learning discount rate.
The reward achieved in each decision cycle is a linear combi- tile maps, with at least six territories to conquer and at least
                                                      nine troops ﬁghting from each side. To exhibit transfer learn-
nation of temporal changes in the state features. Eligibilities
                                                      ing at level 4, the target scenario consisted of an increase in
represent the cumulative contribution of individual state and
action combination in the previous time steps. After factor- the map components (number of territories, enemy troops and
                                                      buildings) of at least 1.5. The experimental setup for testing
ing in , the exploration factor, the ﬁnal action selected by the
                                                      the transfer learning at level 3 involves mirroring the initial
decision cycle has its eligibility increased in those cases that
                                                      ownership of territories when moving from source scenario
combined to form the k-NN. This helps perform correspond-
                                                      to target scenario. Part of the difﬁculty is that whereas in
ing utility updates based on the case’s past contributions:
                                                     the source scenario, the agent’s troops start with initial posi-
              P     K(di)
             γ            K(dh)  the chosen action    tions in adjacent territories (large army), in the target task, the
     ei ←       ∀Ch∈ k-NN(Sq )
             λγei                otherwise            new territories are separated by a signiﬁcant distance coupled
                                                      with geographical obstructions. This forces the agent to de-
                                                      velop new sequences of tactics: forces are now two smaller
The remainder of the cases in the library receive a scaling battalions of troops rather than one large army.
down for eligibilities of all their actions by a factor of λγ.
                                                        A trial terminates only after all the agent’s troops are dead
Retention                                             or all the opponent troops are dead and the remaining agent
New cases are added to the case library only if the distance of troops have conquered the maximum number of territories
the closest neighbor dmin is larger than the threshold param- possible (given their current strength). Conquering a territory
eter τd. Thus, τd acts as both a smoothing parameter and as a requires that a troops need to reach the territory, kill any oppo-
mechanism for controlling the density of cases in memory. nent troops, and leave behind a minimum number of troops.

                                                IJCAI-07
                                                  1044                                                               Figure 4: Transfer Learning on Level 3.
Figure 3: Transfer Learning on Level 4. T, S and T/S are
the respective performance of the learning curves obtained as  TL Metric     TL Level 4  TL Level 3
average over ﬁve iterations of the agent solving Target sce-                        ∗           ∗
nario alone, Source scenario alone and Target scenario after   Jump Start      2.608       1.995
                                                            Asymptotic Gain    0.870∗      -0.004
transfer of knowledge over the Source scenario. Trend(*) is a                       ∗           ∗
smooth ﬁtting of the points in the curve *.                   Overall Gain     1.118       0.594
                                                      Table 3: Results on Transfer Learning Performance Metrics
  In these experiments, we use the value of the Q-Learning for Levels 4 and 3. ∗ indicates that the results are statistically
discount rate constant γ = 0.9, the Temporal Difference con- signiﬁcant with a conﬁdence of at least 98%.
stant λ = 0.6, the exploration factor  = 0.7, gradient-descent
based utility update factor α = 0.6, and number of nearest
neighbors k = 5. The task dependent smoothing parameter ﬁdence of the overall gain for transfer learning level 3 and 4 is
                                                      99.95%. All the resultant values, but one, have been veriﬁed
Tk is maintained larger than the density parameter Td,en-
abling several cases to contribute to the value of the query to be at conﬁdence of at least 98%. The only exception is the
point while, at the same time, ensuring that large portions of asymptotic gain of Level 3, suggesting that curves T and T/S
the domain are covered with fewer cases. We analyze the have converged.
agent’s performance by averaging over ﬁve independent iter- The results show effective jump start transfer learning. In
ations. Each task was executed for a hundred trials with the Level 4 this appears to be because the agent learns when
worst case running time of 60 seconds per trial.      to exploit the attack tactic. This gave the agent a valuable
                                                      method for performing well immediately, even for the in-
4.2  Results                                          creased number of enemy troops in the target scenario. Fur-
                                                      ther, reusing the conquering nearest territory tactic from the
The results are shown in Figure 3 for TL level 4 and in Fig- existing case library worked well despite the larger number of
ure 4 for TL level 3. Curves T, S and T/S represent the territories. In case of TL level 3, the agent beneﬁts from the
agent’s performance as a function of the completed game tri- learned use of the explore tactic. This proved useful with the
als with learning only on the target scenario; learning only player’s troops starting as two geographically separated bat-
on the source scenario; and learning on the target scenario af- talions. When acting without prior learning, the agent takes
ter learning on the source scenario, respectively. For clarity, more trials to discover the use of that exploration tactic to
we also show trend lines that are smooth ﬁts for each curve. unite smaller forces.
Both Trend(S) and Trend(T) show a continual increase in per-
formance with S appearing easier than T. The vertical axis
in Figures 3 and 4 is the performance of the agent (P) mea- 5 Related Work
sured at the end of each trial (a game of just Source or Target There has been an increasing interest in transfer learning
task): P =2∗  AvgHealth  +2∗   %So +3∗   %Sp +3∗      recently. For example, given a task hierarchy, Mehta et
%Tp +50/T imeElapsed   (see Table 2). TimeElapsed is a al. [2005] transfer value functions within a family of variable-
quantity measured as a function of the time scale in the RTS reward Markov Decision Process (MDPs). Their primary
game. Completion of each task is guaranteed to take more objective was speedup learning, and demonstrated on a dis-
than 50 time units. Hence, the best that anyone (our agent, cretized version of an RTS game. Rosenstein et al. [2005]
a hard-coded agent, random player) can do in each trial is a investigate whether transfer should be attempted when there
performance of 11.                                    is no prior guarantee that the source and the target tasks are
  Both Figures 3 and 4 show that T/S has a signiﬁcant jump sufﬁciently similar. To the best of our knowledge, there are
start and performs much better overall than T. Table 3 sum- no existing case-based reasoning approaches that address the
marizes all of the metrics. Using the signed rank test, the con- problem of transfer learning.

                                                IJCAI-07
                                                  1045