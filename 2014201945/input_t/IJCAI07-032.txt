 An Information-Theoretic Analysis of Memory Bounds in a Distributed Resource
                                      Allocation Mechanism

                                Ricardo M. Araujo    and  Luis C. Lamb
                  Instituto de Informatica,´ Universidade Federal do Rio Grande do Sul
                                    91501-970, Porto Alegre, Brazil
                              rmaraujo@inf.ufrgs.br, LuisLamb@acm.org

                    Abstract                          to be much smaller than that of analyzing multiple outcomes
                                                      produced by individual agents. More importantly, such a cost
    Multiagent distributed resource allocation requires does not depend on the number of agents, and its computation
    that agents act on limited, localized information may be feasible even without knowledge of such number.
    with minimum communication overhead in order        In repeated dispersion games, agents may learn from a se-
    to optimize the distribution of available resources. ries of past outcomes in order to improve their decisions. A
    When requirements and constraints are dynamic,    natural issue is how large should the observation window be.
    learning agents may be needed to allow for adap-  While bounds in agents’ memory or observation capabilities
    tation. One way of accomplishing learning is to   are often assumed [14], there is a paucity of explanation about
    observe past outcomes, using such information to  how such limits are established. In this paper we tackle this
    improve future decisions. When limits in agents’  problem in a specialized version of a dispersion game known
    memory or observation capabilities are assumed,   as the Minority Game (MG) [4]. In the MG, a ﬁnite number of
    one must decide on how large should the obser-    agents decide over a (also ﬁnite) set of actions and those who
    vation window be. We investigate how this de-     choose the action chosen by the smallest number of agents are
    cision inﬂuences both agents’ and system’s per-   rewarded. When the game is repeated, agents have the chance
    formance in the context of a special class of dis- to learn from previous rounds and improve their performance
    tributed resource allocation problems, namely dis- by considering a ﬁnite window of past outcomes (they have
    persion games. We show  by numerical experi-      ﬁnite memory) in order to try and learn temporal patterns.
    ments over a speciﬁc dispersion game (the Minority  Our contribution is twofold. First, we systematically simu-
    Game) that in such scenario an agent’s performance late cases in the MG composed of agents with different mem-
    is non-monotonically correlated with her memory   ory sizes and compare their performances to a ﬁxed class of
    size when all other agents are kept unchanged. We (other) agents with ﬁxed memory sizes, using two different
    then provide an information-theoretic explanation learning algorithms. The results show a clear non-monotonic
    for the observed behaviors, showing that a down-  relation between access to information and agent’s perfor-
    ward causation effect takes place.                mance, unveiling cases where more information translates
                                                      into higher gains and cases where it is harmful. Further, we
1  Introduction                                       relate those ﬁndings to the system’s global efﬁciency state.
An important class of natural problems involving distributed Second, we provide an information-theoretic explanation to
resource allocation mechanisms may be modeled by disper- the observed behaviors. We show that the size of the gen-
sion games [9]. Problems in this class are of anti-coordination erated outcome patterns does not always match the system’s
nature: agents prefer actions that are not taken by other memory size, thus allowing agents with larger memory sizes
agents. Stock exchange and load balancing in networks are to exploit this information.
instances of such problems [8]. In iterated versions of such The remainder of the paper is organized as follow. We start
games learning is an important component of the decision- by discussing relevant related work. We then present the MG
making process. While each agent may try and learn individ- model and terminology. Next, we detail our methodology
ual behaviors of all agents, this is a complex task especially and the results of the experimental studies using two different
when the total number of agents in the system is unknown, learning algorithms. Finally, we analyze the results and point
not constant or very large. A possible solution is to consider out directions for future research.
other agents’ actions as indistinguishable from the environ-
ment - an idea behind the so-called 0-level agents [17] which, 2 Related Work
for some scenarios, yields the best results [10]. In such cases, While previous works have focused on problems relating ho-
only the joint actions of all agents are considered and they mogeneous memory size to global system performance, little
may be represented as a single global outcome. The computa- effort has been put into understanding the relation between
tional cost of analyzing one single global outcome is expected individual memory size and individual agent’s performance.

                                                IJCAI-07
                                                   212Homogeneous agents involved in self-play are typically as- agents were modeled using a simple inductive learning algo-
sumed, with emphasis in system dynamics rather than indi- rithm to make decisions, corresponding to a simpliﬁed model
vidual agents. However, as argued in [15], a more appropriate of how human decisions in complex environments are actu-
agenda for multi-agent learning “asks what the best learning ally made [3]. Since this algorithm has been widely used and
strategy is for a ﬁxed class of the other agents in the game”, studied in other applications, it is of our interest to make use
as this allows the construction of effective agents. We fol- of it in a straightforward way (we refer to it as the traditional
low this methodology here. In the context of the MG, little learning algorithm). In this algorithm, each agent has ac-
work so far has been directed to and concerned with indi- cess to the m last outcomes of the game and she is given
vidual agent’s performance. An exception to that is [16], in a set S of hypotheses, randomly selected from a ﬁxed hy-
which a genetic algorithmic-based agent is shown to outper- potheses space H. A hypothesis maps all possible patterns
form traditional ones, but little investigation about the condi- of past outcomes over m to a decision (i.e. what group to
tions on why this happens is done. Other learning algorithms choose in the next round). m is known as the memory size

have been applied to the MG [1, 2, 5]; however, homogeneous of the game. A hypothesis is effectivelym represented by a bi-
agents are assumed with focus on global properties.   nary string of length 2m and |H| =22 . The learning al-
  Heterogeneous memory sizes are studied in [11], where an gorithm keeps a measure fi,j(t) of the effectiveness of each
evolutionary setup is used to search for the best memory size hypothesis j held by agent i in time t, which is updated by
for the game; however, they do not ﬁx a class of agents, al- fi,j (t)=fi,j(t − 1) + aisign(A(t − 1)). This measure is
lowing every agent to evolve independently. A similar setup often called virtual points. An agent then uses her hypothesis
is used in [5], but their concern is average memory size. We with the maximum number of virtual points argmaxjfi,j (t)
shall provide alternative explanation for some of their results. to commit to a decision in the next round of the game. Ties
In [6] it is stated that agents with larger memories perform are broken by coin toss. This algorithm is very simple and
better than agents with smaller ones. Nonetheless, they con- provides more adaptation than learning, since agents are un-
sider a varying number of agents endowed with only one ex- able to better explore strategies outside their initial set. In or-
tra bit of memory; we will show that having larger memories der to overcome the limitations of this algorithm and to allow
may not lead to better performance.                   for comparisons with a different learning strategy, we shall
                                                      introduce a new evolutionary-based learning algorithm.
3  A multiagent market scenario
                                                      4   Methodology and Experiments
The Minority Game (MG) was initially proposed in the con-
text of Econophysics, as a formalization of the El Farol Bar Due to the nature of our interest in the model, we consider
Problem [3]. In these models, the main interest is the type of agents with different hypotheses space sizes (i.e. different
                                                      memory sizes), in order to observe the effects of such dif-
information structure created by multiple interacting bounded                    1
rational agents. In the MG, there are two groups and an ferences in agents’ performance . We split the MG into two
odd number (N) of agents. At each round, agents must  parts. The environment consists of a traditional MG with a set
choose concurrently and without communication to be in N of agents, as described above (with homogeneous agents).
only one of the two groups. At each round t, after all de- The control group G is a group of agents which sample hy-
                                             ( )=     potheses from a different space from that of the agents in the
cisions are made, a global outcome is calculated: A t                                |  |
  N    ( )        ( )                                 environment, differing only in its size H . Since hypotheses
  i=0 ai t , where ai t is the decision of agent i in round space size in our setup is deﬁned only by each agent’s mem-
t and ai(t) ∈{+1, −1}. Agents’ wealth is distributed ac-
                                                      ory size, we shall denote the environment’s by me and the
cording to wi(t)=wi(t − 1) + ai(t)sign(A(t − 1)), where
    (·) +1    −1              ( − 1)                  control group’s by mg.
sign   is   or   according to A t   being positive or   In this work we assume |G| =1. [6] investigates larger
negative (other functions of A(t) may be used). Initial values
              ( )                                     groups in similar fashion; however in their experiments the
are set to zero. A t encodes which group had the minority of memory size is not systematically modiﬁed and their interest
agents and only those in this group are rewarded, while those is on the effects of changes in group sizes. We will observe
in the majority are penalized. There is no equilibrium point the performance for the resulting control agent when her hy-
in which all agents proﬁt, and the expectation of the majority
                                                      potheses space size mg varies in relation to me. We also
is always frustrated due to the anti-coordination nature of the want to verify how her performance (deﬁned by the average
game: if many believe a particular group will be the minority wealth w ) changes with m and compare it to the environ-
one, then they will actually end up in the majority. In spite g               g
                                                      ment’s performance (deﬁned by the average wealth we of a
of its simplicity, the MG offers a rich and complex behavior random agent in the environment). In order to do so, we mea-
that was shown to represent several scenarios of decentral-
                                                      sure the control agent’s gain: wg/we. All results are averages
ized resource allocation, such as ﬁnancial markets and trafﬁc over 200 randomly initialized games and they have been sta-
commuters scenarios [4]. In [9], this model was generalized tistically tested for signiﬁcance using a standard t-test with
and shown to belong to a broader class of dispersion games. 95% conﬁdence interval. Each game was run for 100,000
  One of the main concerns in the MG is with the game’s rounds and we have used the standard values found in the
global efﬁciency, i.e. the number of agents winning the game. literature for all other parameters, namely: |N| + |G| = 101
From the viewpoint of the system as a whole, one is interested
in maximizing the number of winners, so that resources are 1When dealing with homogeneous agents, hypotheses are drawn
better distributed. When the MG was originally introduced, from the same hypotheses space.

                                                IJCAI-07
                                                   213                                                      Figure 2: Control agent gain for me =3(solid line) and
                                                      me =6(dashed line) with agents using the traditional learn-
Figure 1: Control agent gains for combinations of me and ing algorithm
mg. Lighter shades denote higher gains.

and |S| =2, so comparisons could be easily made 2. We shall
detail experiments using the traditional and the evolutionary
learning algorithms.
4.1  Traditional Learning Algorithm
We start with the traditional learning algorithm in both the
control agent and agents in the environment. In Fig. 1
we show the control agent’s gain for every combination of
me  ∈ [1, 15] and mg ∈ [1, 15]. These ranges may seem
somewhat limited, but are adequate for our purposes. The ex-
ponential increase of hypotheses space with m and the large
number of agents often makes experimentation with larger Figure 3: Control agent gain for varying m and m = m +1
memories intractable. In the resulting topography, we ob-                                e      g    e
serve some interesting behaviors. For me ≤ 3, higher than
unit gains are obtained whenever mg >me. On the other dow is harmful to the agent, except when the environment has
hand, for values immediately above m =3we can observe small memory sizes. To observe such phenomena, we plot the
                                e                                                               =    +1
that whenever mg >me, wg falls below unit, showing that control agent’s gain when me varies and we set mg me .
the target agent does worse by having a larger hypotheses This is shown in Fig. 3, where it is made clear that there is
space. For larger me, wg is always below unit except where a non-monotonic relation between memory size and agent’s
mg = me, when the gain is, as expected, exactly unitary. performance. We observe that gains are high for small values
  Let us now observe in detail what happens in speciﬁc of me, then they become smaller than unit, reaching a mini-
points of the observed regions by sectioning the topography mum and later increasing again to ﬁnally converge to a value
                                                      slightly below unit. Having worse performance when access-
(see Fig. 2). For the ﬁrst case, we take me =3. We can
see that our control agent beneﬁts from a larger hypotheses ing more information may seem counter-intuitive. One could
space. Interestingly, having smaller spaces seem to cause no argue that this is due to the larger hypotheses space, which
                                                      makes ﬁnding a good hypothesis harder. Even though this
harm and the agent performs as if mg = me. We can also
observe that the transition from one of the above cases to an- may be part of the explanation, it does not account for all of it
                                                      since we observe a non-linear relationship between gains and
other is quite sharp and further increases in mg provide no ad-
ditional gains. Instead, a logarithmic decrease of wealth may hypotheses space size, as we shall further investigate.
be observed with further increases in . The highest gain
                                mg                    4.2  Evolutionary learning
occurs precisely where mg = me +1, where a spike may be
observed. As for the second case, we detail the behavior for It could be argued that the observed behavior is only but a pe-
me =6. We observe a logarithmic drop in wg for mg >me. culiar effect of the learning algorithm used, whose main lim-
For mg <me, a small decrease of gains can be observed. itation is the inability to explore the hypotheses space during
Thus, in this region no mg does better than unit and, interest- the game, i.e. an agent may only use the hypotheses given at
ingly, having access to a larger input window is harmful to the the start of a run. In order to address this concern, we have re-
agent. For larger values of me, losses become larger when- peated the experiments using an evolutionary-based learning
ever mg <me. Having access to a larger information win- algorithm so as to allow agents to further explore the hypothe-
                                                      ses space. Some different evolutionary-based algorithms for
  2It is known that larger S does not change the qualitative behav- the MG have been proposed [5, 11, 16]. We chose an adapta-
ior of the system [4]                                 tion of the one proposed in [2], due to its simplicity and good

                                                IJCAI-07
                                                   214  Algorithm Evol-Learning
     w ← 0;
     S ← random hypotheses ∈ H;
     foreach s ∈ S do
        ﬁtness(s) ← 0;
     end
     while not end of the game do
        obs ← window of size m of past outcomes;
        h ← arg maxs ﬁtness(s);
        decision ← decision of h using observation obs;
        commit(decision);
        if decision = outcome then w ← w+1;
        foreach s ∈ S do
           if decision of s using obs = outcome then
               ﬁtness(s) ← ﬁtness(s) + 1;             Figure 5: A(t) for agents using evolutionary-based algorithm
           else
               ﬁtness(s) ← ﬁtness(s) - 1;
           end
        end
        with probability pr do
           worst hypothesis s ← best hypothesis k;
           with probability pm, ﬂip bits in s;
        end
     end
  end
       Figure 4: Evolutionary Learning Algorithm

performance. While we could have applied a more elaborated
algorithm used in similar games, such as [7], we have chosen
to create an algorithm that is closely related to the traditional
one, but presenting better learning characteristics. By doing
so we are able to better understand and compare results ob- Figure 6: Control agent gains for combinations of me and
tained using both algorithms.                         mg. Lighter shades denote higher gains.
  In this algorithm (depicted in Fig. 4), each agent starts
with S hypotheses and, at every round, with probability pr, As with the traditional learning algorithm, for mg <me no
she discards her worst performing hypothesis and replaces considerable losses or gains are observed.
it with a copy of her best performing one. Each bit of this
copy is then ﬂipped with probability pm. This allows agents 5 Analyzing the Results
to search for better hypotheses, continually introducing new
ones to the game. It is interesting to note that, differently 5.1 Dynamics of the game and its efﬁciency regions
from other evolutionary learning algorithms such as [16], this As stated above, one of the main concerns with the MG is its
one does not require a global ordering of agents based on their global efﬁciency, i.e. how many agents are winning the game
performances. Agents retain their autonomous characteristics during a run. A typical way of measuring the temporal efﬁ-
by not relying on a central authority to decide which hypothe- ciency of resource distribution is by means of the statistical
ses among all agents are to be replaced, thus preserving the variance σ2 of A(t) [4]. The larger the variance is, the larger
distributed nature of the game. Figure 5 shows A(t) for a the waste of resources, making the system less efﬁcient. The
typical run when agents are using the new proposed learning variance σ2 is a function only of the number of agents in the
algorithm and pr =0.01 and pm =0.001. Clearly, learn- game and their (homogeneous) memory size [13]. Since we
ing is taking place (when using the traditional algorithm, no keep the number of agents ﬁxed and the control group is uni-
                                                                       2
decrease in oscillations is observed, even for very long runs). tary, we consider σ only as a function of me, the memory
For all experiments using this algorithm we consider only re- size of agents within the environment. Figure 8 shows σ2 as
sults after 2,000 rounds, in order to observe the “steady” state a function of me when all agents use the traditional learning
of the system. Figure 6 shows the control agent’s gain for algorithm. The same plot is observed for any mg. Three re-
every combination of me ∈ [1, 15] and mg ∈ [1, 15] when gions of efﬁciency can be observed. For small me, high vari-
agents use the evolutionary-based learning algorithm. We ance is observed, thus low efﬁciency characterize the system.
now observe a different behavior when compared to our pre- For large me, the system has precisely the variance expected
vious case. There are no regions where larger memories are if all agents were deciding randomly (i.e. the Random Case
beneﬁcial, each extra bit beyond the environment’s memory Game - RCG). Intermediate values of me are correlated with
size is harmful to the target agent. A logarithmic decrease smaller, better than random, variances. This last case is often
with mg is observed for mg >me, for all tested values of the main subject of interest in the MG, since it indicates that
me. In Fig. 7 we detail the behavior for some values of me. agents are able to self-organize to improve efﬁciency.

                                                IJCAI-07
                                                   215                                                               2
Figure 7: Target agent’s gain versus mg for me =3(solid Figure 8: σ as a function of me for the traditional (solid line)
line) and me =6(dashed line), using the evolutionary learn- and the evolutionary algorithm (dashed line)
ing algorithm
                                                      have measured this information by the conditional probability
  Comparing Fig. 8 with the plotted gain using the traditional P (1|μ ) - the probability of having a “1” following a binary
algorithm (Fig. 3), we observe that higher than unit gains are k
                                                      sequence μk of length k. They have shown that the inefﬁ-
associated with regions with high variances, gains below unit cient region is actually informationally efﬁcient, in the sense
are associated with regions with small variance and increases
                                                      that P (1|μk) is exactly 0.5 for all μk whenever all agents
in gains follow increases in variance. This indicates a cor- have memory of size k. On the other hand, in the efﬁcient
relation between system’s global efﬁciency and exploitation region there is an asymmetry in the probability distribution
possibilities of individual agents, where larger memories are
                                                      and there are μk that offer predictive power. The informa-
beneﬁcial when the system is behaving inefﬁciently (worse tion analysis in [13] focused in cases where k = m, i.e.
than random) whereas when it is efﬁcient having larger mem- the measurement of information available to agents with the
ories becomes harmful. The same behavior is observed in the same memory size as their peers. We wish to access the in-
evolutionary setup. We can also observe in Fig. 8 that the formation available within k = m, since it represents our
variance curve is quite different from the one using the tradi- target agent with different memory sizes. In order to do so,
tional algorithm - there is a smooth transition between a very we have to deﬁne a more precise information measure. To
low variance and the expected for the RCG. Since there is no measure the asymmetry in the distribution of the predictive
inefﬁcient region we would expect, following analogous rea- power of each hypothesis, we use the concept of mutual in-
soning, that no agent with mg >me would perform better
                                                      formation [12] between a string μk and its outcome, comput-
than the agents in the environment and this was actually ob- ing the average information contained in each string: I(k)=
                                                                                                   
served (see Fig. 6). Such results are interesting, since they re- 1       p1(μk)                 p0(μk)
                                                              p1(μ )log2          +  p0(μ )log2         ,
late the efﬁciency of the system as a whole with individual ex- |H| μk k pu(μk)p1        k      pu(μk)p0
ploitation possibility (by means of larger memories) and indi- where p1 and p0 are the probabilities of a “1” or a “0” occur-
cate the existence of stability points in the system’s efﬁciency ring, respectively; p1(μk) and p0(μk) are the probabilities of
regions. For instance, if we take an evolutionary version of “1” or a “0” immediately occurring after a string μk ∈ H, re-
the game where all agents start with a small memory and are spectively; pu(μk)=p1(μk)+p0(μk). In the above equation,
allowed to increase or decrease their memory size during a I(k) is zero whenever the probabilities of observing “1” or
run, we would expect that there would be an initial incentive “0” are the same for all μk. The highest the value of I(k), the
towards larger memories, leading the system towards higher highest the average asymmetry of probabilities, indicating the
efﬁciency points. However, such incentive would stop when existence of hypotheses with predictive power. We have mea-
the system reaches an efﬁcient point as larger memories then sured the information by executing runs of 200,000 rounds
become harmful. Thus, this efﬁcient point would be a stabil- and recording each outcome, resulting in a binary string of
ity point. Such experiment was conducted in [5] where initial 200,000 symbols. Mutual information is then measured over
memory growth was observed, halting at the predicted mem- this string and averaged over 50 independent runs.
ory size. Such behavior was attributed to the simple nature of We are now in position to measure the information avail-
the game. Here we propose a more detailed, distinctive expla- able for some hypotheses of different lengths when agents
nation relating individual agent gain to the efﬁciency region are using the traditional learning algorithm. For each value of
of the system. This is a case of downward causation, where me, we let k assume values below, equal to, and above me.
game dynamics are initially ﬁxed by agents but such dynam- Figure 9 shows results for me =3and me =6(the same val-
ics end up ﬁxing possible (or proﬁtable) agents’ behaviors. ues detailed in the previous sections). We observe that I(k)
                                                      for m  =3remains close to zero for k ≤ m . Information
5.2  Mutual Information                                    e                                e
                                                      available to memory sizes below me is almost the same as for
In [13] homogeneous memory sizes and the traditional learn- me, which explains why agents with smaller memories do not
ing algorithm are used to show that there is different informa- present lower gains in Fig. 2. For k = me +1we observe
tion available to the agents for different memory sizes. They a substantial increase in the available information and further

                                                IJCAI-07
                                                   216