         Learning Strategies for Open-Domain Natural Language Question 
                                        Answering 
       
        
                          Eugene Grois         David C. Wilkins 
         Department of Computer Science and Beckman Institute for Advanced Science and 
                                         Technology 
                           University of Illinois at Urbana-Champaign 
                                   {e-grois, dcw}@uiuc.edu 
       
       
                                                  Several recent systems have specifically addressed the 
                  Abstract                      task of story comprehension.  The Deep Read reading 
  We present an approach to automatically learning comprehension system [Hirschman et al., 1999] uses a 
  strategies for natural language question answering statistical bag-of-words approach, matching the question 
  from examples composed of textual sources,    with the lexically most similar sentence in the story.  Quarc 
  questions, and answers.  Our approach formulates [Riloff and Thelen, 2000] utilizes manually generated rules 
  QA as a problem of first order inference over a that selects a sentence deemed to contain the answer based 
  suitably expressive, learned representation.  This on a combination of syntactic similarity and semantic 
  framework draws on prior work in learning action correspondence (i.e., semantic categories of nouns).  The 
  and problem-solving strategies, as well as relational Brown University statistical language processing class 
  learning methods.  We describe the design of a project systems [Charniak, et al., 2000] combine the use of 
  system implementing this model in the framework of manually generated rules with statistical techniques such as 
  natural language question answering for story bag-of-words and bag-of-verb matching, as well as deeper 
  comprehension.  Finally, we compare our approach semantic analysis of nouns.  As a rule, these three systems 
  to three prior systems, and present experimental are effective at identifying the sentence containing the 
  results demonstrating the efficacy of our model.  correct answer as long as the answer is explicit and 
                                                contained entirely in that sentence.  They find it difficult, 
1  Introduction                                 however, to deal with semantic alternations of even 
                                                moderate complexity.  They also do not address situations 
This paper presents an approach to automatically learning 
                                                where answers are split across multiple sentences, or those 
strategies for natural language question answering from 
                                                requiring complex inference. 
examples composed of textual sources, questions, and 
                                                  Our framework, called QABLe (Question-Answering 
answers.  Our approach is focused on one specific type of 
                                                Behavior Learner), draws on prior work in learning action 
text-based question answering known as story 
                                                and problem-solving strategies [Tadepalli and Natarajan, 
comprehension.  Most TREC-style QA systems are designed 
                                                1996; Khardon, 1999].  We represent textual sources as sets 
to extract an answer from a document contained in a fairly 
                                                of features in a sparse domain, and treat the QA task as 
large general collection [Voorhees, 2003].   They tend to 
                                                behavior in a stochastic, partially observable world.  QA 
follow a generic architecture, such as the one suggested by 
                                                strategies are learned as sequences of transformation rules 
[Hirschman and Gaizauskas, 2001], that includes 
                                                capable of deriving certain types of answers from particular 
components for document pre-processing and analysis, 
                                                text-question combinations.  The transformation rules are 
candidate passage selection, answer extraction, and response 
                                                generated by instantiating primitive domain operators in 
generation.  Story comprehension requires a similar 
                                                specific feature contexts.  A process of reinforcement 
approach, but involves answering questions from a single 
                                                learning [Kaebling, et al., 1996] is used to select and 
narrative document.  An important challenge in text-based 
                                                promote effective transformation rules.  We rely on recent 
question answering in general is posed by the syntactic and 
                                                work in attribute-efficient relational learning [Khardon et 
semantic variability of question and answer forms, which 
                                                al., 1999; Cumby and Roth, 2000; Even-Zohar and Roth, 
makes it difficult to establish a match between the question 
                                                2000] to acquire natural representations of the underlying 
and answer candidate.  This problem is particularly acute in 
                                                domain features.  These representations are learned in the 
the case of story comprehension due to the rarity of 
                                                course of interacting with the domain, and encode the 
information restatement in the single document. 
                                                features at the levels of abstraction that are found to be 
                                                conducive to successful behavior.  This selection effect is                                                   apply
                                               reinforcement to
                                                 rule base


                                                return FAIL
                               no                                       no


                            more                                          more
                                   yes   lookup existing  valid rule no
                          processing                                     primitive
                                         applicable rule   exists?
                            time?                                         ops?

                                                acting by     yes           yes acting by
                            no                  inference                      search
                                                                        instantiate
                     yes   goal state                                    new rule
                           reached?                                   generalize against
                                                                        rule base
                                               START

                         extract current
                                                                execute rule in ABSTRACT
                         state features &
                                                                  domain      LEARNING/
                         compare to goal
                                                                             REASONING
                                                                             FRAMEWORK

      match candidate
                          lexically pre-                                    INTERMEDIATE
         sentence                                               modify raw text
                         process raw text                                    PROCESSING
       extract answer                                                          LAYER

                                                                                RAW
       lexicalized answer                                                     TEXTUAL
                                       raw text,   question,  (answer)         DOMAIN
   Figure 1.  The QABLe architecture for question answering. 

achieved by a fusion of abstraction space generalization system is provided with a set of training instances, each 
[Sacerdoti, 1974; Knoblock, 1992] and reinforcement consisting of a textual narrative, a question, and a 
learning elements.                              corresponding answer.  During the performance phase, only 
  The rest of this paper is organized as follows.  Section 2 the narrative and question are given. 
presents the details of the QABLe framework.  In section 3 At the lexical level, an answer to a question is generated 
we describe preliminary experimental results which indicate by applying a series of transformation rules to the text of 
promise for our approach.  In section 4 we summarize and the narrative.  These transformation rules augment the 
draw conclusions.                               original text with one or more additional sentences, such 
                                                that one of these explicitly contains the answer, and matches 
2  QABLe – Learning to Answer Questions         the form of the question. 
                                                  On the abstract level, this is essentially a process of 
2.1  Overview                                   searching for a path through problem space that transforms 
                                                the world state, as described by the textual source and 
Figure 1 shows a diagram of the QABLe framework.  The 
                                                question, into a world state containing an appropriate 
bottom-most layer is the natural language textual domain.  It 
                                                answer.  This process is made efficient by learning answer-
represents raw textual sources, questions, and answers.  The 
                                                generation strategies.  These strategies store procedural 
intermediate layer consists of processing modules that 
                                                knowledge regarding the way in which answers are derived 
translate between the raw textual domain and the top-most 
                                                from text, and suggest appropriate transformation rules at 
layer, an abstract representation used to reason and learn. 
                                                each step in the answer-generation process.  Strategies (and 
  This framework is used both for learning to answer 
                                                the procedural knowledge stored therein) are acquired by 
questions and for the actual QA task.  While learning, the explaining (or deducing) correct answers from training Phrase Type    Comments 
examples.  The framework’s ability to answer questions is              “Who” and nominal “What” 
                                                 SUBJ 
tested only with respect to the kinds of documents it has             questions 
seen during training, the kinds of questions it has practiced VERB event “What” questions 
answering, and its interface to the world (domain sensors              “Who” and nominal “What” 
                                                 DIR-OBJ 
and operators).                                                       questions 
  In the next two sections we discuss lexical pre-                     “Who” and nominal “What” 
                                                 INDIR-OBJ 
processing, and the representation of features and relations          questions 
                                                                      descriptive “What” questions 
over them in the QABLe framework.  In section 2.4 we look ELAB-SUBJ 
at the structure of transformation rules and describe how             (eg. what kind) 
they are instantiated.  In section 2.5, we build on this ELAB-VERB-TIME  
information and describe details of how strategies are ELAB-VERB-PLACE  
learned and utilized to generate answers.  In section 2.6 we ELAB-VERB-MANNER  
explain how candidate answers are matched to the question, ELAB-VERB-CAUSE  “Why” question 
                                                                       “Why” as well as “What for” 
and extracted.                                   ELAB-VERB-INTENTION  
                                                                      question 
                                                                      smooth handling of undefined 
2.2  Lexical Pre-Processing                      ELAB-VERB-OTHER 
                                                                      verb phrase types 
Several levels of syntactic and semantic processing are               descriptive “What” questions 
                                                 ELAB-DIR-OBJ 
required in order to generate structures that facilitate higher       (eg. what kind) 
order analysis.  We currently use MontyTagger 1.2, an off-            descriptive “What” questions 
                                                 ELAB-INDIR-OBJ 
the-shelf POS tagger based on [Brill, 1995], for POS                  (eg. what kind) 
tagging.  At the next tier, we utilize a Named Entity (NE)            WHERE/WHEN/HOW 
tagger for proper nouns a semantic category classifier for VERB-COMPL questions concerning state or 
nouns and noun phrases, and a co-reference resolver (that is          status 
limited to pronominal anaphora).  Our taxonomy of Table 1.  Phrase types used by QABLe framework. 
semantic categories is derived from the list of unique 
beginners for WordNet nouns [Fellbaum, 1998].  We also 
have a parallel stage that identifies phrase types.  Table 1 raw textual input and tags which are generated by pre-
gives a list of phrase types currently in use, together with processing modules. 
the categories of questions each phrase type can answer.  In A lexical sentence is represented as a sequence of words  
the near future, we plan to utilize a link parser to boost 〈w1, w2,…, wn〉, where word(wi, word) binds a particular 
phrase-type tagging accuracy.  For questions, we have a word to its position in the sentence.  The kth sentence in a 
classifier that identifies the semantic category of passage is given a unique designation sk.  Several simple 
information requested by the question.  Currently, this functions capture the syntax of the sentence.  The sentence 
taxonomy is identical to that of semantic categories.  Main (e.g., main verb) is the controlling element of the 
However, in the future, it may be expanded to accommodate sentence, and is recognized by main(wm, sk).  Parts of speech 
a wider range of queries.  A separate module reformulates are recognized by the function pos, as in pos(wi, NN) and 
questions into statement form for later matching with pos(wi, VBD).  The relative syntactic ordering of words is 
answer-containing phrases.                      captured by the function before(wi, wj).  It can be applied 
                                                recursively, as before(wi, before(wj, wk)) to generate the 
2.3  Representing the Question-Answering Domain entire sentence starting with an arbitrary word, usually the 
In this section we explain how features are extracted from sentence Main.  Thus for each word wi  in the sentence, 
                                                inSentence(wi, si) ⇒ main(wm, sk) ∧ (before(wi, wm) ∨ 
                                                before(wm, wi)).  A consecutive sequence of words is a 
 Instantiate Rule                               phrase entity or simply entity.  It is given the designation ex  
 Given:                                         and declared by a binding function, such as entity(ex, NE) 
 • set of primitive operators                   for a named entity, and entity(ex, NP) for a syntactic group 
 • current state specification                  of type noun phrase.  Each phrase entity is identified by its 
 • goal specification                           head, as head(wh, ex), and we say that the phrase head 
                                                controls the entity.  A phrase entity is defined as head(wh, 
                                                  ∧             ∧ … ∧
 1. select primitive operator to instantiate    ex)  inPhrase(wi, ex)    inPhrase(wj, ex). 
 2. bind active state variables & goal spec to existentially We also wish to represent higher-order relations such as 
                                                functional roles and semantic categories.  Functional 
    quantified condition variables   
                                                dependency between pairs of words is encoded as, for 
 3. execute action in domain 
                                                example, subj(wi, wj) and aux(wj, wk).  Functional groups are 
 4. update expected effect of new rule according to represented just like phrase entities.  Each is assigned a 
    change in state variable values 
                                                designation rx, declared for example, as func_role(rx, SUBJ), 
                                                and defined in terms of its head and members (which may 
 Figure 2.  Procedure for instantiating transformation be individual words or composite entities).  Semantic  categories are similarly defined over the set of words and relevant attributes of the world state.  G R  represents the 
 syntactic phrase entities – for example, sem_cat(cx, expected effect of the action.  G R  is inductively acquired 
         ∧             ∧             ∧
 PERSON)    head(wh, cx)  pos(wh, NNP)  word(wh, from prior applications of the rule.  For example,  
 “John”). 
    Semantically, sentences are treated as events defined by ()∧    ()∧       R ()   ()→
                                                  before w1 , w2 before w2 , w3 G before w3 , w4
 their verbs.  A multi-sentential passage is represented by 
                                                                     −     −     −     () 
 tying the member sentences together with relations over         add  word   after word w4 , w3
 their verbs.  We declare two such relations – seq and cause.  
                                                 indicates that when the phrase “w1 w2 w3” is found in the 
 The seq relation between two sentences, seq(si, sj) ⇒ 
                                                 text, this operator is expected to attach w4 to the end, 
 prior(main(si), main(sj)), is defined as the sequential 
ordering in time of the corresponding events.  The cause generating the phrase “w1 w2 w3 w4”.   For the rule to be 
                                                 effective in a given state, G R  must match all or part of the 
 relation cause(si, sj) ⇒ cdep(main(si), main(sj)) is defined 
such that the second event is causally dependent on the first.   system’s goal specification in that state.   
                                                 An instantiated rule is assigned a rank composed of: 
 2.4  Primitive Operators and Transformation Rules 
                                                    •   priority rating (p) 
 The system, in general, starts out with no procedural 
                                                    •   level of experience with rule (f) 
 knowledge of the domain (i.e., no transformation rules).  • 
 However, it is equipped with 9 primitive operators that confidence in current parameter bindings (c) 
 define basic actions in the domain.  Primitive operators are The first component, priority rating, is an inductively 
 existentially quantified.  They have no activation condition, acquired measure of the rule’s performance on previous 
 but only an existence condition – the minimal binding instances.  The second component modulates the priority 
 condition for the operator to be applicable in a given state.  rating with respects to a frequency of use measure.  The 
                                        E
 A primitive operator has the form C E → Aˆ , where C  is the third component captures any uncertainty inherent in the 
 existence condition and Aˆ  is an action implemented in the underlying features serving as parameters to the rule.  The 
 domain.  An example primitive operator is       rank of a rule is computed by the following function: 
                                                  
   primitive-op-1 :     ∃ wx, wy →  add-word-after-word(wy, wx) 
                                                           rank = p × c × ()1+ log()1+ f  
 Other primitive operators delete words or manipulate entire  
 phrases.  Figure 3 lists all nine primitive operators.  Note Each time a new rule is added to the rule base, an 
 that primitive operators act directly on the syntax of the attempt is made to combine it with similar existing rules to 
 domain.  In particular, they manipulate words and phrases.  produce more general rules having a wider relevance and 
 A primitive operator bound to a state in the domain applicability. 
                                                    Given a rule ∧  ∧  R ∧ R →  covering a set of 
 constitutes a transformation rule.  The procedure for         ca cb  gx g y  A1
 instantiating transformation rules using primitive operators 
                                                 example instances    E  and another rule 
 is given in Figure 2.  The result of this procedure is a              1
                                                         R   R
                                     R             ∧  ∧   ∧   →   covering a set of examples E , we 
 universally quantified rule having the form C ∧ G → A .  A  cb cc g y g z A2             2
                                                                      ∧ R →
 may represent either the name of an action in the world or add a more general rule cb g y A3  to the strategy.  The 
 an internal predicate.  C represents the necessary condition 
                                                 new rule A  is consistent with E and E .  In addition it will 
for rule activation in the form of a conjunction over the 3              1     2
                                                 bind to any state where the literal cb  is active.  Therefore 
                                                 the hypothesis represented by the triggering condition is 

     1.  ∃ wx, wy →  add-word-after-word(wy, wx) likely an overgeneralization of the target concept.  This 
                                                 means that rule A  may bind in some states erroneously.  
     2.  ∃ wx, wy →  add-word-before-word(wy, wx)              3
                                                 However, since all rules that can bind in a state compete to 
     3.  ∃ wx →  delete-word(wx) 
                                                 fire in that state, if there is a better rule, then A  will be 
     4.  ∃ w , p  →  add-word-after-phrase(p , w )                                     3
           x  y                     y x          preempted and will not fire. 
     5.  ∃ wx, py →  add-word-before-phrase(py, wx) 
                                                 2.5  Generating Answers 
     6.  ∃ px, wy →  add-phrase-after-word(wy, px) 
     7.  ∃ p , w  →  add-phrase-before-word(w , p ) Returning to Figure 1, we note that at the abstract level the 
           x  y                      y  x        process of answer generation begins with the extraction of 
         ∃    →
     8.    px, py   add-phrase-after-phrase(py, px) features active in the current state.  These features represent 

     9.  ∃ px, py →  add-phrase-before-phrase(py, px) low-level textual attributes and the relations over them 
                                                 described in section 2.3. 
   
                                                    Immediately upon reading the current state, the system 
  Figure 3.  Primitive operators used to instantiate checks to see if this is a goal state.   A goal state is a state 
  transformation rules.                          whose corresponding textual domain representation contains an explicit answer in the right form to match the questions.  QABLe’s pre-processing stage analyzes text with respect to 
In the abstract representation, we say that in this state all of various syntactic and semantic types.  In addition to 
the goal constraints are satisfied.             supporting abstract feature generation, these tags can be 
  If the current state is indeed a goal state, no further used to analyze text on a lexical level.  Thus, the question 
inference is required.  The inference process terminates and above is marked up as [ELAB-VERB <quantity-distance> (WRB 
the actual answer is identified by the matching technique How) (RB far)] [VERB (VBZ is)] [SUBJ <action> (DT the) (NN 
described in section 2.6 and extracted.         drive)] [VERB-COMPL (TO to) (NNP <place> Chicago)].  Once 
  If the current state is not a goal state and more reformulated into statement form, this becomes [ELAB-VERB 
processing time is available, QABLe passes the state to the <quantity-distance> ______ ] [VERB (VBZ is)] [SUBJ <action> (DT 
Inference Engine (IE).  This module stores strategies in the the) (NN drive)] [VERB-COMPL (TO to) (NNP <place> Chicago)].  
form of decision lists of rules.  For a given state, each The goal now is to find a sentence whose syntactic and 
strategy may recommend at most one rule to execute.  For semantic analysis matches that of the reformulated 
each strategy this is the first rule in its decision list to fire.  question’s as closely as possible.  Thus, for example the text 
The IE selects the rule among these with the highest relative may contain the sentence “The drive to Chicago is 2 hours” 
rank, and recommends it as the next transformation rule to with the corresponding analysis [SUBJ <action> (DT the) (NN 
be applied to the current state.                drive)] [VERB-COMPL (TO to) (NNP <place> Chicago)] [VERB 
  If a valid rule exists it is executed in the domain.  This (VBZ is)] [VERB-COMPL <quantity-time> (CD 2) (NNS hours)].  
modifies the concrete textual layer.  At this point, the pre- Notice that all of the elements of this candidate answer 
processing and feature extraction stages are invoked, a new match the corresponding elements of the question, with the 
current state is produced, and the inference cycle begins exception of the semantic category of the ELAB-VERB 
anew.                                           phrase.  This is likely not the answer we are seeking.  The 
  If a valid rule cannot be recommend by the IE, QABLe text may contain a second sentence “The drive to Chicago is 
passes the current state to the Search Engine (SE).  The SE 130 miles”, that analyses as [SUBJ <action> (DT the) (NN 
uses the current state and its set of primitive operators to drive)] [VERB-COMPL (TO to) (NNP <place> Chicago)] [VERB 
instantiate a new rule, as described in section 2.4. This rule (VBZ is)] [VERB-COMPL <quantity-distance> (CD 130) (NNS 
is then executed in the domain, and another iteration of the miles)].  In this case, all of the elements match their 
process begins.                                 counterparts in the reformulated question.  Thus, the second 
  If no more primitive operators remain to be applied to sentence can be matched as the correct answer with high 
the current state, the SE cannot instantiate a new rule.  At confidence. 
this point, search for the goal state cannot proceed, 
processing terminates, and QABLe returns failure. 3  Experimental Evaluation 
  When the system is in the training phase and the SE 
instantiates a new rule, that rule is generalized against the 3.1  Experimental Setup 
existing rule base.  This procedure attempts to create more We evaluate our approach to open-domain natural language 
general rules that can be applied to unseen example question answering on the Remedia corpus.  This is a 
instances.                                      collection of 115 children’s stories provided by Remedia 
  Once the inference/search process terminates  Publications for reading comprehension.  The 
(successfully or not), a reinforcement learning algorithm is comprehension of each story is tested by answering five 
applied to the entire rule search-inference tree.  Specifically, who, what, where, and why questions.   
rules on the solution path receive positive reward, and rules The Remedia Corpus was initially used to evaluate the 
that fired, but are not on the solution path receive negative Deep Read reading comprehension system [Hirschman et 
reinforcement.                                  al., 1999], and later also other systems, including Quarc 
                                                [Riloff and Thelen, 2000] and the Brown University 
2.6  Candidate Answer Matching and Extraction   statistical language processing class project [Charniak, et 
As discussed in the previous section, when a goal state is al., 2000].   
generated in the abstract representation, this corresponds to The corpus includes two answer keys.  The first answer 
a textual domain representation that contains an explicit key contains annotations indicating the story sentence that is 
answer in the right form to match the questions.  Such a lexically closest to the answer found in the published answer 
candidate answer may be present in the original text, or may key (AutSent).  The second answer key contains sentences 
be generated by the inference/search process.  In either case, that a human judged to best answer each question 
the answer-containing sentence must be found, and the (HumSent).  Examination of the two keys shows the latter to 
actual answer extracted.  This is accomplished by the be more reliable.  We trained and tested using the HumSent 
Answer Matching and Extraction procedure.       answers.  We also compare our results to the HumSent 
  The first step in this procedure is to reformulate the results of prior systems.  In the Remedia corpus, 
question into a statement form.  This results in a sentence approximately 10% of the questions lack an answer.  
containing an empty slot for the information being queried.  Following prior work, only questions with annotated 
For example, “How far is the drive to Chicago?” becomes answers were considered.     
“The drive to Chicago is ______.”  Recall further that 