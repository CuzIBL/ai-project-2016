                     Using Neutral Examples for Learning Polarity 

                                Moshe Koppel, Jonathan Schler 
                                      Bar-Ilan University 
                                 Computer Science Department 
                                  Ramat-Gan, 52900, ISRAEL 
                                 { koppel ,schlerj }@cs.biu.ac.il 

                   Abstract                        The purpose of this paper is to show that there is no basis 
                                                 for either of those myths and that neutrals can be exploited 
    Sentiment analysis is an example of polarity 
                                                 in interesting ways to great effect. 
    learning. Most research on learning to identify We consider two labeled corpora. The first consists of 
    sentiment ignores “neutral” examples and instead 1974 posts to chat groups devoted to popular U.S. television 
    performs training and testing using only examples 
                                                 shows. The second consists of about 14,000 posts to 
    of significant polarity. We show that it is crucial to shopping.com’s product evaluation pages. Both are equally 
    use neutral examples in learning polarity for a distributed among positive, negative and neutral documents. 
    variety of reasons and show how neutral examples 
                                                   Is it in fact the case that neutral documents lie near the 
    help us obtain superior classification results in two boundary of a learned model that distinguishes positive and 
    sentiment analysis test-beds.                negative examples? To test this, we trained a linear SVM on 
     
                                                 all positive and negative documents in the TV corpus. In 
 Many machine-learning problems involve predicting an Figure 1, we show the signed distance from the boundary of 
 example’s polarity: is it (significantly) greater than or less the positive and negative training examples, in ascending 
 than some standard. One canonical example of learning 
                                                 order from left to right. (This SVM correctly classifies 
 polarity is sentiment analysis, the determination of whether 79.1% of the training examples.) In addition, we show the 
 a particular text expresses positive or negative sentiment signed distance from the boundary of all neutral examples. 
 regarding some issue.  
                                                 There is no band near the boundary in which the 
  The problem of how to exploit a labeled corpus to learn preponderance of examples is neutral. We indicate the band 
models for sentiment analysis has attracted a good deal of around the boundary that is optimal in terms of overall 
interest in recent years [Dave et al 2003, Pang et al 2002, 
                                                 classification accuracy (positive, negative, or neutral) when 
Shanahan et al 2005]. One common characteristic of almost all examples in the band are classed as neutral. Even using 
all this work has been the tendency to define the task as a this optimal band, we attain accuracy of only 54.8%. (Note 
two-category problem: positive versus negative. In almost 
                                                 that simply using the SVM boundary to distinguish positive 
 all actual polarity problems, including sentiment analysis, from negative and not classifying any examples as neutral 
 there are, however, three categories that must be would yield accuracy of 52.7%.) 
 distinguished: positive, negative and neutral. Not every 
 comment on a product or experience expresses purely 3
                                                                                   pos
 positive or negative sentiment. Some – in many cases, most 
                                                   2
 – comments might report objective facts without expressing                        neut
 any sentiment, while others might express mixed or 1
 conflicting sentiment.                                                            neg
                                                   0
  Researchers are aware, of course, of the existence of 
 neutral documents. The rationale for ignoring them has been -1                    optimal upper
 a reliance on two tacit assumptions:                                              boundary
                                                   -2                              optimal lower
  •  Solving the binary positive vs. negative                                      boundary
                                                   -3
     problem automatically solves the three-category                                           
     problem since neutral documents will simply lie Figure 1 Distance from boundary in the TV shows corpus 
     near the boundary of the binary model          
  •  There is less to learn from neutral documents Similar results are obtained for the second dataset. 
     than from documents with clearly defined      What happens when we try to solve the three-class 
     sentiment                                   problem using positive, negative and neutral training 
                                                 documents? Five-fold cross-validation experiments using 
                                                 Weka’s implementation of multi-class SVM yields accuracy  of 56.5% for the TV corpus and 63.8% for the    classed the example as Class1. The optimal stack for this 
 shopping.com corpus. Interestingly, we will see that these data can be neatly summarized as follows: 
 results are far inferior to results obtainable by making even • If positive > neutral > negative then class=positive 
 stronger use of neutral documents.                 •   If negative > neutral > positive then class=negative 
  Consider the algorithm used in this experiment for •  Else class=neutral 
extending a binary algorithm to handle multiple classes, This simple stack yields accuracy of 74.9% on the three-
namely, pairwise coupling. In this approach, a model is class problem, which is significantly better than multi-class 
learned for each pair of classes (positive-negative, positive- SVM (and better than any of the constituent two-class 
neutral, negative-neutral) and these models are then problems).  
combined. Weka’s implementation [Witten and Frank 2000] What is most astonishing about this table is the following: 
of the [Hastie and Tibshirani 1998] algorithm treats the When, according to our model for positive vs. neutral, a test 
three constituent pairwise problems identically. That is, no example is classified as positive, it is not necessarily 
allowance is made for the particular relationships that positive, but we can assert with certainty that it is not 
positive, negative and neutral examples stand in to each negative (despite not a single negative example being used 
other.                                           in training.) Likewise, when, according to our model for 
  The main point of this paper is that it is crucial to take negative vs. neutral, a test example is classified as negative, 
these special relationships into account. We begin by it is not necessarily negative, but we can assert with 
running the following experiment. For each of the pairs, certainty that it is not positive (despite not a single positive 
negative-positive, negative-neutral, and positive-neutral, we example being used in training.) 
ran five-fold cross-validation experiments. For each An analogous (though not identical) principle holds in the 
example, we recorded how it was classed in the holdout set shopping.com corpus. 
in each of the three experiments.                  These results strongly suggest that polarity problems be 
  Table 1 shows the actual class distribution of examples in attacked by stacking results of pairwise coupling in non-
 the TV corpus assigned to each of the eight possible standard ways, taking full advantage of neutral examples. 
 outcomes.  
                                                 References 
 Pos Vs  Pos Vs   Neut Vs   original category    [Dave, K., Lawrence, S., and Pennock, D. M., 2003] 
 Neg     Neut     neg       neg neut   pos         Mining the peanut gallery: Opinion extraction and 
                                                   semantic classification of product reviews. In 
 Neg Neut Neg               354  52   
                                                   Proceedings of the Twelfth International World Wide 
 Neg Neut Neut 117 154                148          Web Conference (WWW-2003) 
 Neg Pos          Neg          47                 [Hastie, T. and R. Tibshirani, 1998] Classification by 
 Neg Pos          Neut         9 108               pairwise coupling in M. I. Jordan, M. J. Kearns, and 
 Pos Neut Neg 145                69                S. A. Solla (eds.) Advances in Neural Information 
 Pos Neut Neut 42 225                  46          Processing Systems 10 (NIPS-97), pp. 507-513. MIT 
                                                   Press, 1998. 
 Pos Pos Neg                   90        
                                                  [Pang, B., Lee, L. and Vaithyanathan, S., 2002] Thumbs 
 Pos Pos Neut                  12      356 
                                                   up? Sentiment Classification using Machine Learning 
 Table 1: Class distribution of examples per pairwise outcomes Techniques. In Proceedings of the 2002 Conference 
                  in TV corpus 
                                                   on Empirical Methods in Natural Language 
  
                                                   Processing (EMNLP) 
  As can easily be computed from the table, the accuracies 
of the pairwise models in five-fold cross-validation trials on [Savicky, P. And Fuernkranz, J., 2003] Combining 
their respective category pairs are: positive-negative, 67.3%; Pairwise Classifiers with Stacking. in Advances in 
positive-neutral, 73.7%; negative-neutral, 68.5%. We want Intelligent Data Analysis V. (Ed.: Berthold M.R., 
to parlay these pairwise models into the best possible three- Lenz H.J., Bradley E., Kruse R., Borgelt Ch.) - 
class model. To do this, let us define a stack [Wolpert 1992] Berlin, Springer 2003, pp. 219-229. 
as a mapping from each of the eight possible outcomes to [Shanahan, J. G., Yan Q., Janyce W. (Eds.), 2005] 
some class. Let an optimal stack be the mapping from each Computing Attitude and Affect in Text, Springer, 
of the eight possible outcomes to the majority class of the Dordrecht, The Netherlands, 2005  
examples with that outcome. [Savicky and Furnkranz 2002]  [Witten I. H. and Frank E. 2000] “Data Mining: 
have considered when such optimal stacks (determined Practical machine learning tools with Java 
using holdout data) might permit optimal use of pairwise implementations” Morgan Kaufmann, San Francisco, 
coupling.                                          2000 
  For a given example, let’s use the shorthand Class1 >  [Wolpert, D.H., 1992], Stacked Generalization, Neural 
Class2 to mean that the learned model of Class1 vs. Class2 Networks, Vol. 5, pp. 241-259, Pergamon Press 