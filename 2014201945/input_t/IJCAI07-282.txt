                 Correlation Clustering for Crosslingual Link Detection

                                  Jurgen Van Gael and Xiaojin Zhu
                                    Computer Sciences Department
                                   University of Wisconsin-Madison
                                          Madison, WI 53706
                               {JVANGAEL,    JERRYZHU@CS.WISC.EDU}

                    Abstract                            In this paper we propose a principled approach to the
                                                      crosslingual link detection task using correlation cluster-
    The crosslingual link detection problem calls for ing [Bansal et al., 2004; Demaine and Immorlica, 2003].Cor-
    identifying news articles in multiple languages that relation clustering is a recent graph-based clustering frame-
    report on the same news event. This paper presents work with interesting theoretical properties. It can be for-
    a novel approach based on constrained clustering. mulated to solve constrained clustering (also known as semi-
    We discuss a general way for constrained cluster- supervised clustering), which we will use for crosslingual link
    ing using a recent, graph-based clustering frame- detection. In constrained clustering, one performs clustering
    work called correlation clustering. We introduce  with additional constraints (or preferences) on the data points.
    a correlation clustering implementation that fea- Two typical constraints are must-link (where two items must
    tures linear program chunking to allow processing be in the same cluster) and cannot-link (where two items can-
    larger datasets. We show how to apply the corre-  not be in the same cluster). Constrained clustering has re-
    lation clustering algorithm to the crosslingual link ceived considerable attention in machine learning [Bilenko et
    detection problem and present experimental results al., 2004; Wagstaff et al., 2001; Xing et al., 2003]; we point
    that show correlation clustering improves upon the to [Basu et al., 2006] for further references. Solving the cor-
    hierarchical clustering approaches commonly used  relation clustering problem is hard but one natural way to ap-
    in link detection, and, hierarchical clustering ap- proximate the best solution is to encode it in a linear program-
    proaches that take constraints into account.      ming optimization framework. We combine correlation clus-
                                                      tering with a large-scale linear program solution technique
                                                      known as ‘chunking’ in order to solve larger crosslingual link
1  Introduction                                       detection problems. The contribution of our paper is twofold:
Crosslingual link detection is the problem of identifying news 1. we introduce a practical way for solving the complex
articles in multiple languages that report on the same news correlation clustering algorithm in [Demaine and Im-
event. It is an important component in online information morlica, 2003];
processing systems, with applications in security and infor- 2. we demonstrate good performance on crosslingual link
mation retrieval. Existing link detection systems are mostly detection using the correlation clustering approach.
monolingual, with a small number of bilingual link detec-
                                                        In the rest of the paper, we start by reviewing correlation
tion systems [Allan et al., 2000; Chen and Chen, 2002;
                                                      clustering and discuss how to implement it using linear pro-
Spitters and Kraaij, 2002] and very few crosslingual link
                                                      gramming chunking in section 2. We discuss related work in
detection systems [Pouliquen et al., 2004] that work across
                                                      constrained clustering and crosslingual link detection in sec-
many languages. Like the latter, we assume monolingual link
                                                      tion 3. Finally we present experiments in section 4 where we
detection has been done, such that news articles on the same
                                                      improve upon existing crosslingual link detection systems.
event in a single language already form a single group. This
assumption is mild, as existing systems like Google News
(http://news.google.com,the‘alln      related’ links) 2   Correlation Clustering
do just this. Our goal is thus to cluster these monolingual Consider the following problem: we are given a weighted
groups from different languages over a period of time, so that graph for which we want to partition the nodes into clusters.
groups reporting on the same event are in the same cluster. If two nodes share an edge with positive weight, we prefer
One needs to take two things into consideration: 1. We would they be in the same cluster; if they share an edge with negative
rather not cluster any monolingual groups from the same lan- weight, we prefer they end up in different clusters. The goal
guage together since we assume monolingual link detection of correlation clustering is to partition the graph into clusters
has done a reasonable job. This is known as ‘cannot-links’ to maximally satisfy these preferences.
in constrained clustering as we will discuss later; 2. We in We review the discussion in [Demaine and Immorlica,
general do not know the number of clusters in advance. 2003] on how to formally describe correlation clustering as

                                                IJCAI-07
                                                  1744an integer program (IP). Let G =(V,E) be a graph with node in the graph and gradually grow a ball centered around
                                 +
weight we for every edge1 e ∈ E.LetE be the set of edges this node. While increasing the radius of the ball, all the
                    +                       −
with positive weights, E = {e ∈ E|we > 0} and E be the nodes that are at a distance smaller than the radius away from
                               −
set of edges with negative weight, E = {e ∈ E|we < 0}. the center of the ball will be included in the ball. The radius
We now associate a binary variable xuv with every edge grows until some technical termination condition is met. All
(uv) ∈ E with the following interpretation: if xuv =1then the nodes in the ball are then put into one cluster and removed
u, v are in different partitions, if xuv =0then u, v are in the from the graph. This procedure is repeated until there are no
same partition. Intuitively xuv is the binary indicator variable more nodes left in the graph. [Demaine and Immorlica, 2003]
for whether we cut the edge or not. Correlation clustering prove that the original objective function (equation (1)) of the
minimizes the following objective                     LP relaxation will be bounded above by O(log n) times the
                                                    objective function of the IP where n is the number of nodes
                wexe +     −we(1  − xe).        (1)   in the graph.2
           e∈E+        e∈E−                             Unfortunately, the triangle inequalities could introduce up
                                                        O  n3
We want the variables to correspond to a valid partitioning: to ( ) constraints in the LP, which puts a heavy burden
if u, v are in the same cluster and v, t are in the same cluster, on memory requirements. Next we discuss how we tradeoff
then u, t must be so too. This can be achieved by the triangle memory for runtime so we can solve correlation clustering
                                                      for larger problem sizes.
inequality constraints xuv +xvt ≥ xut below. Simplifying the
objective function we ﬁnd the correlation clustering integer 2.2 LP Chunking
program:
                                                     Linear program chunking [Bradley and Mangasarian, 2000]
    minx        e∈E wexe                              is a technique to convert a large linear program into an iter-
    subject to xe ∈{0, 1},       ∀e ∈ E               ative procedure on much smaller sub-problems, thus reduc-
                                                (2)
              xuv + xvt ≥ xut, ∀uv, vt, ut ∈ E        ing the memory need. The iterative procedure produces the
              xuv = xvu,        ∀u, v ∈ V             same solution and is guaranteed to terminate. It works as fol-
           w                                          lows: one ﬁrst breaks up all the constraints into chunks and
The weights  are input to the algorithm, and can encode solves the optimization problem using only the ﬁrst chunk of
must-links and cannot-links besides similarities between data constraints. The active constraints are those inequality con-
items. As formulated above, correlation clustering has two straints that achieve equality at the solution. Next, one keeps
attractive properties that make it suitable for crosslingual link only the active constraints from the ﬁrst chunk, adds all con-
detection in particular and constrained clustering in general. straints from the second chunk, and solves the LP again. This
First of all, one does not need to specify the number of clus- procedure is repeated, looping through all chunks over and
ters; the algorithm determines the optimal number of clusters over until some convergence criterion is met. One can arbi-
automatically. Secondly, the graph edge weights can be arbi- trarily set the size of the chunks to reduce the memory load
trary and do not need to satisfy any metric condition. of the iterative procedure.
2.1  Linear Program Approximation                       Let a general linear program be described as,
                                                                            
Unfortunately solving the correlation clustering IP in (2) ex-         min{c  x|Hx ≥ b},              (3)
      NP                                                                x
actly is  -Hard. Recent theoretical results on approxima-        n        m×n        m
tion algorithms [Bansal et al., 2004], in particular [Demaine with c ∈ R ,H ∈ R ,b ∈ R . Let the constraints
and Immorlica, 2003], propose practical approaches to cor- [Hb] be partitioned into l blocks, possibly of different
                                                      sizes, as follows:
relation clustering. We build on the work in [Demaine and                    ⎡           ⎤
Immorlica, 2003] where the authors describe an O(log n) ap-                    H1     b1
proximation by relaxing the IP to a linear program (LP), and                 ⎢           ⎥
                                                                     Hb      ⎣   .     . ⎦
rounding the solution of the LP by a region growing tech-           [     ]=     .     .              (4)
                                                                                 l     l
nique. We replace constraint xe ∈{0, 1} by xe ∈ [0, 1] in                       H      b
equation (2) to relax the IP to an LP. The solution to this LP  j           xj
might include fractional values which we will have to round. At iteration we compute by solving the following linear
We point to [Demaine and Immorlica, 2003] for a detailed program,
                                                              j  (j  mod l) j  (j mod l)   j j    j
description and theoretical analysis of the rounding algorithm min{c x |H  x ≥ b         ∧H¯ x  ≥ ¯b }, (5)
and limit ourselves to a qualitative description in this paper. xj
One can interpret the value of the LP variables as distances:
                                                         2Although the theoretical analysis in [Demaine and Immorlica,
when a variable has value 0, the two adjacent nodes go in the 2003] requires the ball to grow continuously, this is not practical.
same cluster as their distance is 0 while if a variable is 1,the By redeﬁning the volume in [Demaine and Immorlica, 2003] to be
two adjacent nodes go into different clusters. The rounding the total volume of edges inside the ball as well as the total volume
procedure now needs to decide on how to partition the graph inside the cut, i.e., replace pvw · xvw · (r − xuv) by pvw · xvw
given that some nodes are at fractional distances away from in their deﬁnition, and modifying the description of step 3 in their
each other. Intuitively, the rounding algorithm will pick a algorithm as: ‘Grow r by min{xuv − r>0,v∈ / B(u, r)} so
                                                      that B(u, r) includes another entire edge’, the theoretical guarantee
  1We will denote an edge both as e ∈ E and as a pair of vertices stays the same but we only need to check the radius r a ﬁnite number
(uv) ∈ E                                              of times.

                                                IJCAI-07
                                                  1745where [H¯ 0 ¯b0] is empty and [H¯ j ¯bj] is the set of active
constraint, i.e. all inequalities satisﬁed as equalities by xj at
iteration j. We stop iterating when cT xj = cT xj+ν for some
pre-speciﬁed integer ν. We point to [Bradley and Mangasar-
ian, 2000] for more details and proofs of the ﬁnite termination
of this algorithm.

3  Related Work

Constrained or semi-supervised clustering has enjoyed some             Figure 2: Toy dataset
recent attention [Basu et al., 2006; Bilenko et al., 2004;
Davidson and Ravi, 2005; Wagstaff et al., 2001; Xing et
al., 2003].In[Basu et al., 2006], the authors categorize all respondence.
semi-supervised methods into two classes: constraint-based
and distance-based methods. The constraint-based methods,
such as [Wagstaff et al., 2001] and to which our approach 4 Experiments
belongs, rely on the must-link and cannot-link constraints to
guide the clustering algorithm in ﬁnding a partitioning that In this section, we ﬁrst illustrate correlation clustering on a
does not violate the constraints. Distance-based methods, toy dataset. We then discuss how to solve the crosslingual
such as [Xing et al., 2003], learn a metric using the con- link detection problem using a correlation clustering based
straint information and then apply existing clustering algo- constrained clustering approach and show how this improves
rithms to the data points in the learned metric space. These upon existing hierarchical clustering approaches.
approaches require specifying the number of clusters before-
hand. One solution to this issue is to use variants of hierarchi- 4.1 Correlation Clustering on a Toy Dataset
cal clustering that take constraints into account, e.g. [David-
son and Ravi, 2005]. By changing where to cut the dendro- It is straightforward to adapt correlation clustering for con-
gram, one can control the number of clusters. The main dif- strained clustering. Say we are given a set of items U =
ference between hierarchical clustering with constraints and {u1,u2, ··· ,ul}, a pairwise similarity measure S : U ×U →
correlation clustering is that the former makes local, greedy R,asetCM ⊂ U × U of must-link constraints and a set
decisions at every step while correlation clustering optimizes CC ⊂ U × U of cannot-link constraints. We build a graph
the clustering over the whole graph at once. One motivation G where the set of vertices is U. As a ﬁrst step, we add an
for our work is the observation that the crosslingual link de- edge for all pairs of nodes not in CM ∪ CC and set the edge
tection systems in [Pouliquen et al., 2004; Allan et al., 2000; weight according to the similarity measure S.LetM be a
Chen and Chen, 2002] do not use constrained clustering tech- constant that is sufﬁciently larger than the sum of the abso-
niques.                                               lute values of all weights in the graph so far. In the second
  So far, correlation clustering has not been applied to ma- step, for all the pairs in CM and CC , we add either hard or
chine learning tasks very often. We are only aware of [Mc- soft preferences: if we assume that the constraints are hard,
Callum and Wellner, 2005] who implement a more restricted we add an edge for every must-link constraints with weight
version of correlation clustering in [Bansal et al., 2004] for M and an edge for every cannot-link constraint with weight
noun co-reference.                                    −M. If we want soft preferences, we can use values smaller
                                                          M
  The only crosslingual link detection system that cov- than according to the strength of the preferences.
ers a large set of languages we are aware of is described Figure 2 shows a toy dataset consisting of four nodes with
in [Pouliquen et al., 2004]. The authors describe a system a cannot-link constraint between nodes 1 and 2. The weights
which performs crosslingual link detection as well as mono- are speciﬁed in the ﬁgure. The edge not shown in the ﬁgure
lingual news tracking, i.e. the identiﬁcation of related news has a similarity of zero. We use −1000 for the cannot-link
over time in one particular language. Their approach uses constraint edge weight. The objective function for this dataset
a very rich article representation based on extracting named is to minimize −1000x(1,2) +30x(1,3) +25x(2,3) +20x(2,4) +
entities, keywords and geographical names. In addition, 15x(3,4) subject to the triangle inequality constraints. Solv-
the articles are mapped onto the multilingual thesaurus EU- ing the IP exactly would give us a solution that assigns 1 to
ROVOC  [Steinberger et al., 2002] which categorizes the arti- all variables except x(2,3) = x(2,4) = x(3,4) =0; this cor-
cles in several of 6000 hierarchically organized subjects. Our responds to the clustering {1}, {2, 3, 4}. Although nodes 1
system, on the other hand, uses machine translation tools to and 3 have the highest similarity, the cannot-link constraint
represent articles in a uniform way. This is a common [Diab guides the correlation clustering algorithm to not take node 1
and Resnik, 2001] way of working with multilingual corpora. into the cluster with 2 and 3. Note how a hierarchical clus-
Our experiments show that although the translation is noisy, tering algorithm would start off wrong as it merges nodes 1
it does not signiﬁcantly affect performance. Our crosslingual and 3 together and thus fails to ﬁnd the best clustering. Even
link detection task is also related to the work in [Diaz and a hierarchical clustering algorithm that takes constraints into
Metzler, 2007], where the authors introduce a framework for account will not ﬁnd the best clustering as it will greedily
aligning documents in parallel corpora based on topical cor- merge nodes 1 and 3 together.

                                                IJCAI-07
                                                  1746                                   Figure 1: Samples from the large dataset.

4.2  Crosslingual link detection                      ticle groups often have a small but positive similarity due
We generated ﬁve datasets by crawling Google News. We to common words. If we use the similarity (6) directly as
speciﬁcally focused our experiments on news articles which graph edge weights for correlation clustering, many irrele-
Google categorized as ‘world news’ as we assume this is the vant groups will be clustered together. For the problem of
category where the most interesting cross-lingual links can be link detection, this is clearly not desirable. We therefore
                                                                           t
made. The ﬁrst four datasets each consist of 60 monolingual subtract a bias constant from all similarity values so that
                                                      w     s   − t
‘world news’ article groups from three languages: English, uv = u¯v¯ . Intuitively, too small a similarity (6) between
German and French. Each of these four datasets was gener- two article groups is in fact evidence that they should not be
                                                                                         t
ated one week apart by selecting the top 20 article groups for in the same cluster. By changing the bias we change the re-
each language in April 2006. This results in a total of 60 ar- sulting clustering, which is how we generate precision-recall
ticle groups in each dataset. In May 2006, we generated the curves. For all the experiments presented below, we chose our
ﬁfth dataset which is larger and consists of 160 article groups bias values as follows: we started with a bias such that only
from the ‘world news’ category in eight different languages: one edge in the graph remains positively weighted. Next, we
                                                                                           .
English, German, Italian, French, Portuguese, Spanish, Ko- steadily increase the bias such that another 0 1% of the edges
rean and Chinese. Figure 1 shows a sample from the larger becomes positively weighted. On the small datasets, we re-
dataset. For all ﬁve datasets, we manually created a ground peated the experiments until 75% of the edges are positively
truth clustering3.                                    weighted while on the larger datasets we repeat the experi-
  For correlation clustering, we construct a fully-connected ments until 10% of the edges are positively weighted. We
graph where each node is a monolingual article group. We compute precision and recall values relative to our manually
create cannot-links between all pairs of article groups from labeled ground truth. We count an edge as true positive (TP),
the same language and choose −108 as the weight for these if its two article groups appear in the same cluster in both
cannot-link edges. We compute similarity values between ar- ground truth and our results, false positive (FP) if they do not
ticle groups from different languages with the following pro- appear in the same cluster in ground truth but do appear to-
cedure: ﬁrst we concatenate all the article titles in a mono- gether in our results, and so on. Precision and recall is a better
lingual group to form a ‘document representation’ for the measure than accuracy for our task, since the baseline of clas-
group. We then use Google machine translation to automati- sifying every edge as ‘not in same cluster’ would have high
cally translate the ‘document’ into English, and remove stop- accuracy because of the large number of true negatives. We
words from the translation. Therefore monolingual groups used CPLEX 9.0 on a 3.0 GHz machine with 2GB RAM to
in different languages are represented by their corresponding solve the linear programs.
(noisy) English translation, providing a way to compute their Our ﬁrst round of experiments are designed to illustrate
similarities. Empirically we found no difference in perfor- how taking constraints into account improves performance
mance using different machine translation tools such as Ba- on the crosslingual link detection problem. We compare
belﬁsh and Wordlingo. Next, for each monolingual group, our correlation clustering algorithm to the hierarchical clus-
we convert the translated document into a TF.IDF vector tering approach which has commonly been used for the
w¯ =(w1w2 ···w|V |), with wi = ni · log (|D|/|Di|),where crosslingual link detection problem, [Chen and Chen, 2002;
ni is the number of times word i appears in the document Pouliquen et al., 2004], and constrained hierarchical cluster-
representing the article group, D represents the set of arti- ing such as [Davidson and Ravi, 2005]. Hierarchical clus-
cle groups in the dataset and Di represents the set of article tering is done by choosing a bias value and adding edges to
groups that include word wi. We compute the similarity swv the graph in descending order according to their weight until
between any two TF.IDF vectors w,¯ v¯ as their inner product, the edge weights become smaller than the bias. We then out-
                                                      put the connected components as the resulting clusters. Con-
                       |V |                          strained hierarchical clustering is similar, except that at every
                 sw¯v¯ =   wi · vi.             (6)   step we only add an edge if it does not introduce a path be-
                        i=1                           tween two nodes in a cannot-link constraint. Again, we out-
  Note that even with stop-word removal, two unrelated ar- put the connected components as the resulting clusters. The
                                                      left plot in Figure 3 shows the average precision-recall over
  3The datasets are available at http://www.cs.wisc.edu/ our four small datasets. If we keep the number of positively
∼jvangael/newsdata/.                                  weighted edges small (large bias) then both types of hierar-

                                                IJCAI-07
                                                  1747                                                                            3
    1                                  1                                  10
                                                                                                  LP
  0.8                                 0.8                                                         IP
                                                                            2
                                                                          10
  0.6                                 0.6

                                                                            1
  0.4    LP Solution                  0.4   LP Solution                   10
         IP Solution                        IP Solution

  0.2    HC Solution                  0.2   HC Solution                   Runtime  in seconds 0
                                                                          10
         CHC Solution                       CHC Solution
    0                                  0
                                                                                    2      3      4
    0    0.2  0.4   0.6  0.8   1        0    0.2  0.4  0.6   0.8   1         Bias t 10   10     10

Figure 3: Left: average precision-recall over four small datasets. Middle: precision-recall for the large dataset. Right: average
runtime over four small datasets.

chical clustering perform as well as correlation clustering. the precision-recall curves: at very low bias, due to the sym-
Inspecting the datasets, this behavior can be explained by the metry of the graph, the optimal LP solution has a number of
fact that there are a number of news events for which the arti- variables with 0.5 values. From the theoretical analysis of the
cles use a subset of the vocabulary that is not commonly used rounding algorithm, we know that a radius cannot grow to be
in other articles. Our similarity measure assigns large weights 0.5. As a result of these properties, in the low bias regime
among article groups in different languages on these events a large number of nodes will end up as singleton clusters.
and very small weight between these article groups and arti- This prohibits recall from increasing to 1.0 and we observe
cle groups on a different topic. In a sense these are ‘easy’ in- the precision-recall curve loop back towards lower recall and
stances which both hierarchical clustering approaches as well higher precision. Because the curve essentially follows the
as correlation clustering get right. If we increase the num- ﬁrst path ‘in the opposite direction’ we did not include this in
ber of positive edges (small bias) then the simple hierarchi- Figure 3 for clarity.
cal clustering algorithm performs much worse than correla- Our next experiment was designed to evaluate how much
tion clustering. As a simple hierarchical clustering approach the LP approximation algorithm improves the runtime over
has no notion of cannot-link constraints, it will cluster groups solving the IP exactly. The rightmost plot in Figure 3 shows
from the same language together. Usually, crosslingual link the average runtime over the four small datasets of solving the
detection systems choose to leave these clusters out, but this exact IP compared to solving the approximation algorithm.
decision comes at the price of lower recall. Constrained hi- Every dot in the graph represents the time required to solve
erarchical clustering performs a little better as it takes our either the IP or the LP with rounding for a speciﬁc bias. It
assumption about the correctness of the monolingual groups is clear from this ﬁgure that the LP approximation algorithm
into account. Nonetheless, Figure 3 also shows that corre- for correlation clustering is signiﬁcantly faster than solving
lation clustering, which takes the whole graph into account the IP directly. However, even on the larger dataset the main
instead of making local greedy decisions can still outperform bottleneck is not so much the runtime but rather the memory
constrained hierarchical clustering. We attempted to compare requirements. On this large dataset, the underlying graph has
our approach to the constrained clustering in [Bilenko et al., 160 nodes which results in over 2, 000, 000 constraints for
2004] using their UTWeka implementation. The implemen- both the IP and LP. This is about as large a correlation clus-
tation ended up returning many empty clusters, resulting in tering instance we can solve without using chunking on our
low precision and recall.                             machine with 2GB RAM.
  The middle plot in Figure 3 shows the precision-recall for Our last experiment shows the results of applying chunk-
the large dataset; it indicates the trend we observed with the ing to the LP for correlation clustering. Our experimental
smaller datasets: taking into account constraints can still im- setup is the following: we create instances of the correlation
prove the performance of crosslingual link detection. clustering with random edge weights, distributed roughly ac-
  Next, let us consider the solution found by the approxi- cording to the instances of interest to crosslingual link detec-
mation algorithm and the exact integer solution. Figure 3 tion. We chose our chunk size to be as large as possible while
shows that on the small datasets the two solution are exactly still having some workspace memory for the processing in
equal. Inspecting the LP solutions, we ﬁnd that in the high between iterations: this resulted in 106 constraints per chunk.
bias regime, almost no rounding is necessary as the LP so- Finally we use a value of ν =4as our stop condition. Table 1
lution is the exact IP solution. Only in the low bias regime, shows the runtime for chunking versus solving the whole LP
when more edges are positively weighted, rounding becomes at once. Correlation clustering instances of size 128 are the
necessary. On the large dataset, Figure 3 shows that although ﬁrst instances where the number of constraints is larger than
there is a small difference between the two solutions, the LP the chunk size. At this size, the runtime overhead for chunk-
relaxation with rounding does well to ﬁnd a good approx- ing is mostly due to the stop condition. Starting from graphs
imation to the integer solution. We observed rather unex- with around 200 nodes we cannot ﬁt the whole LP in memory
pected behavior from the rounding algorithm that inﬂuences anymore and we must apply chunking to tradeoff memory for

                                                IJCAI-07
                                                  1748