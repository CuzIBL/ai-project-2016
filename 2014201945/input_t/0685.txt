                  Reinforcement Learning in POMDPs Without Resets

           Eyal Even-Dar                   Sham M. Kakade                    Yishay Mansour
    School of Computer Science     Computer and Information Science      School Computer Science
         Tel-Aviv University           University of Pennsylvania           Tel-Aviv University
       Tel-Aviv, Israel 69978           Philadelphia, PA 19104             Tel-Aviv, Israel 69978
        evend@post.tau.ac.il          skakade@linc.cis.upenn.edu          mansour@post.tau.ac.il

                    Abstract                          contain even asymptotic results for general POMDPs which
                                                      guarantee that the average reward of an agent will be near
    We consider the most realistic reinforcement learn- optimal in the limit.
    ing setting in which an agent starts in an unknown
    environment (the POMDP) and must follow one         Part of the technical difÔ¨Åculty is that there are currently no
    continuous and uninterrupted chain of experience  general results for belief state tracking with an approximate
    with no access to ‚Äúresets‚Äù or ‚ÄúofÔ¨Çine‚Äù simula-    model showing that divergence in the belief state does not
    tion. We provide algorithms for general connected eventually occur. Crudely, the issue is that if a belief state
    POMDPs that obtain near optimal average reward.   is being tracked in an approximate manner, then it is impor-
    One algorithm we present has a convergence rate   tant to show that this approximation quality does not contin-
    which depends exponentially on a certain horizon  ually degrade with time ‚Äî otherwise the agent will eventu-
    time of an optimal policy, but has no dependence  ally loose track of the belief state in the inÔ¨Ånite horizon (this
    on the number of (unobservable) states. The main  of course is not an issue in an MDP where the current state
    building block of our algorithms is an implemen-  is observable). Boyen and Koller (1998) address the issue
    tation of an approximate reset strategy, which we of approximate belief state tracking, but in their setting the
    show always exists in every POMDP. An interest-   model is known perfectly and their goal is to keep a compact
    ing aspect of our algorithms is how they use this representation of the belief state. Note that approximate be-
    strategy when balancing exploration and exploita- lief state tracking is much simpler if the agent is only acting
    tion.                                             over a Ô¨Åxed Ô¨Ånite horizon, since then one can bound the error
                                                      accumulation as a function of the horizon.
                                                        We present new algorithms for learning in POMDPs which
1  Introduction                                       guarantee that the agent will obtain the optimal average re-
We address the problem of lifelong learning in a partially ob- ward in the limit. Furthermore, we provide a Ô¨Ånite time con-
servable Markov decision process (a POMDP). We consider vergence rates for one of our algorithms which has an expo-
the most general setting where an agent begins in an unknown nential dependence on a certain horizon time (of an optimal
POMDP and desires to obtain near optimal reward. In this strategy) but has no dependence on the number of states in
setting, the agent is forced to obey the dynamics of the envi- the POMDP. This result is reminiscent of the trajectory tree
ronment, which, in general, do not permit resets.     algorithm of Kearns et al. (1999) which has similar depen-
  The problem of lifelong learning has been well studied dencies (though there they assumed access to a generative
for observable MDPs. Kearns and Singh (1998) provide the model, which allowed simulation of the POMDP). Given the
E3 algorithm, which has Ô¨Ånite (polynomial) time guarantees plethora of complexity results in the literature on planning in
until the agent obtains near optimal reward. Unfortunately, POMDPs (see Lusena et al. (2001)), we feel these depen-
such an algorithm is not applicable in the more challenging dencies are the best one could hope for in the most general
POMDP setting. In fact, none of the guarantees in the litera- setting.
ture for learning in the limit for MDPs apply to POMDPs, for Central to our algorithms is the implementation of an ap-
reasons which are essentially due to the partially observabil- proximate reset strategy or a homing strategy. The idea of a
ity.                                                  reset strategy is not new to the literature ‚Äî homing sequences
  For POMDPs, the problem of balancing exploitation with were used in the learning of deterministic Ô¨Ånite automata (see
exploration has received rather little attention in the literature Rivest and Schapire (1993), though there the sequence pro-
‚Äî typically most results in POMDPs are on planning (see for vided exact resets). Here, the agent follows a homing strat-
example Sondik (1971); Lovejoy (1991a, 1991b); Hauskrecht egy in order to move approximately towards a reset. We show
(1997); Cassandra (1998)). Most of the existing learning that such a strategy always exists, and our Ô¨Ånite convergence
algorithms such as Parr and Russell (1995); Peshkin et al. rates also depend on a characteristic time it takes to approx-
(2000) either assume a goal state or assume a reset button. imately reset. However, note that existence of such a strat-
In fact, to the best of our knowledge, the literature does not egy alone does not imply that such a strategy will be useful.The reason is that the agent must take actions to reset, which 3 Homing Strategies
might otherwise be better spent exploring or exploiting. It
turns out that our algorithms use the homing strategy while Clearly, having an action which resets the agent to some des-
both exploring and exploiting. In fact, they use the homing ignated state would be useful, as it would allow us to test and
strategies inÔ¨Ånitely often, which, unfortunately, detracts from compare the performance of various policies, starting at the
exploiting. However, we are able to show that the ratio of the same start state. However, in general, such an action is not at
time these homing strategies are used compared to the time our disposal.
spent exploiting is decreasing sufÔ¨Åciently rapidly, such that Instead our algorithms utilize an approximate reset, which
near optimal average reward can be obtained.          we show always exists. There are a few subtle points when
                                                      designing such a reset. First, we must select actions to
2  Preliminaries                                      achieve the approximate reset, i.e. the approximate reset is
A Partially Observable Markov Decision Process (POMDP) done through the use of a homing strategy. Hence, while
                                                      homing, the agent is neither exploring nor exploiting. Sec-
is deÔ¨Åned by a Ô¨Ånite set of states S, an initial state s0, a set
of actions A, a set of observations O, with an output model ond, rather than moving to a Ô¨Åxed state, the homing strategy
Q, where Q(o, r|s, a) is the probability of observing o and can only hope to move to toward a Ô¨Åxed (unknown) belief
                                                      state. Third, as we shall see, since the POMDP might be
reward r after performing action a at state s (we assume that                                       1
r ‚àà [0, 1]), and a set of transitions probabilities P , where periodic the stopping time must be a random variable . To
P (s0|a, s) is the transition probability from s to s0 after per- implement this randomized stopping time, we introduce Ô¨Åc-
forming action a. We deÔ¨Åne r(s, a) as the expected reward titious ‚Äôstay‚Äô actions, in which the agent does not take an ac-
under Q(¬∑|s, a) after performing action a in state s. tion that period. By this, we mean that if the homing strategy
  A  history h  is a  sequence  of actions, rewards   decides to take a ‚Äôstay‚Äô action at some time ‚Äî which may not
and observations of some Ô¨Ånite length, i.e.  h   =    be possible if the true POMDP does not permit ‚Äôstay‚Äô actions
                                                      ‚Äî  then the agent just ignores this ‚Äôstay‚Äô action and obtains
{(a1, r1, o1), ..., (at, rt, ot)}. A strategy or policy in
a POMDP    is deÔ¨Åned as a mapping from  histories to  another action from the homing strategy to execute. Hence,
actions. We deÔ¨Åne a  belief state B to be a distribu- after the agent has taken t homing actions (which are either
tion over states. Given an initial belief state B let real or ‚Äôstay‚Äô actions), the agent has taken t ‚àí m real actions
                                              0       in the POMDP and m stay actions. We now deÔ¨Åne an approx-
Pr[h|B0]  =   Pr[r1, o1. . . . , rt, ot|a1, . . . , at, B0] be the
probability of observing the sequence of reward-observations imate reset strategy.
(r1, o1, . . . rt, ot) after performing the actions a1 . . . at.
  For each  strategy œÄ we   deÔ¨Åne its t-horizon ex-   DeÔ¨Ånition 3.1 H is an (, k)-approximate reset (or homing)
                                          œÄ           strategy if for every two belief states B1 and B2, we have
pected reward from  a belief state B as Rt (B)   =
          Pt                                          kHE(B1)  ‚àí HE(B2)k1  ‚â§  , where HE(B) is the expected
(1/t)Eh‚àºœÄ[  i=1 r(si, ai)|B0 = B]. A t-Markov strategy
is a strategy that depends only on the last t observations. The belief state reached from B after k homing actions of H (so at
optimal t-Markov strategy‚Äôs expected return from initial be- most k real actions have been taken) and H(B) is a random
                      ‚àó                               variable such that HE(B) = EH(B)‚àºH,B[H(B)].
lief state B is deÔ¨Åned as Rt (B).
  The only assumption we make is that the POMDP is con-
                                                        The above deÔ¨Ånition only states that H will approximately
nected, i.e. for all states s, s0, there exists a strategy œÄ which
                                                      reset, but this approximation quality could be poor. We now
can reach s0 with positive probability starting from s. (We do
                                                      show how to amplify the accuracy of an approximate hom-
not make any ergodicity assumptions, since strategies are by
                                                      ing strategy, and then we show that an approximate homing
deÔ¨Ånition non-stationary). Note that if the POMDP is discon-
                                                      strategy always exists.
nected, then the best statement we could hope for is to obtain
the optimal average reward for one of its connected compo-
                                                      Lemma 3.1  Suppose that H is an (, k) approximate reset
nents.                                                      `       `                            `
  Connectivity implies that there exists a strategy œÄ‚àó that then H is an ( , k`) approximate reset, where H consec-
maximizes the average reward. More formally, there exists utively implements H for ` times. Furthermore, this implies
   ‚àó                                œÄ‚àó                there exists a unique belief state BH such that HE(BH ) =
a œÄ such that: i) for every B, limt‚Üí‚àû Rt (B) exists and
does not depend on B, which we denote by R‚àó, and ii) for all BH .
          ‚àó                œÄ
œÄ and B, R  ‚â• limt‚Üí‚àû  sup Rt (B). Hence, for all  > 0
there exist a œÑ, such that for all B and t ‚â• œÑ:         Proof: The proof is a standard contraction argument, and
                                                      we use induction. For l = 1, the claim follows by deÔ¨Ånition.
                   ‚àó    œÄ‚àó                                              `‚àí1         `‚àí1          `‚àí1
                 |R ‚àí Rt  (B)| ‚â§                     Assume now that kHE  (B1) ‚àí HE   (B2)k1 ‚â§    . Let
                                                        l‚àí1                 l‚àí1              P
                                                      HE   (B1) =  Q1 and HE   (B2) =  Q2, so  s |Q1(s) ‚àí
and we refer to œÑ as the -horizon time of the optimal strategy. `‚àí1                    0
Essentially, œÑ is the timescale in which the optimal strategy Q2(s)| ‚â§  . For an arbitrary state s , and using the fact
achieves close to its average reward.
  When we say that we restart a t-Markov strategy œÄ from 1Without randomizing over the stopping times (i.e. allowing
a belief state B we mean speciÔ¨Åcally that we reset the his- ‚Äôstay‚Äô actions), the state transition matrix may be periodic and no
tory, i.e., h = ‚àÖ, and run œÄ starting from a state s distributed stationary distribution may exist, e.g. if the states deterministically
according to B.                                       alternate between states 1 and 2.that H is a linear operator, we have
                                                        Input  : H /*a (1/2, KH ) approximate reset strategy */
   kH` (B ) ‚àí H` (B )]k                                 for t = 1 to ‚àû do
      E  1     E   2   1                                   /*Exploration in Phase t */;
     =   kH  (Q ) ‚àí H  (Q )]k
           E   1     E   2  1                               t      1     2    
                                                           k1 = O   2 log(t |Œ†t|) ;
          X                                                         t
     =   k   (Q1(s) ‚àí Q2(s))HE(s)k1
                                                           foreach Policy œÄ in Œ†t do
           s                                                              t
                                                              for i = 1 to k1 do
          X                     0
     =   k   (Q1(s) ‚àí Q2(s))HE(s )k1                              Run œÄ for t steps;
           s                                                      Repeatedly run H for log(1/t) times;
            X
         +k    (Q (s) ‚àí Q (s))(H (s) ‚àí H (s0))k               end
                 1       2      E       E     1               Let vœÄ be the average return of œÄ in from these
             s                                                  t
            X                                                 k1 trials;
     =   0 +    |Q1(s) ‚àí Q2(s)|                           end
             s                                             /*Exploitation in Phase t */;
         `                                                      ‚àó               œÄ
     ‚â§                                                    Let œÄÀÜt = arg maxœÄ‚ààŒ†t v ;
                                                                  
                                                           kt = O  1 ([current time T]
where the Ô¨Årst term is 0 since for any two distributions    2      t
P                                                0                                                 
  s Q1(s) ‚àí Q2(s) = 1 ‚àí  1 =  0 (and the vector H(s )              +[time in t + 1-th exploration phase]) ;
is constant in this sum), and we have used the fact that               t
kH(s) ‚àí H(s0)k ‚â§                H                        for i = 1 to k2 do
              1    (by deÔ¨Ånition of ).                              ‚àó
  We now  show  that the random walk strategy (includ-        Run œÄÀÜt for t steps;
ing ‚Äôstay‚Äô actions) is an approximate reset strategy in every Repeatedly run H for log(1/t) times;
POMDP (including periodic ones), though with prior knowl-  end
                                                        end
edge we might have better approximate reset strategies at our
disposal.                                                         Algorithm 1: Policy Search
Lemma 3.2  For all POMDPs, the random walk strategy (us-
                                                                                           ‚àó
ing ‚Äôstay‚Äô actions) constitutes an (, k) approximate reset the homing sequence between every run of œÄÀÜt , asymptotically
                               1
strategy for some k ‚â• 1 and 0 <  < 2 .               we never stop homing. Nonetheless, we able to show that
                                                      there exists an algorithm which obtains near optimal reward
  Proof: By our connectivity assumption, for all states s and
                                                      in a POMDP, since the ratio of the time spent exploiting vs.
s0, there exists some strategy that reaches s0 from s with pos-
                                                      homing decreases sufÔ¨Åciently fast.
itive probability. This implies that under H (the random walk
strategy), there is positive probability of moving from one Theorem 4.1 There exists an algorithm A, such that in any
state to another, i.e. the Markov chain is irreducible. Fur- connected POMDP, A obtains the optimal average reward in
thermore, since H performs ‚Äôstay‚Äô actions, then the Markov the limit, with probability 1.
chain is aperiodic. Thus, there exists a unique stationary dis- We later provide an algorithm with a better convergence
tribution. We choose k to be the time at which the error in rate (see Theorem 4.5). However, we start with a simpler pol-
convergence is less than 1/2, from all starting states. Hence, icy search algorithm which establishes the above Theorem.
by linearity of expectation, the error is less than 1/2 from all
belief states in k steps.                            4.1  Policy Search

                                                      Algorithm 1 takes as input a (1/2, KH )-approximate reset
4  Reinforcement Learning with Homing                 strategy, which could be the random walk strategy with a
We now  provide two algorithms which demonstrate how  very crude reset. The algorithm works in phases, interleav-
near-optimal average reward can be obtained, with different ing exploration phases with exploitation phases. Let us start
rates of convergence. The key to the success of these algo- by describing the exploration phases. Let Œ†t be the set of
rithms is their use of homing sequences in both exploration all t-Markov strategies. An estimate of the value of a policy
and in exploitation. For exploration, the idea is that each œÄ ‚àà Œ†t can be found by Ô¨Årst resetting and then running œÄ for t
time we attempt some exploration trajectory we do it after steps. The exploration phase consists of obtaining an estimate
                                                       œÄ
implementing our reset strategy ‚Äî hence our information is v of the return of each policy œÄ ‚àà Œ†t, where each estimate
(approximately) grounded with respect to the belief state BH consists of an average of k1 trials (followed by approximate
(recall HE(BH ) = BH ). The idea of exploration is to Ô¨Ånd resets).
                       ‚àó
a good t-Markov strategy œÄÀÜt from BH . During exploitation, These estimates have both bias and variance. The variance
the goal is to use this t-Markov strategy. Unfortunately, we is just due to the stochastic nature of the POMDP. The bias is
                       ‚àó
have only guaranteed that œÄÀÜt performs well starting from BH due to the fact that we never can exactly reset to BH . How-
and only for t steps. Hence, after each time we exploit with ever, if we run H for log 1/t times (where t is an error pa-
 ‚àó
œÄÀÜT , we run our homing sequence to get back close to BH (and rameter in the t-th phase, which will be Ô¨Åxed latter) then, by
             ‚àó
then we rerun œÄÀÜt ). We gradually increase t in the process. lemma 3.1, all expected belief states we could approach will
                                                              log 1/t
  The problem is that while homing, we are wasting time and be (1/2) = t close to BH . The following lemma
neither exploiting nor exploring. Furthermore, since we use shows that accurate estimates can be obtained.                                        
                       t      1     2                 than previous amount of time spent in the MDP time plus the
Lemma 4.2  In phase t, if k1 = O 2 log(t |Œ†t|) and each
                              t                       amount of time that will be spent in the next exploration phase
reset consists of using the homing sequence log 1/ times,
                                            t         (this latter factor accounts for the case in which time T lies in
then for all policies œÄ ‚àà Œ†t, the estimated t-horizon reward,
 œÄ           œÄ        œÄ                               the exploration phase immediately after t).
v , satisÔ¨Åes |Rt (BH ) ‚àí v | ‚â§ 2t with probability greater
than 1 ‚àí 1 .                                            Now we bound the average reward obtained in the exploita-
        2t2                                           tion phase. First. let us show that the t-average reward of the
                                                                   ‚àó                     ‚àó
  Proof: First, let us deal with the bias. Any expected belief    œÄÀÜt         ‚àó         œÄÀÜt
                                                      policy used, Rt , satisÔ¨Åes Rt (BH ) ‚àí Rt (BH ) ‚â§ 4t with
states b that results from using the homing sequence log 1/t              1
                                                      probability at least 1 ‚àí 2 . By Lemma 4.2 for each policy
                                log 1/t                                   2t
times must satisfy kb ‚àí BH k ‚â§ (1/2)  ‚â§ t. Now it is                  œÄ         œÄ
                                                      œÄ ‚àà Œ†t, we have |Rt (BH ) ‚àí v | ‚â§ 2t with probability at
straightforward to see that if b and BH are belief states such  1              ‚àó
                                                      least 1 ‚àí    2 . Therefore, œÄÀÜt is 4t-optimal with probabil-
that kb ‚àí B k  ‚â§ , then for every strategy œÄ, |RœÄ(b) ‚àí       2|Œ†t|t
          H  1                              t            1 ‚àí 1/(2t2)                             œÄÀÜ‚àó
RœÄ(B  )| ‚â§ . To see this, let the belief states at time t be bt ity . Now the observed average return of t in the
 t   H                                                                  2        R‚àó(B  )
and Bt , which result from following either œÄ starting from exploitation period is t close to t H with probability at
     H                                                least 1‚àí 1 , since our observed average return in exploitation
b or B , respectively. By linearity of expectation, it follows 2t2
     H                                                                                œÄÀÜ‚àó      t    t
      t    t                                 œÄ        is least as good as those used to Ô¨Ånd v t (since k > k ).
that kb ‚àí BH k1 ‚â§ . This directly implies that |Rt (b) ‚àí                                      2    1
 œÄ
Rt (BH )| ‚â§ .                                          However, the average return during the exploitation phase
                                                  t                                        ‚àó
  For the variance, the Hoeffding bound and our choice of k1 is not the observed average return of œÄÀÜt , since we re-
imply that the average return of each policy is t close to its set after each t exploitation steps for a number of steps
expectation (the expectation is both over the initial state and that is KH log(1/t). The resets in the exploitation pe-
on the policy trajectory) with probability 1 ‚àí 1/(2t2).  riod can change the average reward by at most a fraction
                                                 ‚àó    1                                                
  Now during exploitation, the algorithm uses the policy œÄÀÜt t KH log(1/t).
which had the highest return in the exploration phase (and by
the previous lemma this is close to the policy with largest re- 4.2 A Model Based Algorithm
turn). Note that we have only guaranteed a large return for
                                                      The previous algorithm was the simplest way to demonstrate
executing œÄÀÜ‚àó from B for t steps. However, we would like to
         t       H                                    Theorem 4.1. However, it is very inefÔ¨Åcient, since it is testing
exploit for a longer period of time than t. The key is that we
                                                      all t-Markov policies ‚Äî there are doubly exponential, in t,
again reset log(1/ ) times after each time we run œÄÀÜ‚àó, which
                t                          t          such polices2. Here, we provide a more efÔ¨Åcient model based
resets us close t close to BH . Unfortunately, this means we
                                       ‚àó              algorithm, which resembles the algorithms given in Kearns
spend KH log 1/t steps between each run of œÄÀÜ . Hence, our
                      1                               et al. (1999); McAllester and Singh (1999), and is exponen-
average return could be O( t KH log 1/t) less than we would
           1                                          tial in the horizon time, yet it still has no dependence on the
like, since O( t KH log 1/t) is the fraction of time we spend number of states in the POMDP.
resetting. Note that this fraction could be large if we desire We now state a convergence rate in terms of œÑ, the -
that t be very small (thought this would guarantee very ac- horizon time of an optimal policy (see Section 2) and and
curate resets).                                       in terms of the homing time K (recall, such a time exists
  Now, when we do exploit (and reset), we run the exploita-                      H
                          t                           for every POMDP using a random walk policy).
tion phase long enough (for k2 time) such that our overall
average reward is comparable to the average reward in the Theorem 4.5 There exists an algorithm A, such that in any
last exploitation phase.                              connected POMDP and with probability greater than 1 ‚àí Œ¥,
Lemma 4.3  At any time T after phase t, the average reward A achieves an average reward that is 2 close to the optimal
from time 1 to time T satisÔ¨Åes: 1 PT r ‚â• R‚àó(B  ) ‚àí    average reward in a number of steps in the POMDP which is
                            T   i=1 i     t   H       polynomial in |A|,|O|,K and log(1/Œ¥) and exponential in
O( +  1 K log(1/ ))                    1 ‚àí 1                             H
   t   t H        t  with probability at least t2 .   œÑ. Furthermore the computational runtime of this algorithm
  Before proving this lemma, let us state a corollary from is polynomial in |A|,|O|, and log(1/Œ¥) and exponential œÑ.
which Theorem 4.1 follows.
                                                        We provide such an algorithm in the next page. In explo-
                             1 PT          ‚àó
Corollary 4.4 Let t = 1/t. Then T i=1 ri ‚â• Rt (BH ) ‚àí ration phase, the algorithm builds an approximate model of
  KH log(t)                         1                 the transition probabilities after some history has occurred
O(    t   ) with probability at least 1 ‚àí t2 .
                                                      starting from BH . In the t-th phase, it builds a model with
  Importantly, note the loss term goes to 0 as t goes to inÔ¨Ån- respect to the set of all t-length histories, which we denote by
ity. Furthermore, for a large enough phase t, we know that
 ‚àó                                          ‚àó         Ht. In the exploitation phase, it uses the best t Markov strat-
Rt (BH ) will approach the optimal average reward R (since
 ‚àó                                                    egy with respect to this model. The use of homing strategies
R  is independent of the starting B). Theorem 4.1 follows. is similar to that in the previous algorithm.
Essentially, although we have to home inÔ¨Ånitely often, the ra- Let L = |A||O|, and note that 2Lt ‚â• |H |. In the ex-
tio of time spent homing to the time spent using our t-step                                  t
                             log t                    ploration phase, the algorithm takes actions uniformly at ran-
exploitation policies is going as O( t ), which goes to 0. dom for t steps and then resets (running the homing strategy
  Proof:  First, let us show that the average reward,
    T
1 P           ‚àó                                          2
T   i=1 ri ‚â• Rt (BH ), is no less than t from the average The number of histories of length t is exponential in t, and the
reward obtained in the t-th exploitation phase. To do this, we number of t-Markov polices is exponential in the number of t-length
                             t
set the time of exploitation phase, k2, to be 1/t times greater histories                                                      Pr[ÀÜ o|h, B0, a]|, is then
  Input : H /*an (1/2, KH ) approximate reset strategy */
  Let L = |A| ¬∑ |O|;                                                              
                                                      Pr[h(a, o)|B ] Pr[ÀÜ h(a, o)|B ]
  for t = 1 to ‚àû do                                              0 ‚àí            0 
             4t          
      t      L       2                                 Pr[h|B ]Pr[ÀÜ a] Pr[ÀÜ h|B ]Pr[ÀÜ a] 
     k1 = O   2 log(t |Ht|) ;                               0               0     
              t
         t                                                                       t                
     for k times do                                           1  Pr[h(a, o)|B0] + L2t Pr[h(a, o)|B0]
         1                                              ‚â§                           ‚àí              
        Run RANDOM  for t steps;                                               t
                                                            Pr[ÀÜ a]  Pr[h|B0] ‚àí 2t      Pr[h|B0]   
            H    K   log(Lt/ )                                                L
        Run   for  H         t steps.                                  t                          t   
                                                              1       L2t           2 Pr[h(a, o)|B0] L2t 
     end                                                =                      +                        
                                                             ÀÜ               t                       t
     for h ‚àà Ht, a ‚àà A and o ‚àà O do                         Pr[a] Pr[h|B0] ‚àí L2t Pr[h|B0](Pr[h|B0] ‚àí L2t )
         ÀÜ
        Pr[o|h, B0, a] = 0;                                 2|A|
           ÀÜ              t                            ‚â§       ,
        if Pr[h(a, o)|B0] ‚â• Lt then                          Lt
                          ÀÜ
            ÀÜ             Pr[h(a,o)|B0]
           Pr[o|h, B0, a] = ÀÜ   ÀÜ                                                                  1
                          Pr[h|B0]Pr[a]               where the Ô¨Årst inequality holds with probability 1 ‚àí t2 , and
                                                                                                    t 
        end                                           in the last inequality we used the fact that Pr[h|B0] ‚â• Lt .
     end                                                The exploitation policy can be found using dynamic pro-
              ‚àó      ÀÜ
     Compute œÄÀÜt using Pr[o|B0, h, a];                gramming with the model. Note that the POMDP is equiva-
      t     1
     k2 = O   ([current time T]                       lent to an MDP where the histories are states. In the exploita-
             t                                                                                        ‚àó
                                                     tion phase, the algorithm uses the best t-Markov policy, œÄÀÜt ,
            +[time in t + 1-th exploration phase]) ;  (with respect to the approximate model) interleaving it with
         t                                            KH  log(1/t) homing steps.
     for k2 times do
        Run œÄÀÜ‚àó for t steps;                                                                    ‚àó
             t                                        Lemma 4.7  In phase t, the exploitation policy œÄÀÜt , satisÔ¨Åes
                                                                   ‚àó
        Run H for KH log(1/t) steps;                   ‚àó         œÄÀÜt               2|A|         2t
                                                      |Rt (BH ) ‚àí Rt (BH )| ‚â§ t(t + Lt ) + (2t + Lt ) with
     end                                                                  1
  end                                                 probability at least 1 ‚àí t2 .
            Algorithm 2: Model based                    Proof: (sketch) We observe that by ignoring all histories
                                                                                            ÀÜ         t
                                                      (which we view as nodes in a tree) such that Pr(h|B0) ‚â§ Lt ,
                                                      the return of an optimal strategy in this empirical model is
        t                       t      3                                2t( + t )
for log(L /t) times). This is done k times. Then using decreased by at most t Lt , due to the fact that the true
                                1                                                  t    t
the empirical frequencies in these trajectories the algorithm history probability is bounded by Lt + L2t , the return from
              ÀÜ                                       each node is bounded by t and the total number of such nodes
forms estimates Pr[o|h, BH , a], which is just the empirical         t
probability of observing o conditioned on history h followed is bounded by 2L . Next we prove that the return of the opti-
                                                                                               2     2|A|
by taking action a. For histories h which are unlikely, these mal policy in the empirical model loses at most t (t + Lt )
empirical estimates could be very bad, though, as we shall due to the tree approximation on the other nodes (the other
                                                      histories). Using backward induction, we show that the pol-
see, we do not need accurate estimates of Pr[ÀÜ o|h, BH , a] for
                                                          ‚àó                     2     2|A|
such histories. Let h(a, o) be a history with h followed by icy œÄÀÜt has return not less than k (t + Lt ) in comparison to
(a, o).                                               the true optimal value, starting from (t ‚àí k + 1)-length his-
                                                      tories. The base case, for the leaves (the t-length histories),
                                                      holds since the reward (which is encoded through the obser-
                                                                         2|A|
                                 4t         
                        t       L       2             vations) is within t + Lt , where the Ô¨Årst error is due to the
Lemma 4.6  In phase t, if k1 = O  2 log(t |Ht|) and
                                 t                   imperfect reset and the second is due to the marginal distribu-
                                              t
each reset consists of using the homing sequence log(L /t)                   2|A|
                                                      tion error that is bounded by Lt by Lemma 4.6. Assume the
               ÀÜ                     t
times, then: (1) |Pr[h|B0] ‚àí Pr[h|B0]| ‚â§ L2t , and (2) for induction assumption holds for k ‚àí 1. There are two sources
                                         t
every h(a, o) ‚àà Ht such that Pr[h(a, o)|B0] ‚â• Lt , we have of error, the Ô¨Årst is due to the current estimation error (of both
 ÀÜ                            2|A|                    the marginal distribution and the immediate reward) which is
|Pr[o|h, B0, a] ‚àí Pr[o|h, B0, a]| ‚â§ Lt , with probability at
        1                                             bounded by ( + 2|A| )k and the second is due to errors from
least 1 ‚àí t2 .                                                    t   Lt
                                                                                            2     2|A|
                                                      the previous levels and is bounded by (k ‚àí 1) (t + Lt ) by
                                              2       the induction assumption. Summing the terms completes the
  Proof: We Ô¨Årst note that, with probability 1 ‚àí 1/t , for induction step.                             
                           ÀÜ                    t
every history h ‚àà Ht we have |Pr[h|B0] ‚àí Pr[h|B0]| ‚â§ L2t Similarly to Subsection 4.1, we exploit long enough such
(using the Hoeffding bound). The error, | Pr[o|h, B0, a] ‚àí that the overall average reward is essentially the average re-
                                                      ward in the last exploitation period.
                                                      Lemma 4.8  At any time T after phase t, the average reward
  3We can use in the algorithm any approximate homing strategy                     1 PT          ‚àó
                                                      from time 1 to time T satisÔ¨Åes: T i=1 ri ‚â• Rt (BH ) ‚àí
H. However, if H is simply the random policy, then the reset and |A|t
exploration would both use the same policy, and the algorithm would O(tt + Lt + (1/t)KH log(1/t)) with probability at least
                                                          1
slightly simplify.                                    1 ‚àí t2 .