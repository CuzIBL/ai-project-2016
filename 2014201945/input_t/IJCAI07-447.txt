 Computation of Initial Modes for K-modes Clustering Algorithm using Evidence 
                                        Accumulation 

                                        Shehroz S Khan 
                              National University of Ireland Galway, 
                             Department of Information Technology, 
                                   Galway, Republic of Ireland 
                                     s.khan1@nuigalway.ie 

                                         Dr. Shri Kant 
                                    Scientific Analysis Group, 
                                   Defence R&D Organization,  
                               Metcalfe House, Delhi, India-110054 
                                     shrikant@scientist.com 
                                                 
                   Abstract                       [Guha et al, 1998]. The most distinct characteristics of 
                                                  clustering operation in data mining is that the data sets 
    Clustering accuracy of partitional clustering al- often contain both numeric and categorical attribute val-
    gorithm for categorical data primarily depends ues. This requires the clustering algorithm to be capable 
    upon the choice of initial data points (modes) to 
                                                  of dealing with the complexity of the inter- and intra-
    instigate the clustering process. Traditionally ini- relation of the data sets expressed in different types of the 
    tial modes are chosen randomly. As a conse-
                                                  attributes, no matter numeric or categorical [Michalski et 
    quence of that, the clustering results cannot be al, 1998]. The K-means algorithm [Jain and Dubes, 1988] 
    generated and repeated consistently. In this paper is one of the most popular clustering algorithms because 
    we present an approach to compute initial modes of its efficiency in clustering large data sets [Anderberg, 
    for K-mode clustering algorithm to cluster cate-
                                                  1973]. However, K-means clustering algorithm fails to 
    gorical data sets. Here, we utilize the idea of Evi- handle data sets with categorical attributes because it 
    dence Accumulation for combining the results of 
                                                  minimizes the cost function that is numerically measured.  
    multiple clusterings. Initially, n F − dimensional K-means does not guarantee unique clustering because 
    data is decomposed into a large number of com- we get different results with randomly chosen initial clus-
    pact clusters; the K-modes algorithm performs ter centers [Sing-Tze Bow, 2002] and hence the results 
    this decomposition, with several clusterings ob-
                                                  cannot be relied with confidence. The K-means algorithm 
    tained by N random initializations of the K-  gives better results only when the initial partitions are 
    modes algorithm. The modes thus obtained from 
                                                  close to the final solution [Jain and Dubes, 1988]. Several 
    every run of random initializations are stored in a attempts have been reported to generate K-prototype 
    Mode-Pool, PN. The objective is to investigate points that can be used as initial cluster centers. A recur-
    the contribution of those data objects/patterns sive method for initializing the means by running K clus-
    that are less vulnerable to the choice of random 
                                                  tering problems is discussed by Duda and Hart [1973]. 
    selection of modes and to choose the most di- Bradley et al [1997] reported that the values of initial 
    verse set of modes from the available Mode-Pool 
                                                  means along any one of the m coordinate axes are deter-
    that can be utilized as initial modes for the K- mined by selecting the K densest "bins" along that coordi-
    mode clustering algorithm. Experimentally we  nate. Bradley and Fayyad [1998] proposed a procedure 
    found that by this method we get initial modes that refines the initial point to a point likely to be close to 
    that are very similar to the actual/desired modes 
                                                  the modes of the joint probability density of the data. Mi-
    and gives consistent and better clustering results tra et al [2002] suggested a method to extract prototype 
    with less variance of clustering error than the 
                                                  points based on Density Based Multiscale Data Condensa-
    traditional method of choosing random modes.  tion. Khan and Ahmad [2004] presented an algorithm to 
                                                  compute initial cluster centers for K-means clustering 
1   Introduction                                  algorithm. Their algorithm is based on two experimental 
Clustering is one of the most useful tasks in data mining observations that some of the patterns are very similar to 
process for discovering groups and identifying interesting each other and that is why they have same cluster mem-
distributions and patterns in the underlying data. Cluster- bership irrespective to the choice of initial cluster centers. 
ing problem is about partitioning a given data set into Also, an individual attribute may provide some informa-
groups (clusters) such that the data points in a cluster are tion about initial cluster center. The initial cluster centers 
more similar to each other than points in different clusters computed by using their methodology are found to be 


                                            IJCAI-07
                                              2784very close to the desired cluster centers with improved clustering this new similarity matrix, corresponding to the 
and consistent clustering results.                merging of cluster [Fred, 2001]. Topchy et al [2003] pre-
Various clustering algorithms have been reported to clus- sented an algorithm to combine multiple weak clusterings 
ter categorical data. Ralambondrainy [1995] presented an and formulated that combined clustering becomes equiva-
approach by using K-means algorithm to cluster categori- lent to clustering a categorical data based on some chosen 
cal data. The approach is to convert multiple category consensus function. They showed efficacy of combining 
attributes into binary attributes (using 0 and 1 to represent partitions generated by weak clustering algorithms that 
either a category absent or present) and treat the binary uses random data splits. All of this research work is based 
attributes as numeric in the K-means algorithm. Gower on numerical data.  In this paper, we extend the idea of 
and Diday [1991] used a similarity coefficient and other Evidence Accumulation to categorical data sets by gener-
dissimilarity measures to process data with categorical ating multiple partitions as different data organization by 
attributes. The K-mode clustering algorithm [Huang, seeding K-modes algorithm, every time, with random ini-
1997] extends the K-means paradigm to cluster categori- tial modes. The resultant modes are then stored in a Mode 
cal data by using a simple matching dissimilarity measure Pool and the most diverse set of modes were computed, 
(hamming distance) for categorical objects and modes which were used as initial modes.   
instead of means for clusters.                      The rest of the paper is organized as follows. Section 2 
  Most of the above mentioned algorithms for clustering briefly discusses the K-modes algorithm. Section 3 de-
categorical data require a random selection of initial data scribes the proposed approach in computing the initial 
points in addition to apriori knowledge of number of clus- modes of the data sets using Evidence Accumulation. Sec-
ters (K).  This leads to the problem that clustering results tion 4 presents the experimental results on applying the 
are dependent on the selection of initial modes. Choosing proposed approach to compute initial modes for different 
different initial modes lead to different cluster structures categorical data sets [UCI data repository] and demon-
and hence the clustering results cannot be repetitively strates improved and consistent clustering results. Section 
generated. Furthermore, inappropriate choice of initial 5 concludes the presentation. 
modes leads to undesirable clustering results. Machine 
learning practitioners find it difficult to count on such 2 The K-modes algorithm for clustering 
clustering results.  
  Zhexue Huang [1998] presented two methods of ini-  categorical data 
tialization for categorical data for K-mode clustering algo- The K-means clustering algorithm cannot cluster categori-
rithm and showed that if diverse initial modes are chosen cal data because of the dissimilarity measure it uses. The 
then it could lead to better clustering results. Sun et al K-modes clustering algorithm is based on K-means para-
[2002] proposed an iterative method based on initial digm but removes the numeric data limitation whilst pre-
points refinements algorithm for categorical data cluster- serving its efficiency. The K-modes algorithm extends K-
ing to the setting up of the initial points so as to map the means paradigm to cluster categorical data by removing 
categorical data sets to clustering results that have better the limitation imposed by K-means through following 
consistency rates. They applied Bradley and Fayyad’s  modifications:  
iterative initial point refinement algorithm [Bradley and • Using a simple matching dissimilarity measure or the 
Fayyad, 1998] to the K-modes clustering to improve the hamming distance for categorical data objects 
accuracy and repetition of clustering results. They used 
                                                    •
sub-sampling method to carry the clustering iteratively  Replacing means of clusters by their modes 
several times so that effect of skewed distributed data The simple matching dissimilarity measure [Jain and 
should not affect the final clustering results. Khan S.S. et Dubes, 1988] can be defined as following. Let X and Y be 
al [Khan and Ahmad, 2003] presented an algorithm to two categorical data objects described by F categorical 
compute initial modes using Density based Multiscale attributes. The dissimilarity measure d()X ,Y  between X 
Data Condensation. They showed that by choosing initial and Y can be defined by the total mismatches of the corre-
modes this way consistent and efficient clustering results sponding attribute categories of two objects. Smaller the 
were achieved.                                    number of mismatches, more similar the two objects are. 
  Kant et al [1994] presented an Automatic and Stable Mathematically, we can say 
Clustering Algorithm for clustering numerical data. They   F
                                                   ()=      δ ()
showed stable clustering of data by repetitively clustering d X ,Y ∑ x j , y j    (2.1) 
the same data with random initializations to generate sta-  j=1
ble cluster regions such that each pattern fits exactly into     ⎧0    (x = y )
                                                       δ ()=     ⎨      j    j
one of those regions and no single pattern can be fitted in where x j , y j ()≠  
two  clusters regions. More recently, [Fred and Jain,            ⎩1    x j y j
2002a] used the idea of evidence clustering to combine d()X ,Y  gives equal importance to each category of an 
the results of multiple clusterings (N times) into a single attribute. 
data partition, by viewing each clustering result as an in- Let Z be a set of categorical data objects described by 
                                                                           K
dependent evidence of data organization.  They did so by categorical attributes, A1,,A2, AF  a mode of 
                                                    = {}K                      = []K
running a K-means algorithm many times with different Z Z1 , Z 2 , Z n  is a vector Q q1, q2 , qF  that 
parameters or initializations. First, the data is split into a minimizes  
large number of compact and small clusters; different de-  n
                                                    ()=       (    )
compositions are obtained by random initializations of the D Z, Q ∑ d Z i , Q    (2.2) 
K-means algorithm. The final data partition is obtained by  i=1


                                            IJCAI-07
                                              2785Here, Q is not necessarily an element of Z. When the 1. Set K→Number of clusters present in the data set, 
above is used as the dissimilarity measure for categorical N→Number of clusterings 
data objects, the cost function becomes           2. Choose a value of number of clusters (K), i=1. 
       k n F                                      3. While (i ≤ N) do the following 
     =       δ ()
 C(Q)  ∑∑∑     zij , qlj    (2.3)                   (a) Choose random initial modes and execute K-modes 
       l==11i j = 1                                    algorithm; till it converges and create K partitions. 
         = []K        ∈
where Ql   ql1, ql2 , qlm Q                         (b) Store the K modes thus obtained (from each of the 
                                                       K partitions) in a Mode-Pool, Pi  
  The K-modes algorithm minimizes the cost function de- (c) Increment i. 
fined in equation 2.3. 
The K-modes algorithm consists of the following steps: - 3.2 Extracting Initial Modes from Mode-Pool 
a)  Select K initial modes, one for each of the cluster. After the execution of algorithm discussed in 3.1, we are 
b)  Allocate data object to the cluster whose mode is left with a Mode-Pool, PN with N K×F modes (F is the 
    nearest to it according to equation 2.1.      number of attributes of the data set). To extract the most 
c)  Compute new modes of all clusters.            diverse modes, employ this following consensus algo-
d)  Repeat step 2 to 3 until no data object has changed rithm 
    cluster membership.                           1. Set i=1, j=1, k=1 
                                                  2. While (i ≤ K) do the following 
3   Computing Initial Modes Using Evidence        3. While (j ≤ F) do the following 
                                                            ≤
    Accumulation                                  4. While (k  N ) do the following 
                                                  5. Extract the most frequent mode and store it in the Ini-
The idea of Evidence Accumulation clustering [Fred and tial Modes Matrix, Ii×j 
Jain, 2005] is to combine the results of multiple cluster- 6. Increment k 
ings into a single data partition, by viewing each cluster- 7. Increment j 
ing result as an independent evidence of data organization. 8. Increment i 
Fred and Jain [2002] used the K-means algorithm as the  
basic algorithm for decomposing the data into a large The modes generated by each N clusterings are mostly 
number of compact clusters; evidence on pattern associa- representative of those data objects/patterns that are less 
tion is accumulated, by a voting mechanism, over N clus- vulnerable to change cluster membership irrespective of 
terings obtained by random initializations of the K-means the choice of random initial mode selection. After extract-
algorithm. There are several possible ways to accumulate ing the frequent modes, IK×F, from the Mode-Pool, PN, we 
evidence in the context of unsupervised learning: (a) com- shall have captured representations mostly from those 
bine results of different clustering algorithms; (b) produce patterns only. The modes thus obtained for each of the K 
different results by re-sampling the data, such as in boot- partitions should be quite dissimilar from each other with 
strapping techniques (like bagging) and boosting; (c) run- more diversity embodied in them. 
ning a given algorithm many times with different parame-
ters or initializations. In this paper we take the last ap- 4 Experimental Results 
proach, using K-modes algorithm as the underlying clus-
tering for creating multiple partitions of the categorical To test our approach we use the following categorical data 
data.                                             sets obtained from UCI Machine Learning Data Reposi-
  Khan  and Ahmad [2004] presented that in a data set tory [UCI data repository] 
there are some data objects that do not change class mem- (1) Michalski soybean disease data set  [Michalski and 
bership irrespective of the choice of initial point. In other Stepp, 1983] 
words they belong to same clusters irrespective of choice The soybean disease data set consists of 47 cases of soy-
                               = {}K              bean disease each characterized by 35 multi-valued cate-
of initialization. For example, let Di Di1 , Di2 DiF  
be a dataset consisting of n data objects with F attributes. gorical variables. These cases are drawn from four popu-
Let us assume that data objects Dk1, Dk2, Dk3, where 1≤ lations, each one of them representing one of the follow-
k1, k2, k3 ≤ n are very similar, then they have same clus- ing soybean diseases: D1-Diaporthe stem canker, D2-
ter membership whenever K-modes algorithm is executed Charcoat rot, D3-Rhizoctonia root rot and D4-
with different initial modes. This information is quite use- Phytophthorat rot. Ideally, a clustering algorithm should 
ful to compute initial modes. The two major steps of our partition these given cases into four groups (clusters) cor-
algorithm are                                     responding to the diseases. 
(a) Generate N independent evidences of data organiza- (2) Wisconsin Breast Cancer Data 
   tions by performing K-modes clustering using random This data has 699 instances with 9 attributes. Each data 
   initialization of modes and store the resultant modes of object is labeled as benign (458 or 65.5%) or malignant 
   each of the N iteration in a Mode-Pool, PN.    (241 or 34.5%). In our literature, all attributes are consid-
(b) Find the most diverse modes for each cluster to be ered categorical with values 1, 2… 10. There are 16 in-
   used as initial modes                          stances in Groups 1 to 6 that contain a single missing (i.e. 
                                                  unavailable) attribute value, denoted by "?".  For data 
3.1 Generating Independent Data Organization      symmetry we took 241 benign case and 241 malignant 
We  assume that the choice of numbers of clusters (K) is cases for out analysis. 
the same as the number of natural groupings present in the (3) Zoo small data 
data set. The algorithmic steps are:   

                                            IJCAI-07
                                              2786It has 101 instances distributed into 7 categories. This data the actual/desired modes for these data sets and therefore 
is same as Zoo data but with only the important eight at- better clustering and fast convergence was achieved. And 
tributes (feathers, milk, airborne, predator, backbone, fins, since K-mode is executed large number of times (N=100), 
leg and tail). All of these characteristics attributes are the weak clustering results were eliminated and most of 
Boolean except for the character attribute corresponds to the time we get representations from those patterns that 
the number of legs that lies in the set {0, 2, 4, 5, 6, 8} are less susceptible to random selection of modes and 
(4) Congressional Vote Data                       therefore we get repetitive initial modes that leads to con-
This data set includes votes for each of the U.S. House of sistent clustering results with less variance in clustering 
Representatives Congressmen on the 16 key votes identi- error. 
fied by the CQA.  The CQA lists nine different types of  
votes: voted for, paired for, and announced for (these                         Proposed method of 
three simplified to YES), voted against, paired against,      Random Initializa- Initialization using 
and announced against (these three simplified to NO),           tion of modes   Evidence Accumu-
                                                                                     lation 
voted present, voted present to avoid conflict of interest, Data Set 
and did not vote or otherwise make a position known            Avg.             Avg. 
(these three simplified to an unknown disposition). All       Clus-   Standard  Clus-  Standard 
attributes are Boolean with Yes (denoted as y) and No         tering  Deviation tering Deviation 
(denoted as n) values. A classification label of Republican   Error             Error 
or Democrat is provided with each record. The dataset Soybean 0.055     1.89 0.021       1.02 
contains 435 records with 168 Republicans and 267 De- Wisconsin 
mocrats                                              Breast   0.155     1.77 0.132 0.44 
  In the presence of true labels, as in the case of the data Cancer 
sets we used, the clustering accuracy for measuring the 
clustering results was computed as follows. Given the Zoo small 0.162 0.966 0.166 0.54 
                                                    Congres-
final number of clusters, K, clustering accuracy r was de-    0.141 4.52 0.132 0.707 
fined as:              K                           sional Vote 
                       ∑ ai                        
                       =
                    r = i 1                       Table 1. Clustering Error and Standard Deviation comparison 
                        n                         using random initialization of modes and modes supplied using 
                                                  the proposed approach 
where n is the number of patterns in the dataset, ai is the 
number of data objects occurring in both cluster i and its  
corresponding class, which had the maximal value. In                          Error Standard
                                                    18                        Deviation
other words, ai is the number of records with the class                       %age Clustering Error
label that dominates cluster i. Consequently, the clustering  16
error is defined as                                 14
                    e = 1− r                        12
Low value of e suggests better clustering.          10

  To conduct experimental comparison and to verify the  8
efficacy of our proposed method, we supplied initial ran-  
                                                     6
dom modes  to the K-modes algorithm as suggested by  
huang [1997]. Table 1 compiles the clustering results on  4
the categorical data sets (described above) using random  2
initial modes and the modes supplied by our proposed  0
approach using Evidence Accumulation. Results pre-     Soybean W.B. Zoo Small Vote
sented for our approach are based on combination of          Cancer
N=100 K-modes clusterings, a considerable high value to                                        
ensure that convergence of the method is ensured. The Figure 1. Graphical Representation of Clustering Error and 
reported clustering error and standard deviation is average Standard Deviation using Random Selection of Modes  
of 50 executions of the whole process. It can be seen that  
the clustering results have improved with less standard  
deviation in error when the modes were chosen by our 
proposed method in comparison to the random selection 
of initial modes.  
  Figure 1 and 2 represents these results graphically. Fig-
ure 1 show the clustering error and its standard deviation 
when initial modes were randomly chosen. Figure 2 
shows the same statistics when initial modes were picked 
up using our proposed method based on Evidence Accu-
mulation and were fed to the K-mode clustering algo-
rithm. A reduced clustering error with less variance can be 
seen from figure 2.  
  One important observation was that the initial modes 
computed by our proposed approach were quite similar to 


                                            IJCAI-07
                                              2787                            Error Standard           Advances in Neural Information Processing systems 9, 
   18
                            Deviation                MIT Press, 368-374, 1997. 
                            %age Clustering Error
   16                                             [Duda and Hart, 1973]Duda, R.O., Hart, P.E., Pattern 
   14                                                classification and Scene Analysis, John Wiley and 
   12                                                Sons, N.Y. 1973. 
   10                                             [Fred and Jain, 2002] ]Fred, A., Jain, A.K.  Evidence Ac-
   8                                                 cumulation Clustering based on the K-means algo-

   6                                                 rithm, in Proceedings of the International Workshops 
                                                     on Structural and Syntactic Pattern Recognition 
   4                                                 (SSPR), Windsor, Canada, August 2002. 
   2
                                                  [Fred and Jain, 2002a] Fred, A., Jain, A.K.: Data Cluster-
   0                                                 ing Using Evidence Accumulation, in Proceedings of 
     Soybean W.B. Zoo Small Vote
            Cancer                                   the International Conference on Pattern Recognition 
                                                     (ICPR), Quebec City, August 2002. 
Figure 2. Graphical Representation of Clustering Error and [Fred and Jain, 2005] Fred, A., Jain, A.K. Combining 
Standard Deviation using Proposed Approach for Initial Mode Multiple Clustering Using Evidence Accumulation. 
Computation                                          IEEE Transactions on Pattern Analysis and Machine 
                                                     Intelligence, vol. 27, number 6,835-850, 2005. 
5 Conclusions                                     [Fred, 2001] Fred, A.L. Finding Consistent Clusters in 
K-modes algorithm suffers from the drawback of choos- Data Partitions. In J. Kittler and F. Roli, editors, Mul-
ing random initial modes which may lead to formation of tiple Classifier Systems, volume LNCS 2096, Springer, 
non-repetitive clustering structures that are undesirable for 309–318, 2001. 
analysis. In this paper, we have presented an approach to [Gower and Diday, 1991] Gower, J., Diday, E.: Symbolic 
compute the initial modes for K-modes clustering algo- Clustering Using a New Dissimilarity Measure, Pat-
rithm for clustering categorical data using Evidence Ac- tern Recognition Letters¸24(6), 567-578, 1991. 
cumulation. The procedure is motivated by the observa-
tion that some data objects do not change their class [Guha et al, 1998] Guha, S., Rastogi, R., Shim K.: CURE: 
membership even when subjected to different random   An Efficient Clustering Algorithm for Large Data-
initial conditions (modes). We utilized the idea of Evi- bases, Published in the Proceedings of the ACM 
dence Accumulation for combining the results of multiple SIGMOD Conference, 1998. 
K-mode clusterings. The resultant modes of each of these [Huang, 1997] Huang, Z.: A Fast Clustering Algorithm to 
runs were stored in a Mode-Pool. The most diverse set of Cluster very Large Categorical Data Sets in Data Min-
modes were extracted from the Mode Pool as the initial ing, DMKD, 1997. 
modes for the K-mode algorithm. The computed modes 
                                                  [Jain and Dubes, 1988] Jain, A., Dubes, R.,: Algorithms 
were majorly being representative of those patterns that 
                                                     for Clustering Data, Prentice-Hall, Englewood Cliffs, 
are less susceptible to random selection of initial modes. 
                                                     NJ , 1988. 
Also, the modes computed using this method were found 
to be quite similar to the actual/desired modes of the data- [Kant et al, 1994] Kant, S., Rao, T.L., Sundaram, P.N., An 
sets. Therefore, consistent clustering with fast conver- Automatic and Stable Clustering Algorithm, Pattern 
gence was achieved with less variance in clustering error.  Recognition Letters, vol. 15, Issue 6, 543-549, 1994. 
                                                  [Khan and Ahmad, 2003] Khan, S.S., Ahmad, A.: Com-
Acknowledgements                                     puting Initial points using Density Based Multiscale 
                                                     Data Condensation for Clustering Categorical data , 
The authors are thankful to Dr. P.K. Saxena, Director of nd
SAG for his encouragement to pursue this research under 2  International Conference on Applied Artificial In-
his kind patronage. The authors are also highly indebted telligence, ICAAI’03, Kolhapur, India, 2003. 
to Dr. Michael Madden, Lecturer, Department of Informa- [Khan and Ahmad, 2004] Khan, S.S., Ahmad, A.: Cluster 
tion Technology, NUI Galway for his support to publish center initialization algorithm for K-means clustering. 
this research work.                                  Pattern Recognition Letters, 25 (11), 1293-1302, 
                                                     2004. 
References                                        [Michalski and Stepp, 1983] Michalski, R., Stepp, R.: An 
[Anderberg, M, 1973] Anderberg, M. Cluster Analysis for automated construction of Classification: Conceptual 
   Applications, Academic Press, New York, 1973      clustering versus numerical taxonomy, IEEE Trans. 
                                                     Pattern Anal. Machine Intelligence. 5 (4), 396-410, 
[Bradley and Fayyad, 1998] Bradley, P.S, Fayyad, U.M. 1983. 
   Refining Initial Points for K-Means Clustering, Pro-
   ceedings of the 15th International Conference on Ma- [Michalski et al, 1998] Michalski, R., Bratko, I., Kubat, 
   chine Learning (ICML’98), San Francisco, Morgan   M.: Machine Learning and Data mining: Methods and 
   Kaufmann, 1998                                    Applications. Wiley, New York, 1998.  
[Bradley et al, 1997] Bradley, P.S., Mangasarian, O.L., [Mitra et al, 2002] Mitra, P., Murthy, C.A, Pal, S.K., Den-
   Street, W.N. Clustering via Concave Minimization, in sity Based MultiScale Data Condensation, IEEE 


                                            IJCAI-07
                                              2788