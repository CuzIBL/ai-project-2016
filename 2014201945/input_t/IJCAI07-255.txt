           Supervised Latent Semantic Indexing Using Adaptive Sprinkling

           Sutanu Chakraborti, Rahman Mukras, Robert Lothian, Nirmalie Wiratunga,
                                    Stuart Watt,  and David Harper
                              {sc,ram,rml,nw,sw,djh}@comp.rgu.ac.uk
                         School of Computing, The Robert Gordon University,
                              St Andrew Street, Aberdeen, UK. AB25 1HG

                    Abstract                          al., 1990], word clustering [Bekkerman et al., 2003] or rule
                                                      induction [Cohen and Singer, 1996].
    Latent Semantic Indexing (LSI) has been shown to
                                                        Introspective techniques rely on exploiting the duality of
    be effective in recovering from synonymy and pol-
                                                      term and document spaces. Documents are regarded as sim-
    ysemy in text retrieval applications. However, since
                                                      ilar when they have similar terms; but terms are, in turn, re-
    LSI ignores class labels of training documents, LSI
                                                      garded as similar when they occur in similar documents. La-
    generated representations are not as effective in
                                                      tent Semantic Indexing (LSI) is an introspective technique
    classiﬁcation tasks. To address this limitation, a
                                                      that uses factor analysis to resolve this circularity. Words
    process called ‘sprinkling’ is presented. Sprinkling
                                                      and documents are mapped to a lower-dimensional “concept”
    is a simple extension of LSI based on augmenting
                                                      space. LSI has recently been applied to real world text clas-
    the set of features using additional terms that en-
                                                      siﬁcation problems. In [Gee, 2003] LSI has been applied
    code class knowledge. However, a limitation of
                                                      to spam classiﬁcation, and performances competitive with
    sprinkling is that it treats all classes (and classi-
                                                      Naive Bayes classiﬁer reported. In a study by Zelikovitz et al
    ﬁers) in the same way. To overcome this, we pro-
                                                      [2001], LSI-based classiﬁers have been extended to accom-
    pose a more principled extension called Adaptive
                                                      modate background knowledge.
    Sprinkling (AS). AS leverages confusion matrices
    to emphasise the differences between those classes  However, an inherent limitation of LSI when applied to
    which are hard to separate. The method is tested on classiﬁcation is that it fails to exploit class knowledge of
    diverse classiﬁcation tasks, including those where training documents. If taken into account, class knowledge
    classes share ordinal or hierarchical relationships. can lead LSI to promote inferred associations between words
    These experiments reveal that AS can signiﬁcantly representative of the same class, and attenuate word associ-
    enhance the performance of instance-based tech-   ations otherwise. In this paper, we combine LSI with class
    niques (kNN) to make them competitive with the    knowledge to produce revised document representations from
    state-of-the-art SVM classiﬁer. The revised repre- the vector space model. We show that both kNN and SVM
    sentations generated by AS also have a favourable beneﬁt from these revised representations.
    impact on SVM performance.                          Our work expands on and extends recent work by Chakrab-
                                                      orti et al [2006], where a simple approach called “sprinkling”
                                                      was proposed to integrate class knowledge into LSI. The
1  Introduction                                       basic idea involves encoding class labels as artiﬁcial terms
Support Vector Machines (SVM) and k-nearest neighbours which are appended to training documents. In the binary clas-
(kNN) are two well-studied machine learning approaches to siﬁcation case, for example, two additional terms correspond-
text classiﬁcation. They are both applied on the vector space ing to classes c1 and c2 are appended to documents belonging
model deﬁned over a bag of words (BOW). In this model, to c1 and c2 respectively. LSI is performed on the augmented
documents are represented as vectors over a set of dimen- term-document matrix, resulting in class-speciﬁc word asso-
sions, each corresponding to a word in the BOW. Despite its ciations being promoted. Thus, documents and words belong-
wide usage, it has been argued that the BOW approach fails to ing to the same class are pulled closer to each other. To fur-
scale up for classiﬁcation tasks, principally because of its in- ther emphasise class knowledge, more than one term could be
ability to handle polysemy and synonymy [Sebastiani, 2002]. sprinkled per class.
Existing research reveals two broad ways of overcoming  The basic sprinkling approach treats all classes equally.
this limitation. The ﬁrst involves using background knowl- This is a limitation for the many multi-class problems with
edge such as external thesauri or repositories [Zelikovitz and explicit relationships between classes. Two examples are hi-
Hirsh, 2001; Gabrilovich and Markovitch, 2005]. The sec- erarchical classes (e.g. Yahoo directory) and ordinal classes
ond approach is introspective, in that it does not rely on (e.g. ratings 1 to 5 in movie review, each rating treated as a
any external knowledge. Typically complex features are ex- class). Also, sprinkling is blind to classiﬁer needs. In reality,
tracted from the BOW, using factor analysis [Deerwester et pairs of classes found to be easily separable by one classiﬁer

                                                IJCAI-07
                                                  1582could be difﬁcult to discriminate for another.               terms
                                                            1  1  1  0  0  0 1  1  1  0  0  0 1  0 1.10   0.74   1.04  -0.02   0.02  -0.02  1.04  -0.02


                                                         1

                                                         c  1  0  1  0  0  0 1  0  1  0  0  0 1  0 0.90   0.60   0.84   0.01   0.03   0.01  0.84   0.01
  In this paper, we present theoretical insights to explain       Sprinkle       LSI
                                                            1  1  1  0  0  0 1  1  1  0  0  0 1  0 1.10   0.74   1.04  -0.02   0.02  -0.02  1.04  -0.02
why sprinkling works, and validate the same empirically. We 0  0  0  1  1  1 0  0  0  1  1  1 0  1 0.27  -0.08  -0.11   1.03   0.74   1.03 -0.11   1.03


                                                         2


                                                        documents
then propose a principled extension, called Adaptive Sprin- c 0  0  0  1  0  1 0  0  0  1  0  1 0  1 0.21  -0.07  -0.09   0.83   0.60   0.83 -0.09   0.83
                                                            1  0  0  1  1  1 1  0  0  1  1  1 0  1 0.60   0.12   0.18   1.10   0.80   1.10  0.18   1.10
kling (AS) that addresses the limitations mentioned above.
The key idea behind AS is to leverage confusion matrices re-                        1.10   0.74   1.04  -0.02   0.02  -0.02
                                                                  Test document     0.90   0.60   0.84   0.01   0.03   0.01
ported by classiﬁers to emphasise differences between those                     Text 1.10   0.74   1.04  -0.02   0.02  -0.02
                                                                  1  0  0  1  1  1
                                                                            Classifier 0.27  -0.08  -0.11   1.03   0.74   1.03
classes which are hard to separate. We show that this ap-                                           unsprinkle
                                                                                    0.21  -0.07  -0.09   0.83   0.60   0.83
proach implicitly takes into account explicit relationships be-                     0.60   0.12   0.18   1.10   0.80   1.10
                                                                           Result (c  or c  )
tween classes in multi-class problems. Experimental results                    1 2
are reported on three different types of classiﬁcation problems
involving disjoint, ordinal and hierarchical classes.           Figure 1: An Example of Sprinkling.
  The contributions of this paper are threefold. First, we
show that sprinkled LSI can be used to signiﬁcantly improve 2.2 Sprinkling
the performance of instance based learners like kNN, to make We illustrate the basic idea behind sprinkling with an ex-
them competitive with state-of-the-art classiﬁers like SVM. ample. Figure 1 shows a trivial term-document matrix
This has practical implications in situations where fast in- constructed from six documents belonging to two different
cremental updates and lazy learning, which are strengths of classes. Instead of performing LSI directly on this matrix,
kNN, are desirable. Secondly, our research highlights the sprinkling augments the feature set with artiﬁcial terms cor-
wealth of information hidden in confusion matrices, which responding to the class labels. These terms act as carriers of
can be exploited to improve classiﬁcation. The classiﬁer-
                                                      class knowledge. In Figure 1, terms t1,andt2 are “sprinkled”
speciﬁc nature of this information is especially useful in this
                                                      to documents belonging to classes c1,andc2 respectively.
regard. Thirdly, we show that AS-generated LSI representa- SVD is then performed on the augmented term document ma-
tions have a favourable inﬂuence on SVM performance.  trix. Noisy dimensions corresponding to low singular values
                                                      are dropped and a lower rank approximation constructed in
                                                      the usual manner. To make training document representa-
2  Sprinkling                                         tions compatible with test documents, the sprinkled dimen-
                                                      sions are dropped. This is referred to as “unsprinkling” in
2.1  Latent Semantic Indexing                         Figure 1. The test and training documents can now be com-
                                                      pared in the usual manner using kNN or SVM. While in the
The purpose of LSI is to extract a smaller number of dimen- example above we used one sprinkled term per class, in prin-
sions that are more robust indicators of meaning than indi- ciple we can sprinkle more terms per class, to boost the con-
vidual terms. LSI uses the Singular Value Decomposition tribution of class knowledge to the classiﬁcation process.
(SVD) to arrive at these latent dimensions. In principle, SVD
achieves a two-mode factor analysis and positions both terms 2.3 Adaptive Sprinkling
and documents in a single space deﬁned over the extracted Adaptive Sprinkling (AS) is motivated by the need to address
dimensions. SVD breaks down the original term document two main limitations of the basic sprinkling approach. First,
matrix into three matrices, two of which show the revised rep- in a multi-class situation, sprinkling treats all classes equally
resentations of words and documents in terms of the new di- and disregards relationships between classes, as deﬁned over
mensions, and the third associates “weights” to these dimen- ordinal and hierarchical classes. Furthermore, even in cases
sions. The thesis behind LSI is that less important dimen- where classes have no explicit relation between them, some
sions correspond to “noise” due to word-choice variability. classes are more easily separable than others, so the number
A reduced rank approximation to the original matrix is con- of sprinkled terms should depend on the complexity of the
structed by dropping these noisy dimensions. This approx- class decision boundary. Secondly, the classes found confus-
imation is a smoothed (blurred) version of the original, and ing by a kNN classiﬁer could be different from those found
is expected to model associations between terms and docu- confusing by SVM, and ideally the sprinkling process should
ments in terms of the underlying concepts more accurately. adapt to classiﬁer needs.
An interesting property of SVD is that the generated approx- The key to AS is its exploitation of the confusion matrices
imation is the closest matrix of its rank to the original in the generated by classiﬁers like kNN and SVM. A confusion ma-
least-squares sense.                                  trix compares a classiﬁer’s predictions against expert judge-
  While LSI has been shown to be successful in retrieval ap- ments on a class-by-class basis. The non-diagonal values in
plications, it has certain limitations when applied to classiﬁ- this matrix are indicative of classes that the classiﬁer ﬁnds
cation tasks. As noted earlier, since LSI is blind to class labels hard to separate; the lower the values, the more easily sep-
of training documents, the extracted dimensions are not nec- arable the classes. Referring to classiﬁcation errors in the
essarily the best in terms of discriminating between classes. example confusion matrix of Figure 2, we readily infer that
Also, infrequent words with high discriminatory power may classes 1 and 9 are easy to tell apart, while classes 1 and 2 are
be ﬁltered out by LSI. The idea of sprinkling was conceived harder to discriminate. AS is based on the intuition that rela-
as a simple and intuitive way of addressing these limitations. tively more sprinkled terms are to be allocated between hard-

                                                IJCAI-07
                                                  1583                                                        Document Clustering W/B and mean square error with sprinkling Term Clustering: W/B and mean square error with sprinkling
        1. comp.graphics 130    10      14     18     20       0       5        2      1 0.16 0.16
                                                          8      The number alongside each marker 8 The number alongside each marker
        2. comp.os.ms-windows.misc  31    110     13     19     19       2        3       2      1 is the number of terms sprinkled is the number of terms sprinkled
                                                        0.15                     0.15
        3. comp.sys.ibm.pc.hardware  25     19     111    36       6       1        2       0      0
                                                        0.14                     0.14
        4. comp.sys.mac.harware 15       1       41    130      7       1       3       2       0
                                                        0.13                     0.13
        5. comp.windows.xp 34      16       5       6     135      0       2       2       0
                                                                                      6
                       19       1        1       7        3     148     16     4       1 0.12 6 0.12
        6. rec.autos                                   Mean  Square Error 4      Mean  Square Error 4
                                          expert judgement        2                           2
                       13       1        2       5        5      30    143     1       0 0           0
        7. rec.motorcycles                              0.11                     0.11
        8. rec.sport.baseball  4        3        3      13       6       9       7     143   12
                                                        0.1                       0.1
                                                         0.1 0.15 0.2 0.25 0.3 0.35  0.17 0.18  0.19 0.2
        9. rec.sport.hockey  4        3       2       4        2       2        1      12    170 W/B W/B

                           classifiers predictions
                                                              Figure 4: Document and Term Clustering.
Figure 2: A confusion matrix over the 20NG hierarchy.
Shaded regions correspond to the comp and rec subtrees.
                                                      [Kontostathis and Pottenger, 2006]. Their work reveals close
                                                      correspondence between LSI and higher order associations
      for  i = 1  to  m-1  {      /* m is the number of classes. */ between terms. A word w1 is said to have a ﬁrst order as-
            for  j = i+1  to  m  {
                                                      sociation with another word w2, if they co-occur in at least
         1. Compute normalized mutual class complexity between classes c i ,c j
           as follows:                                one document. w1 and w2 share a second order association
                       mcc( i , j )                   if there is at least one term w3 that co-occurs with w1 and
            mcc       ( i , j ) =
              norm     mcc( i , j )                   w
                             max                        2 in distinct documents. Similarly, we can extend this to
         2. s =   MSL     mcc        ( i , j )
                    norm                              orders higher than 2. Kontostathis and Pottenger provide ex-

         3.
           Sprinkle s terms in all documents belonging to class  c  i  and s others perimental evidence to show that LSI boosts similarity be-
           in all documents belonging to class c
                             j                        tween terms sharing higher order associations. In the light of
        }
      }                                               this observation, it is interesting to note that sprinkled terms
                                                      boost second-order associations between terms related to the
                                                      same class, hence bringing them closer.
   Figure 3: Determining the number of Sprinkled terms. We carried out an analysis of the effect of sprinkling on
                                                      LSI, to verify the hypothesis that sprinkling leads to bet-
to-discriminate classes. Interestingly, we found that confu- ter term and document representations. Starting with docu-
sion matrices also implicitly carry information about explicit ment representations generated by sprinkled LSI and treat-
class relationships as in ordinal and hierarchical classes. For ing each class as a cluster, we compute the “within-cluster”
example, in Figure 2, we see that the two shaded regions cor- and “between-cluster” point scatters [Hastie et al., 2001]
respond to confusion between classes within the comp and that measure cluster separability. The ratio of within- and
rec subtrees of 20 newsgroups (20NG) [Lang, 1995].The between-cluster point scatters, referred to as the W/B mea-
confusion between classes from the two disjoint subtrees is sure, is used as a measure of cluster separability. The lower
smaller.                                              the value, the more separable the clusters. For analysis of
  AS determines the number of sprinkled terms for each class term clustering, terms prototypical of classes were manually
                                                      identiﬁed and the impact of sprinkling on their revised repre-
from the confusion matrix. Let qij be a non-diagonal element
of the confusion matrix Q, showing the number of documents sentations investigated.
                                                        Figure 4 shows that with increased number of sprinkled
of class ci being misclassiﬁed as class cj. We deﬁne proba-
                                                      terms, the W/B measure falls conspicuously. However there
bilities P (i|j) and P (j|i) as the probability of class ci being
                                                      is a second factor to be taken into account. Sprinkled LSI dis-
misclassiﬁed as class cj, and vice versa, respectively. These
                                                      torts the original term document matrix D to a class-enriched
probabilities can be estimated from the confusion matrix as
                                                      LSI approximation DS.However,DS  is no longer the best
follows: P (i|j)=qij /  qij ,andP (j|i)=qij /  qij .
                       i                      j       k-rank approximation to the D in the least-square sense. The
We then deﬁne the “mutual complexity” between classes ci
                                                      vertical axes of the graphs in Figure 4 show the mean square
and cj as mcc(i, j)=[P (i|j)+P (j|i)]/2. The asymmet-
                                                      of errors between D and DS. We see that the reduction in
ric confusion matrix Q is now transformed into a mutual
                                                      W/B achieved by sprinkling is at the cost of losing informa-
complexity matrix M, which is symmetric. The pseudo-
                                                      tion on D. Thus very large number of sprinkled terms may be
code in Figure 3 shows how sprinkled terms can be gener-
                                                      detrimental to classiﬁcation performance, as it may overem-
ated based on the matrix M. The maximum sprinkling length
                                                      phasise class-knowledge. Ideally we would like a trade off
MSLis empirically determined. In our experiments we used
                                                      between “under-” and “over-sprinkling”, that gives us the best
MSL   =8. We note that the mutual class complexity val-
                                                      of both worlds: improve class-discrimination while not over-
ues are normalised and used as weights to vary the number
                                                      looking speciﬁc patterns in D.
of sprinkled terms as a fraction of MSL. Thus the inﬂuence
of class knowledge is greater for those classes that are more
difﬁcult to discriminate.                             3   Empirical Evaluation
                                                      We evaluated Adaptive Sprinkling on three types of clas-
2.4  Why does Sprinkling Work?                        siﬁcation problems. The ﬁrst involves hierarchical classes,
The improved performance of sprinkled LSI in classiﬁcation which have an is-a taxonomy deﬁned over them. The second
tasks can be explained using empirical observations made in type has an ordinal relationship deﬁned between classes. For

                                                IJCAI-07
                                                  1584example, a textual review accompanied by a rating of 1 (on a        A: Before sprinkling (hierarchical dataset) B: After sprinkling (hierarchical dataset)
                                                       comp.graphics
10 point scale) is expected to be more similar to one rated at comp.os.ms-windows.misc
2 than another at 10. If numeric ratings are treated as class la- comp.sys.ibm.pc.hardware
                                                       comp.sys.mac.harware
bels, similarity between classes is a function of this ordering. comp.windows.xp
                                                                                                        expert
Finally, we consider orthogonal problems where classes bear rec.autos
                                                       rec.motorcycles

no explicit relationship to each other.                rec.sport.baseball
                                                       rec.sport.hockey
3.1  Experimental Methodology                                            classifier         classifier
                                                                    C: Before sprinkling (ordinal dataset) D: After sprinkling (ordinal dataset)

We used the following datasets in our experiments:             rating 1
1. The hierarchical dataset: This dataset was formed from
the 20 Newsgroups collection [Lang, 1995] which has seven      rating 2
                                                               rating 3
sub-trees: comp, rec, talk, alt, misc, soc, and sci. We selected                                        expert

the comp and rec sub-trees which contain 5 and 4 classes (cor- rating 4

responding to leaf-nodes) respectively. We used 500 docu-      rating 5
ments from each of these nine classes.                                    classifier         classifier
2. The ordinal dataset: Classiﬁcation between ordinal               E: Before sprinkling (orthogonal dataset) F: After sprinkling (orthogonal dataset)

classes is an interesting problem in sentiment analysis litera- acq
ture [Pang and Lee, 2005]. However, due to the relative youth
of the ﬁeld, no suitable benchmark datasets was readily avail-
                                                               crude
able. We therefore compiled a new dataset from reviews on                                               expert
the “actors and actresses” sub-topic of the Rateitall.com opin-
ion website. Each review contained an integer rating (1 to 5    earn
inclusive) assigned by the author. These ratings were used               classifier         classifier
as the class labels. We removed all reviews having less than
10 words, and created 5 equally distributed classes, each with Figure 5: Confusion matrices before (left column) and after
500 reviews.                                          (right column) sprinkling
3. The orthogonal dataset: We used the acq, crude, and
earn classes of the Reuters-21578 collection [Reuters, 1997]
to form this dataset. 500 documents were selected from each Figure 5 is a qualitative illustration of the effects of AS on
class, such that each document belongs to at most one class. the initial confusion matrices, which result from applying a
  All three datasets underwent similar pre-processing. After kNN classiﬁer to three datasets. Each element of the matrix is
stop word removal and stemming, binary valued term-docum- mapped onto a cell colour. Light colours signify many entries
ent matrices were constructed. For each of the datasets, Infor- in that cell, dark ones signify few. Ideally all cells except
mation Gain (IG) [Sebastiani, 2002] was used to select the top those on the diagonal should be dark, as this indicates total
1000 discriminating words. For experiments using SVM, we agreement between the expert and the classiﬁer.
used the SVMmulticlass implementation [Joachims, 1998].A In all three datasets, we observe that AS results in a reduc-
linear kernel was used as this was found to be best for text tion in inter-class confusion. The ﬁrst column in the matrix
classiﬁcation problems [Joachims, 1998]. We use accuracy of Figure 5A and the second one of Figure 5C, reveal pairs
as a measure of classiﬁer effectiveness since this is known to of classes that kNN ﬁnds hard to classify. Interestingly, AS
be appropriate for single labelled documents in datasets with succeeded in reducing inter-class confusion, as is revealed by
equal class distributions [Gabrilovich and Markovitch, 2004]. the near-diagonal patterns in matrices of Figures 5B and 5D.
For all datasets we performed classiﬁcation using 10 equally A closer look at the confusion matrices obtained after
sized train-test pairs, and used the paired t-test to assess sig- sprinkling reveals patterns that are consistent with the rela-
niﬁcance.                                             tionship between classes. In the hierarchical dataset, the con-
                                                      fusion is mainly between classes within the same sub-tree.
3.2  The Effect of Sprinkling on Confusion            There are two broad confusion zones, one between the ﬁve
     Matrices                                         classes of the comp subtree, the other between four classes
As described in Section 2.3, high off-diagonal values in a con- of rec. Furthermore very closely related classes like those
fusion matrix indicate classes that the classiﬁer ﬁnds hard to corresponding to PC and MAC hardware, and those relat-
separate. This forms the intuitive basis for using the con- ing to autos and motorcycles are hard to discriminate, and
fusion matrix to generate the sprinkling codes. In our ex- this is reﬂected in the lighter shades in the corresponding
periments, a 5-fold cross-validation on the raw training data cells of Figure 5B. For ordinal classes, the confusion matrix
yields ﬁve confusion matrices, which are used to construct an of Figure 5D shows that AS has implicitly mined the sim-
average confusion matrix Q. Sprinkled terms are generated ilarity between rating classes and attenuated confusion be-
based on Q, and LSI is performed on the sprinkled repre- tween distant classes. This is evident from the broad pattern
sentation. The same classiﬁer is then applied to the revised of light shades along the diagonal, and darker shades else-
representations, yielding a new confusion matrix Q.Com- where. This is expected as adjacent classes of an ordinal
paring Q and Q provides direct evidence of the quality of dataset are the most similar. The orthogonal dataset has the
the revised representation.                           least confusion between classes since there is no explicit re-

                                                IJCAI-07
                                                  1585                   Hierarchical Ordinal Orthogonal                       Hierarchical Ordinal Orthogonal
          Baseline   48.02     25.84     93.47                  Baseline   65.47     30.12     94.27
   kNNC   LSI        49.53     29.08     94.80           SVM    LSI        65.71     31.12     95.27
          LSI+AS     60.40     31.00     95.20                  LSI+AS     66.33     32.08     95.27
          Baseline   20.80     25.40     78.60
   kNNE   LSI        35.73     29.00     91.87          Table 2: SVM performance before and after sprinkling.
          LSI+AS     59.38     30.16     93.80
   SVM    Baseline   65.47     30.12     94.27
                                                      ﬂected weaknesses speciﬁc to SVM, hence AS should ideally
  Table 1: kNN performance before and after sprinkling. emphasise differences between classes that SVM on its own
                                                      found hard to classify. The results are in line with our ex-
                                                      pectation, as LSI+AS signiﬁcantly (p<0.05) outperforms
lationship between them. Figures 5E and 5F show that sprin- the baseline on all three datasets. There is some evidence to
kling has a positive effect in reducing inter-class confusion. suggest that LSI alone improves SVM performance, but the
In particular, the confusion between classes acq and crude has difference is not statistically signiﬁcant except for the orthog-
been markedly reduced. We sought an empirical explanation onal dataset.
for this by studying similarity between terms [Kontostathis
and Pottenger, 2006] before and after AS. It was observed
that similarity between words were boosted if they related 4 Related Work
strongly to the same class, and attenuated otherwise. For ex- We are aware of three other efforts that extend LSI to ac-
ample, “opec” and “reﬁnery”, both relevant to the class crude, commodate class knowledge. Sun et al [2004] presents a
were drawn closer, while “dividend” (from earn) and “crude” technique called SLSI that is based on iteratively identifying
(from crude) were moved apart.                        discriminative eigenvectors from class-speciﬁc LSI represen-
                                                      tations. In their study, no signiﬁcant improvement of SLSI
3.3  The Effect of Sprinkling on kNN and SVM          over baseline SVM was reported. Additionally, SLSI involves
To assess the impact of sprinkling we constructed three rep- km SVD computations, corresponding to k iterations over
resentations of each dataset: the raw term-document matrix m classes, making it computationally expensive. In another
(baseline), the LSI-generated reduced dimensional represen- study, Wiener et al [1995] use a combination of what they re-
tation (LSI), and the approximation of the original matrix fer to as local LSI and global LSI. Local LSI representations
generated by sprinkled LSI (LSI+AS).                  are constructed for each class, in a similar spirit to SLSI. Test
  Effects of sprinkling on kNN: We used two variants  documents are compared against each local LSI representa-
of kNN, the ﬁrst based on the Euclidean distance measure tion independently. In addition to computation overheads,
(kNNE) and the second on cosine similarity (kNNC). Both one shortcoming of the approach is that similarities between
use a weighted majority vote from the 3 nearest neighbours. test documents across the local LSI representations cannot be
  Table 1 reports kNN performances, before and after sprin- compared easily. Finally, Wang et al [2005] presents a the-
kling, at the LSI dimension empirically found best. These oretical model to accommodate class knowledge in LSI. No
are compared against baseline SVM performance. For each empirical studies were reported, but the authors note that their
dataset, the performances signiﬁcantly better (p<0.05)than approach slows down when a document belongs to more than
the rest, are shown in bold. Firstly, we observe that AS leads one class. In contrast, sprinkled terms can comfortably reﬂect
to sizable improvements in performance of both kNNE and afﬁliation of documents to more than one class, and this has
kNNC over the respective baselines. kNNE and kNNC per- no adverse implication on time performance.
formances with LSI+AS are signiﬁcantly better than LSI on When compared to AS, a general shortcoming of all of
all datasets. Secondly, LSI+AS enhances kNN performance the above mentioned approaches is that they fail to take into
to be competitive with, and occasionally outperform, baseline account relationships between classes. A second relative
SVM. Figure 6 shows kNNC and kNNE performances over   strength of our approach is that it is simple and can eas-
various LSI dimensions. We note that LSI+AS consistently ily be integrated into existing LSI implementations. Unlike
outperforms LSI at all dimensions, on both measures.  most of the approaches above, the time complexity of our
  The poor performance of all classiﬁers on the ordinal algorithm is independent of the number of classes. In all
dataset can be attributed to classes that are not neatly sepa- our benchmark experiments, computing SVD over an aug-
rable. This is partly caused by subjective differences between mented term-document matrix takes less than 5% additional
reviewers, who use different ratings to express similar judge- time compared to SVD on the original matrix.
ments. The positive impact of AS on confusion matrices in
Figure 5D suggests that a regression-based technique can fare
better than a classiﬁer that attempts to predict a precise rat- 5 Conclusion and Future Work
ing. Furthermore, the IG measure used for feature selection The ﬁrst contribution of our paper is extending LSI to su-
assumes classes to be disjoint and needs to be reformulated pervised classiﬁcation tasks and generating revised document
to accommodate inter-class similarity.                representations that can be used by any technique founded
  Effects of sprinkling on SVM: Table 2 shows the im- on the vector space model. The experimental results ver-
pact of sprinkling on SVM performance. It may be noted ify that we have succeeded in enhancing the performance of
that the confusion matrix used to generate sprinkled terms re- instance-based learners like kNN to make them comparable

                                                IJCAI-07
                                                  1586