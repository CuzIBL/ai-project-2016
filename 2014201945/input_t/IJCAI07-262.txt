                               Unsupervised Anomaly Detection

                     David Guthrie, Louise Guthrie, Ben Allison, Yorick Wilks
                                         University of Sheffield
                                  Natural Language Processing Group
                      Regent Court, 211 Portobello St., Sheffield, England S1 4DP
                             {dguthrie, louise, ben, yorick}@dcs.shef.ac.uk

                    Abstract                          tional story as anomalous, because its language is anoma-
                                                      lous with respect to the rest of the documents in the collec-
    This paper describes work on the detection of
                                                      tion. In this example we have no prior knowledge or train-
    anomalous material in text. We show several vari-
                                                      ing data of what it means to be "normal", nor what it means
    ants of an automatic technique for identifying an
                                                      to be news or fiction. As such, if the collection were
    'unusual' segment within a document, and consider
                                                      switched to be mostly fiction stories and one news story
    texts which are unusual because of author, genre
                                                      then we would hope to identify the news story as anomalous
    [Biber, 1998], topic or emotional tone. We evaluate
                                                      with respect to the rest of the collection because it is a mi-
    the technique using many experiments over large
                                                      nority occurrence.
    document collections, created to contain randomly
                                                        There is a very strong unproven assumption that what is
    inserted anomalous segments. In order to success-
                                                      artificially inserted into a document or collection will be the
    fully identify anomalies in text, we define more
                                                      most anomalous thing within that collection. While this
    than 200 stylistic features to characterize writing,
                                                      might not be true in the general case, every attempt was
    some of which are well-established stylistic deter-
                                                      made to ensure the cohesiveness of the collections before
    miners, but many of which are novel. Using these
                                                      insertion to minimize the chance of finding genuine, un-
    features with each of our methods, we examine the
                                                      planned anomalies. In preliminary experiments where a
    effect of segment size on our ability to detect
                                                      genuine anomaly did exist (for example, a large table or
    anomaly, allowing segments of size 100 words,
                                                      list), it was comforting to note that these sections were iden-
    500 words and 1000 words. We show substantial
                                                      tified as anomalous.
    improvements over a baseline in all cases for all
                                                        The focus of this paper is the identification of segments in
    methods, and identify the method variant which
                                                      a document that are anomalous. Identifying anomalies in
    performs consistently better than others.
                                                      segments is difficult because it requires sufficient repetition
                                                      of phenomena found in small amounts of text. This seg-
1  Introduction                                       ment level concentration steered us to make choices and
Anomaly detection refers to the task of identifying docu- develop techniques that are appropriate for characterizing
ments, or segments of text, that are unusual or different and comparing smaller segments.
from normal text. Previous work on anomaly detection    There are several possibilities for the types of anomaly
(sometimes called novelty detection) has assumed the exis- that might occur at the segment level. One simple situation
tence of a collection of data that defines "normal" [e.g. Alli- is an off-topic discussion, where an advertisement or spam
son and Guthrie, 2006; Markou and Singh, 2003], which is inserted into a topic specific bulletin board. Another pos-
was used to model the normal population, and methods were sibility is that one segment has been written by a different
developed to identify data that differs significantly from this author from the rest of the document, as in the case of pla-
model. As an extension to some of that work, this paper giarism. Plagiarism is notoriously difficult to detect when
describes a more challenging anomaly detection scenario, the source of the plagiarism can be found (using a search
where we assume that we have no data with which to char- engine like Google or by comparison to the work of other
acterize "normal" language. The techniques used for this students or writers.) In addition, the plagiarized segments
task do not make use of any training data for either the nor- are likely to be on the same topic as the rest of the docu-
mal or the anomalous populations and so are referred to as ment, so lexical choice often does not help to differentiate
unsupervised.                                         them. It is also possible for a segment to be anomalous be-
  In this unsupervised scenario of anomaly detection, the cause of a change in tone or attitude of the writing. The
task is to find which parts of a collection or document are goal of this work is to develop a technique that will detect
most anomalous with respect to the rest of the collection. an anomalous segment in text without knowing in advance
For instance, if we had a collection of news stories with one the kind of anomaly that is present.
fictional story inserted, we would want to identity this fic-


                                                IJCAI-07
                                                  1624  Unsupervised detection of small anomalous segments can [Stephens, 2000] such as Flesch-Kincaid Reading Ease,
not depend on the strategies for modeling language that are Gunning-Fog Index, and SMOG Index, some vocabulary
employed when training data is available. With a large richness measures such as: the percentage of words that
amount of training data, we can build up an accurate charac- occur only once, percentage of words which are among the
terization of the words in a document. These language- most frequent words in the Gigaword newswire corpus (10
modeling techniques make use of the distribution of the years of newswire), as well as the features described below.
vocabulary in a document and, because language use and  All segments are passed through the RASP (Robust and
vocabulary are so diverse, it is necessary to train on a con- Accurate Statistical Parser) part-of-speech tagger developed
siderable amount of data to see the majority of cases (of any at the Universities of Sussex and Cambridge. Words, sym-
specific phenomenon) that might occur in a new document. bols and punctuation are tagged with one of 155 part-of-
If we have a more limited amount of data available, as in the
                                                      speech tags from the CLAWS 2 tagset. We use this mark-
segments of a document, it is necessary to characterize the
                                                      up to compute features that capture the distribution of part
language using techniques that are less dependent on the
actual distribution of words in a document and thus less of speech tags. The representation of a segment includes the
affected by the sparseness of language. In this paper we i) Percentages of words that are articles, prepositions, pro-
                                                      nouns, conjunction, punctuation, adjectives, and adverbs
make use of techniques that employ some level of abstrac- ii) The ratio of adjectives to nouns
tion from words and focus on characterizing style, tone, and iii) Percentage of sentences that begin with a subordinating or
classes of lexical items.                             coordinating conjunctions (but,so,then,yet,if,because,unless,
  We approach the unsupervised anomaly detection task orâ€¦)
slightly differently than we would if we were carrying out iv) Diversity of POS tri-grams - this measures the diversity in
unsupervised classification of text [Oakes, 1998; Clough, the structure of a text (number of unique POS trigrams divided by
2000]. In unsupervised classification (or clustering) the the total number of POS trigrams)
goal is to group similar objects into subsets; but in unsuper- Texts are also run through the RASP morphological ana-
vised anomaly detection we are interested in determining lyzer, which produces words, lemmas and inflectional af-
which segments are most different from the majority of the fixes. These are used to compute the
document. The techniques used here do not assume anoma-   i) Percentage of passive sentences
lous segments will be similar to each other: therefore we ii) Percentage of nominalizations.
have not directly used clustering techniques, but rather de-
veloped methods that allow many different types of anoma- 2.1 Rank Features
lous segments within one document or collection to be de- Authors can often be distinguished by their preference for
tected.                                               certain prepositions over others or their reliance on specific
                                                      grammatical constructions. We capture these preferences
2   Characterizing Segments of Text                   by keeping a ranked list sorted by frequency of the:
The use of statistical methods with simple stylistic measures i) Most frequent POS tri-grams list
[Milic, 1967, 1991; Kenny, 1982] has proved effective in ii) Most frequent POS bi-gram list
many areas of natural language processing such as genre  iii) Most frequent POS list
detection [Kessler et al., 1997; Argamon et al., 1998;   iv) MostfrequentArticleslist
                                                         v) Most frequent Prepositions list
Maynard et al., 2001], authorship attribution [McEnery and
                                                         vi) Most frequent Conjunctions list
Oakes, 2000; Clough  et al., 2002; Wilks 2004], and      vii) Most frequent Pronouns list
detecting stylistic inconsistency [Glover and Hirst 1996; For each segment, the above lists are created both for the
Morton, 1978, McColly, 1978; Smith, 1998]. Determining
                                                      segment and for the complement of that segment. We use 1
which stylistic measures are most effective can be difficult,
                                                      â€“ r (where r is the Spearman Rank Correlation coefficient)
but this paper uses features that have proved successful in
the literature, as well as several novel features thought to to indicate the dissimilarity of each segment to its comple-
capture the style of a segment of text.               ment.
  For the purposes of this work, we represent each segment 2.2 Characterizing Tone
of text as a vector, where the components of the vector are
based on a variety of stylistic features. The goal of the work The General       Inquirer       Dictionary
is to rank each segment based on how much it differs from (http://www.wjh.harvard.edu/~inquirer/) developed by the
the rest of the document. Given a document of n segments, social science department at Harvard, contains mappings
                                                      from words to social science content-analysis categories.
we rank each of these segments from one to n based on how
                                                      These content-analysis categories attempt to capture the
dissimilar they are to all other segments in the document
                                                      tone, attitude, outlook, or perspective of text and can be an
and thus by their degree of anomaly.
                                                      important signal of anomaly. The Inquirer dictionary con-
  Components of our vector representation for a segment sists of 7,800 words mapped into 182 categories with most
consist of simple surface features such as Average word and words assigned to more than one category. The two largest
average sentence length, the average number of syllables categories are positive and negative which account for 1,915
per word, together with a range of Readability Measures and 2,291 words respectively. Also included are all Har-


                                                IJCAI-07
                                                  1625vard IV-4 and Lasswell categories. We make use of these The work presented here looks only at fixed-length seg-
categories by keeping track of the percentage of words in a ments with pre-determined boundaries, while a real applica-
segment that fall into each category.                 tion of such a technique might be required to function with
                                                      vast differences between the sizes of segments. Once again,
3   Method                                            there is nothing implicit in the method assuming fixed-
                                                      length sizes, and the choice to fix certain parameters of the
All experiments presented here are performed by character- experiments is to better illustrate the effect of segment
izing each segment as well as characterizing the comple- length on the performance of the method. One could then
ment of that segment (the union of the remaining segments). use either paragraph breaks as natural segment boundaries,
This involves constructing a vector of features for each or employ more sophisticated segmentation techniques.
segment and a vector of features for each segment's com- We introduce a baseline for the following experiments
plement as well as a vector of lists (see Rank Features sec- that is the probably of selecting the truly anomalous seg-
tion) for each segment and its complement. So, for every ment by chance. For instance, the probability of choosing
segment in a document we have a total of 4 vectors:   the single anomalous segment in a document that is 51 seg-
   V1 - feature vector characterizing the segment     ments long by chance when picking 3 segments is 1/51 +
   V2 - feature vector characterizing the complement of the seg- 1/50 + 1/49 or 6%.
ment
   V3 - vector of lists for all rank features for the segment 4.1 Authorship Tests
   V4 - vector of lists for all rank features for the complement of
                                                      For these sets of experiments we examine whether it is pos-
the segment
We next create a vector of list scores called V5 by comput- sible to distinguish anomaly of authorship at the segment
ing the Spearman rank correlation coefficient for each pair level. We test this by taking a document written by one
of lists in vectors V3 and V4. (All numbers in V5 are actu- author and inserting in it a segment written by a different
ally 1-Spearman rank coefficient so that higher numbers author. We then see if this segment can be detected using
mean more different). We next sum all values in V5 to pro- our unsupervised anomaly techniques.
duce a Rank Feature Difference Score.                   We create our experimental data from a collection con-
  Finally, we compute the difference between two segments sisting of 50 thousand words of text written by each of 8
by taking the average difference in their feature vectors plus Victorian authors: Charlotte Bronte, Lewis Carroll, Arthur
the Rank Feature Difference Score.                    Conan Doyle, George Eliot, Henry James, Rudyard Kipling,
                                                      Alfred Lord Tennyson, H.G. Wells.
3.1 Standardizing Variables                             Test sets are created for each pair of authors by inserting
                                                      a segment from one author into a document written by the
While most of the features in the feature vector are percent-
                                                      other author. This creates 56 sets of experiments (one for
ages (% of adjectives, % of negative words, % of words that
                                                      each author inserted into every author.) For example we
occur frequently in the Gigaword, etc.) some are on a differ-
                                                      insert a segment, one at a time from Bronte into Carroll and
ent scale, such as the readability formulae. To test the im-
                                                      anomaly  detection is performed. Likewise we insert
pact of different scales on the performance of the methods
                                                      segments one at a time from Carroll into Bronte and per-
we also perform all tests after standardizing the variables.
                                                      form anomaly detection. Our experiment is always to test if
We do this by scaling all variables to values between zero
                                                      we can detect this inserted paragraph.
and one.
                                                        For each of the 56 combinations of authors we insert 30
                                                      random segments from one into the other, one at a time. We
4   Experiments and Results                           performed 56 pairs * 30 insertions each = 1,680 sets of in-
In each of the experiments below all test documents contain sertion experiments. For each of these 1,680 insertion
exactly one anomalous segment and exactly 50 "normal" experiments we also varied the segment size to test its effect
segments. Whilst in reality it may be true that multiple seg- on anomaly detection. We then count what percentage of the
ments are anomalous within a document, there is nothing time this paragraph falls within the top 3, top 5, top 10, and
implicit in the method which looks for a single anomalous top 20 segments labeled by the program as anomalous. The
piece of text; for the sake of simplicity of evaluation, we results shown here report the average accuracy for each
insert only one anomalous segment per document.       segment size (over all authors and insertions).
  The method returns a list of all segments ranked by how The average percent of time we can detect anomalous seg-
anomalous they are with respect to the whole document. If ments in the top n segments varies according to the segment
the program has performed well, then the truly anomalous size, and as expected, the average accuracy increases as the
segment should be at the top of the list (or very close to the
                                                      segment size increases. For 1000 word segments, anoma-
top). Our assumption is that a human wishing to detect
                                                      lous segment was found in the top 20 ranked segments
anomaly would be pleased if they could find the truly
anomalous segment in the top 5 or 10 segments marked  about 95% of the time (81% in the top ten, 74% of the time
most likely to be anomalous, rather than having to scan the in the top 5 and 66% of the time in the top three segments).
whole document or collection.                         For 500 word segments, average accuracy ranged from 76%


                                                IJCAI-07
                                                  1626down to 47% and for 100 word segments it ranged from           20          81.37         92.16   49.16
65% down to 27%.                                                      Segment size: 500 words
                                                                3           40.2         38.24    6.00
                           Percentage of
                           the time found                       5          66.18          51.47  10.21
   Top n    Percentage of  (standardized                       10          77.45         68.14   21.59
   segments the time found features)     chance                20          92.16         88.73   49.16
               Segment size: 100 words                               Segment size: 1,000 words
         3          26.22          27.03    6.00                370686.00
         5          34.59          32.71   10.21                5            81             74   10.21
        10             50          44.41   21.59               10            88             81   21.59
        20          64.73           62.8   49.16               20            91             87   49.16
               Segment size: 500 words
                                                            Table 2: Average results for fact versus opinion
         3          47.49          43.94    6.00
         5           51.9          51.88   10.21      4.3  Newswire versus Chinese Translations
        10          59.71           64.1   21.59
                                                      In this set of experiments we test whether English transla-
        20          71.62          76.38   49.16      tions of Chinese newspaper segments can be detected in a
               Segment size: 1,000 words              collection of English newswire. We used 35 thousand
         3          58.92          66.04    6.00      words of Chinese newspaper segments that have been trans-
         5          69.63          74.22   10.21      lated into English using Google's Chinese to English transla-
                                                      tion engine. English newswire text is the same randomly
        10          79.94          81.47   21.59      chosen 50,000 word segments from the Gigaword corpus.
        20          94.83          92.77   49.16      Again, tests are run using the 3 different segment sizes.
      Table 1: Average Results for Authorship Tests     For 1000 word segments, the average accuracy for detect-
                                                      ing the anomalous document among the top 3 ranked
                                                      segments is 93% and in the top 10 ranked segments 100%
4.2 Fact versus Opinion                               of the time.
In another set of experiments we tested whether opinion can
be detected in a factual story. Our opinion text is made up of                   Percentage of
editorials from 4 newspapers making up a total of 28,200                         the time found
words.                                                    Top n    Percentage of (standardized
  Our factual text is newswire randomly chosen from the   segments the time found features)     chance
English Gigaword corpus [Graff, 2003] and consists of 4               Segment size: 100 words
different 50,000 word segments one each from one of the 4       3          43.14          31.37   6.00
news wire services.
Each opinion text segment is inserted into each newswire        5          52.94          33.33  10.21
service one at a time for at least 25 insertions on each       10          68.63         56.86   21.59
newswire. Tests are performed like the authorship tests        20          74.51         68.63   49.16
using 3 different segment sizes.                                      Segment size: 500 words
  Results in this set of experiments were generally better
and the average accuracy for 1000 word segments, top 20         3          84.31          76.47   6.00
ranking was 91%  (70% in the top 3 ranked segments.)            5          88.24          80.39  10.21
Small segment sizes of 100 words also yielded good results     10           90.2         86.27   21.59
and found the anomaly in the top 20, 92% of the time (al-      20          94.12         94.12   49.16
though only 40% of the time in the top 3.
                                                                     Segment size: 1,000 words
                                                                3          92.86          89.29   6.00
                           Percentage of
                           the time found                       5          96.43          92.86  10.21
   Top n     Percentage of (standardized                       10           100          92.86   21.59
   segments  the time found features)     chance               20           100          96.43   49.16
               Segment size: 100 words
                                                       Table 3: Average Results for Newswire versus Chinese Trans-
         3           43.14          40.2    6.00                             lations
         5           49.02         66.18   10.21
         10          63.73         77.45   21.59


                                                IJCAI-07
                                                  16275 Conclusions                                         [Kenny, 1982] Anthony Kenny. The computation of style: 
                                                         An introduction to statistics for students of literature and 
There experimental results are very positive and show that, humanities, Pergamon Press, Oxford, 1982. 
with a large segment size, we can detect the anomalous 
segment within the top 5 segments very accurately. In the [Kessler et al., 1997] Brett Kessler, Geoffrey Nunberg, Hin-
author experiments where we inserted a segment from one  rich SchÃ¼tze. Automatic Detection of Text Genre. In 
author into 50 segments by another, we can detect the    Proceedings of the 35th Annual Meeting of the Associa-
anomalous author's paragraph in the top 5 anomalous seg- tion for Computational Linguistics, 32-38, 1997. 
ments returned 74% of the time (for a segment size of 1000 [Maynard et al., 2001] Diana Maynard, Valentin Tablan, 
words), which is considerably better than chance.  If you Cristian Ursu, Hamish Cunningham, Yorick Wilks. 
randomly choose 5 segments out of 51 then you only have a Named Entity Recognition from Diverse Text Types. 
10.2% chance of correctly guessing the segment.  Other   Recent Advances in Natural Language Processing  Con-
experiments yielded similarly encouraging figures.  The task ference, Tzigov Chark, Bulgaria, 2001. 
with the best overall results was detecting when a Chinese  [Markou and Singh, 2003] Markos Markou and Sameer 
translation was inserted into a newswire document, followed Sing. Novelty Detection: a review- parts 1 and 2. Signal 
surprisingly by the task of detecting opinion articles   Processing, 83(12):2481-2521, 2003. 
amongst facts.  
  On the whole, our experiments show that standardizing [McColly, 1987] William B. McColly. Style and structure in 
the scores on a scale from 0 to 1 does indeed produce better the Middle English poem Cleanness. Computers and the 
results for some types of anomaly detection but not for all Humanities, 21:169-176. 
the tasks we performed.  We believe that in all cases where [McEnery and Oakes, 2000] Tony McEnery and Michael 
it performed worse than the standard raw scores, were cases Oakes. Authorship Identification and Computational 
where the genre distinction was very great. We performed Stylometry. In Robert Dale, Hermann Moisl and Harlod 
an additional genre test not reported in this paper where Somers (eds.) Hanbook of Natural Language Process-
Anarchist's Cookbook segments were inserted into news-   ing, 545-562, Marcel Dekker, New York, 2000. 
wire.  Many of the readability formulas, for instance, distin-
                                                      [Milic, 1967] Louis Tonko Milic. A quantitative approach to 
guish these genre differences quite well but their effects on 
                                                         the style of Johnathan Swift. Studies in English litera-
anomaly detection are greatly reduced when we standardize 
                                                         ture, 23:317, The Hague: Mouton, 1967. 
these scores. 
                                                      [Milic, 1991] Louis Tonko Milic. Progress in stylistics: 
References                                               Theory, statistics, computers. Computers and the Hu-
                                                         manities, 25:393-400, 1991. 
[Allison and Guthrie, 2006] Ben Allison and Louise 
   Guthrie. Detecting Anomalous Documents, Forthcoming. [Morton, 1978] Andrew Queen Morton. Literary detection: 
                                                         How to prove authorship and fraud in literature and 
[Argamon et al., 1998] Shlomo Argamon, Moshe Koppel,     documents. Bowker Publishing Company, Bath, Eng-
   Galit Avneri. Routing documents according to style. In land, 1978. 
   Proceedings of the First International Workshop on In-
   novative Internet Information Systems (IIIS-98), Pisa, It- [Oakes, 1998] Mickael Oakes. Statistics for Corpus Linguis-
   aly, 1898.                                            tics, Edinburgh Textbooks in Empirical Linguistics, Ed-
                                                         inburgh, 1998. 
[Biber, 1988] Douglas Biber. Variation across speech and 
   writing. Cambridge University Press, Cambridge, 1998. [Stephens, 2000] Cheryl Stephens. All about Readability. 
                                                         Plain Language Network, 
[Clough, 2000] Paul Clough. Analyzing style â€“ readability. http://www.plainlanguagenetwork.org/stephens/readabili
   University of Sheffield, ty.html, 2006. 
   http://ir.shef.ac.uk/cloughie/papers.html, 2000. 
                                                      [Smith, 1998] MWA Smith. The Authorship of Acts I and II 
 [Clough et al., 2002] Paul Clough, Rob Gaizauskas, Scott of Pericles: A new approach using first words of 
   Piao, Yorick Wilks. Measuring Text Reuse. In Proceed- speeches. Computers and the Humanities, 22:23-41, 
   ings of the 40th Annual Meeting of the Association for 1998. 
   Computational Linguistics, Philadelphia, 2002 
                                                      [Wilks, 2004] Yorick Wilks. On the Ownership of Text. 
[Glover and Hirst 1996] Angela Glover and Graeme Hirst.  Computers and the Humanities. 38(2):115-127, 2004. 
   Detecting stylistic inconsistencies in collaborative writ-
                                                       
   ing. In Sharples, Mike and van der Geest, Thea (eds.), 
   The new writing environment:  Writers at work in a 
   world of technology, Springer-Verlag, London, 1996. 
[Graff, 2003] David Graff. English Gigaword. Linguistic 
   Data Consortium, catalog number LDC2003T05, 2003  


                                                IJCAI-07
                                                  1628