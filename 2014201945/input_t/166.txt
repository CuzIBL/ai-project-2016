        Thin Junction Tree Filters for Simultaneous Localization and Mapping 

                                                  Mark A. Paskin 
                                            Computer Science Division 
                                         University of California, Berkeley 
                                                 Berkeley, CA 94720 
                                              paskin@cs.berkeley.edu 


                        Abstract                              and grows over time. The Kalman filter is used to compute 
                                                              the filtered belief state observations to timewhich 
     Simultaneous Localization and Mapping (SLAM) is 
                                                              in this case takes the form of a multivariate Gaussian distribu•
     a fundamental problem in mobile robotics: while 
                                                              tion We regard the meanas the estimate of the 
     a robot navigates in an unknown environment, it 
                                                              map and the covariance matrix a measure of confidence. 
     must incrementally build a map of its surround•
                                                                 The Kalman filter solution is elegant, but it does not scale 
     ings and, at the same time, localize itself within 
                                                              well to large SLAM problems. Because explicitly repre•
     that map. One popular solution is to treat SLAM 
                                                              sents correlations between all pairs of variables, the size of 
     as an estimation problem and apply the Kalman fil•
                                                              the belief state grows as and because each of these 
     ter; this approach is elegant, but it does not scale 
                                                              correlations must be updated whenever a landmark is re-
     well: the size of the belief state and the time com•
                                                              obscrved, the time complexity of its filter operation is also 
     plexity of the filter update both grow quadratically 
                                                                       This quadratic complexity renders the Kalman fil•
     in the number of landmarks in the map. This pa•
                                                              ter inapplicable to large SLAM problems and gives rise to the 
     per presents a filtering technique that maintains a 
                                                              need for principled, efficient approximations. 
     tractable approximation of the belief state as a thin 
                                                                 Unfortunately, the simplest approach—discarding correla•
    junction tree. The junction tree grows under filter 
                                                              tions so each variable is estimated independently—presents 
     updates and is periodically "thinned" via efficient 
                                                              problems. Ignoring correlations between the robot state and 
     maximum likelihood projections so inference re•
                                                              the landmarks' states leads to overconfidence and divergence 
     mains tractable. When applied to the SLAM prob•
                                                              because correlated observations are treated as if they con•
     lem, these thin junction tree filters have a linear-
                                                              veyed independent information [Hebert et al., 1996]. Fur•
     space belief state and a linear-time filtering opera•
                                                              thermore, correlations between pairs of landmark states are 
     tion. Further approximation yields a filtering oper•
                                                              required for quick convergence when the robot closes a loop, 
     ation that is often constant-time. Experiments on a 
                                                              i.e., it reobserves known landmarks after an extended pe•
     suite of SLAM problems validate the approach. 
                                                              riod of mapping unknown territory (see Figure 1). When 
                                                              the robot closes a loop, it reobserves landmarks whose po•
1 Introduction                                                sitions are known with relative certainty; this helps the robot 
                                                              localize. The robot-landmark correlations translate this im•
Simultaneous Localization and Mapping (SLAM)—where a 
                                                              proved localization estimate into improved position estimates 
robot navigating in an unknown environment must incre•
                                                              for recently-observed landmarks. The inter-landmark corre•
mentally build a map of its surroundings and localize itself 
                                                              lations translate these improved position estimates into im•
within that map—has attracted significant attention because 
                                                              provements for the remaining landmarks on the tour.  Thus, 
it is required by many applications in mobile robotics [Thrun,                                                   1
                                                              the correlations give the Kalman filter a valuable property 
2002]. Typically the environment is idealized so that it con•
                                                              normally associated with smoothing algorithms: it can use 
sists of an unknown number of stationary "landmarks"; for 
                                                              current observations to improve estimates "from the past". 
example, in a given SLAM application these landmarks may 
                                                                Because quadratically many correlations are necessary to 
be low-level visual features or structural features such as 
                                                              close loops, we view the challenge of scalable SLAM filtering 
walls and corners, SLAM can then be viewed as the prob•
                                                              as that of estimating and reasoning with quadratically many 
lem of incrementally estimating the locations of the robot and 
                                                              correlations without quadratic time or space complexity. In 
landmarks from noisy and incomplete observations. 
                                                              this paper, we present a novel and general approximate filter•
  One popular approach treats SLAM as a filtering problem 
                                                              ing method satisfying this criterion. Our point of departure 
[Smith et al., 1990]. The hidden state of the system at time 
 is represented by a random variable which includes Xt 
                                                                 1 Robot-landmark correlations, which decay over time due to mo•
the state of the robot at time and , the locations            tion noise, cannot translate an improved localization estimate into 
of the landmarks observed up to time Thus, the size of        improvements for landmarks observed in the distant past; in con•
the state vector is linear in the number of observed landmarks trast, inter-landmark correlations do not decay over time. 


ROBOTICS                                                                                                           1157                                                                tree to grow, making inference more expensive; in Section 4 
                                                               we present a novel "thinning" operation over junction trees 
                                                               called variable contraction. We prove that each variable con•
                                                               traction is a maximum likelihood projection that removes a 
                                                               set of edges from the corresponding graphical model. The ap•
                                                               proximation error introduced by a variable contraction can be 
                                                               computed efficiently, which allows us to choose which edges 
                                                               to remove at each time step so as to minimize the error. 
                                                                  In Section 5 we apply these techniques to the SLAM prob•
                                                               lem and obtain a thin junction tree filter (TJTF) with a O 
                                                               space belief state representation and a O time filter op•
 Figure 1: The robot is travelling counter-clockwise on a      eration. By delaying the incorporation of recent evidence 
 square path. Dots represent landmarks; the true position of   into the majority of the map, we can improve the filter's time 
 the robot is shown by a square; the filter belief state is vi• complexity; we present a method of evaluating the signifi•
 sualized using the 95% confidence ellipses of the variable    cance evidence has on different portions of the map, which 
 marginals (bold for the robot). Left: accumulated noise and   can be used to adaptively interpolate between constant and 
 error has led to uncertain and drifted estimates for the robot linear-time filter operations. Empirically, we find that these 
and landmark positions. Right: after closing the loop, all of  adaptive filters choose constant-time updates when mapping 
the position estimates improve and their confidences increase. new territory, and when closing a loop, they use time lin•
                                                               ear in the length of the loop. This is perhaps the best time 
                                                               complexity one would hope for in the SLAM problem, since 
(in Section 2) is to view the filtered belief state of the Kalman linearly-many estimates cannot be improved in constant time. 
filter as a Gaussian graphical model [Cowell et al, 1999] that Section 6 presents the results of simulation experiments that 
evolves over time; this allows us to express correlations in   compare TJTF to other SLAM filters and Section 7 concludes. 
terms of direct dependencies (edges) and indirect dependen•    A companion technical report contains proofs of all proposi•
cies (paths). Analyzing the evolution of this graphical model  tions as well as additional background, analysis, and experi•
reveals that filter updates add edges to the graphical model,  ments [Paskin, 2002]. 
making inference more expensive. This motivates an approx•
imation scheme in which weak or redundant edges are period•    1.1 Related work 
ically removed to improve the complexity of inference. Note    Significant interest in the SLAM complexity problem has led 
that edge removal is very different than simply discarding cor• to a number of approaches [Thrun, 2002]. For example, there 
relations; because other edges are left intact, paths—and thus are several submap approaches that decompose the prob•
correlations—persist between each pair of variables.           lem into a set of small mapping problems yielding a block-
   Graphical models give us valuable insight into how good     diagonal landmark covariance matrix. These techniques can 
approximate filters can be designed, but using them to rep•    achieve constant time complexity, but converge slowly be•
resent the belief state presents problems. First, variable     cause information cannot pass between the submaps. 
marginals like the robot's current position would not be im•     Recently, the FastSLAM algorithm [Montemcrlo et al., 
mediately available as they are in the Kalman filter repre•    2002]—a Rao-Blackwellized particle filter—has attracted at•
sentation; we would require inference to obtain them. Sec•     tention because of its logarithmic time complexity. However, 
ond, while it is possible to remove edges from a Gaussian      our experiments show FastSLAM is susceptible to divergence 
graphical model using the Iterative Proportional Fitting algo• in large, noisy SLAM problems. We believe this is because the 
rithm [Speed and Kiiveri, 1986], its application in this context number of particles required for a satisfactory solution can 
would be prohibitively slow. Finally, choosing edges whose     grow exponentially over time; see [Paskin, 2002] for details. 
removal leaves a distribution for which inference is tractable   Sparse extended information filters (SEIF) [Thrun et al, 
is itself a complicated process [Kjaerulff, 1993].             2002] can be viewed in terms of the graphical model rep•
   Our solution to these problems is to use a different rep•   resentation described above; at each time-step, edges are re•
resentation of the belief state. Exact inference in graphical  moved so that a constant-time filter operation can be guaran•
models is often implemented by message passing on a junc•      teed. To avoid the additional complexity of inference, SEIF 
tion tree [Cowell et ah, 1999]. Rather than view the junction  employs approximate inference over this approximate model. 
tree algorithm as an "inference engine", we use the junction   Thus, the SEIF paper provided the valuable insight that sparse 
tree itself as our representation of the belief state. This repre• graphical models can constitute an efficient solution to SLAM. 
sentation has many advantages: the belief state has a "built-  Implementing this insight while avoiding additional approxi•
in" inference algorithm (namely, message passing); it gives    mation was one of the primary motivations of this work. 
immediate access to the marginal distribution over any vari•     Each of these approaches described above uses a sublinear-
able; and as we demonstrate, it gives us efficient methods of  time filter update, and therefore, none can improve all of the 
selecting edges to prune and pruning them.                     landmark estimates in a single update (like the Kalman fil•
  To implement such a junction tree filter, we develop meth•   ter). TJTF has the best of both worlds: its update step takes 
ods for updating the junction tree to reflect filtering updates in constant time unless the observation is significant enough to 
Section 3. These updates can cause the width of the junction   warrant a linear-time update. 


1158                                                                                                          ROBOTICS   Outside of the SLAM literature, there are two works that    change in our treatment. When a landmark is first observed, 
are especially relevant. Kjairulff [1993] investigated edge re• its state variable is added to the belief state with a uninfor-
moval as a means of reducing the complexity of inference      mative (infinite variance, zero covariance) prior; the measure•
in graphical models. Our approach is somewhat simpler, as     ment update yields an informed posterior estimate of its state. 
it operates directly on the junction tree without referring to 
the underlying graphical model. Kjaerulff's analysis of the   2.2 Gaussian graphical models 
approximation error inspired ours, and several of his results Under the assumptions outlined above, the filtered belief state 
apply directly to our case.                                                    is a multivariate Gaussian distribution. The 
  Thin junction tree filtering is an assumed density filtering Kalman filter represents this distribution using the moment 
(ADF) algorithm because it periodically "projects" the filter's parameters—the mean vector and covariance matrix 
belief state to some tractable family of distributions—in this If then its probability distribution is 
case, the family of Gaussian distributions characterized by 
thin junction trees. This makes other work on ADF relevant,                                                          (4) 
especially that of Boyen and Koller [1998], in which the be•
lief state of a dynamic Bayesian network is periodically pro• where d is the length of u. In contrast, Gaussian graphical 
jected to a product-of-marginals approximation. In fact, the  models are usually based upon the canonical parameters— 
connection to this work is stronger: Boyen and Koller [1999]  the information vector r/ and matrix 
extended their earlier analysis to filters where the belief state 
is represented by a junction tree whose structure evolves over                                                       (5) 
time; however, no algorithms were presented. To our knowl•
edge, TJTF is the first algorithm to which this analysis applies. 
                                                              where is the (log) nor•
Here we apply TJTF to a Gaussian graphical model, but noth•
                                                              malization constant. The canonical and moment parameters 
ing prevents its application to the discrete variable networks 
                                                              are related by An advantage of the 
considered by Boyen and Koller. 
                                                              canonical parameterization is that multiplication/division of 
                                                              Gaussians reduces to addition/subtraction of the parameters. 
2 A graphical model perspective on SLAM                         Let be a set of random variables indexed 
We begin by presenting the SLAM model and then formulat•      by elements of the finite set V. We will call a subset of V a 
ing SLAM filtering in terms of graphical models.              family. For a family be the 
                                                              associated set of random variables. A potential over a family 
2.1 The SLAM model                                                    is a non-negative function of Let F be a set of 
We assume a general SLAM model where in each time step        families and let be a set of potential 
the robot moves, obtains an odometry measurement of its       functions over these families. (F, \P) defines a distribution 
motion, and makes several observations of landmarks. As 
in the Kalman filter context, we assume that the motion and                                                          (6) 
measurement models are known and that they are linear-
Gaussian.2 The robot motion at time / is governed by          when the normalizer is finite. 
                                                       (1)      The Markov graph associated with has vertex set 
                                                              V and a clique of edges over each there is an 
and the odometry measurement yt at time t is governed by      edge between are bound by a potential. 
                                                       (2)    The primary value of the Markov graph representation comes 
                                                              from the Hammersley-Clifford theorem, which states that s 
yt is typically a noisy measurement of the robot's velocities. separates from in the Markov graph 
  Landmark measurements are typically assumed to depend       iff (provided In other words, graph 
only upon the state of the robot and the state of the observed separation in the Markov graph encodes the conditional inde•
landmark; for example the observation may consist of the      pendence properties of Because conditional independence 
range and bearing to the landmark in the robot's coordinate   properties often translate into efficient inference algorithms 
frame. If the zth landmark measurement at time issued from    (e.g., junction tree), the Markov graph gives good intuitions 
landmark it is governed by                                    into the design of efficient approximations. 
                                                       (3)      We can represent the Gaussian (5) by a Markov graph, 
                                                              since if we partition the vector 
For simplicity, we assume the correspondence between each 
measurement and the landmark from which it issued is 
known. This question of data association, while critically 
important in SLAM, is largely orthogonal to the issues we 
address here; in particular, the standard technique of choos•
ing the maximum likelihood data association applies without 

   2When these models are not linear-Gaussian, they can be approx•
imated as such as in the Extended or Unscented Kalman Filter. 


ROBOTICS                                                                                                           1159  and is the normalization constant. Thus, all the po•
 tentials of a Gaussian graphical model are either unary (node 
 potentials) or binary (edge potentials). We also have the im•
 portant interpretation that if then is 
 unity (and therefore superfluous), meaning there is no edge 
 between i and j in the corresponding Markov graph. 

 2.3 Filtering in Gaussian graphical models 
 Filtering can be viewed as a three-step procedure: estima•
 tion, in which we incorporate the current time step's mea•    Figure 2: Example evolution of a SLAM graphical model, (a) 
 surements; prediction, in which we augment the model with     In the initial belief state, the robot's state and the land•
 the state variables of the next time step; and roll-up, in which marks' states and are marginally independent, (b) 
 we marginalize out the state variables from the past time     Observing each landmark induces a correlation between 
 step. When the measurement and motion models are linear-      and resulting in a new edge, (c) The prediction update 
 Gaussian, the prediction and estimation steps reduce to mul•  adds the new robot state to the model and joins it to the 
 tiplying small Gaussian potentials into the model; these up•  current robot state . (d) The roll-up phase marginalizes 
 dates are summarized by                                           out of the model, adding a clique edges over all of 's 
                                                               neighbors. 
 Proposition 1.3 Ignoring irrelevant normalization constants, 
 the motion update of equation (1) can be implemented by mul•
 tiplying the potential                                        2.4 Filtering the SLAM graphical model 
                                                               Using these results we can characterize how the structure of 
                                                               the SLAM belief state evolves over time (see Figure 2). For 
                                                               each observed landmark we multiply a measurement poten•
 into the model; the odometry measurement update of equation   tial into the graphical model; this adds an edge be•
 (2) can be implemented by multiplying in the potential        tween xt and Thus, after the estimation phase, the robot's 
                                                               state will be connected to the states of all landmarks it has 
                                                               observed. The prediction phase then connects and 
                                                               Finally, the roll-up phase marginalizes out this places a 
 and the landmark measurement update of equation (3) can be    potential over the Markov blanket of which now includes 
 implemented by multiplying in the potential                   all observed landmarks and Now the SLAM graphical 
                                                               model takes the form of a complete graph—i.e., the belief 
                                                               state has no conditional independencies. By induction, this 
                                                               will be true after every time step. 
   The final step of filtering is roll-up, or marginalizing out  An intuition for why the graphical model becomes dense 
 the past state. The standard rule for marginalization in the  over time is valuable. When the robot measures a landmark, 
 canonical parameterization is given by [Cowell et al., 1999]  the landmark's state becomes directly correlated with that of 
                                                               the robot, and thus indirectly correlated with all covariates of 
 Fact 1. If a = V\i and 
                                                               the robot state, e.g., other landmark states. When the robot's 
                                                               state is eliminated from the model during roll-up, these indi•
                                                       (10)    rect correlations must be expressed directly via new edges. 
                                                                 Importantly, these indirect correlations are often much 
                                                               weaker than the direct ones. Thus, even though the SLAM 
                                                               belief state has no true conditional independencies, there are 
                                                       (ID     many "approximate" conditional independencies; e.g., the 
                                                               landmarks observed at the beginning and end of a tour are 
                                                       (12) 
                                                               almost independent given those observed in the middle. By 
 The time complexity of computing (11) and (12) is quadratic   removing "weak" edges from the graphical model we can en•
 in the dimension of and cubic in the dimension of             force these approximate conditional independencies so they 
   The additive updates above can also be viewed as multiply•  can be used to speed inference. 
 ing in a new potential into 
 the model. The Markov blanket is the set of z's               3 Junction tree filtering 
 neighbors in the Markov graph. Because missing edges in the 
Markov graph correspond to zeros in A, we can infer that this  As discussed in the introduction, the graphical model repre•
 is really a potential over and therefore that marginal•       sentation is valuable for motivating our approximate filter, but 
 izing Ui out of the model places a clique of edges over the   it is not an appropriate representation for its implementation. 
Markov blanket of i.                                           Instead, we represent the belief state of the filter using a junc•
                                                               tion tree. We begin by briefly summarizing the relevant con•

    3Model parameter indices are omitted for notational simplicity. cepts; see [Cowell et al, 1999] for details. 


1160                                                                                                          ROBOTICS 3.1 Junction trees                                             update can be implemented by multiplying in small, simple 
Let p be a distribution of the form (6) with families F and po• potentials to the probability distribution, and that the roll-
tentials (C, E) be an undirected                               up phase is implemented by marginalizing variables out of 
graph where each vertex (or cluster) C is a subset of V\ T     the model. In this section we describe how to incrementally 
is ^junction tree for p if the following three properties hold: maintain a consistent junction tree under these updates. 
                                                                 In what follows we will make use of three nonstandard op•
  1. Singly connected property: T is a tree. 
                                                               erations to restructure a consistent junction tree. 
  2. Potential property: For every family F there is 
                                                                 • Cloning: To clone a cluster we create a copy d, attach 
     some cluster such that 
                                                                    d to c with separator and set 
  3. Running intersection property: is present in two 
     clusters and of T, it is also present in all clusters on    • Merging: Let c and d be neighboring clusters with sep•
     the (unique) path between and                                  arator s. To merge d into c, we: (1) update d; 
                                                                    (2) update (3) swing all edges incident 
With each edge E we associate a separator s =                       to d over to c; and (4) remove d from C and s from 5. 
        let S be the set of T\s separators. 
   Given a junction tree 7\ we can perform inference in the      • Pushing: Let c and d be neighboring clusters with sep•
model by passing messages between the clusters of T. We             arator s such that but To push i from c 
begin by associating with T a set of potential functions            to we update and and 
                                   one for each cluster and         pass a message from to d to update and By 
separator. The charge on T is defined to be                         extension we can push from c to a nonadjacent clus•
                                                                    ter by successive pushes along the unique path from r 
                                                                    to (Any nonmaximal clusters created by pushing are 
                                                                    subsequently merged into their subsuming neighbors.) 
                                                               It is easy to check that all of these operations preserve the 
We initialize by setting all cluster and separator potentials 
                                                               three structural constraints as well as the charge and consis•
to unity, multiplying each potential into for some 
                                                               tency of a junction tree. 
    C (which is possible by the potential property), and mul•
tiplying into an arbitrary then                                Multiplying in potentials 
   Let c and be adjacent clusters with separator d. Assume our belief state is represented by a consistent junc•
Passing a message from c to d updates the separator potential  tion tree T. In order to update the charge of T to reflect the 
   and the cluster potential as follows:                       multiplication of a potential into we must find a 
                                                               cluster and multiplyinto To restore consistency, 
                                                       (14)    we could pass messages throughout 7\ but this is twice the 
                                                               work needed: a simple consequence of the message-passing 
                                                               updates (14) and (15) is that we need only distribute evidence 
                                                       (15) 
                                                               from i.e, we must pass messages along edges in a preorder 
                                                               traversal from c. 
Importantly, these updates leave the charge (13) invariant, so 
                                                                 If there is no cluster that covers the family a of the new po•
          Thus, we can view them as reparameterizing the 
                                                               tential, then we must first modify the junction tree T to create 
distribution p. When messages are passed along every edge 
                                                               one. Draper [1995] presents several techniques to do this; in 
in both directions (in an appropriate schedule), the cluster and 
                                                               the Gaussian case the problem is somewhat simpler, since the 
separator potentials are updated so that they are marginals of 
                                                               potentials bind at most two variables. When multiplying in an 
  over their respective variables. A junction tree in this state 
                                                               edge potential requires us to create a cluster cover•
is called consistent and it can be used to obtain marginals over 
                                                               ing , we find the closest pair of clusters and such 
any set of variables that reside together in some cluster. 
                                                               that and d and push / from c to d. We then multiply 
   When T has no nonmaximal clusters, so the 
                                                                        into and distribute evidence from d. 
number of messages required for inference is bounded by 2 • 
                                                                 It is worth noting that in several cases, conditional inde•
     In the case of a Gaussian graphical model, the cluster and 
                                                               pendencies obviate the evidence distribution step. This is a 
separator potentials are Gaussians; if they are represented by 
                                                               significant optimization, since message passing is by far the 
their canonical parameters, the time complexity of passing a 
                                                               most expensive operation. This occurs, for example, when 
message is dominated by the cost of the marginalization in 
                                                               performing the prediction step (because is an unob•
(14) which is implemented via (11) and (12); thus, it is at 
                                                               served directed leaf of the graphical model and therefore does 
worst cubic in the size of the cluster. In sum, inference is 
                                                               not impact the distributions of the other nodes), when ob•
linear in and cubic in the width of T, traditionally defined 
                                                               serving a landmark for the first time (due to its uninformative 
as the size of the largest cluster minus one. 
                                                               prior), and in certain types of odometry updates. 
3.2 Incremental junction tree maintenance                      Marginalizing out variables 
We adopt consistent junction trees as the belief state represen• Assume again that we have a consistent junction tree T rep•
tation of our filter; i.e., the belief state will be represented by resenting As described in Section 2.3, marginalizing 
the charge (13) of a consistent junction tree. Recall from Sec• out of p places a potential over the Markov blanket of . Be•
tion 2.3 that the prediction and estimation phases of the filter cause the junction tree must have a cluster that covers this new 


ROBOTICS                                                                                                            1161 