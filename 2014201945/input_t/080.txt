                              Multi-prototype Support Vector Machine 

                            Fabio Aiolli                                  Alessandro Sperduti 
                   Dept. of Computer Science                    Dept. of Pure and Applied Mathematics 
                     University of Pisa, Italy                         University of Padova, Italy 
                         aiolli@di.unipi.it                              sperduti@math.unipd.it 


                        Abstract                               1.1 Preliminaries and notation 
                                                                                              be a set of n training exam•
     We extend multiclass SVM to multiple prototypes 
                                                               ples,A single-label 
     per class. For this framework, we give a compact 
                                                               multi-class classifier is a functionthat maps in•
     constrained quadratic problem and we suggest an 
                                                               stances to labels We focus on the class of 
     efficient algorithm for its optimization that guaran•
                                                               winneMake-all linear classifiers with form: 
     tees a local minimum of the objective function. An 
     annealed process is also proposed that helps to es•                                                              (i) 
     cape from local minima. Finally, we report experi•

     ments where the performance obtained using linear         where Mr is the r-th prototype vector, R is the set of proto•
     models is almost comparable to that obtained by           type indexes and C(r) the function returning the class associ•

     state-of-art kernel-based methods but with a signif•      ated to the prototype indexed r. Given a classifier hM(x) and 
     icant reduction (of one or two orders) in response        a training example I we will say that misclassi-
     time.                                                     fies 
                                                                 In the following, we denote by the constant that is equal 

                                                               to 1 ifotherwise. For a given example x1,, 
1 Introduction                                                 we denote by the set of "positive" 
                                                               prototypes indexes and by '> the 
Automatic multiclass classification, i.e. the process to auto•
                                                               set of "negative" prototypes indexes. Finally, the real value 
matically assign exactly one from a prefixed set of labels to a 
                                                                            • x is referred to as similarity score (or simply 
stream of input instances, is a central task for many real world 
                                                               score) of the r-th prototype on the instance x. 
problems like speech recognition, OCR, text categorization 
etc. Many supervised learning methods have been studied        2 Single-prototype multi-class SVM 
that help on these tasks. Recently, kernel-based methods, like 
SVM, have been well studied especially for binary settings,    An effective multi-class extension to SVM has been already 
and they yielded state-of-art performance in many different    proposed in [Crammer and Singer, 2000]. The resulting 
tasks. SVM searches for a high margin linear discriminant      classifier is of the same form of (1) where each class has 
model in a very high dimensional space (feature space) where   associated exactly one prototype, i.e. 
examples are implicitly mapped via a function . Since          The solution is obtained through the minimization of a 
kernel-based algorithms need only of dot products in feature   convex quadratic constrained problem. In this section, we 
space, it is possible to resort to the so called kernel trick if present a simpler derivation of an equivalent formulation that 
these dot products can be computed efficiently with a kernel   will be extended in the next section to the multi-prototype 
function K(x, y) = •. When the number of exam•                 framework. 
ples (or support vectors) is large, these approaches tend to be 
less efficient w.r.t. the time spent for classifying new vectors In the multiclass setting, in order to have a correct classi•
since they require to work with the implicit characterization  fication of the instance the prototype of the correct class 
of the discriminant model.                                     should have a score that is greater than the maximum among 
  One of the most effective SVM extension able to deal         the scores of the prototypes associated to incorrect classes. 
with general multiclass problems has been recently provided    We can formally write the constraints for a correct classifica•
[Crammer and Singer, 2000] and it has shown very good re•      tion of the example with a margin greater or equal to 1 by 
sults. In this paper, we extend the multiclass SVM to the      requiring: 
multi-prototype setting where for each class there can be 
more than one prototype vector. This allowed us to get very 

expressive decision functions by using simple models without   where yi; = d is the index of the prototype vector associated 
necessarily requiring the use of kernels.                      to the correct class for the example x,. To allow for margin 


LEARNING                                                                                                              541 violations, for each example, we introduce soft margin slack  3 Multi-prototype multi-class SVM 
variables 
                                                              In this section, the SProtSVM model previously defined will 
                                                              be extended to the case of multiple prototypes per class. The 
                                                              basic idea is that, given multiple prototype vectors, we have a 
where denotes the hinge loss equal to x if x > 0 and          correct classification iff at least one of the prototypes associ•
0 otherwise. Notice that the value is an upper bound to       ated to the correct class has a score higher than the maximum 
the 0 - 1 loss for the example Xi, and consequently its aver• of the scores of the prototypes associated to incorrect classes. 
age value over the training set will be an upper bound for the   In this case, we write the constraints for a correct classifi•
empirical error.                                              cation of the example Xi with a margin greater or equal to 1 
   Motivated by the structural risk minimization (SRM) prin•  requiring: 
ciple in [Vapnik, 1998], we want to search for a set of pro•
totype vectors M = {Mi,.., M\c\} with small norm such 
to minimize the empirical error on the training set. For this, 
we can formulate the problem in a SVM-style by requiring         To allow for margin violations, for each example xi, we 
a set of prototypes of minimal norm that fulfill the soft con• introduce soft margin slack variables 0, one for each 
straints given by the classification requirements. Thus, the  positive prototype, s.t. 
single-prototype multiclass SVM (SProtSVM) will result in: 


                                                              Given a pattern Xi, we arrange the soft margin slack variables 
                                                       (2)    f J* in a vector Let us now introduce, for each 

                                                              example Xi a new vector , whose components 
                                                              are all zero except one component that is 1. In the following, 
Notice that, as desired, the optimal solution for will be the we refer to as the assignment of the pattern xt. Notice that 
maximum value among the negative scores for the instance      the dot product is always an upper bound to the 0—1 
                                                              loss for the example X; independently from its assignment. 
Xi. This problem is convex and it can be solved in the stan•
dard way by resorting to the optimization of the Wolfe dual      Now, we are ready to formulate the multi-prototype prob•
problem. In this case, the lagrangian is:                     lem by requiring a set of prototypes of small norm and the 
                                                              best assignment for the examples able to fulfill the soft con•
                                                              straints given by the classification requirements. 
                                                                 Thus, the MProtSVM formulation will result in: 


                                                                                                                      (6) 

                                                       (3) 
subject to the constraints . By differentiating the           Unfortunately, this is a mixed integer problem that is not 
lagrangian with respect to the primal variables and impos•    convex and it is a difficult problem in general but, as we will 
ing the optimality conditions we obtain the set of constraints see in the following, it is prone to an efficient optimization 
(KKT conditions) that the variables must fulfill in order to be procedure that approximates the global optimum. At this 
an optimal solution:                                          point, it is worth noticing that, since the effective formulation 
                                                              is itself an (heuristic) approximation to the structural risk 
                                                              minimization principle, a good solution, although not opti•
                                                              mal, can anyway give good results. As we see after, this claim 
                                                              is confirmed by the results obtained in our experimental work. 

                                                       (4)       Let suppose now that the assignments are kept fixed. In this 
  By using the fact and substituting condi•                   case the reduced problem becomes convex and can be solved 
tions (4) in (3) and omitting constants that do not change the as above by resorting to the optimization of the Wolfe dual 
solution, the problem can be restated as:                     problem. In this case, the lagrangian is: 

                                                       (5) 

  The next section includes an efficient optimization pro•
cedure for the more general multi-prototype setting that in•  subject to the constraints . As above, by differen•
cludes the single-prototype case as a particular instance.    tiating the lagrangian of the reduced problem and imposing 


542                                                                                                          LEARNING  the optimality conditions, we obtain: 


 Notice that the second condition requires the dual variables 
 associated to positive prototypes and not assigned to the as•
 sociated pattern to be 0. By denoting now as yi the unique 
 index such that , once using the conditions (8) 
 in (7) and omitting constants that do not change the obtained 
 solution, the reduced problem can be restated as: 


                                                        (9) 

                                                                 If the values of a7p and aft, after being updated, would turn 
   It can be trivially shown that this formulation is consistent out to be not feasible for the constraints arp > 0 and aft < 7, 
 with the formulation for SProtSVM's given above.              we select the value for p such to fulfill the violated constraint 
                                                               bounds at the limit. 
3.1 Optimization with static assignments                         Now, we show how to analytically solve the associated 
                                                               problem with respect to an update involving a pair of vari•
 When patterns are statically assigned to the prototypes 
                                                               ables a^ , a  such that r\, r  E N  and = ^ r-i. Since, in 
 via constant vectors the convexity of the associated                  1  p2               2     i
                                                               this case, the update must have zero sum, we have: 
MProtSVM problem implies that the optimal solution for the 
primal problem in (6) can be found through the maximization 
 of the lagrangian in (9). 
   Assuming an equal number q of prototypes per class, the 
dual involves n x m x q variables which lead to a very large 
scale problem. Anyway, the independence of constraints 
among the different patterns allows for the separation of the 
variables in n disjoint sets of m x q variables. 
   The algorithm we propose for the optimization of the prob•
 lem in (9) is inspired by the ones already presented in [Cram•
 mer and Singer, 2000; Aiolli and Sperduti, 2002a] and con•
sists in iteratively selecting patterns from the training set and 
                                                               Similarly to the previous case, if the values of the and 
optimizing with respect to the variables associated to that pat•
                                                               a  , after being updated, would turn out to be not feasible for 
tern. From the convexity, each iteration leads to an increase   rp2
                                                               the constraints we select the value for 
of the lagrangian until no more improvement is possible since 
                                                               p such to fulfill the violated constraint bounds at the limit. 
the optimal solution for the lagrangian has been found. 
                                                                 Iterating multiple times the basic step described above on 
   After selecting a pattern X , to enforce the constraint 
                               i                               pairs of variables chosen among that associated to a given 
                                 two elements from the set     pattern it is guaranteed to find the optimality condition for 
of variables will be optimized in pair                         the pattern. This has been exploited in [Aiolli and Sper-
while keeping the solution inside the feasible region. In par• duti, 2002a] to devise an incremental algorithm that uses the 
ticular, let wi and w2 be the two selected variables, we restrict method on the reduced problem of a single example and then 
the updates to the form with iterates over different examples. This optimization proce•
optimal choices for p. Iterating the application of this basic dure can be considered incremental in the sense that the so•
step over different pairs and then doing the same for different lution previously found for a given pattern forms the initial 
patterns will guarantee to reach the optimum for the overall   condition when the pattern is selected again for optimiza•
problem.                                                       tion. In this version, the basic step of optimization of the 
   Let now compute the optimal value for p. First of all, we   reduced problem can require the optimization over all the 
observe that the update will affect the squared norm of the                                         pairs of variables not 
prototype vector Mr of an amount                               constrained to 0 associated with the selected pattern. Thus 
                                                               the complexity of the optimization of the reduced problem is 
                                                                                where / is the number of iterations. 
   Now, we show how to analytically solve the associated         Now, we perform a further step by giving an algorithm that 
problem with respect to an update involving a single variable  at each iteration has a complexity o(m x q). For this, we ob•
            and the variable Since does not influence          serve that for the variables associated to the pattern 


LEARNING                                                                                                              543  xp to be optimal, the feasible analytic solution p must be 0 
 for each pair. 
    In particular, for the first case above, in order to be able to 
 apply the step, it is required that, one of the following two 
 conditions are verified, i.e.: 


                                                     0) 
 while for the second case we must have: 


   Notice that these facts can be checked in linear time. It is 
 easy to show that the solution obtained at a certain step can 
 be improved iff one of these facts are verified. This suggests 
 an efficient procedure, shown in Figure l(Top), that tries to 
 greedily fulfill the previous condition of optimality. 
 3.2 Optimization of general MProtSVM 
 In this section, we present an efficient procedure that guaran•
 tees to reach a local minimum of the objective function of the 
 problem in (6) associated to MProtSVM. This procedure will 
 try to optimize also with respect to the assignments 
   Let suppose to start with a given assignment I for the 
 patterns. In this case, as we have already seen, the associated 
 problem is convex and can be efficiently solved by using the 
 algorithm given in Section 3.1. Once that the optimal value 
 for the primal has been reached, we observe that the so•
 lution can be further improved by updating the assignments 
 in such a way to assign each pattern to a positive prototype 
 with minimal slack value, i.e. by setting the vector (2) 
 such to have the unique 1 corresponding to the best perform•
 ing positive prototype. However, with this new assignment 
 7r(2), the variables might not fulfill the second KKT con•
 dition in eq. (8) anymore. If it is the case, it simply means 
 that the current solution is not optimal for the new assign•
 ment. Thus, a lagrangian optimization done by satisfying 
 constraints dictated by KKT conditions for the new assign•
 ment is guaranteed to obtain a new with a better optimal 
 primal value . For the optimization algorithm to suc•
 ceed, however, the KKT conditions on the must be restored 
 in order to return back to a feasible solution and then finally 
 resuming the lagrangian optimization with the new assign•
 ment 7r(2). We restore the KKT conditions by setting 
 whenever there exists such that 
   Performing the same procedure over different assignments, 
 each one obtained from the previous one by the procedure 
just mentioned, implies the convergence of the algorithm to a 
 local solution for the primal problem when no improvements 
 are possible and the KKT conditions are all fulfilled by the 
 current solution. This is due to the fact that every step induces 
 an improvement on the primal value. 
   The first problem with this procedure is that it results oner•
 ous when dealing with many prototypes since we must per•
 form many lagrangian optimizations. For this, we observe      Figure 1: Top: algorithm for the optimization of the variables 
 that for the procedure to work, at each step, it is sufficient to associated with a given pattern xp and a tolerance e. Bottom: 
 stop the optimization of the lagrangian when we find a value  algorithm for the optimization of MProtSVM. 


 544                                                                                                          LEARNING for the primal better than the last found and this is going to code for training and classification that works directly with 
happen for sure since the last solution has been found not op• the explicit (compact) version of the model M. 
timal. This requires only a periodic check of the primal value   Since we are primarily interested into the evaluation of 
when optimizing the lagrangian.                                MProtSVM w.r.t. SProtSVM, for each dataset, we performed 
   The second and more stringent problem is that the proce•    validation on the parameter for the SProtSVM model and 
dure will lead to a local minimum that can be very far from    we re-used the obtained value for training every MProtSVM 
the best possible. For this, we suggest to update the assign•  generated for the same dataset. This approach seems us any•
ments on the basis of a stochastic annealed function instead   way a pessimistic estimate of the performance of MProtSVM. 
of the hard decision function described above.                   The annealing process required by MProtSVM has been 
   Specifically, let us view the value of the primal as an energy implemented by decreasing the temperature of the system 
function                                                       with the exponential law: 


Let suppose to have a pattern Xi having slack variables        where 0 < r < 1 and T0 > 0 are external parameters. We 
       and suppose that the probability for the assignment to  used To = 10 for all the following experiments. 
be in the state s (i.e. with the 5-th component set to 1) follows 
the law 
                                        I 
where T is the temperature of the system and 
     the variation of the system energy when the pattern Xi 
is assigned to the s-th prototype. By multiplying every term 
      by the normalization term exp 
            and considering that probabilities over alternative 
states must sum to one, i.e. , we obtain                       Table 1: Comparison of generalization performances be•
                                                               tween LVQ and MProtSVM increasing the number of code-
                                                               books/prototypes on the NIST dataset 

with the partition function. 
   Thus, when assigning the pattern Xi, each positive proto•
type s will be selected with probability Pi{s). Notice that, 
when the temperature of the system is low, the probability 
for a pattern to be assigned to a prototype different from the 
one having minimal slack value tends to 0 and we obtain a 
behavior similar to the not annealed version of the algorithm. 
The simulated annealing is typically implemented by decreas•
ing the temperature as the number of iterations increases by a 
monotonic decreasing function T = T(t,T   ). The full algo•
                                         0                     Table 2: (a) Test error of MProtSVM on the USPS dataset 
rithm is depicted in Figure 1 (Bottom). 
                                                                                                     , with an increasing 
                                                               number of prototypes; (b) Test error of MProtSVM on the 
4 Experimental Results                                         LETTER dataset with an 
We tested our model against three datasets that we briefly de• increasing number of prototypes. 
scribe in the following: 
 N1ST: it consists of a 10-class task of 10705 digits randomly   A set of experiments have been performed to compare the 
     taken from the NIST-3 dataset. The training set con•      generalization performance of our (linear) model versus LVQ 
     sists of 5000 randomly chosen digits, while the remain•   [Kohonen et al., 1996] which seemed to us the most compa•
     ing 5705 digits are used in the test set.                 rable model. For this, we have reported the best results that 
 USPS: it consists of a 10-class OCR task (digits from 0 to 9) have been obtained by LVQ on the NIST dataset. Specifi•
     whose input are the pixels of a scaled digit image. There cally, they have been obtained with the LVQ2.1 version of 
     are 7291 training examples and 2007 test examples.        the algorithm (see [Sona and Sperduti, 2000]). As it is possi•
                                                               ble to see in Table 1, MProtSVM performs significantly better 
 LETTER: it consists of a 26-class task (alphabetic letters    when few prototypes per class are used, while the difference 
     A-Z). Inputs are measures of the printed font glyph. The  gets lower when the number of prototypes per class increases. 
     first 16000 examples are used for training and the last   This can be due to the more effective control of the margin for 
     4000 for testing.                                         SVM w.r.t. LVQ models. On the same dataset, the tangent-
   Even if any kernel function can be in principle used, for the distance based TVQ algorithm [Aiolli and Sperduti, 2002b] 
following preliminary experiments, we used the linear kernel   has obtained the best result, a remarkable 2.1% test error, 
K(x, y) = (x - y + 1). This allowed us to write an optimized   and polynomial SVM's have obtained a 2.82% test error. In 


LEARNING                                                                                                             545 