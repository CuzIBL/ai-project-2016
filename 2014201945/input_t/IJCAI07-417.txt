AEMS: An Anytime Online Search Algorithm for Approximate Policy Reﬁnement
                                        in Large POMDPs

                                Stephane´  Ross & Brahim Chaib-draa
                      Department of Computer Science and Software Engineering
                              Laval University, Quebec, Canada, G1K 7P4
                                    {ross,chaib}@damas.ift.ulaval.ca

                    Abstract                          subset of belief states when acting in the environment. This is
                                                      the strategy online POMDP algorithms tries to exploit [Satia
    Solving large Partially Observable Markov Deci-   and Lave, 1973; Washington, 1997; Geffner and Bonet, 1998;
    sion Processes (POMDPs) is a complex task which   McAllester and Singh, 1999; Paquet et al., 2005].Sincewe
    is often intractable. A lot of effort has been    only need to plan for the current belief state when acting on-
    made to develop approximate ofﬂine algorithms to  line, one needs only to compute the best action to do in this
    solve ever larger POMDPs. However, even state-    belief state, considering the subset of belief states that can be
    of-the-art approaches fail to solve large POMDPs  reached over some ﬁnite planning horizon.
    in reasonable time. Recent developments in on-
    line POMDP search suggest that combining of-        One drawback of online planning is that it generally needs
    ﬂine computations with online computations is of- to meet hard real-time constraints when one is in face of
    ten more efﬁcient and can also considerably reduce large POMDPs. Nevertheless, recent developments in on-
                                                                                  [                     ]
    the error made by approximate policies computed   line POMDP search algorithms Paquet et al., 2005; 2006
    ofﬂine. In the same vein, we propose a new anytime suggest that combining approximate ofﬂine and online solv-
    online search algorithm which seeks to minimize,  ing approaches may be the most efﬁcient way to tackle large
    as efﬁciently as possible, the error made by an ap- POMDPs. Effectively, we can generally compute a very ap-
    proximate value function computed ofﬂine. In ad-  proximate policy ofﬂine using standard ofﬂine value itera-
    dition, we show how previous online computations  tion algorithms and then use this approximate value function
    can be reused in following time steps in order to as a heuristic function in an online search algorithm. Us-
    prevent redundant computations. Our preliminary   ing this combination enables online search algorithm to plan
    results indicate that our approach is able to tackle on shorter horizons in order to respect online real-time con-
    large state space and observation space efﬁciently straints and retain a good precision. Furthermore, doing an
    and under real-time constraints.                  exact online search on a certain horizon also reduces the er-
                                                      ror made by approximate value functions, and consequently,
                                                      does not require as much precision in the value function to be
1  Introduction                                       efﬁcient.
The POMDP framework provides a powerful model for se-   In this paper, we propose a new anytime online search al-
quential decision making under uncertainty. However, most gorithm which aims to reduce, as efﬁciently as possible, the
real world applications have huge state space and obser- error made by approximate ofﬂine value iteration algorithms.
vation space, such that exact solving approaches are com- Our algorithm can be combined with any approximate ofﬂine
pletely intractable (ﬁnite-horizon POMDPs are PSPACE- value iteration algorithm to reﬁne and improve the approx-
complete [Papadimitriou and Tsitsiklis, 1987] and inﬁnite- imate policies computed by such algorithm. It can also be
horizon POMDPs are undecidable [Madani et al., 1999]). used alone, as a simple online search algorithm that can be
  Most of the recent research in the area has focused on de- applied in stationary or dynamic environments.
veloping new ofﬂine approximate algorithms that can ﬁnd We ﬁrst introduce the POMDP model and some ofﬂine and
approximate policies for larger POMDPs [Braziunas and online approximate solving approaches. Then we present our
Boutilier, 2004; Pineau et al., 2003; Poupart, 2005; Smith new algorithm and some experimental results which show its
and Simmons, 2005; Spaan and Vlassis, 2005]. Still, suc- efﬁciency.
cessful application of POMDPs to real world problems has
been limited due to the fact that even these approximate al-
gorithms are intractable in the huge state space of real world 2 POMDP Model
applications. One of the main drawbacks of these ofﬂine ap-
proaches is that they need to compute a policy over the whole In this section we introduce the POMDP model and present
belief state space. In fact, a lot of these computations are different approximate ofﬂine and online approaches to solve
generally not necessary since the agent will only visit a small POMDPs.

                                                IJCAI-07
                                                  25922.1  Model                                              Similarly to the deﬁnition of the optimal value function, we
                                                                               π∗
A Partially Observable Markov Decision Process (POMDP) can deﬁne the optimal policy as in equation 4.
is a model for sequential decision making under uncertainty.                     
                                                         ∗                                   ∗
Using such a model, an agent can plan an optimal sequence π (b) = arg max R(b, a)+γ P (o|b, a)V (τ(b, a, o))
                                                                  a∈A
of action according to its belief by taking into account the                     o∈Ω
uncertainty associated with its actions and observations.                                             (4)
  A   POMDP    is  generally  deﬁned  by   a  tuple
 S, A, ,T,R,O,γ         S                  A            However, one problem with this formulation is that there
(    Ω          ) where   is the state space, is the  is an inﬁnite number of belief states and as a consequence, it
action set, Ω is the observation set, T (s, a, s):S ×A×S →
 ,                                                    would be impossible to compute such a policy for all belief
[0 1] is the transition function which speciﬁes the probability states in a ﬁnite amount of time. But, since it has been shown
of ending up in a certain state s, given that we were in state
s              a R  s, a  S  × A  →                  that the optimal value function of a POMDP is piecewise lin-
  and did action , (   ):              is the reward  ear and convex, we can deﬁne the optimal value function and
function which speciﬁes the immediate reward obtained by                                               S
           a       s  O o, a, s   × A × S →    ,     policy of a ﬁnite-horizon POMDP using a ﬁnite set of -
doing action in state , (    ):Ω              [0 1]   dimensional hyper plan, called α-vector, over the belief state
is the observation function which speciﬁes the probability of space. This is how exact ofﬂine value iteration algorithms
observing a certain observation o, given that we did action a          V ∗
               s    γ                                are able to compute in a ﬁnite amount of time. However,
andendedinstate  and  is the discount factor.         exact value iteration algorithms can only be applied to small
  In a POMDP, the agent does not know exactly in which problems of 10 to 20 states due to their high complexity. For
state it currently is, since its observations on its current state                          [
                                                 b    more detail, refer to Littman and Cassandra Littman, 1996;
are uncertain. Instead the agent maintains a belief state Cassandra et al., 1997].
which is a probability distribution over all states that speci-
ﬁes the probability that the agent is in each state. After the 2.2 Approximate Ofﬂine algorithms
agent performs an action a and perceives an observation o,
the agent can update its current belief state b using the belief Contrary to exact value iteration algorithms, approximate
                                                                                                       α
update function τ(b, a, o) speciﬁedinequation1.       value iteration algorithms try to keep only a subset of -
                                                     vectors after each iteration of the algorithm in order to limit
                                  
         b (s )=ηO(o, a, s )  T (s, a, s )b(s)  (1)   the complexity of the algorithm. Pineau [Pineau et al., 2003;
                          s∈S                         Pineau, 2004] has developed a point based value iteration al-
                                                      gorithm (PBVI) which bounds the complexity of exact value
       b                     b
  Here,  is the new belief state and is the last belief state of iteration to the number of belief points in its set. Instead of
the agent. The summation part speciﬁes the expected proba- keeping all the α-vectors as in exact value iteration, PBVI
                      s
bility of transiting in state , given that we performed action only keeps a maximum of one α-vector per belief point, that
a              b
  and belief state . Afterward, this expected probability is maximizes its value. Therefore, the precision of the algo-
                                           o
weighted by the probability that the agent observed in state rithm depends on the number of belief points and the loca-
s               a η
  after doing action . is a normalization constant such that tion of the chosen belief points. Spaan [Spaan and Vlassis,
the new probability distribution over all states sums to 1. 2005] has adopted a similar approach (Perseus), but instead
                                                π∗
  Solving a POMDP consists in ﬁnding an optimal policy of updating all belief points at each iteration, Perseus up-
                                                 b
which speciﬁes the best action to do in every belief state . dates only the belief points which have not been improved
This optimal policy depends on the planning horizon and on by a previous α-vector update in the current iteration. Since
the discount factor used. In order to ﬁnd this optimal policy, Perseus generally updates only a small subset of belief points
we need to compute the optimal value of a belief state over at each turn, it can converge more rapidly to an approximate
the planning horizon. For the inﬁnite horizon, the optimal policy, or use larger sets of belief points, which improves
value function is the ﬁxed point of equation 2.       its precision. Another recent approach which has shown
                                                     interesting efﬁciency is HSVI [Smith and Simmons, 2004;
   ∗                                ∗                     ]
 V  (b)=maxR(b, a)+γ       P (o|b, a)V (τ(b, a, o)) (2) 2005 , which maintains both an upper bound deﬁned by a set
         a∈A                                                                           α
                       o∈Ω                            of points and a lower bound deﬁned by -vectors. HSVI uses
                                                      an heuristic that approximates the error of the belief points in
  In this equation, R(b, a) is the expected immediate reward order to select the belief point on which to do value iteration
of doing action a in belief state b and P (o|b, a) is the proba- updates. When it selects a belief to update, it also updates its
bility of observing o after doing action a in belief state b.This upper bound using linear programming methods.
probability can be computed using equation 3.
                                                      2.3  Approximate Online algorithms
                            
                                                               [                 ]
      P (o|b, a)=   O(o, a, s ) T (s, a, s )b(s) (3)  Satia & Lave Satia and Lave, 1973 developed the ﬁrst online
                s∈S         s∈S                      algorithm to solve POMDPs. Their heuristic search algorithm
                                                      uses upper and lower bounds, computed ofﬂine, on the value
  This equation is very similar to the belief update function, function to conduct branch-and-bound pruning in the search
except that it needs to sum over all the possible resulting tree. The POMDP is represented as an AND-OR graph in
states s in order to consider the global probability of observ- which belief states are OR-nodes and actions are AND-nodes.
ing o over all the state space.                       The root node (b0) of such an AND-OR graph represents the

                                                IJCAI-07
                                                  2593current belief state as it is presented in Figure 1. The authors straints since it needs to do a value iteration with α-vectors
suggested solving the underlying MDP to get an upper bound online, and this has a very high complexity.
and to use the value function of any reasonable policy for
the lower bound. The heuristic they proposed to guide their 3AEMS
search algorithm will be compared to our proposed heuristic
in section 3.1.                                       We now present our new  online search algorithm, called
                                                      Anytime Error Minimization Search (AEMS). This algorithm
                                                      aims to determine the best action to do in the current belief
                           b
                            0                         state by doing a look-ahead search. This search is done by
                                                      exploring the tree of reachable belief states from the current
               aa                     a
                           2    ...    n              belief state, by considering the different sequence of actions
                                                      and observations. In this tree, belief states are represented as
                                                      OR-nodes (we must choose an action child node) and actions
        o    o     o
              2 ... n                                 are represented as AND-nodes (we must consider all belief
        b     b     b                                 state child nodes, associated to the different possible obser-
               2     3
                                                      vations) as presented before in Figure 1.
                                                        This tree structure is used to determine the value of the cur-
                                                      rent belief state b0 and the best action to do in this belief state.
               Figure 1: A search tree.               The values of the actions and belief states in the tree are eval-
                                                      uated by backtracking the fringe belief state values, according
  The BI-POMDP algorithm  [Washington, 1997] uses the to equation 2. However, since the search cannot be conducted
classic AO* algorithm [Nilsson, 1980] online to search the on an inﬁnite horizon, we use an approximate value function
AND-OR graph. The author slightly modiﬁed AO* to use  at the fringe of the tree to approximate the inﬁnite-horizon
                                                      value of these fringe belief states. As the tree is expanded,
lower and upper bounds on the value function. For the lower                                     b
bound, BI-POMDP pre-computes ofﬂine the min MDP ( low- the estimate we will get for the current belief state 0 is guar-
est possible value in every state), and uses this approximation anteed to be more precise by the discount factor.
at the fringe of the tree. For the upper bound, they use the AEMS conducts the search by using a heuristic that pro-
               [                 ]                    vides an efﬁcient way to minimize the error on the current
QMDP algorithm  Littman et al., 1995 to solve the under-        b
lying MDP and use this bound as an heuristic to direct the belief state 0 and to handle large observation space. AEMS
search of AO* toward promising actions. They also use the is also able to reuse the computations done at previous time
difference between the lower and upper bound to guide the steps in order to prevent the redundant computations. Finally,
search toward fringe nodes that require more precision. AEMS is also an anytime algorithm which is able to exploit
                                                      every bit of time available at each turn.
  The RTBSS algorithm [Paquet et al., 2005] is another sim-
                                                        The key idea of AEMS consists in exploring the search tree
ilar algorithm which uses a branch and bound technique to
                                                      by always expanding the fringe node that has the highest ex-
search the AND-OR graph from the current belief state on-
                                                      pected error contribution to the current belief state b0.Ex-
line. The search in the tree is done in a depth-ﬁrst-search
                                                      panding this belief state will reduce its error and will lead to
fashion up to a certain pre-determined ﬁxed depth. When it
                                                      better precision at the current belief state b0 for which we are
reaches this depth, it uses a lower bound heuristic to evaluate
                                                      planning.
the long term value of the fringe belief state. RTBSS also uses
an upper bound heuristic in order to prune some branches in 3.1 Expected Error evaluation
the tree. Pruning is only possible when the upper bound value
of doing an action is lower than the lower bound of another There are three key factors that inﬂuence the error introduced
action in the same belief state.                      by a fringe belief state on the current belief state. The ﬁrst
  Several other techniques have been proposed to conduct is the actual error committed by using a lower bound value
online search in POMDPs. Geffner [Geffner and Bonet,  function instead of the exact value function to evaluate the
1998] adapted the RTDP algorithm to POMDPs. This ap-  value of the fringe belief state. In order to evaluate this error,
proach requires the belief state space to be discretized and we can compute the difference between our upper and lower
generally needs a lot of learning time before it performs well. bound to get the maximal possible error introduced by our
                                                      lower bound function on the fringe belief state. We will refer
Another online search algorithm which uses observation sam-                          
pling has also been proposed by McAllester [McAllester and to this approximation as the function ˆ deﬁned by equation 5.
          ]
Singh, 1999 . Instead of exploring all possible observations,           b   U  b − L b
this approach samples a pre-determined number of obser-                ˆ( )=   ( )   ( )              (5)
vations, at each AND-node from a generative model of the Here, U(b) is the upper bound on V ∗(b) and L(b) is the
environment. A more recent online approach, called SOVI lower bound on V ∗(b). The real error (b), which is deﬁned
[G. Shani and Shimony, 2005], extended HSVI into an online by (b)=V ∗(b) − L(b), is always lower or equal to our
value iteration algorithm. Its authors also proposed a few im- approximation ˆ(b).
provements to speed up the upper bound updates and evalua- However, this error is multiplied by different factors, when
tions. The main drawback of this approach is that it is hardly it is backtracked into the search tree, that must be taken into
applicable online in large environments with real time con- account to get a good evaluation of its impact. By looking

                                                IJCAI-07
                                                  2594back at equation 2, we notice that the value of a child belief Furthermore, if we want to know what the probability is,
state is always multiplied by the discount factor γ and the that a certain fringe belief state can be reached by a sequence
probability P (o|b, a) of reaching this child belief state given of optimal actions, we can use the product rule to combine
the action taken in the parent belief state. Since these factors the probabilities of optimal action at each depth.
have a value in interval [0, 1], they reduce the contribution of Combining all these factors, we ﬁnd that the probability of
this error on the parent belief state’s value.        reaching a certain fringe belief state bd at depth d, denoted
  Another implicit factor that must be considered is the max P (bd), can be computed using equation 8.
operator, since it indicates that we need to consider only the
values of belief states that can be reached by doing a sequence          d−1
                                                                     d          i i  i    i i
of optimal actions. In other words, if we know the optimal       P (b )=     P (o |b ,a )P (a |b )    (8)
action in a certain belief state, than we would only need to             i=0
pursue the search in this action’s subtree, because the other In this equation, oi, ai and bi denote the observation, action
action values will not be considered in the value of b0.In
                                                      and belief state encountered at depth i that leads to belief state
our case, since we only have an approximate value function, d
                                                      b  at depth d.
we are generally not sure whether a certain action is optimal
                                                        Consequently, we can compute the expected error intro-
or not. Nevertheless, we can take this uncertainty into ac-                             d
                                                      duced by a certain fringe belief state b at depth d on the
count by considering the probability that an action becomes
                                                      current belief state b0 by using equation 9.
the optimal action, given its current bounds. In particular, if
                                   a
the upper bound value of a certain action is lower than the           E bd    γdP bd  bd
lower bound value of another action a in a belief state, then         (  )=     (  )ˆ( )             (9)
wearesurethata is not the optimal action, (i.e., its probabil- Therefore, we can use equation 9 as an heuristic to choose
ity of becoming the optimal action is 0). However, most of the fringe node that contributes the most to the error in
the time we might encounter cases where the upper bound is b0. Since we propose two different deﬁnitions for the term
higher than the highest lower bound. To handle such case, we P (a|b), we will refer to E(bd) using equation 6 as the heuris-
will assume the other actions lower bound ﬁxed and we will tic AEMS1 and E(bd) using equation 7 as the heuristic
assume that the exact value of the parent belief state is evenly AEMS2.
distributed between its current lower and upper bounds. We Intuitively, E(bd) seems a sound heuristic to guide the
could also consider other types of distributions, in particular search since it has several desired properties. First, it will fa-
if we know that a certain bound is more precise than the other. vor exploration of nodes with loose bounds. Loose bounds
Using these assumptions, we can evaluate the probability that generally indicate that at least one of them is ﬂawed and
a certain action can still become the best action in the future therefore, exploring such node is generally important to get
using equation 6.                                     more precision and make better decisions afterwards. In ad-
                                                      dition, if we have very tight bounds on the value of a belief
                       U(a, b) − L(b)
              P (a|b)=                          (6)   state then we do not need to search this belief state any longer
                        U(b) − L(b)                   since it would have a very low impact on the quality of our
                                                                                         d
  Here U(a, b) is the upper bound on the value of action solution in belief state b0. Moreover, E(b ) favors the explo-
a. L(b) and U(b)  corresponds to the current lower and ration of the most probable belief states we will encounter in
upper bound of belief state b, i.e. which can be obtained the future. This is good for two reasons. Firstly, if a belief
from the maximum lower and upper bound of the actions state has a really low probability of occurring in the future,
in belief state b. So basically, what we are computing is then we do not need a high precision on its value because
P (U(a, b) ≥ V ∗(b)|V ∗(b) ∼ Uniform(L(b),U(b))),i.e. better precision on this belief state would only have a small
the probability that U(a, b) is greater than V ∗(b), assuming impact on the value of the actions in b0, and consequently on
that V ∗(b) follows a uniform distribution between L(b) and our action choice in b0. Secondly, exploring the most proba-
U(b). This formula is valid when U(a, b) >L(b) and, as ble belief states also increases the chance that we will be able
we mentioned earlier, if this is not the case, then P (a|b)=0 to reuse the computations done for this belief state in the fu-
because we are sure that action a will not be the optimal ac- ture, which will improve our precision in the future. Finally
tion. This probability can also be interpreted as the prob- E(bd), favors the exploration of actions that look promising.
ability that we cannot prune action a in belief state b if This behavior is desired for multiple reasons. Generally, we
V ∗(b) ∼ Uniform(L(b),U(b)). While  P (a|b), is not a will only hesitate between a few actions for the best action
probability distribution, it still gives a measure of how likely choice. These actions will have the highest probability of be-
a certain action will not be pruned in the future and remain as ing optimal, and by concentrating the search to these actions,
the optimal action.                                   we should be in a better position to decide which one is the
  An alternative way to approximate the max operator would best. If for some reason the promising actions were not opti-
be to consider the current action with the highest upper bound mal, then we should ﬁnd it pretty quickly when we get better
as the optimal action. In such a case, we can use the alterna- precisions on their upper bounds.
tive deﬁnition for P (a|b) presented in equation 7.     We can also compare how this heuristic differs from other
                                                     heuristics that have been proposed to guide best-ﬁrst-search
                                        
                 1  if a =argmaxa∈A U(a ,b)          in POMPDs. Satia & Lave actually proposed a similar heuris-
     P (a|b)=                                   (7)
                 0  otherwize                         tic, i.e. they suggested exploring at each iteration the k fringe

                                                IJCAI-07
                                                  2595                                 
                          d(b)     d(b)−1  i  i i
nodes that maximize the term γ ˆ(b) i=0 P (o |b ,a ). resulting from all possible action and observation combina-
This term differs from our heuristic by the fact that they do tions. It also computes the lower and upper bounds for all the
not consider the term P (a|b). We will actually see that this next belief states using the lower bound L and upper bound
makes a big difference in terms of performance in practice. U functions. For the actions, the lower and upper bounds are
On the other hand, BI-POMDP always explore the fringe simply computed using equation 2 in which V ∗ is replaced
node, reached by a sequence of actions that maximizes the up- by L and U respectively. Notice that if we reach a belief
                       b
per bound, that maximizes ˆ( ), i.e. it is equivalent to choos- state that is already somewhere else in the tree, it will be du-
                                   d(b)−1   i i       plicated, since our current algorithm does not handle cyclic
ing the fringe node that maximizes ˆ(b) i=0 P (a |b ) us-
ing equation 7 for P (a|b). This heuristic does not take into graph structure. We could possibly try to use a technique
account the probability that a certain belief state is going to be proposed for AO* (LAO* algorithm [Hansen and Zilberstein,
reached, or the discount factor that applies to the value of this 2001]) to handle cycle, but we have not investigated this fur-
belief state, such that it may explore belief nodes in the tree ther and how it affects the heuristic value.
that do not have a lot of impact on the bounds in b0.Again, Once a node has been expanded, we need to backtrack its
our experiments will show that this affects the performance new upper and lower bounds in the tree, in order to update the
of the BI-POMDP approach.                             probabilities that each action is optimal and reconsider our
                                                      best action choices. This is done by the BACKTRACK func-
3.2  Anytime Error Minimization Search                tion which recursively recomputes the bounds (using equa-
                                                      tion 2 in which V ∗ is replaced by L and U), the probabilities
As mentioned before, the expected error E(b) will be the term P (a|b) and the best actions of each ancestor nodes that leads
we will seek to minimize. This can be done efﬁciently in an to the expanded node b∗. Notice that if the bounds of a certain
anytime fashion by always exploring the fringe belief state ancestor node do not change, then we do not need to pursue
that has the highest E(b). Since this term includes the prob- the backtracking process because all the subsequent ancestor
ability P (o|b, a), we can possibly handle large observation node bounds will remain the same.
space because generally in such an environment, only a few
observations will have high probabilities and therefore, the
search will be conducted only in the most probable part of 4 Empirical results
the tree. Furthermore, the probability P (a|b) in E(b) implic- We now present empirical results with our AEMS algorithm.
itly does pruning of the non optimal actions and limits the We have tested our algorithm in the RockSample environ-
search to the parts of the search tree where the actions have ment [Smith and Simmons, 2004] and a modiﬁed version
small probabilities of being optimal. A detailed description of this environment, called FieldVisionRockSample (FVRS).
of our algorithm is presented in algorithm 1.         FVRS is a new environment that we introduce to test our al-
                                                      gorithm in an environment with a big observation space; its
Algorithm 1 AEMS  : Anytime Error Minimization Search observation space size is exponential in the number of rocks,
                                                      as presented below.
  Function AEMS(t)
  Static : G: an AND-OR graph representing the current search In each of these environments, we ﬁrst computed a very ap-
  tree.                                               proximate policy with PBVI by limiting its computation time
  t0 ← CURRENTTIME()                                  and number of belief points to a very small value. Then we
  while CURRENTTIME() − t0 ≤ t do                     evaluated this policy empirically and we compared the im-
   b∗ ←                E  b
        arg maxb∈FRINGE(G) ( )                        provement yielded by different online approaches using this
            ∗                                                                   ∗
    EXPAND(b )                                        policy as a lower bound on V . For the upper bound, we
               ∗
    BACKTRACK(b )                                     used the QMDP algorithm and solved the underlying MDP,
  end while                                           and provided the resulting value function to every online al-
                     G
  return BESTACT(ROOT( )))                            gorithm. The online time available for decision making was
                                                      constrained to 1 second per action and all the different online
  The AEMS algorithm takes the time allowed to search the heuristics presented (Satia, BI-POMDP, AEMS1, AEMS2)
tree in parameter and returns the best action to do in the cur- were implemented in the same best-ﬁrst-search algorithm,
rent belief state. This current belief state is stored in the root such that only the search heuristic could affect the perfor-
node of the AND-OR graph G. The graph G is also kept in mance and not the different implementations. We also com-
memory to resume the search at the fringe in the next time pared our heuristic search to the performance of the depth-
steps. After an action is executed in the environment, the ﬁrst search algorithm RTBSS, using the same PBVI and
graph G is updated such that our new belief state will be the QMDP value functions for the lower and upper bounds.
root of G. This is simply done by setting the root to the node
we reach by following the action-observation path from the 4.1 RockSample
old root node. The value E(b) can be computed quickly since In RockSample (RS) environment, a robot has to explore the
all information needed to compute its value can be stored in environment and sample good rocks. Each rock can be ei-
the belief state nodes when they are created or updated. ther good or bad (no scientiﬁc value) and the robot receives
  The EXPAND  function simply does a one-step look-ahead, rewards accordingly. The robot also receives rewards by leav-
from the fringe belief state given in parameter, by construct- ing the environment (by going to the extreme right of the en-
ing the action AND-nodes and the next belief state OR-nodes vironment). At the beginning, the agent knows the position

                                                IJCAI-07
                                                  2596