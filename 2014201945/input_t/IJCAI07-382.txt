                    Real-Time Heuristic Search with a Priority Queue

        D. Chris Rayner, Katherine Davison, Vadim Bulitko, Kenneth Anderson, Jieshan Lu
                                          University of Alberta
                                   Department of Computing Science
                                 Edmonton, Alberta, Canada T6G 2E8
            {rayner|kdavison|bulitko|anderson|jieshan}@ualberta.ca

                    Abstract                          the updates made to a neighboring state. Prioritizing heuris-
                                                      tic updates for efﬁcient learning is motivated by two obser-
    Learning real-time search, which interleaves plan- vations about asynchronous dynamic programming [Barto et
    ning and acting, allows agents to learn from mul- al., 1995]. First, since states are updated one at a time, new
    tiple trials and respond quickly. Such algorithms state values depend upon the order in which the updates oc-
    require no prior knowledge of the environment     cur; a judicious update ordering can make individual updates
    and can be deployed without pre-processing. We    more effective. Second, a signiﬁcant update to one state often
    introduce Prioritized-LRTA* (P-LRTA*), a learn-   affects its neighbors, and giving priority to these neighbors
    ing real-time search algorithm based on Prioritized can focus computation on the most worthwhile updates.
    Sweeping. P-LRTA* focuses learning on important     The rest of the paper is organized as follows: ﬁrst, we for-
    areas of the search space, where the importance of mally describe the problem. Next, we examine and identify
    a state is determined by the magnitude of the up- the shortcomings of related algorithms, and motivate our ap-
    dates made to neighboring states. Empirical tests proach. We then provide a comprehensive description of our
    on path-planning in commercial game maps show     novel algorithm, and its general properties. Next we justify
    a substantial learning speed-up over state-of-the-art our performance metrics and conduct an empirical evaluation.
    real-time search algorithms.                      We conclude by considering possible extensions to P-LRTA*.

1  Introduction                                       2   Problem Formulation
We introduce Prioritized Learning Real-Time A* (P-LRTA*). We focus on  problems   deﬁned   by  the   tuple
P-LRTA* prioritizes learning to achieve signiﬁcant speed-ups (S, A, c, s0,sg,h0): S is a ﬁnite set of states, A is a
over competing real-time search algorithms.           set of deterministic actions that cause state transitions, and
  Real-time search algorithms interleave planning and acting c(s, a) is the cost of performing action a ∈ A in state s ∈ S;
to generate actions in user-bounded time. These algorithms if executing action a in state s returns the agent to state
are also agent-centered [Koenig, 2001]: they do not require s, that action is said to be blocked. Blocked actions are
a priori knowledge of the map, they learn the transition model discovered when they are within the agent’s visibility radius,
by interacting with the environment, and they improve their the distance at which the agent can sense the environment
solutions by reﬁning a heuristic function over multiple trials. and update its transition model.
Algorithms with these three properties can be used for path- The agent begins in a particular start state s0 ∈ S and aims
planning in computer games, virtual reality trainers [Dini et to reach the goal state sg. Upon reaching a goal state, the
al., 2006], and robotics [Koenig and Simmons, 1998]. In each agent is teleported back to the start state and commences a
of these domains, agents plan and act in potentially unknown new trial. A learning agent has converged when it completes
environments; when identical tasks must be solved in succes- a trial without updating the heuristic value of any state.
sion, the agent has an opportunity to improve its performance For every action executed, the agent incurs a cost associ-
by learning. Examples include commute-type tasks, such as ated with that action. In general, any real-valued costs lower-
resource collection and patrolling.                   bounded by a positive constant can be used. We assume that
  Our focus is on the learning process. Since learning takes the state space is safely explorable insomuch as a goal state
place on-line and on-site, it is critical to minimize learning is reachable from any state.
time by converging rapidly to high-quality solutions.   The agent deals with its initial uncertainty about whether
  We build on existing research and present a learning real- an action is blocked in a particular state by assuming that all
time search algorithm that makes learning more experience- actions from all states are unblocked; this belief is known
efﬁcient by prioritizing updates [Moore and Atkeson, 1993]. as the freespace assumption [Koenig et al., 2003]. As the
States that are deemed important are updated before other agent explores, it can learn and remember which actions are
states, where importance is determined by the magnitude of possible in which states, and plan accordingly.

                                                IJCAI-07
                                                  23723  Related Work                                         PRIORITIZED-LRTA*(s)

A real-time agent is situated at any given time in a single state while s = s do
known as its current state. The current state can be changed      g
                                                          STATEUPDATE(s)
by taking actions and incurring an execution cost. An agent repeat
can reach the goal from the current state in one of two ways: p = queue.pop()
the agent can plan a sequence of actions leading to the goal if p = s then
and then act, or the agent can plan incompletely, execute this     g
                                                              STATEUPDATE(p)
partial plan, and then repeat until it reaches the goal. We end if
review search algorithms for both techniques.             until N states are updated or queue = ∅
  An agent using Local Repair A* (LRA*) [Silver, 2005]    s ⇐ neighbor s’ with lowest f(s, s)=c(s, s)+h(s)
plans a complete path from its current state to the goal, and end while
then executes this plan. This path is optimal given the agent’s
current knowledge. The agent may discover that its planned
path is blocked, and at this point will stop and re-plan with Figure 1: The P-LRTA* agent updates the current state, s, and
updated world knowledge. Each plan is generated with an as many as N states taken from the queue, where N is an
A* search [Hart et al., 1968], so the time needed for the ﬁrst input parameter deﬁned by the user. The agent takes a single
move and for re-planning is O(n log n), where n is the num- greedy move, and then repeats this process until reaching the
                                                      goal state, s .
ber of states. Algorithms such as Dynamic A* (D*) [Stentz,      g
1995] and D* Lite [Koenig and Likhachev, 2002] efﬁciently
correct the current plan rather than reproduce it, but they do
                                                        Koenig’s LRTA* [Koenig, 2004] updates the heuristic val-
not reduce the computation needed before the ﬁrst move of
                                                      ues of all LSS states on each move to accelerate the conver-
each trial can be executed. A large ﬁrst-move delay nega-
                                                      gence process. While the Dijkstra-style relaxation procedure
tively affects an agent’s responsiveness in interactive environ-
                                                      it uses produces highly informed heuristics, the process is ex-
ments such as computer games and high-speed robotics.
                                                      pensive, and limits the size of LSS that can be used in real-
  In its simplest form, Korf’s Learning Real-Time A*  time domains [Koenig and Likhachev, 2006].
(LRTA*) [1990] updates the current state only with respect to In an attempt to reduce convergence execution cost, PR-
its immediate neighbors. We refer to this version of LRTA* LRTS [Bulitko et al., 2005] builds a hierarchy of levels of
as LRTA*(d=1). If the initial heuristic is non-overestimating abstraction and runs search algorithms on each level. The
                            [         ]
then LRTA* converges optimally Korf, 1990 . The amount search at the higher levels of abstraction constrains lower-
of travel on each trial during learning can oscillate unpre- level searches to promising sections of the map, reducing ex-
dictably, causing seemingly irrational behavior. Heuristic ploration in lower levels. As a result, convergence travel and
weighting and learning a separate upper bound reduces the ﬁrst move delay are improved at the cost of a complex imple-
                  [
path length instability Shimbo and Ishida, 2003; Bulitko and mentation and the additional computation required to main-
        ]
Lee, 2006 , but can result in suboptimal solutions.   tain the abstraction hierarchy during exploration.
  We deﬁne the local search space (LSS) as those states A different line of research considers sophisticated up-
whose neighbors are generated when planning a move.   date schemes for real-time dynamic programming algorithms.
LRTA*(d=1) only looks at the current state’s immediate As Barto, Bradtke, and Singh note, “[the] subset of states
neighbors, but a deeper lookahead gives an agent more in- whose costs are backed up changes from stage to stage, and
formation to decide on the next action [Korf, 1990]. For in- the choice of these subsets determines the precise nature of
stance, LRTS [Bulitko and Lee, 2006] considers all of the the algorithm” [1995]. Prioritized Sweeping, a reinforce-
states within a radius d of the current state. Alternatively, ment learning algorithm, performs updates in order of pri-
Koenig uses an LSS deﬁned by a partial A* search from the ority [Moore and Atkeson, 1993]. A state has high priority
current state to the goal [2004; 2006].               if it has a large potential change in its value function. Only
  Updating more than one state value in each planning states with a potential update greater than (>0,∈ R) are
stage can result in more efﬁcient learning. One example added to the queue. Prioritized Sweeping is shown to be more
of this is physical backtracking, which helps to keep state experience efﬁcient than Q-learning and Dyna-PI [Moore and
updates in close physical proximity to the agent: SLA*, Atkeson, 1993], and is a core inﬂuence for our algorithm.
SLA*T  [Shue and Zamani, 1993a; 1993b; Shue  et al.,
2001], γ-Trap [Bulitko, 2004], and LRTS [Bulitko and Lee,
2006] all physically return to previously visited states, po- 4 Novel Algorithm
tentially reducing the number of trials needed for conver- Prioritized-LRTA* (P-LRTA*) combines the ranked updates
gence with the possibility of increasing travel cost. Al- of Prioritized Sweeping with LRTA*’s real-time search as-
ternatively, LRTA*(k) [Hernandez´ and Meseguer, 2005b; sumptions of a deterministic environment and a non-trivial
2005a] uses mental backups to decrease the number of con- initial heuristic. In this section we describe P-LRTA*’s algo-
vergence trials. These two techniques are difﬁcult to com- rithmic details, then comment on the nature of its execution.
bine, and a recent study demonstrates the fragile and highly A P-LRTA* agent has a planning phase and an acting phase
context-speciﬁc effects of combining physical and mental which are interleaved until the agent reaches the goal (Fig-
backups [Sigmundarson and Bjornsson,¨ 2006].          ure 1). During its planning phase, the agent gains knowl-

                                                IJCAI-07
                                                  2373                                                                                                     
  STATEUPDATE(s)                                      and h(s ) is the current estimated cost of traveling from s to
                                                      the closest goal state (Figure 1).
  ﬁnd neighbor s with the lowest f(s, s)=c(s, s)+h(s)
  Δ ⇐  f(s, s) − h(s)                                5   Algorithm Properties
  if Δ > 0 then                                       In this section we make general observations of the key fea-
        ⇐       
    h(s)   f(s, s )                                   tures that make P-LRTA* different from other learning real-
    for all neighbors n of s do                       time search algorithms. Next, we discuss P-LRTA*’s conver-
      ADDTOQUEUE(n,   Δ)                              gence, as well as its space and time complexity.
    end for
  end if                                              Key Features and Design
                                                      Unlike Prioritized Sweeping’s  parameter, which restricts
Figure 2: The value of state s is set to the lowest available potentially small updates from entering the queue, P-LRTA*
c(s, s)+h(s), where c(s, s) is the cost of traveling to s, and speciﬁes a maximum queue size. This guarantees a strict limit
h(s) estimates the distance from s to the goal. If the value of on the memory and computational time used while enabling
s changes, its neighbors are enqueued.                the algorithm to process arbitrarily small updates.
                                                        Because P-LRTA* speciﬁes that the current state always be
                                                                                                    [
  ADDTOQUEUE(s,   Δs)                                 updated, P-LRTA* degenerates to Korf’s LRTA*(d=1) Korf,
                                                      1990] when the size of the queue is 0.
  if s/∈ queue then                                     A P-LRTA* agent disallows duplicate entries in its queue,
    if queueF ull() then                              and retains the contents of its queue between acting phases.
      ﬁnd state r ∈ queue with smallest Δr            This design is a key difference between P-LRTA* and real-
                                                                                           [
      if Δr < Δs then                                 time search algorithms such as LRTA*(k) Hernandez´ and
        queueRemove(r)                                Meseguer, 2005b; 2005a] and Koenig’s LRTA*  [Koenig,
                                                          ]
        queueInsert(s, Δs)                            2004 since a P-LRTA* agent’s local search space is not nec-
      end if                                          essarily contiguous or dependent on the agent’s current state.
    else                                              This property is beneﬁcial, as a change to a single state’s
      queueInsert(s, Δs)                              heuristic value can indeed affect the heuristic values of many,
    end if                                            potentially remote, states. Empirically, limiting how far up-
  end if                                              dates can propagate into unexplored regions of the state space
                                                      does not signiﬁcantly impact P-LRTA*’s performance.
                                                        The P-LRTA* algorithm we present speciﬁes only a pri-
Figure 3: State s is inserted into the queue if there is room in oritized learning process and greedy action selection. But
the queue or if its priority is greater than that of some previ-
                  r                                   P-LRTA* can be modiﬁed to select actions in a way that
ously enqueued state . If a state has already been enqueued, incorporates techniques from other algorithms, such as in-
it will not be inserted a second time into the queue.
                                                      creased lookahead within a speciﬁed radius [Bulitko and
                                                      Lee, 2006], heuristic weighting [Shimbo and Ishida, 2003;
edge about how to navigate the world by updating the dis- Bulitko and Lee, 2006], or preferentially taking actions that
tance heuristics at select states. During its acting phase, the take the agent through recently updated states [Koenig, 2004].
agent simply uses greedy action-selection to choose it next
move. We describe these two phases in detail.         Theoretical Evaluation
  At the beginning of the planning phase, the agent updates Similar to Korf’s LRTA* [Korf, 1990], P-LRTA* can be
the current state s by considering its immediate neighbors viewed as a special case of Barto et al.’s Trial-Based Real-
(Figure 2). The amount by which the current state’s value Time Dynamic Programming (RTDP). In the following the-
changes is stored in a variable, Δ. Next, each of the cur- orems, we outline why the theoretical guarantees of Trial-
rent state’s neighbors are slated for entry into the queue with Based RTDP still hold for P-LRTA*, and explain P-LRTA*’s
priority Δ. If the queue is full, the entry with lowest prior- time and space complexity.
ity is removed (Figure 3). P-LRTA* requires no initial world Theorem 1 P-LRTA* converges to an optimal solution from
model; updates that spread to unseen states use the freespace
                                                      a start state s0 ∈ S to a goal state sg ∈ S when the initial
assumption.                                           heuristic is non-overestimating.
  Once the current state’s heuristic is updated, a series of pri-
oritized updates begins. At most N states, where N is spec- Our algorithm meets the criteria set out for convergence
iﬁed by the user, are taken from the top of the queue and up- by Barto et al. in [Barto et al., 1995]. It is shown that Trial-
dated using the procedure in Figure 2. If there are still states Based RTDP, and by extension P-LRTA*, converges to an op-
in the queue after the N updates, they remain in the queue to timal policy in undiscounted shortest path problems for any
be used in the next planning phase.                   start state s0 and any set of goal states Sg when there exists a
  Having completed the planning phase, the agent moves to policy that takes the agent to the goal with probability 1.
the acting phase. The agent takes a single action, moving to Theorem 2 P-LRTA*’s per-move time complexity is in
the neighboring state s with the minimum-valued f(s, s)= O(m · log(q)), where m is the maximum number of updates
c(s, s)+h(s), where c(s, s) is the cost of traveling to s per time step and q is the size of the queue.

                                                IJCAI-07
                                                  2374  The primary source of P-LRTA*’s per-move time complex- puter game. The use of this particular testbed enabled com-
ity comes from the m updates the algorithm performs, where parison against existing, published data. The number of states
m can be speciﬁed. Each of these m updates potentially re- in each map is 2,765, 7,637, 13,765, 14,098, and 16,142. We
sults in b queue insertions, where b is the branching factor generated 2,000 problems for each map, resulting in a total of
at the current state. Each of these insertions requires that we 10,000 unique path planning problems to be solved by each
check whether that state is already queued (O(1) using a hash of the 12 competing parameterizations of learning real-time
table) and whether or not it has high enough priority to be search algorithms (Table 1).
queued (O(log(q)) using an ordered balanced tree).      The agent incurs unit costs for√ moving between states in
Theorem 3 P-LRTA*’s memory requirements are of space  a cardinal direction, and costs of 2 for moving diagonally.
complexity O(|S|), where |S| is the size of the state space. All the algorithms experimented with use octile distance [Bu-
                                                                       ]
  P-LRTA* requires memory for both its environmental  litko and Lee, 2006 as the initial heuristic h0, which, for
model and the states on the queue. Learning a model of an every state s, is the precise execution cost that would be in-
a priori unknown environment necessitates keeping track of curred by the agent if there were no obstacles between s and
a constant number of connections between up to |S| states. the goal, sg. The maps are initially unknown to each of the
Also, because duplicate states are disallowed, the maximum agents, and the uncertainty about the map is handled using
number of states in the queue, q, cannot exceed |S|.  the aforementioned freespace assumption. The visibility ra-
                                                      dius of each algorithm is set to 10, which limits each agent in
6  Performance Metrics                                what it knows about the world before it begins planning.
                                                        We   compare P-LRTA*   to ﬁve  algorithms: LRA*,
Real-time search algorithms are often used when system re- LRTA*(d=1), LRTS(d=10,γ=0.5,T=0), PR-LRTS with LRA*
sponse time and user experience are important, and so we on the base level and LRTS(d=5,γ=0.5,T=0) on the ﬁrst ab-
measure the performance of search algorithms using the fol- stract level. The LRTS parameters were hand-tuned for
lowing metrics.                                       low convergence execution cost. Since the size of Koenig’s
  First-move lag is the time before the agent can make its ﬁrst A*-deﬁned strict search space is a similar parameter to P-
move. We measure an algorithm’s ﬁrst-move lag in terms of LRTA*’s queue size, we compare against Koenig’s LRTA*
the number of states touched on the ﬁrst move of the last trial; using local search spaces of size 10, 20, 30, and 40.
that is, after the agent has converged on a solution. This is in

                                                          
line with previous research in real-time search and enables us                                         
to compare new results to old results.                         
   

  Suboptimality of the ﬁnal solution is the percentage-wise      

amount that the length of the ﬁnal path is longer than the            
                                                                    

 
length of the optimal path.                               
                                                               
  Planning time per unit of distance traveled indicates the    
amount of planning per step. In real-time multi-agent applica-
tions, this measure is upper bounded by the application con-

straints (e.g., producing an action at each time step in a video 
                                                        
game). An algorithm’s planning time is measured in terms of
the number of states touched per unit of distance traveled.

  The memory consumed is the number of heuristic values  


 

 

stored in the heuristic table. This measure does not include 
the memory used to store the observed map, as the algorithms 
we run use the same map representation.
  Learning heuristic search algorithms typically learn over
multiple trials. We use convergence execution cost to measure
                                                          
                                                          
the total distance physically traveled by an agent during the                       
learning process. We measure this in terms of the distance                     

traveled by the agent until its path converges, where distance
is measured in terms of the cost c of traveling from one state Figure 4: Convergence execution cost in semi-log averaged
                                                      over 1,000 problems plotted against optimal solution length.
to the next.                                          LRTA*(d=1) has a large convergence execution cost, while P-
  Note that to keep the measures platform independent, we LRTA*(queueSize=39,updates=40) has convergence execu-
report planning time as the number of states touched by an tion cost comparable to LRA*.
algorithm. A state is considered touched when its heuristic
value is accessed by the algorithm. There is a linear correla- Each algorithm’s performance is tabulated for compari-
tion between the number of states touched and the wall time son in Table 1. LRA* and LRTA*(d=1) demonstrate ex-
taken for our implementation.                         tremes: LRA* has the smallest convergence execution cost
                                                      and stores no heuristic values, but its ﬁrst-move lag is the
7  Experimental Results                               greatest of all algorithms because it plans all the way to the
We ran search algorithms on path-planning problems on ﬁve goal. LRTA*(d=1) has the largest convergence execution cost
different maps taken from a best-selling commercial com- as a result of its simplistic update procedure, but its ﬁrst-

                                                IJCAI-07
                                                  2375               Algorithm             Execution  Planning     Lag      Heuristic Memory Suboptimality (%)
                LRA*                 158.3 ± 1.3 49.8 ± 1.1 2255.2 ± 28.0   0                0
              LRTA*(d=1)            9808.5 ± 172.1 7.2 ± 0.0 8.2 ± 0.0   307.8 ± 5.1         0
         LRTS(d=10, γ=0.5, T=0)     3067.4 ± 504.4 208.6 ± 1.4 1852.9 ± 6.9 24.6 ± 1.2    0.9 ± 0.02
        PR-LRTS(d=5,γ =0.5,T=0)     831.9 ± 79.1 57.6 ± 0.3 548.5 ± 1.7   9.3 ± 0.4       1.0 ± 0.02
         Koenig’s LRTA*(LSS=10)     2903.1 ± 51.5 70.9 ± 1.62 173.1 ± 0.3 424.5 ± 7.6        0
         Koenig’s LRTA*(LSS=20)     2088.6 ± 39.3 130.1 ± 3.5 322.1 ± 0.6 440.2 ± 8.2        0
         Koenig’s LRTA*(LSS=30)     1753.2 ± 32.4 188.6 ± 5.0 460.1 ± 1.1 448.8 ± 8.2        0
         Koenig’s LRTA*(LSS=40)     1584.4 ± 31.5 250.8 ± 7.5 593.9 ± 1.6 460.4 ± 8.7        0
     P-LRTA*(queueSize=9, updates=10) 1236.0 ± 21.5 40.3 ± 0.9 8.2 ± 0.0 339.0 ± 5.2         0
    P-LRTA*(queueSize=19, updates=20) 708.2 ± 11.9 83.7 ± 1.8 8.3 ± 0.0  393.4 ± 5.8         0
    P-LRTA*(queueSize=29, updates=30) 539.1 ± 8.8 129.7 ± 2.7 8.4 ± 0.0  444.7 ± 6.3         0
    P-LRTA*(queueSize=39, updates=40) 462.4 ± 7.3 175.5 ± 3.6 8.3 ± 0.1  504.1 ± 7.2         0

Table 1: Results on 10,000 problems with visibility radius of 10. The results for LRA*, LRTA*, LRTS, and PR-LRTS are taken
from [Bulitko et al., 2005].

move lag and planning costs are extremely low. For each the queue size with a ﬁxed maximum number of updates (Fig-
parameterization, P-LRTA* has a ﬁrst-move lag comparable ure 5). This demonstrates that P-LRTA*’s parameters provide
to LRTA*(d=1), and its convergence execution cost is even two effective ways of changing the algorithm to meet speciﬁc
lower than algorithms that use sophisticated abstraction rou- time and/or space requirements.
tines, such as PR-LRTS.
  Figure 4 shows the average convergence execution cost 8 Future Work
of the algorithms on 1,000 problems of various lengths: P-
LRTA*’s convergence execution cost is similar to LRA*’s. Prioritized LRTA* is a simple, effective algorithm that invites
This indicates that heuristic learning in P-LRTA* is efﬁcient further analysis. Further experimentation with randomized
enough that the expense of learning the heuristic function start-locations in the problems we explore could provide ad-
scales similarly with the expense of learning the a priori un- ditional insight. We would also like to extend P-LRTA* to
known map itself.                                     include dynamic parameterization, as in some settings it may
                                                      be beneﬁcial to use extra memory as it becomes available. P-
                                                      LRTA* provides a convenient way to take advantage of free
                                                      memory: the queue size can be dynamically adjusted.
                                                        P-LRTA* may beneﬁt from a more sophisticated action-
                                                      selection scheme than the purely greedy decision-making
                                                      process we present. P-LRTA*’s simplicity makes it easy to
  10000                                               experiment with a number of recent real-time search tech-
                                                      niques that improve convergence time.
   8000                                                 State abstraction has been successfully applied to real-time
                                                      heuristic search [Bulitko et al., 2005]. A hybrid approach that
   6000                                               combines prioritized updates with state abstraction is likely
                                                      to produce a search routine that is more powerful than ei-
   4000                                               ther method alone. Another interesting direction is to extend
                                                      P-LRTA* to handle dynamic environments by including fo-
   2000                                         0     cused exploration and allowing heuristic values to decrease.

 Convergence  Execution Cost                 10
     0
     0                                    20          9   Conclusions
           10
                  20                  30
                        30                            Incremental search algorithms and real-time search algo-
                               40  40     Queue Size  rithms meet different requirements. Incremental search al-
      Maximum Number of Updates                       gorithms such as LRA* converge quickly, but can suffer from
                                                      arbitrarily long delays before responding. Real-time search
Figure 5: The impact of queue size and maximum number of works under a strict per-move computational limit, but its
updates on convergence execution cost.                long on-line interactive learning process reduces its applica-
  Using the same set of 10,000 problems used to generate bility when good solutions are needed from the start.
Table 1, we explore P-LRTA*’s parameter space. This exper- P-LRTA* has a response time on par with the fastest known
iment reveals that the queue size and the maximum number real-time search algorithms. Meanwhile, its learning process
of updates per move exhibit some independence in how they converges in a time that scales with that of mere map discov-
affect performance. That is, when we increase the maximum ery in LRA*. A P-LRTA* agent can learn twice as fast on
number of updates with a ﬁxed queue size, the convergence the actual map as a state-of-the-art PR-LRTS agent learns on
execution cost decreases; the same happens when we increase a smaller, abstract map.

                                                IJCAI-07
                                                  2376