                   A Subspace Kernel for Nonlinear Feature Extraction

                                     Mingrui Wu, Jason Farquhar
              Max Planck Institute for Biological Cybernetics, 72076 Tubingen,¨  Germany
                                {ﬁrstname.lastname}@tuebingen.mpg.de


                    Abstract                          A kernel based algorithm essentially applies linear methods
                                                                                      n
                                                      in F for the mapped data {(φ(xi)} . For example, in
    Kernel based nonlinear Feature Extraction (KFE)                                   i=1
                                                      the Kernel Principal Component Analysis (KPCA) algorithm
    or dimensionality reduction is a widely used pre-
                                                      [Scholkopf¨ and Smola, 2002], PCA is used to extract a repre-
    processing step in pattern classiﬁcation and data
                                                      sentative subspace of F. Compared with the traditional linear
    mining tasks. Given a positive deﬁnite kernel func-
                                                      approaches, kernel methods are more powerful since they can
    tion, it is well known that the input data are implic-
                                                      explore nonlinear structures of the data, and more ﬂexible as
    itly mapped to a feature space with usually very
                                                      we can recover the linear algorithms by simply using the lin-
    high dimensionality. The goal of KFE is to ﬁnd
                                                      ear kernel in the kernel based methods.
    a low dimensional subspace of this feature space,                             F
    which retains most of the information needed for    Usually the dimensionality of is very high or even inﬁ-
                                                      nite, which is helpful for separating different classes of data.
    classiﬁcation or data analysis. In this paper, we pro-                              F
    pose a subspace kernel based on which the feature However, such a high dimensional space may contain some
    extraction problem is transformed to a kernel pa- redundancy that is irrelevant or even noisy for the given clas-
    rameter learning problem. The key observation is  siﬁcation or data mining tasks. Hence, as is the case for fea-
    that when projecting data into a low dimensional  ture extraction in the input space, it may be also helpful for
                                                      classiﬁcation or data mining tasks to ﬁnd a lower dimensional
    subspace of the feature space, the parameters that        S   F
    are used for describing this subspace can be re-  subspace  of  .
    garded as the parameters of the kernel function be- Many  Kernel based Feature Extraction (KFE) approaches
                                                                                                       S
    tween the projected data. Therefore current kernel have been proposed to ﬁnd a lower dimensional subspace
                                                                       F
    parameter learning methods can be adapted to op-  of the feature space . For example, KPCA [Scholkopf¨ and
    timize this parameterized kernel function. Exper- Smola, 2002] is widely used for this task. As mentioned
    imental results are provided to validate the effec- above, it essentially performs linear PCA in the feature space
                                                      F
    tiveness of the proposed approach.                  . The goal is to ﬁnd directions along which the data vari-
                                                      ance is the largest.
                                                        In this paper, we discuss feature extraction methods with
1  Introduction                                       the focus on improving the classiﬁcation accuracy. In the
Feature extraction or dimensionality reduction is a widely c-class classiﬁcation problem, each data point xi is associ-
                                                                           c                       
used pre-processing step for classiﬁcation and data mining ated with a label yi ∈ R , where yi =[yi1,...,yic] , and
tasks, since extracting proper features can reduce the effect yik =1(1 ≤ k ≤ c)ifxi belongs to class k, and 0 oth-
of noise and remove redundant information in the data that is erwise.1 It can be seen that KPCA may be not effective for
irrelevant to the classiﬁcation or data analysis tasks. classiﬁcation problems since it is an unsupervised feature ex-
                               n           {x }n
  Suppose that we are given a set of data points, i i=1, traction method, which ignores the labels of the given data.
                 d
where xi ∈X  ⊂R   is the input data, X is the input space. Hence several supervised KFE algorithms have been pro-
Traditional feature extraction approaches, such as the Prin- posed, which make use of both the input data and the corre-
ciple Component Analysis (PCA) and Linear Discriminant sponding labels. Like KPCA, they also perform linear feature
Analysis (LDA) are linear methods and they project the input extraction or linear dimensionality reduction in the feature
data xi into a low dimensional subspace of the input space X . space F.
  Recently, constructing nonlinear algorithms based on the The Kernel Fisher Discriminant Analysis (KFDA) [Mika
kernel methods [Scholkopf¨ and Smola, 2002] have proved et al., 2001] aims to ﬁnd a data projection by minimizing
successful. For a given positive deﬁnite kernel function the within-class variance and maximizing the between-class
K  : X×X    →R                x  1 ≤  i ≤ n
                , the input data i,         are im-   variance simultaneously, thus achieving good discrimination
plicitly mapped to a feature space F with usually very high
dimensionality. Let φ(·) denote the map from X to F, then 1
                                                         Other strategies for constructing the label yi (1 ≤ i ≤ n) are
       K(xi, xj)=φ(xi),φ(xj),  1 ≤ i, j ≤ n         also possible.

                                                IJCAI-07
                                                  1125between different classes. An efﬁcient variant of KFDA 2.2 The Subspace Kernel
based on QR decomposition, called AKDA/QR, is proposed Suppose S is an nf dimensional subspace of F and O =
in [Xiong et al., 2005]. A distinct property of AKDA/QR is [o1,...,on ] is a matrix whose columns constitute an orthog-
             O(ndc)                                             f
that it scales as   . And in AKDA/QR, the number of   onal basis of S. Let T denote the subspace spanned by the
features extracted is ﬁxed to the number of classes.  mapped data φ(xi) (1 ≤ i ≤ n)inF, then each oi can be
  The Partial Least Squares (PLS) algorithm [Wold, 1975] uniquely decomposed into two parts, one is contained in T
has been widely applied in the domain of chemometrics. Un- and the other one is in the orthogonal complement of T ,
like the PCA algorithm, which extracts features only based
                                                                             ⊥
on the variance of the input data, the PLS algorithm uses the      ok = ok + ok , 1 ≤ k ≤ nf
covariance between the inputs and the labels to guide the ex-
                                                                       ⊥
traction of features. The Kernel PLS (KPLS) algorithm is where ok ∈T and ok ,φ(xi) =0for 1 ≤ i ≤ n. Therefore
proposed in [Rosipal and Trejo, 2001].                for any φ(xi), its projection into S can be computed as
                             [                 ]
  The Orthogonal Centroid (OC) Park and Park, 2004 al-               Oφ(x  )=(O)φ(x   )
gorithm is a linear dimensionality reduction method that pre-              i             i            (1)
serves the cluster structure in the data. In this algorithm, the          2
                                                      where O  =[o  ,...,onf ].
given data are ﬁrstly clustered, and then projected into a space   1
                                                        Equation (1) indicates that to compute the projection of
spanned by the centroids of these clusters. An orthogonal
                                                      φ(xi) in S, it is enough to only consider the case where S
basis of this subspace is computed by applying QR decom-
                                                      is a subspace of T , which implies that any vector in S can
position to the matrix whose columns consist of the cluster
                                                      be expressed as a linear combination of φ(xi), 1 ≤ i ≤ n.
centroids. In [Kim et al., 2005], this method is applied for          n        z ,...,z   ∈S     Z
                                                      Therefore, for any f vectors 1   nf    , let denote
dimensionality reduction in text classiﬁcation tasks and ex- [z ,...,z ] X    [φ(x ),...,φ(x )]     Z
hibits good results. Its kernel based nonlinear extension, i.e. 1 nf , and denote 1         n , then  can
                                                      be written as
the Kernel Orthogonal Centroid (KOC) algorithm is also pre-                Z = XW
sented in [Park and Park, 2004]. To incorporate the label in-                                         (2)
                                                                         n×n
formation, the KOC (and OC) algorithm treats input data in where W =[wik] ∈ R f is a matrix of combination coef-
the same class as one single cluster, therefore the number of ﬁcients.
                                                                   z ,...,z
extracted features equals the number of classes. However this Moreover, if 1 nf are linearly independent, then the
method can be easily extended by allowing more clusters in nf dimensional subspace S can be spanned by these nf vec-
each class.                                           tors. Thus the elements of W introduce a subspace S of F,
  In this paper, we propose a subspace kernel, based on for which we have the following lemma.
which the nonlinear feature extraction problem can be trans- Lemma 1. When projecting the data φ(xi) into S, the kernel
formed into a kernel parameter learning problem.      matrix of the projected data in S can be computed as,3
  The rest of this paper is organized as follows. In section 2, w                −1      
we propose the basic idea of our approach and formulate the   K    =(X     Z)(Z  Z)   (X  Z)          (3)
subspace kernel. Some connections to the related methods are       =(KW)(WKW)−1(KW)                 (4)
described in section 3. In section 4, we present one possible
                                                                         n×n
way to optimize the proposed subspace kernel. Experimental where K =[kij] ∈ R is the kernel matrix of the input
results are provided in section 5 and we conclude the paper in data, i.e. kij = K(xi, xj).
the last section.
                                                      Proof. For any φ(xi), 1 ≤ i ≤ n, in order to calculate its
                                                      projection into the subspace S, spanned by the columns of
2  Nonlinear Feature Extraction via Kernel            Z = XW, we need an orthogonal basis U of S. We build U
   Parameter Learning                                 as follows:
                                                                            U = ZT
2.1  Basic Idea                                                                                       (5)
                                                      In the above equation, T is computed as follows: Assume
As mentioned before, a given positive deﬁnite kernel K im-   
                                         φ(x ) 1 ≤    Kz  = Z  Z then
plicitly introduces a mapping of the given data i ,                               − 1
                                                                          T =  VΛ   2
i ≤ n, to a usually high dimensional feature space F. When                                            (6)
                                                                  n ×n
projecting φ(xi) (1 ≤ i ≤ n) into a subspace S of F, the where Λ ∈ R f f is a diagonal matrix of eigenvalues of
                                                                          n ×n
kernel function has to be modiﬁed correspondingly since the matrix Kz, and V ∈ R f f is a matrix whose columns are
feature space has changed from F to S. For convenience, eigenvectors of Kz. Equation (6) leads to
we call this modiﬁed kernel function the subspace kernel.As
                                                                          K−1 =  TT
will be shown later, the parameters that are used for describ-              z                         (7)
ing S are also the parameters of the corresponding subspace
                                                         2More precisely, the result of equation (1) is the coordinate of
kernel. Therefore current kernel parameter learning methods
                                                      the projection of φ(xi) in S. As is widely done in the literature of
can be adapted to optimize this kernel function. This way we feature extraction and dimensionality reduction, this coordinate will
can ﬁnd a discriminating subspace S where different classes be used as the extracted features for classiﬁcation.
of data are well separated. In the following, we will explain 3Here the “kernel matrix of the projected data” refers to the ma-
the above idea in detail by formulating the aforementioned trix whose elements equal the inner product of the projected data in
subspace kernel.                                      S.

                                                IJCAI-07
                                                  1126and                                                   where .∗ denotes the component-wise product between two
                                                                                            
                   T   KzT  = I                 (8)   vectors.  Namely, for θ  =[θ1,...,θd]    and  u  =
                                                                                         
      I                                               [u1,...,ud] , θ. ∗ u =[θ1u1,...,θdud] . By optimizing
where  is the unit matrix. The following equation follows               θ
from (5) and (8),                                     the kernel parameter with margin maximization or Radius-
                                                      Margin bound [Chapelle et al., 2002] minimization, and with
                        
                U  U = T  KzT  =  I                   a 1-norm or 0-norm penalizer on θ, feature selection can be
                                                      done by by choosing the features corresponding to the large
So the columns of U form an orthogonal basis of the subspace
                                                      elements of the optimized θ.
S.
                                                        Feature selection locates a discriminating subspace of the
  Thus, for φ(xi) ∈F, 1 ≤ i ≤ n, their projections into the
                                                      input space X . Similarly as the above approaches, we also
subspace S can be computed as
                                                      use kernel parameter learning algorithms to ﬁnd a discrim-
                               
              Xw  = U  X  = T  Z  X             (9)   inating subspace. However, in this paper, we address the
                                                      problem of feature extraction but not feature selection,and
where Xw is the matrix whose columns are the projections of the subspace we want to ﬁnd is contained in the feature space
φ(xi) in S, 1 ≤ i ≤ n.                                F but not the input space X .
  Having obtained the projected data Xw, we can now com-
pute the inner product between points in the subspace as the 3.2 Sparse Kernel Learning Algorithms
following:
                                                      The subspace kernel function given by (14) is in a general
     w                                                                                            Z =
   K    =   Xw Xw  = X  UU   X                 (10)   form. As described before, each column in the matrix
                                                      [z ,...,z ]                                 F
        =   XZTTZX                                   1     nf  (c.f (2)) is a vector in the feature space .Now
                                                      we show that this kernel relates to the work of [Wu et al.,
        =(XZ)K−1(XZ)
                    z                                 2005] in the special case where each column of Z has a pre-
        =(XZ)(ZZ)−1(XZ)                    (11)   image [Scholkopf¨ and Smola, 2002] in the input space X .
                                                                     z ∈F                    zˆ ∈X
        =(XXW)(WXXW)−1(XXW)                      That is, for each i  , there exists a vector i , such
                                                      that zi = φ(zˆi). So now the subspace S can be spanned by
        =(KW)(WKW)−1(KW)                            φ(zˆ ),...,φ(zˆ )
                                               (12)      1         nf .
                                                                          Zˆ =[φ(zˆ ),...,φ(zˆ )]
where we used equation (5) in the second line, equation (7) For convenience, let   1        nf   (note that
in the third line and equation (2) in the ﬁfth line. The equa- Zˆ = Z). Then in this case, according to (13), the subspace
tions (11) and (12) are identical to (3) and (4) respectively, kernel function now becomes:
therefore the lemma is proven.                                                 ˆ ˆ  ˆ −1 ˆ   
                                                             Kw(x, x )=φ(x)      Z(Z  Z)  Z  φ(x )
  The proof also tells that for a given W, the projection of the                  −1     
                                                                        =  ψzˆ(x)K   ψzˆ(x )         (16)
data into the subspace S introduced by W can be computed                          zˆ
as equation (9).                                            ψ (x)=φ(x)Zˆ     =[K(x,  zˆ ),...,K(x, zˆ )]
                                                      where  zˆ                        1           nf   ,
  Let Kw(·, ·) denote corresponding subspace kernel func-
                                                         K  = Zˆ Zˆ
tion. Then according to (3) and (4), for any x, x ∈X, the and zˆ    .
subspace kernel Kw(·, ·) between them can be computed as In [Wu et al., 2005], an algorithm for building Sparse
                                                      Large Margin Classiﬁers (SLMC) is proposed, which builds
                            −1     
   Kw(x, x )=φ(x)      Z(Z  Z)   Z  φ(x )      (13)   a sparse Support Vector Machine (SVM) [Vapnik, 1995] with
                                  −1              n                        n
              =   ψ(x) W(W     KW)    W   ψ(x )(14)    f expansion vectors, where f is an given integer. In [Wu
                                                      et al., 2005], it is pointed out that building an SLMC is
where                                                 equivalent to building a standard SVM with the kernel func-
                                       
          ψ(x)=[K(x,  x1),...,K(x, xn)]               tion computed as (16). And the SLMC algorithm essentially
                                                             n                       F
is the empirical kernel map [Scholkopf¨ and Smola, 2002]. ﬁnds an f dimensional subspace of , which is spanned by
                                                      φ(zˆ ),...,φ(zˆ )
  Equation (14) illustrates that the elements of W, by which 1     nf , and where the different classes of data are
the subspace S is described, also serve as the kernel parame- linearly well separated.
ters of Kw(·, ·). So in order to ﬁnd a discriminating subspace In [Wu et al., 2005], the kernel function (16) is obtained
S where different classes of data are well separated, we can with the Lagrange method, which is different from the one
turn to optimize the corresponding subspace kernel Kw. adopted in the above. And the kernel function (16) is a special
                                                      case of the subspace kernel (14). Therefore it can be seen
3  Connections to Related Work                        that based on the general subspace kernel (14), useful special
                                                      cases can be derived for some applications.
3.1  Feature Selection via Kernel Parameter
     Learning                                         4   Optimizing   Kw
  [                                     ]
In Weston et al., 2000; Chapelle et al., 2002 , kernel pa-        K
rameter learning approaches are adopted for feature selection We optimize w based on the Kernel-Target Alignment
                                                            [                   ]
problem. The kernel of the following form is considered (KTA) Cristianini et al., 2002 , which is a quantity to mea-
                                                      sure the degree of ﬁtness of a kernel for a given learning task.
            Kθ(u, v)=K(θ.   ∗ u, θ. ∗ v)       (15)   In particular, we compute W by solving the following KTA

                                                IJCAI-07
                                                  1127maximization problem:                                   Similarly, to compute ∇A(W), we can use the following
                             w    y                   equations:
                           K  , K F
      max   A(W)=                             (17)                 w            c        w
        n×n              w    w     y   y                       ∂K                      ∂K
    W∈R    f           K  , K F K , K F                         , Ky   =      yˆ(     )yˆ
                                                                ∂w        F           i ∂w     i     (23)
                                                                   uv            i=1       uv
where ·, ·F denotes the Frobenius product between two ma-
                                                                   w             nf       w
trices that are of the same size, i.e. for any two equally sized ∂K    w              ∂K
                                                                    , K F  =      xˆj (    )xˆj    (24)
matrices M and N, M, NF  =     ij MijNij. In (17),            ∂w                      ∂w
                                                                   uv            j=1       uv
Ky ∈  Rn×n is the gram matrix between the labels, deﬁned
by                                                    where wuv (1 ≤  u ≤  n, 1 ≤ v ≤ nf ) is the element of
                    Ky = YY                          W. Inspired by (23) and (24), we investigate how to compute
                                               (18)          w
                                                      α( ∂K  )α, where α ∈ Rn is an arbitrary vector. Actually,
     Y  =[y  ,...,y ] ∈ Rc×n     y              x         ∂wuv
where       1      n        , and i is the label of i, by performing linear algebra straightforwardly, we have
1 ≤ i ≤ n.
                  y                                                          w
  The elements in K reﬂect the similarities between labels,              ∂K
  Ky        1  x     x                           0                     α       α =2tuβv              (25)
as  ij equals if i and j belong to the same class, and                    ∂wuv
                           Kw      Ky
otherwise. Therefore ’aligning’ with   will make the        β      v                    β
similarities between the data points in the same class higher where v is the -th element of a vector , computed as
than the similarities between the points in different classes.                  −1   
                                                                   β =(W    KW)    (W   K)α          (26)
Thus by maximizing A(W), we can ﬁnd a subspace of F,
where points in the same class are closer to each other than and in (25), tu is the u-th element of a vector t, deﬁned as:
those in different classes. Hence a good classiﬁcation perfor-
mance can be expected for the data projected into this sub-             t = Kα − KWβ                 (27)
space.
                                                      Note that for any given α, the vectors β and t need to be
  Note that the subspace kernel allows us to apply many ker-
                                                      computed only once, according to (26) and (27) respectively,
nel parameter learning algorithms to the feature extraction     w
                                                      then α ∂K  α can be calculated as (25) for 1 ≤ u ≤ n and
problem. Therefore apart from KTA, we can also choose        ∂wuv
                                                      1 ≤ v ≤  n
other approaches to compute W, such as the one based on         f . Now we can apply (25) to (23) and (24), and
                                                      ∇A(W)
the Radius-Margin Bound [Chapelle et al., 2002]. For sim-     can be calculated.
plicity, we use KTA in this paper.
  Gradient based algorithms can be used to maximize   5   Experimental Results
A(W)
     . In our implementation, we use the conjugate gradient 5.1 Experimental Settings
algorithm to solve problem (17). To compute A(W), we uti-
                w                       y     
lize the fact that K = Xw Xw (see (10)) and K = Y Y   We empirically investigate the performance of the follow-
(see (18)). Thus, we can decompose Kw and Ky as follows ing KFE algorithms on classiﬁcation tasks: KPLS, KOC,
                                                      AKDA/QR and the proposed Subspace Kernel based Feature
                         nf
                  w                                  Extraction (SKFE) method. Following the same scheme in
                K     =      xˆixˆi            (19)   [Xiong et al., 2005], the features extracted by each KFE al-
                          i=1                         gorithm are input to a 1-Nearest Neighbor (1-NN) classiﬁer,
                         c
                   y                                 and the classiﬁcation performance on the test data is used to
                 K    =      yˆjyˆj            (20)   evaluate the extracted features. As a reference, we also re-
                          j=1                         port the classiﬁcation results of the 1-NN algorithm using the
                                                      input data directly without KFE.
     ˆ     n                                     
where xi ∈ R (1 ≤ i ≤ nf ) denotes the i-th column of Xw As mentioned before, in a c-class classiﬁcation problem,
         n                                    
and yˆj ∈ R (1 ≤ j ≤ c) denotes the j-th column of Y . the number of features nf extracted by both AKDA/QR and
  Based on the above two equations, we have           KOCisﬁxedatc. To compare with these two algorithms,
                                                      the value of nf for SKFE is also set to c in the experiments,
                           nf  c
                                                    although the number of features extracted by SKFE can be
           Kw, Ky    =         (xˆyˆ )2
                   F               i j         (21)   varied. For KPLS, three different values of nf are tried: c/4,
                          i=1 j=1                     c/2 and c. The best results are reported for KPLS.4
                           n  n
                          f  f                        For our proposed SKFE algorithm, the function A(W) in
             w   w                ˆ ˆ 2
          K  , K F   =         (xi xj)       (22)   (17) is not convex, so the optimization result depends on the
                          i=1 j=1                     initial choice of W. To get a good initial guess, we can use
                                                      the subspaces found by other KFE algorithms for initializa-
  Equation (21) and (22) can be computed with time com- tion. In the experiments, for efﬁciency we use the KOC algo-
      O(ncn  )     O(nn2 )                      n
plexity     f  and     f  respectively. When both f   rithm to compute the initial W.
and c are small, they are more efﬁcient than computing the
                                                         4
Frobenius product directly, which requires time complexity When c =2, only two values of nf are tried for KPLS: 1 and
      2
of O(n ).                                             2.

                                                IJCAI-07
                                                  11285.2  Experiments on Microarray Gene Expression          Seven text datasets from the TREC collections are adopted:
     Data                                             tr11, tr23, tr41, tr45, la1, la2 and hitech. More information
                                                      about these seven datasets are available at Table 1.
In this subsection, we take seven microarray gene datasets
                                                        Similar to the microarray gene data, the data used in text
to test various KFE methods: Brain Tumor1, Brain Tu-
                                                      classiﬁcation tasks are also of very high dimensionality. An-
mor2, Leukemia1, Leukemia2, Prostate Tumor, DLBCL and
                                                      other characteristic of these seven datasets is that they are
11 Tumors.5 Descriptions of these datasets are presented in
                                                      highly unbalanced, which means that the number of data con-
Table 1. As shown in Table 1, a typical characteristic of these
                                                      tained in different classes are quite different. For example, in
datasets is that the number of data n is much smaller than the
                                                      the tr11 dataset, there are 132 data points contained in the
data dimensionality d.
                                                      seventh class, while just 6 data points in the ninth class, only
                                                      4.6% of the former.
Table 1: Datasets adopted in the experiments. The ﬁrst seven On each dataset, we randomly select half of the data from
are microarray gene datasets, while the last seven are text each class to form the training set and use the remaining data
datasets. For each of them, the number of data n, the di- for test. As is done in the OC algorithm, the linear kernel is
mensionality d and the number of classes c are provided. used in this set of experiments. Similarly as before, for each
                                                      dataset, the experiment is repeated independently 20 times.
            Dataset  type  n     d    c               The average test error and the standard deviation over these
           B.Tumor1 GENE   90   5920  5               20 runs are reported in Table 3.
           B.Tumor2 GENE   50   10367 4                 Table 3 illustrates that SKFE outperforms other KFE meth-
           Leukemia1 GENE  72   5327  3               ods on most datasets. Also it can be seen from both Table 2
           Leukemia2 GENE  72   11225 3
            P.Tumor GENE   102  10509 2               and 3 that in most cases, all the KFE algorithms obtain bet-
            DLBCL   GENE   77   5469  2               ter performances than the 1-NN algorithm with the raw data,
           11 Tumors GENE  174  12534 11              whilst reducing the data dimensionality dramatically from d
             tr11   TEXT   414  6424  9               to nf , where nf << d. (c.f. section 5.1 for the choice of nf .)
             tr23   TEXT   204  5832  6
             tr41   TEXT   878  7454  10                Although SKFE compares favorably to the other KFE
             tr45   TEXT   690  8261  10              methods in terms of the classiﬁcation accuracy, its compu-
             la1    TEXT  3204  31472 6               tational cost is higher than the others. For the problems re-
             la2    TEXT  3075  31472 6
            hitech  TEXT  2301  10080 6               ported in Table 1, on a 2.2 GHz Pentium-4 PC, KPLS requires
                                                      from 0.15 to 39 seconds, AKDA/QR takes between 0.35 and
                                                      3 seconds, KOC requires between 0.11 and 5 seconds, while
  A Gaussian kernel is used in the experiments:
                                                      SKFE takes between 0.38 to 69 seconds. The optimization
           K(x, x)=exp(−γ   
 x − x 
2)      (28)   step of SKFE is implemented in C++, and the others are im-
                                                      plemented in Matlab.
  Five fold cross validation is conducted for parameter selec-
tion, and the best cross validation error rate is used to mea- 6 Conclusion
sure the performance of different algorithms. The experiment
                                                      We have presented a subspace kernel based on which nonlin-
is repeated 20 times independently. And the results in Ta-
                                                      ear feature extraction can be conducted by kernel parameter
ble 2 show the mean cross validation error and the standard
                                                      learning. Connections to related work have been explained.
deviation over these 20 runs.
                                                      In particular, the comparison with the Spare Large Margin
  From Table 2, we can observe that SKFE and KPLS com- Classiﬁer (SLMC) [Wu et al., 2005] illustrates that useful
pare favorably to the other KFE algorithms. In particular, special cases can be derived from the proposed subspace ker-
SKFE improves the results of KOC algorithm in all cases, nel for some applications. We have also described a method
although KOC is used to initialize SKFE. It can also be to optimize the subspace kernel by Kernel-Target Alignment
seen that SKFE and KPLS are competitive with each other. (KTA) [Cristianini et al., 2002] maximization. But other ker-
They are are not signiﬁcantly different (judged by t-test) on nel parameter learning approaches can also be applied. Fi-
Leukemia1, Leukemia2, DLBCL and 11 Tumors, and KPLS   nally, experimental results have been provided to validate the
is better than SKFE on Brain Tumor2, while SKFE outper- effectiveness of our approach.
forms KPLS on Brain Tumor1 and Prostate Tumor.
5.3  Experiments on Text Classiﬁcation                References
                                                      [                 ]
In this subsection, we investigate different KFE methods on Chapelle et al., 2002 O. Chapelle, V. Vapnik, O. Bousquet,
the text classiﬁcation task. It has been observed that there and S. Mukherjee. Choosing multiple parameters for sup-
usually exist cluster structures in the text data. The OC al- port vector machines. Machine Learning, 46(1-3):131–
gorithm (or equivalently the KOC algorithm with the linear 159, 2002.
kernel), which can keep these structures, is used for dimen- [Cristianini et al., 2002] N. Cristianini, J. Shawe-Taylor,
sionality reduction in text classiﬁcation tasks in [Kim et al., A. Elisseeff, and J. Kandola. On kernel-target alignment.
2005] and exhibits good results.                         In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,
                                                         Advances in Neural Information Processing Systems 14,
  5They are available at http://www.gems-system.org.     Cambridge, MA, 2002. MIT Press.

                                                IJCAI-07
                                                  1129