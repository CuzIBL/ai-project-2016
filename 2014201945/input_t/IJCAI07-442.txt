         Semi-supervised Learning for Multi-component Data Classiﬁcation

                          Akinori Fujino, Naonori Ueda,     and  Kazumi Saito
                               NTT Communication Science Laboratories
                                            NTT Corporation
                     2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0237
                               {a.fujino,ueda,saito}@cslab.kecl.ntt.co.jp

                    Abstract                          P (y|x) and learn mapping from x to y. Multinomial logistic
    This paper presents a method for designing a semi- regression [Hastie et al., 2001] can be used for this purpose.
    supervised classiﬁer for multi-component data such  It has been shown that discriminative classiﬁers often
    as web pages consisting of text and link informa- achieve better performance than generative classiﬁers, but
    tion. The proposed method is based on a hybrid    that the latter often provide better generalization performance
    of generative and discriminative approaches to take than the former when trained by few labeled samples [Ng and
    advantage of both approaches. With our hybrid ap- Jordan, 2002]. Therefore, hybrid classiﬁers have been pro-
    proach, for each component, we consider an indi-  posed to take advantage of the generative and discriminative
                                                      approaches [Raina et al., 2004]. To construct hybrid classi-
    vidual generative model trained on labeled samples                           j
    and a model introduced to reduce the effect of the ﬁers, the generative model p(x |y) of the jth component is
    bias that results when there are few labeled sam- ﬁrst designed individually, and all the component models are
    ples. Then, we construct a hybrid classiﬁer by com- combined with weight determined on the basis of a discrim-
    bining all the models based on the maximum en-    inative approach. It has been shown experimentally that the
    tropy principle. In our experimental results using hybrid classiﬁer performed better than pure generative and
    three test collections such as web pages and techni- discriminative classiﬁers [Raina et al., 2004].
    cal papers, we conﬁrmed that our hybrid approach    On the other hand, a large number of labeled samples are
    was effective in improving the generalization per- often required if we wish to obtain better classiﬁers with gen-
    formance of multi-component data classiﬁcation.   eralization ability. However, in practice it is often fairly ex-
                                                      pensive to collect many labeled samples, because class labels
1  Introduction                                       are manually assigned by experienced analysts. In contrast,
Data samples such as web pages and multimodal data usually unlabeled samples can be easily collected. Therefore, effec-
contain main and additional information. For example, web tively utilizing unlabeled samples to improve the generaliza-
pages consist of main text and additional components such tion performance of classiﬁers is a major research issue in
as hyperlinks and anchor text. Although the main content the ﬁeld of machine learning, and semi-supervised learning
plays an important role when designing a classiﬁer, additional algorithms that use both labeled and unlabeled samples for
content may contain substantial information for classiﬁcation. training classiﬁers have been developed (cf. [Joachims, 1999;
Recently, classiﬁers have been developed that deal with multi- Nigam et al., 2000; Grandvalet and Bengio, 2005; Fujino et
component data such as web pages [Chakrabarti et al., 1998; al., 2005b],see[Seeger, 2001] for a comprehensive survey).
Cohn and Hofmann, 2001; Sun et al., 2002; Lu and Getoor, In this paper, we focus on Semi-Supervised Learning for
2003], technical papers containing text and citations [Cohn Multi-Component data classiﬁcation (SSL-MC) based on
and Hofmann, 2001; Lu and Getoor, 2003],andmusicdata  probabilistic approach and present a hybrid classiﬁer for SSL-
with text titles [Brochu and Freitas, 2003].          MC problems. The hybrid classiﬁer is constructed by ex-
  In supervised learning cases, existing probabilistic ap- tending our previous work for semi-supervised learning [Fu-
proaches to classiﬁer design for arbitrary multi-component jino et al., 2005b] to deal with multiple components. In our
data are generative, discriminative, and a hybrid of the formulation, an individual generative model for each compo-
two. Generative classiﬁers learn the joint probability model, nent is designed and trained on labeled samples. When there
p(x,y), of feature vector x and class label y, compute are few labeled samples, the class boundary provided by the
P (y|x) by using the Bayes rule, and then take the most prob- trained generative models is often far from being the most
able class label y. To deal with multiple heterogeneous com- appropriate one. Namely, the trained generative models of-
ponents, under the assumption of the class conditional inde- ten have a high bias as a result of there being few labeled
pendence of all components, the class conditional probabil- samples. To mitigate the effect of the bias on classiﬁcation
ity density p(xj|y) for the jth component xj is individually performance, for each component, we introduce a bias cor-
modeled [Brochu and Freitas, 2003]. By contrast, discrim- rection model. Then, by discriminatively combining these
inative classiﬁers directly model class posterior probability models based on the maximum entropy principle [Berger et

                                                IJCAI-07
                                                  2754al., 1996], we obtain our hybrid classiﬁer. The bias correc- drawn from the marginal generative distribution p(x;Θ) =
                                                      K
tion models are trained by using many unlabeled samples to k=1 p(x,k;Θ). Model parameter set Θ is computed by
incorporate global data distribution into the classiﬁer design. maximizing the posterior p(Θ|D) (MAP estimation). Ac-
  We can consider some straightforward applications of the cording to the Bayes rule, p(Θ|D) ∝ p(D|Θ)p(Θ), we can
conventional generative and discriminative semi-supervised provide the objective function of model parameter estimation
learning algorithms [Nigam et al., 2000; Grandvalet and Ben- such as
gio, 2005] to train the above-mentioned classiﬁers for multi-        N           J
component data. Using three test collections such as web                               xj |  θj
                                                             F (Θ) =     log P (yn)  p( n yn; yn )
pages and technical papers, we show experimentally that our          n=1          j=1
proposed method is effective in improving the generaliza-
                                                                      M    K      J
tion performance of multi-component data classiﬁcation es-                               xj |  θj
pecially when the generative and discriminative classiﬁers         +     log    P (k)   p( m k; k)
provide similar performance.                                         m=1    k=1     j=1
                                                                   +logp(Θ).                          (2)
2  Straightforward Approaches to                      Here, p(Θ) is a prior over Θ. We can obtain the Θ value that
   Semi-supervised Learning                           maximizes F (Θ) using the Expectation-Maximization (EM)
                                                               [                 ]
2.1  Multi-component Data Classiﬁcation               algorithm Dempster et al., 1977 .
                                                        The estimation of Θ is affected by the number of unlabeled
In multi-class (K classes) and single-labeled classiﬁcation samples used with labeled samples. In other words, when
problems, one of the K classes y ∈{1,...,k,...,K} is     
                         x                            N    M, model parameter Θ is estimated as almost unsuper-
assigned to a feature vector by a classiﬁer. Here, each vised clustering. Then, training the model by using unlabeled
feature vector consists of J separate components as x =
{x1     xj     xJ }                                   samples might not be useful in terms of classiﬁcation accu-
   ,...,  ,...,   . In semi-supervised learning settings, racy if the mixture model assumptions are not true for actual
the classiﬁer is trained on both labeled sample set Dl =
{x    }N                                 {x  }M       classiﬁcation tasks. To mitigate the problem, a weighting pa-
  n,yn n=1 and unlabeled sample set Du =    m m=1.    rameter λ ∈ [0, 1] that reduces the contribution of the un-
Usually, M is much greater than N. We require a framework labeled samples to the parameter estimation was introduced
that will allow us to incorporate unlabeled samples without (EM-λ [Nigam et al., 2000]). The value of λ is determined
class labels y into classiﬁers. First, we consider straight- by cross-validation so that the leave-one-out labeled samples
forward applications of conventional semi-supervised learn- are, as far as possible, correctly classiﬁed.
ing algorithms [Nigam et al., 2000; Grandvalet and Bengio,
2005] with a view to incorporating labeled and unlabeled 2.3 Discriminative Classiﬁers
samples into generative, discriminative, and hybrid classiﬁers Discriminative classiﬁers directly model class posterior prob-
proposed for multi-component data.                    abilities P (y|x) for all classes. We can design a discrimina-
                                                                   |x
2.2  Generative Classiﬁers                            tive model P (y ) without the separation of components. In
                                                                                        [               ]
Generative classiﬁers model a joint probability density Multinomial Logistic Regression (MLR) Hastie et al., 2001 ,
p(x,y). However, such direct modeling is hard for arbi- the class posterior probabilities are modeled as
trary components that consist of completely different types                       exp(wk  · x)
                                                              P (y = k|x; W )=                  ,    (3)
of media. To deal with multi-component data in the genera-                       K
                                                                                 k=1 exp(wk · x)
tive approach, under the assumption that all components have
                                                      where W  =  {w1,...,wk,...,wK  } is a set of unknown
class conditional independence, the joint probability model          w  · x                          w
                    x                J    xj|  θj     model parameters. k   represents the inner product of k
can be expressed as p( ,k;Θ) = P (k) j=1 p(  k;  k)       x
                                    j                 and  .
(cf. [Brochu and Freitas, 2003]), where θk is the jth com- Certain assumptions are required if we are to incorpo-
                                              j
ponent model parameter for the kth class and Θ={θk}j,k rate unlabeled samples into discriminative classiﬁers, because
is a set of model parameters. Note that the class conditional p(x) is not modeled in the classiﬁers. A Minimum Entropy
                   j   j                              Regularizer (MER) was introduced as one approach to the
probability model p(x |k; θk) should be selected according
to the features of the jth component. The class posteriors for semi-supervised learning of discriminative classiﬁers [Grand-
all classes are computed according to the Bayes rule, such as valet and Bengio, 2005]. This approach is based on the empir-
                                                     ical knowledge that classes should be well separated to take
                             J     j    j
                      P (k)     p(x |k; θˆk)          advantage of unlabeled samples because the asymptotic in-
         |x ˆ                j=1
 P (y = k  , Θ) =                        j  , (1)   formation content of unlabeled samples decreases as classes
                    K          J    xj|  θˆ
                    k=1 P (k ) j=1 p(  k ; k )      overlap. The conditional entropy is used as a measure of class
             j                                        overlap. By minimizing the conditional entropy, the classiﬁer
      ˆ    {θˆ }
where Θ=     k j,k is a parameter estimate set. The class is trained to separate unlabeled samples as well as possible.
label y of x is determined as the k that maximizes P (k|x; Θ)ˆ . Applying MER to MLR, we estimate W to maximize the
  For the semi-supervised learning of the generative classi- following conditional log-likelihood with the regularizer:
ﬁers, unlabeled samples are dealt with as a missing class la-      N
bel problem, and are incorporated in a mixture of joint prob- F (W )= log P (yn|xn; W )
                                       x   ∈
ability models [Nigam et al., 2000].Thatis, m Du is                n=1

                                                IJCAI-07
                                                  2755               M   K
                                                    objective function for Θ estimation as
          + λ        P (k|xm; W )logP (k|xm; W )                                                    
                                                                J   N                   K
              m=1 k=1                                                        xj |   θj            θj
                                                       F1(Θ) =           log p( n yn; yn )+  log p( k) .
          +logp(W   ).                          (4)             j=1  n=1                  k=1
Here, λ is a weighting parameter, and p(W ) is a prior over                                           (6)
W . A Gaussian prior [Chen and Rosenfeld, 1999] is often     θj              θj
employed as p(W ).                                    Here, p( k) is a prior over k.
                                                        When there are few labeled samples, the classiﬁer obtained
2.4  Hybrid Classiﬁers                                by using the trained component generative models often pro-
Hybrid classiﬁers learn an individual class conditional prob- vides a class boundary that is far from the most appropriate
               j   j                                  one. Namely, the trained component generative models often
ability model p(x |y; θ ) for the jth component and directly
                   y                                  have a high bias. To obtain a classiﬁer with a smaller bias, we
model the class posterior probability by using the trained
                                                      newly introduce another class conditional generative model
component models [Raina et al., 2004; Fujino et al., 2005a].
                                                      per component, called bias correction model. The generative
Namely, each component model is estimated on the basis of a
                                                      and bias correction models for each component belong to the
generative approach, while the classiﬁer is constructed on the                                    {ψj }
basis of a discriminative approach. For the hybrid classiﬁers, same model family, but the set of parameters Ψ= k j,k
the class posteriors are provided as                  of the bias correction models is different from Θ. We con-
                                                      struct our hybrid classiﬁer by combining the trained com-
            R(y =  k|x; Θˆ , Γ)                       ponent generative models with the bias correction models to
                    J           j                    mitigate the effect of the bias.
                  μk        j      γj
                 e    j=1 p(x |k; θˆk)
          =  K        J            j   ,      (5)   3.2  Discriminative Class Posterior Design
                    μk       xj|  θˆ γj
               k=1 e    j=1 p(  k ; k )             We deﬁne our hybrid classiﬁer by using the class posterior
                                                      probability distribution derived from a discriminative combi-
where Θ=ˆ    {θˆj,k}j,k is a parameter estimate set of
                                                      nation of the component generative and bias correction mod-
                          {  xj|  θj }
component generative models p(  k; k) j,k,andΓ=       els. The combination is provided based on the Maximum En-
{{  }J   {  }K  }
  γj j=1, μk k=1  is a parameter set that provides the com- tropy (ME) principle [Berger et al., 1996].
bination weights of components and class biases. In super- The ME principle is a framework for obtaining a proba-
vised learning cases, Γ is estimated to maximize the cross- bility distribution, which prefers the most uniform models
validation conditional log-likelihood of labeled samples. that satisfy any given constraints. Let R(k|x) be a target
  To incorporate unlabeled samples in the hybrid classiﬁers, distribution that we wish to specify using the ME princi-
we can consider a simple approach in which we train the com- ple. A constraint is that the expectation of log-likelihood,
ponent generative models using EM-λ as mentioned in Sec-    j    j
                                                      log p(x |k; θˆ ), with respect to the target distribution R(k|x)
tion 2.2. With this approach, we incorporate unlabeled sam-      k
                                                      is equal to the expectation of the log-likelihood with respect
ples into the component generative models and then discrimi-                           
                                                                                x        N    x − x    −
natively combine these models. For convenience, we call this to the empirical distribution p˜( ,k)= n=1 δ( n,k
                                                      yn)/N of the training samples as
approach Cascade Hybrid (CH).                                     
                                                                                  j    j
                                                                     p˜(x,k)logp(x |k; θˆk)
3  Proposed Method                                                x,k
As mentioned in the introduction, we propose a semi-                                      j
                                                                       x     |x      xj|  θˆ  ∀
supervised hybrid classiﬁer for multi-component data, where    =     p˜( )R(k  )logp(   k; k), j,     (7)
bias correction models are introduced to use unlabeled sam-       x,k
                                                                   
ples effectively. For convenience, we call this classiﬁer Hy-        N
                                                      where p˜(x)=   n=1 δ(x − xn)/N is the empirical distribu-
brid classiﬁer with Bias Correction Models (H-BCM classi-   x                                   xj|  ψj
ﬁer). In this section, we present the formulation of the H- tion of . The equation of the constraint for log p( k; k)
BCM classiﬁer and its parameter estimation method.    can be represented in the same form as Eq. (7). We also re-
                                                      strict R(k|x) so that it has the same class probability as seen
                                                      in the labeled samples, such that
3.1  Component Generative Models and Bias                                   
     Correction                                                    p˜(x,k)=     p˜(x)R(k|x), ∀k.      (8)
We ﬁrst design an individual class conditional probability       x            x
                                       j
model (component generative model) p(xj|k; θ ) for the jth
                                       k              By  maximizing the  conditional entropy H(R)=
component xj of data samples that belong to the kth class,
                                                      −   x,k p˜(x)R(k|x)logR(k|x) under these constraints, we
          {θj }
where Θ=     k j,k denotes a set of model parameters over can obtain the target distribution:
all components and classes. In our formulation, the com-
ponent generative models are trained by using a set of la- R(y = k|x; Θˆ , Ψ, Γ)
                                                                              j
beled samples, Dl. Θ is computed using MAP estimation:          μk  J      j     γ1j   j    j γ2j
                                                               e    j=1 p(x |k; θˆk) p(x |k; ψk)
Θ=maxˆ   Θ{log p(Dl|Θ) + log p(Θ)}. Assuming Θ is in-
                                                       =  K        J            j             j    , (9)
                                                                 μ          j     γ1j   j      γ2j
dependent of class probability P (y = k), we can derive the k=1 e k  j=1 p(x |k ; θˆk ) p(x |k ; ψk )

                                                IJCAI-07
                                                  2756                    J       K
where Γ={{γ1j,γ2j}j=1,  {μk}k=1} is a set of Lagrange 4   Experiments
multipliers. γ1j and γ2j represent the combination weights 4.1 Test Collections
of the generative and bias correction models for the jth com-
ponent, and μk is the bias parameter for the kth class. The Empirical evaluations were performed on three test collec-
                                                                    1 Cora  2                           3
distribution R(k|x, Θˆ , Ψ, Γ) gives us the formulation of the tions: WebKB , ,and20newsgroups (20news)
discriminative classiﬁer, which consists of the trained com- datasets, which have often been used as benchmark tests for
                                                                                     [               ]
ponent generative models and the bias correction models. classiﬁers in text classiﬁcation tasks Nigam et al., 1999 .
  According to the ME principle, the solution of Γ in Eq. (9) WebKB contains web pages from universities. This dataset
is the same as the Γ that maximizes the log-likelihood for consists of seven categories, and each page belongs to one
                                                      of them. Following the setup in [Nigam et al., 1999],we
   |x ˆ                        xn   n  ∈   l [
R(k  ; Θ, Ψ, Γ) of labeled samples ( ,y ) D  Berger   used only four categories: course, faculty, project,andstu-
et al., 1996; Nigam et al., 1999].However, l is also used
                                      D               dent. The categories contained 4199 pages. We extracted
to estimate Θ. Using the same labeled samples for both Γ four components, Main Text (MT), Out-Links (OL), In-Links
and Θ may lead to a bias estimation of Γ. Thus, a leave-one- (IL), and Anchor-Text (AT), from each page. Here, MT is the
out cross-validation of the labeled samples is used to estimate
                                                      text description except for tags and links. The OL for a page
                       ˆ (−n)
Γ [Raina et al., 2004].LetΘ  be a generative model pa- consists of web page URLs linked by the page. The IL for
rameter set estimated by using all the labeled samples except a page is the set of web page URLs linking the page. AT
 x
( n,yn). The objective function of Γ then becomes     is the anchor text set for each page, which consists of text
          N                                          descriptions expressing the link to the page found on other
                           (−n)
 F2(Γ|Ψ) =    log R(yn|xn; Θˆ  , Ψ, Γ) + log p(Γ), (10) web pages. We collected IL and AT from the links within the
          n=1                                         dataset. For the MT and AT components, we removed stop
where p(Γ)  is a prior over Γ.  We used the Gaus-     words and vocabulary words included in only one web page.
sian prior [Chen and Rosenfeld, 1999] as p(Γ)  ∝    We also removed URLs included in only one page for the OL
          2   2                  2    2
  k exp(−μk/2ρ )  l,j exp{−(γlj −1) /2σ }. Global con- and IL components. There were 18525 and 496 vocabulary
                                               (−n)   words in the MT and AT of the dataset, respectively. OL and
vergence is guaranteed when is estimated with ﬁxed ˆ
                        Γ                    Θ        IL contained 4131 and 500 different URLs, respectively.
and Ψ,sinceF2(Γ|Ψ) is an upper convex function of Γ.
                                                        Cora contains more than 30000 summaries of technical pa-
3.3  Training Bias Correction Models                  pers, and each paper belongs to one of 70 groups. For our
In our formulation, the parameter set Ψ of the bias correction evaluation, we used 4240 papers included in 7 groups: /Ar-
models is trained with unlabeled samples to reduce the bias tiﬁcial Intelligence/Machine Learning/*. We extracted three
that results when there are few labeled samples. According components, Main Text (MT), authors (AU), and citations
                                                      (CI) from each paper. Here, MT consists of the text distri-
to R(k|x; Θˆ , Ψ, Γ) as shown in Eq. (9), the class label y of
             x                                        bution included in the papers, and AU is the set of authors.
a feature vector is determined as the k that maximizes the CI consists of citations to other papers. We removed vocab-
discriminative function:                              ulary words, authors, and cited papers for each component
                   J
                              j           j           in the same way as for WebKB. There were 9190 vocabulary
       x        μk      xj|  θˆ γ1 xj|  ψ   γ2
    gk( ;Ψ) =  e      p(   k; k) p(   k;  k) . (11)   words, 1495 authors, and 13282 cited papers in the dataset.
                   j=1                                  20news consists of 20 different UseNet discussion groups.
Here, when the values of gk(x;Ψ) for all classes are small, Each article belongs to one of the 20 groups. The test col-
the classiﬁcation result for x is not reliable, because gk(x;Ψ) lection has been used to evaluate a supervised hybrid classi-
is almost the same for all classes. Thus, we expect our clas- ﬁer for multi-component data [Raina et al., 2004]. Following
siﬁer to provide a large gk(x;Ψ)difference between classes the setup in [Nigam et al., 1999], we used only ﬁve groups:
for unseen samples, by estimating Ψ that maximizes the sum comp.*. There were 4881 articles in the groups. We extracted
of the discriminative function of unlabeled samples:  two components, Main (M) and Title (T), from each article,
                M     K                             where T is the text description following “Subject:” and M
     F3(Ψ|Γ) =      log   gk(xm;Ψ)+logp(Ψ),    (12)   is the main content of each article except for the title. We
                m=1    k=1                            removed stop words and vocabulary words included in only
                                                      one article. There were 19273 and 1775 vocabulary words in
where p(Ψ) is a prior over Ψ. We incorporate unlabeled
                                                      components M and T of the dataset, respectively.
samples into our hybrid classiﬁer directly by maximizing
    |
F3(Ψ Γ) in the same way as for a mixture model in genera- 4.2 Experimental Settings
tive approaches, where joint probability models are regarded
as discriminative functions.                          We used a naive Bayes (NB) model [Nigam et al., 2000] as a
  If Γ is known, we can estimate the Ψ that provides the lo- generative model and a bias correction model for each com-
cal maximum of F3(Ψ|Γ) around the initialized value of Ψ, ponent, assuming that different features (works, links, cita-
with the help of the EM algorithm [Dempster et al., 1977]. 1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-
However, Γ is not a known value but an unknown parameter 20/www/data/webkb-data.gtar.gz
that should be estimated using Eq. (10) with Θ and Ψ ﬁxed. 2http://www.cs.umass.edu/˜mccallum/data/cora-classify.tar.gz
Therefore, we estimate Ψ and Γ iteratively and alternatively. 3http://people.csail.mit.edu/jrennie/20Newsgroups/20news-
Ψ and Γ are updated until some convergence criterion is met. 18828.tar.gz

                                                IJCAI-07
                                                  2757tions, or authors) included in the component are independent. mentally that the H-BCM classiﬁer was useful for improving
xj was provided as the feature-frequency vector of the jth the generalization performance of multi-component data clas-
component. For the MAP estimation in Eqs (2), (6), and (12), siﬁcation with both labeled and unlabeled samples.
we used Dirichlet priors as the priors over NB model param- More speciﬁcally, the H-BCM classiﬁer outperformed the
eters.                                                MLR/MER classiﬁer except when there were many labeled
  For our experiments, we randomly selected labeled, un- samples for WebKB. This result is because MLR/MER tends
labeled, and test samples from each dataset. We made ten to be overﬁtted to few labeled samples. In contrast, H-BCM
different evaluation sets for each dataset by this random se- inherently has the characteristic of the generative models,
lection. 1000 data samples from each dataset were used as the whereby such an overﬁtting problem is mitigated. When
test samples in each experiment. 2500, 2000, and 2500 un- many labeled samples are available such that the overﬁtting
labeled samples were used with labeled samples for training problem can be solved, it would be natural for the discrimi-
these classiﬁers in WebKB, Cora, and 20news, respectively. native classiﬁer to be better than the H-BCM classiﬁer.
The average classiﬁcation accuracy over the ten evaluation The classiﬁcation performance with H-BCM was better
sets was used to evaluate the methods.                that with EM-λ except when there were few labeled samples
  We compared our H-BCM    classiﬁer with three semi- for WebKB. It is known that discriminative approaches often
supervised classiﬁers for multi-component data as mentioned provide better classiﬁcation performance than generative ap-
in Section 2: an NB based classiﬁer with EM-λ [Nigam et proaches when there are many labeled samples [Ng and Jor-
al., 2000], an NLR classiﬁer with MER (MLR/MER) [Grand- dan, 2002]. Our experimental result indicates that there might
valet and Bengio, 2005], and a CH classiﬁer. We also exam- be an intrinsic limitation preventing EM-λ from achieving a
ined three supervised classiﬁers: NB, MLR, and hybrid (HY) high level of performance because only weighting parameter
classiﬁers. NB, MLR, and HY classiﬁers were trained only λ is trained discriminatively.
on labeled samples.                                     H-BCM   provided better classiﬁcation performance than
  In our experiments, for EM-λ, the value of weighting pa- CH except when there were few labeled samples for WebKB.
rameter λ was set in the manner mentioned in Section 2.2 The H-BCM and CH classiﬁers are constructed based on a
Note that in our experiments we selected the value from four- hybrid of generative and discriminative approaches, but CH
teen candidate values of {0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, differs from H-BCM, in that the former does not contain bias
0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 1}to save computational time, but correction models. This experimental result indicates that in-
we carefully selected these candidate values via preliminary troducing bias correction models was effective in incorporat-
experiments.                                          ing unlabeled samples into the hybrid classiﬁer and thus im-
  For MLR/MER, the value of weighting parameter λ in  proves its generalization ability.
Eq. (4) was selected from sixteen candidate values of {{0.1×
  −n         −n         −n 4
10  , 0.2 × 10 , 0.5 × 10 }n=0, 1} that were carefully 5Conclusion
selected via the preliminary experiments. For a fair compar- We proposed a semi-supervised classiﬁer design method for
ison of the methods, the value of λ should be determined us- multi-component data, based on a hybrid generative and dis-
ing training samples, for example, using a cross-validation criminative approach, called Hybrid classiﬁer with Bias Cor-
of labeled samples [Grandvalet and Bengio, 2005].Wede- rection Models (H-BCM classiﬁer). The main idea is to de-
termined the value of λ that gave the best classiﬁcation per- sign an individual generative model for each component and
formance for test samples to examine the potential ability of to introduce models to reduce the bias that results when there
MLR/MER because the computation cost of tuning λ was  are few labeled samples. For evaluation, we also consid-
very high.                                            ered straightforward applications of conventional generative
                                                      and discriminative semi-supervised learning algorithms to the
4.3  Results and Discussion                           classiﬁers proposed for multi-component data. We conﬁrmed
Table 1 shows the average classiﬁcation accuracies over the experimentally that H-BCM was more effective in improv-
ten different evaluation sets for WebKB, Cora, and 20news ing the generalization performance of multi-component data
for different numbers of labeled samples. Each number in classiﬁcation than the straightforward approaches. Our exper-
parentheses in the table denotes the standard deviation of the imental results using three test collections suggest that the H-
ten evaluation sets. |Dl| (|Du|) represents the number of la- BCM classiﬁer is useful especially when the generative and
beled (unlabeled) samples.                            discriminative classiﬁers provide similar performance. Fu-
  As reported in [Raina et al., 2004; Fujino et al., 2005a],in ture work will involve applying H-BCM to multimodal data
supervised cases, the hybrid classiﬁer was useful for achiev- in which different generative models are employed.
ing better generalization performance for multi-component
data classiﬁcation. In our experiments, the average classiﬁ- References
cation accuracy of HY was similar to or better than that of [Berger et al., 1996] A. L. Berger, S. A. Della Pietra, and V. J.
NB and MLR.                                             Della Pietra. A maximum entropy approach to natural language
  We examined EM-λ, MLR/MER, CH, and H-BCM classi-      processing. Computational Linguistics, 22(1):39–71, 1996.
ﬁers for semi-supervised cases. The H-BCM classiﬁer out- [Brochu and Freitas, 2003] E. Brochu and N. Freitas. “name that
performed the other semi-supervised classiﬁers except when song!”: A probabilistic approach to querying on music and text.
the generative classiﬁer performed very differently from the In Advances in Neural Information Processing Systems 15, pages
discriminative classiﬁer for WebKB. We conﬁrmed experi- 1505–1512. MIT Press, Cambridge, MA, 2003.

                                                IJCAI-07
                                                  2758