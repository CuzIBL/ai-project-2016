                                 Recognizing Plan/Goal Abandonment* 

                      Christopher W. Geib,                                Robert P. Goldman, 
                     Honeywell Laboratories                                      SIFT Inc. 
                      3660 Technology Drive                              2119 Oliver Ave. South 
                     Minneapolis, MN 55418                               Minneapolis, MN 55405 
                christopher.geib@honeywell.com                           rpgoldman@siftech.com 

                        Abstract 

     The ability to recognize when an agent abandons a 
     plan is an open problem in the plan recognition lit•
     erature and is a significant problem if these meth•
     ods are to be applied in real systems. This paper 
     presents an explicit, formal, and implemented solu•
     tion to the problem of recognizing when an agent 
     has abandoned one of its goals based on a theory of 
     probabilistic model revision. 
                                                                       Figure 1: A simple model of plan execution. 

 1 Introduction 
                                                               pending set of primitive actions. The agent executes one of 
There is a large body of research on the topic of intent recog• the pending actions, generating a new pending set from which 
nition or task tracking, focused on identifying the goals of a the next action will be chosen, and so on. 
 user from observations of their actions. However, none of this  This process is illustrated in Figure 1. In this light, the 
work has directly addressed the problem of recognizing when    observed actions are nothing more than the observations of 
a user has abandoned a goal.                                   a hidden Markov model and the process of plan recognition 
   In general, the ability to infer abandoned goals is an op•  is the inference of the underlying state of the model.Space 
erational requirement for any plan recognition system that is  prohibits a full exposition of this approach. We refer readers 
executing incrementally and continuously. Abandoning goals     to [Geib and Goldman, 2001; Goldman et al., 1999] for a 
is something that any real observed agent will do. If a plan   more complete discussion. 
recognition system is unable to recognize this fact, the system 
will build up an ever increasing set of active or open plans   3 Exact Solutions 
that the agent has no intention of completing. A system at•
tempting to find completions for these open plans will wind    Given our model of plan execution, a formal and general 
up considering unreasonable situations such as the first step  model of probabilistic abandonment of goals, will be forced 
of a time critical two step plan simple plan are taken but the to deal with an exponentially larger model and will be re•
plan is not elder a two step plan with a required seconds or   quired to obtain the prior probabilities that each of the goals 
minutes duration but not attempting the second step of the     is abandoned. For real world applications this approach is 
plan until days or weeks later. Unfortunately, existing plan   simply untenable. A complete discussion of these issues is 
recognition systems cannot draw such inferences.               provided in the full paper. 

2 Background                                                   4 Model Revision 
Recent work in execution based plan/intent recognition [Bui 
                                                               Rather than explicitly considering all of the possible plans 
etai, 2002; Geib and Goldman, 2001; Goldman et al, 1999] 
                                                               that could be abandoned, the problem can be looked at as 
has been based on a model of the execution of simple hierar•
                                                               a question of model revision. If we are using a model of 
chical task network (HTN) plans [Erol et al, 1994]. 
                                                               plan execution that does not consider plan abandonment to 
   The idea behind this approach is that initially the executing recognize observation streams in which the agent is aban•
agent has a set of goals and chooses a set of plans to execute doning plans, we expect that the computed probabilities for 
to achieve these goals. The set of plans chosen determines a   the observation streams will be quite low. Laskey [1991], 
   *This material is based upon work supported by DARPA/IPTO   Jensen [Jensen et ai, 1990], and others have suggested that 
and the Air Force Research Laboratory under Contract No. F30602- cases of an unexpectedly small P(observations\M odd) 
02-C-0116                                                      should be used as evidence of a model mismatch. 


POSTER PAPERS                                                                                                       1515         Figure 2: A very simple example plan library. 

   Instead of the general P(observations\model) statis•
 tic we propose the probability that none of the ob•
 served actions in a subsequence (from say s to t) con•
 tribute to one of the goals (call it G), and we denote it 
                                               If this prob•
 ability gets unexpectedly small, we consider this as evidence 
 of a mismatch between the model and the real world. Namely         Figure 3: Required Evidence Theoretical Curves. 
 the model predicts that the agent is still working on the goal, 
 while the agent may have abandoned it.                        contributing to a given plan or goal. By computing this value 
                                                               and setting a threshold, we can consider any drop in this prob•
4.1 Computing notContrib 
                                                               ability below the threshold as sufficient evidence of a model 
 Consider the plan library shown in Figure 2. The first plan is mismatch and revise the model to reflect the goals abandon•
 a very simple plan for achieving S by executing a, 6, and c   ment. This requires removing all the elements from the cur•
 and the second plan for R has only the single step of g. Next, rent pending set that contribute to the abandoned goal. Mod•
 assume the following sequence of observations:                eling the rest of the plans continues as before. 

                                                               4.2 Evidential Requirements 
   In this case we know that at time 0 and 1 that the agent has This approach creates an interesting linkage between the size 
 as a goal achieving S. Let us assume that all of the elements of the pending set, the number of elements that contribute 
 of the pending set are equally likely to be selected for execu• to the goal of interest, and the number of actions that don't 
tion next. Note that this is an assumption that we will make   contribute to the goal that must be observed before the goal is 
for the rest of this paper. Nothing about the algorithm hinges considered abandoned. 
on this uniformity assumption. It is made solely for ease of     Figure 3 shows three theoretical curves for the probability 
computation and discussion.                                    of notContrib for different sets of values. The curves are 
   Given this assumption, the probability of seeing c at time 2 labeled with the number of actions that contribute to a goal 
is given by: where m is the number of elements                 and the size of the pending set. Thus the curve labeled " 1 of 
in the pending set that have c as the next action. The proba•  2" shows the drop in the probability given each observation 
bility that we don't see c (that is the probability that any other if there is one action that contributes to the desired goal out 
element of the pending set is chosen at time 2) is just:       of a pending set of size two. Note that in these curves, we 
                                                               are again making the assumption that all of the actions in the 
                                                               pending set are equally likely to be chosen. 
                                                                 Notice that as the ratio of the number of contributing ac•
or more generally the probability that we have seen b at time  tions to the size of the pending set drops the number of actions 
 (s - 1) and not seen c by time t:                             required to drive notContrib down to a particular threshold 
                                                               value increases significantly. We will see the effects of this in 
                                                               the empirical results. 

                                                               4.3 Estimating P(abandoned{g) \Obs) 
   To handle partially ordered plans, this formula must be     If we compute for each 
generalized slightly. With partially ordered plans it is pos•  g and threshold our explanations as described in the previ•
sible for more than a single next action to contribute to      ous section, we can now produce explanations of the obser•
the specified root goal. Thus, if mqi represents the num•      vations in which goals have been abandoned. By considering 
ber of elements (with any next action) in the pending set      the complete and covering set of such explanations for the ob•
at time i that contribute to goal q, (s-1) is the last time    servations we can estimate the probability of a specific goal's 
we saw an action contribute to q and t is the current time,    abandonment. It is given by: 


                                                               where Exp represents the set of all explanations for the ob•
   Thus, under the assumptions that we have made we can        servations, and represents the set of explanations in 
compute the probability of the subsequence of actions not     which goal g is marked as abandoned. 


1516                                                                                                  POSTER PAPERS                                                                 The number of test sequences that are not abandoned rises 
                                                              as the PAT is raised. As the threshold rises the system requires 
                                                              more and more evidence to be convinced that the goal has 
                                                              been abandoned. This allows more data points to reach the 
                                                              end of the observation sequence without being convinced of 
                                                              the goal's abandonment. 
                                                                Finally, abandoned rises and peaks giving the algorithm a 
                                                              maximum accuracy of about seventy five percent at a PAT of 
                                                              between 0.6 and 0.7. The algorithm's subsequent dip is again 
                                                              a result of the increasing confidence in abandonment required 
                                                              by the rising PAT. As with not abandoned at this point the 
                                                              system's accuracy is falling prey to the limited length of the 
                                                              test sequences. Since each test sequence is of limited length 
                                                              the test runs are ending before enough evidence can be ob•
                                                              served for the higher PAT values. Figure 3 will show us why. 
               Figure 4: Empirical Accuracy                     Consider the curve in Figure 3 labeled "l of 5." This is 
                                                              approximately the ratio of contributing actions to the size of 
                                                              the pending set in this example. At a PAT of 0.9 the system 
5 Accuracy                                                    will require approximately ten actions in a row that do not 
To test this theory we have extended Geib and Goldman's       contribute to the goal in order to convince itself of the goals 
Probabilistic Hostile Agent Task Tracker (PHATT) to esti•     abandonment. Since the longest any of the tests can be is 
mate goal abandonment. We will not cover the details of the   twenty three actions if the plan is abandoned more than half 
PHATT algorithm here; instead we refer the interested reader  way through the observation stream it will have a hard time 
to [Geib and Goldman, 2001]. We used a very simple plan li•   producing enough evidence to convince the system. 
brary with three root goals each having eight unordered steps. 
To generate test cases, we chose an ordering for the actions  6 Conclusions 
for each of the three goals. These plans were then randomly 
                                                              This paper presents a solution to the problem of recognizing 
interleaved preserving the intra-plan ordering. To simulate   when an agent has abandoned a goals based on probabilistic 
goal abandonment, at each time step one of the goals could    model revision. A number of issues are covered in more detail 
be chosen for abandonment. If chosen, all remaining steps of  in the full version of this paper available from the author. 
the plan were removed from the observation stream. When 
given to PHATT, each of these test sequences produced one     References 
of three possible results: 
abandoned where PHATT believed with probability greater       [Bui et al, 2002] Hung H. Bui, Svetha Venkatesh, and Geoff 
     than 0.5 that the correct goal had been abandoned.          West. Policy recognition in the abstract hidden markov 
                                                                 model. In Technical Report 4/2000 School of Computer 
not abandoned where PHATT did not believe believe with           Science, Curtin University of Technology, 2002. 
     probability greater than 0.5 that the correct goal had 
     been abandoned                                           [Erol etai, 1994] Kutluhan Erol, James Hendler, and 
                                                                 Dana S. Nau. UMCP: A sound and complete procedure 
not explained where PHATT was unable to explain the set          for hierarchical task network planning. In Proceedings of 
     of observations. In all cases, this was a result of the     the Second International Conference on Artificial Intelli•
     system believing that a goal was abandoned before it ac•    gence Planning Systems (AIPS 94), pages 249-254,1994. 
     tually had been. The system was therefore unable to ac•
     count for the remaining actions in that test data point. [Geib and Goldman, 2001] Christopher W. Geib and 
                                                                 Robert P. Goldman. Plan recognition in intrusion detec•
  The results of one thousand such randomly generated data 
                                                                 tion systems. In Proceedings of DISCEXII, 200I, 2001. 
points at each of nine notContrib threshold values between 
0.1 and 0.9 can be seen in Figure 4. To aid in understand•    [Goldman et al., 1999] Robert P. Goldman, Christopher W. 
ing, the X-axis plots (l - the notContrib threshold) which we    Geib, and Christopher A. Miller. A new model of plan 
will call the probability of abandonment threshold(PAT). The     recognition. In Proceedings of the 1999 Conference on 
y-axis plots the percentage of test points for each of the pos•  Uncertainty in Artificial Intelligence, 1999. 
sible results at that threshold value. The results confirm our [Jensen et al, 1990] Finn Verner Jensen, Bo Chamberlain, 
intuitions.                                                      Torsten Nordahl, and Frank Jensen. Analysis in hugin of 
  The number of test sequences that are not explained drops      data conflict. In Proceedings of the 6th Conference on Un•
to zero as the PAT is raised. The PAT is specifying how much     certainty in Artificial Intelligence, 1990. 
evidence the algorithm needs to have before it can consider a 
                                                              [Laskey, 1991] Kathy Blackmond Laskey. Conflict and sur•
goal abandoned. Since the system's failure to explain a test 
                                                                 prise: Heuristics for model revision. In Proceedings of the 
sequence is a result of prematurely believing a goal has been 
                                                                 7th Conference on Uncertainty in Artificial Intelligence, 
abandoned, as the PAT rises and more evidence is required 
                                                                 1991. 
this number should drop to zero. 


POSTER PAPERS                                                                                                      1517 