                    Learning to Understand Web Site Update Requests

         William W. Cohen                    Einat Minkov                    Anthony Tomasic
        Center for Automated        Language Technologies Institute   Institute for Software Research
       Learning & Discovery           Carnegie Mellon University        Carnegie Mellon University
     Carnegie Mellon University            einat@cs.cmu.edu                tomasic@cs.cmu.edu
        wcohen@cs.cmu.edu

                    Abstract                           Add the following contact to the Staff list.
                                                       Arthur Scott ascott@ardra.com Rm 7992 281 1914
    Although Natural Language Processing (NLP) for     On the events page, delete row "December 23 Assembly
    requests for information has been well-studied,    for Automotive Engineers Conference Room A21"
    there has been little prior work on understand-    On the people page under Tommy Lee delete 281 2000
    ing requests to update information. In this pa-    Please delete Kevin Smith’s phone number - thanx, Martha
    per, we propose an intelligent system that can     Change Mike Roberts to Michael Roberts.
    process natural language website update requests
    semi-automatically. In particular, this system can Figure 1: Example update requests (edited slightly for space
    analyze requests, posted via email, to update the and readability)
    factual content of individual tuples in a database-
    backed website. Users’ messages are processed us-
    ing a scheme decomposing their requests into a se- a waiting period before the human webmaster can incorporate
    quence of entity recognition and text classiﬁcation corrections, leading to long processing times, and a web site
    tasks. Using a corpus generated by human-subject  that is not up to date.
    experiments, we experimentally evaluate the per-
                                                        In this paper, we describe an intelligent system that can
    formance of this system, as well as its robustness
                                                      process website update requests semi-automatically. First,
    in handling request types not seen in training, or
                                                      natural language processing is used to analyze an incoming
    user-speciﬁc language styles not seen in training.
                                                      request. Based on the analysis, the system then constructs an
                                                      executable version of the proposed change, which is repre-
1  Introduction                                       sented as a pre-ﬁlled instance of a form. By examining the
In this paper, we present a natural language system that form, the end user can efﬁciently determine whether the anal-
helps a webmaster maintain the web site for an organiza- ysis step was correctly accomplished, and, if necessary, over-
tion. Speciﬁcally, we describe a system for understanding ride the results of the agent’s analysis by changing values in
certain natural-language requests to change the factual con- the form. Prior experiments with human subjects have shown
tent on a website. We will assume that the website is based that this process is an effective means of reducing human ef-
on a database, and focus on requests to update speciﬁc facts fort, even if the initial analysis step is imperfect [2004].
in this database.                                       This paper focuses on the natural-language processing part
  To motivate this, we note that although NLP for requests to of this system. As is typical of informal text like email, users’
deliver information(i.e., question-answering) has been well- messages are often ungrammatical, use capitalization patterns
studied, there has been little prior work on NLP for requests inconsistently, use many abbreviations and include typos (as
to update information. However, NLP for update requests illustrated in Figure 1). As a consequence, standard shallow-
is an attractive research problem, in part because a user can NLP tools such as part-of-speech tagging and noun-phrase
more easily detect an imperfectly-processed utterance. chunking, which are preliminary steps for text parsing, are
  As a concrete example of update requests, we consider quite unreliable. We therefore suggest here a learning ap-
here requests for web-site updates. Such a system would be proach, where rather than parse the text into a framework
practically useful, as many organizations maintain a single of pre-modeled domain knowledge, we decompose the gen-
large database-backed web site that includes information that eral task into a sequence of entity extraction and classiﬁcation
can be contributed or corrected by many individuals. Since sub-tasks. All of these sub-tasks can be learned from incom-
individual users, each of whom may only contribute a few ing messages, improving system performance over time.
database changes a year, may be reluctant to learn how to in- We will ﬁrst describe a scheme for decomposing request-
terface with the database to make their occasional updates, in understanding into a sequence of learning tasks. Next, we
many orginazations users submit update requests via email in describe the corpus of requests that is used for performance
natural language to a human webmaster. Frequently, there is evaluation. We then describe each of the learning sub-tasksin detail, along with experimental results. We also present sider request 4 in the ﬁgure: the previous analysis tells us we
experimental results on the robustness of the system – in par- should delete some attribute value from the tuple of the “per-
ticular, how the system will perform on request types not seen son” relation with the key value of “Tommy Lee”, but does
in training, or on user-speciﬁc language usage not seen in not specify the value to be deleted. Hence, to complete the
training. Finally, we evaluate the end-to-end system’s perfor- analysis for deleteValue requests, it is necessary to determine
mance, to determine what fraction of messages can be pro- the attribute that needs to be deleted. This is again a text clas-
cessed completely without errors. We conclude with a review siﬁcation task: given a database schema, only a ﬁxed number
of related work and our conclusions.                  of attributes need to be considered as possible targets.
                                                        For pedagogical reasons, we have described these steps
2  Understanding Update Requests                      as if they are taken separately. However, the steps are not
                                                      independent—i.e., information from each step of analysis
2.1  Analysis procedure                               may affect other steps. In section 6 we describe and eval-
Figure 1 gives some example web site update requests that are uate a particular sequence, where outputs of some steps are
addressed by the given analysis procedure. General requests propagated as inputs to the next steps.
that are not for factual update (e.g., “The animated gif in the
logo doesn’t ﬂash properly when I view it from my home
PC”) will simply be ﬂagged and forwarded to the real human 3 The Experimental Corpus
webmaster.
                                                      In order to collect an appropriate corpus, a series of controlled
  The analysis procedure contains the following steps.
                                                      human-subject experiments were performed, in which partic-
   Request type classiﬁcation. An informal prelimi-
                                                      ipants were given a series of tasks in pictorial form and asked
nary analysis of real webmaster request logs suggested that
                                                      that they compose and send an appropriate e-mail messages
factual-update requests are in one of the following forms: add
                                                      to a webmaster agent. In response to the user’s request, the
a new tuple to the database; delete an existing tuple; delete a
                                                      agent returned a preview of the updated page, and also pre-
value from an existing tuple; or alter (add or replace) a value
                                                      ﬁlled form that contained a structured representation of the
of an existing tuple. One step of the analysis is thus deter-
                                                      user’s request. The user could correct errors by editing text
mining the type of request. This is a text classiﬁcation task:
                                                      in various slots of the form, or by choosing from pull-down
each request will be mapped to one of the categories addTu-
                                                      menus.
ple, deleteTuple, deleteValue, alterValue. If it is not in one of
these categories, it will be mapped to otherRequest.    Overall, the human-generated corpus contains a total of
  Named entity recognition (NER). Another step of the only 617 example requests, involving approximately 20 sub-
analysis is to identify all entity names in a request. Figure 2 jects, and about 30 different tasks.
shows the result of correctly recognizing person names, email Note that the same pictorial task descriptions were pre-
addresses, phone numbers, room numbers, and event titles in sented to multiple users. This sort of duplication can lead
some sample requests. The subscript after an entity indicates to undesirable behavior for a learning system: if a certain
its type (for instance, “person” or “room number”).   pictorial task ,demonstrating addition of a phone number to
  Role-based entity classiﬁcation. We distinguish between a person named Greg Johnson for example, is represented by
four different roles for an entity in an update request. (a) multiple similar examples in the data, then the system might
An entity is a keyEntity if it serves to identify the database learn a correlation between the phrase “Greg Johnson” and
tuple which is to be modiﬁed. In the ﬁgure, key entities the task of adding a phone number. To address this problem,
are marked with a superscript K. An example is the entity we manually replaced duplicate entity names with alterna-
“Freddy Smith” in the sentence “please delete Freddy Smith’s tive values throughout the corpus, preserving surface features
phone number”. (b) An entity is a newEntity (marked with a such as capitalization patterns and misspellings.
superscript N) if it is a value to be stored in the database. (c) The requests in the corpus are largely factual updates con-
An entity is an oldEntity (superscript O) if it is a value cur- cerning a single tuple in the database, so we will focus our at-
rently in the database which the user expects to be replaced tention on such requests. Also, the relations in the underlying
with a newEntity. (d) Entities unrelated to the execution of the database schema of the corpus do not contain two attributes or
request are considered to be noiseEntities. In the ﬁgure, they more of the same type, where “type” is deﬁned by the output
have no superscript marking. Role-based entity classiﬁcation of the entity recognizer. For instance, personal details might
is an entity classiﬁcation task, in which entities produced by include a home phone number and an ofﬁce phone number,
the earlier NER step are given an additional classiﬁcation. but our corpus has no such duplications. Duplications of this
  Target relation classiﬁcation. The second column of Fig- sort would require an additional entity classiﬁer.
ure 2 shows the relation associated with each request. For As mentioned, the text itself is often ungrammatical and
any ﬁxed database schema, there is a ﬁxed set of possible re- noisy. We pre-processed the text, annotating it with a ver-
lations, so this is a text classiﬁcation operation.   sion of Brill’s part-of-speech tagger [Brill, 1995] and a hand-
  Target attribute classiﬁcation. Given entities, the roles of coded noun-phrase chunker which was tuned for email (us-
entities, the target relation, and the request type, the seman- ing a different corpus). In learning, however, we rely mainly
tics of the many tuple-based commands will be often com- on alternative features that exploit syntactic properties of the
pletely determined. One type of request that may still be un- messages. These features prove to be informative for the
derspeciﬁed is the deleteValue request. As an example con- noisy text in our corpus.                                   Request                          Request    Target    Target
                                                                     Type     Relation  Attribute
                                                       N
            Add the following contact to the Staff list. [Arthur Scott]person
        1                   N           N              N            addTuple   people      −
            [ascott@ardra.com]email Rm [7992]room [412 281 1914]phone
                                               K
            On the events page, delete row "[December 23]date [Assembly for
        2                               K               K          deleteTuple events      −
            Automotive Engineers Conference]eventTitle Room [A21]room"
                                          K
            On the people page under [Tommy Lee]person delete [412 281
        3       O                                                  deleteValue people  phoneNum
            2000]phone
            Please delete [Freddy Smith’s]K ’s phone number - thanx,
        4                          person                          deleteValue people  phoneNum
            [Martha]person
                              K                    N
        5   Change [Mike Roberts]person to [Michael Roberts]person on the alterValue people personName
            People page.
                                K
            Please add [Greg Johnson]person’s phone number- [412 281
        6       N                                                  alterValue  people  phoneNum
            2000]phone

                                     Figure 2: Analyzed update requests.

4  Learning                                                                     Test Set
                                                                 Type    Full Corpus Validation
Below we describe each of the individual learning tasks. Rel-    Time       95.7        n/a
evant experimental results are given for every component.        Date       96.1        97.7
                                                                 Email     100.0       100.0
4.1  Entity Recognition
                                                                           (a) Rules
Named Entity Recognition (NER), or the identiﬁcation of the
substrings of a request that correspond to entity names, is       Base f.  Tuned f.    Tuned features
a well-studied yet non-trivial Natural-Language Processing Type    5CV      5CV     5CVUSR    5CVREQ
task. We evaluated NER performance for seven linguistic  Time      87.7     91.2      88.2      93.9
types: time, date, amount, email addresses, phone numbers, Date    88.5     94.4      95.8      88.9
room numbers, and personal names. The data includes some Amount    89.7     93.1      93.1      85.4
mentions of additional entity types (e.g., job titles and orga- Phone 87.3  94.2      92.4      82.3
nization names) but not in sufﬁcient quantity for learning. Room#  81.9     90.4      87.1      83.0
  We experimented with two approaches to entity extraction: Person 80.9     90.3      83.6      88.3
a rule-based approach, in which hand-coded rules are used                 (b) Learning
to recognize entities; and learning-based extraction. The rule
language we used is based on cascaded ﬁnite state machines. Table 1: Entity recognition results: F1 measures
The learning algorithm we use here is VPHMM, a method
for discriminatively training hidden Markov models using a
voted-perceptron algorithm [Collins, 2002].           entity types, applying the VPHMM algorithm. Here NER is
  We found that manually constructed rules are best suited reduced to the problem of sequentially classifying each token
for entities such as e-mail addresses and temporal expres- as either “inside” or “outside” the entity type to be extracted.
sions. These types are based on limited vocabularies and Performance is evaluated by the F1-measure1, where entities
fairly regular patterns, and are therefore relatively easy to are only counted as correct if both start and end boundaries
model manually. Email addresses are an extreme example are correct (i.e., partially correct entity boundaries are given
of this: a simple regular expression matches most email ad- no partial credit.) The left-hand columns in the table (titled
dresses.                                              “5CV”) show F1-measure performance on unseen examples,
  Table 1(a) shows the results of extraction using hand-coded as estimated using 5-fold cross validation. The right-hand
rules for email and temporal expressions. We evaluated the columns will be discussed later.
rules on the main corpus, which was used for generating the Performance is shown for two sets of features. The base
rules, and also on a 96-message “validation set”, containing feature set corresponds to words and capitalization templates
messages which were collected in a second, later series of over a window including the word to be classiﬁed, and the
human-subject experiments (unfortunately, no time expres- three adjacent words to each side. The second set of features,
sions were present in this additional set.) As shown in the labeled tuned features in the table, is comprised of the base
table, the entity F1 performance is above 95% for all cases features plus some additional, entity-type speciﬁc features,
that could be evaluated.
  In Table 1(b) we show results for learning on the full set of 1F1 is the geometric mean of recall and precision.which are constructed using the same rule language used to “5CV” column. As shown by these results, the task of re-
build the hand-coded extractors. For example, in extracting lation determination is relatively straight-forward, provided
dates we added an indicator as to whether a word is a number sufﬁcient training data.
in the range 1-31; for personal names, we added an indica-
tor for words that are in certain dictionaries of ﬁrst and last Target       F1/Error             Def.
names.                                                   Relation   5CV      5CVUSR    5CVREQ     Error
  Overall, the level of performance for extraction – better people 99.7 / 0.3 99.3 / 0.8 97.3/ 3.4 38.7
than 90% for every entity type, using the tuned features – budget  100.0 / 0.0 99.2 / 1.6 78.8 / 3.6 10.0
is very encouraging, especially considering the irregularity events 99.6 / 0.2 97.4 / 1.1 97.8 / 1.0 22.7
of the text and the relatively small amount of training data sponsors 100.0 / 0.0 98.6 / 0.2 98.6 / 0.2 6.0
available. We found that users tend to use the terminology
and formats of the website, resulting in reduced variability. Table 3: Target relation classiﬁcation results
4.2  Role-based entity classiﬁcation
Once an entity span has been identiﬁed, we must determine 4.4 Request type classiﬁcation
its functional role—i.e., whether it acts as a keyEntity, newEn- In many cases the type of a request can be determined from
tity, oldEntity, or noiseEntity (as outlined in Section 2.1). We the roles of the entities in the request. For instance, an addTu-
approach this problem as a classiﬁcation task, where the ex- ple request has no keyEntities but may have multiple newEn-
tracted entities are transformed into instances to be further tities; conversely a deleteTuple request has keyEntities, but
classiﬁed by a learner.                               no newEntities; and only an alterValue request can have both
  The features used for the learner are as follows. (a) The keyEntities and newEntities. This means that most request
closest preceding “action verb”. An action verb is one of a types can be determined algorithmically from the set of en-
few dozen words generally used to denote an update, such as tity roles found in a request.
“add”, “delete”, etc. (b) The closest preceding preposition. The primary need for a request-type classiﬁer is to distin-
(c) The presence or absence of a possessive marker after the guish between deleteValue and deleteTuple requests. These
entity. (d) An indication whether the entity is part of a deter- types of requests are often syntactically quite similar. Con-
mined NP.                                             sider for instance the requests “delete the extension for Dan
  The experimental results for the important classes are Smith” and “delete the entry for Dan Smith”. The ﬁrst is a
shown in Table 2, in the column marked “5CV”. We used deleteValue for a phone number, and the second is a delete-
here an SVM learner with a linear kernel [Joachims, 2001]. Tuple request. The action verb (“delete”) and the included en-
We show results for each class separately, and in addition to tities, however, are identical. To distinguish the two request-
F1 performance for each category, we also show error rate. types, it is necessary to determine the direct object of the verb
The “Default Error” is the error obtained by always guessing “delete”—which is difﬁcult, since shallow parsing is inaccu-
the most frequent class.                              rate on this very noisy corpus—or else to construct features
                                                      that are correlated with the direct object of the verb.
 Entity                F1/Error            Default      Thus, we used the following as features. (a) The counts
                                                      of keyEntities, oldEntities, and newEntities in a request. (b)
 Role         5CV     5CVUSR    5CVREQ     Error
 keyEntity  87.0/11.5 83.5/14.4 84.0/14.3   44.2      The action verbs appearing in a request. (c) The nouns that
 newEntity   88.8/ 7.5 85.0/10.6 83.4/10.7  34.4      appear in an NP immediately following an action verb, or
 oldEntity   81.0/ 2.5 81.3 /2.5 76.4/ 3.0   6.7      that appear in NPs before an action verb in passive form. (d)
                                                      Nouns from the previous step that also appear in a dictionary
                                                      of 12 common attribute names (e.g., “phone”, “extension”,
     Table 2: Role-based entity classiﬁcation results “room”, “ofﬁce”, etc).
                                                        The results are shown in Table 4. With these features, one
  The  results for the role determination are almost- can distinguish between these request types quite accurately.
surprisingly good, considering the difﬁcult, linguistic nature
of this role assignment task. The set of features suggested Request            F1/Error            Def.
here is small and simple, and yet very informative, support- Type     5CV     5CVUSR    5CVREQ    Error
ing effective learning of roles even for semi-ungrammatical deleteTuple 93.1 / 2.4 92.6 / 2.6 74.7 / 9.2 18.0
texts.                                                  deleteValue 82.9 / 3.1 86.0 / 2.4 57.5 / 6.0 9.1

4.3  Target relation classiﬁcation                            Table 4: Request type classiﬁcation results
To determine the target relation, we used the same SVM
learner. The input features to the classiﬁer are a “bag-of-
words” representation of a request, as well as the entity types 4.5 Target attribute classiﬁcation
included in the request (for example, presence of a “phone The classiﬁcation of requests by target attributes is very sim-
number” entity in a request indicates a “people” relation, in ilar to request type classiﬁcation, except that rather than de-
our database schema.). Results are shown in Table 3 in the termining if a delete request concerns an attribute, one mustdetermine which attribute the request concerns. Given our as- every test fold are for tasks that were not encountered in the
sumptions, this step need only be performed for deleteValue training set. The results of this split are given in the columns
requests that do not specify an oldEntity value.      titled as “REQ”.
  Here in fact we learn a vocabulary for attributes names. A To summarize the results, the loss in performance for NER
simple bag-of-words feature works quite well for this task, problems is moderate, but larger than that seen when splitting
as is shown by the results in Table 5 in the “5CV” column. by users. Entity-role classiﬁcation drops off only slightly, and
The vocabulary used in the corpus to describe each attribute performance for target-relation classiﬁcation also remains ex-
is fairly small: e.g., phone is usually described as ”phone”, cellent for most relations. However, performance for request-
”line” or ”extension”. Perhaps this is because users tend to type classiﬁcation does drop off noticibly. This drop is al-
use the terminology of the website, or because the relevant most certainly due to lack of appropriate training data: there
vocabularies are limited by nature.                   are only a handful of tasks updating the “budget” relation, and
                                                      also only a relatively small number of tasks requiring request-
 Request                  F1/Error            Def.    type classiﬁcation. Similarly, the task of classiﬁcation by at-
 Type            5CV     5CVUSR    5CVREQ     Error   tribute name is practically infeasible for some attribute types
 personal name  77.3 / 2.8 66.7 / 4.2 17.5 / 7.6 7.0  in this settings, due to the small number of attribute names
 phone#         92.7 / 1.0 92.9 / 1.0 8.2 / 7.3 9.1   mentions in the corpus. However, provided that the system
 room#          87.0 / 2.9 91.5 / 1.0 56.9 / 9.1 18.0 is given sufﬁcient training data for the relevant relation and
 publication    79.6 / 3.7 81.2 / 2.1 44.8 / 5.2 9.1  attribute, it should perform well on different requests.
 photo          93.1 / 2.4 78.6 / 3.9 71.3 / 5.3 18.0
 CV             82.9 / 3.1 86.5 / 0.8 - / -    9.1    6   Overall Evaluation
 amount         93.1 / 2.4 92.5 / 1.0 92.7 / 1.0 18.0 In this section, we complement the component-level evalua-
                                                      tions with an evaluation of the entire end-to-end process. We
         Table 5: Attribute classiﬁcation results     executed the tasks in the following order: NER is run for
                                                      each entity type; then, roles of the extracted entities are as-
                                                      signed; ﬁnally, relation and request types are assigned. Note
5  Robustness Issues                                  that the noisy predicted entities (i.e., entities extracted by a
                                                      NER model) were used as input to the entity-role classiﬁer,
One practically important question is how robust this auto- as well as to the relation to the request-type classiﬁers. Here
mated webmaster is to changes in the distribution of users we used VPHMMs and hand-coded rules for extraction, and
and/or requests. To investigate such questions, one can use a non-sequential multi-class voted perceptron [Freund and
a different sampling strategy in performing cross-validation. Schapire, 1998] for classiﬁcation.2
For instance, to determine how robust the system is to queries From the user’s perspective, it is interesting to note what
from new users, we grouped all the examples generated by percentage of the requests can be successfully processed at a
each subject into a single set, and then performed a cross- message level, at different levels of automation. In the exper-
validation constrained so that no set was split between train- iments, 79.2% of the messages got both their relation and re-
ing and test. In other words, in every test fold, all of the quest type classiﬁed correctly. In these cases the user would
example requests were from subjects that had not contributed have received the correct form, with some entries ﬁlled out
to the training set. This split thus estimates performance of incorrectly. In more than half of the cases (53.4%), the user
a system that is used for a very large pool of users. Cross would have received the correct form, with all entities cor-
validation by user results are given in the results tables, in the rectly extracted, but with some entity roles mislabeled. In
columns marked as “USR”.                              39.5% of the messages, the automatic processing encountered
  In the corpus, users usually have some personal stylistic no errors at all.
quirks—for instance, a user might consistently give dates, Note that in the end-to-end scenario errors from the entity
names etc. in a particular format. Thus one would expect that recognition phase are propagated to role classiﬁcation task.
performance with this sort of split will be worse than perfor- Also, in order for a message to be considered fully correct,
mance with the default uniform splits. As can be seen from assignments must be accurate for each one of the multiple
the results, the F1 for most NER task drops only slightly, and entities included in this message. That is, many correct de-
is above 80 for all entity types. Slight drops in performance cisions must be made for perfect performance per request.
are also seen on two of the three entity-role tasks, and notici- Overall, we ﬁnd these results to be very promising, consider-
ble drops are seen on two of the seven attribute-classiﬁcation ing the limited size of our corpus.
tasks (person name and photo). Overall, performance seems
to be affected only slightly in this setting.         7   Related work
  Similarly, to determine how robust the system is to fu-
ture requests that are quite different from requests encoun- Lockerd et. al [2003] propose an automated Webmaster
tered during training, we grouped together examples for the called “Mr. Web” which has a similar email-based interface.
same request type (including all requests generated from a 2The voted perceptron is another margin-based classiﬁer. For im-
particular pictorial task), and then again performed a cross- plementation reasons, it was more convenient to use in these exper-
validation constrained so that no set was split between train- iments than an SVM, although its performance was generally quite
ing and test. In this scenario, all of the example requests in not as good.