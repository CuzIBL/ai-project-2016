                                   All Common Subsequences

                                               Hui Wang
                                 School of Computing and Mathematics
                               University of Ulster, Northern Ireland, UK
                                          h.wang@ulster.ac.uk


                    Abstract                          the longest common subsequence (LCS) [Hirschberg, 1977;
                                                      Bergroth et al., 2000].
    Time series data abounds in real world problems.
                                                        A subsequence is obtained from a sequence by removing
    Measuring the similarity of time series is a key
                                                      0 or more symbols, and a common subsequence of a set of
    to solving these problems. One state of the art
                                                      sequences is a subsequence of every sequence in the set. LCS
    measure is the longest common subsequence.This
                                                      advocates the use of the longest common subsequence as an
    measure advocates using the length of the longest
                                                      indication of the similarity relationship between sequences.
    common subsequence as an indication of simi-
                                                      However the common information shared between sequences
    larity between sequences, but ignores information
                                                      is more than that contained in the longest common subse-
    contained in the second, third, ..., longest subse-
                                                      quence. The second longest, third longest and, in general, all
    quences.
                                                      common subsequences contain some common information to
    In order to capture the common information in se- a certain degree. The use of all possible common information
    quences maximally we propose a novel measure of   between sequences is intuitively appealing and may provide
    sequence similarity – the number of all common    a fuller picture about the similarity relationship between the
    subsequences. We show that this measure satisﬁes  sequences. How to measure all such common information
    the common properties of similarity functions. Cal- between two sequences?
    culating this measure is not trivial as a brute force
                                                        The above question can be answered by addressing a new
    approach is exponential in time. We present a novel
                                                      problem, all common subsequences (ACS) – counting the
    dynamic programming algorithm to calculate this
                                                      number of all common subsequences. We expect that two
    number in polynomial time. We also suggest a dif-
                                                      sequences are more similar to each other if they have more
    ferent way of extending a class of such measures
                                                      common subsequences. ACS is conceptually novel as far as
    to multidimensional, real-valued time series, in the
                                                      we know. To motivate ACS, we consider three sequences
    spirit of probabilistic metric spaces.
                                                      {α, β, γ} = {cbabca, bcabac, abcade}.Thesetofallcom-
    We conducted an experimental study on the new     mon subsequences of α and β is (ignoring the empty se-
    similarity measure and the extension method for   quence): {a, aa, ab, aba, abc, ac, b, ba, baa, bab, baba, babc,
    classiﬁcation. It was found that both the new sim- bac, bb, bba, bbc, bc, bca, c, ca, caa, cab, caba, cabc, cac,
    ilarity and the extension method are consistently cb, cba, cbac, cbc, cc}. The set of all common subsequences
    competitive.                                      of α and γ is: {a, aa, ab, aba, abc, abca, ac, aca, b, ba, bc,
                                                      bca, c, ca}. Clearly lcs(α, β)=lcs(α, γ)=4.However
1  Introduction                                       acs(α, β)=31, acs(α, γ)=152, suggesting there is a rela-
                        1                             tively higher degree of commonality in α and β than in α and
A time series is a sequence of symbols or real numbers, γ. However this difference can not be revealed by LCS.
representing the measurements of a variable at, usually, equal
time intervals [Gunopulos and Das, 2000]. When more than Solving the ACS problem is not trivial. A brute-force ap-
one variable are involved we get a multidimensional time se- proach has an exponential time complexity as we need to ver-
ries. Examples of time series include: stock price move- ify all subsequences. We take a dynamic programming ap-
ments, volume of sales over time, daily temperature read- proach, and develop a novel algorithm to calculate ACS be-
ings, ECG signals, and gene expressions. In tasks involv- tween two sequences in polynomial time.
ing time series we need to consider the issues of indexing In order to evaluate ACS as a similarity measure we need
and searching [Agrawal et al., 1995; Goldin et al., 2004] and to extend ACS, as well as LCS, to real-valued, multidimen-
similarity measuring [Gunopulos and Das, 2000] – the latter sional time series as both LCS and ACS are deﬁned in terms
being the focus of this paper. One state of the art measure is
                                                         2We use a convention here: when talking about a concept like
  1Throughout this paper we use the terms “time series” and “se- LCS or ACS, we use capital letters. When talking about the same
quence” interchangeably.                              concept as a measure function, we use lower case letters.

                                                IJCAI-07
                                                   635            of symbolic sequences. We suggest a novel method for this 3 All common subsequence
            purpose, which requires determination of equality/inequality. Here we consider sequences of discrete symbols. In the next
            This is done in the spirit of probabilistic metric spaces,where section we will consider sequences of multidimensional, real-
            we consider the probability that two data points are the same. valued sequences.
            This extension method is different from the threshold-based Let A be a ﬁnite set of symbols. A sequence α is an ordered
            extension method [Vlachos et al., 2003],wheretwodata         n
                                                                  set {si}0 = {s0,s1, ··· ,sn},wheresi ∈A. For simplicity
            points in a multidimensional time series are deemed equal we also write the above sequence as s0s1 ···sn.Theempty
            if they are close enough (judged by a threshold) in every di- sequence is an empty set and is denoted by . The data space
            mension. Under this extension the ACS similarity is evalu- V is then the set of all sequences.
            ated in an experimental study with some real-valued, multi- Consider two symbols a, b in a sequence α. Looking from
            dimensional time series. The study shows that ACS is favor- left to right, if a comes before b we write a ≤α b. Consider
            ably comparable to LCS in general, and offers some property two sequences α and β.Ifα can be obtained by deleting
            that is absent from LCS. Additionally the extension method some symbols from β,wesayα is a subsequence of β (or,
            is sound and competitive consistently.                equivalently, β is a supersequence of α) and we write α  β.
              The rest of the paper is organized as follows. The longest It is obvious that the  relation is reﬂexive and transitive.
            common subsequence concept is brieﬂy reviewed in next sec- A common subsequence of two sequences is a subsequence
            tion as it inspired the work presented here. Our all common of both sequences. For two identical sequences, they have the
            subsequence concept is presented in Section 3, along with a maximal number of common subsequences relative to their
            dynamic programming algorithm to calculate ACS. A method length. For two different sequences, they have the minimal
            is presented in Section 4, which can be used to extend ACS number of common subsequences. Therefore the number of
            as well as LCS to multidimensional, real-valued time series. common subsequences is an indication of similarity. Whether
            Evaluation is discussed in Section 5 and related work is dis- or not it is a good similarity indicator is unclear, but will be
            cussed in Section 6, followed by some concluding remarks. evaluated in a later section. We call this all common subse-
                                                                  quences (ACS) similarity. For two sequences α and β,we
            2  Longest common subsequence                         write acs(α, β) for the ACS similarity of the two sequences.
                                                                  Lemma 1.  The ACS similarity has the following properties:
            The longest common subsequence, as a computer science
            problem, is concerned with the search for an efﬁcient method • acs(α, β) ≥ 0.
            of ﬁnding the longest common subsequence of two or more • acs(α, β) ≤ acs(α, α) and acs(α, β) ≤ acs(β,β).
            sequences. A milestone in the study of LCS is the devel- •                      (    )=    (    )
                                                    [                 ACS is symmetric, i.e., acs α, β acs β,α ;
            opment of dynamic programming algorithms Hirschberg,                       r  r         r
            1977; Bergroth et al., 2000], which calculates LCS in poly- • acs(α, β)=acs(α ,β ),whereα is the reverse se-
            nomial time.                                              quence of α.
              The dynamic programming algorithm for calculating LCS The ﬁrst two properties are common to all similarity mea-
            was originally designed to work for one dimensional se- sures [Osborne and Bridge, 1997], but the last two are ad-
            quences of discrete values. It was extended in [Vlachos et al., ditional. The proof of the lemma follows directly from the
            2003] to work for multidimensional, real-valued sequences as deﬁnition of ACS. The following lemma shows how many
            follows:                                              subsequences there are in one sequence, which can be proved
              Let A and B be two d-dimensional sequences with length by mathematical induction.
                                     =(      ···   )         =
            n and m  respectively: A      a1,   ,an  and B        Lemma 2.  For a sequence  of length m, the number of its
            (  ···    )            =(      ···    )          =                    m   i      m
             b1,  ,bm  ,whereai        ai,1,  ,ai,d ,andbj        subsequences is        =2   , which includes the empty
            (   ···     )            ( )=(       ···     )                        i=0 m
             bj,1,  ,bj,d .LetHead   A        a1,   ,an−1  and    sequence.
            Head(B)=(b1,   ··· ,bm−1). Then the LCS similarity be-
            tween A and B is [Vlachos et al., 2003]:                The relationship between sequence and subsequence is
                                                                  shown in the following lemmas. Let N(α|γ) be the set of
            (1)          ⎧                                                                                        ( )
                         ⎪0                                       subsequences of α that are supersequences of γ,andN α
                         ⎪  ,    if A or B is empty
                         ⎪1+       (     ( )      ( ))            be the set of all subsequences of α.
                         ⎪    lcsδ, Head A ,Head  B  ,
                         ⎨                                        Lemma 3.  Consider γ1,γ2  α.Then
                                 if |an,i − bm,i| <for 1 ≤ i ≤ d
            lcsδ,(A, B)=                                                          ⇐⇒    (  | ) ⊆   ( | )
                         ⎪       and |n − m|≤δ                               γ1   γ2     N α γ2    N α γ1
                         ⎪
                         ⎪     (  δ,(    ( )   )                       ⇒                            ∈   ( | )
                         ⎩⎪max  lcs  Head  A ,B  ,                Proof.  : Suppose γ1  γ2. Consider x  N α γ2 .Then
                                 lcsδ,(A, Head(B))), otherwise   γ2   x. Since the relation  is clearly transitive we have
                                                                  γ1  x. Therefore x ∈ N(α|γ1).WethenhaveN(α|γ2)  ⊆
            where δ is an integer that controls matching in time, and N(α|γ1).
            0 <<1   is a real number that controls matching in space. ⇐: Suppose N(α|γ2) ⊆ N(α|γ1). By deﬁnition we have
            The LCS algorithm, thus restricted, has a computational com- γ2  x for all x ∈ N(α|γ2),andγ1  y for all y ∈ N(α|γ1).
            plexity of O(δ(n + m)) if we only allow a matching window To show that γ1  γ2, we assume it is not true. Then γ2  ∈
            δ in time [Das et al., 1997]. The advantage of LCS is that it N(α|γ1),butγ2 ∈ N(α|γ2). Therefore it is not possible
            supports elastic and imprecise matches.               N(α|γ2) ⊆ N(α|γ1), contradicting the assumption.

                                                            IJCAI-07
                                                               636                            ( ) ⊆  ( )
Lemma 4.  α   β if and only if N α N β .               Algorithm 3.1: calACS – calculate all common subse-
                                                       quence
                                                        Input: Two sequences A and B.
      ⇒                          ∈   ( )               Output: The number of all common subsequences
Proof.  : Suppose α  β. Consider x  N  α .Thenx  is             acs(A, B).
a subsequence of α, i.e., x  α. By the transitivity of  we
                                                      1 begin
have x  β,sox is a subsequence of β. Therefore we have
                                                      2    for i =0to |A| do N(i, 0) = 1
x ∈ N(β). This proves N(α) ⊆ N(β).
                                                      3    for j =0to |B| do N(0,j)=1
⇐: Suppose N(α) ⊆ N(β). To show that α  β we assume  4    for i =1to |A| do
it is not true. Then α  ∈ N(β).Howeverα ∈ N(α).There- 5       for j =1to |B| do
fore it is not possible that N(α) ⊆ N(β), contradicting the 6     if Ai = Bj then
assumption. This proves α  β.                                    N(i, j)=N(i − 1,j−  1) × 2
                                                      7           else N(i, j)=
                                                                  N(i − 1,j)+N(i, j − 1) − N(i − 1,j− 1)
3.1  Calculating the number of all common
                                                      8       end
     subsequences
                                                      9    end
                                                      10   acs(A, B)=N(|A|,  |B|)
Now we discuss how to calculate the number of all common
                                                      11 end
subsequences, acs(A, B), of any two sequences A and B.
Since the number of subsequences of a sequence is exponen- Note that the δ parameter as discussed in Section 2 can also
tial in its length, a straightforward brute force approach is be used together with the above calACS algorithm.
clearly not feasible. We turn our attention to dynamic pro- The acs(A, B) is unbounded. We can normalize it by the
gramming.                                             number of all subsequences, which can be calculated as in
  “Dynamic programming can be thought of as being the re- Lemma 2.
verse of recursion. Recursion is a top-down mechanism – we
take a problem, split it up, and solve the smaller problems that 4 Extension of LCS and ACS
are created. Dynamic programming is a bottom-up mecha- The ACS is presented in the previous section on the assump-
nism – we solve all possible small problems and then combine tion that a sequence is an ordered set of symbols or dis-
them to obtain solutions for bigger problems. The reason that crete values. When a sequence is real-valued or multidimen-
this may be better is that, using recursion, it is possible that sional, the dynamic programming algorithm for calculating
we may solve a small subproblem many times. Using dynamic ACS must be extended.
programming, we solve it once. ” [Hirschberg, 2005]     This is also the case for LCS, but a method is presented in
                                                      [Vlachos et al., 2003] to extend LCS. This extension method
Theorem 1. Consider two sequences, A =(A1, ··· ,Am)   can be directly used for ACS but, as discussed in Section 2,
and B =(B1, ··· ,Bn).LetN(i, j) be the number of com- the threshold  is hard to choose and is usually different for
mon subsequences of (A1, ··· ,Ai) and (B1, ··· ,Bj), i.e., different data. This is not desirable [Keogh et al., 2004].
the preﬁxes of sequences A and B of lengths i and j.Then Here we present a different solution for the problem
                                                      of extending LCS and ACS, possibly some other similar-
 N(i, j)=N(i  − 1,j−  1) × 2, if Ai = Bj              ity/distance measures as well, where we need to decide if two
 N(i, j)=N(i  − 1,j)+N(i, j − 1) − N(i − 1,j− 1),     points are equal (equality checking). Our solution is in the
                                                                                      [
                                     =                spirit of Probabilistic Metric Spaces Schweizer and Sklar,
                                if Ai Bj              1983], where the line between ’equal’ and ’not equal’ is prob-
                                                      abilistic. Consider two sequences A =(A1, ··· ,Am) and
               (    )=   (    )
Consequently acs A, B  N  m, n .                      B  =(B1, ··· ,Bn). For any pair Ai and Bj we need to de-
                                                      cide, at some point in the LCS and ACS algorithms, if they
                                                      are equal. The answer to this question may not be determin-
                                                      istic, especially when real numbers are involved. In general
Proof. Let S(i − 1,j − 1) be the set of all common subse- the question can be answered by measuring the probability,
                                                                                     3
quences between (A1, ··· ,Ai−1) and (B1, ··· ,Bj−1).So Prob(Ai = Bj), that they are equal .
N(i − 1,j− 1) = |S(i − 1,j− 1)|.                        A general template for both LCS and ACS is the following.
                                                                                             (    ···   )
  If Ai = Bj,thenS(i, j)=S(i−1,j−1)∪S(i−1,j−1)Ai.     The distance/similarity between a sequence A1, ,Ai
                                                                         (   ···   )
Here S(i − 1,j − 1)Ai = {xAi : x ∈ S(i − 1,j − 1)}.   and another sequence B1,  ,Bj  is
                                                                             
Therefore N(i, j)=N(i − 1,j− 1) × 2.                                                    =
                                                                     (  )=   DS1    if Ai  Bj
     i  = j                                           (2)         DS  i, j
  If A   B , then some new common subsequences may be                        DS0    otherwise
added to S(i, j − 1) or S(i − 1,j) on top of S(i − 1,j− 1).
Therefore S(i, j)=S(i, j −1)∪S(i−1,j)−S(i−1,j−1).        3Probabilistic Metric Spaces are concerned with a more general
Consequently we have N(i, j)=N(i, j − 1)+ N(i − 1,j)− question: what is the probability that the distance between Ai and
N(i − 1,j− 1).                                        Bj is less than a given threshold δ?

                                                IJCAI-07
                                                   637In the case of LCS, DS1 =1+DS(i − 1,j− 1),andDS0 =      • ncm:   the NCM   similarity (see [Wang and Dub-
max(DS(i−1,j),DS(i, j−1)). In the case of ACS, DS1 =      itzky, 2005]), used in our extended similarity measures
DS(i − 1,j−  1) × 2,andDS0  = DS(i, j − 1) + DS(i −       (lcs w ncm, acs w ncm).
1  ) −   ( − 1   − 1)
 ,j   DS  i   ,j     .                                  • lcs vhgk: the extended LCS presented in [Vlachos et
  We propose to use the following template instead:       al., 2003] (see Section 2).
(3)
                                                        • acs vhgk: ACS extended in a way similar to lcs vhgk.
DS(i, j)=DS1  × Prob(Ai = Bj)+D0   × Prob(Ai  = Bj)
                                                        • lcs w euc: LCS extended as in Eq.3, where the prob-
where Prob(Ai  = Bj)=1−    Prob(Ai  = Bj). This ap-       ability is estimated (simplistically) as the normalized
proach avoids using the threshold .                      Euclidean distance.
                                      (   =    )
  The question now is how to determine Prob Ai Bj .In   • acs w euc: ACS extended as in Eq.3, where the proba-
the case of symbols or discrete values, we can simply and bility is estimated as the normalized Euclidean distance.
reasonably take Prob(Ai = Bj)=1if    Ai  = Bj,and
                                                        •
Prob(Ai =  Bj)=0otherwise. In the case of real num-       lcs w ncm: where the probability is the normalized
bers, either 1-dimensional or multidimensional, we can esti- NCM.
mate Prob(Ai =  Bj) by the distance between Ai and Bj   • acs w ncm: where the probability is the normalized
normalized to the range of [0, 1]. Note that the distance can NCM.
be the Euclidean distance or other distance. This is a sim- We used the leave-one-out evaluation strategy in the k-
plistic approach, but a more involved approach is beyond the nearest neighbors (kNN) classiﬁcation framework, meaning
scope of this paper.                                  that we classify every time series based on all other time se-
                                                      ries and then we measure the classiﬁcation accuracy. There
5  Experimental evaluation                            are two parameters in the experiment: δ and , which are
                                                      required by acs and lcs. For simplicity we set δ =3
We conducted an experimental study to evaluate the ACS for all measures, without loss of generality. For  we test-
similarity measure along with the extension method. In this experimented on samples of the data with various values and
study we demonstrate the utility of ACS for classiﬁcation set it to a value that gives the best overall result, that is,
in an experiment on some public datasets. We consider the  =0.13. Additionally we set parameter k (required for kNN)
following multidimensional, real-valued time series: ECG2c, to 1, 4, 7, 10, 13, 16, 19.
ECG3c and Auslan.                                       Experimental results are presented in Tables 1, 2, 3, and 4,
  The  ECG   datasets are from the BIDMC   Conges-    from which we observe the following:
tive Heart Failure Database.  The  original database    • when used as part of acs or lcs, ncm gives consistently
                                            [
contains two ECG   signals, but Keogh et al  Keogh        better results than euc: acs w ncm > acs w euc and
            ]
et al., 2004  separated the signals and  create two       lcs w ncm > lcs w euc.
datasets (ECG2c –  ecg clustering.txt and ECG3c  –
                                                        •
ecg clustering 3class.txt) of one-dimensional time series. acs has an edge over lcs, especially for relatively
Each instance of 3,200 contiguous data points (about 20   large k: acs w euc > lcs w euc and acs w ncm >
heartbeats) of each signal is randomly extracted from each lcs w ncm.
long ECG signals of each patient (class). Twenty instances • The extension method presented in this paper (see Sec-
are extracted from each class, resulting in eighty total in- tion 4) has an edge over that suggested in [Vlachos et
stances for each dataset.                                 al., 2003], especially for small k values: lcs w euc >
  The Australian Sign Language (Auslan) data is available lcs vhgk.
from [Keogh and Folias, 2002]. We consider a clean, pre- We believe that the superior performance of ACS over LCS
processed version of Auslan. There are 10 classes: ‘come’, is due to the fact that ACS is able to capture more common
‘girl’, ‘man’, ‘maybe’, ‘mine’, ‘name’, ‘read’, ‘right’, ‘sci- information in a pair of sequences than LCS.
ence’, ‘thank’. There are 20 examples in each class, and all
classes come from one signer. All time series are of the same
length (by interpolation).                            6   Related work
  Auslan is an interesting and challenging dataset as “it is Paper [Lin et al., 2005] presents a novel measure of dissimi-
multivariate, and without exploiting the multivariate property larity between two time sequences – the degree of difference
it is hard to do much better than random guessing.” [Keogh for pattern p, weighted by the conﬁdence of the pattern, in
and Folias, 2002] We use the following similarity/distance two time series. Given two time series, s1 and s2,theyare
measures on the datasets: Euclidean (euc), Longest Common ﬁrst of all represented as symbolic SAX sequences. A pattern
Subsequence (lcs) and All Common Subsequences (acs). Be- is a subsequence in the SAX representation of a time series.
cause all of the datasets are real-valued and some are mul- Without weighting, the dissimilarity is
tidimensional, these distance/similarity measures need to be                 
    |  ( ) −  ( )|
                                                                    (    ) ∝       f1 p   f2 p
extended. We consider the following combinations:             DSim   s1,s2           ( ( )   ( ))
                                                                              p  max f1 p ,f2 p
  • euc: the standard Euclidean distance, used in our ex-
    tended similarity measures (lcs w euc, acs w euc). where fi(p) is the frequency of pattern p in sequence si.

                                                IJCAI-07
                                                   638  Paper [Yang et al., 2003] introduces a new measure of sim- We feel that the way in which we estimate the probability
ilarity/distance between symbolic sequences. The measure is of two data points being equal is a bit crude. In our future
based on counting the occurrences of m-bit (i.e., consisting work we intend to develop more involved methods in order to
of m symbols) words (i.e., subsequences) in a symbolic se- make it more effective and efﬁcient.
quence. Without weighting by the probability of each word
in a sequence, the distance between two sequences s1 and s2 References
is deﬁned as follows:
                                                      [Agrawal et al., 1995] R. Agrawal, K.-I. Lin, H.S. Sawhney,
                      m
                     
2                                  and K. Shim. Fast similarity search in the presence of
         Dm(s1,s2) ∝    |R1(wk) − R2(wk)|                noise, scaling, and translation in time-series databases. In
                     k=1                                 Proc. VLDB95, 1995.
                                       4
where Ri(wk) is the rank of subsequence wk in sequence [Bergroth et al., 2000] L. Bergroth, H. Hakonen, and
si.                                                      T. Raita. A survey of longest common subsequence
  DSim  and Dm  are both related to ACS in the sense they algorithms. In SPIRE’00: Proceedings of the Seventh In-
consider all common subsequences in s1 and s2.However    ternational Symposium on String Processing Information
there are signiﬁcant differences:                        Retrieval, page 39. IEEE Computer Society, 2000.
                                                      [Das et al., 1997] G. Das, D. Gunopulos, and H. Mannila.
  • Conceptually both DSim and Dm measure the average
    (weighted) difference of the frequency of a subsequence Finding similar time series. In Proc. of PKDD97: Prin-
    p in s1 and the frequency of p in s2, while ACS counts ciples of Data Mining and Knowledge Discovery in Data-
    the number of common subsequences of two sequences.  bases, pages 88–100, 1997.
                                                      [               ]
  • They all need to look through a large, exponential search Goldin et al., 2004 Dina Goldin, Todd Millstein, and
    space for subsequences, but ACS has a dynamic pro-   Ayferi Kutlu. Bounded similarity querying for time-series
    gramming algorithm to calculate the similarity in poly- data, 2004.
    nomial time.                                      [Gunopulos and Das, 2000] Dimitrios Gunopulos and Gau-
                                                         tam Das. Time series similarity measures. 2000. Tutorial
  • ACS can be applied to sequences of real numbers with-
                                                         notes of the sixth ACM SIGKDD international conference
    out the need of transformation, while Dm can not. To
                                                         on Knowledge discovery and data mining.
    use DSim  for sequences of real numbers, the symbol
    SAX representation (a type of transformation) is needed. [Hirschberg, 1977] Daniel S. Hirschberg. Algorithms for the
                                                         longest common subsequence problem. Journal of ACM,
7Conclusion                                              24(4):664–675, 1977.
                                                      [Hirschberg, 2005] Dan Hirschberg. Dynamic program-
This paper presents a conceptually novel similarity measure
                                                         ming, 2005.  http://www.ics.uci.edu/∼dan/
for time series data – all common subsequences or ACS. It
                                                         class/161/notes/6/Dynamic.html.
is intended to maximally capture the common information
shared by two time series. A novel dynamic programming [Keogh and Folias, 2002] E. Keogh and T. Folias. The UCR
algorithm is presented to calculate the number of all common time series data mining archive, 2002. http://www.
subsequences.                                            cs.ucr.edu/∼eamonn/TSDMA/index.html.
  To use ACS and other equality-checking based sequence [Keogh et al., 2004] Eamonn Keogh, Stefano Lonardi, and
similarity measures (e.g., LCS) to handle multidimensional, Chotirat Ann Ratanamahatana. Towards parameter-free
read-valued time series, we need to extend these measures. data mining. In Proc. SIGKDD04, 2004.
This paper presents an extension method, which is inspired
                                                      [             ]
by the theory of probabilistic metric spaces. We seek to use Lin et al., 2005 Jessica Lin, Eamonn Keogh, and Stefano
the probability that two points are equal in calculating the Lonardi. Visualizing and discovering non-trivial patterns
similarity when a time series is real-valued or multidimen- in large time series databases. Information Visualization,
sional.                                                  4(2):61–82, 2005.
  We conducted an experimental study to evaluate the new [Osborne and Bridge, 1997] Hugh Osborne and Derek
similarity measure and the extension method. Our exper-  Bridge. Models of similarity for case-based reasoning.
imental results show that ACS is favorably comparable to In Procs. of the Interdisciplinary Workshop on Similarity
LCS, and it displays a distinctive feature – stable perfor- and Categorisation, pages 173–179, 1997.
mance as more data are considered for decision (i.e., as k gets [Schweizer and Sklar, 1983] B. Schweizer and A. Sklar.
larger), making ACS suitable for those tasks whose aim is to Probabilistic metric spaces. North-Holland, 1983.
ﬁnd a set of ranked sequences (e.g., information retrieval).
The results also show that the extension method presented in [Vlachos et al., 2003] Michail Vlachos, Marios Hadjieleft-
this paper works well, and appears better than that discussed heriou, Dimitrios Gunopulos, and Eamonn Keogh. Index-
in [Vlachos et al., 2003].                               ing multi-dimensional time-series with support for mul-
                                                         tiple distance measures. In Proceedings of SIGKDD03:
  4The frequency of occurrence is calculated for every words, and ACM International Conference on Data Mining August
is used as the basis for ranking all the words.          24-27, 2003, Washington, DC, USA, 2003.

                                                IJCAI-07
                                                   639