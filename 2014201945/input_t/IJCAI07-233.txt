       Quality Guarantees on         k-Optimal Solutions for Distributed Constraint
                                     Optimization Problems

                                Jonathan P. Pearce and Milind Tambe∗
                                    University of Southern California
                                     Computer Science Department
                                  {jppearce@usc.edu, tambe@usc.edu}

                    Abstract                          to a system that scales up easily and is more robust to dy-
                                                      namic environments. Researchers have introduced k-optimal
    A  distributed constraint optimization problem    algorithms in which small groups of agents optimize based
    (DCOP) is a formalism that captures the rewards   on their local constraints, resulting in a k-optimal DCOP as-
    and costs of local interactions within a team of  signment, in which no subset of k or fewer agents can im-
    agents. Because complete algorithms to solve      prove the overall solution. Some examples include the 1-
    DCOPs are unsuitable for some dynamic or any-     optimal algorithms DBA [Yokoo and Hirayama, 1996] and
    time domains, researchers have explored incom-    DSA  [Fitzpatrick and Meertens, 2003] for distributed con-
    plete DCOP algorithms that result in locally opti- straint satisfaction problems (DisCSPs), which were later ex-
    mal solutions. One type of categorization of such tended to DCOPs [Zhang et al., 2005a], as well as the 2-
    algorithms, and the solutions they produce, is k- optimal algorithms in [Maheswaran et al., 2004], in which
    optimality; a k-optimal solution is one that cannot optimization was done by agents acting in pairs. Previous
    be improved by any deviation by k or fewer agents. work has focused on upper bounds on the number of k-
    This paper presents the ﬁrst known guarantees on  optima in DCOPs [Pearce et al., 2006], as well as experi-
    solution quality for k-optimal solutions. The guar- mental analysis of k-optimal algorithms [Zhang et al., 2005a;
    antees are independent of the costs and rewards in Maheswaran et al., 2004].
    the DCOP, and once computed can be used for any
    DCOP of a given constraint graph structure.         Unfortunately, the lack of theoretical guarantees on the
                                                      quality of solutions obtained by k-optimal algorithms was a
                                                      fundamental limitation; until now, we could not guarantee a
1  Introduction                                       lower bound on the quality of the solution obtained with re-
In a large class of multi-agent scenarios, a set of agents spect to the quality of the global optimum. In this paper, we
chooses a joint action as a combination of individual ac- introduce such guarantees. These guarantees can help deter-
tions. Often, the locality of agents’ interactions means that mine an appropriate k-optimal algorithm, or possibly an ap-
the utility generated by each agent’s action depends only propriate constraint graph structure, for agents to use in sit-
on the actions of a subset of the other agents. In this uations where the cost of coordination between agents must
case, the outcomes of possible joint actions can be com- be weighed against the quality of the solution reached. If in-
pactly represented in cooperative domains by a distributed creasing the value of k will provide a large increase in guaran-
constraint optimization problem (DCOP) [Modi et al., 2005; teed solution quality, it may be worth the extra computation or
Zhang et al., 2005a]. A DCOP can take the form of a graph in communication required to reach a higher k-optimal solution.
which each node is an agent and each edge denotes a subset of For example, consider a team of autonomous underwater ve-
                                                                  [                ]
agents whose actions, taken together, incur costs or rewards hicles (AUVs) Zhang et al., 2005b that must quickly choose
to the agent team. Applications of DCOP include sensor net- a joint action in order to observe some transitory underwa-
works [Modi et al., 2005], meeting scheduling [Petcu and ter phenomenon. The combination of individual actions by
Faltings, 2005] and RoboCup soccer [Vlassis et al., 2004]. nearby AUVs may generate costs or rewards to the team, and
  Globally optimal DCOP algorithms can incur large com- the overall utility of the joint action is determined by their
putation or communication costs for domains where the num- sum. If this problem were represented as a DCOP, nearby
ber of agents is large or where time is limited. However, in- AUVs would share constraints in the graph, while far-away
complete algorithms in which agents react on the basis of lo- AUVs would not. However, the actual rewards on these con-
cal knowledge of neighbors and constraint utilities can lead straints may not be known until the AUVs are deployed, and
                                                      in addition, due to time constraints, an incomplete, k-optimal
  ∗This material is based upon work supported by the Defense Ad- algorithm, rather than a complete algorithm, must be used to
vanced Research Projects Agency (DARPA), through the Depart- ﬁnd a solution. In this case, worst-case quality guarantees
ment of the Interior, NBC, Acquisition Services Division, under for k-optimal solutions for a given k, that are independent of
Contract No. NBCHD030010.                             the actual costs and rewards in the DCOP, are useful to help

                                                IJCAI-07
                                                  1446 R                                      R             the group {2, 3} deviated, making the assignment a˜ = [100],
  12  0  1                               23  0  1     team reward would increase from 16 to 20. The globally op-
                                                                    ∗ =                        ∈{ , , }.
    0 10 0         1      2      3         0 20 0     timal solution, a [000]is k-optimal for all k 1 2 3
    1 0  5                                 1 0 11       In addition to categorizing local optima, k-optimality pro-
                                                      vides a natural classiﬁcation for DCOP algorithms. Many
              Figure 1: DCOP example                  algorithms are guaranteed to converge to k-optima, including
decide which algorithm to use. Alternatively, the guarantees DBA [Zhang et al., 2005a], DSA [Fitzpatrick and Meertens,
can help to choose between diﬀerent AUV formations, i. e. 2003], and coordinate ascent [Vlassis et al., 2004] for k = 1,
diﬀerent constraint graphs.                           and MGM-2 and SCA-2  [Maheswaran et al., 2004] for k = 2.
  We present two distinct types of guarantees for k-optima. Globally optimal algorithms such as Adopt [Modi et al.,
The ﬁrst, in Sections 3 and 4, is a lower bound on the qual- 2005], OptAPO [Mailler and Lesser, 2004] and DPOP [Petcu
                                                                                                =
ity of any k-optimum, expressed as a fraction of the quality and Faltings, 2005] convergetoak-optimum for k n.
of the optimal solution. The second, in Section 5, is a lower 3 Quality guarantees on k-optima
bound on the proportion of all DCOP assignments that a k-
                                                      This section provides reward-independent guarantees on so-
optimum must dominate in terms of quality. This type is use-
                                                      lution quality for any k-optimal DCOP assignment. If we
ful in approximating the diﬃculty of ﬁnding a better solution
                                                      must choose a k-optimal algorithm for agents to use, it is use-
than a given k-optimum. For both, we provide general bounds
                                                      ful to see how much reward will be gained or lost in the worst
that apply to all constraint graph structures, as well as tighter
                                                      case by choosing a higher or lower value for k. We assume
bounds made possible if the graph is known in advance.
                                                      the actual costs and rewards on the DCOP are not known a
2  DCOP and     k-optima                              priori (otherwise the DCOP could be solved centrally ahead
                                                      of time). We provide a guarantee for a k-optimal solution as a
We consider a DCOP in which each agent controls a variable
                                                      fraction of the reward of the optimal solution, assuming that
to which it must assign a value. Constraints exist on subsets
                                                      all rewards in the DCOP are non-negative (the reward struc-
of these variables; each constraint generates a cost or reward
                                                      ture of any DCOP can be normalized to one with all non-
to the team based on the values assigned to each variable in
                                                      negative rewards as long as no inﬁnitely large costs exist).
the corresponding subset. Although we assume in this paper
that each agent controls a single variable, all results are valid Proposition 1 For any DCOP of n agents, with maximum
                                                      constraint arity of m, where all constraint rewards are non-
for cases in which agents control more than one variable.               ∗
  Formally, a DCOP is a set of variables (one per agent) negative, and where a is the globally optimal solution, then,
   = { ,..., }                   A  =  {A ,...,A },   for any k-optimal assignment, a, where m ≤ k < n,
N :   1    n  and a set of domains  :    1      n                               
         th                     ∈A                                             n−m
where the i variable takes value ai i. We denote the                           −
assignment of the multi-agent team by a = [a ···a ]. Val-             R(a) ≥   k m  R(a∗).         (1)
                                       1    n                                n − n−m
ued constraints exist on various subsets S ⊂ N of these vari-                k    k
ables. A constraint on S is expressed as a reward function Proof: By the deﬁnition of k-optimality, any assignmenta ˜
RS (a). This function represents the reward generated by the such that d(a, a˜) ≤ k must have reward R(˜a) ≤ R(a). We call
constraint on S when the agents take assignment a; costs are this set of assignments A˜. Now consider any non-null subset
expressed as negative rewards. θ is the set of all such subsets ˆ ⊂ ˜            ∈ ˆ              θ
                                 ∈ θ                  A    A. For any assignmenta ˆ A, the constraints in the
S on which a constraint exists, and no S is a subset of any DCOP can be divided into three discrete sets, given a anda ˆ:
other S ∈ θ. For convenience, we will refer to these subsets S
                                                        • θ (a, aˆ) ⊂ θ such that ∀S ∈ θ (a, aˆ), S ⊂ D(a, aˆ).
as “constraints” and the functions RS (·) as “constraint reward 1                 1
functions.” The solution quality for a particular complete as- • θ2(a, aˆ) ⊂ θ s.t. ∀S ∈ θ2(a, aˆ), S ∩ D(a, aˆ) = ∅.
signment a is the sum of the rewards for that assignment from
                                                       • θ3(a, aˆ) ⊂ θ s.t. ∀S ∈ θ3(a, aˆ), S  θ1(a, aˆ) ∪ θ2(a, aˆ).
all constraints in the DCOP: R(a) = ∈θ R (a).
                                S   S                   θ   ,
  In [Pearce et al., 2006], the deviating group between two 1(a aˆ) contains the constraints that include only the vari-
                                                                                                   θ  ,
assignments, a anda ˜, was deﬁned as D(a, a˜):= {i ∈ N : ables ina ˆ which have deviated from their values in a; 2(a aˆ)
a   a˜ }, i.e. the set of variables whose values ina ˜ diﬀer contains the constraints that include only the variables ina ˆ
 i    i                                                                             θ   ,
from their values in a. The distance between two assignments which have not deviated from a; and 3(a aˆ) contains the con-
was deﬁned as d(a, a˜):= |D(a, a˜)| where |·|denotes the size straints that include at least one of each. Thus:
of the set. An assignment a is classiﬁed as a k-optimum if                              
    −      ≥   ∀             ,  ≤   .
R(a)  R(˜a)  0  a˜ such that d(a a˜) k Equivalently, at     R(ˆa) =    RS (ˆa) +   RS (ˆa) +  RS (ˆa).
a k-optimum, no subset of k or fewer agents can improve the       S ∈θ1(a,aˆ) S ∈θ2(a,aˆ) S ∈θ3(a,aˆ)
overall reward by choosing diﬀerent values; every such subset
is acting optimally given the values of the others.     And, the sum of rewards of all assignmentsa ˆ in Aˆ is:
                                                                                                
                                                               =               +          +
Example 1 Figure 1 is a binary DCOP in which agents        R(ˆa)          RS (ˆa)     RS (ˆa)    RS (ˆa)
                 { , }                  = { , }          ∈ ˆ      ∈ ˆ S ∈θ (a,aˆ) S ∈θ (a,aˆ) S ∈θ (a,aˆ)
choose values from 0 1 , with constraints S 1 1 2 and    aˆ A    aˆ A 1        2          3
   = { , }                                 =
S 2  2 3 with rewards shown. The assignment a [1 1 1]          ≥         RS (ˆa) +      RS (ˆa).
                                                                     ∈θ ,          ∈θ ,
is 1-optimal because any single agent that deviates reduces      aˆ∈Aˆ S 1(a aˆ) aˆ∈Aˆ S 2(a aˆ)
the team reward. However, [1 1 1] is not 2-optimal because if

                                                IJCAI-07
                                                  1447  Since R(a) > R(ˆa), ∀aˆ ∈ Aˆ,                         We now show that Proposition 1 is tight, i.e. that there exist
                              
                           +                          DCOPs with k-optima of quality equal to the bound.
             aˆ∈Aˆ S ∈θ (a,aˆ) RS (ˆa) aˆ∈Aˆ S ∈θ (a,aˆ) RS (ˆa)
      R(a) ≥       1                2       .   (2)                ∀ ,  ,             ≤   <
                           |Aˆ|                       Proposition 2 n m k such that m   k    n, there exists
                                                      some DCOP with n variables, with maximum constraint ar-
                                                                                                   ∗
  Now, if the two numerator terms and the denominator can ity m with a k-optimal assignment, a, such that, if a is the
be expressed in terms of R(a∗) and R(a), then we have a bound globally optimal solution,
                   ∗                                                            
on R(a) in terms of R(a ). To do this, we consider the partic-                 n−m
                                                                               −
ular Aˆ which contains all assignmentsa ˆ such that d(a, aˆ) = k,     R(a) =   k m  R(a∗).         (3)
   ∀  ∈  ˆ, ∀ ∈    ,  ,  =  ∗                                                n − n−m
and  aˆ A   aˆi D(a aˆ) aˆi ai . This means that exactly                     k    k
k variables ina ˆ have deviated from their value in a, and these
                                            ∗         Proof: Consider a fully-connected m-ary DCOP where the
variables are taking the same values that they had in a .
             , ∗                                      domain of each variable contains at least two values {0,1} and
  There are d(a a ) assignmentsa ˆ ∈ Aˆ.Forevery constraint
             k                                      every constraint RS contains the following reward function:
                    d(a,a∗)−|S |                                         ⎧
S ∈ θ, there are exactly    diﬀerent assignmentsa ˆ ∈ Aˆ                    n−m
                      k−|S |                                             ⎪ (k−m)
                                                                         ⎪    −   , ∀i ∈ S, a = 0
           ∈ θ  ,                                                        ⎨⎪ (n)−(n m)    i
for which S  1(a aˆ). This is because there exists a unique            =  k   k
  ∈  ˆ                                 , ∗       ⊂                RS (a) ⎪1       , ∀i ∈ S, a = 1
aˆ  A for every subset of k variables in D(a a ). If S                   ⎪               i
                                                                         ⎩        ,
D(a, aˆ), as stipulated by the deﬁnition of θ1(a, aˆ), then there         0       otherwise
are d(a, a∗) −|S | remaining variables from which k −|S | must
                                                                         ∗    ∗
                                           d(a,a∗)−|S |                          =  , ∀
                       ,                                The optimal solution a is ai 1 i.Ifa is deﬁned such
be chosen to complete D(a aˆ), and so there are k−|S |
                                                      that ai = 0, ∀i, then Equation 3 is true. Now we show that a
possible assignmentsa ˆ for which this is true. For alla ˆ, for                              ,  =
     ∈ θ  ,         =     ∗                      =    is k-optimal. For any assignmenta ˆ, such that d(a aˆ) k,
all S   1(a aˆ), RS (ˆa) RS (a ), so aˆ∈Aˆ S ∈θ1(a,aˆ) RS (ˆa)                     
      ∗             ∗  
     d(a,a )−|S | ∗ ≥ d(a,a )−m ∗                      R(ˆa) =    R(ˆa ) +   R(ˆa ) +    R(ˆa ).
 S ∈θ  k−|S | RS (a )  k−m  R(a ).                                S           S           S
                                             , ∗ −| |       S ∈θ (a,aˆ) S ∈θ (a,aˆ) S ∈θ (a,aˆ)
  Similarly, for every constraint S ∈ θ, there are d(a a ) S  1           2        3      
                                              k             
   
      n−m         k n − n−m +  n−k n−m
 ﬀ                   ∈  ˆ             ∈ θ   ,                k    n − k  k−m        m  k    k     m  k−m
di erent assignmentsa ˆ A for which S    2(a aˆ). If       ≤   +           + 0 =          
                                                                       n   n−m             n   n−m
S ∩D(a, aˆ) = ∅, as stipulated by the deﬁnition of θ2(a, aˆ), then m m   −                  −
          , ∗ −| |                                                     k    k              k    k
there are d(a a ) S remaining variables from which kmust         n!        n!(n − m − k)! − (n − m)!(n − k)!
                      ,                d(a,a∗)−|S |        =               ÷
be chosen to complete D(a aˆ), and so there are k pos-      m!(k − m)!(n − k)!   k!(n − k)!(n − m − k)!
sible assignmentsa ˆ for which this is true. For alla ˆ, for all     n!k!(n − m − k)!
  ∈ θ  ,  ,     =                                =         =
S    2(a aˆ) RS (ˆa) RS (a), and, so aˆ∈Aˆ S ∈θ2(a,aˆ) RS (ˆa)  −        −  −   −   −    −
      ∗            ∗                                   m!(k  m)![n!(n m  k)! (n k)!(n m)!]
     d(a,a )−|S |   d(a,a )−m                               
 
  ∈θ        R  (a) ≥       R(a).                                         −      −   −
 S      k     S        k                                   = n         (n m)!k!(n m  k)!
  Therefore, from Equation 2,                                m (k − m)![n!(n − m − k)! − (n − m)!(n − k)!]
                                                                               
         , ∗ −       , ∗ −            , ∗ −                 
   −              
     −
       d(a a ) m R(a∗) + d(a a ) m R(a) d(a a ) m               n m k!(n − m − k)!    n m
    ≥    k−m        k       ≥     k−m     ∗          = n  k−m            = n   k−m  =
 R(a)             ∗                ∗      ∗   R(a )               − −                         R(a)
                d(a,a )         d(a,a ) − d(a,a )−m          m  n!(n m k)! − (n − m)! m n − n−m
                 k                k       k                      (n−k)!             k    k
                           ∗                                                  
which is minimized when d(a, a ) = n, so Equation 1 holds                     n
                                                       because in a, each of the constraints in the DCOP are
as a guarantee for a k-optimum in any DCOP. It is possible                  m
                               −                      producing the same reward. Since this can be shown for
that k > n − m; in this case we take n m to be 0. 
                               k                      d(a, aˆ) = j, ∀ j such that 1 ≤ j ≤ k, a is k-optimal. 
  For binary DCOPs (m = 2), Equation 1 simpliﬁes to:
                       (k − 1)                        4   Graph-based quality guarantees
               R(a) ≥         R(a∗).
                     (2n − k − 1)                     The guarantee for k-optima in Section 3 applies to all possible
                                                      DCOP graph structures. However, knowledge of the structure
The following example illustrates Proposition 1:      of constraint graphs can be used to obtain tighter guarantees.
Example 2 Consider a DCOP with ﬁve variables numbered This is done by again expressing the two numerator terms in
1 to 5, with domains of {0,1}, fully connected with binary Equation 2 as multiples of R(a∗) and R(a). However, for a
constraints between all variable pairs. Suppose that a = sparse graph, if Aˆ is chosen as deﬁned in Proposition 1, there
                                   ∗
[00000]is a 3-optimum, and that a    =  [11111]       may be many assignmentsa ˆ ∈ Aˆ that have few or no con-
                              ∗
is the global optimum. Then d(a, a ) = 5, and Aˆ contains straints S in θ (a, aˆ) because the variables in D(a, aˆ) may not
   ∗                                                            1
d(a,a ) =                                             share any constraints. Instead, exploiting the graph structure
  k     10 assignments: [11100], [11010], [11001],
[10110],  [10101],  [10011],   [01110],  [01101],     by choosing a smaller Aˆ can lead to a tighter bound. We can
                                                      take Aˆ from Proposition 1, i.e. Aˆ which contains alla ˆ such
[01011], [00111]. Whatever the values of the rewards  are,                                     ∗
                        ∗                   −                ,  =       ∀  ∈  ˆ, ∀ ∈    ,  ,  =
                                            n 2 =     that d(a aˆ) k and aˆ  A  aˆi  D(a aˆ) aˆi ai . Then,
every constraint reward RS (a ) will equal RS (ˆa) for k−2 3
                        ∗                             we restrict this Aˆ further, so that ∀aˆ ∈ Aˆ, the variables in
assignments in A(e.g.Rˆ { , }(a ) = R{ , }(ˆa) for aˆ = [11100],
                    1 2       1 2                     D(a, aˆ) form a connected subgraph of the DCOP graph (or
[11010], and [11001]) and similarly, every constraint re-
                                                    hypergraph), meaning that any two variables in D(a, aˆ) must
                        n−2 =               ˆ
ward RS (a) equals RS (ˆa) for k 1 assignment in A. Thus, be connected by some chain of constraints. This allows us to
    ≥  3     ∗ = 1   ∗ .                                                                              ∗
R(a)  10−1 R(a ) 3 R(a )                              again transform Equation 2 to express R(a) in terms of R(a );

                                                IJCAI-07
                                                  1448this new method can produce tighter guarantees for k-optima in contrast to the constant-time guarantees of Equations 1,
in sparse graphs. As an illustration, provably tight guaran- 4 and 5. An LFP such as this is reducible to a linear pro-
tees for binary DCOPs on ring graphs (each variable has two gram (LP) [Boyd and Vandenberghe, 2004]. The objective is
                                                                  R(a)
constraints) and star graphs (each variable has one constraint to minimize ∗ such that ∀a˜ ∈ A˜, R(a) − R(˜a) ≥ 0, given
                                 −                               R(a )
except the central variable, which has n 1) are given below. A˜ as deﬁned in Proposition 1. Note that R(a∗) and R(a) can
                                                                            ∗            ∗
Proposition 3 For any binary DCOP of n agents with a  be expressed as S ∈θ RS (a ) and S ∈θ RS (a ). We can now
ring graph structure, where all constraint rewards are non-
             ∗                                        transform the DCOP so that every R(˜a) can also be expressed
negative, and a is the globally optimal solution, then, for in terms of sums of R (a∗) and R (a), without changing or
                               <                                         S         S
any k-optimal assignment, a, where k n,               invalidating the guarantee on R(a). Therefore, the LFP will
                                                                                                    ∗
                        k − 1                         contain only two variables for each S ∈ θ, one for RS (a ) and
                  R(a) ≥    R(a∗).              (4)
                        k + 1                         one for RS (a), where the domain of each one is the set of non-
                                                      negative real numbers. The transformation is to set all reward
                             | ˆ| =             ,
Proof: Returning to Equation 2, A  n because D(a aˆ)  functions RS (·) for all S ∈ θ to 0, except for two cases: when
could consist of any of the n connected subgraphs of k vari-      ∈                          ∗
                                 ∈ θ           −      all variables i S have the same value as in a , or when all
ables in a ring. For any constraint S , there are k 1 i ∈ S have the same value as in a. This has no eﬀect on R(a∗)
assignmentsa ˆ ∈ Aˆ for which S ∈ θ (a, aˆ) because there are            ∗
                              1                       or R(a), because RS (a ) and RS (a) will be unchanged for all
k − 1 connected subgraphs of k variables in a ring that contain                                 ∗
                                     ∗              S ∈ θ. It also has no eﬀect on the optimality of a or the k-
S . Therefore, ∈ ˆ ∈θ , R (ˆa) = (k−1)R(a ). Also, there
             aˆ A S 1(a aˆ) S                         optimality of a, since the only change is to reduce the global
    −  −              ∈ ˆ           ∈ θ  ,                                           ∗
are n k 1 assignmentsa ˆ A for which S 2(a aˆ) because reward for assignments other than a and a. Thus, the tight
there are n − k − 1 ways to choose S in a ring so that it does      R(a)
                                                      lower bound on  ∗ still applies to the original DCOP.
not include any variable in a given connected subgraph of k       R(a )
variables. Therefore, ∈ ˆ ∈θ , R (ˆa) = (n − k − 1)R(a).
                   aˆ A S 2(a aˆ) S                   5   Domination analysis of   k-optima
So, from Equation 2,
                                                      In this section we now provide a diﬀerent type of guaran-
                 (k − 1)R(a∗) + (n − k − 1)R(a)
           R(a) ≥                                     tee: lower bounds on the proportion of all possible DCOP as-
                           n                          signments which any k-optimum must dominate in terms of
and therefore Equation 4 holds.                      solution quality. This proportion, called a domination ratio,
                                                      provides a guide for how diﬃcult it may be to ﬁnd a solu-
Proposition 4 For any binary DCOP of n agents with a  tion of higher quality than a k-optimum; this metric is com-
star graph structure, where all constraint rewards are non-
negative, and a∗ is the globally optimal solution, then, for monly used to evaluate heuristics for combinatorial optimiza-
any k-optimal assignment, a, where k < n,             tion problems [Gutin and Yeo, 2005].
                                                        For example, suppose for some k, the solution quality guar-
                        k − 1
                  R(a) ≥    R(a∗).              (5)   antee from Section 4 for any k-optimum was 50% of optimal,
                        n − 1                         but, additionally, it was known that any k-optimum was guar-
Proof: The proof is similar to the previous proof. In a star anteed to dominate 95% of all possible assignments to the
               n−1                                    DCOP. Then, at most only 5% of the other assignments could
graph, there are   subgraphs of k variables, and there-
             k−1                                    be of higher quality, indicating that it would likely be com-
    | ˆ| = n−1                  ∈ θ
fore A    k−1 . Every constraint S  includes the cen- putationally expensive to ﬁnd a better assignment, either with
                                               n−2    a higher k algorithm, or by some other method, and so a k-
tral variable and one other variable, and thus there are −
                                               k 2    optimal algorithm should be used despite the low guarantee
connected  subgraphs of k variables  that contain S , and there-
                         n−2   ∗                      of 50% of the optimal solution quality. Now suppose instead
fore  ∈ ˆ  ∈θ , R  (ˆa) =   R(a ). Finally, there are no
     aˆ A S 1(a aˆ) S    k−2                          for the same problem, the k-optimum was guaranteed to dom-
ways to choose S so that it does not include any variable inate only 20% of all assignments. Then it becomes more
in a given connected subgraph of k variables. Therefore, likely that a better solution could be found quickly, and so the
  ∈ ˆ  ∈θ , R  (ˆa) = 0R(a). So, from Equation 2,
 aˆ A S 2(a aˆ) S                                     k-optimal algorithm might not be recommended.
                       
                      n−2  ∗ +                          To ﬁnd the domination ratio, observe that any k-optimum a
                      − R(a ) 0R(a)
               R(a) ≥ k 2                           must be of the same or higher quality than alla ˜ ∈ A˜ as deﬁned
                           n−1
                           k−1                        in Proposition 1. So, the ratio is:
                                                                             + | ˜|
and therefore Equation 5 holds.                                             1  A  .
                                                                                |A |                   (6)
  Tightness can be proven by constructing DCOPs on ring                      i∈N  n
and chain graphs with the same rewards as in Proposition 2;
proofs are omitted for space. The bound for rings can also be If the constraint graph is fully connected (or not known, and
applied to chains, since any chain can be expressed as a ring so must be assumed to be fully  connected), and each variable
                                                                     | ˜| = k  n   −  j         |A | = n
where all rewards on one constraint are zero.         has q values, then A  j=1 j (q 1) and  i∈N  n   q .
  Finally, bounds for DCOPs with arbitrary graphs and non- If the graph is known to be not fully connected, then the set
negative constraint rewards can be found using a linear- A˜ from Equation 6 can be expanded to include assignments of
fractional program (LFP). This method gives a tight bound distance greater than k from a, providing a stronger guarantee
for any graph, since it instantiates the rewards for all con- on the ratio of the assignment space that must be dominated
straints, but requires a globally optimal solution to the LFP, by any k-optimum. Speciﬁcally, if a is k-optimal, then any

                                                IJCAI-07
                                                  1449                                                                              
                                               ≤                             ≥               
assignment where any number of disjoint subsets of size k and so S ∈θ:S ∩D ∅ RS (a) S ∈θ:S ∩D ∅ RS (a ) because the
                                                                    n                 n              
have deviated from a must be of the same or lower quality as rewards from agents outside Dn are the same for a and a .
                                                                                       =               n
a, as long as no constraint includes any two agents in diﬀerent We also know that S ∈θ:S ∩Dn=∅ RS (a) S ∈θ:S ∩Dn=∅ RS (˜a )
such subsets; this idea is illustrated below:         because the rewards from agents outside Dn are the same for
                                                      a anda ˜n; therefore,
Example 3 Consider a binary DCOP of ﬁve variables, num-                                     
                                                                         n−1                          n
bered 1 to 5, with domains of two values, with unknown con- R(a) ≥   RS (˜a ) +      RS (a ) +     RS (˜a )
straint graph. Any 3-optimum must be of equal or greater     ∈θ ∩ n−1∅      S ∈θ:S ∩D ∅   S ∈θ:S ∩Dn=∅
                                                     S :SD              n          
               5    5   5   5
quality than 1 + +    +   /2  = 81.25% of all possible                   n             n             n
               1    2   3                                  =         RS (˜a ) +    RS (˜a ) +     RS (˜a )
                                                                  −                            n
assignments, i.e. where 0, 1, 2, or 3 agents have deviated.  S ∈θ:S ∩Dn 1∅ S ∈θ:S ∩Dn∅  S ∈θ:S ∩D =∅
  Now, suppose the graph is known to be a chain with vari-
                                                       because the rewards from Dn−1 are the same fora ˜n−1 anda ˜n,
ables ordered by number. Since a deviation by either the                                        n
variables {1,2} or {4,5} cannot increase global reward, and and the rewards from Dn are the same for a anda ˜ . There-
                                                               ≥    n 
no constraint exists across these subsets, then neither can a fore, R(a) R(˜a ).
deviation by {1,2,4,5}, even though four variables are deviat-
                      {     }    {      }             6   Experimental results
ing. The same applies to 1,3,4,5 and 1,2,3,5 , since both While the main thrust of this paper is on theoretical guaran-
are made up of subsets of three or fewer variables that do tees for k-optima, this section gives an illustration of the guar-
not share constraints. So,  a 3-optimum    is now of equal or antees in action, and how they are aﬀected by constraint graph
                   +  5 + 5 +  5 +  / 5 =   .
greater quality than 1 1  2    3   3 2    90 63% of   structure. Figures 2a, 2b, and 2c show quality guarantees for
all assignments.                                     binary DCOPs with fully connected graphs, ring graphs, and
  An improved guarantee can be found by enumerating the star graphs, calculated directly from Equations 1, 4 and 5.
set A˜ of assignmentsa ˜ with equal or lower reward than a; this Figure 2d shows quality guarantees for binary-tree DCOPs,
set is expanded due to the DCOP graph structure as in the obtained using the LFP from Section 4. The x-axis plots the
above example. The following proposition makes this possi- value chosen for k, and the y-axis plots the lower bound for k-
ble; we introduce new notation for it: If we deﬁne n diﬀerent optima as a percentage of the optimal solution quality for sys-
                        =   ...         m = ∪m        tems of 5, 10, 15, and 20 agents. These results show how the
subsets of agents as Di for i 1 n, we use D   i=1Di,
i.e. Dm is the union of the ﬁrst m subsets. The proof is by worst-case beneﬁt of increasing k varies depending on graph
induction over each subset Di for i = 1 ...n.         structure. For example, in a ﬁve-agent DCOP, a 3-optimum is
                                                      guaranteed to be 50% of optimal whether the graph is a star
Proposition 5 Let a be some k-optimal assignment. Let a˜n
                                                      or a ring. However, moving to k = 4 means that worst-case
be another assignment for which D(a, a˜n) can be expressed
                                                      solution quality will improve to 75% for a star, but only to
as Dn = ∪n D where:
        i=1 i                                         60% for a ring. For fully connected graphs, the beneﬁt of in-
  •∀   , | |≤
     Di  Di  k. (subsets contain k or fewer agents)   creasing k goes up as k increases; whereas for stars it stays
                                                                                                     =
  •∀Di, D j, Di ∩ D j = ∅. (subsets are disjoint)     constant, and for chains it decreases, except for when k n.
                                                      Results for binary trees are mixed.
  •∀Di, D j, i ∈ Di, j ∈ D j such that i, j ∈ S, for any S ∈ θ.
    (no constraint exists between agents in diﬀerent subsets) Figure 3 shows the domination ratio guarantees for k-
                                                      optima from Section 5, for DCOPs where variables have do-
Then, R(a) ≥ R(˜an).
Proof:
                          n                   n
  Base case: If n = 1 then D = D1 and R(a) ≥ R(˜a )by
deﬁnition of k-optimality.
  Inductive step: R(a) ≥ R(˜an−1) ⇒ R(a) ≥ R(˜an).
  The set of all agents can be divided into the set of agents in
 n−1                                             n
D   , the set of agents in Dn, and the set of agents not in D .
Also, by inductive hypothesis, R(a) ≥ R(˜an−1). Therefore,
                                  
R(a) =        RS (a) +     RS (a) +      RS (a)
           −         ∈θ ∩ ∅      ∈θ ∩ n=∅
     S ∈θ:S∩Dn 1∅ S :S Dn      S :S D 
    ≥            n−1 +           n−1 +           n−1
              RS (˜aS )      RS (˜a )        RS (˜a )
      ∈θ ∩ n−1∅      S ∈θ:S ∩D ∅    S ∈θ:S ∩Dn=∅
    S :S D               n
                    ≥                n−1
so  S ∈θ:S ∩Dn−1∅ RS (a) S ∈θ:S ∩Dn−1∅ RS (˜a ) because the
rewards from agents outside Dn−1 are the same for a anda ˜n−1.
                                      
  Let a be an assignment such that D(a, a ) = Dn =
D(˜an−1, a˜n). Because a is k-optimal, R(a) ≥ R(a); therefore,
                                   
 R(a) =        RS (a) +      RS (a) +     RS (a)
            −         ∈θ ∩ ∅       ∈θ ∩ n=∅
       S ∈θ:S∩Dn 1∅ S :SDn      S :S D
                                             
     ≥         RS (a ) +     RS (a ) +      RS (a ).  Figure 2: Quality guarantees for k-optima with respect to the
            −                            n
       S ∈θ:S ∩Dn 1∅ S ∈θ:S ∩Dn∅  S ∈θ:S ∩D =∅      global optimum for DCOPs of various graph structures.


                                                IJCAI-07
                                                  1450