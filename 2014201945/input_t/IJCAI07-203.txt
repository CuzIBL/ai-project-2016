      Learning Policies for Embodied Virtual Agents Through Demonstration

                   Jonathan Dinerstein                   Parris K. Egbert     Dan Ventura
                 DreamWorks Animation                CS Department, Brigham Young University
             jondinerstein@yahoo.com                   {egbert,ventura}@cs.byu.edu


                    Abstract                            Reinforcement learning (RL) is a powerful scheme and an
                                                      obvious choice for policy learning. However, it does not meet
    Although many powerful AI and machine learning    our requirements — RL does not always scale well to contin-
    techniques exist, it remains difﬁcult to quickly cre- uous state spaces of high dimensionality (which are common
    ate AI for embodied virtual agents that produces  for EVAs), and it is challenging to encode aesthetic goals into
    visually lifelike behavior. This is important for ap- a reward structure. We take a different approach.
    plications (e.g., games, simulators, interactive dis- We present a technique for automatic learning of a policy
    plays) where an agent must behave in a manner that by mimicking demonstrated human behavior. This technique
    appears human-like. We present a novel technique  not only provides a natural and simple method for the con-
    for learning reactive policies that mimic demon-  struction of a policy, but it also allows an animator (or other
    strated human behavior. The user demonstrates the non-technical person) to be intimately involved in the con-
    desired behavior by dictating the agent’s actions struction of the policy. We learn a reactive policy rather than
    during an interactive animation. Later, when the  more advanced behavior because this learning is simple and
    agent is to behave autonomously, the recorded data nearly automatic, thereby allowing for AI creation by non-
    is generalized to form a continuous state-to-action technical users. This approach is empirically shown to be ef-
    mapping. Combined with an appropriate animation   fective at quickly learning useful policies whose behavior ap-
    algorithm (e.g., motion capture), the learned poli- pears natural in continuous state (and action) spaces of high
    cies realize stylized and natural-looking agent be- dimensionality.
    havior. We empirically demonstrate the efﬁcacy of
    our technique for quickly producing policies which 2  Related Work
    result in lifelike virtual agent behavior.
                                                      Designing and developing embodied virtual agents is funda-
                                                      mentally a multi-disciplinary problem [Gratch et al., 2002;
1  Introduction                                       Cole et al., 2003]. Indeed the study of EVAs overlaps with
                                                      several scientiﬁc ﬁelds, including artiﬁcial intelligence, com-
An Embodied Virtual Agent,orEVA,isasoftwareagentwith  puter graphics, computer-human interfaces, etc.
                                       [
a body represented through computer graphics Badler et al., The computer graphics community has been interested in
                       ]
1999; Dinerstein et al., 2006 . If given sufﬁcient intelligence, EVAs since Reynold’s seminal work on ﬂocking behavior
these characters can animate themselves by choosing actions [Reynolds, 1987]. One of the most well-known examples in
to perform (where each action is a motion). Applications of ﬁlm is the Lord of the Rings trilogy [Duncan, 2002], depict-
these agents include computer games, computer animation, ing epic-scale battle scenes of humanoids. Such large-scale
virtual reality, training simulators, and so forth.   animation would be implausible if the characters were ani-
  Despite the success of EVAs in certain domains, some im- mated manually. Several notable techniques for constructing
portant arguments have been brought against current tech- EVAs have been developed (e.g., [Monzani et al., 2001])but
niques. In particular, programming the agent AI is difﬁcult the explicit programming of agent AI remains a difﬁcult task.
and time-consuming, and it is challenging to achieve natural- Game-oriented EVA research includes the development
looking behavior. We address these issues by learning a pol- of action selection and learning architectures [Laird, 2001].
icy for controlling the agent’s behavior that is:     Computational models of emotion have also been proposed
 1. Scalable — The policy learning must scale to problems [Gratch and Marsella, 2005]. Recent work has examined
    with continuous and high-dimensional state spaces. rapid behavior adaption such that an EVA can better inter-
                                                      act with a given human user [Dinerstein and Egbert, 2005;
 2. Aesthetic — The virtual agent behavior must appear Dinerstein et al., 2005]. In other work, a scheme has been de-
    natural and visually pleasing.                    veloped whereby a non-embodied agent in a text-based com-
 3. Simple — The policy learning must be directable by a puter game learns through interaction with multiple users [Is-
    non-technical user.                               bell et al., 2001].

                                                IJCAI-07
                                                  1257  As discussed in the introduction, designing and program-
ming EVA AI is often challenging. Several authors have ad-
dressed this through reinforcement learning (a survey is given
in [Dinerstein et al., 2006]). RL is a natural choice since it has
long been used for agent policy learning. However, RL does
not meet our requirements in this paper. For example, it is
quite challenging to achieve natural-looking behavior since
these aesthetic goals must be integrated into the ﬁtness func- Figure 1: The workﬂow of our technique. (a) The simula-
tion. Also, many EVAs live in continuous virtual worlds and tor, which drives the animation, provides a current state s.
thus may require continuous state and/or action spaces, such This is visualized for the demonstrator. The demonstrator re-
as ﬂocking EVAs [Reynolds, 1987]. Traditional RL is in- sponds with an action a, which is recorded and used to up-
tractible for large or continuous state/action spaces, though date the simulation. (b) The state-action examples are pro-
recent research has begun to address this problem [Ng and cessed to eliminate conﬂicts and then generalized into a con-
Jordan, 2000; Wingate and Seppi, 2005]. Nevertheless, the tinuous policy function. This policy is used online to control
learning is still computationally expensive and limited. Our the agent.
technique learns stylized EVA behaviors quickly as empiri-
cally shown later.                                         (b) Eliminate conﬂicts.
  The agents and robotics communities have long recog-                                           μ
nized the need for simpliﬁed programming of agent AI.      (c) Generalize state-action pairs into a policy .
There has been some interesting work performed in pro-  2. Autonomous behavior:
gramming by demonstration,suchas[Pomerleau, 1996;          (a) Use μ to compute decisions for the agent, once per
van Lent and Laird, 2001; Mataric, 2000; Kasper et al., 2001; ﬁxed time step Δt.
                               ]
Angros et al., 2002; Nicolescu, 2003 , where an agent is in- This is a process of learning to approximate intelligent de-
structed through demonstrations by a user. Our technique ﬁts cision making from human example. We formulate decision
in this category but is speciﬁcally designed for EVAs trained making as a policy:
by non-technical users. In contrast to these existing tech-
                                                                           μ : s → a                  (1)
niques, our approach does not require signiﬁcant effort from
a programmer and domain expert before demonstration can where s ∈ Rn is a compact representation of the current state
be performed. The existing technique that is most related to of the character and its world, and a ∈ Rm is the action chosen
our work is “Learning to Fly” [Sammut et al., 1992].How- to perform. Each component of s is a salient feature deﬁning
ever, our technique is unique in many aspects, in particular some important aspect of the current state. If the character
because it performs conﬂict elimination, can operate in con- has a ﬁnite repertoire of possible actions, then a is quantized.
tinuous action spaces, and can be operated by non-technical A state-action pair is denoted s,a. The demonstrator’s
users. There has been some previous work in conﬂict elim- behavior is sampled at a ﬁxed time step Δt, which is equal
ination through clustering [Friedrich et al., 1996],butthat to the rate at which the character will choose actions when it
approach quantizes the training examples and culls many po- is autonomous. Thus the observed behavior is a set of dis-
tentially useful cases.                               crete cases, B = {s,a1, ..., s,aq}. Each pair represents
                                                      one case of the target behavior. There is no ordering of the
                                                                               μ
Contribution: We present a novel scheme for program-  pairs in the set. We construct by generalizing these cases.
                                                      Speciﬁcally, to ensure that the character’s behavior is smooth
ming EVA behavior through demonstration. While many of                                   μ
the components of our scheme are pre-existing, our overall and aesthetically pleasing, we construct by interpolating
                                                                                               s ∈ Rn
system and application are unique. Moreover, some notable the actions associated with these cases. Because and
                                                      a ∈ Rm
and critical components of our scheme are novel. Speciﬁc    , we can theoretically use any continuous real-vector-
contributions made in this paper include:             valued interpolation scheme. In our implementation, we ei-
                                                      ther use ε-regression SVM or continuous k-nearest neighbor,
  •
    The application of agent programming-by-demonstra- as detailed later. If the action set is symbolic, we use voting
    tion concepts to policy learning for humanoid EVAs. k-nearest neighbor.
  • An intuitive interface for non-technical users to quickly For generalization of cases to succeed, it is critical that the
    create stylized and natural-looking EVA behavior. state and action spaces be organized by the programmer such
  • A  novel conﬂict elimination method (previous tech- that similar states usually map to similar actions. In other
                                                      words, si −s j < α ⇒ai −a j < β,whereα and β are
    niques in agents/robotics have usually ignored or                        ·
    blended conﬂicts).                                small scalar thresholds and is the Euclidean metric. Cer-
                                                      tainly, this constraint need not always hold, but the smoother
                                                                                                μ
3  EVA Programming by Demonstration                   the mapping, the more accurate the learned policy will be.
Learning an EVA policy through demonstration involves the 3.1 Training
following steps (see Figure 1):                       In training mode, both the demonstrator and programmer
 1. Train:                                            need to be involved. First, the programmer integrates our
     (a) Observe and record state-action pairs.       technique into an existing (but thus far “brainless”) character.

                                                IJCAI-07
                                                  1258                                                       For each recorded pair s,a ...
This integration involves designing the state and action spaces                       
such that μ will be learnable. Once integration is complete, Find l closest neighbors of s,a
                                                           Compute median action avm of l neighbors using Eq. 3
the demonstrator is free to create policies for the character        
                                                           if (avm – a  > η)
at will. In fact, the creation of multiple policies for one agent       
may be interesting to achieve different stylized behaviors, etc. Mark s,a
  Demonstration proceeds as follows. The character and its Delete all marked pairs
virtual world are visualized in real-time, and the demonstra-
tor has interactive control over the actions of the character. Figure 2: Conﬂict elimination algorithm.
Note that the continuous, real-time presentation of state in-
formation to the demonstrator is critical to making the char-
                                                      In our experiments, we have found that it works well to use
acter training process as natural as possible, as this is analo-
                                                      l = 5. We use a threshold η of about 10% of the possible
gous to how humans naturally perceive the real world. As the
                                                      range of component values in a.
simulation-visualization of the character and its world pro-
ceeds in real-time, the demonstrator supplies the character 3.2 Autonomous Behavior
with the actions it is to perform. This information is saved
in the form of state-action pairs. Once enough state-action Once an adequate set of state-action pairs has been collected,
examples have been collected, all conﬂicting examples are we must construct the continuous policy, μ : s → a.We
automatically eliminated, as described below. We now have a do this through one of two popular machine learning tech-
discrete but representative sampling of the entire policy func- niques: ε-regression SVM [Haykin, 1999] or k-nearest neigh-
tion μ. Moreover, because the demonstrator has had control bor [Mitchell, 1997] (or through a classiﬁcation scheme if the
of the character, she has forced it into regions of the state actions are symbolic). We have chosen these two techniques
space of interest — therefore the density of the sampling cor- because they are powerful and well-established, yet have con-
responds to the importance of each region of the state space. trasting strengths and weaknesses in our application.
  Elimination of conﬂicting examples is important because The primary strength of continuous k-nearest neighbor (k-
human behavior is not always deterministic, and therefore nn) is that there are strong guarantees about its accuracy, as
some examples will likely conﬂict. This is an important issue, it merely interpolates local cases (e.g., it can robustly handle
because the machine learning schemes we utilize will “aver- non-smooth mappings, and the outputs will be within the con-
age” conﬂicting examples. This can result in unrealistic or vex hull of the k local cases). Therefore we use k-nn when-
unintelligent-looking behavior. Note that conﬂict resolution ever the demonstrated policy is rough. However, k-nn does
might also be possible by augmenting the state with context have a large memory footprint (∼1 MB per policy), and more
(e.g. the last n actions performed). However, this increase in state-action cases are required than with SVM due to weaker
the state-space dimensionality will only make the learning μ generalization.
more challenging and require additional training examples. The support vector machine (SVM) is an interesting alter-
Thus we have designed our technique to directly eliminate native to k-nn. It is a compact and global technique. As a
conﬂicts.                                             result, it performs powerful generalization, but can struggle
  Conﬂicting examples are formally deﬁned as:         with highly non-smooth mappings. We have found SVM to
                                                      be useful when μ is somewhat smooth, especially when it is
 if s −s  < ν and a −a  > υ, then conflict, (2)
     i   j           i   j                            C0 or C1 continuous. We use the popular ε-regression scheme
where ν and υ are scalar thresholds. To eliminate conﬂicts, with a Gaussian kernel to perform function approximation.
we cannot arbitrarily delete cases involved in a conﬂict —
this can lead to high frequencies in the policy. Our goal is to 4 Experimental Results
remove those examples that represent high frequencies.
  Pseudo-code for our conﬂict elimination technique is given These experiments were designed to cover, in a general fash-
in Figure 2. To complement this pseudo-code, we now de- ion, most of the major distinguishing aspects of popular uses
scribe our technique. In brief, each state-action pair is tested of embodied virtual agents. Between our favorable experi-
in turn to determine whether it is an outlier. First, the l neigh- mental results, and the results from the agents/robotics litera-
bors (according to state) of the current example are found ture on programming-by-demonstration (see the related work
and their median action avm is computed, using the follow- section), we have some empirical evidence that our scheme is
ing vector-median method [Koschan and Abidi, 2001]:   a viable tool for creating some popular types of EVA policies.
                                                      The results we achieved in our experiments are summa-
                               l                      rized in Table 1. Comparisons against reinforcement learn-
        avm = arg   min       ∑  ai −a j      (3)
                a ∈{a ,a , ...,a }                    ing, A*, and PEGASUS are summarized in Table 2 and dis-
                 i  1 2   l   =
                              j 1                     cussed in more detail later (while we do not take an RL ap-
In other words, avm is equal to the action of the neighbor that proach in our technique, we believe it is valuable to compare
is the closest to all other actions of the neighbors according against RL since it such a popular approach). The results of
                                     
to the Euclidean metric. Finally, if avm – a  > η, then the an informal user case study are reported in Table 3. Videos of
case is an outlier and is marked for deletion. Marked cases are the animation results are available from:
retained for testing the other cases, and then are all deleted as http://rivit.cs.byu.edu/a3dg/
a batch at the conclusion of the conﬂict elimination algorithm. publications.php

                                                IJCAI-07
                                                  1259  Pre-  Number of state-action pairs     6,000
        Time to demonstrate all pairs    8min.
        Time to eliminate conﬂicts       16 sec.
  k-nn  Compute target function values   13 μsec.
        Storage requirements             ∼1MB
        Total decision-making time to cross ﬁeld 7.04 msec.
        Total animation time to cross ﬁeld 36.1 sec.
 SVM    Time to train ε-regression SVM   2.5 min.     Figure 3: Spaceship pilot test bed. The pilot’s goal is to cross
        Compute new target function values 2 μsec.    the asteroid ﬁeld (forward motion) as quickly as possible with
        Storage requirements             ∼10 KB       no collisions.
        Total decision-making time to cross ﬁeld 1.13 msec.
        Total animation time to cross ﬁeld 37.7 sec.
                                                      approximation-based RL using a multi-layer neural net, but
                                                      this failed (the learned policies were very poor). This type of
Table 1: Summary of average usage and performance results learning is typically considered most stable when employing
in our spaceship pilot case study (with a 1.7 GHz processor,
               ∈ [ , ]                                a linear function approximator, but this can limit the learning
512 MB RAM,  k   2 7 ). Similar results were achieved in [Tesauro, 1995; Sutton and Barto, 1998]. Thus, for our ap-
other experiments.                                    plication, demonstration-based learning is appealing since it
                                                      is effective in high-dimensional, continuous state and action
4.1  Spaceship Pilot                                  spaces.
                                                        We also tested a non-standard form of RL designed specif-
In our ﬁrst experiment, the virtual agent is a spaceship pilot ically for continuous state spaces: PEGASUS [Ng and Jor-
(see Figure 3). The pilot’s task is to maneuver the spaceship dan, 2000; Ng et al., 2004]. In PEGASUS, a neural network
through random asteroid ﬁelds, ﬂying from one end of the learns a policy through hill climbing. Speciﬁcally, a small
ﬁeld to the other as quickly as possible with no collisions. set of ﬁnite-horizon scenarios is used to evaluate a policy
The animation runs at 15 frames per second, with an action and determine the ﬁtness landscape. Unlike traditional RL,
computed for each frame. The virtual pilot controls the ori- PEGASUS succeeded in learning an effective policy in our
entation of the spaceship.                            case study (see Table 1). Interestingly, the performance of the
  μ is formulated as follows. The inputs are the space- policies learned through our scheme and PEGASUS are sim-
ship’s current orientation (θ,φ), and the separation vector ilar. However, PEGASUS learning was far slower than our
between the spaceship and the two nearest asteroids (ps − technique (hours versus minutes). Moreover, we found it ex-
pa1 and ps − pa2). Thus the complete state vector is s = tremely difﬁcult and technical to tune the PEGASUS reward
θ,φ,Δx1,Δy1,Δz1,Δx2,Δy2,Δz2. There are two outputs, structure for aesthetics. Thus, while PEGASUS is a powerful
which determined the change in the spaceship’s orientation: and useful technique, it does not fulﬁll the requirement given
a = Δθ,Δφ.                                          in the introduction of providing non-technical users with an
  We achieved good results in this experiment, as shown in intuitive and robust interface for programming EVA AI. As a
Table 1 and in the accompanying video. The SVM and k-nn result, RL does not appear to be a good ﬁt for our application.
policies worked well for all random asteroid ﬁelds we tried, We also analyzed explicit action selection through A* (Ta-
and the animation was natural-looking, intelligent, and aes- ble 2). A* has empirically proven successful, but requires
thetically pleasing (veriﬁed in an informal user case study). signiﬁcantly more CPU than our policies to make decisions,
  To gain a point of reference and to illuminate the interest- sometimes produces behavior that does not appear natural,
ing computational properties of our technique, we analyzed and requires the programmer to develop an admissible heuris-
learning the policy through reinforcement learning (Table 2). tic.
This analysis helps validate why a non-RL approach is im-
portant in fulﬁlling our goals. It is straightforward to formu- 4.2 Crowd of Human Characters
late our case study as an MDP. The state and action spaces In our next experiment we created policies to control groups
have already been deﬁned. We have a complete model of the of human characters (see Figure 4). The characters milled
deterministic environment (needed for animation), the initial about, then a boulder fell from the sky and they ran away.
state is a position on one side of the asteroid ﬁeld, and the The policies controlled the decision making of the charac-
reward structure is positive for safely crossing the ﬁeld and ters (i.e., “turn left,” “walk forward,” etc), while the nuts-and-
negative for crashing. However, even using state-of-the-art bolts animation was carried out by a traditional skeletal mo-
table-based techniques for solving large MDP’s [Wingate and tion system. Thus the policy only speciﬁed a small set of
Seppi, 2005], it is currently implausible to perform RL with a discrete actions (the real-valued policy output was quantized
continuous state space of dimensionality greater than 6 (this to be discrete). We created several crowd animations. In each
size even requires a supercomputer). Our MDP state space animation, all characters used the same policy, showing the
dimensionality is 9. Even gross discretization is intractable: variety of behavior that can be achieved. Note that each pol-
e.g., for 10 divisions per axis, 109 states. The best result icy we constructed only required a few minutes to capture all
reported in [Wingate and Seppi, 2005] is a 4-dimensional necessary state-action examples, and only a few thousand ex-
state space discretized into approximately 107.85 states. We amples were required for use online.
also experimented with learning a policy through function- To train agents to interact in a crowd we performed sev-

                                                IJCAI-07
                                                  1260   RL   Storage requirements             ∼8G                Test bed User        Instruct. time Dem. time
        Time to learn policy             ∼1 week          Space ship Author      N.A.        7min.
 PEG.   Storage requirements             ∼5KB                        Artist      3min.       9.5 min.
        Time to learn policy             ∼2.3 hrs.                   Game player 1min.       8.5 min.
        Time to select actions           2 msec.             Crowd   Author      N.A.        12 min.
        Total decision-making time to cross ﬁeld 6.95 sec.           Designer    4min.       13 min.
        Total animation time to cross ﬁeld 36.6 sec.                 Student     3min.       13.5 min.
   A*   Average storage requirement      ∼8M
                                         ∼
        Max storage requirement            370 M      Table 3: Summary of an informal user case study. Instruct.
        Time to select actions           0.2 sec.     time is how long a new user needed to be tutored on the use
        Total decision-making time to cross ﬁeld 104.1 sec. of our system. Our technique is intuitive (little instruction re-
        Total animation time to cross ﬁeld 34.7 sec.
                                                      quired) and user-directed policies can be quickly constructed.
                                                      The users in this study prefered the aesthetics of their demon-
Table 2: Summary of average results using reinforcement strated policies over traditional policies.
learning, PEGASUS, and A* in the spaceship pilot case
study. Compare to Table 1. The purpose of this comparison
to help validate why we have selected demonstration-based thetic value of these policies was veriﬁed through informal
learning for our problem domain. Reinforcement learning user case studies (Table 3). Also, its use is natural and
takes a prohibitive amount of time with static or adaptive straightforward, it is applicable to non-technical users, and
table-based learning, and function-approximation-based RL there is empirical evidence that it is a viable tool for current
did not learn effective policies in our experiments. PEGA- uses of EVAs (e.g., computer games and animation). In ad-
SUS successfully learns a useful policy in our case study, but dition, our approach has a nearly ﬁxed online execution time,
requires signiﬁcantly more learning time than our approach, which is a useful feature for interactive agents.
and it is difﬁcult to tune the learning for aesthetics. A* takes Improvements to our approach that are the subject of ongo-
far more CPU time to make decisions than the policy learned ing research include the following. First, note that a demon-
through our technique, and requires an admissible heuristic. strator’s choice of actions may not make the character traverse
Thus, from a computational standpoint, our technique is an all regions of the state space, leaving gaps in μ. This problem
interesting alternative to these traditional schemes for agent can be automatically solved, during training, by periodically
action selection.                                     forcing the character into these unvisited regions of the state
                                                      space. Another important issue is perceptual aliasing, which
                                                      can result in unanticipated character behavior. However, per-
                                                      ceptual aliasing can be minimized through effective design of
                                                      the compact state space.
                                                        We have opted for learning standard policies (no task
                                                      structure or rules, e.g. [Nicolescu, 2003]) because it allows
                                                      for the most work to be done through programming-by-
        Figure 4: Virtual human crowd test bed.       demonstration versus explicit programming. However, the
                                                      scalability of policies is limited. Our technique could be
eral brief demonstration iterations, randomly placing some augmented with a task learning method from the literature
static characters in a scene. The human trainer then guided and thereby gain additional scalability and planning behavior,
the learning agent through the scene. Thus the agent learned but at the cost of greater programmer involvement. A good
how to behave in response to other agents around it in arbi- example is our crowd case study. Pre/post conditions (e.g.
trary positions.                                      [van Lent and Laird, 2001]) for the discrete contexts could be
  We created two policies. The ﬁrst was used before the learned, determining when to switch between policies.
boulder impact, and the second afterwards. In the ﬁrst pol- Because our current technique learns deterministic poli-
icy (pre-boulder), the environment was discretized into a 2D cies, there are some interesting behaviors that it cannot learn.
grid. s was formulated as the current orientation of the char- For example, complex goal-directed behaviors that must be
acter, and the separation between it and the next three clos- broken down into sub-tasks. Moreover, our technique is lim-
est characters. Decision making was computed at a rate Δt ited to policies that are smooth state-action mappings.
of about twice per second. After the boulder hit the ground, Conﬂict elimination is important because demonstrator be-
we switched to another policy (with continuous state/action havior may be non-deterministic, and conﬂicts may arise due
spaces) which simply directed the characters to run straight to varying delays in demonstrator response time. These con-
away from the boulder. This modularity made the policy ﬂicts can result in unintended behavior or temporal aliasing
learning easier.                                      (dithering) in the behavior. Our conﬂict elimination tech-
                                                      nique safely removes high frequencies in the state-action
                                                      pairs, helping make μ a smooth and representative mapping.
5  Discussion                                         Also, unlike elimination through clustering [Friedrich et al.,
Our technique has a number of strong points. As has been 1996], our method does not quantize or eliminate examples.
shown empirically, policies can be learned rapidly. The aes- As an alternative to eliminating conﬂicts, one may consider

                                                IJCAI-07
                                                  1261