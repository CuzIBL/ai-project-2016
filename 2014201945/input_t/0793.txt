                            Optimized Execution of Action Chains
                Using Learned Performance Models of Abstract Actions∗

                                    Freek Stulp and Michael Beetz
               Intelligent Autonomous Systems Group, Technische Universitat¨    Munchen¨
                            Boltzmannstrasse 3, D-85747 Munich, Germany
                                   {stulp,beetz}@in.tum.de

                    Abstract                          fewer actions because viewed at an abstract level the actions
                                                      are applicable to a broader range of situations. Also the mod-
    Many plan-based autonomous robot controllers      els themselves become more concise. This not only eases
    generate chains of abstract actions in order to   the job of the programmers but also the computational task
    achieve complex, dynamically changing, and pos-   of the automatic planning systems. At more abstract levels
    sibly interacting goals. The execution of these   the search space of plans is substantially smaller and fewer
    action chains often results in robot behavior that interactions between actions need to be considered.
    shows abrupt transitions between subsequent ac-     The advantages of abstraction, however, come at a cost.
    tions, causing suboptimal performance. The result- Because the planning system considers actions as black boxes
    ing motion patterns are so characteristic for robots with performance independent of the prior and subsequent
    that people imitating robotic behavior will do so by steps, the planning system cannot tailor the actions to the
    making abrupt movements between actions.          contexts of their execution. This often yields suboptimal be-
    In this paper we propose a novel computation      havior with abrupt transitions between actions. The resulting
    model for the execution of abstract action chains. motion patterns are so characteristic for robots that people
    In this computation model a robot ﬁrst learns     trying to imitate robotic behavior will do so by making abrupt
    situation-speciﬁc performance models of abstract  movements between actions. In contrast, one of the impres-
    actions. It then uses these models to automatically sive capabilities of animals and humans is their capability to
    specialize the abstract actions for their execution in perform chains of actions in optimal ways and with seamless
    a given action chain. This specialization results in transitions between subsequent actions.
    reﬁned chains that are optimized for performance.   Let us illustrate these points using a simple scenario in au-
    As a side effect this behavior optimization also ap- tonomous robot soccer that is depicted in Figure 1. The start-
    pears to produce action chains with seamless tran- ing situation is shown in the left sub-ﬁgure. The planner is-
    sitions between actions.                          sues a three step plan: 1) go to the ball; 2) dribble the ball to
                                                      shooting position; 3) shoot. If the robot naively executed the
                                                      ﬁrst action (as depicted in the center sub-ﬁgure), it might ar-
1  Introduction                                       rive at the ball with the goal at its back. This is an unfortunate
In recent years, a number of autonomous robots, including position from which to start dribbling towards the goal. The
WITAS  [Doherty et al., 2000], Minerva [Thrun et al., 1999], problem is that in the abstract view of the planner, being at
and Chip [Firby et al., 1996], have shown impressive per- the ball is considered sufﬁcient for dribbling the ball and the
formance in long term demonstrations. These robots have in dynamical state of the robot arriving at the ball is considered
common that they generate, maintain, and execute chains of to be irrelevant for the dribbling action.
discrete actions to achieve their goals. The use of plans en-

ables these robots to ﬂexibly interleave complex and interact-                              Plan:
                                                          Goal: Score!      Plan:
ing tasks, exploit opportunities, and optimize their intended               − go to ball     − go to ballin order to
                                                                            − dribble ball   − dribble ball
course of action.                                                           − shoot          − shoot
  To allow for plan-based control, the plan generation mech-
anisms are equipped with libraries of actions and causal mod-
els of these actions. These models include speciﬁcations of
action effects and of the conditions under which the actions
are executable. For good reasons, the action models are spec- a)       b)                c)
iﬁed abstractly and disregard many aspects of the situations
before, during, and after their execution. This abstractness Figure 1: Alternative execution of the same plan
has several big advantages. Programmers need to supply

  ∗The work described in this paper was partially funded by the What we would like the robot to do instead is to go to the
Deutsche Forschungsgemeinschaft in the SPP-1125.      ball in order to dribble it towards the goal afterwards. Therobot should, as depicted in the Figure 1c, perform the ﬁrst or STRIPS operators in the sense that they are temporally ex-
action sub-optimally in order to achieve a much better posi- tended actions that can be treated by the planner as if they
tion for executing the second plan step. The behavior shown were atomic actions.
in Figure 1c exhibits seamless transitions between plan steps
and has higher performance, achieving the ultimate goal in       State−space        State−space
less time.                                                        Pre−Cond.  Action
  In this paper we propose a novel computational model for                             Post−Cond.
plan execution that enables the planner to keep its abstract
action models and that optimizes action chains at execution
time. The basic idea of our approach is to learn perfor- The goToPoseTOP   has the empty pre-condition, as it
mance models of abstract actions off-line from observed ex- can be executed from any state in the state space. Its post-
perience. These performance models are rules that predict condition is [xt ≈ xd,yt ≈ yd, φt ≈ φd]. Its action is
the situation- and parameterization-speciﬁc performance of goToPoseAction.
abstract actions, e.g. the expected duration. Then at execu- TOP libraries contain a set of TOPs that are frequently
tion time, our system determines the set of parameters that used within a given domain. In many domains, only a small
are not set by the plan and therefore deﬁne the possible ac- number of control routines sufﬁces to execute most tasks, if
tion executions. It then determines for each abstract action they are kept general and abstract, allowing them to be ap-
the parameterization such that the predicted performance of plicable in many situations. Our library contains the TOPs:
the action chain is optimal.                          goToPoseTOP   and dribbleBallTOP.
  The technical contributions of this paper are threefold. A TOP chain for a given goal is a chain of TOPs such
First, we propose a novel computational model for the execu- that the pre-condition of the ﬁrst top is satisﬁed by the current
tion time optimization of action chains, presented in section 2. situation, and the post-condition of each step satisﬁes the pre-
Second, we show how situation-speciﬁc performance models condition of the subsequent TOP. The post-condition of the
for abstract actions can be learned automatically (section 3). last TOP must satisfy the goal. It represents a valid plan to
Third, we describe a mechanism for subgoal (post-condition) achieve the goal.
reﬁnement for action chain optimization. We apply our im-
plemented computational model to chains of navigation plans Pre−Cond.          Pre−Cond.
                                                                      Action  Post−Cond. Action
with different objectives and constraints and different task Current state i            i+1
                                                                                           Goal
contexts (section 4). We show for typical action chains in                                  Post−Cond.
robot soccer our computational model achieves substantial      State−space
and statistically signiﬁcant performance improvements for
action chains generated by robot planners (section 5).  Performance models of actions map a speciﬁc situation
                                                      onto a performance measure. In this paper the performance
2  System overview                                    measure is time. Alternatives could be chance of success or
                                                      accuracy. These models can be used to predict the perfor-
This section introduces the basic concepts upon which we mance outcome of an action if applied in a speciﬁc situation,
base our computational model of action chain optimization. by specifying the current state (satisfying the pre-conditions)
Using these concepts we deﬁne the computational task and and end state (satisfying the post-conditions).
sketch the key ideas for its solution.
                                                        goToPoseAction.performance(xt,yt,φt,xd,yd,φd) → t
2.1  Conceptualization                                2.2  Computational task and solution idea
Our conceptualization for the computational problem is based The on-line computational task is to optimize the overall per-
on the notion of actions, performance models of actions, formance of a TOP chain. The input consists of a TOP chain
teleo-operators, teleo-operator libraries, and chains of teleo- that has been generated by a planner, that uses a TOP library
operators. In this section we will introduce these concepts. as a resource. The output is an intermediate reﬁned subgoal
  Actions are control programs that produce streams of con- that optimizes the chain, and is inserted in the chain. Exe-
trol signals, based on the current estimated state, thereby in- cuting the TOP chain is simply done by calling the action of
ﬂuencing the state of the world. One of the actions used each TOP. This ﬂow is displayed in Figure 2.
here is goToPose, which navigates the robot from the cur- To optimize action chains, the pre- and post-conditions of
rent pose (at time t) [xt,yt,φt] to a future destination pose the TOPs in the TOP chains are analyzed to determine which
[xd,yd,φd] by setting the translational and rotational velocity variables in the subgoal may be freely tuned. These are the
of the robot:                                         variables that specify future states of the robot, and are not
  goToPoseAction(xt,yt,φt,xd,yd,φd) → vtra,vrot       constrained by the pre- and post-conditions of the respec-
  Teleo-operators (TOPs) consist of an action, as well as tive TOP. For the optimization of these free variables, per-
pre- and post-conditions [Nilsson, 1994]. The post-condition formance models of the actions are required. Off-line, these
represents the intended effect of the TOP, or its goal. It spec- models are learned from experience for each action in the
iﬁes a region in the state space in which the goal is sat- TOP library. They are used by the subgoal reﬁnement sys-
isﬁed. The pre-condition region with respect to a tempo- tem during execution time, but available as a resource to other
rally extended action is deﬁned as the set of world states in systems as well.
which continuous execution of the action will eventually sat- One of the big advantages of our approach is that neither
isfy the post-condition. They are similar to Action Schemata TOP library, nor the generation of TOP chains (the planner)     TOP Library                                      tial for learning accurate performance models. In Figure 3 it
                           3) Learn Performance Model is shown how exploiting transformational and rotational in-
       TOP1                                           variance reduces our original six-dimensional feature space
        Action                                        into a three-dimensional one, with the same predictive power.
        Pre/Post−conds           Perf.Model1
 Off−line

                                                       6−D: xt , yt ,ϕt ,xd , yd ,ϕd 3−D: dist, angle_at_dest ,
    Generate TOP chain                                                                       angle_to_dest
                           4) Subgoal refinement
                                                       d
   TOP chain                4.2) Find relevant variables y
    State−space State−space State−space                                      ϕ
    Pre−Cond. Pre−Cond. Goal                                     ϕ            d
        Actioni Actioni+1                                         t
            Post−Cond. Post−Cond. 4.3) Optimize relevant variables                             angle_at_dest
                                                       t
                                                       y
 On−line

                             Refined (optimal) subgoal                                 angle_to_dest
     Execute TOP chain                                                                    dist
                                                             x          x
              Figure 2: System Overview                       t          d
                                                      Figure 3: Transformation of the original state space into a
nor the TOP chain executor need to be modiﬁed in any way lower-dimensional feature space.
to accommodate the action chain optimization system.
                                                        Currently, we perform the transformation manually for
3  Learning performance models                        each action. In our ongoing research we are investigating
                                                      methods to automate the transformation. By explicitly rep-
The actual optimization of TOP chains, to be discussed in resenting and reasoning about the physical meaning of state
section 4, needs performance models of each action in the variables, we research feature language generation methods.
TOP library. For each action, the robot learns a function that The last step is to approximate a function to the trans-
maps situations to the cost of performing this action in the re- formed data. This is done using model trees. Model trees are
spective situation. In this paper, the performance measure is functions that map continuous or nominal features to a con-
time, although our mechanisms applies to other cost functions tinuous value. The function is learned from examples, by a
without requiring any change. The robot will learn the per- piecewise partitioning of the feature space. A linear function
formance function 1) from experience 2) using a transformed is ﬁtted to the data in each partition. Model trees are a gener-
state space 3) by partitioning the state space 4) by approxi- alization of decision trees, in which the nominal values at the
mating functions to the data in each of these partitions. We leaf nodes are replaced by line segments. We use model trees
will ﬁrst motivate why, and then explain how this has been because 1) they can be transformed into sets of rules that
implemented.                                          are suited for human inspection and interpretation 2) com-
  Let us consider the navigation action goToPoseAction. parative research shows they are the best [Belker, 2004;
This navigation action is based on computing a Bezier Balac, 2002] 3) They tend to use only relevant variables.
curve, and trying to follow it as closely as possible. This means we can start off with many more features
dribbleBallAction    uses the same method, but restricts than are needed to predict performance, having the model
deceleration and rotational velocity, so as not to loose the ball. tree function as an automatic feature selector. The tree
We abstract away from their implementation, as our methods was actually learned on an 11-dimensional feature space
consider the actions to be black boxes, whose performance [x,y,φ,xg,yg,φg,dx,dy,dist,angle to dest,angle at dest],
we learn from observed experience.                    the model tree algorithm automatically discovered that
  The robot learns the performance function from experi- only [dist,angle to dest,angle at dest] are necessary to
ence. It executes the action under varying situations, observes accurately predict performance.
the performance, and logs the experience examples. Since We have trained a model tree on the gathered data, yield-
the method is based solely on observations, it is also possi- ing rules of which we will now present an example. In Fig-
ble to acquire models of actions whose internal workings are ure 4, we depict an example situation in which dist and
not accessible. The robot executed each action 1000 times, angle to dest are to 2.0m and 0◦ respectively. Given these
with random initial and destination poses. The robot recorded values we could plot a performance function for varying val-
the direct variables and the time it took to reach the destina- ues of angle at dest. These plots are also depicted in Fig-
tion state at 10Hz, thereby gathering 75 000 examples of the ure 4, once in a Cartesian, once in a polar coordinate system.
format [xt,yt,φt,xd,yd,φd,time] per action. These examples In the linear plot we can clearly see ﬁve different line seg-
were gathered using our simulator, which uses learned dy- ments. This means that the model tree has partitioned the
namics models of the Pioneer I platform. It has proven to be feature space for dist=2 and angle to dest=0 into ﬁve areas,
accurate enough to port control routines from the simulator each with its own linear model. Below the two plots one of
to the real robot without change. Using our Pioneer I robots, the learned model tree rules that applies to this situation is
acquiring this amount of data would take approximately two displayed. An arrow indicates its linear model in the plots.
hours of operation time.                                The polar plot clearly shows the dependency of predicted
  The variables that were recorded do not necessarily corre- execution time on the angle of approach for the example sit-
late well with the performance. We therefore design a trans- uation. Approaching the goal at 0 degrees is fastest, and
formed feature space with less features, but the same poten- would take a predicted 2.1s. Approaching the goal at 180     situation:                                         For the robot, a subset of the state variables is observable to
      dist = 2.0                2m
      angle_to_dest = 0.0                             its perceptive system, and they can be estimated using a state
      angle_at_dest = [−180,180]                      estimation module. For any controller there is a distinction
                                                      between direct and derived observable state variables. All

                                     90   8           direct state variables for the navigation task are depicted in
    7                           120       60
                                         6            Figure 5. Direct state variables are directly provided by state
    6
                            150         4     30      estimation, whereas derived state variables are computed by
    5
                                        2             combinations of direct variables. No extra information is con-
    4                                                 tained in derived variables, but if chosen well, derived vari-
                           180                  0


   Time  (s) 3                                        ables are better correlated to the control task.

    2
                            210               330
                                                               i


    1                                                          y
                                                                        ϕt


   predicted  execution time (s)                                                   ϕ
    0                           240       300                                       i
    −180     0  59.2  180            270                       t
       angle_at_destAngle at goal (degree) (degree)            y
                                                               g
                                                               y
model tree rule:                                                                               ϕg
 if (2.3 > dist > 1.86) & (angle_to_dest < 49.7) & (angle_at_dest < 59.2)
                                                                              x
   then time = 1.26*dist + 0.018*angle_to_dest + 0.0037*angle_at_dest − 0.42 xt i        xg

Figure 4: An example situation, two graphs of time prediction Figure 5: Direct state variables relevant to the navigation task
for this situation with varying angle at dest, and the model
tree rule for one of the line segments.
                                                        State variables are also used to specify goals internal to
                                                      the controller. These variables are bound, conform to plan-
                                                      ning terminology. It is the controller’s goal to have the bound
degrees means the robot would have to navigate around the internal variables (approximately) coincide with the external
goal point, taking much longer (6.7s). p To evaluate the ac- observable variables. The robot’s goal to arrive at the inter-
curacy of the performance models, we again randomly gen- mediate position could be represented by the state variables
erate 1000 new test situations. For the goToBall routine,
                                                      [xi,yi]. By setting the velocities, the robot can inﬂuence its
the mean absolute error and root-mean-square error between current position [x ,y ] to achieve [x ≈ x ,y ≈ y ].
predicted and actual execution time were 0.31s and 0.75s.             t t            t   i  t   i
For the dribbleBall  routine these values were 0.29s and 4.2 Determining the search space
0.73s. As we will see, these errors are accurate enough to
optimize action chains.                               To optimize performance, only variables that actually inﬂu-
                                                      ence performance should be tuned. In our implementation,
                                                      this means only those variables that are used in the model
4  Automatic subgoal reﬁnement                        tree to partition the state space at the nodes, or used in the
As depicted in Figure 2, the automatic subgoal reﬁnement linear functions at the leaves.
system takes the performance models and a chain of teleo- In both the learned model trees for  the actions
operators as an input, and returns a reﬁned intermediate goal goToPoseAction and dribbleBallAction, the rel-
state that has been optimized with respect to the performance evant variables are dist, angle to dest and angle at dest.
of the overall action chain. To do this we need to specify These are all derived variables, computed from the direct vari-
all the variables in the task, and recognize which of these ables [xt,yt,φt,xi,yi,φi] and [xi,yi,φi,xg,yg,φg], for the ﬁrst
variables inﬂuence the performance and are not ﬁxed. These and second action respectively. So by changing these direct
variables form a search space in which we will optimize the variables, we would change the indirect variables computed
performance using the learned action models.          from them, which in effect would change the performance.
                                                        But may we change all these variables at will? Not xt,yt,
4.1  State variables                                  or φt, as we cannot simply change the current state of the
In the dynamic system model [Dean and Wellmann, 1991] world. Also we may not alter bound variables that the robot
the world changes through the interaction of two processes: has committed to, being [xi,yi,xg,yg,φg]. Changing them
the controlling process, in our case the low-level control pro- would make the plan invalid.
grams implementing the action chains generated by the plan- This only leaves the free variable φi, the angle at which the
ner, and the controlled process, in our case the behavior of the intermediate goal is approached. This acknowledges our intu-
robot. The evolution of the dynamic system is represented ition from Figure 1 that changing this variable will not make
by a set of state variables that have changing values. The the plan invalid, and that it will also inﬂuence the overall per-
controlling process steers the controlled process by sending formance of the plan. We are left with a one-dimensional
control signals to it. These control signals directly set some search space to optimize performance.
of the state variables and indirectly other ones. The affected
state variables are called the controllable state variables. The 4.3 Optimization
robot for instance can set the translational and rotational ve- To optimize the action chain, we will have to ﬁnd those val-
locity directly, causing the robot to move, thereby indirectly ues for the free variables for which the overall performance
inﬂuencing future poses of the robot.                 of the action chain is the highest. The overall performance isestimated by summing over the performance models of all ac- Before ﬁltering Total Higher Equal Lower
tions that constitute the action chain. In Figure 6 the ﬁrst two # runs 1000   533    369      98
polar plots represent the performance of the two individual improvement 10%   21%      0%    -10%
actions for different values of the only free variable, which is After ﬁltering Total Higher Equal Lower
the angle of approach. The overall performance is computed # runs      1000    505    485      10
by adding those two, and is depicted in the third polar plot. improvement 12% 23%      0%     -6%

                                      goToPose +
    goToPoseTime action 1 (s)(s) dribbleBallTime action 2 (s) (s) dribbleBallTotal time (s)  (s) Table 1: Results, before and after ﬁltering for cases in which
        90   12          90   12          90   12
    120    60        120    60        120    60       performance loss is predicted.
           9   +            9                9
 150       6  30  150       6  30 = 150      6  30
          3                3                 3

180            0 180            0 180            0    trained a decision tree to predict this nominal value. This tree
                                                      yields four simple rules which predict the performance dif-
 210         330  210         330  210          330   ference correctly in 86% of given cases. The rules declare
    240    300       240    300       240    300
       270              270              270          that performance will stay equal if the three points are more
                                                      or less aligned, and will only decrease if the ﬁnal goal po-
total = 7.5s             total = 6.1s
                                            3.8s      sition is in the same area as which the robot is, but only if
              5.4s                                    the robot’s distance to the intermediate goal is smaller than
                                                      1.4m. Essentially, this last rule states that the robot using the
                                                                 goToBallAction
       2.1s                                           Bezier-based                 has difﬁculty approaching
                                       2.3s           the ball at awkward angles if it is close to it. In these cases,
                                                      small variations in the initial position lead to large variations
                                                      in execution time, and learning an accurate, general model
Figure 6: Selecting the optimal subgoal by ﬁnding the opti- of the action fails. The resulting inaccuracy in temporal pre-
mum of the summation of all action models in the chain. diction causes suboptimal optimization. Note that this is a
                                                      shortcoming of the action itself, not the chain optimization
                                                      methods. We will investigate if creating a specialized action
  The fastest time in the ﬁrst polar plot is 2.1s, for angle of
                                                      for the cases in which Bezier based navigation is unsuccessful
approach of 0.0 degrees. The direction is indicated from the
                                                      could solve these problems.
center of the plot. However, the total time is 7.5s, because
the second action takes 5.4s for this angle . These values can We then gathered another 1000 runs, as described above,
be read directly from the polar plots. However, this value but only applied subgoal reﬁnement if the decision tree pre-
is not the optimum overall performance. The minimum of dicted applying it would yield a higher performance. Al-
the overall performance is 6.1s, as can be read from the third though increase in overall performance is not so dramatic
polar plot. Below the polar plots, the situation of Figure 1 is (from 10% to 12%), the number of cases in which perfor-
repeated, this time with the predicted performance for each mance is worsened by applying subgoal reﬁnement has de-
action.                                               creased from 98 (10%) to 10 (1%). Apparently, the decision
  We expect that for higher-dimensional search spaces, ex- tree correctly ﬁltered out cases in which applying subgoal re-
haustive search may be infeasible. Therefore, other optimiza- ﬁnement would decrease performance.
tion techniques will have to be investigated.           Summarizing: subgoal reﬁnement with ﬁltering yields a
                                                      23% increase in performance half of the time. Only once in a
5  Results                                            hundred times does it cause a small performance loss.
To determine the inﬂuence of subgoal reﬁnement on the over- 6 Related Work
all performance of the action chain, we generated 1000 situ-
ations with random robot, ball and ﬁnal goal positions. The Most similar to our work is the use of model trees to learn
robot executed each navigation task twice, once with subgoal performance models to optimize Hierarchical Transition Net-
reﬁnement, and once without. The results are summarized work plans [Belker, 2004]. In this work, the models are used
in Table 1. First of all, the overall increase in performance to select the next action in the chain, whereas we reﬁne an
over the 1000 runs is 10%. We have split these cases into existing action chain. Therefore, the planner can be selected
those in which the subgoal reﬁnement yielded a higher, equal independently of the optimization process.
or lower performance in comparison to not using reﬁnement. Reinforcement Learning (RL) is another method that seeks
This shows that the performance improved in 533 cases, and to optimize performance, speciﬁed by a reward function.
in these cases causes a 21% improvement. In 369 cases, there Recent attempts to combat the curse of dimensionality in
was no improvement. This is to be expected, as there are RL have turned to principled ways of exploiting temporal
many situations in which the three positions are already opti- abstraction. Several of these Hierarchical Reinforcement
mally aligned (e.g. in a straight line), and subgoal reﬁnement Learning methods, e.g. (Programmable) Hierarchical Ab-
will have no effect.                                  stract Machines, MAXQ, and Options, are described in the
  Unfortunately, applying our method causes a decrease of overview paper [Barto and Mahadevan, 2003]. All these ap-
performance in 98 out of 1000 runs. To analyze in which proaches use the concept of actions (called ‘machines’, ‘sub-
cases subgoal reﬁnement decreases performance, we labeled tasks’, or ‘options’ respectively). In our view, the beneﬁts
each of the above runs Higher, Equal or Lower. We then of our methods are that they acquire more informative per-