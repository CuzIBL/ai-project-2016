          Named Entity Translation with Web Mining and Transliteration 

                   Long Jiang*, Ming Zhou*, Lee-Feng Chien+, Cheng Niu* 
                                   *Microsoft Research Asia 
          *5F Sigma Center, No. 49, Zhichun Road, Haidian District, Beijing 100080, PRC 
                    +Institute of Information Science, Academia Sinica, Taiwan 
             *{longj, mingzhou, chengniu}@microsoft.com, +lfchien@iis.sinica.edu.tw 

                  Abstract                      rules for a deterministic generation of translation. However, 
                                                it is hard to systematically select the best Chinese character 
   This paper presents a novel approach to improve the 
                                                from multiple Chinese characters that have the same pro-
   named entity translation by combining a translit-
                                                nunciation, such as  and   . Statistics-based trans-
   eration approach with web mining, using web in- literation approaches select the most probable translations 
   formation as a source to complement transliteration, 
                                                based on knowledge learned from the training data. This 
   and using transliteration information to guide and approach, however, still may not work perfectly when there 
   enhance web mining. A Maximum Entropy model  are multiple standards [Gao, 2004]. For example, “ford” at 
   is employed to rank translation candidates by com-
                                                the end of an English NE is transliterated into  in 
   bining pronunciation similarity and bilingual con- most cases (e.g.,            ), but some-
   textual co-occurrence. Experimental results show times, it is transliterated into (e.g., 
   that our approach effectively improves the precision 
                                                   ). Obviously, there is significant flexibility in translit-
   and recall of the named entity translation by a large eration generation of foreign names in real world, and trans-
   margin.                                      literation selection is somewhat subjective. Hence, transla-
                                                tion relying on the statistical machine transliteration only 
1 Introduction                                  may not work well. 
Named entity (NE) translation accepts a named entity from a   The web contains an enormous dataset of languages, many 
source language as input and outputs its translations into a of which are instantly available and up to date. In recent 
target language. For instance,         ” (Eng-  years, some researchers, like [Wang et al., 2004; Cheng et al., 
lish to Chinese). Large quantities of new named entities 2004; Nagata et al., 2001; Zhang et al, 2005] have used it to 
appear each day in newspapers, web sites and technical lit- extract translations with promising results. In particular, 
erature, but their translations normally cannot be found in [Wang et al., 2004; Cheng et al, 2004] propose a novel ap-
translation dictionaries. Improving named entity translation proach. They search target language web pages using the 
is important to translation systems and cross-language in- source term or NE. Then it extracts the translation candidates 
                                                              1
formation retrieval applications. Moreover, it benefits bi- based on   score + Local Maxima algorithm and 
lingual resources acquisition from the web and translation ranks the generated candidates with Chi-Square method and 
knowledge acquisition from corpora. While named entities context vector method. This approach gets excellent results 
normally refer to a range of concepts like people names, for high-frequency terms and NEs. However, it cannot han-
place names, organization names, product names etc., the dle the low-frequency words or ambiguous words (e.g. 
focus of this paper is the translation of people names from   ) well, because their translations scarcely 
English to Chinese.                             appear in the search results.  
  Many of the previous works extract named entity transla-   Other efforts have been made to undertake combinations 
tion dictionaries from bilingual corpora [Kupiec, 1993; Feng of transliteration and corpora mining [Shao and Ng, 2004; 
et al., 2004]. To alleviate the strong dependency on parallel Huang et al., 2004] or web mining [Al-Onaizan and Knight, 
corpora, [Rapp, 1999; Fung and Yee, 1998] try to obtain 2002]. In particular, [Al-Onaizan and Knight, 2002] use 
named entity translations from nonparallel, comparable texts, 
or unrelated bilingual corpora.                                                                  
                                                  1                      ( 1... ) ( 1... )
  Since a large proportion of English named entities can be ( 1... )
translated by transliteration, many works try to build trans-     1     1
                                                                           ( 1... ) (   1... )
literation models with a rule-based approach [Wan and               1  1
Verspoor, 1998] and statistics-based approach [Knight and Where  is the n-gram to be estimated, is the 
Graehl, 1998; Lin and Chen, 2002; Virga and Khudanpur, number of unique left adjacent characters.  is the 
2003; Gao, 2004]. Both approaches, however, still have room number of unique right adjacent characters. is the 
for improvement. Rule-based approaches adopt linguistic frequency of the N-gram  . [Cheng et al, 2004]. 


                                           IJCAI-07
                                            1629 statistical machine transliteration model to generate transla- are selected from the snippets as translation candidates 
 tion candidates, and use the web for translation identification. if they are pronounced similarly to the input NE. 
 This method demonstrates positive results when using the 3. Web mining for translation candidates using queries 
 web for selecting correct translation from candidates, but not expanded by transliteration: The input English NE is 
 for helping generate translation candidates.        combined with each Chinese character (anchor) in the 
  In this paper, we are interested in the question of : how to transliteration results to form expanded queries. Then 
and to what extent the NE translation can benefit from the the expanded queries are used to search web pages. 
combination of transliteration and web mining in both can- From the returned snippets, we select the n-grams 
didate generation and translation identification. Specifically, which contain the anchors and have strong pronuncia-
 we are concerned with how to enhance the transliteration tion similarity to the input NE as additional translation 
 results with web information. That means, for example, candidates. This approach aims to deal with ambiguous 
      (output of transliteration system for  ) could words whose majority usage is not a proper name (e.g.
 be corrected as     (the correct translation of             ).  
   ”), and how to guide web mining with transliteration 4. Finally, all the translation candidates generated from 
information, rather than using statistical association alone. the step 2 and step 3 are ranked by a ME model that has 
To do this, we propose a novel framework of NE translation features covering pronunciation similarity and bilingual 
that effectively incorporates approaches of transliteration and contextual co-occurrences. 
web mining. Specifically, we build a transliteration model 
first, and the transliteration result is used to guide web min- 3 Transliteration 
ing for generating translation candidates. Furthermore, a 
Maximum Entropy (ME) model is designed to rank the Given an English NE, denoted as  , we first syllabicate it into 
translation candidates with various relevant features.  a “syllable” sequence   with the following 
                                                 linguistic rules we defined: 
  Benchmarks are performed on each component and the  
overall system to reveal our improvement to previous works. 1.  are defined as vowels.  is defined as a 
Experimental results show that NE translation can be con- vowel when it is not followed by a vowel. All other 
                                                      characters are defined as consonants;  
sistently improved with our new approach by a large margin.   
  1.  Using transliteration-guided web mining, the trans- 2. Duplicate the nasals  and  whenever they are sur-
      lation candidate coverage is improved from 54.5% to rounded by vowels. And when they appear behind a 
      74.5%.                                          vowel, they will be combined with that vowel to form 
                                                      a new vowel; 
  2.  By including pronunciation similarity and bilingual  
      contextual co-occurrence features, a ME model help 3. Consecutive consonants are separated;  
                                                   4. Consecutive vowels are treated as a single vowel;  
      to improve the overall translation precision from  
      18.5% to 47.5% in top 1 answer.              5. A consonant and a following vowel are treated as a 
                                                      syllable;  
  The remainder of this paper is structured as follows. Sec-  
tion 2 presents steps of our approach. In Section 3, the 6. Each isolated vowel or consonant is regarded as an 
transliteration model is described. Section 4 introduces our individual syllable. 
new approach to provide translation candidates by combining   For example,  is split into 
transliteration with web mining. The ranking method for .    is split into               is split 
translation candidates with a ME model is presented in Sec- into .      is split into        . 
tion 5. Experiments and performance comparison with pre-   Then a generative model is used to transliterate the syl-
vious works are reported in Section 6. The last section points labicated English name into Chinese character string, which 
out the directions of future work and concludes our study. follows [Knight and Graehl, 1998]. Specifically, for the 
                                                 generated “syllable” sequence        , we seek a 
                                                 Chinese Character sequence           that maxi-
 2  Our Approach                                 mizes the product of   ,      and    ,  
 Specifically, the translation process we proposed can be  
 described as follows:                                   *  argmax  (  |  ) (   | ) ( ) 
 1. Transliteration: To transliterate English NE into Chi-  
    nese word base on their pronunciation similarity. In this where  is a Chinese Pinyin sequence, is the 
    step, a three-level transliteration model is built: English probability of translating  into  ,  is the prob-
    surface string to Chinese Pinyin string, Chinese Pinyin ability of translating  into  and  is the generative 
    string to Chinese character string and Chinese character probability of a character-based Chinese language model. 
    language model.                                Different from [Knight and Graehl, 1998; Virga and Khu-
 2. Web mining for translation candidates enhanced by danpur, 2003] whose transliteration model is based on pho-
    transliteration: Compared with [Wang et al., 2004; neme, we use “syllable” as translation units (TU). 
    Cheng et al, 2004], transliteration-based similarity is   Here, a Chinese “syllable” means a Chinese Pinyin string 
    used to improve the translation candidate generation for that is corresponding to a Chinese character. And English 
    low-frequency words. The Chinese pages are searched “syllable” means a combination of several English letters that 
    using the input English NE as query. The n-gram strings can generate a corresponding Chinese Pinyin “syllable”. 


                                            IJCAI-07
                                             1630   Using syllables as TU has the following advantages: 1) where  is the Pinyin sequence of  and  is the 
 compared with phoneme-based approach, a syllable has far Pinyin sequence of  .  and  are the lengths 
 less ambiguity in finding the corresponding Chinese Pinyin of  and  respectively, and    is the 
 string. For instance,  has fewer corresponding Chinese Edit Distance between  and  .  
 Pinyin strings than the phoneme sequence / /; 2) a   Based on the formulas above, we compute the scores for 
 syllable always corresponds to a legal Pinyin sequence. all n-grams in the snippets. And then select those n-grams as 
   The translation model     can be trained with candidate whose score of being a translation candidate is over 
 GIZA++2 on parallel English names and the Pinyin repre- a threshold. 
 sentation of Chinese names. The translation model   This approach can make adequate use of the translitera-
 can be approximately obtained from a pronunciation dic- tion information obtained in the previous model and hand 
 tionary. In our experiment, we used an in-house dictionary. those transliterated translation even though it is a 
 The language models  is approximated by using a tri- low-frequency word in the snippets. By the way this method 
 gram model and trained with a list of Chinese transliterated is also independent of NE tagger. 
 names. 
                                                 4.2  WM-NE-Anchor: Enhancing Web Mining 
 4  Web Mining                                       Using Transliteration Result as Anchors 
 After we get the transliteration result of source NE, we want In addition to WM-NE, the result of transliteration 
 to use it to help finding the translation candidates on the web.  is used as anchor information to guide the web 
 The specific methods are as follows.            mining to obtain additional translation candidates by using 
                                                 the following steps.  
 4.1  WM-NE: Candidate Collection by Searching     1. Search the web with an anchor character and the input 
     the Web with NE                                  NE. Each character, , is combined with source Eng-
                                                      lish NE as a query to search for web pages. The first 
 Same with [Wang et al., 2004], we use the input English NE 30 snippets are selected; 
 as a query to search for the Chinese pages and extract can- 2. Surrounding the position of  in a snippet, we extract 
 didates in the returned top 100 snippets. In our experiments, 
                 3                                    all the n-gram character strings that contain and its 
 Google search engine   is used for obtaining the snippets.  score of being a translation candidate described in 
   It is noticed that usually the results of statistical translit- Subsection 4.1 is over a threshold as candidates.  
 eration model are not exactly correct, but very similar with   For example,  is transliterated into  by 
 the correct translation. If we use web data to correct the our transliteration model in Section 3. Then the Chinese word 
 transliteration result, the accuracy would be improved a lot. is split into three characters: , and . Each of 
 Based on this insight, the n-grams that have similar pronun- these characters is combined with  to form a Boolean 
 ciation with the transliteration results are collected from the query to search for web pages. For each query, we select the 
 search engine snippets, and are used for transliteration cor- top 30 returned snippets to form a small corpus, e.g. for the 
 rection.                                        query            , we get a corpus {
   Suppose  is the best transliteration result output by pre-                          }. In the 
 vious transliteration model. We define the score of an n-gram corpus we search the position where  appears, and select 
    being a translation candidate as the product of  all n-gram strings that contains  and has score higher 
 and    , ,                                      than the threshold to be a translation candidate, and the re-
                                                 sults are {          }. 
        Score(wn)   TL(wn,W)    p(wn)              This approach can effectively enhance WM-NE. In 
                                                 WM-NE, the returned snippets may not contain the anchor + 
where,          indicates the pronunciation similarity NE cases within 100 snippets. One of the reasons is that the 
 between the n-gram  and the source English NE, and source NE may be quite ambiguous. It can be used as a per-
      indicates the probability of  being a transliterated son name sometimes, but this is not its majority usage. For 
 name which is the generative probability of the Chinese example, “Spain” can be translated into “ ” when used 
 character-based language model used in Section 3. And as a person name, but more popularly, it should be translated 
         can be computed using the following formula. into “ ”. When we search web using only “Spain” as 
                                                 query, all returned snippets by Google do not contain “
                          (      ,     )           ”, but when we search web using “ ”, the top 30 
        (  ,   )  1                       
                       (    )    (      )        returned snippets contain “ ” ({
                                                                }). So the WM-NE-Anchor method can 
                                                 find the specific translation of many ambiguous words. 

                                                  5  Ranking Translation Candidates 
   2 http://www-i6.informatik.rwth-aachen.de/Colleagues/och/soft A ME model is used to rank the translation candidates ob-
ware/GIZA++.html.                                tained above, which contains the following four features 
   3 http://www.google.com 


                                            IJCAI-07
                                             1631 functions              . Where,  denotes a Chinese 6.1  Transliteration Evaluation  
 candidate,  denotes the source English NE. 
                                                 To evaluate our transliteration model using syllables as 
 1. Same with [Cheng et al., 2004], the Chi-Square of translation units, we compare its performance with [Virga 
    translation candidate  and the input English named and Khudanpur, 2003] which is also on English to Chinese 
    entity  is used as one of our features. The Chi-Square is 
                                                 transliteration. The measures are Pinyin error rate (Pinyin 
    computed using the formula below:            ER) and Chinese character error rate (Char ER). Here, Pinyin 
                                  2              ER is the Edit Distance between the “correct” Pinyin repre-
                                          
        1  ,                                     sentation of the correct transliteration and the Pinyin se-
   
                                                 quence output by the system [Virga and Khudanpur, 2003]. 
        = the number of pages containing both  and  Char ER is defined similarly, , it is the Edit Distance 
        = the number of pages containing  but not  
                                                 between the character sequence of the “correct” translitera-
        = the number of pages containing  but not  
                                                 tion and the character sequence output by the system. The 
        = the number of pages containing neither  nor  results are listed below: 
        = the total number of pages,  ,  
  In our experiments,  is set to 4 billion. Actually, the value           Pinyin ER  Char ER 
of  doesn't affect the ranking once it’s positive. To get the 
value of parameters in the formula above, we combine  and [Virga and Khudanpur, 2003]  42.5%  N/A 
   as a query to search with Google for web pages. And the Our transliteration model  39.6%  46.1% 
returned page contains the number of total pages containing 
both  and  which is corresponding to a in the formula     Table 1 Transliteration model evaluation 
above. Then we use  and  as queries respectively to search 
the web and we can get the two numbers  and  , which From Table 1, we can see that our model outperforms 
 represent the number of pages contain  and  respectively. [Virga and Khudanpur, 2003]. The results support using 
 Then we can compute    ,        and             “syllable” as translation unit in both sides in transliteration 
 .                                               model. In our experiments, the TU on English side contains 
 2. Contextual feature    , where  is the number of about 1,260 English syllables and the TU on Chinese side 
    occurrence, in any of the snippets selected, that  is in a contains 370 Pinyin syllables. 
    bracket following  or  is in a bracket following  ;  
 3. Contextual feature      where  is the number of 6.2  Effectiveness of Candidate Generation by 
    occurrence, in any of the snippets selected,  and  are Combining Transliteration with Web Mining  
    immediate neighbors;                         We want to know whether our new web mining approach,
 4. Similarity of  and  in terms of transliteration similar- , WM-NE and WM-NE-Anchor, improves the coverage of 
    ity      , which is equal to    described in translation candidates. So we made some experiments to 
    Subsection 4.1.                              compare our method with [Al-Onaizan and Knight, 2002] 
                                                 and [Wang et al., 2004]. Because they used non-public data 
  With these features, the ME model is expressed as:  set for evaluation and [Al-Onaizan and Knight, 2002] per-
                         4                       formed Arabic-English translation instead of Eng-
                     exp[     ( , )]             lish-Chinese translation, which make the direct comparison 
                          1                      difficult. Instead, we reimplemented their methods and 
    ( | )   4 ( | )       4
            1
                      exp[     ( ', )]           benchmarked our improvement using LDC data set. The 
                                                 results are listed below: 
                     '     1
                                                                    Candidates Average number of 
  To estimate the parameters  we use the Generalized It-  
erative Scaling algorithm of [Darroch and Ratcliff, 1972].           coverage     candidates 
                                                  [Al-Onaizan and 
                                                                     46.5% 100 
                                                  Knight, 2002] 
 6 Experiments  
                                                  [Wang , 2004]      54.5%         295.1 
 We use the same method as [Gao, 2004] to collect the 
 training data,  , we extract all named entity pairs from the WM-NE 58% 64.0 
 LDC Chinese-English Name Entity Lists Version 1.04, which 
                                    5             WM-NE + 
 also appear in CMU pronunciation dictionary . We finally            74.5% 144.0 
obtain 25,718 person names. Out of these name pairs, 200 are WM-NE-Anchor 
selected as development set, 200 as testing set, and the rest is Table 2 Candidates coverage comparison with previous approaches 
for training. 
                                                   In [Al-Onaizan and Knight, 2002], N-best outputs of 
                                                 transliteration model are used as translation candidates for 
                                                  re-ranking. Obviously, when  increases, the coverage of 
   4                                             translation candidates increases. So which number should we 
     Catalog Number by LDC: LDC2003E01           assign to  to make a fair comparison? In our reimplemen-
   5 http://www.speech.cs.cmu.edu/cgi-bin/cmudict 


                                            IJCAI-07
                                             1632tation experiment of their algorithm, we found that after N less effective on low-frequency candidates. Moreover, 
increases over 100, the increment of candidate coverage Chi-Square cannot differentiate candidates based on con-
becomes negligible (see Figure 1). So we finally decide to set tents. However, transliteration scores bring information on 
N equal to 100 for our comparison experiment.   linguistic association between NE and candidates. Therefore, 
                                                they can improve ranking effects. In addition, from our re-
                                                sults we can see that contextual features are also helpful in 
                Candidate Coverage              improving rankings with about 4.5% for the  . 
 
       50.0                                     6.4  Comparing Previous Approaches 
       40.0                                     To compare with the previous approaches for transliteration 
                                                and web mining, three experiments are conducted: 
       30.0
                                                  1. Exp_1 (Based on [Al-Onaizan and Knight, 2002]): 
       20.0                                          uses transliteration model to generate candidates and 

     Coverage                                        then combine straight web counts of candidates to 
       10.0                                          select correct translations. 
        0.0                                       2. Exp_2 (Based on [Wang et al., 2004]) provides 
 
             1  3   5  10  20 30  50 100             translation candidates via  + Local Maxima 
                                                     Algorithm and ranks them by Chi-Square method and 
               Number of Candidates                  Context vector method. 
                                                  3. Exp_3 (Our approach) gets translation candidates via 
  Figure 1 Candidates coverage of N best transliteration output WM-NE and WM-NE-Anchor and ranks them with 
  In [Wang et al., 2004], candidates are extracted by  all of the four features mentioned in Section 5.  
+ Local Maxima Algorithm. This method is good at handling   Table 4 indicates our new approach surpasses the previous 
high-frequency words, but worse for low-frequency words. approaches of transliteration and web mining by a substantial 
In our testing data, the average frequency of the correct margin.  
translation appearing in the search result is about eight, and  Exp_1 Exp_2 Exp_3 
for 40.5% of NEs, the frequency of its correct translation is 
                                                                    18.0% 18.5% 47.5% 
less than three. So  + Local Maxima Algorithm is 
hard to extract these translations. From Table 2, we also           35.5% 35.0% 66.0% 
notice that WM-NE-Anchor improved the coverage signifi-             41.5% 39% 69.5% 
cantly. That means our WM-NE-Anchor method does work 
for many ambiguous NEs.                                  Table 4 Comparing previous approaches 
                                                  To analyze errors, we checked a sample of 50 NEs. 24 
6.3  ME Model Evaluation                        (48%) NEs provide a correct translation. In the remaining 
To test the effectiveness of each feature used in our ME data, 10 (20%) NEs acquired translations different from the 
model, we conduct four experiments. Like [Cheng et al., ones defined in LDC data though they are acceptable. Further 
2004], the metric is       which is defined as the study shows that most are actually more popular than that 
percentage of the NE whose translations could be found in LDC data. For instance, the translation of “Dockery” in LDC 
the first  extracted translations.              data is “   ”. Our system outputs “  ”.  We check 
                                                with Google, “    ” and “     ” co-occurs in 119 
         Chi-Squ         Chi-Square 
                  TL               All features pages, but “   ” and “    ” co-occurs in only three 
          are              + TL                 pages. This may indicate that “ ” is also a popular 
          7.5% 34.0%      43.0%      47.5%      translation.  
         23.0% 55.5%      59.0%      61.5%        We further analyze the error sources for the remaining 16 
         33.0% 63.5%      63.5%      66.0%      (32%) incorrect NE translations. For five NEs, the correct 
         49.0% 69.5%      66.5%      69.5% 
                                                translations do not co-occur in any Chinese web page. For 
       Table 3 Candidate ranking experiment result eight NEs, the correct translations are not ranked high 
                                                enough in the candidate sets by TL score. For the other three 
  Table 3 shows that incrementally adding features is effec- NEs, the translation errors result from mistakenly identifying 
tive in improving the           . Ranking with  lexical boundaries in the candidate generation stage. It should 
Chi-Square method alone, the precision is 7.5%. Ranking be noted that, since our testing data is randomly selected 
with TL score alone, the precision is 34.0%. Combining these from a large LDC data set, and many of them are scarcely 
two features together, precision reaches up to 43.0%, and used in real world, the performance of our system should not 
using all features mentioned in Section 5, it reaches a 47.5% be compared directly to those work based on their specific 
precision rate.                                 testing data. 
  We further analyze the reasons. Chi-Square, as the statis-
tical association of the English NE and the translation can-
didates, is effective mainly on high-frequency candidates, but 


                                           IJCAI-07
                                            1633