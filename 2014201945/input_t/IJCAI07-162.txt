                        Learning to Count by Think Aloud Imitation

                                            Laurent Orseau
                                     INSA/IRISA, Rennes, France
                                            lorseau@irisa.fr


                    Abstract                          2001]. He uses a paper-and-pencil approach to learn, by im-
                                                      itation, algorithms as complex as making a division. How-
    Although necessary, learning to discover new solu- ever, still many assumptions are made that clearly ease learn-
    tions is often long and difﬁcult, even for suppos- ing: the paper-and-pencil approach provides an external inﬁ-
    edly simple tasks such as counting. On the other  nite memory and useful recall mechanisms; the system also
    hand, learning by imitation provides a simple way already contains some basic arithmetic tools, such as the no-
    to acquire knowledge by watching other agents do. tion of number, of numerical superiority, etc. Furthermore,
    In order to learn more complex tasks by imitation it is able to learn to make function calls (access to subrou-
    than mere sequences of actions, a Think Aloud pro- tines), but the teacher has to explicitly specify which function
    tocol is introduced, with a new neuro-symbolic net- to use, with which inputs and outputs. Calling a function is
    work. The latter uses time in the same way as in  thus a special action.
    a Time Delay Neural Network, and is added basic     In this paper, there is no speciﬁc action: the sequences are
    ﬁrst order logic capacities. Tested on a benchmark composed of completely apriorimeaningless symbols, i.e.,
    counting task, learning is very fast, generalization all these symbols could be interchanged without changing the
    is accurate, whereas there is no initial bias toward task. It is able to make basic automatic function calls, without
    counting.                                         explicitly specifying it. There is also no initial internal feature
                                                      that biases the system toward counting.
                                                        In section 2, Think Aloud Imitation Learning is described,
1  Introduction                                       with a usual neural network. The latter is superseded in
Learning to count is a very difﬁcult task. It is well-known the next section by the Presence Network, a growing neuro-
that children do not learn only by themselves: their parents symbolic network [Orseau, 2005a]. In section 4, the latter
and their teachers provide an important active help.  is then augmented by special connections, and its develop-
  On the other hand, in the ﬁeld of on-line machine learn- ment is explained on a simple task. Then experiments such
ing where the teacher does not provide help, some Recur- as counting are given, showing that learning can be faster by
rent Neural Networks are interestingly successful, such as the several orders of magnitude than with a usual neural network,
Long Short-Term Memory (LSTM) [Hochreiter and Schmid- and that some important learning issues can be circumvented,
huber, 1997]. However, to learn some counters the network so that learning complex tasks on-line in a few examples is
still requires to be trained through tens of thousands of se- possible.
quences, and it already contains a counting feature, although
it had not been speciﬁcally designed for this.        2   Think Aloud Imitation Learning
  Between active teaching and automatic discovery stands In imitation, the environment, which gives the task to solve,
learning by imitation [Schaal, 1999]: the learner watches the produces events env(T ) and the teacher, who knows how to
“teacher” solve the task, but the teacher may not be aware of solve it, produces tch(T ),whereT denotes the present time.
the presence of the learner. This is helpful for sequences of During learning, the agent tries to predict the teacher’s next
actions, for example to make a robot acquire a complex motor action. When it succeeds a(T )=tch(T ).
behavior [Schaal, 1999; Calinon et al., 2005]: the learner can The agent is given sequences of events such as
see what intermediate actions should be done.         /abcdEfgH/, where the events provided by the environment
  Fewer studies on imitation focus on learning sequences that are written in lowercase letters and those provided by the
are more complex than sequences of physical actions, i.e., teacher are in capitals, but should be seen as the same sym-
algorithms. Programming by Demonstration [Cypher, 1993] bols. The 26 letters of the alphabet are used as events all
deals with this issue, but is more interested in human-machine along this paper. For better legibility only, spaces will some-
graphical interaction and thus makes many assumptions about times be introduced in the sequences. The goal of the learner
how tasks are represented. Maybe the work that is the clos- is to predict the teacher’s events, so that once it is on its own,
est to the one presented in this paper is that of Furse [Furse, it not only predicts but in fact acts like the teacher would do.

                                                IJCAI-07
                                                  1005  In imitation, one problem with Recurrent Neural Networks 3 The Presence Network
(for example) is that they have internal states; the learner can-
                                                      In a TDNN, the number of input connections of a single hid-
not “see” these states inside the teacher, which are thus difﬁ-     |  | =
cult to learn.                                        den neuron j is wj    τNI ,whereNI  is the number of
                                                      inputs. It grows quickly when τ or NI grows and makes
  Therefore, in the Think Aloud paradigm, the teacher is
                                                      learning difﬁcult: the number of training examples is often
forced to “think aloud”, this means that it does not have inter-    3
                                                      said to be in |w| . In the experiments described in section 5,
nal states but it can “hear” what it itself says: the recurrence
                                                      many time steps, neurons and inputs are needed, so that using
is external, not internal. Then it can be easily imitated by
                                                      a mere TDNN is unpractical. TDNN and backpropagation
the learner, who “listens” to the teacher when learning. Once
                                                      also have important limitations when learning several tasks
on its own, the agent, not learning anymore, also thinks aloud
                                                      [Robins and Frean, 1998]. They are prone to catastrophic for-
and can hear itself. This allows the teacher to make intermedi-
                                                      getting: learning a concept B after a concept A often modiﬁes
ate computation steps that can be imitated by the learner. Dur-
                                                      too much the knowledge of concept A. Another issue arises
ing learning, the agent thus receives the sequence of events of
                                                      with unmodiﬁed weights: the network will tend to provide an-
the teacher and of the environment e(T )=env(T )+tch(T ).
                                                      swers even before learning. Thus, even if weights are frozen
After learning, during testing, the agent is on its own, so
                                                      after concept A, adding new neurons may (sometimes not)
tch(T )=φ and the agent listens to itself, not to the teacher:
                                                      temporarily mask other already acquired knowledge. Learn-
e(t)=env(T  )+a(T  ). This can be seen as some teacher
                                                      ing must then be adapted to prevent this.
forcing technique [Williams and Zipser, 1989].
                                                        Instead of a mere TDNN, we use a Presence Network
  A ﬁrst, simpliﬁed view of the architecture is now given. [Orseau, 2005a], which is a neuro-symbolic network more
In order to take time into account without having internal tailored for learning sequences of events such as those dealt
states, we use a sort of Time Delay Neural Network (TDNN) with here. It uses time in the same way as a TDNN and its
[                           ]
Sejnowski and Rosenberg, 1988 , that computes its outputs main properties are very fast learning, local connectivity and
given only the last τ time steps. At present time T ,itcom-
         ( )=   ( ( )  (  − 1)    (  −   +1))         automatic creation of neurons. A slight modiﬁcation is made:
putes: Net T   f e T ,e T     ,...e T  τ     .        “speciﬁcity” is introduced to circumvent an issue of the ini-
  In the sequel, the indices i, j, k are used respectively only tial network. The Presence Network is described below but
for inputs, hidden neurons, output neurons. xi is the activa- for its use in a less speciﬁc context, see [Orseau, 2005a].
tion of input i. As with other variables, writing yi is a shortcut The heuristic idea is that new hidden neurons are connected
for yi(T ),whereT is the present time, with the only excep-
         d           (  −  )          ( )=            only to inputs that were active in the short past (that are in
tion that xi stands for xi T d . Writing e t i means  the short-term τ-memory), so that the number of connections
that input i is activated at time t, where inputs are encoded in does not fully depend on the number of inputs. This is partic-
a 1-of-N way.                                         ularly useful when one event type is coded on a single input.
  Usually, the TDNN architecture is explained with delayed Since sequences are composed of mutually exclusive events,
copy sets of inputs; here we describe it as follows: each con-
                d                                     there is only one event per time step, so a neuron would have
nection (weight) wji from input unit i to hidden unit j has a only one connection per time step in such a network. Initially
                          d
delay d,sothatife(t)=i, wji transmits this activation to there is no hidden neuron, and new ones are added on-line
unit j at time t + d. A hidden neuron j is then connected when errors occur.
                                        d
to input i through a number τ of connections wji, each one First, the hidden neuron and its optimization rule is given,
having a different delay τ<d≤ 0. Such a network cannot in order to show what the network can represent. The action
produce recurrent, inﬁnite behaviors.                 selection scheme can then be given to show how to circum-
  It is the fact that the agent can hear what it says during vent one issue of the representation capacities of the network.
testing that gives it recurrent capacities: since actions are re- Then the learning protocol follows.
injected on inputs, the resulting recurrent network is quite
similar to a NARX neural network [Lin et al., 1996], with- 3.1 Hidden Neuron
out learning considerations through the recurrence (no back-
propagation through time). For example, if the agent is given If, on a sequence e, a new neuron must be created (when spec-
sequences like /abABABAB. . . /, it then learns to say a af- iﬁed in section 3.4), the new neuron j is connected to the in-
ter any /ab/, and b after any /ba/. Then, once it hears /ab/, puts that were active in the short past with a corresponding
it says a. It has therefore heard /aba/,andthensaysb,has delay d.Itssetwj of input connections is then:
heard /abab/, says a, and so on. It then produces an inﬁnite = { d |∀ −     ≤    ∃! d   =  ( )   =   − }
behavior, but learning occurs only in the TDNN.       wj    wji   t, T τ<t    T,   wji,i   e t ,d  T  t .
  Instead of direct supervision of outputs, we use instant re- The sequence used to create j is called its sequence of ref-
           [                   ]
inforcements Kaelbling et al., 1996 on one single output. erence, rj .
This is more general for an autonomous agent like a robot
                                                        Let θj =1/|wj|, which is not modiﬁable, and then set the
and may be used not only for imitation. While imitating, act-          ∀ d ∈      d =
ing like the teacher is innerly rewarding. Actions are selected initial weight value wji wj ,wji θj, which ensures that
                                                            d =1
among the inputs. Thus, at each time step, to predict the next i,d wji . The output weight wkj toward the reinforce-
action/input, each input is tested and the one that predicts the ment output is set to 1. 
                                                                                        d  d
best reinforcement for the next time step is chosen.    Let σj be the weighted sum σj = wjixi . The activation

                                                IJCAI-07
                                                  1006            function yj is:                                       3.3  Action Selection
                ⎧
                ⎨  0          if σj ≤ 1 − θj     (inactive neuron) The action selection scheme is basically the same as in sec-
                    σj −(1−θj )                                   tion 2. In passive mode, a neuron is a ﬁlter that recognizes a
            yj =              if 1 − θj <σj < 1  (active neuron)
                ⎩   1−(1−θj )                                     sequence and predicts a reinforcement value. In active mode,
                   1               ≥ 1
                              if σj              (active neuron)  the agent predicts an action (so that at the next time step the
              Note that 1 − θj is the activation threshold. Thus, if rj sequence is recognized).
            is presented again to the agent, j recognizes it and yj =1. Since neurons only consider events that are present, i.e.,
            Before optimization, only rj can activate j. If at least one active inputs, the main drawback of the Presence Network is
            event is substituted for another one, yj =0.          that inactive inputs are not taken into account, even if this in-
                                                                  activity is important. Said in another way, the network cannot
            3.2  Optimization of a Hidden Neuron                  distinguish exception cases from general cases. For example,
            To optimize the neuron j, each w can be seen as either infor- consider the sequences /abcZ/, /adcZ/, /aecZ/, etc. Suppose
            mative (close to θ) or not (close to 0). There is no negative now that there is also the exception case /aycX/. If one neuron
            weight. If the neuron should be more activated, “quanta” of embodies this case, then after /ayc/, it naturally predicts X .
            weights are moved from inactive connections (that transmit- But at the same time, the neuron representing the general case
            ted no activation) to active ones. The converse is done if the /a*cZ/predictsZ. Then how to select the right action? The
            neuron should not be active.                          neuron /a*cZ/ should then have an inhibitory link with input
                                                                  y, but to preserve the interesting properties of the Presence
                                                                  Network, i.e., not to connect inactive inputs (in potentially
             Algorithm 1: Update rule for input weights of a hidden great number) to a new neuron, precedence in action selec-
             neuron j. α is the learning rate. Er is the error received tion is automatically given to cases that are the most speciﬁc:
             from Algorithm 3.                                    more speciﬁc neurons take more events into account, and thus
              δ = α.θj.Er        // the maximum modiﬁcation of w  “explain” more accurately the given sequence.
                 d                                                                                              d
              ∀wji ∈ wj:                                            First, the usefulness of a connection is deﬁned: U(wji)=
                  d  0          0     d =0           0            min(1   d   )
              if (xi > and Er >  )or(xi     and Er <  ) then           ,wji/θj . Speciﬁcity ψj means how well the neuron j
                 Δd =       1 −  d                                                                     d     d
                   i  min(δ,    wji)                              explains the current sequence: ψj = d xi .U(wji).Then
                    d           d
              else Δi = −min(δ, wji)                              it is preferred to select the most speciﬁc neuron, among the
                                                                 activated ones (see Algorithm 2).
               + =             |Δd|
              S       i∈{i | xd>0} i                                The speciﬁcity of the neuron that recognizes /a*cZ/is
                          i
               −                  d                               ψ  =3, because the middle weight has become 0; and the
              S  =    i∈{i | xd=0} |Δi |
                           i                                      speciﬁcity of the neuron that recognizes /aycX/, which has
              S = min(S+,S−)                                      not been optimized, is ψ =4. This means that after the se-
              ∀  d ∈        =0                                   quence /ayc/, the agent will always choose X preferentially.
               wji  wj: if S    then                              A neuron representing an exception case similar to a general
                    d   0      d =  +      d =  −
                 if xi > then Si   S  else Si S                   case has more non-zero weights, which ensures it to be se-
                  d ←    d +Δd   S
                 wji   wji    i . Sd                              lected in priority.
                                 i
                                                                    A interesting property is that the general case needs no
                                                                 modiﬁcation at all when learning one, or more speciﬁc cases.
                                               d
              This learning algorithm ensures i,d wji =1, i.e., the se-
            quence or reference rj can always be recognized by j.All Algorithm 2: Action selection.
            weights also stay in [0, 1]: since corresponding events were
            present when the neuron was created, it would not be logical // Find event that activates each j most at next time step:
            that the weights become inhibitory. A neuron is thus a ﬁl- ∀j, ej = argmaxe(T +1) yj(T +1)
            ter that is active when it recognizes a conjunction of events Compute corresponding speciﬁcity ψj(T +1)
            appearing at different time steps, which describes a poten- // Keep only active and most specialized neurons:
            tially general sequence of events, and where weights of non- A = {j | yj(T +1) > 0,ψj(T +1) = maxh ψh(T +1)}
            informative time steps are pushed toward zero.          // Select p, the most specialized and active neuron:
              The output weight of a chosen neuron j is updated by a p = argmaxj∈A yj(T +1)
            simple delta rule and is bounded in [0, 1]: wkj ← wkj +ηEr, Select action a(T +1)=ep if p exists, φ otherwise
            where η is a learning rate, but its value is not critical. In fact,
            the output weight has not much inﬂuence on the concept em-
            bodied by the hidden neuron and may be used only to “dis-
            card” a neuron/concept if statistically found wrong.  3.4  Learning Protocol
              For example, a single neuron can represent the sequence Catastrophic forgetting, e.g. with backpropagation, happens
            /abcZ/, meaning that if the agent receives the sequence /abc/, partly because knowledge is shared between many neurons,
            this neuron proposes to answer z. After some examples such and many neurons can be modiﬁed at the same time. To
            as /adcZ/, /aecZ/, this neuron can get generalized to /a*cZ/, avoid this, here at most 3 neurons can be modiﬁed at the same
            where * means any possible symbol.                    time. First, in order to compare two neurons on the present

                                                            IJCAI-07
                                                              1007sequence, when a neuron is not active (yj =0) preactivation Now since a single event can be referred to by several con-
is deﬁned1: y˜j = σj/(1 − θj).                        nections, the deﬁnition of speciﬁcity must be modiﬁed so that
      ˜                                               at most one event by delay d is taken into account. For exam-
  Let N be the most preactivated neuron, which must have a                 =3
connection with d =0pointing to e(T ), in order to be able ple, on /abc/, initially ψ .On/aba/, there are 3 normal
                                                      connections plus one TDIC, thus ψ =4, but should logically
to predict it:                                                                  d    d
˜                                    d                be 3. Thus: ψj = d maxi (xi .U(wji)).
N = argmaxj  {y˜j | yj =0, y˜j > 0 and ∃wji,d=0,
                      ˜                                 The resulting system can represent almost Prolog logic
i = e(T )}. If none exists, N = φ.                    programs [Muggleton, 1999] without lists, but in an on-line
  With an accurate selection mechanism of the nearest neu- way, and with no explicit function name and parameter. The
ron, a high learning rate can be used. Of course, the nearest TDICs bind variables (through time) and allow comparison
neuron may not always be the correct one to optimize, but it between variables (here variables are events, or time steps), a
then will probably be optimized again in the other way. typical ﬁrst order logic capacity.
  Algorithm 3 describes the learning protocol, to decide
when neurons must be created or modiﬁed. A neuron can be 4.1 A Simple Task: Equality
added at the same time another one is modiﬁed, because the To give an example of how the network works, its develop-
latter may be wrongly optimized. If a neuron is already ac- ment is shown when trained on a simple equality task, where
tive, it means it has recognized the sequence, can thus predict the agent must answer True or False whether two given events
the right answer, and no neuron needs to be added.    are identical or not. It is a static task in essence. Examples of
                                                      sequences: /ab F/, /dd T/, /ph F/, /cc T/, /vv T/, etc.
 Algorithm 3: Learning protocol.                        The network has 26 inputs and τ =3(or higher). Initially
                                                      there is no hidden neuron.
  Time T : Predict teacher action: a(T +1)
    ←    +1                                             The ﬁrst sequence /ab F/ is provided to the agent. During
  T    T                                              the 2 ﬁrst time steps, nothing happens, except that the net-
  if ( )=   ( ) then         // correct action selected
    a T    e T                                        work puts these symbols in its τ-memory. On symbol F,the
     Optimize selected neuron p with error
                                                      network should have predicted F, but did not, because there is
     Er =(1−   wkp.yp)
                                                      no neuron. Therefore, a new neuron N1 is created, in order to
  else
     Create new neuron on current speciﬁc case        make it correlate F with /ab/. It is connected to input a with
                                                      a delay d =2,tob with a delay d =1,andtof with d =0.
     if ∃h, yh > 0, wkh.yh = maxj wkj .yj then
                                                      Input weights are set to 1 | N | =1 3. If the sequence /ab/
        // h should have been selected, not p                              / w  1    /
                                                      is presented again to the agent, the network predicts F,then
        Optimize p if exists, Er =(0− wkp.yp)
                                                      the teacher says F, N1 is activated, expecting a reward, effec-
        Optimize h, Er =(1− wkh.yh)
                                                      tively receives a reward, and no modiﬁcation is needed.
     else
                 ˜           =1                         The sequence /dd T/ is then provided to the agent. Once
        Optimize N if exists, Er                      again, there is no active or preactive neuron that could predict
                                                      T,soanewneuronN2     is added. N2 has 4 connections: 3
                                                      are similar to the previous case, and 1 is a TDIC. The latter
                                                      is created because of the repetition of d, with delays d =1
4  Time Delayed Identity Connections                  and D =2.  N2  can now answer correctly for /dd T/. Note
Time Delayed Identity Connections (TDIC) allows to know that the TDIC is not necessary to learn this case by heart; it is
when there has been a repetition of an event in the short term. useful only if N2 is optimized.
In [Orseau, 2005b], they are presented in the form of short The sequence /ph F/ispresented.N2’s TDIC is now point-
term memory special inputs. Here, they are dynamic connec- ing to p, but does not transmit an activation. Again, no neuron
tions, but it is the same. They enhance the network gener- predicted F.SoanewneuronN3 is added to answer correctly
alization capacity, and allow both to compare two events in to this sequence. But now N1 is preactivated, so it is chosen to
passive mode, and to repeat a past event in active mode. be slightly modiﬁed to predict F. The weight of its active con-
  When creating a neuron j at present time T with a sequence nections (those that transmitted an activation) are increased,
of events e,iftwoeventse(t1) and e(t2) are identical such whereas the other input weights are decreased. Thus, input
                                  dD
that τ>T−  t1 >T−  t2, a connection wji is added (before connections from a and b of N1 are considered as noise.
setting θj) to the neuron with the following properties. In On the sequence /cc/, N1 predicts F because it would
addition to the normal delay d = T − t2, it has another one slightly activate N1, although there is a repetition. But the
D  = T − t1, pointing to the event to compare with. This teacher says T instead. No neuron is activated, so a new neu-
connection is dynamic: at each time step T , the target input ron N4 is added, and N1 is not modiﬁed. And N2,being
     dD           2
i of wji is changed such that i = e(T − D). Then the  the most preactivated neuron, is modiﬁed: in particular, the
connection behaves exactly as a normal one of delay d.Note weight of the TDIC is increased because it is pointing to c.
that the special case D = d is a normal connection.     On the sequence /vv/, N1 predicts F and N2 predicts T.
                                                      Since ψN1 =1(approximately) and ψN2 =2,  N2 explains
  1
   (1−θj )=0never happens: the neuron has a single connection more speciﬁcally this sequence and thus the network predicts
and then either yj =1or i = e(T ).                   T. This is correct, so no neuron is added, and N2 is optimized
  2If e(T − D) does not exists, i can be an idle input unit. again to better predict T (with a higher value).

                                                IJCAI-07
                                                  1008  Now  the network predicts correctly any sequence like in group f. The agent must ﬁrst learn by heart in which group
/γγ T/, recognized by N2,and/γβ F/, recognized by N1, is each letter. Then, given a letter and a group, it must learn
where γ and β are any two letters.                    and generalize to answer yes if the letter is in the group, and
                                                      nootherwise.
5  Experiments                                          This task nicely shows how intermediate computations to
                                                      re-use background knowledge is necessary to generalize.
The present work is close to the continual learning framework
                                                        On the ﬁrst task, the agent is given the 26 sequences of
[         ]
Ring, 1997 where the agent must re-use previously acquired the type /gpaA/, /gpzF/, etc. Here, gp is the name of the
knowledge in order to perform better on future tasks. Here, task, the context. Learning stops when the whole training set
we are interested in tasks where the re-use of knowledge is
                                                      is correctly processed. After learning, neurons are frozen to
not only useful, but necessary to generalize, and is called as a facilitate learning on the second task.
function. Thus, even for static tasks, using a temporal frame-
                                                        On the second task, the agent is given sequences like
work can give access to these generalization capacities.
                                                      /is fb GPFB Y/(“isf in the group b?Thegroup of f is b,
  In order to show the usefulness of context and how the
agent can make automatic function calls, in section 5.1 we so yes”), /is bc GPBA N/, etc. The context gp is used to
use the task described in [Orseau, 2005b], where it is tested call the appropriate function. Also, without this context in
with a TDNN with backpropagation and “TDIC” inputs.   the ﬁrst task, there would be conﬂicts between the two tasks:
  The previous tasks can be seen as static tasks. In section 5.2 neurons would be too general and would give answers most of
is given a typical dynamic one: in the domain of languages, the time when not needed. The context also allows to choose
a sequence of xnyn is a sequence of a ﬁnite number of x the nearest neuron more accurately. After a sequence /is bc/,
followed by a sequence of the same amount of y.       the agent makes intermediate computations by saying GPB.
  Inputs are the 26 letters of the alphabet, α =1and η =0.5. Thus, since it has now heard GPB, it is auto-stimulated to
τ is set to 20, but shorter sequences may create less connec- answer A, given its knowledge of the ﬁrst task. The TDICs
tions with the Presence Network (τ should be set to a sufﬁ- allows to repeat the letter b given by the teacher to put it auto-
ciently high value so as to take enough events into account). matically after GP. The agent has learned to call the function
Learning stops after a given number of correctly predicted se- gp. But there is no speciﬁc symbol to tell what is the function
quences. This number is not taken into account in the table. or the parameter. The function call is only a consequence of
Since learning in the Presence Network is deterministic, the how knowledge is represented and used. Then the TDICs are
order of the training sequences is randomized. During test- used to compare the group given by the teacher and the one
ing, the agent is on its own, so its predicted actions take the the agent knows.
place of the teacher’s events. 10 successive trials (learning During training, only the 10 ﬁrst letters and the 4 ﬁrst
and testing) are done for each task (see results in Table 1). group letters are used. Learning stops after 20 successive cor-
                                                      rectly processed sequences. With the TDNN used in [Orseau,
                                                      2005b] it is necessary to have 5 more letters, generalization
Table 1: Average results for 10 successive trials. Tr (Ts) is still induces some errors, and more importantly it is necessary
the number of training (testing) sequences; ETr (ETs) is the to force the agent not to say anything during 4 time steps after
number of sequences on which the agent made errors during the end of the sequences of the ﬁrst task in order to prevent
training (testing); #N is the number of neurons created only these neurons from disturbing learning on the second task.
for the task being learned; #W is the total number of weights None of these problems arise with the Presence Network.
for the #N neurons; the * is for tasks trained with a normal Furthermore, the results (see Table 1) show that learning is
TDNN.                                                 much faster, which is important for on-line learning, general-
 Task        Tr   ETr   Ts   ETs     #N      #W       ization is perfect to any letter and any group.
 gp*     41 600  N.A.  100     0      8     968
 gp        97.5  27.3  100     0    27.3   139.8      5.2  Counting
 is gp*   9 300  N.A.  100     2      5     605       The agent has no special internal feature that can generalize a
 is gp     15.8   7.6  100     0    14.9   140.5      counter by itself, so it must acquire knowledge about numbers
 ﬂw       219.3  81.9  675     0    92.7    887       beforehand. First, the agent needs to learn to increment any 2-
 xnyn      13.2   6.4   10     0   114.2  3082.5      digit number, and then uses this incrementation as a function
                                                      to solve the task xnyn.
  The system could hardly be directly compared with other Incrementation
methods. The closest one is [Furse, 2001], which has yet In this task, given a 2-digit number the agent must answer
many important differences (section 1). There are also some its successor: which number followsaa?ab.Examplesof
similarities with Inductive Logic Programming [Muggleton, sequences: /ﬂw aa AB/, /ﬂw ab AC/,.../ﬂw az BA/, /ﬂw ba
1999], but the aims are different. ILP is not on-line, has ex- BB/,.../ﬂw zy ZZ/.
plicit function names and uses global search mechanisms. There are 25 general cases to learn (/ﬂw γa γb/, /ﬂw γb
                                                      γc/, .../ﬂw γy γz/), and 25 speciﬁc cases (/ﬂw az ba/, /ﬂw
5.1  Groups                                           bz ca/,.../ﬂw yz za/), for a total of 675 cases.
The alphabet is decomposed into 6 groups of consecutive let- The training set contains all the 25 speciﬁc cases and the
ters: a to e are in group a, f to j are in group b, ...and z is 150 ﬁrst cases, in which sequences are then chosen randomly

                                                IJCAI-07
                                                  1009