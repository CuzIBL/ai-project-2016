                                   r-grams: Relational Grams

                                 Niels Landwehr     and Luc De Raedt
                       Machine Learning Lab, Department of Computer Science
                                  Albert-Ludwigs-University Freiburg
                          Georges-K¨ohler-Allee 79, 79110 Freiburg, Germany
                            {landwehr,deraedt}@informatik.uni-freiburg.de

                    Abstract                          tuples or atoms such as that indicated in Example 1. The Re-
                                                      lational Markov Model (RMM) approach by [Anderson et al.,
    We introduce relational grams (r-grams). They up- 2002] extends traditional Markov models to this domain, and
    grade n-grams for modeling relational sequences of the Logical Hidden Markov Models (LOHMMs) by [Kerst-
    atoms. As n-grams, r-grams are based on smoothed  ing et al., 2006] upgrade traditional HMMs towards relational
    n-th order Markov chains. Smoothed distributions  sequences and also allow for logical variables and uniﬁca-
    can be obtained by decreasing the order of the    tion. Motivated by these models and the success and simplic-
    Markov chain as well as by relational generaliza- ity of n-gram models, we introduce relational grams, which
    tion of the r-gram. To avoid sampling object iden- upgrade the traditional n-grams towards relational sequences.
    tiﬁers in sequences, r-grams are generative models There are two distinguished features of the r-grams. First, as
    at the level of variablized sequences with local ob- LOHMMs and RMMs they allow one to work with general-
    ject identity constraints. These sequences deﬁne  ized, i.e. abstract atoms in the sequences. For instance, in the
    equivalence classes of ground sequences, in which bioinformatics example the atom helix(right,alpha,short)
    elements are identical up to local identiﬁer renam- can be generalized to helix(X,alpha,Y) to describe an alpha-
    ing. The proposed technique is evaluated in several helix of any orientation and length. Secondly, the use of
    domains, including mobile phone communication     variables and uniﬁcation allows one to make abstraction of
    logs, Unix shell user modeling, and protein fold  object identiﬁers and to share information between events.
    prediction based on secondary protein structure.  For example, the (abstract) sub-sequence outcall(X,fail),
                                                      outtxt(X) describes that a user, after failing to reach a per-
1  Introduction                                       son, writes a text message to thesameperson, without stating
Probabilistic sequence models occupy an important position the identity of the person. This is especially important when
within the ﬁeld of machine learning. They are not only generalizing the patterns across phones or users, as the ob-
theoretically appealing but have also proven to provide ef- jects referred to will typically be different, and the precise
fective learning algorithms across many application areas, identiﬁers do not matter but the relationships and events they
ranging from natural language processing to bioinformatics occur in do.
and robotics. In traditional sequence models, sequences are The paper is structured as follows: in Section 2, we intro-
                                                      duce relational sequences and deﬁne the notion of a genera-
strings s = w1 ...wk over a (ﬁnite) alphabet Σ. Typically,
the goal is to estimate the joint distribution P(s) over all pos- tive model that takes into account the nature of identiﬁers and
sible strings. Given P(s) several tasks can be solved, includ- object identity; in Section 3, we then start from n-grams to
ing sequence classiﬁcation, sampling, or predicting the next derive r-grams; in Section 4, we report on some experiments,
event in a sequence given a history of preceding events. and ﬁnally, in Section 5, we conclude and touch upon related
  Recent technological advances and progress in artiﬁcial work.
intelligence has led to the generation of structured data se-
quences. One example is that of the smart phone, where 2  Relational Sequences
communication events of many phone users have been logged An atom p(t1,...,tn) is a relation p of arity n that is fol-
during extended periods of time [Raento et al., 2006].Other lowed by n terms ti. We will work with three kinds of terms:
ones are concerned with logging the activities and locations constants (in slanted font), identiﬁers (in italic)andvari-
of persons [Liao et al., 2005], or visits to websites [Ander- ables (starting with an upper case character). Furthermore,
son et al., 2002]. Finally, in bioinformatics, sequences of- the relations are typed, i.e. for each argument position i
ten contain also structural information [Durbin et al., 1998]. of the relation p, one can either have constants or identi-
These developments together with the interest in statistical ﬁers. Variables may appear at any position. We shall also
relational learning (see [De Raedt and Kersting, 2003] for distinguish constant-variables from identiﬁer-variables; the
an overview) have motivated the development of probabilis- former can be instantiated to constants, the latter to identi-
tic models for relational sequences. These are sequences of ﬁers. They will be written in italic or slanted font respec-

                                                IJCAI-07
                                                   907Example 1. The following sequences over atoms are examples for relational sequences from different domains:
  outcall(116513 ,fail), outcall(116513 ,fail), incall(116513 ,succ), outtxt(213446 ), intxt(213446 ),...
  mkdir(rgrams), ls(rgrams), emacs(rgrams .tex), latex(rgrams .tex),...
  strand(sa,plus,short), helix(right,alpha,medium), strand(blb,plus,short), helix(right,f3to10,short),. . .
The ﬁrst domain describes incoming and outgoing calls and text messages for a mobile phone user. In the second domain, Unix
shell commands executed by a user are described in terms of command name and arguments. In the third domain, helices and
strands as protein secondary structure elements are deﬁned in terms of their orientation, type, and length.

tively. For instance, the relation outcall has identiﬁers at Given
the ﬁrst position, and constants at the second one. Therefore,
                                                        • a relational alphabet Σ
outcall(111365 ,fail) is type-conform. The types, constants,
identiﬁers, variables and relations together specify the alpha- • asetS of ground sequences over Σ
bet Σ, i.e. the set of type-conform atoms. Σ¯ ⊆ Σ is the set of
                                                        • n ∈ N
atoms that do not contain identiﬁers. A relational sequence is
                            ∗                           •         Λ                                Σ
then a string s = w1,...,wm in Σ . An expression is ground a family of generative models of order n over ,
if it does not contain any variable. Example sequences will Find a model λ ∈ Λ that maximizes
typically be ground, cf. Example 1.                                            
  Notice that constants typically serve as attributes describ-      P (S | λ):=   P ([s] | λ)
ing properties of the relations, and identiﬁers identify particu-              s∈S
lar objects in the domain. Identiﬁers have no special meaning
and they are only used for sharing object identity between where [s] denotes the equivalence class of s with regard to n-
events. Moreover, there is no ﬁxed vocabulary for identiﬁers congruence. A simple instance of such a family of generative
which is known a priori, rather, new identiﬁers will keep ap- models will be introduced in the next section.
pearing when applying a model to unseen data. Therefore, it
is desirable to distinguish ground sequences only up to iden- 3 r-gram Models for Relational Sequences
tiﬁer renaming, which motivates the following deﬁnition.
                                                      Markov Chains are amongst the simplest yet most success-
Deﬁnition 1 (Sequence Congruence). Two relational se-
                                                      ful approaches for sequence modeling. n-grams are based on
quences  1      ,   1       are n-congruent  if for
        p  ...pm   q ...qm                            higher-order Markov chains, and employ certain smoothing
all  ∈{1        −  }  the subsequences       +  −1,
    i    ,...,m   n                    pi ...pi n     techniques to avoid overﬁtting the sample distribution. In this
      + −1 are identical up to identiﬁer renaming. Two se-
ri ...ri n                                            section, we will brieﬂy review n-grams, and then propose a
quences 1  2 are identical up to identiﬁer renaming if there
       s ,s                                           simple extension of n-grams for relational sequences.
exist a one-to-one mapping ϕ of the identiﬁers in s1 to those
in s2 such that s1 equals s2 after replacing the identiﬁers ac- 3.1 n-grams: Smoothed Markov Chains
cording to ϕ.
                                                      n-grams deﬁne a distribution over sequences w1...w of
Example 2.  The sequences    r(x)p(a,y)r(y)p(b,x) and                                               m
                                                      length m by an order n − 1 Markov assumption:
r(z)p(a,w)r(w)p(b,u) are 3-congruent (but not 4-congruent).
                                                                           m
  m-congruent sequences are identical up to identiﬁer re-
                                                             P (w1...w )=     P (w | w − +1...w −1)
naming. For n<m, the deﬁnition takes into account a                  m            i  i n      i
                                                                           =1
limited history: two sequences are congruent if their object               i

identity patterns are locally identical. Finally we note that n- (where wi−n+1 is a shorthand for wmax(1,i−n+1)). In the
congruence deﬁnes an equivalence relation, i.e. it is reﬂexive, most basic case, the conditional probabilities are estimated
symmetric and transitive.                             from a set S of training sequences in terms of ”gram” counts:
  Let us now deﬁne a generative model for relational se-
                                                                                     (           )
quences. It should not sample actual identiﬁer values, but                         C wi−n+1 ...wi
                                                         Pn(wi | wi−n+1 ...wi−1)=                     (1)
rather equivalence classes of congruent sequences. This                           C(wi−n+1 ...wi−1)
yields the following deﬁnition:
                                                      where C(w1...wk) is the number of times w1...wk appeared
Deﬁnition 2 (Generative Model). Let Σ be a relational al-                    ∈
           (Σ)                                        as a subsequence in any s S. Note that this is indeed the
phabet. Let S  be the set of all ground sequences of length estimate maximizing the likelihood.
m over Σ, and let Sn(Σ) be the set of equivalence classes
           (Σ)                                          The gram order n deﬁnes the tradeoff between reliability of
induced on S   by n-congruence. Then a generative model probability estimates and discriminatory power of the model.
of order n over Σ deﬁnes a distribution over S (Σ).
                                      n               Rather than selecting a ﬁxed order n, performance can be in-
  Such a generative model can be learned from a set of creased by combining models of different order which are
ground sequences and the alphabet Σ. In its simplest form, discriminative but can still be estimated reliably [Manning
the learning problem can be stated as maximum likelihood and Sch¨utze, 1999]. The two most popular approaches are
estimation:                                           back-off and interpolation estimates. In this paper, we will

                                                IJCAI-07
                                                   908                                                                      ∀  :        i ∈ Σ¯ ∗
            focus on the latter approach, which deﬁnes conditional distri- 1. i l1...ln−1ln ;
            butions as linear combinations of models of different order: 2. ∀i : li contains no constant-variables;
                                   n                                      n
                                                                    3. ∀i : li is annotated with the probability values
            P (wi | wi−n+1 ...wi−1)=  αkPk(wi  | wi−k+1 ...wi−1)           n                 
                                                                         ( i |      )          d     ( i |      )=1
                                   k=1                                Pr ln  l1...ln−1 such that i=1 Pr ln l1...ln−1
                                                           (2)       ∀  =  :         i         j
                                                    n               4. i    j  l1...ln−1ln l1...ln−1ln; i.e. the heads are
            where the α1, ..., αn are suitable weights with =1 αk =1,
                                          (   |     k         )       mutually exclusive.
            and the lower-order distributions Pk wi wi−k+1 ...wi−1                                             2
            are estimated according to maximum likelihood (Equation 1). Example 4. The following is an example of an order rela-
            Several more advanced smoothing techniques have been pro- tional gram in the mobile phone domain⎫ (see Example 1).
                     [                       ]                              0.3 outtxt(X)     ⎪
            posed (cf. Manning and Sch¨utze, 1999 ), but are beyond the                       ⎪
            scope of this paper.                                          0.05  outtxt(Y )    ⎬
                                                                            0.2 outcall(X, fail) ← outcall(X, fail)
                                                                                              ⎪
            3.2  r-grams: Smoothed Relational Markov Chains                 ...               ⎭⎪
            Consider ground  relational sequences of  the form            0.05  intxt(Y )
                    ∈  (Σ)
            g1 ...gm  S   .   The key idea behind r-grams is a    It states that after not reaching a person a user is more likely
            Markov assumption                                     to write a text message to this person than to somebody else.
                                m
                                                                    We still need to show that an r-gram model R deﬁnes a dis-
                      ( 1    )=      (  |  − +1    −1)      (3)
                    P g ...gm      P  gi gi n  ...gi  .           tribution over relational sequences. We ﬁrst discuss a basic
                                 =1
                                i                                 model by analogy to an unsmoothed n-gram, before extend-
            However, deﬁning conditional probabilities at the level of ing it to a smoothed one in analogy to Equation 2.
            ground grams does not make sense in the presence of object
            identiﬁers. Thus, ground grams will be replaced by general- ABasicModel
            ized ones. Generality is deﬁned by a notion of subsumption: In the basic r-gram model, for any ground sequence g1...gn−1
                                                                                            1 ∨   ∨ d ←
                                                                  there is exactly one gram ln  ... ln   l1...ln−1 with
            Deﬁnition 3 (Subsumption). A relational sequence l1,...,lk
                                                                  l1...ln−1 θ g1...gn−1. Its body l1...ln−1 is the most speciﬁc
            subsumes another sequence k1,...,kn with substitution θ,         ¯ ∗
                                                                  sequence in Σ subsuming g1...gn−1. According to Equa-
            notation l1,...,lk θ k1,...,kn, if and only if k ≤ n
                                                                  tion 3, we start by deﬁning a probability PR(g | g1...gn−1)
            and ∀i, 1 ≤ i ≤  k : liθ = ki. A substitution is a set
                                                                  for any ground atom g given a sequence g1...gn−1 of ground
            {V1/t1,...,V/t } where the V are different variables and
                       l  l           i                           literals. Let g be a ground literal and consider the above gram
            the ti are terms such that no identiﬁer or identiﬁer variable                          ∈{1      }
                         {       }                                r subsuming g1...gn−1.Ifthereisani   , ..., d such that
            occurs twice in t1, ..., tl .                                 i
                                                                  l1...ln−1l θ g1...gn−1g it is unique and we deﬁne
              The restriction on the allowed substitutions implements the n
                                                                     ( |        ):=    ( |        ):=    ( i |      )
            object identity subsumption of [Semeraro et al., 1995].The PR g g1...gn−1 Pr g g1...gn−1  Pr ln  l1...ln−1
            notion of sequence subsumption is due to [Lee and De Raedt, Otherwise, PR(g | g1...gn−1)=0.FromPR(g | g1...gn−1),
            2003]. It can be tested in linear time.               a probability value PR(g1...gm) can be derived according to
            Example 3. Let Y,Z,U,V be identiﬁer variables.        Equation 3. Note that this is not a distribution over all ground
                    r( )p(a  )r(  )   r( )p(a  )r( )             sequences of length m, as the model does not distinguish be-
                     X     , Y  Y    θ1  w    , u u               tween n-congruent sequences. Instead, the following holds:
                    r( )p(a   )r( )   r(  )p(a   )r( )
                     X     , Y  Z    θ2  W     , U  V             Lemma 1.   Let R  be an order n r-gram over Σ, and
            but                                                   s, s ∈ S(Σ) be relational sequences with sn-congruent to
                                                                           ( )=    ( )
                    r(X )p(a, Y )r(Z )  r(W )p(a, U )r(U ).     s .ThenPR  s    PR s  .
                                                                    Let us therefore deﬁne P ([s]) :=  P  (s) for any
            with θ1 = {X/w, Y/u} and θ2 = {X/W, Y/U, Z/V }.                                R            R
                                                                  [s] ∈ Sn(Q).Furthermore,  [ ]∈S (Σ) PR([s]) = 1.There-
              We can now reﬁne equation 3 to take into account general-                     s  n
                                                                  fore,
            ized sequences. This will be realized by deﬁning
                                                                                                 Σ
                                 m                               Theorem 1. An order n r-gram over is a generative model
                                                                  over Σ.
                     P (g1...gm)=   P (li | li−n+1...li−1)
                                 i=1                              Example 5. Consider the r-gram model R with grams
                                                                               p(a   ) ∨ p(b ) ← r(  )
            where li−n+1...li θ gi−n+1...gi. This generalization ab-             , X      , X     X
            stracts from identiﬁer values and at the same time yields                     r(X ) ← p(b, X )
            smoothed probability estimates, with the degree and charac-            r(X ) ∨ r(Y ) ← p(a, X )
            teristic of the smoothing depending on the particular choice
                                                                                          r(X ) ← 
            of li−n+1...li. This is formalized in the following deﬁnition.
            Deﬁnition 4 (r-gram model). An r-gram model R of order n and uniform distributions over head literals.  is an
            over an alphabet Σ is a set of relational grams       artiﬁcial start symbol that only matches at the begin-
                                                                                                                   =
                            1       d                             ning of the sequence. The ground sequence g1...g5
                           l ∨ ... ∨ l ← l1...ln−1
                            n       n                             r(u)p(a,u)r(v)p(b,v)r(v) has probability PR(g1...g5)=1·
            where                                                 0.5 · 0.5 · 0.5 · 1=0.125.

                                                            IJCAI-07
                                                               909Smoothing r-grams                                     3.3  Building r-grams From Data
                                          ∈
In the basic model, there was exactly one gram r R sub- To learn an r-gram from a given set S of training sequences,
suming a ground subsequence g1...gn−1, namely the most we need to 1) choose the set R of relational grams; 2) es-
speciﬁc one. As for n-grams, the problem with this approach timate their corresponding conditional probabilities; and 3)
is that there is a large number of such grams and the amount
                                                      deﬁne the weights αr for every r ∈ R. Before specifying our
of training data needed to reliably estimate all of their fre- algorithm, we need to deﬁne counts in the relational setting:
quencies is prohibitive unless n is very small. For n-grams,
grams are therefore generalized by shortening their bodies, C(l1 ...lk)=|{i|s1 ...sm ∈ S and l1 ...lk θ si ...si+k}|
i.e., smoothing with k-gram estimates for k<n(Equation 2).                                            (4)
  The basic idea behind smoothing in r-grams is to generalize Our algorithm to learn r-grams is speciﬁed below:
grams logically, and mix the resulting distributions:
                                                     r-grams(input: sequences S; alphabet: Σ; parameters: γ,n)
                         αr
    P (g | g1...g −1)=      P (g | g1...g −1)                             ¯ ∗
     R         n             r        n               1  B := {l1 ...lk−1 ∈ Σ |C(l1 ...lk−1) > 0 and k ≤ n}
                        ˆ α
                     r∈R                              2  for each l1 ...lk−1 ∈ B
        (  |         )                                             { 1   d}
where Pr g   g1...gn−1 is the probability deﬁned by r 3      do let lk ...lk contain all maximally speciﬁc
                   ˆ                                                     i ∈ Σ¯         (         i ) 0
as explained above, R is the subset of grams in R sub-            literals lk such that C l1 ...lk−1lk >
                                                                       1 ∨···∨  d ←
suming g1...gn−1,andα is a normalization constant, i.e. 4addr       = lk      lk   l1 ...lk−1 to R with
                                                                                     (      i )
  =     ˆ                                                            i             C l1...lk−1lk
α     r∈R αr. The more general r, the more smooth the             Pr(l |l1 ...lk−1)= (      )
                   ( |        )                                      k              C l1...lk−1
probability estimate Pr g g1...gn−1 will be. The actual de-      ( ):=                  ( |  1   −1)
                                                      5         L r      g1...gn−1g∈S(r) Pr g g ...gn .
gree and characteristic of the smoothing is deﬁned by the set 6 t := |S(r)|
of matching r-grams together with their relative weights α .              γ
                                                r                  :=  ( ) t
  By analogy with n-grams, additional smoothing can be 7        αr    L r
obtained by also considering relational grams r ∈ R with 8 return R
shorter bodies l1...lk−1, k<n. However, there is a sub- In Line 1 of the algorithm, one computes all r-grams that
tle problem with this approach: Relational grams of order occur in the data. Notice that no identiﬁers occur in these
                                    S (Σ)                         ¯ ∗
k<ndeﬁne a probability distribution over k rather than r-grams (cf. Σ ). This can be realized using either a fre-
S (Σ)
 n   , i.e. the sequences are partitioned into a smaller num- quent relational sequence miner, such as MineSeqLog [Lee
ber of equivalence classes. However, this can be taken care and De Raedt, 2003], or using an on-the-ﬂy approach. In
of by a straightforward normalization, which distributes the
                                                      the latter case, a relational gram with body l1...lk is only
probability mass assigned to an equivalence class modulo k built and added to R when and if it is needed to evaluate
equally among all subclasses modulo .
                               n                      PR(g | g1...gn−1) with l1...lk  g1...gk on unseen data. In
                                                                               i
Example 6. Consider the r-gram model R with grams r,q Line 3, all possible literals lk are sought that occur also in
given by                                              the data. They are maximally speciﬁc, which means that they
            r :     r(X ) ∨ r(Y ) ← r(X )             do not contain constant-variables (cf. condition 2 of Deﬁni-
                                                      tion 4). Line 4 then computes the maximum likelihood esti-
            q :           r(X ) ←
                                                      mates and Lines 5–7 the weight α . Here S(r) denotes the set
                                       =     =05                                  r
uniform distribution over head literals and αr αq . . of all ground subsequences g1...g −1g appearing in the data
           ( ( ) | ( )) +  ( ( ) | ( )) = 1                                       n
We expect PR r u r v    PR  r v  r v     . However,   which are subsumed by r. The likelihood L(r) of r deﬁned
when directly mixing distributions                    in Line 5 is a measure for how well the distribution deﬁned
PR(r(u) | r(v)) = αrPr(r(Y ) | r(X ))+αqPq(r(X )) = 0.75 by r matches the sample distribution. The αr as in Line 7 is
                                                      then deﬁned in terms of |S(r)| and the parameter γ,which
PR(r(v) | r(v)) = αrPr(r(X ) | r(X ))+αqPq(r(X )) = 0.75,
                                                      controls the tradeoff between smoothness and discrimination.
as the r-gram q does not distinguish between the sequences
                                                      Highly discriminative (speciﬁc) rules have higher likelihood
r(x)r(x) and r(x)r(y). Instead, we mix by
                                     1                than more general ones as they are able to ﬁt the sample dis-
  P (r(x) | r(y)) = α P (r(X ) | r(X )) + α P (r(X )) tribution better, and thus receive more weight if γ>0.
   R               r r               γ q  q
where γ   =2is the number of subclasses modulo        4   Experiments
2-congruence of the class [r(X)].
                                                      This section reports on an empirical evaluation of the pro-
  The ultimate level of smoothing can be obtained by a rela- posed method in several real-world domains. More speciﬁ-
                     1 ∨  ∨  d ←          i
tional gram r of the form ln ... ln where the ln are fully cally, we seek to answer the following questions:
variablized (also for non-identiﬁer arguments). For this gram
                              a                      (Q1) Are r-grams competitive with other state-of-the-art ap-
         ( |        )=    ( i )    (   =   )              proaches for relational sequence classiﬁcation?
      Pr  g g1...gn−1   Pr ln    P Xj    xj
                              j=1                     (Q2) Is relational abstraction, especially of identiﬁers, use-
                                              i           ful?
where X1, ..., Xa are the non-identiﬁer arguments of ln and
x1, ..., xa their instantiations in g. This case corresponds to Experiments were carried out on real-world sequence clas-
an out-of-vocabulary event (observing an event that was not siﬁcation problems from three domains. In the Unix Shell
part of the training vocabulary) for n-grams.         domain  [Greenberg, 1988; Jacobs and Blockeel, 2003],

                                                IJCAI-07
                                                   910 Domain       r-grams   LOHMM      LOHMM + FK          Domain       r-grams    n-grams   n-grams w/o IDs
 Protein       83.3        74.0         82.7           Protein        83.3       79.7          83.3
 Domain       r-grams     kNN           C4.5           Unix-50     93.8 ± 2.7    74.4       95.6 ± 2.7
 Unix-50     93.8 ± 2.7    91.0         88.8           Unix-1000   97.2 ± 0.4    76.3       97.1 ± 0.4
 Unix-1000   97.2 ± 0.4    95.3         94.7           Phone I     95.0 ± 2.0    30.0       86.8 ± 3.3
                                                       Phone II    93.3 ± 14.9   33.3       86.7 ± 18.3
Table 1: Comparison of classiﬁcation accuracy of r-grams to
Logical Hidden Markov models and Fisher kernels in the Pro- Table 2: Accuracy comparison of r-grams to n-grams, and
tein Fold domain, and to k-nearest neighbor and C4.5 in the to n-grams w/o IDs. For Protein/Unix domains settings are
Unix Shell domain. For protein fold prediction, a single split as before. For the Phone I domain, 5 subsets of size 100
into training and test set is used. In the Unix Shell domain, 10 have been sampled from the data, a 5-fold cross-validation
subsets of 50/1000 examples each are randomly sampled, ac- is performed on each set and results are averaged. For Phone
curacy determined by 10-fold cross-validation and averaged. II, results are based on one 5-fold cross-validation. Results
                                                      for n-grams are based on one sample only.


thetaskistoclassifyusersasnovice programmers     or         1
                                                                                               r-grams
non-programmers based on logs of 3773 shell sessions con-                                   n-grams w/o IDs
taining 94537 commands (constants) and their arguments
(identiﬁers). To reproduce the setting used in [Jacobs and  0.95
Blockeel, 2003], we sampled 10 subsets of 50/1000 instances
each from the data, measured classiﬁcation accuracy on these
using 10-fold cross-validation, and averaged the results. In  0.9
the Protein fold classiﬁcation domain, the task is to classify
                                                        Accuracy
proteins as belonging to one of ﬁve folds of the SCOP hi-  0.85
erarchy [Hubbard et al., 1997]. Strand names are treated as
identiﬁers, all other ground terms as constants. This problem
has been used as a benchmark before [Kersting et al., 2006;  0.8
Kersting and G¨artner, 2004], and we reproduce the experi-
mental setting used in this earlier work: the same 200 ex-
                                                           0.75
amples per fold are used for training, and the remaining ex-  20  30   40    50   60   70   80    90   100
amples as the test set. In the Context Phone domain, data                    Number of examples
about user communication behavior has been gathered using
a software running on Nokia Smartphones that automatically Figure 1: Accuracy for different training set sizes ranging
logs communication and context data. In our study, we only from 20 to 100 examples in the Phone I domain. For each
use information about incoming and outgoing calls and text size, 5 sets are sampled, a 5-fold cross-validation is per-
messages. Phone numbers are identiﬁers, other ground terms formed and the result averaged.
constants. The task in Phone I is to discriminate between
real sequences of events and ”corrupted” ones, which con- as input in a support vector machine [Kersting and G¨artner,
tain the same sequence elements but in random order. For 2004]. The Unix Shell log classiﬁcation problem was orig-
k ∈{20, 40, 60, 80, 100}, 5 subsets of size k were sampled
         5                                            inally tackled using a k-nearest neighbor method based on
randomly, -fold cross-validation performed and averaged for customized sequence similarity [Jacobs and Blockeel, 2003].
each k.InPhone II, the task is to classify communication In the same paper, the authors present results for a C4.5 de-
logs as belonging to one of three users, based only on their cision tree learner using a bag-of-words representation. In
communication patterns but without referring to actual phone both cases, r-grams yield competitive classiﬁcation accuracy,
numbers in the event sequence.                        which is a positive answer to question (Q1).Furthermore,we
  In all domains sequence classiﬁcation is performed by note that even using a na¨ıve implementation r-grams are com-
building an r-gram model RC for each class C and la-  putationally efﬁcient. Times for building an r-gram model
beling unseen sequences s with the class that maximizes          3   240                               1
  ( )  ( )                                            ranged from to    seconds in the presented experiments .
PC s P C  . We used bigram models in the Phone II domain In a second set of experiments, the effect of using rela-
and trigram models for all other domains, and the smoothing tional abstraction was examined in more detail. More pre-
parameter γ was set to 1 in all experiments. Learning the r- cisely, r-grams were compared to n-grams which implement
gram model was done on-the-ﬂy as explained in Section 3.3. non-relational smoothing as outlined in Section 3.1, treating
  Table 1 compares the classiﬁcation accuracy of r-grams the atoms in Σ as ﬂat symbols. For these experiments, we
with accuracy results from the literature in the Protein Fold tried keeping identiﬁers in the events (n-grams)orremoving
and Unix Shell domains. In the Protein Fold domain, a hand-
                                         74%          them from the data (n-grams w/o IDs). Accuracy results for
crafted Logical Hidden Markov Model achieves  accu-   the Protein Fold, Unix Shell and Context Phone domains are
racy [Kersting et al., 2006]. This has been improved to 82.4%
by a ﬁsher kernel approach, in which the gradient of the like- 1All experiments were run on standard PC hardware with
lihood function of the Logical Hidden Markov Model is used 3.2GHz processor and 2GB of main memory.

                                                IJCAI-07
                                                   911