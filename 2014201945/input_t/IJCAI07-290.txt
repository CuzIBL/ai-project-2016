              Learning Question Paraphrases for QA from Encarta Logs

                              Shiqi Zhao1, Ming Zhou2, Ting Liu1
                 1 Information Retrieval Laboratory, Harbin Institute of Technology 
                      No. 27 Jiaohua Street, Nangang, Harbin, China, 150006 
                                    {zhaosq, tliu}@ir-lab.org 
                                   2 Microsoft Research Asia 
                       No. 49 Zhichun Road, Haidian, Beijing, China, 100080 
                                   mingzhou@microsoft.com
                   Abstract                      tion types and focuses. These are all indicative features 
                                                 when identifying question paraphrases. Thus, a method spe-
    Question paraphrasing is critical in many Natural cially designed for question paraphrasing is worthy of study. 
    Language Processing (NLP) applications, espe-
                                                   Question reformulation in QA has been widely researched. 
    cially for question reformulation in question an- Some researchers have written reformulation templates 
    swering (QA). However, choosing an appropriate manually [Brill et al., 2002]. Others have expanded ques-
    data source and developing effective methods are 
                                                 tions using dictionaries such as WordNet [Hovy et al., 2001]. 
    challenging tasks. In this paper, we propose a Some researchers have used the web as a resource for ques-
    method that exploits Encarta logs to automatically tion reformulation [Yang et al., 2003]. Although employing 
    identify question paraphrases and extract templates. 
                                                 various reformulation methods, all the above researches 
    Questions from Encarta logs are partitioned into have verified the effectiveness of question reformulation. 
    small clusters, within which a perceptron classier is This paper exploits a new resource, the Encarta logs, for 
    used for identifying question paraphrases. Experi-
                                                 question paraphrasing. An automatic method is designed to 
    ments are conducted and the results have shown: (1) process the logs and identify paraphrases, including ques-
    Encarta log data is an eligible data source for ques- tion classification, question partition, and paraphrase identi-
    tion paraphrasing and the user clicks in the data are 
                                                 fication using a classifier. In recognizing paraphrases, some 
    indicative clues for recognizing paraphrases; (2) novel features are presented. Especially, user click informa-
    the supervised method we present is effective, tion is used, which proves effective in experiments. 
    which can evidently outperform the unsupervised Templates are extracted from the derived question para-
    method. Besides, the features introduced to identify phrases and applied in question reformulation. Experiments 
    paraphrases are sound; (3) the obtained question show that the templates achieve wide coverage when tested 
    paraphrase templates are quite effective in question on a TREC-QA question corpus, demonstrating that the En-
    reformulation, enhancing the MRR from 0.2761 to carta log data is a good resource to learn question para-
    0.4939 with the questions of TREC QA 2003.   phrase templates. Experiments also show that the extracted 
                                                 templates are quite effective in question reformulation. 
1 Introduction                                     The remainder of the paper is organized in this way: Sec-
Paraphrases are alternative ways of conveying the same tion 2 introduces related work. Our method of question 
information. In recent years, there has been growing re- paraphrasing is presented in Section 3. Experiments and 
search interest on paraphrasing since it is important in many results are described in Section 4. Section 5 is the conclu-
NLP applications, including multi-document summarization, sion and discusses future work. 
QA, text generation, and machine translation. 
  Question paraphrases, as a sub-class of paraphrases, are 2 Related Work 
formally distinct questions that actually mean the same Various resources have been employed for paraphrase ex-
thing and have the same answer. Question paraphrasing is traction. One resource is parallel monolingual corpus, such 
crucial in the question reformulation phase of a QA system. 
                                                 as multiple translations of literary works [Barzilay and 
If an input question can be expanded with its various para- McKeown, 2001]. While the translation-based methods fa-
phrases, the recall of answers can be improved.  cilitate the identification of paraphrases, such corpora are of 
  Compared with declarative sentences, questions contain 
                                                 limited availability since multiple translations on a large 
some additional information, such as question words, ques- scale are not readily available in non-literary domains. 
                                                   Other researchers exploit nonparallel monolingual cor-
     This work was finished while the first author was visiting pora. Lin and Pantel (2001) discovered paraphrases by 
   Microsoft Research Asia as a member of the project of AskBill parsing a large unlabeled monolingual corpus and extracting 
   Chatbot led by Dr. Ming Zhou.                 semantically similar paths from dependency trees. The dis-


                                            IJCAI-07
                                             1795advantage is that, only the templates with two arguments are proved a helpful feature. Encarta log data used in our ex-
considered. Another kind of nonparallel monolingual re- periments contains 4,946,932 query sessions. Though the 
source is the comparable news articles that report the same number of logs is substantial, most of them are keywords or 
events [Shinyama et al., 2002; Barzilay and Lee, 2003]. The phrases rather than well-formed questions. Therefore, we 
assumption behind it is that articles derived from different need to filter the query logs and only retain questions. Here, 
newspapers can contain paraphrases if they report the same simple heuristic rules are used: a query is recognized as a 
event on the same day [Shinyama et al., 2002]. However, question if it contains three words or more, and a question 
these methods seem to be of limited generality and difficult word (i.e. who, what, when, where, why, and how).
to be extended to other domains.                   Note that, in fact not all questions contain question words. 
  Bannard and Callison-Burch (2005) have sought to derive For example, “Name a stimulant.” is a question from 
paraphrases from parallel bilingual corpora. They equated TREC-QA which contains no question word. Currently, we 
different English phrases aligned with the same phrase in do not process this kind of questions since it is difficult to 
another language based on the assumption that phrases differentiate them from declarative sentences. Future work 
mapped onto a single foreign language phrase tend to mean may apply our method to these questions. 
the same thing. Though this is a promising method, its per- After extracting questions using the above method, a cor-
formance depends greatly on word alignments.     pus containing 127,679 questions is constructed. In what 
  None of the resources above are suitable in question para- follows, this corpus is called “question corpus”. 
phrasing because of domain limitation as well as the 
sparseness of question sentences. In contrast, Encarta logs 3.2 Question Type Classification 
are not domain limited, the queries in which can be about In principle, any pair of questions in the question corpus 
any topic. In addition, a sizable corpus of questions can be should be considered when identifying paraphrases. How-
easily constructed from Encarta logs.            ever, the corpus contains over 120,000 questions, it is infea-
  There is very limited work reported on question para- sible to identify paraphrases for each pair of questions. 
phrasing. Tomuro (2003) employed an FAQ corpus and Therefore, a two-step process, involving question type clas-
defined patterns manually for question paraphrasing. Com- sification (described in this section) and question partition 
pared with FAQ corpora, Encarta logs supply additional (in Section 3.3), is employed to divide the whole corpus into 
information, i.e. the user click information, which proves a thousands of small clusters and the identification of para-
good indicator of paraphrases in our experiments (described phrases is performed within each cluster. 
in Section 4.1.3). Another difference from Tomuro’s The question type is an important attribute of a question, 
method is that our method identifies question paraphrases which usually indicates the category of its answer. In QA, 
and extracts templates automatically.            question type classification is a necessary preprocessing 
                                                 stage. Table 1 shows a widely accepted question type tax-
3 Our Approach                                   onomy in QA [Li and Roth, 2002]. 
The method comprises five steps: (1) extracts questions 
                                                  abbreviation, explanation
from Encarta logs; (2) classifies the extracted questions ac-
                                                  animal, body, color, creative, currency, disease, event, food, 
cording to question types; (3) partitions the classified ques-
                                                  instrument, language, letter, other, plant, product, religion, sport, 
tions into fine-grained clusters; (4) identifies paraphrases substance, symbol, technique, term, vehicle, word
from all question pairs within each cluster; (5) extracts tem- definition, description, manner, reason
plates from the identified paraphrases.           group, individual, title, human-description
3.1  Question Extraction                          city, country, mountain, other, state
                                                  code, count, date, distance, money, order, other, period, percent, 
Encarta is an online encyclopedia (http://encarta.msn.com). speed, temperature, size, weight
Encarta logs are user logs containing queries and documents Table 1. Question type taxonomy 
that users clicked on for review. A small segment of Encarta 
logs is shown in Figure 1. For each line (a query session), Based on the observation that two questions with differ-
the first half is a query. The codes following the query, ent question types can hardly be paraphrases, questions in 
separated by “#”, are IDs of clicked documents.  the corpus are first classified into 50 different types (Table 
                                                 1). Our question classification method is similar to that in-
 ……                                              troduced by Metzler and Croft (2005). We also build a 
 Plant Cells: #761568511                         two-level classifier. At the first level, questions are divided 
 Malaysia: #761558542 #761558542                 into six sets. Each set corresponds to a type of question 
 rainforests: #761552810 #761552810 #761552810   word (i.e. who, what, when, where, why,and how). At the 
 what is the role of a midwife: #761565842       second level, a Support Vector Machine (SVM) classifier 
 ……                                              based on the taxonomy in Table 1 is trained for each set 
Figure 1. Encarta logs                           using the words as features. When classifying new questions, 
                                                 the process closely mimics the training steps. Given a new 
  The Encarta logs have been used for query clustering question, its question word is first identified. A feature vec-
[Wen et al., 2002], in which the user click information 


                                            IJCAI-07
                                             1796tor is then created using the same features as in the training.         | S  S |
                                                                          1   2             (2)
Finally, the SVM corresponding to the question word is     OR(S1 , S 2 )
                                                                      max(| S |,| S |)
used for classification [Metzler and Croft, 2005].                          1   2
  The reason why a two-level classifier is employed is that where S1 and S2 are two sets. | .| is the cardinality of a set. 
the question words are prior knowledge and imply a great User Click Feature (UCF): It is easy to understand that if 
deal of information about the question types. A two-level two questions often lead to the same document clicks, then 
classifier can make better use of this knowledge than a flat these two questions tend to be similar [Wen et al., 2002]. 
classifier that uses the question words simply as classifica- The feature of user click similarity of two questions is cal-
tion features. The training and testing data are the UIUC culated using Equation (3): 
corpus [Li and Roth, 2002] and the TREC-10 questions. The                 RD(q1 ,q2 )
                                                       Simuser _ click (q1 , q2 )          (3)
experimental result shows that the classifier can achieve a            max(rd(q1 ), rd(q2 ))
classification accuracy of 84.2%.                where rd(.) is the number of clicked documents for a 
                                                 question and RD(q ,q )  is the number of document clicks 
3.3  Question Partition                                         1 2
                                                 in common. 
In this stage, questions in each of the 50 classes are further WordNet Synonyms Feature (WSF): The pair of ques-
partitioned into more fine-grained clusters. In this work, the tions is expanded with the synonyms extracted from Word-
paraphrases that have no common words are not considered. Net synset entries. Specifically, a question q can be ex-
  Formally, given a content word (non-stopword) w, all panded to q’, which contains the content words in q along 
questions within each question class that contain w are put with their synonyms. Then for the expanded questions, the 
into the same cluster (we may take it that this cluster is “in- overlapping rate is calculated and selected as a feature. 
dexed” by w). Apparently, if a question contains n different Unmatched Word Feature (UWF): The above features 
content words, it will be put into n clusters. After this proc- measure the similarity of two questions while the un-
ess, the 50 classes obtained in the last step are further parti- matched word feature is designed to measure the divergence 
tioned into about 37,000 clusters.               of two questions. Given questions q1, q2 and q1’s content 
  The question partition approach will be improved in fu- word w1, if neither w1 nor its synonyms can be found in q2, 
ture, in which two questions will be put into a same cluster w1 is defined as an unmatched word of q1. We calculate the 
if at least one pair of their content words are identical or unmatched rate as in Equation (4) and use it as a feature. 
synonymous. 
                                                         UR(q1 , q2 ) max(ur(q1 ),ur(q2 ))          (4)
3.4  Question Paraphrase Identification          where ur(.) denotes the percentage of unmatched words in a 
                                                 question. 
3.4.1 Feature  Selection                         Syntactic Similarity Feature (SSF): In order to extract the 
At this step, a classifier is used to identify paraphrases syntactic similarity feature, the question pairs are parsed by 
within the clusters obtained at the last step. If a cluster has n a shallow parser whereby the key dependency relations can 
questions, n*(n-1)/2 question pairs are generated by pairing be extracted from a sentence. Four types of key dependency 
any two questions in the cluster. For each pair, the classifier relations are defined: SUB, OBJ, ATTR, and ADV. For ex-
learns whether they are paraphrases (classifier outputs 1) or ample, for the question “What is the largest country,” the 
not (classifier outputs -1).                     shallow parser will generate (What, is, SUB), (is, country, 
  There are other researchers taking paraphrase identifica- OBJ), (largest, country, ATTR) as the parsing result. As can 
tion as a problem of classification [Brockett and Dolan, be seen, the parsing result of each question is represented as 
2005]. However, different features are used. The following a set of triples, where a triple comprises two words and their 
are the features used in our work.               syntactic relation. The overlapping rate of two questions’ 
Cosine Similarity Feature (CSF): The cosine similarity of syntactic relation triples is computed and used as their syn-
two questions is calculated after stemming and removing tactic similarity. 
stopwords. Suppose q1 and q2 are two questions, Vq1 and Vq2 Question Focus Feature (QFF): The question focus can be 
are the vectors of their content words. Then the similarity of viewed as the target of a question. For example, in the ques-
q1 and q2 is calculated as in Equation (1).      tion “What is the capital of China?” the question focus is 
                             V  ,V               “capital”. Obviously, two questions are more likely to be 
                               q1 q2        (1)
        Sim(q1 ,q2 ) cos(Vq1,Vq2 )               paraphrases if they have identical question focus. Currently, 
                             V  V
                              q1  q2             the question focuses are extracted using predefined rules. 
where .,. denotes the inner product of two vectors and The QFF feature has a binary value, namely, 1 (two ques-
 .   denotes the length of a vector.             tions have identical question focus) or 0 (otherwise). 
Named Entity Overlapping Feature (NEF): Since named Translation Similarity Feature (TSF): Translation infor-
entities (e.g. person names, locations, time…) should be mation proves useful in paraphrase identification [Wu and 
preserved across paraphrases [Shinyama et al., 2002], the Zhou, 2003]. In our experiments, Google online translation 
overlapping rate of named entities in two questions is se- (http://translate.google.com/translate_t) is called to translate 
lected as a feature. The overlapping rate of two sets can be each English question into Chinese. Then the cosine simi-
computed as in Equation (2):                     larity of the translations of two questions is calculated. 


                                            IJCAI-07
                                             17973.4.2  PAUM Classifier                           of the data is used in training while 1/5 (333 positive and 
It is found in the experiments that the input data for the 13,284 negative) is left for testing. 
paraphrase identifier is rather unbalanced, in which, only a 
                                                 4.1.2  Performance of the Identifier 
very small proportion of the question pairs are paraphrases. 
The methods dealing with classification with unbalanced In this experiment, the precision and recall of the identifier 
                                                                     S
data include the Positive Example Based Learning (PEBL), are computed. Given that cp is the set of paraphrases auto-
                                                 matically recognized by the identifier; S  is the set of 
one-class SVMs and Perceptron Algorithm with Uneven                              mp
Margins (PAUM). Among these methods we use PAUM in paraphrases manually annotated. Then precision and recall 
our experiments [Li et al., 2002]. PAUM is an extension of are defined as in Equations (6) and (7): 
                                                           precision | S S | | S |            (6)
the perceptron algorithm, which is specially designed to             cp  mp   cp
                                                                                            (7)
cope with two class problems where positive examples are           recall | Scp Smp | | Smp |            
very rare compared with negative ones, as is the case in the The classification margins in a PAUM classifier can be 
paraphrase identification task. PAUM considers the positive adjusted so as to get different trade-offs between precision 
and negative margins separately. The positive (negative) and recall. In most applications of the paraphrase templates, 
margin   1(w,b, z) is defined as:                precision is more important than recall. For instance, in 
                         ( w, x b)
                             i                   question reformulation of QA, a false expansion might do 
            1(w,b, z) min                    (5) 
                   ( x , 1) z
                    i       w                    more harm than good, since it may bring about noise and 
                                 m               lead to incorrect answers. Therefore, when setting the clas-
where z  ((x1, y1),...,(xm , ym )) ( { 1, 1})  is a training 
sample.:        R n is a feature mapping into an sification margin parameters, many different combinations 
                                                 have been tried on a small development set and ultimately 
n-dimension vector space . xi (xi ) . w ,b R  are 
parameters. .,.  denotes the inner product in    we set positive margin parameter 1 6  and  negative 

                                                 margin parameter 1 1 , which are deliberately skewed 
3.5 Template Extraction                          towards precision. Experimental results show that the preci-
Templates are extracted from the derived question para- sion and recall are 77.60% and 71.77%, respectively. 
phrases. There is some research work on paraphrase tem- 4.1.3 Feature Contributions 
plate generation [Barzilay and Lee, 2003]. In our work, we To investigate the contributions of different features, we 
use a simple method to extract templates. A better method omitted each feature from several runs. The results are 
will be presented in future.                     shown in Table 2. 
  As mentioned above, paraphrases are identified from each  
cluster in which a common content word w is shared by all        Precision(%) Recall(%) 
questions. Hence, the paraphrase templates are formalized All Features 77.60 71.77 
by simply replacing the index word w with wildcard “*”. No CSF      75.47 72.07 
For example, the questions “What is the length of Nile?” and No NEF 76.68 72.07 
“How long is Nile?” are recognized as paraphrases from the No UCF   74.76 71.17 
cluster indexed by “Nile.” Then the paraphrase template No WSF      77.10 71.77 
“What is the length of *  How long is *” is induced by No UWF       78.44 63.36 
replacing “Nile” with “*.”                           No SSF         75.40 70.87 
                                                    No QFF          74.70 73.57 
4   Evaluation                                       No TSF         75.00 72.97 
                                                 Table 2. Effect of eliminating each feature 
To evaluate the effectiveness of our method, three experi-  
ments are carried out. The first one is designed to evaluate Table 2 shows that eliminating the features CSF, UCF, SSF, 
the paraphrase identifier, especially feature selection. The QFF, and TSF can all produce a notable degradation in pre-
second experiment evaluates the performance of the whole cision while eliminating the UWF feature can have large 
paraphrase acquisition process, which includes question impact on recall. Of all the features, removal of the feature 
type classification, question partition, and paraphrase identi- WSF appears to have the least impact. The reason may be 
fication. The third is designed to verify the usefulness of the that the WordNet synonyms used in WSF are also used in 
generated templates in question reformulation.   the feature UWF, which makes the WSF feature redundant. 
4.1  Evaluation of Paraphrase Identifier         Besides, the effect of NEF feature is also small. In our fu-
                                                 ture work, the NE information will be used in preprocessing. 
4.1.1  Data                                      Specifically, only the question pairs with identical NEs are 
As mentioned in Section 3.4.2, the paraphrase identifier is a retained and identified by the paraphrase identifier. 
PAUM classifier. In order to train and test the classifier, we In particular, we can see that feature UCF improves the 
extracted 67,379 question pairs from the question corpus precision evidently, which indicates that the user click in-
and annotated them manually. The resulting data is ex- formation is an effective constraint in paraphrase identifica-
tremely unbalanced, which contains 1,629 positive (para- tion. Figure 2 (a) shows an example of non-paraphrases that 
phrases) and 65,750 negative (non-paraphrases) examples. only can be identified correctly when the UCF feature is 
In the experiment, 4/5 (1,296 positive and 52,466 negative) considered. As can be seen, the two questions are differenti-


                                            IJCAI-07
                                             1798ated by their user clicks though they are highly similar at phrases (which overlap with the 305 hand-tagged ones). 
string level. Figure 2 (b) shows an example of paraphrases Precision and recall are 66.88% and 36.15%.  
that only can be recognized when the UCF feature is used. Compared with the result shown in Table 2, precision de-
Evidently, it is the identical user click that makes it possible creases from 77.60% to 66.88% and recall decreases from 
to identify these two formally distinct questions. However, 71.77% to 36.15%. There are two main reasons for the no-
it can be seen from Table 2 that the UCF feature only makes table decreases. One is that the question classification and 
a slight improvement in recall. We conclude that the calcu- partition bring about mistakes. Especially, some paraphrase 
lation of the user click similarity should be improved so as pairs are divided into different classes or clusters; the other 
to enhance the recall. Since documents in Encarta are or- reason is the irregularity of the Encarta logs. There are many 
ganized into a hierarchy that contains four levels, the hier- spelling mistakes in the logs. (e.g. “Egyptian” was written 
archy of logs will be taken into account in our future work. as “Egyption”) These mistakes influence the calculation of 
                                                 similarity and the performance of paraphrase recognition. 
 (a) Where can I find information on automobiles #761576902 Additionally, the questions from the Encarta logs are quite 
    Where can I find information about 1930's automobiles flexible in expression, some of which are even ungram-
    #761563934                                   matical. E.g. “Russia what do they wear?”, “Atomic bomb 
 (b) When did Florida become a state: #761557601 dropped on Hiroshima why?” and the like. For these ques-
    When did Florida join the United States: #761557601 tions, the extraction of question focuses and the recognition 
Figure 2. Examples that benefit from UCF feature of syntactic relations are extremely difficult, which makes 
                                                 the SSF feature and the QFF feature fail to work. 
4.1.4  Comparison of Feature Selection Approaches  We also compared our method with the unsupervised 
[Brockett and Dolan, 2005] used different features in their method presented by Wen et al. [2002]. In their method, 
SVM classifier to identify paraphrases from related news they clustered query logs using a density-based clustering 
sentences. Four feature classes are involved in their work, method. They combined similarity based on query contents 
including string similarity features, morphological features, and that based on user clicks in clustering. In our experi-
WordNet lexical mappings, and word association pairs. To ment, the minimal density parameter was set to 3, which 
compare our feature selection strategy with theirs, we have means that only those clusters containing at least 3 queries 
tested their features on the question corpus in our experi- were kept. Then we varied the similarity threshold from 0.5 
ments. The comparison result is in Table 3:      to 0.8. The performance is shown in Table 4. 
                                                  
               Precision Recall                   Threshold Precision         Recall 
 Ours          77.60% 71.77%                      0.5          26.67% 8.11% 
 B & D         59.35% 21.92%                      0.6          24.14% 7.09% 
Table 3. Comparison of feature selection approaches 0.7        30.36% 11.49% 
                                                  0.8          35.48% 7.43% 
  As can be seen, our feature selection dramatically out- Table 4. Performance of the clustering method 
performs that of [Brockett and Dolan, 2005] on the question  
corpus. This shows that the feature set we have designed is As can be seen, both the precision (66.88%) and recall 
more effective in indicating question paraphrases. (36.15%) of our method are much higher than the clustering 
4.2  Evaluation of the Whole Method              method, which indicates that the supervised method pre-
                                                 sented in this paper is more effective than the unsupervised 
In the last section, the paraphrase identifier is evaluated. In method in recognizing question paraphrases. 
this section, the paraphrasing method, including question 
type classification, question partition, and paraphrase identi- 4.3  Evaluation of Templates in QA 
fication, is evaluated as a whole. Question classification and To evaluate the templates in question reformulation, 380 
partition divide the large question corpus into many small factoid questions from TREC-2003 QA Track are used. Of 
clusters, which makes it feasible for the identifier to detect the 380 questions, 134 (35.26%) are matched with the ex-
paraphrases. However, they also bring about lost. Thus our tracted templates and reformulated while the left 246 ques-
main purpose is to evaluate the effect of these two stages. tions are not reformulated.  
  To evaluate the performance of paraphrase acquisition, Since we have not built a TREC QA system at this point, 
we randomly selected 660 questions from the question cor- we evaluate the reformulation templates using a web QA 
pus, from which 305 pairs have been manually annotated as method. Specifically, for each question, top 100 web snip-
paraphrases. The 660 questions are first classified by ques- pets are retrieved by Google and Mean Reciprocal Rank 
tion types. After that, further partition is done within each (MRR) is used in the evaluation of question reformulation 
class as described in Section 3.3. Finally the paraphrase [Wang et al., 2005]. MRR is defined as follows: 
identifier is employed in each cluster to detect paraphrases.  
                                                                         1 n 1
  After the above process, 160 pairs of questions are recog-                    MRR                (8)
nized as paraphrases, of which 107 pairs are true para-                  n i 1 ri


                                            IJCAI-07
                                             1799