          Continuous nonlinear dimensionality reduction by kernel eigenmaps 

                                                  Matthew Brand 
                                    Mitsubishi Electric Research Laboratories 
                                            Cambridge, MA 02460 USA 


                        Abstract                                       with minimal distortion vis-a-vis the weights, in the 
                                                                   sense that a larger stipulates a shorter embedding 
     We equate nonlinear dimensionality reduction                  distance. Formally, the embedding 
     (NLDR) to graph embedding with side information 
     about the vertices, and derive a solution to either                                                              (4) 
     problem in the form of a kernel-based mixture of 
     affine maps from the ambient space to the target 
     space. Unlike most spectral NLDR methods, the 
     central eigenproblem can be made relatively small,                                                               (5) 
     and the result is a continuous mapping defined over 
     the entire space, not just the datapoints. A demon­
     stration is made to visualizing the distribution of           for any integer . The norm constraint = 
     word usages (as a proxy to word meanings) in a                I sets the scale of the embedding and causes vertices of 
     sample of the machine learning literature.                    high cumulative weight to be embedded nearer to the 
                                                                   origin. 
1 Background: Graph embcddings                                  4. Y can be rigidly rotated in without changing its dis­
                                                                   tortion. The distortion measure is also invariant to rigid 
Consider a connected graph with weighted undirected edges          translations, but the eigenproblem is not, thus there is 
specified by edge matrix W. Let be the posi­                       an unwanted degree of freedom (DOF) in the solution. 
tive edge weight between connected vertices i and j zero           Due to stochasticity, this DOF is isolated in the uniform 
otherwise. Let D = diag(Wl) be a diagonal matrix where             eigenvector \\, which is suppressed from the embedding 
             the cumulative edge weights into vertex /. The        without error (because . Addingto Y 
following points are well known or easily derived in spectral      rigidly translates the embedding by 
graph theory [Fiedler, 1975; Chung, 1997]: 
                                                                5. Premultiplying by , and rearranging equates equa­
  1. The generalized eigenvalue decomposition (EVD)                tion 1 to the EVD of the graph Laplacian D - W: 
                         WV=DVA (1)                                                                                   (6) 
     has real eigenvectors and eigenvalues                      6. Premultiplying by connects equation 1 to the 
                                                                   (symmetric) KVD of the normalized Laplacian: 

  2. Premultiplying equation (1) by D_1 makes the general­                                                            (7) 
     ized eigenproblem into a stochastic eigenproblem 
                                                                   with 
                                                       (2)    In summary: Equation 1 gives an optimal embedding of 
    where is a stochastic transition matrix having            a graph in Rc/ via eigenvectors eigenvalue Ω-iis 
    nonnegative rows that sum to one. The largest eigen­      stochastic and the corresponding eigenvector \\ is uniform; 
     value of equation (1) is therefore stochastic and        this is an important property of the EVD solution because it 
     its paired eigenvector is uniform                        isolates the problem's unwanted translational degree of free­
                                                              dom in a single eigenvector, leaving the remaining eigenvec­
  3. Expanding and collecting terms in WiJ reveals the geo­   tors unpolluted. 
    metric meaning of the eigenvalues:                           Many embedding algorithms can be derived from this anal­
                                                              ysis, including the Fiedler vector [Fiedler, 1975], locally lin­
                                                       (3) 
                                                              ear embeddings (LLE) [Roweis and Saul, 2000], and Lapla­
    The d eigenvectors paired to eigenvalues through cian eigenmaps [Belkin and Niyogi, 2002]. For example, di­
          therefore give an embedding of the vertices in      rect solution of equation 1 gives the Laplacian eigenmap; as 


LEARNING                                                                                                             547  a historical note, the symmetrized formulation was proposed   and Y affine and guarantees that is uniform, but it can 
 by Fiedler in the 1970s and has been used for heuristic graph also force the eigenvectors to model additional variance that 
 layout since the 1980s [Mohar, 1991].                         is not part of the problem. 
                                                                  Working backward from the desiderata that the leading col­
 2 Transformational embeddings                                 umn of V Z should be uniform, let such 
                                                               that ZK is a modified representation of the vertices with val­
 Now consider a more general problem: We are given some 
                                                               ues of z() reweighted on a per-vertex basis: 
 information about the vertices in a matrix Z = [z\, • • • ,z ] c 
                                                       N       Clearly has a uniform first column, since each row 
 R   , whose columns are generated by applying a vector-
  JxN                                                          is divided by its first element. 
 valued function z(-) —► z £ R  to each vertex of the graph. 
                               d                                 It follows immediately that the related eigenproblem 
 We seek a linear operator which transforms Z to the optimal 
 graph embedding: G(Z) —♦ Y. We will call this the "transfor­                                                        (12) 
mational embedding," to distinguish it from the "direct em­
bedding" discussed above.                                      is stochastic, and , is an embedding 
   A natural candidate for the algebraic statement of the trans­ with the unwanted translational degree of freedom totally re­
formational embedding problem is the generalized EVD           moved. Note that the raw and stochastic approximations are 
                                                               orthogonal (under metric is a diagonal matrix; 
                 (ZWZT)V = (ZDZT)VA, (8) the other methods are not. 
                                                                 It should be noted that—when scaled to have equal norm 
because setting Y = VTZ makes this equivalent to the orig­                  — none of these approximations has uniformly 
inal direct embedding problem. Again, there is an equiva­      superior distortion scores; but in Monte Carlo trials with ran­
lent symmetric eigenproblem: Make Cholesky1 decomposi­         dom graphs, wc find a clear ordering from lowest to highest 
tion RTR <- ZDZT into upper-triangular R G Rdxd and let        distortion: reweighted, affine, stochastic, raw (see figure 1). 

              B= (R- ' ZWZ1 IT1) eRuxd. (9) 

Then 
                        BV' = V'A (10) 
with 
                                                       (in 
This gives an embedding , and a computational 
advantage: is a short matrix, the origi­
nal N x N eigenproblem can be reduced to a very small d xd 

problem, and the matrix multiplications also scale as 0(d2N) 

rather than 0(N3), due to the sparsity of W and D. 
2.1 Correcting problematic eigenstructure 
It is generally the case that —there is no lin­
ear combination of the rows of Z giving Y, so the desired lin­ Figure 1: Comparison of methods for modifying the row-
ear mapping G(Z) —* Y does not exist. Equations 8-11 give      space of Z. The graph shows distortion from the optimal 
the optimal least-squares approximation                        embedding, averaged over 106 trials with 50-node matrices 
This approximation can have a serious flaw: I having random edge weights and random Z e_ R4x50. 
then the first eigenvector vi is not uniform; it cannot be dis­
carded as the unwanted translational DOF. Worse, all the       The raw approximation is suboptimal because information 
other eigenvectors will be variously contaminated by the un­   about the d-dimensional embedding is spread over d + 1 
wanted DOF, resulting in an embedding polluted with arti­      eigenvectors, no subset of which is optimal. The stochas­
facts. For this reason, we call direct solution of equation 8 a tic approximation is also suboptimal—it optimizes a differ­
raw approximation.                                             ent measure implied by equation 12. In practice, when com­
  Our options for remedy are limited to those that modify      puting embeddings of graphs whose embedding structure is 
the row-space of Z to reintroduce the uniform eigenvector.     known a priori, wc find that the reweighted and stochastic 
For reasons that will become obvious below, we will restrict   approximations give results that are clearly very similar, and 
ourselves to operations that can be applied to any column of   superior to the other approximations. 
Z without knowing any other column.                              The need for any such correction stems from the fact 
  The simplest such operation is to append a uniform row to    that—the literatures of spectral graph theory and NLDR 
Z, so that . This makes the relation between Z                 notwithstanding—equation 1 is not a completely correct 
                                                               statement of the embedding problem. We will show in a 

   1 Any gram-like factorization will work. For example, given EVD forthcoming paper that, as a statement of the embedding 
                           . The Cholesky is especially attrac­ problem, equation 1 is both algebraically underconstraincd 
tive for its numerical stability, sparsity, and easy invertibility. and numerically ill-conditioned. In particular, point #2 is 


548                                                                                                           LEARNING not strictly true: The stochastic eigenvalue is not always     / should be monotonically decreasing, relatively insensitive 
paired to a uniform eigenvector. This leads to patholo•        to noise (d/ should be small), and it should lead to exact re•
gies that can ruin the embedding, whether obtained from the    constructions of data sampled from manifolds that are already 
basic or derived formulations. NLDR algorithms that can        flat. Straightforward calculus shows that equation 13 has the 
be derived from equation 1 (e.g., [Roweis and Saul, 2000;      desired minimum when , or more gen•
Belkin and Niyogi, 2002; Teh and Roweis, 2003]) do not re•     erally, the multiplicative inverse of whatever distance mea•

mediate the problem.                                           sure is appropriate in the ambient space2. (By contrast, the 
   A forthcoming paper makes a full analysis of these is•      LLE weightings are not correlated with distances.) To make 
sues, identifies the correct problem statements for both equa• the problem scale invariant, we scale W such that its largest 
tions 1 & 8, and gives closed-form optimal solutions to both   nonzero off-diagonal value is 1 (consequently every•
problems. The approximation methods discussed in this sec•     where / is computed). 
tion are still useful in that they are faster and give reasonably Let us now situate some Gaussian kernels p*(x) = 
high-quality embeddings. For the NLDR method and datasets                  on the manifold. In this paper, we will take a 
considered below, the result of the reweighted approxima•      random subset of data points as kernel centers, and set all 
tion is almost numerically indistinguishable from the optimal           ; these kernels are radial basis functions. Let vector 
embedding, and requires substantially less calculation. The 
reweighting method can also be justified as a Pade approxi•
                                                                                                                     (14) 
mation of the optimal solution. 

3 Nonlinear dimensionality reduction                           be the kth local homogeneous coordinate of Xi scaled by 
                                                               the posterior of the kth kernel. K; is an optional local 
Let . , ] be a set of points sampled from a                    dimensionality-reducing linear projection. Let representation 
low-dimensional manifold embedded in a high-dimensional        vector 
ambient space. A reduced-dimension embedding Y =                                                                     (15) 
                                 is a set of low-dimensional 
points with the same local neighborhood structure. We de•      be the vertical concatenation of all such local coordinate vec•
sire instead a mapping which will general•                     tors. Collect all such column vectors into a basis matrix 
ize the correspondence to the whole continuum, with rea•
sonable interpolation and extrapolation to be expected in the    To summarize thus far, our goal now is to find a linear 
neighborhood of the data. Spectral methods for NLDR typ•       transform of the basis (kernel-weighted coor•
ically require solution of many and/or very large eigenvalue   dinates) that is maximally consistent with our local distance 
or generalized eigenvalue problems [Kruskal and Wish, 1978;    constraints, specifically 
Kambhatla and Leen, 1997; Tcnenbaum et al., 2000; Roweis 
and Saul, 2000; Belkin and Niyogi, 2002], and with the ex•
                                                                                                                     (16) 
ception of [Teh and Roweis, 2003; Brand, 20031, offer em-
beddings of points rather than mappings between spaces. 
   Here we show how a leverage the transformational embed•     This is isomorphic to the graph embeddings of section 2; the 
ding of section 2 into a continuous NLDR algorithm, specifi•   methods developed there apply directly to W and Z. The con•
cally a kernel-based mixture of affine maps from the ambient   tinuous mapping from ambient to embedding space immedi-
space to the target space. To do so, we must show how the      atly follows from the continuity and smoothness of z{-): 
edge weight matrix W and vertex matrix Z are specified. Let 
                iff x; and Xj satisfy some locality criterion, 
                    otherwise As stated above, an 
                                                               where the EVD determines the transformation G = 
embedding Y of X should satisfy 
                                                                              of the continuous kernel representation de•
                                                               fined over the entire ambient space: 

where larger Wy penalize large distances between y,- and y,.                                                         (17) 
  How should Wij be computed? / is a measure of similar•
ity: The graph-theoretic literature usually takes /(•,) = 1. 
while NLDR methods typically take                                 2Proof: Consider three pointson a 1D 
           to be a Gaussian kernel, on analogy to heat dif•    manifold. What similarity measure causes the 
fusion models [Belkin and Niyogi, 2002]. The uninforma-        distortion to have a global minimum 
tive setting Wij = 1 is only usable when there is a very large at Without loss of generality, we fix the global location and 
                                                               scale of the embedding by fixing the endpoints: 
number of points (and edges), so that connectivity informa•
                                                               Solving for the unique zero of the distortion's first derivative, we 
tion alone suffices to determine metric properties of the em•
                                                               obtain the optimum at y2 = W23 W12 -I- W23). Since this is a har•
bedding. The Gaussian setting has a complementary weak•        monic relation, the unique continuous satisficing measure is 
ness: It can be very sensitive to small variations in distance             . This sets and ; some 
to neighbors (that may be introduced by the curvature of the   simple algebra confirms that indeed .at the optimum. The 
data manifold or measurement noise in the ambient space).      induction to multiple points in multiple dimensions is direct. 


LEARNING                                                                                                             549     As a matter of numerical prudence, we recommend using         We now show some kernel eigenmaps computed using the 
 the reweighted approximation:                                 transformational embedding of section 2. All embedding 
                                                               methods are given the same inputs. 
                                                       (18)                                      Figure 4 shows a raw 
                                                                                               kernel eigenmap embedding 
                                                                                               computed using a basis (Z 
 At first blush, it would seem that reweighting should not be 
                                                                                               matrix) created from 64 
 necessary: By construction, ), thus — 
                                                                                               Gaussian unit-a kernels 
 and the denominator—should be uniform at the datapoints. 
                                                                                               placed on random points. 
 However, as mentioned above, even when the algebra predicts 
                                                                                               This required solving a 
 this structure, numerical eigensolvers may not find it. 
                                                                                               much more manageable 
   To obtain an approximate inverse mapping, we map the 
                                                                                               256 x 256 eigenproblem. 
 means and covariances of each kernel into the target 
                                                                                               100 trials were performed 
 space to obtain kernels there. Then, 
                                                                                               with different sets of ran­
 breaking                  into blocks corresponding to each 
          fc j                                                                                domly placed kernels. In all 
 kernel, take the Moore-Penrose pseudo-inverse of each, and 
                                                                                               trials, the reweighted and 
 set If using the reweighted map, the ap­                      Figure 4: Kernel eigenmap 
                                                                                               stochastic maps gave the 
 proximate inverse map is                                      embedding, raw result. 
                                                               the raw and affine maps exhibited substantial folding at the 
                                                               edges and corners of the embedding. 
                                                                 Figure 5 shows a 
                                                               reweighted kernel eigen­
                                                               map computed from the 
 4 Illustrative example                                        same W and Z as figures 3 
                                                               & 4. The result is smoother 
                             We will use a variant of the      and actually exhibits less 
                           "swiss roll", a standard test man­  folding than the original 
                           ifold in the NLDR community, to     Laplacian eigenmap. 
                           illustrate the arguments and meth­
                                                                 The problem can be reg­
                           ods developed in this paper. We 
                                                               ularized by putting positive 
                           sampled a twisted version of the 
                                                               mass on the diagonal of W 
                           manifold regularly on a 30 x 30 
                                                               (e.g., W+W +1), thereby 
                           grid and added a small amount 
                                                               making the recovered ker­
                           of Gaussian isotropic noise. Fig­
                                                               nel eigenmap more isomet­
                           ure 2 shows the ideal R  param­
                                                  2            ric (bottom figure 5). This 
                           eterization and two views of the 
                                                               regularization is appropri­
                           ambient R   embedding. Points 
                                     3                         ate when it is believed 
                           are shown joined into a line to aid 
                                                               that all neighborhoods are 
 Figure 2: The swiss roll. visual interpretation of the em-
                                                               roughly the same size. 
                           beddings. All experiments in this 
 section use a W matrix that was generated using the 12 near­    The recently proposed 
est neighbors to each point and the inverse distance function. Locality Preserving Projec­
                                                               tion (LPP) [He and Niyogi, 
   The Laplacian eigenmap                                                                   Figure 5: Kernel eigenmap 
                                                               2002], is essentially the raw 
embedding (figure 3) shows                                                                  embedding, reweighted and 
                                                               approximation (direct solu­
the embedding specified by                                                                  regularized results. 
                                                               tion of equation 8) with 
the W matrix. Note 
that it exhibits some fold­                                                      and Z = X, thereby giving a linear pro­
ing at the corners and                                         jection from the ambient space to the target space that best 
top and bottom edges, due                                      preserves local relationships. 
partly to problems with the                                                                      LPP is admirably simple, 
uniform eigenvector and                                                                        but it can be shown that the 
exacerbated by the fact                                                                        affine approximation from 
that spectral embeddings                                                                       section 2 will always have 
tend to compress near the                                                                      less distortion. LPP can also 
boundaries. The Laplacian                                                                      suffer from loss of the uni-

eigenmap requires solution Figure 3: Laplacian eigenmap        Figure 6: LPP embedding and form eigenvector. Figure 6 
of a large 900 x 900 eigen- embedding,                         our affine upgrade. show embeddings of the 
problem, and offers no mapping off the points. Kernel eigen-   swiss roll produced by LPP and by an affine modification of it 
maps will be approximations to this embedding.                 that is equivalent to our method with a trivial single uniform-


550                                                                                                           LEARNING density kernel. Upgrading LPP to an affinc projection cap•     G(x)—there is no need to compute a new global embedding 
tures more of the data's structure. Even so, there is no affine or revise the EVD. 
"view" of this manifold that avoids folding.                     The reweighting scheme, although theoretically mooted by 
                                                               our subsequent discovery of a better problem formulation and 
5 Visualizing word usages                                      closed-form solution, is still practically viable as a fast ap•
                                                               proximation for large problems, and as a post-conditioning 
In statistical analyses of natural language, similar usage pat•
                                                               step for unavoidable numerical error of any NLDR algorithm 
terns for two words are taken to indicate that they have sim•
                                                               based on eigenvalue decompositions. 
ilar meanings or strongly related meanings. Latent semantic 
                                                                 In this paper we have used random kernels. There are nu•
analysis (LSA) is a linear dimensionality reduction of a term-
                                                               merous avenues to discovering stronger methods by investi•
document co-occurence matrix. The principal components of 
                                                               gating placement and tuning of the kernels, stability of the 
this matrix give an embedding in which similarly used words 
                                                               embedding and its topological structure, and sample com•
are similarly located. Literally, co-location is a proxy for col•
                                                               plexity. In short, all the issues that proved fertile ground 
location (the propensity of words to be used together) and 
                                                               for research in classification and regression can be studied 
synonymy. We may expect that the kernel eigenmap offers a 
                                                               anew in the context of estimating the geometry and topology 
more powerful nonlinear analysis: 
                                                               of manifolds. 
   The NIPS 12 corpus3 features a matrix counting occur•
rences of 13000+ words in 1700+ documents. We mod•             References 
eled the first 1000 words and the last 200 documents in the 
matrix. This roughly corresponds to one year's papers, a       [Belkin and Niyogi, 2002] Mikhail Belkin and Partha 
reasonable "snapshot" of the ever-changing terminology of         Niyogi. Laplacian eigenmaps for dimensionality re•
the field. We stemmed the words and combined counts for           duction and data representation. Technical Report 
the same roots, then determined distance between two word         TR-2002-01, University of Chicago Computer Science, 
roots as the cosines of the angles between their log-domain-      2002. 
transformed occurrence vectors (x,, —* log2(l + xij)). The W   [Brand, 2003] Matthew Brand. Charting a manifold. In Proe. 
matrix was generated by adding an edge from each word to          NIPS-15, 2003. 
its 30 closest neighbors in cosine-space. The representation 
Z was made using 4 random words as kernel centers. Fig•        [Chung, 1997] Fan R.K. Chung. Spectral graph theory, vol•
ure 7 discusses the resulting 2D embedding, in which techni•      ume 92 of CBMS Regional Conference Series in Mathe•
cal terms arc clearly grouped by field and many of the more       matics. American Mathematical Society, 1997. 
common English words arc tightly clustered by common se•       [Fiedler, 1975] Miroslav Fiedler. A property of eigenvectors 
mantics. The first two LSA dimensions (also shown in fig•         of nonncgative symmetric matrices and its application to 
ure 7) of the same data arc reveal significantly less semantic    graph theory. Czech. Math. Journal 25:619-633, 1975. 
structure. 
                                                               [He and Niyogi, 2002] Xiafei He and Partha Niyogi. Local•
                                                                  ity preserving projections. Technical Report TR-2002-09, 
6 Discussion                                                      University of Chicago Computer Science, October 2002. 
The kernel eigenmap generates continuous nonlinear map•        [Kambhatla and Leen, 1997] N. Kambhatla and Todd Leen. 
ping functions for dimensionality reduction and manifold re•      Dimensionality reduction by local principal component 
construction. Suitable choices of kernels can reproduce the       analysis. Neural Computation, 9, 1997. 
behavior of several other NLDR methods. One could put a ker•
                                                               [Kniskal and Wish, 1978] J. B. Kruskal and M. Wish. Mul•
nel at every local group of points, perform local dimensional•    tidimensional Scaling. Sage Publications, Beverly Hills, 
ity reduction (e.g., a PC A) at each kernel, and thereby obtain   CA, 1978. 
from equations 8 and 17 an NLDR algorithm much like chart•
ing [Brand. 2003] or automatic alignment [Teh and Roweis,      [Mohar, 1991] B. Mohar. The laplacian spectrum of graphs. 
2003]. Or, as in the demonstrations above, the kernel eigen•      In Y. Alavi, editor, Graph Theorv, Combinatorics and Ap•
map can simultaneously determine the local dimensionality        plications, pages 871-898. J. Wiley, New York, 1991. 
reductions and their global merger.                            [Roweis and Saul, 2000] Sam T. Roweis and Lawrence K. 
  The kernel eigenmap typically substitutes a small dense         Saul. Nonlinear dimensionality reduction by locally lin•
CVD for the the large sparse LVD of graph embedding prob•         ear embedding. Science, 290:2323-2326, December 22 
lems. In the sparse case, a specialized power method can          2000. 
compute the desired eigenvectors in significantly less than the 
                                                               [Teh and Roweis, 2003] Yee Whye Teh and Sam T. Roweis. 
0(N ) time required for a full EVD. In the kernel setting, 
    3                                                             Automatic alignment of hidden representations. In Proc. 
similar efficiencies apply because both W and Z are typically     NIPS-15, 2003. 
sparse, allowing fast construction of the reduced EVD prob•
                                                               [Tenenbaum e/ al., 2000] Joshua B. Tenenbaum, Vin 
lem ZWZ1; this too is amenable to fast power methods. Of 
course, the most important efficiency of the kernel method        de Silva, and John C. Langford. A global geomet•
is its ability to embed new points quickly via the function       ric framework for nonlinear dimensionality reduction. 
                                                                  Science, 290:2319-2323, December 22 2000. 

   3Courtesy S. Roweis, available from the U. Toronto website. 


LEARNING                                                                                                              551 