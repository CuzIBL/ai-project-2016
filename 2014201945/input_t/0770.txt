                     Generalization Error of Linear Neural Networks
                               in an Empirical Bayes Approach
                             Shinichi Nakajima   †‡ and Sumio Watanabe     †
                                    † Tokyo Institute of Technology
          Mailbox R2-5, 4259 Nagatsuda, Midori-ku, Yokohama, Kanagawa, 226-8503 Japan
                         nakajima.s@cs.pi.titech.ac.jp, swatanab@pi.titech.ac.jp
                                          ‡ Nikon Corporation
                     201-9 Oaza-Miizugahara, Kumagaya, Saitama, 360-8559 Japan
                    Abstract                          is neglected, was proposed [Hinton and van Camp, 1993;
                                                      MacKay, 1995; Attias, 1999; Ghahramani and Beal, 2000].1
    It is well known that in unidentiﬁable models, the  In this paper, we consider another alternative, which we
    Bayes estimation has the advantage of generaliza- call a subspace Bayes (SB) approach. An SB approach is an
    tion performance to the maximum likelihood esti-  empirical Bayes (EB) approach where a part of the parame-
    mation. However, accurate approximation of the    ters of a model are regarded as hyperparameters. If we re-
    posterior distribution requires huge computational gard the parameters of one layer as hyperparameters, we can
    costs. In this paper, we consider an empirical Bayes analytically calculate the marginal likelihood in some three-
    approach where a part of the parameters are re-   layer models. Consequently, what we have to do is only to
    garded as hyperparameters, which we call a sub-   ﬁnd the hyperparameter value maximizing the marginal like-
    space Bayes approach, and theoretically analyze   lihood. The computational costs of an SB approach is thus
    the generalization error of three-layer linear neu- much less than that of posterior distribution approximation by
    ral networks. We show that a subspace Bayes ap-   MCMC methods. At ﬁrst in this paper, we prove that in three-
    proach is asymptotically equivalent to a positive- layer linear neural networks, an SB approach is equivalent
    part James-Stein type shrinkage estimation, and be- to a positive-part James-Stein (JS) type shrinkage estimation
    haves similarly to the Bayes estimation in typical [James and Stein, 1961]. Then, we clarify its generalization
    cases.                                            error, also considering delicate situations, the most important
                                                      situations in model selection problems and in statistical tests,
                                                      when the Kullback-Leibler divergence of the true distribution
1  Introduction                                       from the singularities is comparable to the inverse of the num-
                                                                          2
Unidentiﬁable parametric models, such as neural networks, ber of training samples. We conclude that an SB approach
mixture models, and so on, have a wide range of applica- provides as good performance as the Bayes estimation in typ-
tions. These models have singularities in the parameter space, ical cases.
hence the conventional learning theory of the regular sta- In Section 2, neural networks and linear neural networks
tistical models does not hold. Recently, generalization per- are brieﬂy introduced. The framework of the Bayes estima-
formance of some unidentiﬁable models has been theoreti- tion, that of an EB approach, and that of an SB approach are
cally clariﬁed. In the maximum likelihood (ML) estimation, described in Section 3. The signiﬁcance of singularities for
which is asymptotically equivalent to the maximum a poste- generalization performance and the importance of analysis of
rior (MAP) estimation, the generalization error of linear neu- delicate situations are explained in Section 4. The SB solu-
ral networks was proved to be greater than that of the reg- tion and its generalization error are derived in Section 5. Dis-
ular models whose dimension of the parameter space is the cussions and conclusions follow in Section 6 and in Section 7,
same when the model is redundant to learn the true distri- respectively.
bution [Fukumizu, 1999]. On the other hand, in the Bayes
estimation, the generalization error of neural networks, lin- 2 Linear Neural Networks
ear neural networks, mixture models, and so on was proved x ∈ RM                         y ∈ RN
                                    [                 Let        be an input (column) vector,    an output
to be less than that of the regular models Watanabe, 2001;      w
Aoyagi and Watanabe, 2004; Yamazaki and Watanabe, 2003]. vector, and a parameter vector. A neural network model
                                                      can be described as a parametric family of maps {f(·; w):
  However, the Bayes posterior distribution can seldom be RM → RN }                             H
exactly realized. Furthermore, Markov chain Monte Carlo          . A three-layer neural network with hidden
(MCMC) methods, often used for approximation of the pos- 1We have just derived the variational Bayes solution of linear
terior distribution, require huge computational costs. As an neural networks and clariﬁed its generalization error and training
alternative, the variational Bayes approach, where the corre- error in [Nakajima and Watanabe, 2005b].
lation between parameters and the other parameters, or the 2We have also clariﬁed the training error, which is put into
correlation between the parameters and the hidden variables [Nakajima and Watanabe, 2005a].units is deﬁned by                                    where
                                                                    
                      H
                                  t                         n   n                     q(y|x)
             f(x; w)=   h=1 bhψ (ahx),          (1)     G X  ,Y       q x q y|x                 dxdy
                                                          (      )=    ( ) (  )log p y|x, X n,Yn      (9)
                      M     N                                                       (          )
where w = {(ah,bh) ∈ R  × R  ; h =1,...,H}  summa-
rizes all the parameters, ψ(·) is an activation function, which is the Kullback-Leibler (KL) divergence of the predictive dis-
is usually a bounded, non-decreasing, antisymmetric, nonlin- tribution from the true distribution, and ·q(Xn,Y n) denotes
ear function like tanh(·),andt denotes the transpose of a the expectation value over all sets of n training samples.
matrix or vector. Assume that the output is observed with a
                     2                                3.2  Empirical Bayes Approach
noise subject to NN (0,σ IN ),whereNd(µ, Σ) denotes the
d-dimensional normal distribution with average vector µ and         and Subspace Bayes Approach
covariance matrix Σ,andId denotes the d×d identity matrix. We often have little information about the prior distribution,
Then, the conditional distribution is given by        with which an EB approach was originally proposed to cope.
                                          
                              y − f x w 2           We can introduce hyperparameters in the prior distribution;
  p y|x, w       1          −       ( ; )    .        for example, when we use a prior distribution that depends
   (     )=      2 N/2 exp           2          (2)
             (2πσ )                2σ                 on a hyperparameter τ1 such as
                                                                                            
In this paper, we focus on linear neural networks, whose ac-               1            w2
tivation function is linear, as the simplest multilayer models.3 φ(w)=           exp  −        ,     (10)
                                                                         πτ2 K/2         2τ 2
A linear neural network model (LNN) is deﬁned by                       (2  1 )             1
                                                                                              τ
                 f(x; A, B)=BAx,                (3)   the marginal likelihood, Eq.(5), also depends on 1.InanEB
                                                      approach, τ1 is estimated by maximizing the marginal likeli-
                     t
where A =(a1,...,aH ) is an H × M input parameter ma- hood or by a slightly different way [Efron and Morris, 1973;
trix and B =(b1,...,bH ) is an N × H output parameter Akaike, 1980; Kass and Steffey, 1989]. Extending the idea
matrix. Because the transform (A, B) → (TA,BT−1) does above, we can introduce hyperparameters also in a model dis-
not change the map for any non-singular H × H matrix T , tribution. What we call an SB approach is an EB approach
the parameterization in Eq.(3) has trivial redundancy. Ac- where a part of the parameters of a model are regarded as hy-
cordingly, the essential dimension of the parameter space is perparameters. In the following sections, we analyze two ver-
given by                                              sions of SB approach: in the ﬁrst one, we regard the output
                                  2                   parameter matrix B of the map, Eq.(3), as a hyperparame-
               K = H(M   + N) − H  .
                                                (4)   ter and then marginalize the likelihood in the input parameter
We assume that H ≤ N ≤ M throughout this paper.       space (MIP); and in the other one, we regard the input pa-
                                                      rameter matrix A, instead of B, as a hyperparameter and then
3  Framework of Learning Methods                      marginalize in the output parameter space (MOP).
3.1  Bayes Estimation
     n                    n                           4   Unidentiﬁability and Singularities
Let X  = {x1,...,xn} and Y = {y1,...,yn} be arbitrary
n training samples independently and identically taken from We say that a parametric model is unidentiﬁable if the map
the true distribution q(x, y)=q(x)q(y|x). The marginal con- from the parameter to the probability distribution is not one-
ditional likelihood of a model p(y|x, w) is given by  to-one. A neural network model, Eq.(1), is unidentiﬁable be-
                                                     cause the model is independent of ah when bh =0,orvice
                          
           n  n             n                         versa. The continuous points denoting the same distribution
       Z(Y  |X )=    φ(w)      p(yi|xi,w)dw,    (5)
                            i=1                       are called the singularities, because the Fisher information
                                                      matrix on them degenerates. When the true model is not on
where φ(w) is the prior distribution. The posterior distribu-
                                                      the singularities, asymptotically they do not affect prediction,
tion is given by
                                                      and therefore, the conventional learning theory of the regular
                           n
                      φ(w)      p(yi|xi,w)            models holds. On the other hand, when the true model is on
        p(w|Xn,Yn)=          i=1          ,     (6)
                           Z(Y n|Xn)                  the singularities, they signiﬁcantly affect generalization per-
                                                      formance as follows: in the ML estimation, the extent of the
and the predictive distribution is deﬁned as the average of the set of the points denoting the true distribution increases its
model over the posterior distribution as follows:
                                                     neighborhoods and hence the ﬂexibility of imitating noises,
           n   n                   n   n              and therefore, accelerates overﬁtting; while in the Bayes es-
   p(y|x, X ,Y  )=    p(y|x, w)p(w|X ,Y )dw.    (7)   timation, the large entropy of the singularities increases the
                                                      weights of the distributions near the true one, and therefore,
The generalization error, a criterion of generalization perfor- suppresses overﬁtting. In LNNs, the former property appears
mance, is deﬁned by                                   as acceleration of overﬁtting by selection of the largest singu-
                        n   n                         lar value components of a random matrix, and in the SB ap-
            G(n)=G(X    ,Y  )q(Xn,Y n),       (8)
                                                      proaches of LNNs, the latter property appears as James-Stein
  3A linear neural network model is not a toy but an useful model, type shrinkage, as shown in the following sections.
known as a reduced-rank regression model, in many applications Suppression of overﬁtting accompanies insensitivity to the
[Reinsel and Velu, 1998].                             true components with small amplitude. There is a trade-off,which would, however, be ignored in asymptotic analysis if (The proof is given in Appendix A.)
we would consider only situations when the true model is dis- Because the independence between A and B makes the
tinctly on the singularities or not. Therefore, in this paper, we posterior distribution localized, the following lemma holds.
also consider delicate situations when the KL divergence of
the true distribution from the singularities is comparable to Lemma 1 The predictive distribution in the SB approaches
the inverse of the number of training samples, n−1,which can be written as follows:
                                                                                −1/2
are important situations in model selection problems and in    n   n        N
statistical tests with ﬁnite number of samples for the follow- p(y|x, X ,Y )= (2π) |Vˆ |
                                                                                       

ing reasons: ﬁrst, that there naturally exist a few true compo-            −1
                               n−1/2                                    t Vˆ                   −3/2
nents with amplitude comparable to  when neither the   · exp −(y−Vˆ BˆAxˆ )  (y−Vˆ BˆAxˆ ) +O(n    ), (17)
smallest nor the largest model is selected; and secondly, that            2
whether the selected model involves such components essen-
                                                                         −1
tially affects generalization performance.            where Vˆ = IN + O(n   ), and |·|denotes the determinant
                                                      of a matrix.
5  Theoretical Analysis
                                                      (Proof) The predictive distribution is written as follows:
5.1  Subspace Bayes Solution
                                                                 n  n
                                           τ            p(y|x, X ,Y )=p(y|x, AB)p(A|Xn,Y nB)
By   we, hereafter, distinguish the hyperparameter from                                
the parameter w, for example, p(y|x, wτ). Assume that the                 t        ∗ ∗
                                                            ∝ q(y|x) exp  y (BˆAˆ − B A )x  ,        (18)
variance of a noise is known and equal to unity. Then, the                                 p(A|Xn,Y nB)
conditional distribution of an LNN in the MIP version of SB
approach is given by                                  where ·p denotes the expectation value over a distribution p.
                                                                  ∗ ∗       −1/2
                               y − BAx2             Since (BˆAˆ − B A )=O(n     ) in the SB approaches, we
  p y|x, AB       1         −              .
   (        )=    π N/2 exp                    (11)   can expand the predictive distribution as follows:
                (2 )                2                                         
We use the following prior distribution:                p y|x, X n,Yn ∝ q y|x      yt BˆAˆ − B∗A∗ x
                                                       (          )    (   ) 1+   (           )
                                    t                                                 
                    1           tr (A A)                                         t  t
        φ(A)=            exp  −          .     (12)                             y vv y        −3/2
                (2π)HM/2           2                                          +         + O(n     ), (19)
                                                                                 2n   p(A|Xn,Y nB)
                               p(y|x, BA)    φ(B)
Note that we can similarly prepare        and                   √
for the MOP version. We assume that the true conditional where v = n(BˆAˆ− B∗A∗)x is an N-dimensional vector of
            p y|x, A∗B∗        B∗A∗
distribution is (       ),where      is the true map  order O(1). Calculating the expectation value and expanding
         H∗  ≤  H               ∗
with rank        .Wedenoteby      the true value of a the logarithm of Eq.(19), we arrive at Lemma 1. (Q.E.D.)
parameter as above, and by a hat an estimator of a parameter, Comparing Eq.(16) with the ML estimator
            ˆ ˆ
for example, A, bh, etc.. For simplicity, we assume that the              
                                     t                                        H      t    −1
input vector is orthonormalized so that xx q(x)dx = IM .          BˆAˆ           ω  ω  RQ
                                                                      MLE =   h=1 bh bh              (20)
Consequently, the central limit theorem leads to the following
two equations:                                        [Baldi and Hornik, 1995], we ﬁnd that the SB estimator of
                  
          n    −1   n    t            −1/2            each component is asymptotically equivalent to a positive-
     Q(X   )=n         xx  = IM + O(n     ),   (13)
                  i=1                                part JS type shrinkage estimator. Moreover, by virtue of
      n   n    −1   n    t    ∗  ∗      −1/2
  R(X  ,Y  )=n      i=1 yx = B A  + O(n     ), (14)   Lemma 1, we can substitute the model at the SB estimator
where Q(Xn)   is an M   ×  M  symmetric matrix and    for the predictive distribution with asymptotically insigniﬁ-
R(Xn,Yn)  is an N × M  matrix. Hereafter, we abbreviate cant impact on generalization performance. Therefore, we
Q(Xn)  as Q,andR(Xn,Yn)  as R.                        conclude that the SB approach is asymptotically equivalent to
  Let γh be the h-th largest singular value of the matrix the shrinkage estimation. Note that the variance of the prior
RQ−1/2  ω                                             distribution, Eq.(12), asymptotically has no effect upon pre-
       ,  ah the corresponding right singular vector, and
ω                                                     diction and hence upon generalization performance, as far as
 bh the corresponding left singular vector. We ﬁnd from
                   ∗                         −1/2     it is a positive, ﬁnite constant. We call L thedegreeofshrink-
Eq.(14) that γh for H <h≤    H  is of order O(n  ).
                                                      age. Remember that we can modify all the theorems in this
Hence, combining with Eq.(13), we get
                                                      paper for the ML estimation only by letting L =0.
   ω  RQρ    ω  R   O n−1       H∗  <h≤   H,
    bh     =  bh  +   (   )  for               (15)
where −∞  <ρ∈     R <  ∞  is an arbitrary constant. The 5.2 Generalization Error
SB estimator, deﬁned as the expectation value over the SB Using the singular value decomposition of the true map
posterior distribution, is given by the following theorem: B∗A∗, we can transform arbitrary A∗ and B∗ without change
Theorem 1 Let L = M  in the MIP version or L = N in the of the map into a matrix with its orthogonal row vectors and
                              2
MOP version, and Lh =max(L, nγh). The SB estimator of another matrix with its orthogonal column vectors, respec-
the map of an LNN is given by                         tively. Accordingly, we assume the above orthogonalities
        
          H        −1      t    −1      −1           without loss of generality. Then, Lemma 1 implies that the
  BˆAˆ =     (1 − L  L)ωb  ω  RQ    + O(n   ). (16)
          h=1      h      h bh                        KL divergence, Eq.(9), with a set of n training samples isgiven by                                              5.3  Large Scale Approximation
                                 
                     ∗ ∗         2                    In a similar fashion to the analysis of the ML estimation in
                 (B  A  − BˆAˆ)x
   G(Xn,Yn)=                         + O(n−3/2)       [Fukumizu, 1999], the second term of Eq.(23) can be analyt-
                         2
                                   q(x)               ically calculated in the large scale limit when M, N, H,and
                                                        ∗
                H                                    H   go to inﬁnity in the same order. We deﬁne the following
                     G   Xn,Yn     O n−3/2 ,                                  ∗         ∗          
             =    h=1  h(       )+  (     )    (21)   scalars: α = N /M =(N  −H  )/(M −H   ), β = H /N =
                                                      (H −  H∗)/(N  − H∗),andκ   = L/M   = L/(M  −  H∗).
where                                                                                          
                                                    Let W  be a random matrix subject to WN  (M ,IN  ),and
      n   n   1     ∗ ∗t     t t ∗  ∗t     t                                         −1
Gh(X   ,Y  )=  tr (bhah − ˆbhaˆh) (bhah − ˆbhaˆh) (22) {u1,...,uN  } the eigenvalues of M W . The measure of
              2                                       the empirical distribution of the eigenvalues is deﬁned by
                     h                    ·
is the contribution of the -th component. Here tr( ) denotes       −1
                                                         p u du  N    {δ u     δ u    ···  δ u  } ,
the trace of a matrix. We denote by Wd(m, Σ, Λ) the d-    ( )  =        ( 1)+   ( 2)+    +  ( N )    (24)
dimensional Wishart distribution with m degrees of freedom, δ u                         u
          Σ                       Λ                   where  ( ) denotes the Dirac measure at . In the large scale
scale matrix , and noncentrality matrix , and abbreviate as limit, the measure, Eq.(24) , converges almost everywhere to
Wd(m, Σ) the central Wishart distribution.                       
Theorem 2 The generalization error of an LNN in the SB             (u − um)(uM − u)
                                                       p(u)du =                    θ(um <u<uM   )du, (25)
approaches can be asymptotically expanded as                           2παu
                                                                   √                   √
                       −1      −3/2                                        2                   2
             G(n)=λn      + O(n    ),                 where um  =(   α − 1) and uM  =(   α +1)   [Watcher,
                                                      1978]. Calculating moments of Eq.(25), we obtain the fol-
where the coefﬁcient of the leading term, called the general- lowing theorem:
ization coefﬁcient in this paper, is given by
                                                      Theorem 3  The generalization coefﬁcient of an LNN in the
 2λ =(H∗(M    + N) − H∗2)
                                                    large scale limit is given by
                H−H∗                  
                                   L   2                                                  ∗        ∗
                        2                2                     ∗            ∗2    (M − H  )(N − H  )
            +        θ(γh >L)   1 −  2 γh   . (23)      2λ =(H   (M + N) − H   )+
                                    γh                                                      πα
                 h=1                        q({γ2})                                      2    
                                               h                                      2
                                                                 J(st;1)− 2κJ(st;0)+κ  J(st; −1) ,   (26)
                                              2
Here θ(·) is the indicator function of an event, γh is
the h-th largest eigenvalue of a random matrix subject to where
               ∗                                                       
WN−H∗  (M  − H  ,IN−H∗ ), and ·q({γ2}) denotes the ex-                     2     −1
                                  h                      J(s;1)=2α(−s    1 − s +cos    s),
pectation value over the distribution of the eigenvalues.           √  
                                                         J(s;0)=−2    α  1 − s2 +(1+α)cos−1  s
(Proof) According to Theorem 1, the difference between the                              √
SB and the ML estimators of a true component with a posi-                            −1  α(1 + α)s +2α
                            −1                                           − (1 − α)cos        √         ,
tive singular value is of order O(n ). Furthermore, the gen-                            2αs +  α(1 + α)
eralization error of the ML estimator of the component is the J(s; −1)
same as that of the regular models because of its identiﬁabil-   √                          √
                                                             √         2                      α(1+α)s+2α
                                                              α √  1−s   −    −1s  1+α    −1    √
ity. Hence, from Eq.(4), we obtain the ﬁrst term of Eq.(23) 2   2 αs+1+α   cos   + 1−α cos  2αs+ α(1+α)
as the contribution of the ﬁrst H∗ components. On the other
                                                         =                             (0 <α<1)       ,
hand, we ﬁnd from Eq.(15) and Theorem 1 that for a redun- 
                                                               1−s −   −1 s               α
dant component, identifying RQ−1/2 with R affects the SB     2  1+s   cos                 (  =1)
                   O n−1                                                         √               
estimator only of order ( ), which, hence, does not affect                             −1
                                     U                and st =max   (κ − (1 + α))/2 α, J  (2παβ;0) .Here
the generalization coefﬁcient. We say that is the general −1
diagonalized matrix of an N × M matrix T if T is singular J (·; k) denotes the inverse function of J(s; k).
                   T    Ω UΩ          Ω      Ω
value decomposed as  =   b   a,where   a and  b are        Delicate
an M × M  and an N × N orthogonal matrices, respectively. 5.4       Situations
Let D be the general diagonalized matrix of R,andD the In ordinary asymptotic analysis, one considers only situations
(N − H∗) × (M −  H∗) matrix created by removing the ﬁrst when the amplitude of each component of the true model is
H∗ columns and rows from D. Then, the ﬁrst H∗ diagonal zero or distinctly-positive. Also Theorem 2 holds only in such
elements of D correspond to the positive true singular value situations. However, as mentioned in the last paragraph of
components and D consists only of noises. Therefore, D is Section 4, it is important to consider delicate situations when
                                                                  ∗  ∗
the general diagonalized matrix of n−1/2R,whereR is an the true map B√A has tiny but non-negligible singular values
       ∗           ∗                                                 ∗
(N − H  ) × (M − H  ) random matrix whose elements are such that 0< nγh <∞. Theorem 1 still holds in such situa-
                                       t                                                         −1/2
independently subject to N1(0, 1),sothatR R is subject tions by replacing the second term of Eq.(16) with o(n ).
                 ∗                                               ∗
  W     ∗ M  − H  ,I    ∗
to  N−H  (          N−H  ). The redundant components  We regard H as the number of√distinctly-positive true singu-
       −1/2                                                            ∗−1
imitate n  R  . Hence, using Theorem 1 and Eq.(22), we lar values such that γh = o( n). Without loss of general-
obtain the second term of Eq.(23) as the contribution of the ity, we assume that B∗A∗ is a non-negative, general diagonal
last (H − H∗) components. Thus, we complete the proof of matrix with its diagonal elements arranged in non-increasing
Theorem 2. (Q.E.D.)                                   order. Let R∗ be the true submatrix created by removing the     2                                                     2
                                   SB(MIP)                                               SB(MIP)
    1.8                           SB(MOP)                 1.8                           SB(MOP)
    1.6                               ML                  1.6                               ML
                                    Bayes                                                Regular
    1.4                            Regular                1.4
    1.2                                                   1.2
     1                                                     1
    0.8                                                   0.8

   2  lambda / K 0.6                                     2  lambda / K 0.6
    0.4                                                   0.4
    0.2                                                   0.2
     0                                                     0
      0   2   4   6   8   10  12  14  16  18  20            0      2      4     6      8     10     12
                          H*                                                sqrt(n) gamma*
            Figure 1: Generalization error.                   Figure 2: With delicate true components.

    H∗                     B∗A∗        D                  2
ﬁrst   columns and rows from    .Then,   ,deﬁnedin                                       SB(MIP)
the proof of Theorem 2, is the general diagonalized matrix of 1.8                       SB(MOP)
 −1/2                                    t        1.6                             Bayes
n    R  ,whereR   is a random matrix such that R R is                                 ML(=Regular)
                       ∗          ∗ ∗                 1.4
        W     ∗ M  − H ,I     ∗ ,nR R
subject to N−H (         N−H            ). Therefore,     1.2
we obtain the following theorem:                           1
Theorem 4 The generalization coefﬁcient of an LNN in the  0.8
general situations when the true map B∗A∗ may have delicate 2  lambda / K 0.6
                         √   ∗                            0.4
singular values such that 0 < nγh < ∞ is given by
                                                         0.2
                         H         H−H∗                  0
       ∗            ∗2        ∗2           2              0      2      4     6      8     10     12
2λ=(H   (M +N)  −H    )+    nγh +       θ(γh >L)                            sqrt(n) gamma*
                       h=H∗+1        h=1                           Figure 3: Single-output LNN.
                                     
        L  2            L        √
   1 −      γ2−2 1 −      γωt nR∗ω  , (27)   which might seem to be inconsistent with the proved superior-
       γ2  h         γ2  h bh       ah
        h               h                    q(R )  ity of the Bayes estimation to any other learning method when
                                                      we use the true prior distribution. This suspicion is cleared by
     γ ω    ω       h
where h , ah , and bh are the -th largest singular value of consideration of delicate situations in the following.
R, the corresponding right singular vector, and the corre- Using Theorem 4, we can numerically calculate the SB,
sponding left singular vector, respectively, of which ·q(R ) as well as the ML, generalization error in delicate situa-
denotes the expectation value over the distribution.  tions when the true distribution is near the singularities. Fig-
                                                      ure 2 shows the coefﬁcients of an LNN with M =50input,
6  Discussions                                        N  =30output, and  H  =5hidden units on the assump-
                                                      tion that the true map consists of H∗ =1distinctly-positive
6.1  Comparison with the ML Estimation                component, three delicate components whose singular val-
             and with the Bayes Estimation
                                                      ues are identical to each other, and the√ other one null com-
                                                                                          ∗        ∗    ∗
Figure 1 shows the generalization coefﬁcients of an LNN with ponent. The horizontal axis indicates nγ ,whereγh = γ
M  =50input,  N =30output, and  H =20hidden units.    for h =2,...,4. The Bayes generalization error in deli-
                                       ∗
The horizontal axis indicates the true rank H . The verti- cate situations was previously clariﬁed [Watanabe and Amari,
cal axis indicates the coefﬁcients normalized by the param- 2003], but unfortunately, only in single-output (SO) LNNs,
eter dimension K, given by Eq.(4). The lines correspond to i.e., N = H =1.5 Figure 3 shows the coefﬁcients of an
the generalization coefﬁcients of the SB approaches, clariﬁed SOLNN with M =5input units on the assumption that
in this paper, that of the ML estimation, clariﬁed in [Fuku- H∗ =0and the true singular value of the one component,
mizu, 1999], that of the Bayes estimation, clariﬁed in [Aoyagi indicated by the horizontal axis, is delicate.WeseeinFig.3
and Watanabe, 2004], and that of the regular models, respec- that the SB approaches have a property similar to the Bayes
tively.4 The results in Fig. 1 have been calculated in the large estimation, suppression of overﬁtting by the entropy of the
scale approximation, i.e., by using Theorem 3. We have also singularities. We also see that in some delicate situations, the
numerically calculated them by creating samples subject to MIP is worse than the Bayes estimation, which shows consis-
the Wishart distribution and then using Theorem 2, and thus tency with the superiority of the Bayes estimation. We con-
found that the both results almost coincide with each other so clude that in typical cases, the suppression by the singularities
that we can hardly distinguish. We see in Fig. 1 that the SB in the MIP is comparable to, or sometimes stronger than, that
approaches provide as good performance as the Bayes esti-
                                                         5
mation, and that the MIP, moreover, has no greater general- An SOLNN is regarded as a regular model at a view point of the


                                                                                   b           M
                                                                                    a  → w ∈ Ê
ization coefﬁcient than the Bayes estimation for arbitrary H∗, ML estimation because the transform 1 1 makes the
                                                      model linear and hence identiﬁable, and therefore, the ML general-
  4In the regular models, the normalized generalization coefﬁcient ization error is identical to that of the regular models. Nevertheless,
is always equal to one, which leads to the penalty term of Akaike’s an SOLNN has a property of unidentiﬁable models at a view point
information criterion [Akaike, 1974].                 of the Bayesian learning methods, as shown in Fig. 3.