                       Forward Search Value Iteration For POMDPs

                   Guy Shani    and  Ronen I. Brafman    and  Solomon E. Shimony∗
              Department of Computer Science, Ben-Gurion University, Beer-Sheva, Israel


                    Abstract                            Heuristic Search Value Iteration (HSVI - [Smith and Sim-
                                                      mons, 2005]) is currently the most successful point-based al-
    Recent scaling up of POMDP solvers towards re-    gorithm for larger domains. HSVI maintains both a lower
    alistic applications is largely due to point-based bound (V) and an upper bound (V¯ ) over the optimal value
    methods which quickly converge to an approximate  function.¯ It traverses the belief space using a heuristic based
    solution for medium-sized problems. Of this family on both bounds and performs backups over the observed be-
    HSVI, which uses trial-based asynchronous value   lief points in reversed order. Unfortunately, using V¯ slows
    iteration, can handle the largest domains. In this down the algorithm considerably as updating V¯ and comput-
    paper we suggest a new algorithm, FSVI, that uses ing upper bound projections (V¯ (b)) are computationally in-
    the underlying MDP to traverse the belief space to- tensive. However, this heuristic pays off, as the overall per-
    wards rewards, ﬁnding sequences of useful back-   formance of HSVI is better than other methods.
    ups, and show how it scales up better than HSVI on  In this paper we suggest a new method for belief point se-
    larger benchmarks.                                lection, and for ordering backups which does not use an upper
                                                      bound. Rather, our algorithm uses a search heuristic based on
                                                      the underlying MDP. Using MDP information is a well known
1  Introduction                                       technique, especially as an initial guess for the value function
                                                      (starting with the Q  method of [Littman et al., 1995]),
Many interesting reinforcement learning (RL) problems can              MDP
                                                      but did not lead so far to very successful algorithms. Our
be modeled as partially observable Markov decision problems
                                                      novel algorithm — Forward Search Value Iteration (FSVI)
(POMDPs), yet POMDPs are frequently avoided due to the
                                                      —  traverses together both the belief space and the underlying
difﬁculty of computing an optimal policy. Research has fo-
                                                      MDP space. Actions are selected based on the optimal policy
cused on approximate methods for computing a policy (see
                                                      for the MDP, and are applied to the belief space as well, thus
e.g. [Pineau et al., 2003]). A standard way to deﬁne a policy
                                                      directing the simulation towards the rewards of the domain.
is through a value function that assigns a value to each be-
                                                      In this sense, our approach strongly utilizes the underlying
lief state, thereby also deﬁning a policy over the same belief
                                                      MDP policy.
space. Smallwood and Sondik [1973] show that a value func-
                                                        We tested our algorithm on all the domains on which HSVI
tion can be represented by a set of vectors and is therefore
                                                      was tested, and it both converges faster than HSVI and scales
piecewise linear and convex.
                                                      up better, allowing us to solve certain problems for which
  A promising approach for computing value functions is the HSVI fails to converge within reasonable time.
point-based method. A value function is computed over a
ﬁnite set of reachable belief points, using a backup operation
over single belief points - point-based backups, generating 2 Background and Related Work
α-vectors. The assumption is that the value function would We review the POMDP model and the associated notation,
generalize well to the entire belief space.           and provide a short overview of relevant methods.
  Two of the key issues in point-based methods are the 2.1 MDPs, POMDPs and the belief-space MDP
choice of the set of belief points and the order of backups. A
                                                                                                       
successful approach for addressing both of these issues uses A Markov Decision Process (MDP) is a tuple S, A, tr, R
trial-based value iteration — simulating trajectories and exe- where S is a set of world states, A is a set of actions,
                                                      tr(s, a, s) is the probability of transitioning from state s to
cuting backups over the encountered belief-states in reversed 
order (see e.g. [Bonet and Gefner, 1998]). Such methods use state s using action a,andR(s, a) deﬁnes the reward for ex-
some type of heuristic to ﬁnd useful trajectories.    ecuting action a in state s.
                                                        A   Partially Observable Markov  Decision Process
  ∗Partially supported by the Lynn and William Frankel Center for (POMDP) is a tuple S, A, tr, R, Ω,O,b0 where S, A, tr, R
Computer Sciences, and by the Paul Ivanier Center for Robotics and are the same as in an MDP, Ω is a set of observations and
Production Management at BGU.                         O(a, s, o) is the probability of observing o after executing a

                                                IJCAI-07
                                                  2619and reaching state s. The agent maintains a belief-state —a Algorithm 1 RTDP
vector b of probabilities such that b(s) is the probability that 1: Initialize Q(s, a) ← 0
the agent is currently at state s. b0 deﬁnes the initial belief 2: while true do
state — the agent belief over its initial state.             ←
                                                      3:   s   s0
  The transition from belief state b to belief state b using 4: while s is not a goal state do
action a is deterministic given an observation o and deﬁnes 5: for each a ∈ A do  
                                                                                                
the τ transition function. We denote b = τ(b, a, o) where: 6:   Q(s, a) ← R(s, a)+    tr(s, a ,s)V (s )
                                                                                  s
                                                     7:     a ← argmax   Q(s, a )
              O(a, s ,o)   b(s)tr(s, a, s )                            a
         b (s )=            s                   (1)    8:     s ←  sample from tr(s, a, ∗)
                         pr(o|b, a)
                       
                                        
      pr(o|b, a)=   b(s)   tr(s, a, s )O(a, s ,o) (2)
                                                      updated and then a new trial is executed. RTDP [Barto et al.,
                  s     s
                                                      1995] is a trial-based algorithm for MDPs (Algorithm 1).
  While the agent is unable to observe the true state of the
                                                        RTDP-BEL —    an adaptation of RTDP to POMDPs was
world, the world itself still behaves as an MDP, which we call
                                                      suggested by Bonet and Geffner [Bonet and Gefner, 1998].
the underlying MDP.
                                                      RTDP-BEL discretizes belief states and maintains Q-values
  In many MDP and POMDP examples the agent should ei-
                                                      for the discretized belief states only. Trials are executed over
ther reach some state (called the goal state) where it receives
                                                      the POMDP mapping the real belief state into the closest dis-
a reward, or collect rewards that can be found in a very small
                                                      cretized belief state which is then updated.
subset of the state space. Other problems provide varying re-
wards in each state.                                    An important aspect for the convergence speed of trial-
                                                      based algorithms is a good heuristic that leads towards im-
2.2  Value Functions for POMDPs                       portant states and updates. Unfocused heuristics may cause
                                                      the algorithm to spend much time updating useless states.
It is well known that the value function V for the belief-
                                               | |
space MDP can be represented as a ﬁnite collection of S - 2.4 Heuristic Search Value Iteration (HSVI)
dimensional vectors known as α vectors. Thus, V is both
piecewise linear and convex [Smallwood and Sondik, 1973]. Computing an optimal value function over the entire belief
A policy over the belief space is deﬁned by associating an ac- space does not seem to be a feasible approach. A possible
                          ·
tion a to each vector α,sothatα b = s α(s)b(s) represents approximation is to compute an optimal value function over
the value of taking a in belief state b and following the pol- a subset of the belief space. An optimal value function for a
icy afterwards. It is therefore standard practice to compute subset of the belief space is no more than an approximation
a value function — a set V of α vectors. The policy πV is of a full solution. We hope, however, that the computed value
                                                ·
immediately derivable using: πV (b) = argmaxa:αa∈V αa b. function will generalize well for unobserved belief states.
  V can be iteratively computed using point-based backup Point-based algorithms [Pineau et al., 2003; Spaan and
steps, efﬁciently implemented [Pineau et al., 2003] as: Vlassis, 2005; Smith and Simmons, 2005] compute a value
                                     b                function only over a reachable subset of the belief space us-
          backup(b) = argmax b : ∈ b · g        (3)
                           ga a A   a                ing point-based backups (Equations 3- 5).
           b                             α              Of this family of algorithms, HSVI (Algorithm 2) has
          g =  r + γ    argmax α  : ∈ b · g     (4)
           a   a               ga,o α V  a,o          shown the best performance over large scale problems. HSVI
                      o
                                                     [Smith and Simmons, 2005] maintains both an upper bound
           α                           i 
          ga,o(s)=    O(a, s ,o)tr(s, a, s )α (s ) (5) (V¯ ) and lower bound (V) over the value function. It traverses
                   s                                 the belief space following¯ the upper bound heuristic, greedily
                                                      selecting successor belief points where V¯ (b) − V(b) is max-
  The vector representation is suitable only for lower bounds.                                ¯
An upper bound can be represented as a direct mapping be- imal, until some stopping criteria have been reached. It then
tween belief states and values. The H operator, known as the executes backup and H operator updates over the observed
Bellman update, updates such a value function:        belief points on the explored path in reversed order.
                                                       HSVI can be viewed as a trial-based value iteration for
                ·             |
    QV (b, a)=b  ra + γ  o pr(o a, b)Vn(τ(b, a, o)) (6) POMDPs, even though the next belief state is not sampled
              HV  (b)=maxa  QV (b, a)           (7)   but selected using the upper and lower bounds.
                                                        Even though HSVI is able to handle large problems, the
2.3  Trial-Based Value Iteration                      H  operator update and the interpolations used to ﬁnd the
Synchronous value iteration assumes that the entire state value V¯ (b) are computationally intensive, becoming more
space is updated over each iteration. Asynchronous value it- time consuming with each explored belief state. As such,
eration allows some states to be updated more than others, HSVI spends much time maintaining the upper bound that
based on the assumption that value function accuracy is more is used only as a heuristic directing the belief space traversal.
crucial in these states. A well known form of asynchronous We note, however, that Smith and Simmons also present
value iteration is trial-based updates, where simulation trials theoretical results for the convergence of HSVI, relying on
are executed, creating trajectories of states (for MDPs) or be- the existence of the upper bound. As we suggest to remove
lief states (for POMDPs). Only the states in the trajectory are the upper bound, these theoretical results no longer apply.

                                                IJCAI-07
                                                  2620Algorithm 2 HSVI                                      Algorithm 3 FSVI
Function HSVI                                         Function FSVI
 1: Initialize V and V¯                                1: Initialize V
 2: while V¯ (b¯0) − V(b0) >do                        2: while V has not converged do
 3:  Explore(b0, V¯, V¯ )                              3:   Sample s0 from the b0 distribution
                ¯                                      4:   MDPExplore(b0,s0)
Function Explore(b, V, V¯ )
 1: if Stopping criteria¯ have not been reached then  Function MDPExplore(b, s)
      ∗                   
 2:  a  ←  argmax  Q ¯ (b, a ) (see Equation 6)        1: if s is not a goal state then
      ∗          a  V                                        ∗
 3:  o  ←  argmax (V¯ (τ(b, a, o)) − V(τ(b, a, o))     2:   a ←  argmax  Q(s, a)
                 o∗ ∗                                                  a      ∗
 4:  Explore(τ(b, a ,o ), V, V¯ ) ¯                    3:   Sample s from tr(s, a , ∗)
                                                                            ∗  
 5:  add(V,backup(b, V))¯                              4:   Sample o from O(a ,s, ∗)
                                                                            ∗     
 6:  V¯ ←¯HV  (b)    ¯                                 5:   MDPExplore(τ(b, a ,o),s)
                                                       6: add(V,backup(b, V ))
2.5  Using the underlying MDP
Using the underlying MDP optimal policy for the POMDP is belief state b0 to the goal (or towards rewards). As we assume
a very well known idea and has been explored from many as- that the value function of the underlying MDP is optimal, this
pects in the past. Littman et al. [Littman et al., 1995] suggest heuristic will lead the agent towards states where rewards can
to use the optimal Q-values of the underlying MDP to create be obtained.
the QMDP  value function for a POMDP:                   The trial is ended when the state of the underlying MDP is
                                                      a goal state. When the MDP does not have a goal state we
            QMDP   (b)=maxQ(s, a)b(s)           (8)
                         a                            can use other criteria such as reaching a predeﬁned sum of
                                                      rewards or number of actions. If the goal is unreachable from
Many grid-based techniques (e.g. [Zhou and Hansen, 2001]) some states we may also add a maximal number of steps after
initialize the upper bound over the value function using the which the trial ends.
underlying MDP. RTDP-BEL initializes a Q function for the FSVI (Algorithm 3) is apparently very simple. Its simplic-
POMDP using the optimal Q function for the MDP.       ity translates into increased speed and efﬁciency in generating
  An important drawback of using the underlying MDP is belief points and updating the values associated with belief
the inability of the MDP to assess actions that gather informa- points. FSVI’s method for selecting the next action is very
tion. Agents in a partially observable environment occasion- fast, requiring to check only O(|A|) values (MDP Q-values)
ally need to execute actions that do not move them towards a as opposed to any action selection method based on the cur-
reward, but only improve their state of information about the rent belief state. For example, RTDP-BEL takes O(|S|) op-
current world state, such as activating a sensor. In an MDP erations for discretizing the belief state before it can select
the world state is always known and therefore information the next action and HSVI needs O(|A||O||S||V¯ |) operations,
gathering actions produce no additional value. Agents rely- where |V¯ | is the number of points in the upper bound, to com-
ing on the QMDP heuristic, for example, will therefore never pute the values of all the successors of the current belief state.
execute any such actions.                             As FSVI generates trajectories using forward projection, it is
                                                      easy to determine good sequences of backups, simply going
3  Forward Search Value Iteration                     in reversed order. This ability is shared by HSVI, but not
We propose a new algorithm, Forward Search Value Itera- by other point-based algorithms such as Perseus [Spaan and
tion (FSVI), using trial-based updates over the belief space of Vlassis, 2005] and PBVI [Pineau et al., 2003].
the POMDP. FSVI maintains a value function using α-vectors Other algorithms, such as HSVI and RTDP-BEL, use a
and updates it using point-based backups.             heuristic that is initialized based on the MDP Q-function and
  The heuristic FSVI uses to traverse the belief space is based use some form of interpolation over these Q-values. These al-
on the optimal solution to the underlying MDP. We assume gorithms also improve the heuristic by updating the Q-values
that such a solution is given as input to FSVI in the form of to ﬁt the POMDP values. Such algorithms therefore work ini-
a Q-function over MDP states. This assumption is reason- tially much like the QMDP heuristic which is known to pre-
able, as a solution to the underlying MDP is always simpler form badly for many POMDP problems and in many cases
to compute than a solution to the POMDP.              gets stuck in local optima. These algorithms can potentially
  FSVI (Algorithm 3) simulates an interaction of the agent need many updates to improve the heuristic to be able to reach
with the environment, maintaining both the POMDP belief rewards or goal states.
state b and the underlying MDP state s — the true state of the As noted earlier, a major drawback of MDP based ap-
environment the agent is at within the simulation. While at proaches is their inability to perform in information gather-
policy execution time the agent is unaware of s, in simulation ing tasks. FSVI is slightly different in that respect. FSVI
we may use s to guide exploration through the environment. uses point-based backups in which information gathering ac-
  FSVI uses the MDP state to decide which action to apply tions are also evaluated and considered. It is therefore able
next based on the optimal value function for the underlying to perform single step information gathering actions such as
MDP, thus providing a path in belief space from the initial the activation of a sensor. For example, in the RockSample

                                                IJCAI-07
                                                  2621domain [Smith and Simmons, 2005], the robot should acti-       11
vate a sensor to discover whether a rock is worthwhile, and    10
indeed FSVI performs very well in the RockSample domain,        9
executing such actions. However, when the information gath-     8
ering requires a lengthy sequence of operations, such as in     7
the heaven-hell problem [Bonet and Gefner, 1998] where the      6
                                                                5
agent is required to pass a corridor to read a map and then      0     50   100   150   200   250
return to take a reward, FSVI’s heuristic will fail to lead it               FSVI HSVI
through the corridor, and therefore it cannot learn of the ex-                (a)
istence of the map. FSVI can learn to read the map (using
an information gathering action) if it is on the path from an  21
initial state to the goal.                                     19
  This limitation can be removed by adding an exploration      17
factor causing FSVI to occasionally choose a non-optimal ac-   15

tion. In practice, however, it is unlikely that random explo-  13

ration will rapidly lead towards meaningful trajectories.      11
                                                                 0      100    200    300     400
4  Empirical Evaluations                                                     FSVI HSVI
                                                                              (b)
4.1  Evaluation Metrics

We evaluate the following criteria:                            19
  Value function evaluation — Average discounted reward        17
       P#trials P#steps j                                      15
         i=0    j=0  γ rj                                      13
(ADR):       #trials    . ADR is ﬁltered using a ﬁrst          11
order ﬁlter to reduce the noise in ADR.                         9
                                                                7
  Execution time — we report the CPU time but as this is        5
an implementation speciﬁc measurement, we also report the        0      100    200    300     400
amount of basic operations such as backup, τ function, dot                   FSVI HSVI
         ·       α
product (α b)andga,o (Equation 5) computations .                              (c)
  Memory —   we show the size of the computed value func-
tion and the number of maintained belief points.      Figure 1: Convergence on the Noisy Rock Sample 5,5 prob-
4.2  Experimental Setup                               lem (a), the Rock Sample 7,8 problem (b) and the Rock Sam-
                                                      ple 8,8 problem (c). The X axis shows CPU time and the Y
As HSVI is known to perform better than other point-based axis shows ADR.
algorithms such as Perseus [Spaan and Vlassis, 2005] and
PBVI  [Pineau et al., 2003], we compare FSVI only to                                          20
HSVI. Comparison is done on a number of benchmarks
from the point-based literature: Hallway, Hallway2 [Littman                                   15
et al., 1995],TagAvoid[Pineau et al., 2003], RockSample                                       10
[Smith and Simmons, 2005] and Network Ring [Poupart and                                       5
Boutilier, 2003]. These include all the scalability domains on
                                                                                              0
which HSVI was tested in [Smith and Simmons, 2005],ex-            4,4   5,5   5,7  7,8   8,8
cept for Rock Sample 10,10 which could not be loaded on our                FSVI    HSVI
system due to insufﬁcient memory. Table 1 contains the prob-
lem measurements for the benchmarks including the size of Figure 2: Normalized comparison of CPU time for the Rock
the state space, action space and observation space, the num- Sample problems.
ber of trials used to evaluate the solutions, and the error in
measuring the ADR for each problem.
  The Rock Sample domain provides an opportunity for test- forth. All experiments were executed on identical machines:
ing the scalability of different algorithms. However, these x86 64-bit machines, dual-proc, 2.6Ghz CPU, 4Gb memory,
problems are somewhat limited: they assume deterministic 2Mb cache, running linux and JRE 1.5.
state transitions as well as full observability for the robot loca- We focus our attention on convergence speed of the value
tion, making the problems easier to solve. To overcome these function to previously reported ADR. We executed HSVI and
limitations we added a new domain — Noisy Rock Sample, FSVI, interrupting them from time to time to compute the ef-
in which agent movements are stochastic and it receives noisy ﬁciency of the current value function using ADR. Once the
observations as to its current location.              ﬁltered ADR has reached the same level as reported in past
  We implemented in Java a standard framework that incor- publications, execution was stopped. The reported ADR was
porated all the basic operators used by all algorithms such then measured over additional trials (number of trials and er-
as vector dot products, backup operations, τ function and so ror in measurement is reported in Table 1).

                                                IJCAI-07
                                                  2622                                                                       α                                             ¯       ¯              ¯
  Method          ADR           |V |     Time     # Backups         #ga,o          |B|          #τ      # α · b     |V |     V (b)      #HV
                                                                         6           4           3            6       3          3
                                         (secs)                      x 10        x 10        x 10        x 10      10         10
  Hallway                                             187ms         0.13μs                  0.37μs         33ns              0.5μs       65ms
  HSVI            0.516         182        314           634          5.85          3.4       34.52        6.67    0.42      106.1       1268
  FSVI            0.517         233         50           655          7.71        0.05         0.51        7.78
              ±0.0024          ±71        ±15          ±100        ±2.46        ±0.01       ±0.08       ±2.49
  Hallway2                                            222ms         0.17μs                     1μs         36ns               1μs       126ms
  HSVI            0.341         172         99           217          1.56        2.11        11.07        1.81    0.23       37.2         434
  FSVI            0.345         296         49           355          4.28        0.03          0.3        4.33
              ±0.0027          ±22         ±8           ±33        ±0.73           ±0       ±0.03       ±0.74
  Tag Avoid                                           311ms         0.92μs                  0.56μs        8.3ns              0.6μs     24.1ms
  HSVI           -6.418         100         52           304           0.5        0.29         1.74        0.53     1.1       29.3       1635
  FSVI           -6.612         174         45           182          0.37        0.01         0.14        0.39
                ±0.15          ±25         ±8           ±27          ±0.1          ±0       ±0.02       ±0.11
  Rock Sample 4,4                                       92ms         0.2μs                  0.46μs     0.031μs               0.2μs     4.67ms
  HSVI           18.036         123          4           207          1.08          0.1        1.17        1.09    0.34       6.06         414
  FSVI           18.029          84          1           204          0.24           0         0.04        0.25
               ±0.024          ±76         ±1          ±122        ±0.41           ±0       ±0.01       ±0.42
  Rock Sample 5,5                                     114ms          1.1μs                   2.5μs     0.033μs             0.64μs      24.1ms
  HSVI            18.74         348         85          2309         10.39        0.26         2.34        10.5     0.8      101.1       1883
  FSVI           19.206       272.5       11.1         626.2          1.47        0.02          0.1        1.49
               ±0.063          ±75         ±5          ±247        ±0.88           ±0       ±0.02       ±0.89
  Noisy Rock Sample 5,5                               314ms         0.86μs                   1.9μs     0.035μs             0.69μs      44.5ms
  HSVI           10.318        2639       1586          3528        129.11        2.01        23.05       132.4    0.88      264.9       9294
  FSVI           10.382         924        210          1153         11.93        0.48         0.53       12.09
               ±0.069        ±170         ±52          ±224        ±4.79        ±0.12       ±0.14         ±4.8
  Rock Sample 5,7                                     143ms          3.7μs                  26.6μs      0.11μs               3.0μs    256.6ms
  HSVI            22.67         274        205           350          0.65          1.0         4.3        0.99    3.44      14.09         702
  FSVI            22.82       306.9       34.3           500        39684         3722          0.4         2.1
                ±0.63       ±91.5      ±13.6        ±400.1         ±0.02      ±0.014       ±0.001        ±2.9
  Rock Sample 7,8                                     567ms          20μs                   0.25ms        1.1μs              15μs       3.4sec
  HSVI           20.029         533       3045          2089         14.51          1.5         4.1       14.66    3.42       9.53         628
  FSVI           20.369       343.1        239         512.1         2.389       0.049        0.024       2.457
               ±0.265      ±146.6      ±78.7        ±284.8         ±2.32      ±0.059       ±0.013      ±2.357
  Rock Sample 8,8                                     570ms          25μs                   0.35ms        2.6μs              18μs       2.7sec
  HSVI           19.474         762     13917           1317         10.83        4.01        58.09       11.43    17.3      58.38       2638
  FSVI           19.646       261.4        783         367.8         0.042       0.816        0.176       1.183
               ±0.337       ±76.9       ±295        ±125.1         ±0.02      ±0.013        ±0.06      ±0.661
  Network Ring 8                                      164ms            1μs                   2.6ms        4.5ns              11μs      31.8ms
  HSVI            42.27          80         19           153         0.004        0.25         8.44        0.31    0.39       8.46         307
  FSVI            42.39        40.5       6.75           252         0.004       0.022         0.24        0.19
                ±0.18          ±16       ±6.1          ±146       ±0.002      ±0.012        ±0.14       ±0.19
  Network Ring 10                                     553ms         10.6μs                  13.3ms       23.3ns            99.4μs       369ms
  HSVI            51.43          69        141           103        0.0036        0.29          6.9       0.144    1.11        6.9         206
  FSVI            51.44       33.25         47         267.7         0.002       0.025        0.255        0.19
                ±0.03       ±6.14      ±14.3        ±85.78       ±0.0004      ±0.008       ±0.081      ±0.086

            Table 2: Performance measurements. Model rows show rough estimates of basic operations execution time.

  Problem             |S|       |A|     |O|     #trials   ADR Error                      6. Number of computed belief states.
                                                          ±0   0015
  Hallway             61        5       21      10,000        .                          7.    function computations count.
  Hallway2            93        5       17      10,000    ±0.004                             τ
  Tag Avoid           870       5       30      10,000    ±0.045                         8. Dot product operations count.
  Rock Sample 4,4     257       9       2       10,000    ±0.075                         9. Number of points in the upper bound (|V¯               |).
  Rock Sample 5,5     801       10      10      10,000    ±0.3                           10. Upper bound projection computations count (V¯                   (b)).
  Noisy RS 5,5        801       10      27      10,000    ±0.3                                                                                  ¯
  Rock Sample 5,7     3,201     12      2       10,000    ±0.25                          11. Upper bound value updates count (HV                  (b)).
  Rock Sample 7,8     12,545    13      2       1,000     ±0.25                        The last   3  items refer only to HSVI as FSVI does not main-
  Rock Sample 8,8     16,385    13      2       1,000     ±0.25                        tain an upper bound. The reported numbers do not include
  Network Ring 8      256       17      2       2,000     ±1.1
  Network Ring 10     1024      21      2       2,000     ±0.98                        the repeated expensive computation of the ADR, or the ini-
                                                                                       tialization time (identical for both algorithms).
             Table 1: Benchmark problem parameters                                        To illustrate the convergence rate of each algorithm, we
                                                                                       also plotted the ADR as a function of CPU time in Figure 1.
                                                                                       These graphs contain data collected over separate executions
4.3     Results                                                                        with fewer trials, so Table 2 is more accurate.
Table 2 presents our experimental results. For each problem
and method we report:                                                                  4.4     Discussion
  1. Resulting ADR.                                                                    Our results clearly indicate that FSVI converges faster than
  2. Size of the ﬁnal value function (|V            |).                                HSVI and scales up better. The convergence rate shown in
  3. CPU time until convergence.                                                       Figure 1 is always faster, the time required to reach optimal
  4. Backups count.                                                                    ADR is also always faster, and this difference become more
       α
  5.  ga,o  operations count.                                                          pronounced as problem size increases. Figure 2 presents a

                                                                             IJCAI-07
                                                                                2623