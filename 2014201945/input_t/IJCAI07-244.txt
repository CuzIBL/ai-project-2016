                                   Information-Based Agency

                       Carles Sierra                              John Debenham
      Institut d’Investigacio en Intel.ligencia Artiﬁcial Faculty of Information Technology
        Spanish Scientiﬁc Research Council, UAB           University of Technology, Sydney
             08193 Bellaterra, Catalonia, Spain                    NSW, Australia
                     sierra@iiia.csic.es                      debenham@it.uts.edu.au

                    Abstract                          needs into goals and these into plans composed of actions.
                                                      Second, a reactive machinery, that uses the received messages
    Successful negotiators look beyond a purely util- to obtain a new world model by updating the probability dis-
    itarian view. We propose a new agent architec-    tributions in it. Agents summarise their world models using
    ture that integrates the utilitarian, information, and a number of measures (e.g. trust, reputation, and reliability
    semantic views allowing the deﬁnition of strate-  [Sierra and Debenham, 2006]) that can then be used to de-
    gies that take these three dimensions into account. ﬁne strategies for “exchanging information” — in the sense
    Information-based agency values the information   developed here, this is the only thing that an agent can do.
    in dialogues in the context of a communication lan- We introduce the communication language in Section 2,
    guage based on a structured ontology and on the no- the agent architecture in Section 3, some summary measures
    tion of commitment. This abstraction uniﬁes mea-  based on the architecture in Section 4, and some associated
    sures such as trust, reputation, and reliability in a interaction strategies in Section 5.
    single framework.
                                                      2   The Multiagent System
1  Introduction                                       Our agent α has two languages: C is an illocutionary-based
In this paper we introduce an agency framework grounded language for communication, and L is a language for internal
on information-based concepts [MacKay, 2003]. It presents representation. C is described following, and L in Section 3.
an agent architecture that admits a game-theoretical reading
and an information-theoretical reading. And, what is more 2.1 Communication Language  C
interesting, permits a connection between these two worlds The shape of the language that α uses to represent the infor-
by considering negotiation as both cooperative, in the sense mation received and the content of its dialogues depends on
that all interaction involves the exchange of information, and two fundamental notions. First, when agents interact within
competitive, in the sense that agents aim at getting the best an overarching institution they explicitly or implicitly accept
they can. This approach contrasts with previous work on the the norms that will constrain their behaviour, and accept the
design of negotiation strategies that did not take information established sanctions and penalties whenever norms are vi-
exchange into account, but focused on the similarity of of- olated. Second, the dialogues in which α engages are built
fers [Jennings et al., 2001; Faratin et al., 2003], game theory around two fundamental actions: (i) passing information, and
[Rosenschein and Zlotkin, 1994], or ﬁrst-order logic [Kraus, (ii) exchanging proposals and contracts. A contract δ =(a, b)
1997].                                                between agents α and β is a pair where a and b represent the
  We     assume    that   a     multiagent   system   activities that agents α and β are respectively responsible for.
{α, β1,...,βo,ξ,θ1,...,θt}, contains an agent α that  Contracts signed by agents and information passed by agents,
interacts with negotiating agents, βi, information providing are similar to norms in the sense that they oblige agents to be-
agents, θj, and an institutional agent, ξ, that represents have in a particular way, so as to satisfy the conditions of the
the institution where we assume the interactions happen contract, or to make the world consistent with the information
[Arcos et al., 2005]. Institutions give a normative context to passed. Contracts and Information can then be thought of as
interactions that simplify matters (e.g an agent can’t make normative statements that restrict an agent’s behaviour.
an offer, have it accepted, and then renege on it). We will Norms, contracts, and information have an obvious tempo-
describe a communication language C based on an ontology ral dimension. Thus, an agent has to abide by a norm while
that will permit us both to structure the dialogues and to it is inside an institution, a contract has a validity period, and
structure the processing of the information gathered by a piece of information is true only during an interval in time.
agents. Agents have an internal language L used to build a The set of norms affecting the behaviour of an agent deﬁne
probabilistic world model.                            the context that the agent has to take into account.
  We understand agents as being built on top of two basic The communication language that α needs requires two
functionalities. First, a proactive machinery, that transforms fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ,

                                                IJCAI-07
                                                  1513what is the world α aims at bringing about and that β has distance in the ≤ graph means less semantic similarity), and
the right to verify, complain about or claim compensation the depth of the subsumer concept (common ancestor) in the
for any deviations from, and Done(a) to represent the event shortest path between the two concepts (the deeper in the hi-
that a certain action a1 has taken place. In this way, norms, erarchy, the closer the meaning of the concepts). Semantic
contracts, and information chunks will be represented as in- similarity could then be deﬁned as:
stances of Commit(·) where α and β can be individual agents
                                                                                   κ2h    −κ2h
or institutions, C is:                                                      −κ l e    − e
                                                                 Sim(c, c )=e  1 ·
                                                                                  eκ2h + e−κ2h
       a ::= illoc(α, β, ϕ, t) | a; a |
                                                      where l is the length (i.e. number of hops) of the shortest
            Let context In End
                         a                            path between the concepts, h is the depth of the deepest con-
                 |        |             |  ∧   |
      ϕ ::= term  Done(a)  Commit(α, ϕ)  ϕ   ϕ        cept subsuming both concepts, and κ1 and κ2 are parameters
            ϕ ∨ ϕ |¬ϕ |∀v.ϕv |∃v.ϕv                   scaling the contribution of shortest path length and depth re-
  context ::= ϕ | id = ϕ | prolog clause | context; context spectively.
                                                        Given a formula ϕ ∈Cin the communication language we
where ϕv is a formula with free variable v, illoc is any ap- deﬁne the vocabulary or ontological context of the formula,
propriate set of illocutionary particles, ‘;’ means sequencing, O(ϕ), as the set of concepts in the ontology used in it. Thus,
and context represents either previous agreements, previous we extend the previous deﬁnition of similarity to sets of con-
illocutions, or code that aligns the ontological differences be- cepts in the following way:
tween the speakers needed to interpret an action a.
                                                             Sim(ϕ, ψ)=   max    min  {Sim(ci,cj)}
  For example, we can represent the following offer: “If you             ci∈O(ϕ) cj ∈O(ψ)
spend a total of more than e100 in my shop during October
then I will give you a 10% discount on all goods in Novem- The following does not depend on this particular deﬁnition.
ber”, as:
Offer( α, β,spent(β, α, October,X)∧ X ≥ e100 →        3   Agent Architecture
     ∀ y. Done(Inform(ξ, α, pay(β, α, y), November)) → Agent α receives all messages expressed in C in an in-box X
         Commit(α, β, discount(y,10%)))               where they are time-stamped and sourced-stamped. A mes-
                                                      sage μ from agent β (or θ or ξ) is then moved from X to a
Note the use of the institution agent ξ to report the payment.         t
                                                      percept repository Y where it is appended with a subjective
  In order to deﬁne the language introduced above that               t
                                                      belief function R (α, β, μ) that normally decays with time.
structures agent dialogues we need an ontology that in-
                                                      α acts in response to a message that expresses a need.A
cludes a (minimum) repertoire of elements: a set of con-
                                                      need may be exogenous such as a need to trade proﬁtably and
cepts (e.g. quantity, quality, material) organised in a is-a
                                                      may be triggered by another agent offering to trade, or en-
hierarchy (e.g. platypus is a mammal, australian-dollar is
                                                      dogenous such as α deciding that it owns more wine than it
a currency), and a set of relations over these concepts (e.g.
                                                      requires. Needs trigger α’s goal/plan proactive reasoning de-
price(beer,AUD)).2 We model ontologies following an alge-
                                                      scribed in Section 3.1, other messages are dealt with by α’s
braic approach [Kalfoglou and Schorlemmer, 2003] as:
                                                      reactive reasoning described in Section 3.2.
  An ontology is a tuple O =(C, R, ≤,σ) where C is a ﬁ-
nite set of concept symbols (including basic data types), R 3.1 Proactive Reasoning
is a ﬁnite set of relation symbols, ≤ is a reﬂexive, transi-
tive and anti-symmetric relation on C (a partial order), and α’s goal/plan machinery operates in a climate of changing un-
σ : R → C+  is the function assigning to each relation sym- certainty and so typically will pursue multiple sub-goals con-
bol its arity. ≤ is a traditional is-a hierarchy, and R contains currently. This applies both to negotiations with a potential
relations between the concepts in the hierarchy.      trading partner, and to interaction with information sources
  The concepts within an ontology are closer, semantically that either may be unreliable or may take an unpredictable
                                                      time to respond. Each plan contains constructors for a world
speaking, depending on how far away are they in the structure t
            ≤                                         model M   that consists of probability distributions, (Xi),in
deﬁned by the  relation. Semantic distance plays a funda-                      L Mt
mental role in strategies for information-based agency. How ﬁrst-order probabilistic logic . is then maintained from
                                                      percepts received using update functions that transform per-
signed contracts, Commit(·) about objects in a particular se-                t
                                   ·                  cepts into constraints on M — described in Section 3.2.
mantic region, and their execution Done( ), affect our deci-             Mt
sion making process about signing future contracts on nearby The distributions in are determined by α’s plans that
semantic regions is crucial to model the common sense that are determined by its needs. If α is negotiating some con-
                                                      tract δ in satisfaction of need χ then it may require the dis-
human beings apply in managing trading relationships. A       Pt
measure [Li et al., 2003] bases the semantic similarity be- tribution (eval(α, β, χ, δ)=ei) where for a particular δ,
tween two concepts on the path length induced by ≤ (more eval(α, β, χ, δ) is an evaluation over some complete and dis-
                                                      joint evaluation space E =(e1,...,en) that may contain
  1Without loss of generality we will assume that all actions are hard (possibly utilitarian) values, or fuzzy values such as “re-
dialogical.                                           ject” and “accept”. This distribution assists α’s strategies to
  2Usually, a set of axioms deﬁned over the concepts and relations decide whether to accept a proposed contract leading to a
                                                                             t
is also required. We will omit this here.             probability of acceptance P (acc(α, β, χ, δ)). For example,

                                                IJCAI-07
                                                  1514 t                                                                         Xi
P (eval(α, β, χ, δ)=ei) could be derived from the subjec- satisﬁes the constraints Js (μ). Then let q(μ) be the distribu-
            t
tive estimate P (satisfy(α, β, χ, δ)=fj) of the expected ex- tion:
tent to which the execution of δ by β will satisfy χ, and an     t                     t
                 t                                       q(μ) = R (α, β, μ) × p(μ) +(1− R (α, β, μ)) × p (2)
objective estimate P (val(α, β, δ)=gk) of the expected val-
uation of the execution of δ possibly in utilitarian terms. This and then let:
second estimate could be derived by proactive reference to         
                                                         t          q(μ) if q(μ) is more interesting than p
the {θi} for market data. In a negotiation α’s plans may also P (Xi(μ))=                              (3)
construct the distribution Pt(acc(β,α,δ)) that estimates the        p    otherwise
probability that β would accept δ — we show in Section 3.2
                                                      A general measure of whether q(μ) is more interesting than
how α may derive this estimate from the information in β’s  K    D          K  D             K   
                                                      p is: (q(μ) (Xi)) >   (p (Xi)), where  (x y)=
proposals.                                                    xj
                                                          xj ln  is the Kullback-Leibler distance between two
  α’s plans may construct various other distributions such j  yj
    t
as: P (trade(α, β, o)=ei) that β is a good person to sign probability distributions x and y.
                             t
contracts with in context o, and P (conﬁde(α, β, o)=fj) Finally merging Eqn. 3 and Eqn. 1 we obtain the method
that α can trust β with conﬁdential information in context o. for updating a distribution Xi on receipt of a message μ:
  The integrity of percepts decreases in time. α may have         t+1                 t
                                                                 P   (Xi)=Δi(D(Xi),  P (Xi(μ)))       (4)
background knowledge concerning the expected integrity of
a percept as t →∞. Such background knowledge is repre- This procedure deals with integrity decay, and with two prob-
sented as a decay limit distribution. If the background knowl- abilities: ﬁrst, the probability z in the percept μ, and second
                                                                t
edge is incomplete then one possibility is for α to assume that the belief R (α, β, μ) that α attached to μ.
the decay limit distribution has maximum entropy whilst be- In a simple multi-issue contract negotiation α may esti-
                                                            t
ing consistent with the data. Given a distribution, P(Xi), and mate P (acc(β,α,δ)), the probability that β would accept
a decay limit distribution D(Xi), P(Xi) decays by:    δ, by observing β’s responses. Using shorthand notation, if
             t+1                 t                    β sends the message Oﬀer(δ1) then α may derive the con-
           P   (Xi)=Δi(D(Xi),   P (Xi))         (1)           acc(β,α,δ)             t
                                                      straint: J      (Oﬀer(δ1)) = {P (acc(β,α,δ1)) = 1},
where Δi is the decay function for the Xi satisfying the prop- and if this is a counter offer to a former offer of α’s, δ0,
                t                                           acc(β,α,δ)             t
erty that limt→∞ P (Xi)=D(Xi). For example, Δi could  then: J       (Oﬀer(δ1)) = {P (acc(β,α,δ0)) = 0}.In
         t+1                            t
be linear: P (Xi)=(1−νi)×D(Xi)+νi    ×P (Xi), where   the not-atypical special case of multi-issue bargaining where
νi < 1 is the decay rate for the i’th distribution. Either the the agents’ preferences over the individual issues only are
decay function or the decay limit distribution could also be a known and are complementary to each other’s, maximum en-
                t     t
function of time: Δi and D (Xi).                      tropy reasoning can be applied to estimate the probability
                                                      that any multi-issue δ will be acceptable to β by enumerating
3.2  Reactive Reasoning                               the possible worlds that represent ’s “limit of acceptability”
                                                  t                               β
In the absence of in-coming messages the integrity of M [Debenham, 2004].
decays by Eqn. 1. The following procedure updates Mt for
all percepts expressed in C. Suppose that α receives a mes- 3.3 Reliability
sage μ from agent β at time t. Suppose that this message Rt(α, β, μ) is an epistemic probability that takes account of
states that something is so with probability z, and suppose                                    Rt
                               t                      α’s personal caution. An empirical estimate of (α, β, μ)
that α attaches an epistemic belief R (α, β, μ) to μ — this may be obtained by measuring the ‘difference’ between com-
probability reﬂects α’s level of personal caution. Each of α’s mitment and enactment. Suppose that μ is received from
active plans, s, contains constructors for a set of distributions agent β at time u and is veriﬁed by ξ as μ at some later time t.
{  }∈Mt                                         ·                     u
 Xi       together with associated update functions, Js( ), Denote the prior P (Xi) by p. Let p(μ) be the posterior min-
         Xi
such that Js (μ) is a set of linear constraints on the posterior imum relative entropy distribution subject to the constraints
                                                       Xi                                          Xi  
distribution for Xi. Examples of these update functions are Js (μ), and let p(μ) be that distribution subject to Js (μ ).
                                          Pt                                u
given in Section 3.4. Denote the prior distribution (Xi) by We now estimate what R (α, β, μ) should have been in the
p, and let p(μ) be the distribution with minimum relative en- light of knowing now, at time t, that μ should have been μ.
    3                                        rj                                 t
tropy with respect to p: p(μ) =argminr j rj log p that The idea of Eqn. 2, is that R (α, β, μ) should be such that,
                                              j                          t
                                                      on average across M , q(μ) will predict p(μ) — no matter
  3
   Given a probability distribution q, the minimum relative entropy whether or not μ was used to update the distribution for Xi,as
distribution p =(p1,...,pI ) subject to a set of J linear constraints determined by the condition in Eqn. 3 at time u. The observed
g = {gj (p)=Paj · p − cj =0},j =1,...,J (thatP must include                      Rt         | 
                                                rj    reliability for μ and distribution Xi, Xi (α, β, μ) μ , on the
the constraint pi − 1=0) is: p = arg minr rj log .                                 
             i                            j     qj    basis of the veriﬁcation of μ with μ , is the value of k that
                                                 
This may beP calculated by introducing Lagrange multipliers λ: minimises the Kullback-Leibler distance:
                 pj                    ∂L
           j         ·             {       j
L(p, λ)= j p log qj + λ g. Minimising L, ∂λj = g (p)= t         
                                                      RX  (α, β, μ)|μ =argminK(k  · p(μ) +(1− k) · p  p(μ))
0},j =1,...,J is the set of given constraints g, and a solution to i      k
∂L =0,i  =1,...,I leads eventually to p. Entropy-based infer-
∂pi                                                   The predicted information in the enactment of μ with respect
ence is a form of Bayesian inference that is convenient when the data to Xi is:
is sparse [Cheeseman and Stutz, 2004] and encapsulates common-
                                                                It           Ht     − Ht
sense reasoning [Paris, 1999].                                   Xi (α, β, μ)=  (Xi)    (Xi(μ))       (5)

                                                IJCAI-07
                                                  1515                                                                                                     
that is the reduction in uncertainty in Xi where H(·) is Shan- The Sim(·) method. Given the observation μ =(φ ,φ),
                                          t
non entropy. Eqn. 5 takes account of the value of R (α, β, μ). deﬁne the vector t by
  If X(μ) is the set of distributions that μ affects, then the t                                    
                                                      ti = P (ϕi|ϕ)+(1−|Sim(φ  ,φ)−Sim(ϕi,ϕ)  |)·Sim(ϕ ,φ)
observed reliability of β on the basis of the veriﬁcation of μ
with μ is:                                           for i =1,...,m. t is not a probability distribution. The mul-
                                                                       
        t             1        t                    tiplying factor Sim(ϕ ,φ) limits the variation of probability
      R (α, β, μ)|μ =          R  (α, β, μ)|μ   (6)
                     |X   |     Xi                    to those formulae whose ontological context is not too far
                       (μ)   i
                                                      away from the observation. The posterior p(φ,φ) is deﬁned to
If X(μ) are independent the predicted information in μ is:                
                                                     be the normalisation of t.
           It                It
            (α, β, μ)=        Xi (α, β, μ)      (7)
                      Xi∈X(μ)                         The valuation method.  α may wish to value ϕ in some
                                                      sense. This value will depend on the future use that α makes
Suppose α sends message μ to β where μ is α’s private infor- of it. So α estimates the value of ϕ using a probability
mation, then assuming that β’s reasoning apparatus mirrors
                                                      distribution (p1,...,pn) over some evaluation space E =
α’s, α can estimate It(β,α,μ).
                                                      (e1,...,en). pi = wi(ϕ) is the probability that ei is the cor-
  For each formula ϕ at time t when μ has been veriﬁed with                                             n
                                                     rect evaluation of the enactment ϕ, and w : L×L→[0, 1]
μ , the observed reliability that α has for agent β in ϕ is: is the evaluation function.
      t+1                    t                                          t             t
     R   (α, β, ϕ)=(1− ν) × R (α, β, ϕ)+                For a given ϕk, (P (ϕ1|ϕk),...,P (ϕm|ϕk)) is the prior
                        t         
                   ν × R (α, β, μ)|μ × Sim(ϕ, μ)      distribution of α’s estimate of what will be observed if β com-
                                                      mitted to ϕk, and w(ϕk)=(w1(ϕk),...,wn(ϕk))  is α’s
where Sim measures the semantic distance between two sec- evaluation over E of β’s commitment ϕk. α’s expected eval-
tions of the ontology as introduced in Section 2.1, and ν is the uation of what will be observed that β has committed to ϕk
                                                          exp
learning rate. Over time, α notes the context of the various μ is w (ϕk):
received from β, and over the various contexts calculates the               m
                 t                                                         
relative frequency, P (μ). This leads to an overall expectation  exp            t
                                                                w   (ϕk)i =    P (ϕj|ϕk) · wi(ϕj)
of the reliability that agent α has for agent β:
                                                                          j=1
          Rt           Pt    × Rt
            (α, β)=      (μ)     (α, β, μ)            for i =1,...,n. Now suppose that α observes the enactment
                     μ                                φ of another commitment φ also by agent β. Eg: α may buy
                                                      wine and cheese from the same supplier. α may wish to revise
3.4  Commitment and Enactment                                                                exp
                                                      the prior estimate of the expected valuation w (ϕk) in the
The interaction between agents α and β will involve β mak- light of the observation (φ,φ) to:
ing contractual commitments and (perhaps implicitly) com- rev      
mitting to the truth of information exchanged. No matter (w (ϕk) | (φ |φ)) =
                                                             exp                                     
what these commitments are, α will be interested in any  g(w (ϕk), Sim(φ ,φ), Sim(ϕ, φ),w(ϕ),w(φ),w(φ ))
variation between β’s commitment, ϕ, and what is actually
observed (as advised by the institution agent ξ), as the en- for some function g — the idea being, for example, that if the
actment, ϕ. We denote the relationship between commit- commitment, φ, a contract for cheese was not kept then α’s
ment and enactment, Pt(Observe(ϕ)|Commit(ϕ)) simply  expectation that the commitment, ϕ, a contract for wine will
as Pt(ϕ|ϕ) ∈Mt.                                      not be kept should increase. We estimate the posterior p(φ,φ)
                                                      by applying the principle of minimum relative entropy as in
  In the absence of in-coming messages the conditional prob-                              m
abilities, Pt(ϕ|ϕ), should tend to ignorance as represented Eqn. 4 with prior p and p(φ,φ) =(p(φ,φ)j)j=1 satisfying the
by the decay limit distribution and Eqn. 1. We now show n constraints:
                                 t                                    m
how Eqn. 4 may be used to revise P (ϕ |ϕ) as observa-  (ϕ|ϕ)  
                                                                                 ·
tions are made. Let the set of possible enactments be Φ= J  ((φ ,φ)) =     p(φ ,φ)j wi(ϕj)=
                                       t                               j=1
{ϕ1,ϕ2,...,ϕm} with prior distribution p = P (ϕ |ϕ). Sup-
                                                                                                      n
pose that message μ is received, we estimate the posterior exp                                     
            m     t+1                                gi(w  (ϕk), Sim(φ ,φ), Sim(ϕ, φ),w(ϕ),w(φ),w(φ ))
p(μ) =(p(μ)i)i=1 = P (ϕ |ϕ).                                                                          i=1
  First, if μ =(ϕk,ϕ) is observed then α may use this ob- This is a set of n linear equations in m unknowns, and so

servation to estimate p(ϕk)k as some value d at time t +1. the calculation of the minimum relative entropy distribution

We estimate the distribution p(ϕk) by applying the principle may be impossible if n>m. In this case, we take only
of minimum relative entropy as in Eqn. 4 with prior p, and the m equations for which the change from the prior to the
                          m
the posterior p(ϕk) =(p(ϕk)j)j=1 satisfying the single con- posterior value is greatest. That is, we attempt to select the
        (ϕ|ϕ)     {         }                        most signiﬁcant factors.
straint: J  (ϕk)=   p(ϕk)k = d .
  Second, we consider the effect that the enactment φ of
another commitment φ, also by agent β, has on p. This is 4 Summary Measures
achieved in two ways, ﬁrst by appealing to the structure of The measures here generalise what are commonly called
the ontology using the Sim(·) function, and second by intro- trust, reliability and reputation measures into a single com-
ducing a valuation function.                          putational framework. It they are applied to the execution of

                                                IJCAI-07
                                                  1516contracts they become trust measures, to the validation of in- As above we aggregate this measure for those commitments
formation they become reliability measures, and to socially in a particular context o, and measure reputation as before.
transmitted behaviour they become reputation measures. Computational Note. The various measures given above in-
                                                      volve extensive calculations. For example, Eqn. 8 contains
                                                                                             
Ideal enactments. Consider a distribution of enactments ϕ that sums over all possible enactments ϕ . We obtain a
that represent α’s “ideal” in the sense that it is the best that more computationally friendly measure by appealling to the
α could reasonably expect to happen. This distribution will structure of the ontology described in Section 2.1, and the
be a function of α’s context with β denoted by e, and is right-hand side of Eqn. 8 may be approximated to:
Pt  |
 I (ϕ ϕ, e). Here we measure the relative entropy between                               Pt    |
                    t                                                      t            η,I(ϕ ϕ, e)
this ideal distribution, PI (ϕ |ϕ, e), and the distribution of ex- 1 −    P   (ϕ |ϕ, e)log
                 t                                                         η,I            Pt  |
pected enactments, P (ϕ |ϕ). That is:                         ϕ:Sim(ϕ,ϕ)≥η                η(ϕ ϕ)
                                  Pt   |
                 −    Pt  |        I (ϕ ϕ, e)              Pt    |                        Pt  |
   M(α, β, ϕ)=1        I (ϕ ϕ, e)log  t        (8)   where  η,I(ϕ ϕ, e) is the normalisation of I (ϕ ϕ, e) for
                                    P (ϕ |ϕ)                                          t  
                   ϕ                                 Sim(ϕ ,ϕ)  ≥  η, and similarly for Pη(ϕ |ϕ). The extent
                                                      of this calculation is controlled by the parameter η.An
where the “1” is an arbitrarily chosen constant being the max-                                        ≥
imum value that this measure may have. This equation mea- even tighter restriction may be obtained with: Sim(ϕ ,ϕ)
                                                        and   ≤  for some .
sures one, single commitment ϕ. It makes sense to aggregate η ϕ ψ         ψ
these values over a class of commitments, say over those ϕ
that are in the context o, that is ϕ ≤ o:             5   Strategies
                   
                            t                                                                [
                           P (ϕ)[1− M(α, β, ϕ)]       An approach to issue-tradeoffs is described in Faratin et al.,
                 −   ϕ:ϕ≤o β                         2003]. That strategy attempts to make an acceptable offer
   M(α, β, o)=1                   Pt
                             ϕ:ϕ≤o β(ϕ)               by “walking round” the iso-curve of α’s previous offer (that
where Pt (ϕ) is a probability distribution over the space of has, say, an acceptability of c) towards β’s subsequent counter
       β                                              offer. In terms of the machinery described here:
commitments that the next commitment β will make to α is
                                                                  t              t
ϕ. Similarly, for an overall estimate of β’s reputation to α: arg max{ P (acc(β,α,δ)) | P (acc(α, β, χ, δ)) ≈ c } (9)
                                                            δ
                  −     Pt     −
      M(α, β)=1          β(ϕ)[1  M(α, β, ϕ)]          In multi-issue negotiation the number of potential contracts
                     ϕ                                that satisfy, or nearly satisfy, a utilitarian strategy such as this
Preferred enactments. The previous measure requires that will be large — there is considerable scope for reﬁning the
                   t  
an ideal distribution, PI (ϕ |ϕ, e), has to be speciﬁed for each choice of response.
ϕ.  Here we measure the extent to which the enactment   Everything that an agent communicates gives away infor-
ϕ is preferable to the commitment ϕ. Given a predicate mation. So even for purely self-interested agents interac-
Prefer(c1,c2,e) meaning that α prefers c1 to c2 in environ- tion is a semi-cooperative process. α’s machinery includes
                        t
ment e. An evaluation of P (Prefer(c1,c2,e)) may be de- measures of expected information gain in messages either
ﬁned using Sim(·) and the evaluation function w(·) —but sent to, or received from, β. For example, α can reﬁne a
we do not detail it here. Then if ϕ ≤ o:              strategy such as Eqn. 9 by replying to a message μ from β
                                                           
                     t               t              with μ that attempts to give β equitable information gain:
                    P                P    |            t−1            t      
    M(α, β, ϕ)=       (Prefer(ϕ ,ϕ,o)) (ϕ   ϕ)        I   (α, β, μ) ≈ I (β,α,μ ). This reﬁnement aims to be
                  ϕ                                  fair in sharing α’s private information. Unlike the quasi-
                                                      utilitarian measures, both the measure of information gain
and:                
                             t                        and the semantic measure (see below) apply to all messages.
                           P  (ϕ)M(α, β, ϕ)
                      ϕ:ϕ≤o β                          The structure of the ontology is provided by the Sim(·)
        M(α, β, o)=             Pt                                                        
                           ϕ:ϕ≤o β(ϕ)                 function. Given two contracts δ and δ containing con-
                                                           {         }    {        }
Certainty in enactment. Here we measure the consistency cepts o1,...,oi and o1,...,oj respectively, the (non-
                                                                                                
in expected acceptable enactment of commitments, or “the symmetric) distance of δ from δ is the vector Γ(δ, δ )=(dk :
                                                        i                            
lack of expected uncertainty in those possible enactments that ok)k=1 where dk =minx{Sim(ok,ox) | x =1,...,j},
                                                                                            
are better than the commitment as speciﬁed”. If ϕ ≤ o ok =sup(argminx{Sim(ok,x)   | x = o1,...,oj},ok) and
                     t        
let: Φ+(ϕ, o, κ)={ϕ | P (Prefer(ϕ ,ϕ,o)) >κ} for some the function sup(·, ·) is the supremum of two concepts in the
constant κ, and:                                      ontology. Γ(δ, δ) quantiﬁes how different δ is to δ and en-
                        
                 1 ·           Pt   |    Pt   |     ables α to “work around” or “move away from” a contract
M(α, β, ϕ)=1+     ∗             +(ϕ  ϕ)log  +(ϕ ϕ)    under consideration.
                B    
                    ϕ ∈Φ+(ϕ,o,κ)                        More generally, suppose that α has selected a plan s that
where Pt (ϕ|ϕ) is the normalisation of Pt(ϕ|ϕ) for ϕ ∈ is attempting to select the ‘best’ action in some sense. Then
       +                                                                                 ∈Mt
Φ+(ϕ, o, κ),                                          s will construct a distribution D =(di)  where di is
                                                     the probability that zi is the best action that α can take, and
       ∗    1               if |Φ+(ϕ, o, κ)| =1       {z1,...,zr} are the available actions. s reduces H(D) to an
     B  =
            log |Φ+(ϕ, o, κ)| otherwise               acceptable level before committing itself to act — here α is an

                                                IJCAI-07
                                                  1517