          Optimizing Classiﬁer Performance in Word Sense Disambiguation
                              by Redeﬁning Word Sense Classes
                                 Upali S. Kohomban        Wee Sun Lee
                                    Department of Computer Science
                                    National University of Singapore
                                  3 Science Drive 2, Singapore 117543
                               {upali,leews}@comp.nus.edu.sg

                    Abstract                          and Johnson [2003] used contextual features to classify un-
                                                      known nouns into WordNet LFs.
    Learning word sense classes has been shown to be
                                                        This use of WordNet LFs begs the question: can we do
    useful in ﬁne-grained word sense disambiguation   better if we design the sense classes speciﬁcally for ﬁne-
    [                      ]
     Kohomban and Lee, 2005 . However, the com-       grained WSD? We answer this question in the afﬁrmative, by
    mon choice for sense classes, WordNet lexicogra-
                                                      using clustering techniques to automatically derive the sense
    pher ﬁles, are not designed for machine learning  classes. We show that sense classes constructed in this way
    based word sense disambiguation. In this work, we signiﬁcantly outperform the WordNet LFs when used with
    explore the use of clustering techniques in an effort
                                                      Kohomban and Lee’s [2005] method for all word ﬁne-grained
    to construct sense classes that are more suitable for WSD. One interesting result is that the amount of inevitable
    word sense disambiguation end-task. Our results   losses, caused by multiple ﬁne-grained senses falling into the
    show that these classes can signiﬁcantly improve
                                                      same sense class, can be made dramatically smaller, even
    classiﬁer performance over the state of the art re-
                                                      when the number of sense classes is kept the same as the num-
    sults of unrestricted word sense disambiguation.  ber of WordNet LFs. Additionally, our method can be applied
                                                      to parts of speech other than nouns and verbs, where WordNet
1  Introduction                                       LFs cannot be effectively used. The resulting WSD system
Perhaps the most serious problem faced by research in Word yields state of the art results on the Senseval-2 and 3 English
Sense Disambiguation (WSD) is acquiring labeled training all-words task evaluation datasets; our result on Senseval-3
data for supervised learning. This is a crucial problem, since data is the best that we are aware of.
no system with unsupervised learning has shown comparable
results to those of supervised systems, and labeling data for 1.1 Generic Word Sense Classes: Motivation
WSD is labor-intensive.                               This work borrows from [Kohomban and Lee, 2005; Crestan
  One way of overcoming this problem is to reduce the speci- et al., 2001] one major idea: if we can classify word instances
ﬁcity of senses and focusing on a few dominant senses [Mo- into a coarse-grained set of word sense classes,andifwe
hammad and Hirst, 2006]. In addition to this, one can max- know which ﬁne-grained senses fall into these classes, then
imize the use of knowledge one gathers from available la- we can always use the same system as a ﬁne-grained WSD
beled data, by identifying common ‘classes’ of word senses system by replacing the resulting coarse-grained classes with
depending on the similarities in their usage.         the most frequent ﬁne-grained sense within each class. This
  WordNet lexicographer ﬁles (LFs) are sense groups, that way we lose some senses, hence some accuracy. Kohom-
are created manually during the construction of WordNet ban and Lee [2005] argued that this loss can be affordable,
[Fellbaum, 1998]. They provide a rough classiﬁcation of given that the classiﬁer can gain from coarser granularity, as
senses. For instance, ﬁrst senses of nouns cat and dog fall coarser classes reduce the data sparsity. Our results from this
into the LF ANIMAL, and ﬁrst sense of verb dance falls into paper show that with properly designed classes, this loss can
MOTION. WordNet has 25 LFs deﬁned for nouns, and 15   be made far smaller than the loss of the WordNet LFs previ-
for verbs. LFs have been an intuitive choice for semantic ously used.
classes or ‘supersenses’ for words due to many reasons, in- A system working on this principle uses the ﬁne-to-coarse
cluding the popularity of WordNet as a lexical resource, and sense mapping to convert any available data, labeled with
availability of data labeled with respect to WordNet senses. ﬁne-grained senses, into training examples for coarse-grained
Two works discuss how to use LFs in ﬁne grained WSD:  classes. A classiﬁer uses training examples from different
Crestan et al. [2001] classiﬁed word instances into WordNet words, to label any word instance into a class containing one
LFs in Senseval-2 evaluation exercise. Kohomban and Lee or more of its senses; ﬁne grained sense is then assigned in
[2005] proposed how training examples from different words the manner described above. All one needs for this scheme
can be utilized to learn WordNet LFs, which could then be to work is a mapping of ﬁne-grained senses to a coarse set,
used for ﬁne-grained WSD of nouns and verbs. Ciaramita generic for all words; most previous work used WordNet LFs.

                                                IJCAI-07
                                                  1635  However, WordNet LFs are not designed to work as a  senses that fall into the same class, by showing similar us-
generic set of classes for WSD. Thinking on the WSD ap- age patterns in a labeled corpus, will show consistent behav-
plication setting, one can identify two issues that can hinder ior elsewhere. This is the basic reasoning behind inductive
the WSD performance when LFs are used as sense classes: learning.
                                                        We address the issue of sense-loss due to coarse grain na-
Feature Coherence
                                                      ture of WordNet LFs by having a larger number of classes
Commonly used features in WSD are those available within than WordNet does as LFs.
text, such as collocations and part of speech. To the best of
                                                        It could be argued that the WordNet hierarchy encodes
our knowledge, there is no proven evidence that WordNet LFs
                                                      much of human hand-crafted knowledge, and should be re-
form cohesive classes in term of these features; counter exam-
                                                      tained as much as possible in the construction of coarse-
ples can be found.
                                                      grained classes. We tested this idea by partitioning the Word-
  Wierzbicka [1996], for instance, provides examples that
                                                      Net hierarchy into segments that are ﬁner than the WordNet
show that even closely related word/hypernym pairs do not
                                                      LFs, while retaining the WordNet hierarchical relationships
share same usage patterns: Word pairs such as apple/fruit,
                                                      within each partition. Our result shows that this method does
ﬁsh/animal, insect/animal, are not readily interchangeable in
                                                      not work well, both for reducing sense loss as well as in the
practical language usage although they are cohesive parts of
                                                      ﬁnal classiﬁer performance.
taxonomy; “there is an animal on your collar” sounds very
                                                        In the next section, we will describe these two clustering
odd although insect is an animal in WordNet terms. Where
                                                      schemes, i.e. purely feature based and WordNet-hierarchy
the linguistic usage is much different, assuming all examples
                                                      constrained. Section 3 will analyze how well we managed to
to be in the same class would merely introduce noise. On the
                                                      reach our design goals of feature coherence and sense granu-
other hand, contextually similar usages could have been put
                                                      larity, using feature based sense clustering. In section 4, we
into further-away WordNet taxonomies for semantic reasons,
                                                      present the framework we set up for evaluating the perfor-
making it impractical to differentiate those senses using con-
                                                      mance of the classes in the real end-task: ﬁne grained WSD.
textual features alone. Also some semantically close word
                                                      Section 5 discusses the results, and we show that improve-
senses are assigned totally different LFs; for instance iono-
                                                      ment over state of the art is possible with our system, com-
sphere/1:LOCATION and stratosphere/1:OBJECT.
                                                      paring with previously published results.
  Another problem with WordNet LFs is that some LFs are
subsumed by others: FOOD, for instance, is a subset of SUB-
STANCE. This can create confusion in features when learning. 2 Clustering Schemes
Also some LFs with arguably close meanings, such as COG- This section describes the implementation of our proposal,
NITION, FEELING,andMOTIVE, may be hard to differentiate automatic generation of classes based on features, and the
using contextual features alone. It might possibly be better to control experiment, where we used similar techniques to ob-
group them into a single class.                       tain classes that are constrained within WordNet hierarchies.
  LFs for adjectives do not relate to either the underlying These will be referred to as FB and WN respectively.
concept or contextual features, and are not applicable as Syntactic and lexical features in text do not necessarily cor-
generic semantic classes.                             relate. For this reason, we decided to test two different clus-
Loss of Senses                                        tering arrangements, which are respectively based on local
The coarser the classes, the greater the chance that a given context and Part of Speech features from labeled data (See
class includes more than one sense of a given word. As ﬁne- sections 4.1 and 4.2 for details on data and features). Features
sense to class mapping (described at the beginning of this sec- are represented as a binary vector. Local context feature vec-
tion) is a many-to-one mapping, we can lose a few senses for tors were of large dimension, but were very sparse; we used
each word in the reverse mapping, resulting in errors in ﬁne- singular value decomposition to reduce feature space dimen-
grained WSD. Granularity of senses that a ﬁne-grained WSD sion, and discard elements with singular values smaller than
system can attain with WordNet LFs is poor, having only 25 1% of the largest. Data thus obtained is used in FB and WN
classes for nouns and 15 for verbs.                   schemes. Each scheme has two class arrangements, based on
                                                      local context and POS features.
1.2  Clustering as a Solution
In order to address these two issues, we suggest a more direct, 2.1 Purely Feature-Based Classes (FB)
task-oriented approach.                               In this section, we discuss clustering senses independently
  Using the features within text to ﬁnd common groups of of the original WordNet hierarchy; our target here is better
senses based on their context has been shown to be useful pre- feature-class coherence. The idea is that as long as the cor-
viously [Lin and Pantel, 2002; Magnini and Cavagli`a, 2000]. pus behavior of two senses remain the same, it is possible to
We use this idea for generic sense classes: using clustering assume them as being in some hypothetical generic ‘class’,
techniques, we try to generate automatically a set of ‘classes’ regardless of our being able to understand, or label, the exact
of word senses that are based on lexical and syntactic features semantics of that class. If we can ﬁnd such classes using la-
alone. Since these classes are directly based upon features, beled data, then it must be possible to use them in WSD, in
unlike WordNet LFs, we expect them to be easier to learn us- place of WordNet LFs, as described in section 1.1.
ing the same features. There is no new linguistic assumption A sense is represented by the average of vectors of labeled
made here; the only assumption made on classes is that the instances in the corpus for that sense. We omitted the senses

                                                IJCAI-07
                                                  1636that are absent in the labeled corpus, and in the WSD task    0.14
                                                                                            WN-S2
(section 4), considered them to have their own classes.                                     WN-S3
                                                              0.12                          WN-SC
  Our clustering algorithm is inspired by the k-means+ al-                                  FB-S2
                                                                                            FB-S3
                                                                                            FB-SC
gorithm [Guan et al., 2004]. Instead of initializing the clus- 0.1
ters randomly, we chose to base them on original WordNet
LFs. Instead of iterating with a ﬁxed number of clusters,     0.08
we used a method of growing new clusters from outliers of
existing clusters. After each iteration of k-means algorithm, 0.06

we calculate the variance of clusters formed. Then we check   0.04
the squared distance of each point in the cluster to its cen-
troid; if the ratio of this distance to variance is larger than 0.02
a given constant,1 the point is isolated as a new cluster on    20  30  40   50  60  70  80  90
its own. Upon reaching convergence, another reﬁnement is
made: if a cluster has a smaller number of members than Figure 1: Proportional ‘loss’ of senses vs number of classes
desirable, we merge it with the nearest cluster, chosen by for nouns, for FB and WN: S2, S3, SC are Senseval-2, 3
simple-linkage condition: that is, cluster cj is the ‘nearest’ all-words task data and SemCor; optimal clustering is high-
to cluster ci (i = j)ifcj has the node within the shortest pos- lighted. Left-most points of WN correspond to WordNet
sible distance to any node in ci. This allows for non-spherical LFs. Feature-based classes consistently yield better sense-
clusters, while preserving the size of clusters above a certain separation, even at smaller numbers of classes.
limit.
                                                               0.4
  Once the clusters are formed, it is straightforward to cre-                               WN-S2
ate the sense mapping, which can be used in our classiﬁer as                                WN-S3
                                                              0.35                          WN-SC
discussed above (also in section 4.3). We applied this method                               FB-S2
                                                                                            FB-S3
                          2                                    0.3
for nouns, verbs and adjectives.                                                            FB-SC

                                                              0.25
2.2  Classes Constrained within WordNet (WN)
                                                               0.2
First, we build trees from WordNet hierarchy, with senses as
                                                              0.15
nodes, and their hypernyms as respective parent nodes. Trees
that belong to same LF are connected together with a root      0.1
node. Then, the feature coordinates (as earlier) of each sense
                                                              0.05
are added to the tree at its respective node. For a given tree  10    20    30   40    50   60
segment, the centroid of coordinates can be calculated by av-
eraging all sense coordinates within that segment; average Figure 2: Proportional ‘loss’ of senses vs number of classes
square distance to centroid is a measure of cohesiveness of a for verbs. Details as per ﬁgure 1.
tree. We consider each node in the tree as a candidate break-
ing point, and decide where to break by checking which split
gives the largest reduction in total variance of the system. The 3.1 Sense Resolution
partitioning proceeds in a greedy manner, by selecting at each We compared the ‘sense loss’ of the FB classes with that of
run the node that gives the best overall improvement. the WN classes.
  As earlier, smaller clusters were removed by merging them Recall from section 1.1 how the sense loss occurs; losses
back; however, we cannot pick the geometrically nearest clus- will be counted as errors in the ﬁne-grained WSD system, so
ter to merge as this would distort the WordNet hierarchy con- minimizing losses is a desirable quality of classes. Given a
sistency requirement. So a cluster was merged back to the class mapping and a labeled corpus, we can assess the loss
point from which it was originally detached.          by counting the proportion of labeled instances that belong to
  Adjectives and adverbs cannot be organized into proper ‘lost’ senses. Figure 1 and 2 shows this loss in labeled data
tree forms as they do not have hypernyms. So this method sets due to FB and WN, at different numbers of classes. WN
was limited to nouns and verbs only.                  starting points in the graph are original WordNet LFs.
                                                        Although both schemes seem to beneﬁt from larger num-
3  Effects of Clustering                              bers of classes, there is an additional gain in FB that we did
                                                      not anticipate: it achieves good resolution, even at smaller
In this section, we will analyze empirically the basic effects of numbers of classes. At the same number of classes as the
our clustering schemes, discussing how effectively we man- WordNet LFs, it is possible to obtain more than a 50% reduc-
aged to obtain the properties we desired as design goals. tion in sense loss. Recall that FB can either split clusters or
                                                      reorganize points in order to reduce variance, while WN can
  1This constant was chosen to be slightly below the maximum only split, as reorganizing would violate the hierarchy. This
distance/variance ratio found after the ﬁrst iteration. means that FB can, in theory, achieve better optimization for
  2Lexical ﬁle arrangement of adjectives is not semantically based; the same number of clusters, and this seems to work in prac-
clusters were still initialized with the three available. tice as well. This is an added advantage, as smaller clusters,

                                                IJCAI-07
                                                  1637                   local context     POS              LFs that were used in the original work, and evaluating the
                   nouns  verbs  nouns   verbs        performance of the resulting WSD system.
     WordNet LFs   0.251  0.223  0.011   0.021
     WN            0.336  0.330  0.041   0.065        4.1  Data
     FB            0.352  0.335  0.065   0.072        We use labeled data from SemCor corpus [Fellbaum, 1998,
                                                      chapter 8] as training data. To determine global classiﬁer
        Table 1: Average information gain values      settings, a randomly selected part of this (1000 instances
                                                      for each part of speech) is held-out as validation data set.
although reducing the sense loss, have the undesirable prop- Where word-level validation is employed, randomly picked
erty of including a fewer number of senses. This limits the word instances (up to 20) from the training data were kept
number of training examples per class.                aside. Evaluation was done on Senseval-2 and Senseval-3
  In the actual WSD task, we used cross validation to guess [Edmonds and Cotton, 2001; Snyder and Palmer, 2004] En-
the best number of classes for each clustering; these are glish all-words task data sets. Our tests use WordNet 1.7.1
shown highlighted in the graphs.                      senses.
3.2  Feature Coherence                                4.2  Features
It was observed that a given FB class can include group- Features used for learning are implemented in the same way
ings of different semantics. For instance, a small noun as described in [Kohomban and Lee, 2005].
class was dominated by three distinct ‘themes’ - some of Local context: This is a [−n, +n] symmetric window of
them were {kneecap/1, forearm/1, palm/1}, {coattail/1, over- words to the both sides of word under consideration. The
coat/1, shirtsleeve/1},and{homeland/1, motherland/1}.But size of the window n ∈{1, 2, 3} was chosen by cross valida-
this mix does not pose a problem as similarity weighting of tion. All words were converted to lower case and punctuation
instances (see section 4.3) can lessen the inﬂuence from unre- marks were excluded.
lated words as training instances. A similarity measure based Part of speech: This is similar to the local context window,
on WordNet hierarchical proximity introduces some of the hi- using the same rules of not considering parts of speech of
erarchy information back to the classiﬁcation, ensuring both punctuations and not exceeding sentence boundaries.
contextual and taxonomical coherence.                 Grammatical relations: This included the basic syntactic re-
  To empirically evaluate how well these classes can be sepa- lations such as subject-verb and verb-object, as well as prepo-
rated by features in the end-task classiﬁer, we calculated fea- sitional phrases, such as ‘sound of bells’ for word sound.
ture information gain values for 6-token POS/local context
window on complete SemCor data set. Information gain of a 4.3 Classiﬁer
feature i with set of values Vi is given by           All we obtain from the clustering process is the mapping from
                                                     ﬁne grained senses into their respective class number. Each
          wi = H(C)  −     P (v) · H(C|v),            ﬁne-grained sense labeled instance in the training set is used
                       v∈Vi                           in the classiﬁer as an example for that particular class, using
                                                     the class mapping. Training data for each word is limited to
  where H(C)=−      c∈C P (c)logP (c) is the entropy of
                 C                                    those instances belonging to classes that include one or more
the class distribution . This provides a measure of how well senses of that word. The classiﬁer used is TiMBL memory
a given set of classes can be separated by a given feature.
                       C                  v           based learner [Daelemans et al., 2003] which is essentially a
Since the class distribution and feature values for each k-NN classiﬁer.
feature are available for SemCor, it is straightforward to apply
                  w                                     Kohomban and Lee  [2005] showed that the noise due to
the formula to obtain i for a given feature. Table 1 shows using examples from different words could be reduced if
information gain measures in nouns and verbs for SemCor the examples were weighted according to the relatedness of
data, for local context and POS features (averaged over six example-instance word to the word being labeled. We em-
positions in context windows). It can be seen that both WN ploy the same method for nouns and verbs in our experiment,
and FB clusters improve the gain with smaller class sizes, but using the same relatedness measure for weighting, proposed
FB clusters yield the best gain.                      by Jiang and Conrath (JCn) [1997]. This measure takes in to
                                                      account the proximity of the two senses within the WordNet
4  WSD End-Task Evaluation Framework                  hierarchy, as well as the information content of the nodes of
In order to empirically validate the effect of the two prop- the path which links them together.4
erties of classes, which we thought of as critical for WSD Instance weighting is implemented by modifying the dis-
end-task, we used the system originally described in [Kohom- tance Δ(X, Y ) between a training instance X and testing in-
ban and Lee, 2005] for ﬁne-grained WSD.3 We measure the stance Y , in the following way:
‘quality’ of our classes by using them instead of WordNet
                                                                       E          Δ(X, Y )
                                                                     Δ  (X, Y )=
  3Some improvements were made in the system after [Kohomban                     SX,Y +  
and Lee, 2005] was published, so we re-ran the experiments reported
there, which used WordNet LFs as sense classes. The new results 4Our implementation uses information content data compiled by
correspond to ‘WordNet LFs’ entries in the tables.    Ted Pedersen et al. http://wn-similarity.sourceforge.net/.

                                                IJCAI-07
                                                  1638SX,Y is the JCn similarity between X and the most frequent                  Senseval-2  Senseval-3
sense of Y that falls within the class of X;  is a small con- Baseline         0.658       0.643
stant to avoid division by zero.                              Senseval Best     0.690       0.652
  A separate classiﬁer was used for each of the three feature Crestan et al.    0.618           -
type described above. These three classiﬁers, and a classiﬁer
which always predicts WordNet ﬁrst sense as its output, par-    Table 2: Baseline and previous results
ticipated in simple majority voting. In case of a tie, the ﬁrst
sense was chosen.                                                      noun    verb    adj.  combined
Weighted Majority Voting As Kohomban and Lee [2005]      baseline      0.711  0.439   0.639      0.658
reported, Weighted majority algorithm [Littlestone and War- WordNet LFs 0.724 0.455   0.639      0.668
muth, 1994] can increase classiﬁer performance over simple WN, POS     0.724  0.453   0.639      0.667
majority algorithm. For the ﬁnal combination, we used the WN, LC       0.723  0.457   0.639      0.667
validation data to pick the best clustering scheme (POS or lo- FB, POS 0.725  0.480   0.643      0.674
cal context) as well. Nouns and adjectives did well with local FB, LC  0.747  0.458   0.654      0.681
context based clusters, while verbs did well on POS based baseline     0.700  0.534   0.669      0.643
clusters.                                                WordNet LFs   0.719  0.548   0.669      0.656
Final Results                                            WN, POS       0.717  0.559   0.669      0.658
Once an instance is classiﬁed as a particular class, we have WN, LC    0.719  0.557   0.669      0.659
to decide which ﬁne-grained sense it belongs to. There can FB, POS     0.710  0.568   0.694      0.664
be more than one sense that falls into a given class. As men- FB, LC   0.736  0.541   0.708      0.668
tioned in section 1.1, we use the heuristic of picking the sense
with the smallest WordNet sense number within the class. Table 3: Results for different original WordNet LFs, WN and
This is motivated by the fact that WordNet senses supposedly FB clustering schemes, simple majority voting, for Senseval-
come in descending order of their frequency.          2 (above) and Senseval-3 (below) data. POS and LC are clus-
  For each clustering, the words with senses that fall into terings based on POS and local context features (section 2).
multiple classes were classiﬁed as described in this section.
All the other words are assigned WordNet sense 1. Also, we
                                                        Table 2 shows the baseline (WordNet ﬁrst sense) and the
check for multi-word phrases that have separate WordNet en-
                                                      performance5 of two best systems reported in Senseval, [Mi-
tries. These phrases were assigned sense 1 as they usually
                                                      halcea, 2002] and [Decadt et al., 2004],aswellasthatof
have only one sense. Adjective clustering was not applicable
                                                      [Crestan et al., 2001], which used WordNet LFs.
for WN, as mentioned earlier. In FB, method used for adjec-
                                                        Table 3 shows the results of the clustering schemes we dis-
tives was the same as above; in WN, and for adverbs in both
                                                      cussed, using simple majority voting (see section 4.3), as well
cases, we resorted to WordNet ﬁrst sense.
                                                      as the re-run of the system with original WordNet LFs. Re-
Adjective Similarity                                  sults are given for three parts of speech and the combined
JCn similarity measure depends on hypernym hierarchy, and system (as described in ‘Final Results’). WN clusters’ per-
is not applicable for adjectives, which do not have a hierar- formance is not signiﬁcantly different from that of original
chical organization. As Kohomban and Lee [2005] reported, WordNet LFs; this may be because the constraining on the hi-
only JCn similarity measure could help the classiﬁers outper- erarchy had the same type of localization effect that original
form the baseline. In addition, the adjective LFs, only three system achieved with JCn weighting, thus not yielding any
in number, are not suitable for the WSD framework. How- additional information and even preventing some informative
ever, FB clustering scheme could be applied on adjectives as examples from being used. This supports our idea that one
well, as we can group adjective senses into smaller classes. In cannot obtain good performance merely by splitting LF into
addition, the context vectors we get from SVD provide a way a ﬁner set of classes.
for calculating inter-sense similarity. The coordinate vectors On the other hand, the feature-based (FB) classes provide
resulting from SVD gives a smoothed out measure of average contextually-based information that are not given by the hi-
behavior of a sense. This idea has been successfully used in erarchy, and could have complemented the information from
WSD previously [Strapparava et al., 2004]. We could use it the JCn similarity measure, which is based on the hierarchy.
as a measure of similarity as well, by using the dot product of In other words, clustering independently of the hierarchy is a
coordinate vectors as the similarity between senses.  better way to utilize the two different sources of information:
  In general, when we used this measure in the classiﬁer pro- semantic (from taxonomical hierarchy), and lexical/syntactic
cess, it yielded results that outperformed the baseline. How- (from linguistic usage patterns).
ever the measure could not outperform JCn. So we limited its It can also be seen that for FB, POS based clustering per-
use to adjectives only.                               formed well for verbs, while local context based clustering
                                                      did well with nouns and adjectives. A rough explanation may
5  Results                                            be that verbs generally beneﬁt from syntactic features, which
We evaluated the two clustering schemes FB and WN in the are available through POS. The effect is not consistent for
framework described in section 4, in clustering schemes that
are based on both POS and local context features.        5all numbers shown are recall values using the ofﬁcial scorer.

                                                IJCAI-07
                                                  1639