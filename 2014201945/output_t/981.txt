     effects reduced relational vocabulary whodunit problem                                 daniel halstead kenneth forbus                                        qualitative reasoning group                                          northwestern university                                 sheridan rd evanston il  usa                          abstract                          express information unless language                                                        redundancies detected ahead time relational learning      key issue artificial intelligence lies finding                                                        algorithms suffer      input needed successful                                                          issue particularly relevant learning      learning causes overhead     deployed sort realworld environ      makes learning prone overfitting little                                                        ment environments tend brimming unre      tail possible learn                                                        lated observations robotic systems example      issue particularly relevant                                                        wisely choose focus sensory inputs      puts relational case descriptions ex                                                        analyze carefully cursory analysis      pressive vocabulary lead inconsistent                                                        inputs confuse picture simi      representations example whodunit     lar application automatic extraction knowledge      problem task form hypotheses                                                        text popular internet      identity perpetrator event described                                                        grows crucial identify relationships      ing relational propositions training data                                                        useful know convey practically      sists arbitrary relational descriptions                                                        formation      similar cases paper examine                                                          paper motivated just attempt      possibility translating case descriptions improve knowledge extraction text analysis      alternative vocabulary reduced                                                        contrasting effects using large detailed      number predicates produces                                                        redundant vocabulary small consistent      consistent case descriptions compare                                                        extremely simplified different vocabularies      reduced vocabulary affects three different learning                                                        applied whodunit problem tries learn      algorithms exemplarbased analogy prototype                                                         predict perpetrator terrorist event given      based analogy association rule learning  descriptions similar events expressed proposi      positive effect algorithms                                                        tional assertions descriptions currently entered      negative effect gives                                                        hand eventually extracted text      sight three algorithms indicates                                                        makes choice vocabulary especially relevant each      reduced vocabularies appropriate                                                        vocabulary used three different learning algorithms                                                        order better understand effects vocabulary size    introduction                                       best overall combination  problem consistent nearly applica section  begins introducing whodunit problem  tion machine learning identifying appropriate following each three learning algorithms  input data tempting solving problem explained section  section   learn available data realworld applica describes original large vocabulary reduced  tion irrelevant redundant including section  presents results finally conclude  extraneous analysis slow discussion related work  process lead overfitting  learning incorrect model clearly  whodunit problem  balance little data                                                        important task analysts coming plausible  unlikely learned                                                        hypotheses performed event recall pre    problem worse dealing                                                        election bombing madrid spain spanish gov  lational data relational learning algorithms                                                        ernment originally claimed basque separatist group  operate comparing instances relation eta likely suspect evidence quickly mounted  redundant relations particularly dangerous                                                        al qaeda likely responsible multiple highly  expressive vocabulary ways                                                      ijcai                                                     coordinated attacks example similar al  functions constrained  million facts  qaedas modus operandi previous eta actions descriptions terrorist attacks ranged size    example whodunit problem      propositions average  propositions    stated formally given event perpe whodunit problem excellent domain explor  trator unknown whodunit problem construct ing relationships similarity probability  small set hypotheses hp identity input data consists entirely arbitrarily high order sym  petrator hypotheses include explanations bolic relations arbitrary structure  likely ones able explain means pay careful attention structure  demand likely                    order probabilities correct statements    define restricted class whodunit problems uniformly correspond concept  begin                                        each case large number records    formal inputs assume input information terrorist attacks train large  encoded form structured descriptions including number possible perpetrators  choose during  relational information expressed formal knowledge testing  representation note require uniform  representations each input treat each case  learning algorithms  simply collection arbitrary predicate calculus statements  object predefined slots used three different algorithms try solve  filled                                    dunit problem contribution paper    accurate inputs assume input information effects vocabulary reduction learners  completely accurate noise    selves selected algorithms    oneshot operation outputs produced previously published present terse  given queried explanations description each learner implementation details  does automatically update hypotheses incrementally particular domain  given new information                          three algorithms utilized structural analogy    passive operation hypotheses processed fashion order make comparisons  generate differential diagnosis information tells cases  sought order discriminate small approach analogy based gentners   set likely hypotheses                             structuremapping theory analogy similarity    supervised learning allow train structuremapping analogy similarity defined  set preclassified examples algorithms terms structural alignment processes operating struc  involves forming nonoverlapping generalizations tured representations output comparison proc  examples                                  ess mappings constituting construal    assumption formal inputs reasonable given entities situations concepts called base  producing representations news sources target aligned purposes paper  focus considerable research natural language com mapping consists set correspondences struc  munity days assumptions accurate inputs tural evaluation score correspondence maps item  oneshot passive operation supervised learning entity expression base item  good starting points solve target structural evaluation score indicates overall  stricted version problem makes sense try quality match  solve harder versions                                  used structuremapping engine sme imple                                                        ment theory analogical mapping falkenhainer  table  example whodunit case description       bus  gentner  sme uses greedy algorithm                                                        compute approximately optimal mappings polynomial    isa attack terroristattack                      time forbus  oblinger                                                           formally task each learning algorithm given    eventoccuredat attack fallujah                                                        descriptions train input event produce    cityincountry fallujah iraq                                                        hypotheses identity perpetrator event    thereexistatleast     citizenof iraqwasinjured attack  exemplarbased analogy macfac    corpus use experiments independ algorithm operates purely exemplar retrieval  entlydeveloped knowledge base terrorist incidents pro designed small number input cases  vided cycorp version provided consists similar probe case each   descriptions different terrorist attacks each case hypothesizes perpetrator  handentered checked domain experts attacks perpetrator similar case process iter  expressed using symbolic representation vo ated unique hypotheses generated macfac  cabulary cyc kb subset consists forbus gentner  law  algorithm   concepts  relationships forms similaritybased retrieval stages                                                      ijcai                                                      stage uses special kind feature vector called overlap seql shown substan  content vector automatically computed tially faster connectionist algorithms compared  each structural description content vector consists sim headtohead kuehne et al   ply counts each predicate corresponding using input representations  scription dot product estimate connectionist models seqlbased simulation  correspondences sme generate considering tinued work given noisy versions data  possible mappings descriptions potential drawback seql generaliza  estimate quality match content vector tions – union case descriptions – grow  computations used rapidly select typically quickly size sme polynomial memory require  three candidates large memory                ments seql pare facts    inthesecondstagesmeisusedtodoananalogical          generalization culls lowest probability  comparison subset returned throwing away information preserve higher level  stage probe description returns abstraction reasonable space requirement  close similar cases  memory probe reminded               rule learning    deployed  performance systems sme      method applied statistical  macfac used successfully variety dif nature learns list association rules predicting  ferent domains cognitive models each possible perpetrator order  used account variety psychological results vert structured relational data featurevalue  bus                                            representation used case flattening approach intro                                                        duced halstead  forbus  accomplish   prototypebased analogy seql                     seql applied input cases  second algorithm designed use analogy order build large generalization gener  builds generalizations cases serve alization used framework building fea  prototypes each generalization constructed ture set each assertion generalization corre  cases perpetrator time way each gen sponding feature generalization feature set  eralization serves prototypical description events taken form invertible mapping relational  associated single perpetrator instead case descriptions features  comparing case algorithm interpretation results important note  does algorithm compares prototype  features come types existen    generalizations built using analogy deter tial feature default type simply takes value  concepts case best correspond true false depending assertion repre  concepts case use probabilistic implemen sents present given case characteristic fea  tation seql algorithm seql kuehne ture hand used known  forbus et al  stands sequential learner structure assertion information  designed produce generalizations incrementally conveyed example english sentences “the  stream examples uses sme compare each new ex attack killed someone” “it ambush” existen  ample pool prior generalizations exemplars tial features – true false sentences  new example sufficiently similar existing gener “the attack killed three people” “it happened bagh  alization assimilated generalization dad” characteristic features values   wise sufficiently similar prior exem baghdad respectively show conciseness  plars combined form new generalization reduced relational vocabulary makes easier extract    generalization cases taking union characteristic feature values data  expressions descriptions adjusting rule learning use definition association rule  probability each expression according introduced agrawal et al  each associa  descriptions matching entities identi tion rule conjunction featurevalue pairs aka liter  cal kept generalization nonidentical entities als location  iraq implies  replaced new entities constrained conjunction  statements union              actual rule learning using adtree cache    seql provides interesting tradeoff traditional joint probabilities input data anderson  moore  symbolic generalization algorithms like ebl dejong   adopting anderson  moore’s algorithm start  mooney  statistical generalization algorithms list candidate hypotheses antecedents initially  connectionist systems like ebl operates perform breadthfirst search beam  structured relational descriptions unlike ebl does width  space possible hypotheses each  require complete correct domain theory does iteration search specify each hypothesis  produce generalization single example like adding literals set possible literals  connectionist learning algorithms conservative reevaluate each hypothesis choose best   producing generalizations significant                                                      ijcai                                                     reiterate rules  terms long each new predicate corresponds set old  example rule learned looks like             predicates relationships easily handcoded    location attack philippines             list  translation rules used actual            agentcaptured attack agent             data reduction example following rule handles      perpetrator attack moroislamicliberationfront intentional actions                                                          translate    finally possibly perpetrator algorithm ac doneby event agent  tually learns rulelist recursively calling rule   eventplannedby event agent  learner perpetrator begins learning sin agent event agentgoal agent event  gle rule ⇒ iteration tries learn  rule ⇒   process continues  rule fires facts original cyc rep  reaches maximum  rules rules  resentation predicates doneby event                                                 plannedby specializations doneby event    list rules rp learned perpetrator plannedby each fact translated new facts  input case applied rptoseewhich describing agent played causal role  rules rules fired highest confi event second describing event  dence consequents returned hypothesis fact goal agent facts actually translated  identity perpetrator                      application rule example    learners limited reductive translation process seen table   learning rule learner learns simple higherlevel pat  terns                                                  table  translation example                                                         cyc     thereexistexactly agent     reduced vocabulary                                           agentcaptured attack agent  original humanencoded input data consisted                                                                  translate  predicates drawn vocabulary  rule   translated  predicates  reduction      thereexistexactly number variable fact  vocabulary size knowledge compression ratio         measure variable numberfact    reduced vocabulary used subset      translate  vocabulary designed language corporation            objectactedon event object  polaris bixler moldovan  fowler  rule    theme event object   vocabulary consisted  predicates chosen         predicate object predicate  usefulness natural language processing fea      result event predicate  sibility automatic extraction text par  ticular importance research broadest semantic     measure agent   coverage overlap lcc explains       theme attack agent                                                                 predicate agent agentcaptured      list predicates perfect… polaris                                                                 result attack agentcaptured      list strikes good balance      specific relations making reasoning dif      ficult general information      useful    subset  predicates selected note original cyc vocabulary  vocabulary based simply polaris predicates richer facts original data repre  needed represent information sented fact translation specifically  present whodunit training cases final list average translation rule turns fact  new facts  predicates provided table                     average number facts case increased                                                         results    table  relations reduced vocabulary    used three criteria bounding size set    agent        goal        result        theme        hypotheses restrictive producing sin    predicate    purpose     location      time         gle perpetrator guessing directly did    measure      property             cause        restrictive  list rank ordered estimated like    instrument   topic       belief        associated   lihood middle ground  list    reason       source      experiencer   recipient    virtue providing best hopefully mind    possible      entails                               jogging alternatives                                                      ijcai                                                     chart  results old new vocabularies resp  surprised three strategies                                                        formed nonstatistical ones given difficulty                                                        problem consider each case contains                                                        average  facts  features                                                        dataset means given record                                                         features missing makes sparse                                                        dataset fortunately closed world assumption                                                        held consider arity                                                        output attribute   features                                                        random algorithm select cor                                                        rect perpetrator  time right                                                        guesses  time believe                                                        success rates  quite good    results turn different each algo conclusion given researchers tend use larger  rithm chart  shows conditions origi lational vocabularies extremely interesting  nal vocabulary rule learner performs best able duced relational vocabulary improve three  return correct answer guess  learners tested certainly experiments test  time seql finds correct answers reduced vocabulary domain caution  long run certain beginning providing interpreting results warranted  correct answer guess  time fi duced vocabularies dangerous use learning algo  nally macfac does little better seql rithm relies heavily abstraction seql  guess interestingly continuing construct hy lead data andor loss  potheses macfac point proved useless information vocabulary reduction does trade    new vocabulary exemplarbased algorithm away smaller case descriptions information  improved pvalue  seql performed worse added conciseness learning algorithms known  pvalue  stranger rule learner large amounts data high dimensionality  depends generalization algorithm provided seql advantage extra conciseness  performed better pvalue  rule learner pays attention greater  gets correct answer guess  time number extractable feature values appear likely    closer examination reveals seql algorithm better wellchosen reduced relational vocabulary  hardpressed original vocabulary seql generalized  case descriptions contained average   facts each  facts discarded  related work  preserve memory dimensionality case description glance work appears tie strongly forms  size increased abstraction reduced vocabu dimension reduction feature selection  lary abstraction original data task feature selection automatically select sub  information loss compounded average descrip set concepts form filtering reduced relational vo  tion size increases  does number facts dis cabularies abstract transform filter  carded seql during generalization rises  forms dimension reduction perform transfor  furthermore facts remain carry information mation data principal component analysis  did original vocabulary average result transformation lost  reduced predicate corresponds  different predicates realworld meaning furthermore techniques  original vocabulary                        typically automated given domain par    did rulelearner improve performance ticular reduced vocabulary contrast manually  possible explanation leap takes structed researchers based needs natural  reductive higherorder learning language processing work domain finally  story important number relations data decreases  rule learner able advantage conciseness vocabulary reduction dimensionality data actually  reduced vocabulary conciseness allows flat increases simpler relations used  tening process generate characteristic values different sentences convey information  features average arity increases different ontologies developed   plethora feature values gives rule learner effort balance language expressiveness computabil  grist having relevant options consider ity owl common example targeted use  analysis shows features world wide web work computing  treated existential allowed values expressiveness given ontology example cor   original vocabulary rule cho gomezperez  establish common frame  learner reverts seqllike levels performance work comparing expressiveness reasoning cap                                                      ijcai                                                     
