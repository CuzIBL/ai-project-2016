                             neighborhood minmax projections               ∗                            feiping nie shiming xiang changshui zhang                        state key laboratory intelligent technology systems                  department automation tsinghua university beijing  china                     nfpmailstsinghuaeducn   xsm zcsmailtsinghuaeducn                          abstract                          chen et al  yu yang  vari                                                        ants lda discard subspace important dis      new algorithm neighborhood minmax projec      criminative information lost drawback      tions nmmp proposed supervised dimen   lda distribution assumption lda optimal      sionality reduction paper algorithm  case data distribution each class gaussian      aims learning linear transformation fo satisﬁed real world applications      cuses pairwise points   class distribution complex gaussian lda      points neighbors each trans fail ﬁnd optimal discriminative directions      formation considered pairwise points  number available projection directions lda      class close possible smaller class number duda et al butitmay      different classes far possible insufﬁcient complex problems especially      formulate problem constrained optimiza number class small      tion problem global optimum      effectively efﬁciently obtained compared                                                          distance metric based classiﬁcation methods      popular supervised method linear discrimi                                                        nearest neighbor classiﬁer learning appropriate      nant analysis lda method three signif                                                        distance metric plays vital role recently number      icant advantages able extract                                                        methods proposed learn mahalanobis dis      discriminative features second deal                                                        tance metric xing et al  goldberger et al       case class distributions com                                                        weinberger et al  linear dimensionality reduction      plex gaussian singularity problem                                                        viewed special case learning mahalanobis dis      existing lda does occur naturally                                                        tance metricsee section  viewpoint reason      formance data sets demonstrates ef                                                        able interpretation fact performance nearest      fectiveness proposed method                                                        neighbor classiﬁer improved performing                                                        linear dimensionality reduction    introduction                                                          paper propose new supervised linear di  linear dimensionality reduction important method                                                        mensionality reduction method neighborhood minmax pro  facing highdimensional data algorithms                                                        jections nmmp method largely inspired  proposed during past years algo                                                        classical supervised linear dimensionality reduction method  rithms principal component analysis pca jolliffe                                                         lda recent proposed distance metric learning  linear discriminant analysis lda fukunaga                                                         method large margin nearest neighbor lmnn classiﬁca  widely used methods pca unsu                                                        tion weinberger et al  method focus  pervised method does class information                                                        pairwise points points neigh  account lda popular supervised di                                                        bors each transformation try pull  mensionality reduction techniques classiﬁcation                                                        considered pairwise points class close  exist drawbacks drawback                                                        possible different classes apart  suffers small sample size problem                                                        goal achieved formulating task  dealing high dimensional data case                                                        strained optimization problem global optimum  class scatter matrix sw singular makes                                                        effectively efﬁciently obtained compared  lda difﬁcult performed approaches                                                        lda method avoids three drawbacks lda dis  proposed address problem belhumeur et al                                                         cussed compared lmnn method    ∗this paper funded basic research foundation ts method computationally efﬁcient perfor  inghua national laboratory information science technol mance data sets demonstrates effectiveness  ogy tnlist                                         method                                                    ijcai                                                                                                                                                                                                                                                                    ˜                                                                                       sb                         xi − xj xi − xj                                                                                                                                                              ijxi∈nbcj xj ∈nbci                                                                                                                ˜                                                          ci  cjandsb positive semideﬁnite                                                          achieve goal maximize sb mini                                                mize sw following function used objec                                                        tive                                                                                                          tr wt   s˜ − λs˜  figure  left ﬁgure point belong                                 class circles denote withinclass neighbor                                                                                      ˜  hood respectively b’s withinclass neighbor                  trw  sbw                                                                        hood a’s withinclass neighborhood trans                   ˜                                                                                                trw   sww  formation try pull points close possible  right ﬁgure point belongs class point difﬁcult determine suitable weight λ  longs class circles denote betweenclass mer objective function select objective  neighborhood respectively b’s betweenclass optimization fact just spe  neighborhood a’s betweenclass neighborhood af cial case weight λ automatically  ter transformation try push points far determined  possible                                               formulate problem constrained op                                                        timization problem    problem formulation                                                            tr wt s˜                                                                 w∗                         given data matrix   xn  xi ∈               arg   max                                                                                                wt wi  tr wt s˜  goal learn linear transformationw  rd → rmwhere                                          d×m         ∈         × identity matrix fortunately globally optimal solution problem  original highdimensional data transformed efﬁciently calculated section  lowdimensional vector                             details solving constrained optimization problem                                                                                                                                  constrained optimization problem    let each data point class kinds neighbor                                                        address optimization problem general  hood withinclass neighborhood nwi betweenclass                                                        form described follows  neighborhood nbiwherenwi  set data’s  kwi nearest neighbors class nbi constrained optimization problem given real sym                                                        metric matrix ∈ rd×d positive semideﬁnite ma  set data’s kbi nearest neighbors class d×d             ≤   ≤ −         ≤  ≤ −     trix ∈      rankbr      ≤   matrix   obviously                            d×m                                                     ∈     maximize following objective function   data number class                                   wt        focus pairwise points constraint    points neighbors each transformation                     tr wt  aw                                                                   w∗                           hope distance considered pairwise points          arg   max                                                                                                  wt wi trw   bw  class minimized distance  different classes maximized ﬁrst propose lemma  shows  figure                                             wt    md−    rthevalueoftrwt  bw      transformation sum euclidean dis equal zero  tances pairwise points class lemma suppose ∈ rd×m wt  ∈ rd×d  formulated                                        positive semideﬁnite matrix rankbr ≤                             ˜                                                                   sw  trw   sww                   − holds trw bw         tr ·                                               proof according result rayleigh quotientgolub     denotes trace operator matrix                                                                                                    van loan   min  trw   bw          βi                                                                                                    s˜                           −   −                              wi                                      βββm  ﬁrst smallest eigenvalues         ijxi∈nwcj xj ∈nw ci                     basb    positive semideﬁnite rank rand                                                                      m                                                                                md−    rthen  βi   constraint    ci denote class label xiandcj denote                                                                     ˜                  trw  bw  ≥  min trw  bw     class label xj obviously ci  cjandsw positive                                                          discuss optimization problem cases  semideﬁnite                                                          case  md−      similarly sum euclidean distances pair                                                          lemma  ensures optimal value ﬁnite case  wise points different classes                                       ∗                                                        suppose optimal value λ guo derived                                                                          ∗                     tr wt  s˜                        max  trw   − λ bw                                              wt wi                                                    ijcai                                                       note trwt bw      easy    obtain iterative algorithm obtaining op   max   trwt  − λbw        ⇒  λλ∗and         timal solution described table  al      wi                                                gorithm iterative steps needed   max   trwt  − λbw    ⇒  λλ∗  wt wi                                                obtain precise solution note algorithm need    hand max  trwt  − λbwγ        calculate inverse singularity problem                     wt wi       γ                                       −    does exist naturally   sum ﬁrst largest eigenvalues    case  ≤ −  λb given value λifγ λ just optimal value                       γ           λ                                  case    lies null space ma      implies smaller optimal value trix bthentrwt bw value objec  vice versa global optimal value problem tive function inﬁnite rea  obtained iterative algorithm subsequently sonably replace optimization problem v∗   order suitable value λ need determine                               d−r×m                                                        arg max   trv  azvwherev     ∈        possible bound optimal value theorem  proposed vt vi  solve problem                                 zd−r eigenvectors corresponding    theorem  given real symmetric matrix ∈ rd×d − zero eigenvalues                                      d×d                                   ∗  positive semideﬁnite matrix ∈  rankb     know       μ   μ  μmwhere                   d×m         d×m  ≤ difw   ∈       ∈            μ μ  μm ﬁrst largest eigenvectors                    trwt aw            trwt aw                                            ∗       ∗                                                     az case ﬁnal solution  ·  − rthen max             ≤   max                             trw bw         trw bw            wi              wi    proof theorem  based following lemma input                                            ak    lemma if∀i ai ≥ bi    ≤    ≤ ··· ≤                                      d×d                                            bk        real symmetric matrix ∈ positive      aa···ak ≤ ak                                                          d×d  bb···bk bk                                   semideﬁnite matrix ∈  rankbr  ≤  dthe              ak                      ∀i ai ≥ bi           ai ≤                      proofletbk                  wehave           error constant               aa···ak  ak  qbi                     ≤     bb···bk bk                           output    proof theorem  following                    w∗         w∗    ∈  rd×m                                 ∗                          projection matrix                                                               ∗t   ∗    proof theorem  suppose                                                 trwt aw       w∗                                cm           arg    max            let                          md−                          trw bw                       inthecaseof                           wi                                                                  tra        αα···αm       λλ                                    trw   aw         λ ←       λ ←               λ ←                                                      ≤                trb        ββ···βm           loss generality suppose trwt bw                                                 αα  αm ﬁrst largest eigenvalues                           trw   aw       trw   aw ph                      β β   β                    ≤···≤       ph                                   ﬁrst smallest eigenvalues  trwt  bw            trwt  bw                      ph  ph                                  ∈   rd×m               pi           th combination       λ − λ do                                         elementsnote                        γ       γ                     number combinations                            calculate   sum ﬁrst         m−                                             largest eigenvalues − λb    let cm −  note each wj ≤ ≤ occurs                                                                 γ         λ ←  λ     λ  ← λ  times wp wp  wph according lemma                wehave                                                    λ ← λλ                                      ∗t    ∗                                     trw aw           l·trw aw        max   trwt bw           l·trw ∗t bw ∗             end    wt                                                                                          ∗                                                           ν ν  νm whereν ν  νm  trwpaw ptrwpaw p···trwphaw ph                       trwt  bw    trwt bw    ···trwt bw                                     − λb                  ph  ph          ﬁrst  largest eigenvectors                                     trw   aw ph          trw aw                                  ≤ −  ≤     ph       ≤                                    inthecaseof              trwt  bw         max   trwt bw          ph  ph   wt                                ∗                                                             · μ μ  μmwhereμ μ  μm    according theorem  know reduced      ﬁrst largest eigenvectors zt azandz   dimension  increases optimal value decreased   zd−r eigenvectors corresponding  monotonously   optimal value equal − zero eigenvalues                       tra           trw aw  ≥ tra  trb max   trwt bw   trb           wt wi                                                                                 table  algorithm optimization problem    hand max trw  aw      αiand                    wt wi                            min  trw  bw        βiwhereαααm  wt wi  ﬁrst largest eigenvalues aandβββm  neighborhood minmax projections  ﬁrst smallest eigenvalues                                                      method neighborhood minmax projectionsnmmp         trw  aw    αα···αm   max   trwt bw ≤  β β ···β                      described table   order speed pca  wt wi                         result bound optimal value given used preprocessing step performing nmmp                                                                                           tra          trw aw    α α ···α                  denote covariance matrix data denote       ≤                   ≤                                                                         ⊥  trb    max   trwt bw   β β ···β                              φ                          φ    φ         wt wi                                     null space  orthogonal complement                                                     ijcai                                                                                                                                                                                                               preprocessing eliminate null space                                variance matrix data obtain new data                                                    d×n                                                                     ∈        rank                                                                                                −                                                                  −                 −      input                                                                         −                                                                −                                                                                       −                           d×n                                   − − −     − − − −            xn ∈  kwi kbi             ˜      ˜                                            original data       pca    calculate sw sb according eq eq                                                                                                                                                         calculate using algorithm described table                                                                                                                                                                     output                                                                                                   d×m                               −         xwherew   ∈                                             −                                                                −                                                                                     −                                                                −                                                                  − − − −      − − −                   table  algorithm nmmp                           lda             nmmp    known null space st eliminated figure  ﬁrst dimensions original  lose information fact easy dimensional data set bcd twodimensional sub  prove null space st comprises null space space pca lda nmmp respectively illus  ˜                     ˜                               trates nmmp ﬁnd lowdimensional transformation  sw null space sb deﬁned section  suppose  ∈ φ⊥ ξ ∈ φthen                                 preserving manifold structure discriminability                    ˜             ˜              ξ sbw   ξ    sbw                  distance metric learning important problem                    ˜            ˜                        ξ sww   ξ    sww                distance based classiﬁcation method learning ma                                                        halanobis distance metric learn positive semideﬁ    eq demonstrates eliminating null space                                                        nite matrix using mahalanobis distance met  covariance matrix data affect result pro                                                          ric xi − xj  mxi  −  xj replace euclidean  posed method use pca eliminate null space                                                                             distance metric xi − xj xi −  xjwherem      ∈  covariance matrix data                       d×d                                                                      xi xj ∈  note positive semideﬁnite                                                                                                discussion                                         eigendecomposition  vv  wherev                                                            σvσv  σdvd σ eigenvalues eigen  method closely connected lda                                                        vectors mahalanobis distance metric  supervised dimensionality reduction methods goals                                                                                       formulated xi − xj xi − xjin  similar try maximize scatter                                                        form learning mahalanobis distance  tween different classes minimize scatter                      ˜                    ˜            metric learn weighted orthogonal linear transformation  class matrix sb deﬁned eq sw deﬁned                                                        nmmp learns linear transformation constraint  eq parallel betweenclass scatter matrix sb                                                           viewed special case learn  withinclass scatter matrix sw lda respectively                                                        ing mahalanobis distance metric weight value  fact number neighbors reaches number                                                        σi   note directly learning matrix  total available neighborskwini −andkbin−ni                                                        difﬁcult problem usually formulated  ni data number class iandn number                     ˜    ˜                            semideﬁnite programming sdp problem com  total data sb  sw  st similar                                                        putation burden extremely heavy learn  sb  sw  st lda                                                        transformation instead learning matrix prob    comparison lda impose                                                        lem easier solve  faraway pairwise points class close  each makes focus improvement  discriminability local structure property  experimental results  especially useful distribution class data evaluated proposed nmmp algorithm data  complex gaussian toy example illustrate sets compared lda lmnn method data  itfigure                                          sets used belong different ﬁelds brief description    toy data set consists three classesshown dif data sets list table   ferent shapes ﬁrst dimensions classes use pca preprocessing step eliminate null  distributed concentric circles dimen space data covariance matrix st lda sin  sions gaussian noise large variance figure  gularity problem existing reduce dimen  shows twodimensional subspace learned pca lda sion data withinclass scatter matrix sw  nmmp respectively illustrates nmmp ﬁnd nonsingular  lowdimensional transformation preserving manifold struc each experiment randomly select samples  ture discriminability                      class training remaining samples testing    compared lda method able ex average results standard deviations reported  tract discriminative features singularity problem  random splits classiﬁcation based knearest  existing lda occur naturally             neighbor classiﬁerk experiments                                                    ijcai                                                                                                  iris bal   faces  objects  usps   news                      class                                                                       training number                                                       testing number                                                    input dimensionality                                                 dimensionality pca                                                                table  brief description data sets               data set  method   projection number accuracy   std dev training timeper run             iris      baseline                                               –                        lda                                                                        lmnn                                                                       nmmp                                                             bal       baseline                                               –                        lda                                                                        lmnn                                                                       nmmp                                                             faces     baseline                                             –                        lda                                                                      lmnn                                                                    nmmp                                                            objects   baseline                                             –                        lda                                                                      lmnn                                                                    nmmp                                                            usps      baseline                                              –                        lda                                                                       lmnn                                                                      nmmp                                                            news      baseline                                             –                        lda                                                                       lmnn                                                                     nmmp                                                                                    table  experimental results each data set      worth noting parameters method variations samaria harter  including ex  sensitive fact each experiment simply set kbi pression facial details each image database   set kwi nifor each class iwhereni size  ×   graylevels  training number class                             experiment preprocessings performed    experimental results reported table  use pca preprocessing step result method  recognition result directly performed preprocess better lda baseline lmnn  ing pca baseline                           good performance computation burden    following details each experiment extremely heavy    uci data sets                                     perform experiments face    experiment perform small data sets iris databases obtain similar results say method  balance taken uci machine learning reposi demonstrates better performances uniformly       tory  class distributions data sets object recognition  complex lda works method demon                                                          coil database nene et al  consists im  strates competitive performance                  ages  objects viewed varying angles interval    face recognition                                    ﬁve degrees resulting  images object    att face database orl database                                                          experiment each image downsampled size  cludes  distinct individuals each individual  dif ×  ferent images images taken different times   saving computation time                                                          similar face recognition experiments results    available httpwwwicsuciedu mlearnmlrepositoryhtml method lmnn better lda                                                    ijcai                                                     
