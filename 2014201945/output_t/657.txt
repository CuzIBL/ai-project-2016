 boosting kernel discriminant analysis application tissue classiﬁcation                                       gene expression data                                        guang dai  dityan yeung                              department science engineering                              hong kong university science technology                                   clear water bay kowloon hong kong                                       daiguangdyyeungcseusthk                         abstract                          masip vitria  effectively integrate boost                                                         ing lda techniques developed recently fur      kernel discriminant analysis kda   ther enhance classiﬁcation performance      effective nonlinear techniques dimension   hand major problem linear subspace      ality reduction feature extraction ap methods pca lda fail extract      plied wide range applications involving high nonlinear features representing higherorder statistics      dimensional data including images gene expres   der overcome limitation kernel dimensionality reduc      sions text data paper develops new al tion techniques kernel principal component analy      gorithm improve overall performance sis kpca sch¨olkopf et al  kernel discriminant      kda effectively integrating boosting analysis kda baudat anouar  lu et al      kda   techniques proposed method called      dai qian  xiong et al  dai yeung       boosting kernel discriminant analysis bkda pos zheng et al  proposed recently extend lin      sesses appealing properties like ear dimensionality reduction techniques nonlinear versions      kernel methods handles nonlinearity disci using kernel trick demonstrating better performance      plined manner computationally attrac applications obtained using linear coun      tive second introducing pairwise class discrimi terparts basic idea ﬁrst map each input data point      nant information discriminant criterion ∈ rn feature space nonlinear mapping φ      simultaneously employing boosting robustly ad  apply corresponding linear dimensionality reduc      just information improves classi tion algorithm similar linear counter      ﬁcation accuracy calculating signiﬁ parts kdabased methods generally better kpca      discriminant information null space based methods classiﬁcation tasks kdabased      withinclass scatter operator effectively deals algorithms usually suffer small sample size problem      small sample size problem widely number training examples available usually      encountered realworld applications kda    smaller dimensionality feature space espe      fourth taking advantage boosting    cially highdimensional data order overcome      kda techniques constitutes strong ensemble  problem approaches developed based dif      based kda framework experimental results       ferent criteria recently effective solutions dai      gene expression data demonstrate promising qian  zheng et al  proposed cal      formance proposed methodology              culate optimal discriminant vectors null space                                                         withinclass scatter operator signiﬁcant discrim     introduction                                       inant information exists hand similar                                                         ventional ldabased techniques lotlikar kothari   principal component analysis pca linear discriminant tang et al  dai yeung  performance  analysis lda classical feature extraction dimen kda degrades following deﬁciencies  sionality reduction techniques broadly used tasks referred nonbalanced problems paper  volving highdimensional data generally believed  pattern classiﬁcation problems face recognition  multiclass pattern classiﬁcation problems op  tissue classiﬁcation gene expression data ldabased timal criterion based conventional betweenclass  algorithms usually outperform pcabased ones reason   scatter directly related classiﬁcation accuracy  optimizes lowdimensional representa particular corresponding dimensionality reduc  tion objects classiﬁcation maximizing ratio tion procedure tends overemphasize interclass dis  betweenclass scatter withinclass scatter tances wellseparated outlier classes input space  simply optimizes object reconstruction tak expense classes close each lead  ing class information consideration ldabased   ing signiﬁcant overlap  algorithms proposed attrac  expression average withinclass scatter  tive approach new lda algorithms lu et al  implicit assumption classes weight                                                    ijcai                                                                                                            xio ∈x                             ∈               covariances fact class dominant  corresponding label      covariance simultaneously outlier class input implicit nonlinear mapping φ  rn →fthen images                                                                                        xio       space withinclass scatter fail estimate cor represented set φ   conven      rect value improved classiﬁcation minimizing                           sφ                                                         tional betweenclass scatter operator  withinclass scatter      spread outlier classes neglecting min sφ                            sφ      imization covariances                    operator population scatter operator ex                                                         pressed sφ     mφ −mφmφ   −mφt   sφ     paper improve overall performance                                                                                           ni φxio  − mφφxio  − mφt  sφ  sφ   kda proposing novel kda algorithm called boosting io                        kda  bkda proposed approach effectively integrates sφ      ni  φxio  − mφφxio  − mφt                                                              i  io             i     boosting technique recently developed kda mφ   ni φxio  mφ        ni  φxio   algorithms based pairwise class discriminant informa ni io                  io                                                            maximize fisher criterion obtain optimal  tion bkda approach employs boosting technique                                                         projection directions  robustly calculate pairwise class discriminant information  integrated scatter operators order solve              wt sφw                                                                           φ  nonbalanced problems kda boosting                                                                                                                         wt sφ  applied lda far studied kda                          worthwhile mention appealing properties practical applications major  proposed bkda method                           problems kda socalled small sample size problem    handles nonlinearity disciplined manner respect  degeneracy                                                         class scatter operator general problem solved      computationally attractive like kernel methods                                                         applying techniques pseudoinverse kernel pca    introducing pairwise class discriminant informa qp decomposition recently effective meth      tion discriminant criterion simultaneously ods dai qian  zheng et al       employing boosting robustly adjust information veloped explore signiﬁcant discriminant information      effectively overcomes nonbalanced problems null space withinclass scatter operator general      kda increases classiﬁcation accuracy simple efﬁcient approach ﬁnding signiﬁcant dis                                                         criminant information calculate optimal discriminant    effectively boosts signiﬁcant discriminant informa                                    tion contained null space withinclass scatter                      sφ      sφ                                                         vectors intersection subspace       operator simultaneously deals small sam spect following modiﬁed criterion      ple size problem algorithm analysis                                                                       φ    wt sφw   w      presented applied easily boost        jf                              discriminant information included furthermore situations nonlinear feature extraction      thogonal complement null space withinclass based criterion  enhances      scatter operator                                  overall performance terms classiﬁcation accuracy nu    constitutes strong ensemblebased kernel kda  merical stability addition pointed ac      framework taking advantage boosting cording chen et al  conventional lda      kda techniques                                    dimensionality training examples total    demonstrate effectiveness bkda method number examples useful discriminant information  apply proposed method effectively extract discriminant null space withinclass scatter operator                                                                                                         −  features tissue classiﬁcation gene expression data lost especially case  experimental results conﬁrm bkda method supe discriminant information fully dis  rior existing dimensionality reduction methods carded kda described situation avoided  terms classiﬁcation accuracy                       choosing kernel function appropriately                                                         dimensionality nonlinearly mapped input space     nonlinear feature extraction kernel            larger number training examples      discriminant analysis                                 boosting kernel discriminant analysis  nonlinear extension lda kda essentially performs  learner  lda feature space note inﬁnite  dimensional regarded hilbert space  boosting kernel discriminant analysis  scatter matrices input space lda corre similar lda kda suffers nonbalanced  spond operators kda operator fwe problems described section  present  let denote null space   ax  bkda algorithm combines strengths boosting  anda   orthogonal complement  kda techniques effectively solves nonbalanced  ⊕                                        problems kda time                                                           boosting general machine learning metaalgorithm    suppose denotes training set examples belonging                                              rn         improving accuracy given learning algorithm  classes each example vector let effective boosting algorithms referred ad   ⊂x                                            xio        ith class containing ni examples aboost used conjunction learning al  denoting ioth example addition each example gorithms improve performance boosting algorithms                                                    ijcai                                                                                                              −                           adaptive sense classiﬁers built tweaked generally larger value qiio indicates greater difﬁ  favor instances misclassiﬁed previous classi                       xio                                                             culty classifying example wrt previous boost  ﬁers freund schapire  speciﬁcally                                                                               ing results qiio emphasizes difﬁcult exam  derlying idea adaboost based sample distribu                                      tion essence measure hard classify ples withinclass covariance class  example notable multi based adaboostm algorithm deﬁnitions  class problems version adaboost called adaboostm sφ     sφ                                                            propose bkda algorithm detailed  outperforms adaboostm realworld applications       prefer using adaboostm paper order fig pointed kda involved  effectively overcome nonbalanced problems kda bkda algorithm calculate optimal discriminant  simultaneously form strong connection kda                               sφ                                                         vectors null space new order ad  adaboostm following freund schapire                                                φ                                                         vantage signiﬁcant discriminant information   lu et al pairwise class discriminant distribution                                        introduced basis mislabel distribution ad detailed calculation procedure sub  aboostm tth iteration adaboostm section  addition discriminant analysis quite  pairwise class discriminant distribution dt classes strong feature extraction technique classiﬁcation                                     ij                 result boosting process forward  xi xj calculated follows                                                       small pseudoloss t general sampling procedures                         pn           ´             ni  Γtxio   Γtxjo                                          employed artiﬁcially weaken corresponding discrim  dij                                                                inant technique bkda choose examples                                                                                                                                each class based qiio focus hardest examples   mislabel distribution Γt ∗ measures extent each class features extracted discriminant analy  difﬁculty discriminating example  im sis simple nearest neighbor classiﬁer generally employed  proper label ∗ basis previous boosting results classiﬁcation order consistent adaboost                                                                                           ∗  obviously larger value dij intuitively indicates worse algorithm bkda hypothesis ht  exam  separability classes xi xj embody ple  class ∗ built easily based normalized  ing closer           nearest neighbor classiﬁer gives results identical    address ﬁrst nonbalanced problem kda classical nearest neighbor classiﬁer                                              sφ  place ordinary betweenclass scatter operator  weighted betweenclass scatter operator sφ              calculate optimal discriminant                                                            vectors feature space         xc− xc     sφ            wdt mφ − mφmφ − mφt                     ij                  follows efﬁciently calculate         ji                                       signiﬁcant discriminant information null space                                                                   sφ  weighting function w generally chosen variant  furthermore basis vari                                                             sφ     sφ  monotonically increasing function  simplicity let ants  represent corresponding popu  w  paper obviously based deﬁnitions                 sφ    sφ   sφ                                                         lation scatter operator    described  pairwise class discriminant distribution weighting section  optimal discriminant vectors obtained  function w seen classes sep                                                          sφ   sφ  respect following criterion  arated potentially impair classiﬁcation                                            performance heavily weighted                      φ    wt sφ  w    addition weighting scheme employed       jf                           alleviate second nonbalanced problem negative  effect outlier classes estimating withinclass scat furthermore based criterion cal                                                               sφ     sφ                             sφ  ter operator tth iteration replace ordi culate   requires calculating                               sφ  nary withinclass scatter operator weighted sφ  ﬁrst computation sφ  sφ                     sφ                                                                             class scatter operator follows                   quite intractable extent following reasons                                                                  φ                                φ            xc xni                                        unlike  intractable directly compute      φ                     φ         φ                                                               φx  − φx  −                     φ        φ                iio                     eigenanalysis sinces explicitly expressed                                                                                                                               sφ    aat                                                                         operator explicit                                                                                       φ   ri   ji wdij  relevancebased weight                                                                                   mulation  direct computation  infeasi                          io             io  class xi     Γ letus         sφ                                        iio                                ble  general large   according  highlight characteristics sφ                                   sφ                                                       cevikalp et al   indirectly calculated   −                                                           φ                                φ      incorporating ri  ensures estimated f−s  basis ﬁrst calculating       sφ                                                                                           inﬂuenced slightly class outlier  high computational complexity involved      class reasonable class sep time order efﬁciently solve problem provide      arated classes following theorem      withinclass covariance operator class new      space compact effect clas simplicity kda subsection  described      siﬁcation tang et al                      essentially similar based st                                                    ijcai                                                                                          xio  xio ∈    √                                     sφ     input set training examples                   ΦΦc  dimensionality usu   rn                                                          cio   ni    set mis   ally high inﬁnitedimensional kpca car                             ∈               ∈   labels         ioj   io                                  Ξt Ξ                 ×                                                   ried eigenanalysis instead size    ni   initial mislabel distribution                  xio      ×               io                                              training set φ  ann matrix                                                                                                     Γ    nc−  small constant ε                                                   jo  nj                                                         ﬁned                                                                  ij ij     ij    ioni                 max                                                    io     jo                                                                               φx φx   kernel trick Ξ Ξ     − calculate terms dt  rt qt                                       ij              iio          expressed                                                                                                       − select hardest examples class based qt                                                iio         Ξt Ξ       −                  form training subset ⊂x                                                                                                                                                         − apply kda subsection  st constitute                                                          × matrix terms let λl       kdabased feature extraction technique denoted ith positive eigenvalue                                       yiot ∈ rr               kdat apply kdat  obtain                                     Ξt Ξ                  θ                             ∗  ∈                 yt     corresponding eigenvector   respectively        build hypothesis ht    subset Ξe   −            iot                                          lλl   constitute orthonormal basis         ∈  corresponding st                                                       sφ       sφ     −                                                      dai qian  zheng et al        calculate pseudoloss based ht  t                       ∈ rn                                        Γtxio  yiot − yiot        input        transformed                        ht  ht                                            ∈m                                     lowdimensional space kpca follows           iio                                                                                                                                                             − set βt  t − tifβt ≤ εthentmax  −              φx     −  Υ             break                                                                          −        −                                                          θθ    eλ      λ         −                                xio                                                         update mislabel distribution Γ  Γ                                 Υ                                 yiot − yiot                    identity matrix      kk               io       ht  ht        Γtx  jβ                                                         nc        io      xio                                                          kk kc kc  ki   φ  φ       −             xio                            training examples xio  rn  xio        normalize Γ    Γ                                                                    φ                                                                                                       io           io                    lo                                                                                                                mapped corresponding points       Γ     llog∈m Γ                                                                                            lowdimensional space  scatter matrices sb   end                                                                                               φ                                                         sw  lowdimensional space corresponding   output ﬁnal hypothesis                                                                                                                          sφ  computed following ap              −   tmax          yt                            supi∈c   logβtht   proaches simplicity paper adopts ﬁrst approach                   example ∈  corresponding nonlinear feature                                 φ      φ                                                           − corresponding deﬁnitions  sb   vector extracted kda kdat subsection                                                                                                  reconstructed based training examples                                                           rio  rm                                                                             figure  summary bkda algorithm            − kernel trick scatter matrices directly                                                                                pt sφ         pt sφ                                                             computed using                     theorem  subspace sφ    sφ    ∈     similar kpca                                                                                                     sφ                             sφ        sφ      furthermore directly calculate null space      equivalent subspace                          φ     φ    φ      φ                      sφ                                   ∈fwheres         conventional  eigenanalysis                                                    ×                                   γ      γ  population scatter operator                               matrix speciﬁcally let                                                             eigenvectors corresponding zero eigenvalues    based theorem   propose use fol                                                                                             sφ     sφ                      pv  lowing steps calculate optimal discriminant vectors andthen    spanned                                                            result discriminant criterion     calculate orthonormal basis sφ                                                                                                transformed projection space sφ  sφ                                 sφ       sφ                                                         calculate orthonormal basis      zt vt pt sφ pvz    zt vt vz  z                                                        jf                                let        sφ     sφ                                                                              struct      calculate optimal discrimi zl eigenvectors sbvsortedin  nant vectors respect discriminant criterion                                                         descending order corresponding eigenvalues accord  sφ      sφ                                             ing dai qian  zheng et al  clear                                                               wl  pvzl  constitute optimal dis    follows present detailed computation criminant vectors respect corresponding criterion  procedure discussions ﬁrst need calcu                                                                   sφ      sφ                                               sφ                      sφ         input pattern  correspond  late orthonormal basis  applying kpca  ing nonlinear feature vector extracted kda procedure           sφ                                                                                               rewritten                       described computed   wφx ∈r                                                            wwr expression rewritten                       c                 φ                                    kernel trickp follows                        Φ Φt   ΞΞ                  wφx    vt et −                                                                                                                                                                                 × matrix terms                  − mφ        xni  − mφ       Ξ                                     xnc   Φi φ        φ               identity matrix                                                      ijcai                                                        experimental results                               capable improving overall performance ker  gene expression data usually highdimensional involving nel functions three data sets advantages  large number genes small sample size discussed specially bkda effectively improve  effectively extract discriminant features plays important performance corresponding kdar num  role gene expression data classiﬁcation ye et al ber features addition ﬁnd ﬁxed  evaluate performance proposed bkda algorithm small values bkda fails show effectiveness  conduct gene expression classiﬁcation experiments compared kda ensemblebased learner  compare bkda dimensionality reduction weak  methods                                                                                       nonbalanced problems mentioned ex                bkda training            bkda training                                                                            bkda test                bkda test                                                                            kda−r                     kda−r  ist multiclass classiﬁcation problems experiments            kda                      kda                                                                                     performed three different data sets involving  classes each order demonstrate behavior bkda                             tumors  human tumor examples corresponding                                                                                      classification  accuracy rate        different cancer types                                                 classification  accuracy rate                                                                                        subset  tumors contains  human tumor                                                                                                                                                                                                                                             values bkda     values bkda      examples corresponding  different cancer types          max                       max                                                                                              subset  tumors contains examples cor                                                                                                                                                            bkda training             bkda training      responding various human tumor normal tissue                  bkda test                 bkda test                                                                         kda−r                    kda−r      types each contains examples               kda                       kda                                                                                         each data set randomly partitioned disjoint training                                                                                                          test sets training set  each class xi contains  examples ﬁrst data set challenging                                                                                                         classification  accuracy rate    second data sets set ﬁrst classification  accuracy rate   data set ex                                                                                                                                                                                                                                                                     values bkda       values bkda  periments preprocessing gene selection applied     max                       max  data sets each feature extraction method use                                simple efﬁcient minimum mean distance rule eu                                                                                                bkda training              bkda training                                                                          bkda test                  bkda test                                                                                                  kda−r  clidean distance measure assess classiﬁcation accuracy          kda−r                       kda                                                                          kda  simultaneously build corresponding normalized ver                         sion hypothesis bkda based lu et al                       each experiment repeated  times average classiﬁ                      cation rate reported kernel methods use rbf                                                                                                   classification  accuracy rate    kernel kz zexpz − z σ polynomial kernel classification  accuracy rate                                                                                         zt                addition          σ         σ                                                                                                                                                                                                                                   values bkda                                                                  values bkda        max  noted bkda effectively boost discrim      max  ination ability different features extracted kda                             bkda reduce computational cost simultaneously figure  comparative performance bkda kda dif  consider space limitation paper simply ﬁx ferent tmax values bkda bkdatraining bkdatest  number features bkda −  each data set note results bkda training set test set  number classes                      spectively polynomial kernel  tumors rbf kernel    comparison ﬁrst set experiments implements  tumors polynomial kernel subset  tumors                                                       rbf kernel subset  tumors polynomial kernel  bkda kda    zheng et al  using polyno subset  tumors rbf kernel subset  tumors  mial rbf kernels kda zheng et al   effectively calculate signiﬁcant discriminant informa  tion null space withinclass scatter operator second experiment compares bkda ef  proposed method ye et al  gene expression fective linear dimensionality reduction methods including  data classiﬁcation essence special case kda high bdlda lu et al alpphe niyogi  dimensional spaces furthermore explicitly show effec npe et al  effective kernel  tiveness bkda each comparison kda offers base based nonlinear dimensionality reduction methods including  lines kda denotes maximum classiﬁcation rate kpca sch¨olkopf et al gdabaudat anouar  variant features kdar denotes classiﬁcation rate based  kdda lu et al bwkdadai yeung  ﬁxed features addition bkda number cho  kdaqr xiong et al  table  reports  sen examples class set  l−wherel num maximum classiﬁcation rates different methods three  ber examples each class training set experi data sets bkda generally effective  mental results shown fig  reveal expected bkda methods compared                                                           notice extensive research based clas     data sets available httpdiscovermcvanderbiltedu siﬁers tissue classiﬁcation gene expression data stat  discoverpublicmcsvm                              nikov et al  including knearest neighbor classiﬁer                                                    ijcai                                                     
