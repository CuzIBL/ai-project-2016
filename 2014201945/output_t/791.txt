                                      covariant policy search                                      andrew bagnell jeff schneider                                               robotics institute                                          carnegiemellon university                                              pittsburgh pa                                          dbagnell schneide   ri emu edu                          abstract                            inspired work amari nagaoka                                                           kakade proposed natural gradient algorithm particu•       investigate problem noncovariant behav• lar kakade  proposed scheme generating met•       ior policy gradient reinforcement learning algo• ric parameter space interesting theoretical prop•       rithms policy gradient approach amenable  erties convincingly kakade showed strong empiri•       analysis information geometric methods cal evidence algorithms usefulness particular        leads propose natural metric controller kakade  applies algorithm tetris problem        parameterization results considering bertsekas tsitsiklis  problem partic•       manifold probability distributions paths in• ularly interesting value function methods example        duced stochastic controller investigation   described bertsekas tsitsiklis   demonstrate        approach leads covariant gradient as• nonmonotone performance policy iteration improves        cent rule interesting properties rule policy dramatically small number iterations        discussed including relation actorcritic policy gets worse normal gradient methods in•       style reinforcement learning algorithms al•  cluding secondorder conjugate methods prove        gorithms discussed computationally quite ineffective problem tremendous number        efficient interesting problems lead   rounds mildly increase performance        dramatic performance improvement non     gameplayer method presented kakade  shows        covariant rules                                  rapid performance improvement achieves significantly                                                          better policy valuefunction methods peak     introduction                                        comparable time    recent work reinforcement learning stochastic despite recognizing interesting defect    optimal control focused algorithms search directly general approach intuiting apparently powerful algo•   through space policies building approximate rithm kakade concludes method fails   value functions policy search numerous advantages do• variant leaving problem open present    main knowledge easily encoded policy policy believe appropriate solution    require representational power valuefunction kakades work proper probability manifold    approximation simple extensions multiagent collection each state based    domain convergent algorithms known furthermore policy kakade rely adhoc method    policy search approaches recently scored encour• generating metric instead proper mani•   aging successes bagnell schneider   baxter et al fold distribution paths induced controller     including helicopter control gameplaying  compute metric based special case appropri•     interesting aspect existing gradient search algo• ate average reward rl formulation kakades scheme    rithms noncovariant shown rise bonafide natural metric despite    simple reparameterization policy typically leads observation paper learning noncovariant    different gradient direction applying believe artifact steplength fur•   jacobian reparameterization gradient ther note parametric invariance does require    odd result intuitively difficult justify gradi• metric numerous covariant metrics    ent computation actually indicating direction steep• stronger probabilistically natural invariance demands   est descent problem recognized pattern metric used result motivate method   recognition statistics communities active importantly notion metric path  area research kakade  best distribution allows provide simple natural ex•  knowledge identify problem reinforcement tensions kakades algorithm cover finite horizon    learning suggest techniques information ge• discounted startstate partiallyobserved reinforcement   ometry prove valuable solution             learning problems average reward fi     probabilistic planning                                                                               nally completeness result discovered   kakade relating invariant metric compatible actor  critic methods sutton et al       problem setup notation                      figure  twostate mdp each state controls   stochastic control problem consists paths called self transitions earns reward labels state an•  trajectories space distribution path simply transitions state rewards occur   space function sequence finite controls transitions different states               indexed time space   paper considering partially observed markov   decision problems statevariables   compose path render past fu•  ture independent pomdp defined initial   state state transition probabilities   typically sequence outputs observations              measurable respect  controller   policy usually parameterized feedbacktype   strategy maps history observations   distribution controls derivations   given terms memoryless stochastic controllers   map current state distribution actions figure  log odds actions policies logp   easily extended finite window recurrent rep•  twostate mdp horizontal axis corresponds state    resentations operate observations goal different curves correspond varying notice noncovariant   control problem maximize expected reinforce• policy behavior   ment respect sum   assumed exist controllers reward func•  tion paths pomdp additive time infi• state  state  action  returns state  gets reward   nite time case discounted averaged function  action  transits state  achieves reward   state algorithms discussed neces• consider parameterized probabilistic policies form   sarily predicated assumption reinforcement learning                         arbitrary scale   adaptive version control problem parameter use demonstrate mildly differ•  tept maximize expected reinforcment sampling tra• ent parameterization lead dramatically different behavior   jectories improving policy use notation plot resulting track through space poli•    denote parameter cies using log ratio probabilities actions each                                                       possible states start policy makes   clear context vectors use notation                                                       somewhat likely choose action  state  action          denote inner product respect metric                                                        state  scale    plot log   notation indicates expected value                                                       odds policy state  figure    function  respect distribution                                                         noncovariant behavior quite clear graph    covariance riemannian manifolds               note difficult achieve good                                                       policy using algorithm starting point    meaning steepest ascent                      wrong action overwhelmingly likely chosen   direct policy search methods interested finding state  sampling used compute gradient   direction steepest ascent respect current pa• nearly samples state    rameters wish maximize reward function need improve policy            subject infinitesimal   reparameterize controller terms express  path distribution manifolds   effective infinitesimal policy change terms control problem essentially coupled optimiza•  parameters course remain how• tion integration space possible paths   measure lengths using naive dot product motivates idea instead considering arbitrary   effective change size distance terms parameterization policy   problem leads odd behavior steepest descent consider distance terms change distribution   rules                                              paths resulting policy change view                                                       distribution paths parameterized manifold    noncovariant learning rule                     nominally embedded dimension takes   demonstrate problem consider simple work visualize example consider space   state action mdp described kakadej figure  three possible paths possible distributions   state  executing action  returns state gets three paths smoothly represented parameters   reward  executing action  reward transits figure  left visualization embedding                                                                               probabilistic planning                                                                                                                                                                   derivatives respect each  set zero                                                         solve optimal direction       figure  example probability manifold paths each implies positive definite   axis represents log probability three possible invertible giving   paths manifold formed vary parameters   defining domain right attach   tangent space particular point parameter space                                                                                                       direction steepest descent simply nor•                                                        mal gradient times inverse metric evaluated                                                         point tangency natural gradient     parameters assuming redundancy generate                                                         reimannian manifold amari nagaoka    dimensional manifold case pictured set   distributions  paths general dimension•  ality path probability manifold tremendously  invariance chentsovs theorem   number possible paths important un• confusion surround choice metric   derstand manifold consideration prob• probability manifolds unique answer   ability distribution paths paths metric supposition parametric invariance     study parameterized manifolds like pictured suggested authors issue sub•  domain differential geometry tle metric transforms parameter change ac•  interested establishing riemannian structure man• cording jacobian function connecting parameters   ifold paths mean wish establish metric meets requirement type parametric covariance   tangent space local linear approximation minimum requirement natural suggest met•  manifold point measure small parameter ric preserves essential probabilistic aspects mani•  changes metric tan•       fold consider example functions markov mappings   gent space spanned partials respect congruent cmbeddings sufficient statistics depending   each parameter positive              viewpoint carry distribution paths an•  definite matrix natural thing instead natural recoverable way example con•  just standard dot product dot product sider mapping distributions ppaths   allows represent rotations scalings exactly ppathsgiven congruent embedding depicted   positive definite matrix represent vary   manifold figure  right depict   tangent space point parameterization local   linear approximation point      steepest ascent riemannian manifold   questions naturally arise   steepest descent function defined riemannian                                                         paths   space riemannian metric manifold   paths sense natural quickly answer mapping interchanges role paths   question pursue second sec• splits path probability  each   tions lagrange multiplier argument given schematically split probabilistically fundamental way mani•  makes easy form steepest descent folds ppath ppath imagine ppath   direction                                            smoothly parameterized set distributions similar                                                         each each path uniquely recover                                                         original probability way parameterized distribu•                                                     tion paths embedded different path space                                                      equivalence actually phrased category the•                                                        oretic way morphisms congruent embed    form lagrangian                                dings ichentsov  general congruent embeddings     probabilistic planning                                                                                thought simply generalizations example de•   compute zj  i—z  jdjlograx   picted allow arbitrary permutations arbitrary   probabilities different paths stemming single path   compositions control problem return   arise simple permutation state variables use markov property invariance tran•  change coordinates ones redundant information sition probabilities given actions algorithm   natural require metric congruent em• compute jogp£ simply details ex•  bedding isometry tangent space preserves                                                         tracted proofs potentially   length vectors make change                                                         better ways sample   size metric distribution pathsthen                                                           compute natural gradient simply invert ma•  carrying change through measure                                                         trix multiply gradient computed using standard   change using  adding requirement invariance                                                         policy search methods   respect congruent embeddings leads unique   scale metric manifold chentsov  metric  limiting metric infinite horizon problems   wellknown statistical inference fisherrao metric   written fisher information matrix deg wellknown result statistics degroot  gives dif•  root                                           ferent form fisher metric appropriate regularity                                                         conditions quickly derive result show                                                         gives simple form path probability metric       way derive metric think dis•  tances probability spaces kldivergencc rel•  ative entropy distributions natural diver•  gence changes distribution manifestly in•  variant reparameterization think derivatives   small changes parameters differentials dis•  cover unique metric secondorder   taylor expansion kldivergence agrees   fisher information scale note di•  rection kldivergence irrelevant secondorder                     famari nagaoka     fisherrao metric pathspace                                                                                                               manifold   issue derive fisher metric line follows second integrating   space path distributions turns case parts fifth follows observing total proba•  processes underlying markovian state easy bility constant   involves computations make likeli• show limit leads simple metric   hood ratio approach standard gradientbased reinforcement infinite horizon case convergent metric   learning                                             normalize metric particular total length                                                         path denoted metric defined    derivation finitetime path metric             scale perfectly justified   fisher information metric involves computing      theorem  infinitehorizon metric ergodic                              fortunately easy markov process fisher information matrix limits   essential algorithm gradient methods like expected fisher information policy each state   reinforce gpomdp clever method computing  control stationary distribution states actions                    expected score function proof use indicate tstep finitehorizon metric   expected score gradient correlation   score fisher matrix following simple algorithm   unbiased converges surely   regularity conditions baxter et al  number                                                                                                            sample paths goes infinity   algorithm  finitehorizon metric computation                                                           markov process write likelihood ratio                                                                                      probabilistic planning  using chain rule continue derivation    limiting distri•   indicating stationary distribution               bution states infinite horizon undiscounted                                                          startstate metrics essentially metric                                                          effective weighting states differing                                                           metrics partially observed problems                                                          policies map observation space partially                                                         observed markov decision process distributions ac•                                                         tions just easy derive appropriate metric using                                                          approach tuple markov chain                                                          subtle changes arguments end                                                          metric using limiting distribution                                                          observations instead states                                                           relation compatible value function                                                              actor critic                                                          kakade noticed fascinating connection limit•                                                         ing metric given theorem  improvement direction                                                          computed class actorcritic methods use special      second equality follows noting like• compatible function approximation techniquesutton et al    lihood ratio independent transition probability matrix  konda tsitsiklis  following kakade let    given action probability application ergodic             let compatible function ap•   theorem line follows fisher informa• proximator linear    tion policy given state second term line     vanishes total probability constant respect         interesting consider happens start type value function approximator initially sug•   state discounted formulation problem case gested used compute true gradient prac•   like metric using general definition path tice clear advantage brings    distributions given naturally weigh start start gradient estimation routines baxter et al  how•   necessarily infinite horizon average case folkwisdom performing infinitesi•   wellknown discount factor equivalent mal policy iteration moving direction best policy    undiscounted problem each trajectory terminates according using approximation    probability   each step use fact derive good properties significantly outperforms standard    metric appropriate discounted formalism gradient methods natural gradient provides insight                                                          behavior let minimize squared valuefunction er•   theorem  startstate metric discounted markov ror    process fisher information matrix equals fisher in•  formation policy each state control    limiting distribution states actions proof    proof similiar infinite horizon case    simply sketch                                     exact advantage function sutton et                                                          al easy check kakade  theo•                                                         rem  direction maximum                                                          policy improvement exactly natural gradient direction                                                          seen simply differentiating minimize                                                          respect noting result sutton et al                                                                                                                       demonstration                                                          consequence results section  ikakade                                                           experimental results exist demonstrating                                                          effectiveness natural gradient method re•                                                         sults using policy improvement compatible function ap•                                                         proximators implicitly computing result demon•                                                         stration computed analytically natural gradient      probabilistic planning                                                                                
