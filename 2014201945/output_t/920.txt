                   convergent solution tensor subspace learning                  huan wang             shuicheng yanthomas huang              xiaoou tang                chinese university        ece university illinois      microsoft research asia            hong kong hong kong         urbana champaign usa             beijing china            hwangiecuhkeduhk          scyanhuangifpuiucedu           xitangmicrosoftcom                                                                                                                                                                                               abstract                          arg maxu tru  sk ru      ratio                                                                                  kt   −  kt        recently substantial efforts devoted trace form arg maxu tru      sk      subspace learning techniques based tensor  order obtain closedform solution each iteration      representation dlda ye et al   sequently derived projection matrices unnecessary      dater  yan et al  tensor subspace     converge greatly limits application algo      analysis tsa et al  context rithms unclear select iteration number      vital unsolved problem computa solution optimal local sense      tional convergency iterative algorithms work following graph embedding formula      guaranteed work present novel tion general dimensionality reduction proposed yan      lution procedure general tensorbased subspace et al  present new solution procedure sub      learning followed detailed convergency proof space learning based tensor representation each      solution projection matrices objec eration instead transforming objective function      tive function value extensive experiments real ratio trace form transform trace ratio optimiza      world databases verify high convergence speed tion problem trace difference optimization problem                                                                   kt             proposed procedure superiority maxu tru sk −λs  λ objective func      classiﬁcation capability traditional solution                                                                                        tion value computed solution pre      procedures                                       vious iteration each iteration efﬁciently solved                                                        eigenvalue decomposition method fukunaga    introduction                                       detailed proof presented justify λ value                                                        objective function increase monotonously  subspace learning algorithms brand  princi                                                                                       prove projection matrix converge                                                     pal component analysis pca turk pentland   ﬁxed point based pointtoset map theories hogan                                       linear discriminant analysis lda belhumeur et al    traditionally express input data vectors                                                          worthwhile highlight aspects solution  highdimensional feature space real applications                                                        procedure general subspace learning based tensor rep  extracted features usually form multidi                                                        resentation  mensional union tensor vectorization process  destroys intrinsic structure original tensor form  value objective function guaranteed  drawback brought vectorization process monotonously increase multiple projection ma  curse dimensionality greatly degrade algo trices proved converge properties en  rithmic learnability especially small sample size cases sure algorithmic effectiveness applicability    recently substantial efforts devoted em  ployment tensor representation improving algorith  eigenvalue decomposition method applied  mic learnability vasilescu terzopoulos  iterative optimization makes algorithm ex  dlda  ye et al  dater yan et al     tremely efﬁcient algorithm does suf   tensorized popular vectorbased lda    fer singularity problem encoun  algorithm initial objectives algo tered traditional generalized eigenvalue decompo  rithms different end solving higher sition method used solve ratio trace optimization  order optimization problem commonly iterative pro   problem  cedures used search solution collec  consequent advantage brought sound theo  tive problem encountered solution procedures  retical foundation enhanced potential classiﬁcation  iterative procedures guaranteed                                                                        verge each iteration optimization problem matrices sk positive semideﬁnite  approximately simpliﬁed trace ratio form  detailed deﬁnitions described afterward                                                    ijcai                                                                                                                     capability derived lowdimensional representation fukk kinds deﬁnitions scale      subspace learning algorithms            normalization    rest paper organized follows section ii              n                                                                             x  ×   kn    reviews general subspace learning based tensor rep    fuk           bii       resentation introduce new solution proce                 dure theoretical convergency proof section diagonal matrix nonnegative elements  iii taking marginal fisher analysis mfa algorithm general constraint relies new  proposed yan et al  example verify graph referred penalty graph similarity matrix sp  convergency properties new proposed solution pro                                                        deﬁned   cedure classiﬁcation capability derived low    kn          −   ×    kn    dimensional representation examined set experi fu             sij    ments realworld databases section iv                       ij                                                          losing generality assume constraint    subspace learning tensor data                 function deﬁned penalty matrix simplicity                                                        scale normalization constraint easily similar  section present general subspace learning frame deduction new solution procedure gen  work encoding data tensors arbitrary order extended eral formulation tensorbased subspace learning ex  proposed yan et al  taking pressed                                                                       data inputs vectors concepts tensor inner produc                                                                                                           xi − xj ×k       tion modek production matrix modek unfolding          ij                      ij                                                             arg max                                referred work yan et al                     xi − xj  ×k          ij                                                                   ij                  graph embedding tensor representation         recent studies shashua levin ye ye                                                        et al yan et al  shown dimensional  denote sample set  xxn  xi ∈    ×m ××m                                         ity reduction algorithms data encoded highorder           total number                                                        sors usually outperform data represented vec  samples let  undirected similarity graph                                                        tors especially number training samples small  called intrinsic graph vertex set similarity ma           n×n                                          representing images matrices instead vectors allows  trix ∈    corresponding diagonal matrix                                                        correlations rows columns exploited  laplacian matrix graph deﬁned                                                       subspace learning                                                          generally closedform solution exists  previ              − dii      sij ∀                                                                ous works ye et al yan et al  utilized iter                               ji                                                        ative procedures search approximate solutions                                                                                    task graph embedding determine low    projection matrices initialized arbitrarily                                                      each projection matrix reﬁned ﬁxing  dimensional representation vertex set preserves                    k−         similarities pairs data original high projection matrices solv  dimensional feature space denote lowdimensional em ing optimization problem                                                                           bedding vertices yyn                          kt  −  kt  k         m ×m ××m                                      k∗            ij  yi     yj   sij   ∈                                                                               embedding vertex     argmax                                                                                                            −    k  assumption yi modek production xi               ij  yi     yj   sij                                            ×m  series column orthogonal matrices ∈                     kt                                                                          tru    sk                                                               argmax                                                                  kt                                     yi    xi ×    ×      ×n                 m                         tru                                                                                                                                       ˜                                                      yi modek unfolding matrix tensor yi   im mkbymk identity matrix maintain sim              k−                                                                        xi × ×k−     ×k    ×n    ilarities vertex pairs according graph preserving                                                                                  ij sij yi − yj yi − yj  sk   ij sij yi −  criterion yan et al wehave                                                                                yj yi − yj                                                                            yi − yj  sij            optimization problem  intractable tra         ∗n              ij        argmin                          ditionally solution approximated transforming                   kn       fu                                                    objective function  tractable approximate                                                    ij  xi − xj ×k  sij         form ratio trace form    argmin                                           ∗                                             fu                                             −                                                               argmaxtru           sk                                                                                   fu function poses extra constraint directly solved generalized eigenvalue                                           graph similarity preserving criterion means decomposition method distortion objec  sequence   similar rep tive function leads computational issues detailed  resentations following parts work commonly following subsection                                                    ijcai                                                       computational issues                             algorithm   procedure tensorbased subspace learning                                                                                           objective function each iteration changed  initialization initialize arbitrary col  trace ratio form  ratio trace form  deduced umn orthogonal matrices  solution satisfy aspects  objective  iterative optimization  function value  monotonously increase  tmaxdo  solution uun converge ﬁxed point  ndo                                                                         work present convergent solution procedure                                                                                                           xi−xj ×ou   ×ou                                                                            ij                   ok  ij  optimization problem deﬁned                          set λ                                                                                                      x −x × ok × s                                                                                         ij                                                                         ij                  solution procedure convergency proof                                                                                             compute  sk  based projection                                                                               k−            section ﬁrst introduce new solution procedure matrices ut ut ut− ut−  tensorbased subspace learning problems  conduct eigenvalue decomposition  convergency proof aspects mentioned                                                                                                    described does exist closedform solu       sk − λs vj  λj vjjmk  tion optimization problem  solve op  timization problem iterative manner each  vj eigenvector corresponding  eration reﬁne projection matrix ﬁxing   th largest eigenvalue λj   efﬁcient method proposed reﬁnement  reshape projection directions sake  stead solving ratio trace optimization problem  thogonal transformation invariance  approximate solution transform trace ratio optimiza                                                                                     m  tion problem  trace difference optimization problem  set                                                                                       deﬁned                                                                           kt                                                                    let     vv    xi xi  vv            k∗               kt                                                                            argmaxtru     sk − λs                     xi modek unfolding tensor                                                                                    conduct eigenvalue decomposition  λ value objective function  computed                      projection matrices previous iteration                              ui  γi ui            iterative procedure converge local op                                                                                             set column vectors matrix ut leading  timum optimization problem  monotonously                                                                                      eigenvectors ut uuum   increase objective function value proved later                                        directly leads superiority ratio trace based opti end                                                                                       mization procedure stepwise solution ut − ut−  mkmk ε  nε set  unnecessarily optimal                           − work break    iteratively reﬁne projection matrices  tailed solution procedure solve tensorbased general end                                                                                            subspace learning problem listed algorithm      output projection matrices ut    analysis monotonous increase property                                                                                                                               im  easy prove  rewrite objective function                                                                                                                        m                                                                             k                    xi − xj ×k      sij                                                                     sup gu     λj          kn     ij     gu                                                                               xi − xj ×k      sij                                                                                                         ij                                                                     mk                                                          algorithm  gut  λj   theory                                                                                                                 gut  ≥ gut−    theorem following terms algorithm  eqn                                                 kt                                                                                 trut  sk − λs ut  ≥ asmatrixs  pos             k−                               itive semideﬁnite wehave   gut ut  ut−ut− ut− ≤                                                                              kt                          k−                                    trut  ut               gut ut   ut ut− ut−                                ≥ λ                                                                              kt                                                                        trut  ut     proof denote gutru − λsku                                                                            k−                               k−                       λ  gut ut  ut−ut− ut−              gut ut−ok ≤ gut out−ok                                                                                                                                                                                                                                          zero eigenvalues trut ut                                                                                         gut−                         positive mk larger number zero eigenvalues                                                    ijcai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     traditional                      traditional                traditional                                                                                                                                                                                                                                                                                                                                 objective  function value                                          objective  function value                                                                    objective  function value                                                                     iteration number             iteration number             iteration number                                                                             figure  value objective function  vs iteration number usps database orl database cmu pie  database traditional method means solution procedure based ratio trace optimization      theorem conclude value ob second condition satisﬁed algo  jective function monotonously increases              rithm  strictly monotonic                                                          theorem  meyer  assume algorithm Ω    proof convergency                             strictly monotonic respect generates se  prove  convergency projection matrices  quence xt lies compact set χ normed  uun need concept pointtoset map xt − xt−→  power set ℘χ set χ collection sub theorem conclusion ob                                                                kn  sets χapointtoset map Ω function χ → ℘χ tained ut converge local optimum  solution procedure tensorbased subspace learning χ compact norm deﬁnition                          map ut−ktout considered                              pointtoset map each ut invariant  experiments  thogonal transformation    strict monotony algorithm pointtoset map section systematically examine convergency                                                        properties proposed solution procedure tensorbased  Ωχ →  ℘χ given initial point algorithm gen                                                        subspace learning marginal fisher analysis  erates sequence points rule xt ∈ Ωxt−                                                        mfa instance general subspace learning  suppose  χ → continuous nonnegative function  algorithm called strict monotony  ∈ Ωx implies mfa shown superior traditional subspace  jy ≥ jx  ∈ Ωx jyjx imply learning algorithms linear discriminant analysis                                                                                                                                                    lda details mfa algorithm referred yan                                                                     let set χ direct sum orthogonal ma   et al   evaluate classiﬁcation capability               ×m                                    derived lowdimensional representation solution  trix space     data space χ                                                    procedure compared traditional procedure proposed   m×m       m×m            mn×m                         algo   ye et al  yan et al  tensorbased  rithm  produces pointtoset algorithm respect algorithm image matrix rd tensor used input             kn  jxgu    proved strictly mono image matrix transformed corresponding vector  tonic follows                                     input vectorbased algorithms    theorem  pointtoset map algorithm   strictly monotonic                                                      data sets    proof   theorem gut−k   ≤      kn                                               three realworld data sets used usps hand  gut ﬁrst condition strict monotony                                                               written dataset images handwritten digits  satisﬁed second condition                                                        pixel values ranging    example prove condition satisﬁed                                                                                           benchmark face databases orl cmu pie  gu      gu        proof theorem      t−                                     face databases afﬁne transform performed                                              gut−gut     λ   gut−    samples ﬁx positions eyes mouth                          sk computed ut− proof center orl database contains  images   theorem exists orthogo sons each image normalized size                                    nal transformation ut− ut  shown al pixels cmu pie pose illumination expression  gorithm  kind orthogonal transformation                                                      database contains  facial images  peo  normalized reshaping step ut−ut  ple experiment subset ﬁve near frontal poses                                similarly prove ut  ut−                                                            available httpwwwstatclassstanfordedu tibselemstat    claim based assumption exist learndatahtml  duplicated eigenvalues                           available httpwwwfacerecorgdatabases                                                    ijcai                                                                                                                                                                                                                                            traditional                                                                                                                                                                                                                                                                                                                                                                                          traditional                                                                                                                                                                                                                                                                                                      traditional                                       projection  difference          projection  difference                                                                    projection  difference                                                                                       iteration number             iteration number             iteration number                                                                                              ut − ut−              ut − ut−               ut − ut−                                                                                                                                                                                                                                      traditional                                                                                                                                                                                                                                                                                       traditional                                                                                                      traditional                                                                                                            projection  difference                                       projection  difference          projection  difference                                                                                                                                                                  iteration number              iteration number             iteration number                                                                                              ut − ut−               ut − ut−              ut − ut−    figure  step difference projection matrices vs iteration number ad usps database orl database  cf cmu pie database    illuminations indexed    used normalized size table  recognition error rates  orl database                                                                    method    gp   gp  gp    monotony objective function value                         wo dr                                                                            lda           subsection examine monotony property       mfa rt         objective function value solution procedure com         mfa tr          pared optimization procedure stepwisely trans    tmfa  rt         forms objective function ratio trace form      tmfa  tr          usps orl pie databases used evalua  tion detailed results shown figure  ob  served traditional ratio trace based procedure does table  recognition error rates  pie database  converge new solution procedure guarantees  monotonous increase objective function value       method    gp   gp  gp  commonly new procedure converge          wo dr        iterations ﬁnal converged value objec       lda           tive function new procedure larger      mfa rt         value objective function iteration ratio    mfa tr          trace based procedure                                           tmfa  rt                                                                          tmfa  tr           convergency projection matrices  evaluate solution convergency property compared  face recognition  traditional ratio trace based optimization procedure subsection conduct classiﬁcation experiments  calculate difference norm projection matrices benchmark face databases tensor marginal fisher  successive iterations detailed results dis analysis algorithm based new solution procedure  played figure  demonstrates projection matri tmfa tr compared traditional ratio trace based  ces converge  iterations new solution pro tensor marginal fisher analysis tmfa rt lda ratio  cedure traditional procedure heavy oscilla trace based mfa mfa rt trace ratio based mfa  tions exist solution does converge shown mfa tr mfa tr means conduct tensorbased  figure  recognition rate sensitive oscillations mfa assuming speed model training pca  caused unconvergent projection matrices clas conducted preprocess step vectorbased algorithms  siﬁcation accuracy degraded dramatically          pca dimension set nnc sample number                                                    ijcai                                                     
