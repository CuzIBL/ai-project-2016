         unsupervised dimensionality estimation manifold learning                           highdimensional spaces tensor voting                                  philippos mordohai gerard´     medioni                                      university southern california                                  email mordohaimedioniirisuscedu                          abstract                          orientation each point medioni et al  votes                                                        tensors accumulation process      address dimensionality estimation nonlin  siderably powerful accumulation scalars      ear manifold inference starting point inputs vectors terms types structure      high dimensional spaces using tensor voting   scribed robustness noise point casts vote      proposed method operates locally neigh    neighboring point carries estimate orienta      borhoods does involve global com     tion receiver given orientation voter      putations based information propagation case unoriented inputs meaningful votes cast      neighboring points implemented voting  function relative position points result      process unlike local approaches man   voting new tensor each point encompasses      ifold learning quantity propagated  weighted contribution point’s neighbors      point scalar form      tensor provides considerably richer infor dimensionality manifold each point      mation accumulation votes each point   dimensional space indicated maximum gap      provides reliable estimate local dimensionality eigenvalues tensor eigenvectors span      orientation potential manifold normal space manifold ﬁrst property makes di      going through point reliable dimensionality  mensionality estimation procedure parameters      estimation point level major advantage require tuning estimates reliable averag      competing methods absence  ing shown section  allow handle data      global operations allows process signiﬁcantly varying dimensionality arguably im      larger datasets demonstrate effectiveness portant contributions approach pres      method variety challenging datasets ence outliers boundaries intersections multiple mani                                                        folds pose additional difﬁculties unsupervised                                                        learning ability method furthermore nonmanifolds    introduction                                       hyperspheres inferred  number algorithms manifold learning unor tempt estimate global “unfolding”  ganized points recently proposed machine attempt estimate mapping input  learning literature include kernel pca scholkopf¨ space embedding space brand  teh  et al  locally linear embedding lle roweis roweis  certain advantages  saul  isomap tenenbaum et al  charting rived mapping feasible  brand  brieﬂy described section  case hyperspheres multiple manifolds nec  aim reducing dimensionality input space way essary tasks including interpolation valid paths  preserves geometric statistic properties manifold followed using estimated tangent sub  data isomap instance attempts preserve geodesic spaces input points projecting desired direction  distances points manifold “unfolded” moving projection  mapped space lower dimension common  operations local time complexity  sumption desired manifold consists locally lin odnlogn number points enabling  ear patches relax assumption requiring handle datasets orders magnitude points  manifolds smooth smoothness current state art incurring unreasonable com  context property manifold’s orientation vary putational cost terms storage requirements each  gradually moves point point property point od current implementation probably  encoded votes each point casts neighbors limited order hundreds manifold    representation each point form second learning methods operate assumption  order symmetric nonnegative deﬁnite tensor eigen data randomly sampled manifold way  values eigenvectors fully dimensionality close uniform does favor certain parts ordirections manifold                           ifold learning enforcing local isometry lengths    paper organized follows section brieﬂy sides triangles neighboring points preserved dur  reviews related work section  describes tensor voting ing embedding constraints expressed  framework section  contains experimental results classic terms pairwise distances optimal embedding  datasets ﬁnally section  concludes paper discusses semideﬁnite programming method  directions future work                    computationally demanding lle isomap                                                        reliably estimate underlying dimensionality inputs    related work                                       locating largest gap eigenvalues                                                        gram matrix outputs similarly approach  section brieﬂy present recent approaches learn estimate does require threshold approaches  ing low dimensional embeddings points high dimen estimate residual error function dimensionality  sional spaces extensions linear tech ﬁtted model  niques principal component analysis pca  algorithm computes mapping just  multidimensional scaling mds based assump embedding data described brand   tion nonlinear manifolds approximated locally intrinsic dimensionality manifold estimated ex  linear patches contrast methods scholkopf¨ et amining rate growth number points contained  al  propose kernel pca attempts ﬁnd linear hyperspheres function radius linear patches  patches using pca space typically higher dimension areas curvature noise discriminated using  ality input space correct kernel selection reveal proposed measure afﬁne transformations align  low dimensional structure input data instance ordinate systems linear patches computed  second order polynomial kernel “ﬂatten” quadratic man second stage deﬁnes global coordinate  ifolds                                               embedding mapping input space    roweis saul  address problem locally embedding space  linear embedding lle processing involves computing  construction weights each point neighbors finally brieﬂy review approaches estimate intrin  weights reconstruct point neigh sic dimensionality attempts learn local struc  bors low dimensional embedding space ture approach presented bruske som  neighborhoods assumed linear dimensional mer  optimally topology preserving map otpm  ity embedding given parameter constructed subset data produced  estimated data saul vector quantization pca performed each node  roweis  lle assures local neighborhood struc otpm assumption data locally lin  ture preserved during dimensionality reduction    ear average number signiﬁcant singular values    isomap tenenbaum et al  approximates geodesic nodes estimate intrinsic dimensionality  distances points graph distances mds kegl´  estimates capacity dimension mani  applied geodesic instead euclidean distances fold does depend distribution data  compute embedding forces nearby points remain equal topological dimension efﬁcient approxi  nearby faraway points remain way variation mation based packing numbers proposed algorithm  isomap applied data intrinsic curvature takes account dimensionality variations scale  known distribution faster alternative uses based geometric property data succes  landmark point distance computations sive projections increasingly higherdimensional subspaces  proposed silva tenenbaum  isomap certain percentage data explained costa  variants limited convex datasets              hero  estimate intrinsic dimension manifold    belkin niyogi  compute graph laplacian entropy samples using geodesicminimalspanning  adjacency graph input data approxima trees method similar isomap produces  tion laplacebeltrami operator manifold single global estimate levina bickel  compute  exploit locality preserving properties ﬁrst ob maximum likelihood estimates dimensionality examin  served ﬁeld clustering laplacian eigenmaps ing number neighbors included spheres radius  algorithm viewed generalization lle selected way density data  identical weights graph assumed constant neighbors included  chosen according criteria dimension requirements cause underestimation dimensionality  ality manifold reliably estimated high  data work reviewed donoho difference approach bruske  grimes  propose similar scheme computes sommer brand  kegl´  weinberger  hessian instead laplacian authors claim saul  costa hero  levina  hessian better suited detecting linear patches bickel  produces reliable dimensionality es  manifold major contribution approach timates point level averaged  proposes global method unlike isomap ap entire dataset able estimate local  plied nonconvex datasets                         orientation manifold restricted simple    weinberger saul  address problem man linear models  tensor voting  section ﬁrst review tensor voting framework  medioni et al  infer structures sparse  noisy data local voting process results  published  domains framework  extended higher dimensions tang et al   begin describing basics framework  example tensors  vote generation  data representation voting present contri  bution paper methodology efﬁcient figure  tensor voting shape tensor indicates  implementation terms space time tensor type structure location size indicates  voting highdimensional spaces able operate conﬁdence information tensor strong  spaces impractical impossible based preference orientation higher conﬁdence  formulation medioni et al             preference orientation                                                        vote generation function relative position stick    data representation                              voter receiver orientation voter  representation each point form second  order symmetric nonnegative deﬁnite tensor tensor                                                        λd eigenvalues descending order eˆd  viewed matrix ellipsoid represents corresponding eigenvectors tensor simultaneously  normal space point instance hyperplane                                                      encodes possible types structure conﬁdence  normal encoded tensor form nn  type normals encoded differ  structure normal space rank normals                                                        ence λd − λd  represented tensor form                                                      voting process                         ndn                                                                       tensor voting framework designed enforce                                                                                 straints proximity colinearity cocurvilinearity  point orientation information equivalently human perception  mind  viewed having possible normals encoded straints hold high dimensional spaces long  identity matrix tensor form represents equal looking structures smooth  preference orientations called “ball tensor” section information propagated  corresponding ellipsoid sphere hypersphere point voter receiver  hand tensor contains orientation during voting process each point casts votes each  called “stick tensor” stick tensors represent hyper neighboring points votes secondorder sym  plane ddimensional space normal metric nonnegative deﬁnite tensors encode orienta  −  tangents                                       tion receiver voter receiver    depending type structure point belongs structure begin examining case voter  different number normals tangents associated stick tensor unit length mag  sor’s eigensystem encodes information eigenvectors nitude vote cast stick tensor decays respect  belong tangent space correspond zero eigenvalues length smooth circular path connecting voter  belong normal space correspond receiver circle selected maintains  nonzero eigenvalues typically equal  points stant curvature degenerates straight line vector  known orientation encoded representa connecting voter receiver orthogonal normal  tion appropriately constructed tensors  points voter orientation vote center  unknown orientation represented identity matri circle deﬁned points orientation  ces ball tensors fig example tensors voter vote generated according following    hand given second order symmetric non equation transformed appropriate coordinate  negative deﬁnite tensor type structure encoded  examining eigensystem tensor  properties decomposed following                                                                                        equation                                                          − cκ  −sinθ                                                           θ  σ                −sinθ cosθ                                                                               cosθ                                   λdeˆdeˆd                                                            θkvk        sinθ                                                                                       κ                                                                                        sinθ          kvk         λ  − λ ˆe eˆt  λ − λ ˆe eˆt ˆ eˆt                                               length arc voter receiver                                                 λdˆeeˆ ˆeˆ   ˆdeˆd   κ curvature fig σ scale voting     d−                                                             constant vote generated angle θ        λd −λd      eˆdeˆd λdˆeeˆ ˆdeˆd  ﬁeld truncated extend                                                magnitude vote  magnitude                                                     voter  tensor voting function position voter vn exploit deﬁning new basis  receiver orientation voter vote gener normal space voter includes vn vote  ation stick tensor occurs subspace deﬁned constructed tensor addition votes cast stick  vector connects points normal tensors parallel new basis vectors votes  voter stick voting fully deﬁned fig generated stick tensor parallel vn  eq  regardless dimension input space parallel normal space voter fig   according medioni et al  votes cast illustration scheme applicable changes  types tensors precomputed stored lookup ta case ball voters unoriented points deﬁne  bles called voting ﬁelds presence normal space straight line space vote ball tensor  rank simulated rotating stick − normals tangent vt   tensor                                                                                                sor spans space closed form solution ex parallel vn does cast vote  cutoff rule  ists integration numerically approximated pro remaining −  tensors new basis cast votes  cess impractical dimensionality space equal magnitude span space orthogonal  increases space requirements storing ddimensional represent straight line connecting points                                            voting ﬁelds samples axis odk  tensors unequal eigenvalues decomposed  time advantage reusing precomputed votes van voting according eq  each component votes sep  ishes dimensionality increases propose instead arately vote weighted λd − λd  novel efﬁcient way directly compute votes integra ball component vote weighted λd imple  tion infeasible seek approximate solution  mentation tensor voting efﬁcient terms space                                                        need store voting ﬁelds terms    efﬁcient voting scheme                           time devised direct method computing  new scheme proposed generates votes contain votes replaces numerical integration medioni et  orientation information communicated al  new computation requires compu  receiver uncertainty orig tations stick vote according eq  followed  inal framework conveyed ball component addition votes instead integration dimen  vote integration votes cast rotat sions experiments  included  ing stick tensor instance according medioni et al space limitations validate accuracy new   voting tensor encodes curve vote generation scheme  normals tangent casts vote dominant  curve component uncertainty pro  vote analysis  pose propagate curve orientation receiver votes accumulated each point tensor addition  given orientation voter uncertainty new equivalent addition matrices voting com  formulation comes accumulation votes pleted eigensystem new tensor analyzed  perfectly aligned                            tensor decomposed eq  inference    let vector connecting voting receiving type structure point belongs accomplished  points decomposed vt tangent space follows difference largest eigenvalues  voter null case ball voter vn encodes surface conﬁdence surface normal given  normal space new vote generation process based                                                        difference second eigenvalue  observation curvature eq  factor encodes curve conﬁdence normals curve  θ zero words voting stick orthogonal                                                        smallest eigenvalue encodes junction                                                        conﬁdence outliers receive little inconsistent support                                                        neighborhood identiﬁed low                                                        eigenvalues generalization higher dimensions straight                                                        forward considering manifold types                                                        dimensions dimensionality estimated ﬁnding                                                        maximum difference λd − λd                                                             experimental results                                                        section present experimental results dimension  figure  new scheme vote generation voter ality estimation local structure inference inputs  tensor normals vector connecting sist unoriented points encoded ball tensors  voter receiver decomposed vn vt lie  normal tangent space voter respectively  new basis includes vn deﬁned normal space swiss roll ﬁrst experiment “swiss roll”  each basis component casts stick vote vote dataset available httpisomapstanfordedu  generated orientation parallel vn parallel contains   points manifold fig   normal space tensor addition stick votes produces perform simple evaluation quality orien  combined vote                                    tation estimates projecting nearest neighbors each        figure  “swiss roll” dataset                input            points    point estimated tangent space measuring  centage distance recovered accuracy  dimensionality estimation each point percentage  recovered distances  nearest neighbors execution  times pentium   ghz function σ  seen table  performance boundaries    points          points  number votes cast each point ranges   σ    σ   accuracy high  large range σ                                     figure  data varying dimensionality ﬁrst                                                        three axes input classiﬁed points shown        σ        correct    dist     time                  dim    rec   sec            embedded  space uniform noise                                          added dimensions accuracy dimensionality                                         estimation tensor voting seen table                                                                      intrinsic linear    quadratic space     dim                                          dim      mappings  mappings  dim      est                                                                                                                                                                                                                                                     table  rate correct dimensionality estimation exe                                 cution times functions σ “swiss roll” dataset                                                        table  rate correct dimensionality estimation high                                                        dimensional data  structures varying  dimensionality  second  dataset contains points sampled three  structures line cone hypersphere  conclusions  hypersphere structure three degrees freedom presented approach manifold learning  unfolded unless remove small fers certain advantages stateoftheart  figure shows ﬁrst three dimensions data able obtain accurate estimates intrinsic dimension  dataset contains total   points voting performed                                                       ality point level dimensionality  σ   takes  hours  minutes figures maximum gap eigenvalues tensor  show points classiﬁed according dimensional point thresholds needed approaches  ity performing analysis accuracy dimensionality provided best average  tangent space estimation  distances intrinsic dimensionality estimated entire dataset   nearest neighbors each point lie tangent space bruske sommer  brand  kegl´   cone hypersphere intrin weinberger saul  costa hero  sec  sic curvature accurately approximated linear ond tensor voting surface looks like  models methods presented sec  fail local covariance estimation fact votes tensors  dataset presence structures differ scalars reveals information each point  ent dimensionalities hypersphere properties tensor representation handle  unfolded                                             simultaneous presence multiple orientations allow reli                                                        able inference normal tangent space each point  data high dimensions datasets experiment unsupervised nature tensor voting robust  generated sampling thousand points low outliers demonstrated medioni et al   intrinsic dimensionality   mapping  case property holds higher dimen  medium dimensional space  using linear sions random noise scattered lack  quadratic functions generated points rotated space did allow include outlier rejection results
