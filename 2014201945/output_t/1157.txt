                                  hierarchical semantic classification                         word sense disambiguation world knowledge            massimiliano ciaramita thomas hofmann mark johnson               brown university brown university brown university            massibrownedu thcsbrownedu markj ohnsonbrownedu                               abstract                               number intuitive supplement taskspecific                                                                  training data example senseannotated training instances        present learning architecture lexical se•        specific word background data encoding general        mantic classification problems supplements           world knowledge typically available suf•       taskspecific training data background data en•      ficient quantities need generated separately        coding general world knowledge model               each classification task carry idea crucial is•       compiles knowledge contained dictionary             sues need addressed exactly world knowledge        ontology additional training data inte•         compiled additional training data task       grates taskspecific background data through          specific background data systematically integrated        novel hierarchical learning architecture experi•         address challenge propose generate ad•       ments word sense disambiguation task provide         ditional training data broader semantic categories        empirical evidence hierarchical classifier    extracting training sentences hierarchically structured        outperforms stateoftheart standard flat                                                                  ontology wordnet fellbaum  assumed each                                                                  example sentence associated lexical entry provides ev•   introduction                                                 idence kind contexts specific concept                                                                  ancestors hierarchy appear far   increasing natural language pro•       second challenge concerned introduce novel hierar•  cessing nlp information retrieval ir research      chical learning architecture semantic classification   lexical semantics particular respect word       specifically present simple efficient online training   sense disambiguation yoong hwee  informa•          algorithm generalizing multiclass perceptron cram•  tion extraction riloff jones  named entity         mer singer    recognition collins  automatic thesaurus exten•      finally carry experimental evaluation word   sion hearst  general terms goal tasks sense disambiguation task providing empirical evidence   automatically associating words text seman• hierarchical classifier outperforms stateoftheart stan•  tic labels information extraction namedentity recog•  dard flat classifier task   nition noun phrases proper nouns assigned semantic     paper structured follows section  introduces   categories organization person location main idea section  introduce wordnet   word sense disambiguation thesaurus extension goal     simplified ontology derived used   assign words finergrained categories defined ex•  source world knowledge section  deals ba•  isting dictionaries ontologies                            sic multiclass perceptron proposed hierarchical mul     lexical semantic information useful nlp      ticomponent classifier finally sections     ir applications text categorization parsing  data set used empirical results respectively   language modeling speech recognition furthermore   crucial tasks require complex inferences in•    word sense disambiguation world   volving world knowledge question answering     main difficulties learning semantic annota•        knowledge   tions stems fact training instances nar• word sense disambiguation task assigning each   rowly focused specific class labels relatively occurrence ambiguous word text possible      wc like thank colleagues information re• senses dictionary used decide lexical entry am•  trieval machine learning group irml brown labora•    biguous specify set possible senses   tory linguistic information processing bllip jesse widely used lexical resource task wordnet   hochstadt editing advice material based work section   supported national science foundation grant                                                              paper refer wordnet version        natural language                                                                                                                                      figure  simplified twolayer hierarchy noun chair        illustration consider noun chair accord• know person artifact    ing wordnet ambiguous possible senses ex•      learn reliable information general    plained following wordnet entries                     concepts way addressing problems offered                                                                  wordnet      • chairi  seat person support             • chairi  president chairman chairwoman chair          ontology        chairperson officer presides meetings     wordnet        organization                                                                  wordnet broadcoverage machinereadable dictionary      word sense disambiguation framed multi       widely used nlp english version contains    class pattern classification task useftil features include  entries nouns verbs adjectives    occurring words word bigrams trigrams properties  adverbs wordnet organized network lexical ized   syntactic context contains target word com• concepts called synsets comprise sets synonyms   monly systems trained labeled data specific word example nouns president chairman chairwoman chair   each tested unseen items word set chairperson form synsct word belongs   possible labels set senses ambiguous word synsets ambiguous synsets linked semantic rela•  limitation strategy bases tions important nouns verbs   decision exclusively able learn isa relation hyponymy car hyponym ve•  specific concepts chair chair further• hicle verb noun databases form isa hierarchies   manually sensetagging words required     general concepts thousand   training data slow expensive data quite sparse specific concepts leaf level      great deal information objects like chairs       hierarchical structure database aroused   indirect derived general world        nlp support interesting compu•  knowledge through generalization inference processes     tational language learning models example learning   suppose task disambiguate      predicate selectional preferences light greif    simple senses chair following context              aim use hierarchy improve lexical classification                                                                 methods model present principle make     quality finest chair components       use hierarchy sake simplicity   merged art                                             focused complex hierarchy                                                                 derived wordnet described   sentence components useful hint   dealing sense chairi chairs artifacts                                                                  simple twolevel hierarchy   artifacts components conversely   principle people components     wordnet built regularly updated lexicogra•  sounds little odd intuitively word sense disambigua• phers lexicographers group words synsets   tion access type information      individuate relevant semantic relations synsets   chairs subordinates broader concepts like artifacts process includes classification lexical entries   people  knowledge broader          broad semantic classes paper refer   semantic categories achieve higher accuracy    broad classes term supersenses exam•  disambiguating words notice      ples supersense labels person animal artifact food lo•  previously observed instance noun chair    cation time plant process attribute substance relation   sense having components                      set labels fairly general small      goal complement specific limited       time labels abstract words   knowledge narrow classes richer specific classes natural easily recognizable   knowledge general classes easily recover   probably lexicographers use fact level   fact chairs kinds furniture people dic• generality close used namedentity recogni•  tionaries hierarchically organized ontologies like word  tion location person organization   net learning information general concepts how•      each synset wordnet associated supersense   complicated source complication  label result database implicitly defines addition   problem trying solve lexical ambiguity   hierarchy simpler twolayer hierarchy figure                                                                                                      natural language  illustrates synsets supersenses chair belongs algorithm  multiclass perceptron                                                                                                                                   hierarchy source world knowledge                                                                     thousand concepts wordnet lists            types semantic information example sen•           tences sense chair example sentences        following                                                  • chairi  coat chair           sat                                                                                                                        • chair  address remarks chairperson                                                                    overall  sentences each        associated synset fact sensetagged   instance word words wordnet provides   thousand potential sensetagged training instances          general multiclass classifier word function     unfortunately additional data                    maps feature vectors   help synsets sentences   possible senses  multiclass perceptron   typically sentences short provide introduces weight vector   context situation appears different   defines implicitly socalled winnertakeall rule   light account hierarchy considering ex•  ample sentence synset example sentence                                                             ancestors synsets higher levels hierarchy   number sentences grows larger superordinate lev•    refers matrix weights col•  els consider supersense level set example                                                                 umn corresponding weight vectors    sentences constitutes fact small corpus supersense                                                                   learning algorithm works follows training patterns   annotated data hypothesis                                                                 arc presented time standard online learning   sentences associated each supersense provide use•                                                                setting update step per•  ful source general world knowledge section                                                                 formed weight vectors remain unchanged   general multicomponent learning architecture                                                                 perform update computes error set  con•  used exploit supplementary training data                                                                                                                          taining class labels received higher score                                                                 correct class    multicomponent learning architecture                                                                                                                           idea using hierarchical structure domain   overcome sparseness problems explored text cat•   ultraconservative update scheme general form   egorization methods show improved accuracy ef•     defined follows updatewith learn•  ficiency toutanova et al  dumais chen      ing rates fulfilling constraints    nlp hierarchical structure wordnet used    changes lim•  overcome sparseness data problems estimating class     ited  sum constraint ensures   distributions clark weir  exploit morpho•  update balanced crucial guaranteeing   logical information improve lexical acquisition ciaramita convergence learning procedure cf crammer                                                           singer  focused simplest case uni•                                                                form update weights  algorithm    multiclass perceptron                                                                 summarized algorithm    architecture propose generalization ultra       notice presented multiclass perceptron algorithm   conservative online learning crammer singer     learns weight vectors coupled manner contrast   extension perceptron learning mul• methods perform multiclass classification combining   ticlass case flat version classi•  binary classifiers example training classifier each   fier each noun given training set                                                                  class oneagainsttherest manner              each instance    yw set synsets wordnet assigns     hierarchical multiclass perceptron    summarizes instances noun each instance       hierarchical multiclass perceptron inspired   represented vector features extracted framework learning structured output spaces intro•  context occurred total number features duced hofmann et al  key idea intro•  yi true label                                   duce weight vector leaflevel class                                                                 inner node given class taxonomy      therc total  synsets noun database                                                                 current application word sense disambiguation inner      instances labeled multiple senses cases   taggers uncertain y» actually set labels nodes correspond  supersenses        natural language                                                                                                      introduce additional weight vectors                   second concerns training task specific data   sw refers subset supersenses induced                    classifier makes mistake pattern error   use notation refer supersense corre•     sets computed individual components   sponding synset discriminant functions            synset supersense levels lines     defined additive manner                        updated according standard multiclass update rule                                                                 example suppose given pattern xi chair                                                                  synset error set   thinks terms compatibility function correct label    observation vector synset compatibility person update vector subtracted   score simply sum independent contributions                                                                  vectors relative labels  added   stemming supersense level coming                                                                                                     supersense level error set ef  artifact   detailed synset level multiclass classifier                                                                   subtracted vector artifact added   defined using winnertakeall rule                                                                            through supersense weight vectors                                                                  background data affects classification synset level                                                                                                                              data set features     algorithm  hierarchical multiclass perception                  senseval data                                                                  tested standard word sense disambigua•                                                                 tion data set training test data used                                                                  senseval workshop sensevalacl                                                                   focused exclusively word sense disambiguation train•                                                                 ing set consists  paragraphs contain ambigu•                                                                 ous word sense manually annotated                                                                  inventory senses taken wordnet similarly                                                                  test set consists  unlabeled pairs ran ex•                                                                 periments noun data consists  training                                                                  instances  test instances each instance consists                                                                  short passage taken various sources                                                                  wall street journal british national corpus web pages                                                                    taskspecific training data ty typically smaller                                                                  general ts average ratio equal                                                                                                                                       features                                                                  used feature set described yoong hwee                                                                   compact includes features                                                                  useful task surrounding words                                                                  bigrams trigrams syntactic information yoong                                                                  hwee report results classifiers broken                                                                  speech makes possible compare sys•                                                                 tems performance                                                                    types features following sentence                                                                  serves illustrate dinner table chairs ele•                                                                 gant comfortable feature set described greater                                                                  details yoong hwee                                                                     •part speech neighboring words cc                                                                           nns     aux      complete algorithm summarized algorithm                            algorithm concerns different nature    • single words surrounding context  elegant   types training data explained sec•           dinner  table    tion  supplementary data derived wordnet         • bigrams trigrams cii  andare   provides annotations supersense level use     information perform updates weight vectors vy   adjust weights  supersenseannotated      • head syntactic phrase governs tar•  training instances compute error set                                                                                      grelps  left   supersense level   perform standard multiclass update step tor         syntactic features speech tags extracted                                                                  syntactic parse trees senseval training                                                                                                       natural language  figure  test accuracy flat multiclass perceptron     figure  test accuracy hierarchical continuous line    dashed line hierarchical multiclass perceptron con• vs flat dashed line multiclass perceptron hierarchical    tinuous line word sense evaluation data set          multiclass perceptron trained using supplementary super                                                                 sense training data      test data produced using charniaks parser lcharniak     way created training data ty senseval   figure  plots performances flat hierar•   data exactly way extracted features      chical perceptron trained ts patterns    example sentences wordnet produce additional    different hierarchical model converges af•   training set supersenselevel classes ts overall ter  iterations     features       experiments     experimental setup    tested models described section  flat multi   class perceptron trained tested synset level    hierarchical trained standard synset data    training data supersenses extracted wordnet     table  test accuracy senseval test data    trained tested simple flat naive bayes clas•   sifier different classifier trained tested each  facts    word treated compounds easy chair chair    data greater addition ts takes    different words                                              longer learn second supersense data synset      results report given accuracy            data probably different noisy consequence                                                                  weight vectors continually readjusted possibly                                                                  different dimensions interesting thing                                                                  midst wide oscillations clear                                                                  improvement particularly   iterations     results                                                     present comparative table table  illustrates    figure  shows performance flat perceptron dotted results systems stateoftheart word sense    line during each iteration perceptron fact converges disambiguation ones set number iterations   quickly probably fact  fixed number words equal  given set   relatively training items normally size ty be•  value knowing good sys•   tween check improve•       tems results systems really    ment algorithm combina•   comparable reasonable expect    tion algorithm additional supersense data set possible set stopping criterion using held    trained hierarchical perceptron exclusively  data comparison gives approximate idea    synset data figure  plots performance hier• systems stand respect stateoftheart    archical perceptron trained ty curves    ones terms performance adaboost classifier   virtually indistinguishable meaning additional  gave best result nouns yoong hwee     information gained using hierarchi•  best mihalcea moldovan  refers best  cal classifier words flat data flat performing nouns senseval workshop    classifier good hierarchical                systems results show systems performance        natural language                                                                                                     
