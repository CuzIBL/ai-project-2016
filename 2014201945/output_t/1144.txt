          effective control knowledge transfer through learning skill                                    representation hierarchies          ∗                        mehran asadi                                manfred huber               department science       department science engineering           west chester university pennsylvania        university texas arlington                    west chester pa                         arlington tx                       masadiwcupaedu                             hubercseutaedu                        abstract                          extended actions using framework semimarkov                                                        cision processes smdps sutton et al  learning      learning capabilities systems lag subgoals hierarchical action spaces learning ab      far biological systems rea    stract representations givan et al howevermostof      sons seen inefﬁcient reuse control techniques address aspects transfer      knowledge acquired lifetime arti frequently directly address construction ac      ﬁcial learning address deﬁciency tion representation hierarchies lifelong learning      paper presents learning architecture   work presented focuses construction      transfers control knowledge form behav transfer control knowledge form behavioral skill      ioral skills corresponding representation hierarchies associated representational hierarchies      cepts task subsequent learning tasks context reinforcement learning agent particular      presented uses knowledge  facilitates acquisition increasingly complex behav      struct compact state space representation  ioral skills construction appropriate increasingly      learning assuring bounded optimality abstract compact state representations acceler      learned task policy utilizing represen  ate learning performance ensuring bounded optimality      tation hierarchy experimental results show  forms state hierarchy encodes func      presented method signiﬁcantly outperform  tional properties skill hierarchy providing compact      learning ﬂat state space representation  basis learning ensures bounded optimality      maxq method hierarchical reinforcement      learning                                            reinforcement learning                                                        rl framework learning agent interacts en    introduction                                       vironment series time steps                                                             each time agent observes state environment  learning capabilities biological systems far exceed                                                        st  chooses action  causes environment  ones artiﬁcial agents partially efﬁciency                                                        transition state st andtoemitarewardrtina  transfer reuse control knowledge markovian state reward depend  acquired course lives              preceding state action depend    address knowledge transfer learning tasks stochastic manner objective agent learn  recently received increasing attention ando zhang maximize expected value reward received time   taylor stone  marthi et al  marx et does learning possibly stochastic mapping  al  type knowledge considered transfer states actions called policy precisely objective  includes reusable behavioral macros important state fea                                                        choose each action maximize expected  tures information expected reward conditions         ∞                                                           return γ rti whereγ  ∈   discount  background knowledge knowledge transfer aimed im rate parameter return formulations possible  proving learning performance reducing learning common solution strategy approximate optimal  problem’s complexity guiding learning process actionvalue function qfunction maps each state    recent work hierarchical reinforcement learning  action maximum expected return starting  hrl led approaches learning temporally given state action taking best    ∗                                                   actions     work supported darpa fa  nsf itr government reproduce permit construction hierarchical learning  distribute reprints governmental purposes notwithstanding model learning problem semimarkov decision  copyrights authors’ views conclusions inter problem smdp use options framework sutton et  preted representing ofﬁcial policies endorsements expressed al  deﬁne subgoals option temporally ex  implied darpa nsf government         tended action selected agent executes                                                    ijcai                                                                                                                                                     reward  termination condition satisﬁed option                                                                                      qvalue    learningplanning                                                                                      inconsistencies  executing actions chosen according option’s             actioneligibility  policy option like traditional macro                                                                                               state   qsa                                                             behaviormemory    decisionlevel  stead generating ﬁxed sequence actions follows                 actions                                                                                              hierarchical  closedloop policy react environment conceptmemory                   bpsmdpmodel                                                                          statespace                                                                          construction  augmenting agent’s set primitive actions                         model decisionlevel  set options agent’s performance enhanced                modelrefinement construction  speciﬁcally option triple oi iiπiβi                                 evaluationlevel                                                              newstateconcepts  ii option’s input set set states         subgoalsubtask                                                                         identification  option initiated πi option’s pol             policygeneralization                                                                newactions  icy deﬁned states option execute                               state   decision  βi termination condition option termi  nates probability βis each state each option figure  overview approach hierarchical  use paper bases policy inter behavior state concept transfer  nal value function modiﬁed time  sponse environment value state  smdp policy πo deﬁned boutilier et al   learning transferable skills  sutton et al                                learn skills transfer approach presented tries                                                       identify subgoals subgoals states         π                               π          se   rs oi    oiv        properties useful subsequent learning                            s                          tasks new tasks’ requirements                                                  reward functions generally unknown subgoal crite                     ∞                                 rion used does focus reward local                                                  oi   st  st  oiγ     properties state space context current task                                                    domain particular criterion used attempts iden                                                    tify states locally form signiﬁcantly stronger “attrac                                                    tor” state space trajectories measured relative   rs oiert  γrt  γ rt  πi st                                                       increase visitation likelihood  rt denotes reward time π st denotes                                                        ﬁnd states subgoal discovery method ﬁrst gen  event action policy π initiated time erates random sample trajectories learned policy  state sutton et al                each state trajectories determines                                                                                    ∗            ∗                                                        expected visitation likelihood ch swherech    hierarchical knowledge transfer                    sum states sample trajectories hi ∈ weighed  approach presented skills learned accumulated likelihood pass through change  framework semimarkov decision processes smdp visitation likelihoods sample trajectory hiisthen                                                                                ∗        ∗  new task policies advantage previously determined Δh stch st − ch st−wherest  learned skills leading initial set basic actions tth state path ratio change  formation skill hierarchy time abstract path computed  representation concepts derived capture each skill’s  goal objective conditions use                 Δh st  skill predict achievement objective figure          max Δh st  shows approach                                   state Δh st   finally state st    state representations formed frame considered potential subgoal average change ratio  work bounded parameter markov decision processes bp signiﬁcantly greater expected distribution  mdps  givan et al  include decisionlevel ratios states  subgoals correspond  model complex evaluationlevel model learning ing policies learned offline smdp options oiand  new task performed decisionlevel model added skill hierarchy  using qlearning second value function maintained  evaluationlevel model inconsistency dis deﬁnition  state s direct predecessor state  covered value functions reﬁnement aug learned policy action state s lead  ments decisionlevel model including concepts ssa    action led inconsistency    policy new task learned subgoals deﬁnition  count metric state learned  extracted model corresponding subgoal policy π sum possible state space trajectories  skills learned offline goal probabilistic af weighed accumulated likelihood pass through  fordance concepts learned new subgoal skills state  new skills concepts included skill  representation hierarchies agent’s memory mak threshold computed automatically using ttest based  ing available subsequent learning tasks     criterion signiﬁcance threshold                                                     ijcai                                                         ∗  let cπs count state               learning functional descriptions state                                                                             power complex actions improve learning               cπs      ss πs             formance main sources use reduces                       ss                            number decision points necessary learn policy                                                        ii usually permit learning occur com                                                   pact state representation harness nec                                 t−             cπs     ss πs cπ          essary automatically derive abstract state representations                    ss                               capture functional characteristics actions                                                        presented approach builds hierarchical state                           n                           representation basic framework bpmdps ex                    ∗                             cπs     cπs                  tended smdps forming hierarchical bounded parame                                                    ter sdmp bpsmdp model construction occurs multi                                                  stage actiondependent fashion allowing model adapt  cπ scπ   sthe                                                     rapidly action set changes  condition   prevents counting self loops bpsmdp state space partition original state  ssπs probability reaching state state                                       ∗              space following inequalities hold blocks    executing action   slope                    π               cπ             bpsmdp states bi actions oj asadi huber   path ρ policy π                                                                                                                                                                                          ∗       ∗                                                                         Δπstcπst   − cπst−                                                                                                                   oi −   oi ≤ δ                 th                                             s∈b            s∈b            st state path order identify                                                 Δ     max Δ                                       subgoals gradient ratio π st        π st                    rs oi − rs oi≤                                      Δ         computed states π st   astatest                 considered potential subgoal candidate gradient ratio expected reward executing option                                                        state sandf ss discounted transition probability  greater speciﬁed threshold μ appropriate                                               values userdeﬁned threshold depend largely option initiated state terminate state  characteristics state space result number properties bpsmdp model ensure value  subgoal candidates inversely related value policy learned model ﬁxed bound  μ approach extension criterion goel optimal policy value initial model bound                                                        function  δ givan et al   huber  max Δπst addressing  effects potentially obtaining negative gradients make construction bpsmdp efﬁcient  nondeterministic transitions                         state model constructed multiple steps func  order reduce computational complexity tional concepts each option learned termina  method large state spaces gradient ratio tion concepts cto indicating option’s goal condition  computed using monte carlo sampling                  probabilistic prediction concepts “affordances” cpoxin                                                        dicating context option terminate                                                        successfully probability ±  conditions guar  deﬁnition  let      hn  sample tra  jectories induced policy π sampled count metric antee state space utilizing concepts state  c∗ each state path factorization fulﬁlls conditions equation  sin                                                      gle action  path hi calculated average accumulated                            ≤    ≤                       construct appropriate bpmdp speciﬁc action  likelihoods each path hi   rescaled             total number possible paths environment    set ot   oi  initial model constructed concatenat                                                        ing concepts associated options ot additional  show trajectories hi sample size                                                   conditions derived achieve condition equa                     ∗                                  tion  reward information available reward               maxt  st                         ≥                                  condition equation  construction facilitates efﬁcient                             n log  −                           n                                 adaptation changing action repertoires  following statement true probability     utilize power abstract actions hierarchy                                                        bpsmdp models constructed decision                   ∗        ∗                ch st − cπst≤n                   level model utilizes set options considered necessary                                                        evaluationlevel uses actions considered  theorem   let    hn  sample trajecto dundant current simple heuristic used  ries induced policy π selected according equa decisionlevel set consists learned sub                Δ                 μ  tion           μ                   goal options evaluationlevel set includes ac            maxΔh st      maxΔh st      tions      Δπ st                    ≥  maxΔπ st μwith probability                  let    bbn  partition state space  theorem  implies sufﬁciently large sample size derived actiondependent partitioning method  exhaustive sampling method predict sub using subgoals ssk options subgoals  goals high probability                          ook  goal state belongs set                                                    ijcai                                                    subgoals ssktheng  achievable options  bounds policy utilizes actions decision  ook task learnable according theorem level action set action set se                                                      lected knowledge new task generally                                                        possible guarantee contains required actions  theorem   policy π goal    address approach maintains second value  represented conjunction terminal sets subgoals function evaluationlevel model  available actions original mdp mthereis decisions strictly based decisionlevel states  policy πp reduced mdp mp  achieves  evaluationlevel value function used discover value  long each state st exists inconsistencies indicating signiﬁcant aspect state  path  exists path gstπp st δ space represented evaluationlevel state model                                                        determination inconsistencies relies fact                                                                                                ∗  g∈ssk   task solvable optimal value function bpmdp vp   using options terminate subgoals ﬁxed bound optimal value function ∗  proposed approach solves problem maintaining lying mdp givan et al   separate value function original state space inconsistencies discovered evaluationlevel  learning new task partition space derived value state signiﬁcantly exceeds value cor  subgoal options during learning agent access responding state decision level case action  original actions options makes decisions producing higher value included corresponding  based abstract partition space information block decision level block reﬁned  agent tries solve task abstract partition action fulﬁll equations   illustrated figure   space computes difference qvalues    best actions current state abstract state space                          original state space difference larger            constant value given theorem                signiﬁcant difference different states underlying  particular block captured subgoal options  theorem  kim dean  showsthatifblocksare                              stable respect actions difference  qvalues partition space original state space                                                                                              bounded constant value                       figure   decisionlevel model   initial blocks                                                        bbb block reﬁned  theorem   given mdp       partition state space mp  optimal value function                 ∗  given  optimal value function mp    experiments  given ∗ satisfy bound distance         vp                                             evaluate approach implemented                                                                                                urban combat testbed uct game              ∗ −   ∗  ≤       γ                 vp  ∞           −  p               httpgameairesearchutaedu experiments pre                                     γ                  sented agent given abilities through          min      ∗ −  ∗                            environment shown figure  retrieve deposit  p     vp      vp  ∞                                                       objects           max                           lv         γ                                  s∈s    difference qvalues states block                          γ  bi greater   −γ p primitive action  achieves highest qvalue original state  mdp added action space states  block bi block bi reﬁned stable                                                             figure  urban combat testbed uct domain  new action set signiﬁcant difference exists  goal achievable resulting state space according state characterized agent’s pose  theorem                                                         set local object precepts resulting effective                                                        state space   states    learning hierarchical state space             agent ﬁrst presented reward function learn  learn new tasks qlearning used decision speciﬁc location task learned sub  level bpsmdp hierarchy  compact   goals extracted generating random sample trajectories  decisionlevel state model encodes aspects en shown figure   vironment relevant subset actions ensures number samples increases identiﬁes  learning policy predetermined optimality increasing number subgoals   samples                                                    ijcai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  q−value                                             exhaustive computing                                             monte carlo sampling                                                                                                                                                     transfer          discovered  subgoals  skills                                              skill transfer                                                                                   skill representation transfer                                                                             number samples                                                                                                                    number iterations   figure  number subgoals discovered using sampling                                                        figure  learning performance skill                                                        representationconcept transfer   subgoals using exhaustive calcula  tion captured                                                        ferred illustrating utility presented subgoal crite    subgoals extracted subgoal options oiare                                                        rion including representation transfer hierarchical  learned termination concepts ctoi probabilistic                                                        bpsmdp learning approach results signiﬁcant im  come predictors cpoix generated subgoal op                          ≈   tions termination prediction concepts provement transfer ratio   transferred learning tasks    builds hierarchical bpsmdp    comparison maxq  model decisionlevel utilizes learned sub dietterich dietterich  developed approach hi  goal actions evaluationlevel model built erarchical rl called maxq value function decompo  available actions model second task learned sition called maxq method like options  agent rewarded retrieving ﬂag hams approach relies theory smdps  different location previous goal return like options hams maxq approach does  home base during learning augments rely directly reducing entire problem single  decisionlevel state representation allow learning pol smdp instead hierarchy smdps created  icy bound optimal shown figure  lution learned simultaneously maxq approach                                                        starts decomposition core mdp set                                                                   subtasks mmn subtasks form hierarchy                                                       root subtask means solving                                                      solves actions taken solving consist exe                                                        cuting primitive actions policies solve subtasks                                                        turn invoke primitive actions policies                                                        subtasks                                                                   each subtask mi consists three components                                         task−independent blocks   subtask policy pi select subtasks                              task−specific refinement                                                      set mi ’s children options primitive actions              number  bpsmdp states                    special cases subtasks assume subtask                                                        policies deterministic second each subtask ter                                            number iterations               mination predicate partitions state set softhecore    figure  size decisionlevel state representation mdp si set active states mi’s policy                                                        execute ti set termination states    figure  shows starts initial state entered causes policy terminate each subtask  representation containing  states during learning value mi pseudoreward function assigns reward values  function inconsistencies new actions state states ti pseudoreward function used  splits introduced eventually increasing decisionlevel during learning  state space  states state space bounded op figure  shows comparison maxq decom  timal policy learned indicated figure  graph position learning smdp samplingbase  compares learning performance subgoal discovery actiondependent partitioning  learner transfers discovered subgoal options experiment illustrates maxq outperform  learner transfer mechanism graphs show smdp options subgoals discovered  transfer ratio ≈  subgoal options trans samplingbased subgoal discovery reason                                                        subgoals hand designed maxq decompo    transfer ratio ratio area learning curve sition samplingbased method fully autonomous  notransfer transfer learner     does rely human decision result subgoal dis                                                    ijcai                                                    
