   multidocument summarization maximizing informative contentwords                wentau yih      joshua goodman         lucy vanderwende         hisami suzuki                                            microsoft research                                        redmond wa  usa                            scottyih joshuago lucyv hisamismicrosoftcom                          abstract                          scored average probabilities words                                                        summary generated through simple greedy      show simple procedure based max     search algorithm iteratively selects sentence      imizing number informative contentwords   highestscoring contentword breaking ties using aver      produce best reported results age score sentences continues maximum      multidocument summarization ﬁrst assign    summary length reached order select      score each term document cluster using similar sentence multiple times sumbasic updates      frequency position information probabilities words selected sentence squar      ﬁnd set sentences document cluster ing modeling likelihood word occurring twice      maximizes sum scores subject summary      length constraints overall results best improves sumbasic three ways      reported duc summarization task   contrast sumbasic uses frequency informa      rouge score best sta tion approach uses position information second      tistically signiﬁcantly different best sys introduce discriminative machinelearning based algo      tem mse substantially rithm combine information sources instead      simpler previous best            applying heuristic greedy search sumbasic                                                        malize content selection process optimization prob    introduction                                       lem speciﬁcally use stackdecoding algorithm ﬁnd  paper show simple approach generic summary maximize total scores content  multidocument summarization lead best words subject summary length constraint each  reported results task components three improvements empirically shown lead  component uses machine learning compute scores better summaries achieves best reported  each word set documents summarized rouge results duc summarization task  called “document cluster” component uses mse difference statistically                                                                                                        search algorithm ﬁnd best set sentences signiﬁcant duc  recent wan                                                                        document cluster maximizing scores despite sim yang  reports excellent results duc  plicity techniques results best  task uses different version rouge software  reported multidocument summarization            making comparisons difﬁcult    multidocument summarization increasingly impor rest paper organized follows section   tant task document collections grow larger determined importance position  greater need summarize documents help users information term scoring mech  quickly ﬁnd important information anisms section  details optimiza  generic summarization relevant informa tion technique including stack decoder section   tion user topicfocused summarization examples experimental results showing new scor  multidocument summarization helpful ing method optimization technique lead improve  clude news email threads blogs reviews search results ments combined outperforms best    design  motivated sumba    previous rouge metric compare  sic nenkova vanderwende  nenkova et al  related work section  ﬁnally section   sumbasic extremely simple performance concludes paper  statistical noise best duc sec  ond best mse using rouge measure  scoring  sumbasic ﬁrst computes probability each content section techniques scoring words  word verbs nouns adjectives numbers simply starting point sumbasic scores words purely using fre  counting frequency document set each sentence quency information section  investigate additional                                                    ijcai                                                    possible features looking mismatches human word occurrence document cluster computed po  machinegenerated summaries identify position sition relative beginning document   formation key additional feature section  ﬁrst word  each word com  examine different ways combine position infor puted average occurrences document  mation frequency information cluster average position word  scribed generative discrimina frequent word occurring randomly documents  tive                                                 average position  words                                                        occur disproportionately beginnings documents    position frequency features                  average position   mentioned previously exclusively using word frequency particular compute average position orig  information sumbasic produce quite competitive multi inal document cluster terms summary hu  document summaries nenkova et al  focused man summaries value  term frequency  frequency information comparing machine human based summaries generated sumbasic value  generated summaries ﬁnding machinegenerated  compared value computed document  summaries contained fewer frequent words human gen cluster  fact reconﬁrms importance  erated ones wanted build work identify position course difference   ing information sources useful sum important practice later section  experi  marization addition word frequencies par mental results showing inclusion information  ticular examined summaries generated sumbasic does lead statistically signiﬁcant improvements  human summaries written document set  checked types summaries different  scoring terms  properties example number capitalized words knowing frequency location important  substantially higher lower human summaries question incorporate create good  machine generated summaries expect capi scoring function each term section explore  talization information help improve automatic sys different approaches generative discriminative  tem looked number different properties including  capitalization word length sentence length generative scoring  human machine summaries compara term scoring method used sumbasic thought  ble values properties – word positions generative model based frequency    position information used quite frequently model assume summaries generated random  singledocument summarization simple baseline document cluster according following process  takes ﬁrst sentences summary select term random document cluster  performs summarization systems annual duc probability proportional frequency word  evaluation barzilay lee  zajic et cluster add term summary delete  al  use position scoring candidate summary occurrences term cluster repeat  sentences multidocument summarization duc  process generated summary required  position information used simple length course random set words horrible  baseline systems baselines constructed multi summary practice output summary consists sen  document summarization informed position tences highest probability according model  took ﬁrst words recent news docu selected document cluster section   ment collection baseline use scores create summaries  constructed appending ﬁrst sentences subse addition model based frequency  quent articles document cluster length limit propose generative model favors words  reached                                          beginnings documents tried additional experiments    mutlidocument summarization various systems instead selecting words random  used sentence position feature scoring candidate sen document select terms beginning  tences radev et al  zajic et al  word document experiments duc used  position explored far level words ﬁrst  words mse articles  systems used feature number words short used ﬁrst  words  candidate sentence signature ﬁnal generative model combines mod  kens words frequent near beginning els allowing tradeoff overall frequency fre  article likely highly informative lin quency beginning documents model assumes  hovy  mckeown et al  schiffman  ﬁrst step ﬂip biased coin based bi  conroy et al  signature tokens ased coin ﬂip select terms random  computed based large corpus news articles based document cluster beginnings  word position small cluster articles  document clusters effective probability value    check importance word position information fore linear interpolation values used base  given cluster documents ﬁrst needed deﬁne models trying different bias terms set  position measure procedure follows each ting  worked best empirically                                                    ijcai                                                    discriminative scoring                                  scoring summary  probabilistic approaches considered different methods combining scores  natural language processing discriminative approaches work words overall score productbased method  better generative ones train discriminative sumbased method consider generative model sec  model using data duc multidocument sum tion  given summary multiply  marization task learn probability given probabilities model finding summary  term document summary each highest product probabilities summary  tent word document cluster assign label  highest probability exactly correct accord  appears given human summary label ing generative model   try learn probability term label hand automatic evaluation metrics   given features                                rouge scores favor summaries    learning algorithm chose logistic regression words appear reference human sum  reasons logistic regression predicts probabilities maries score content word represents prob  directly contrast say perceptron svms second lo ability word appears human summary  gistic regression works inconsistent labels summation scores thought ex  contrast say large margin approaches like svms pected number “correct” content words compared  ﬁnd data difﬁcult train important sum product methods sum method  different human summaries each document consistently worked better  cluster term document cluster appears summary scoring principle somewhat different  say three summaries training data identi sumbasic directly compute score based  cal feature vectors representing term three words sumbasic weights sentences ﬁrst tries  labeled  labeled      select sentences better total score    created  basic features using frequency po              best  sition information occr number oc  finding summary  currences document cluster occrratio occr divided iterative algorithm used sumbasic thought  number contentwords document cluster simple greedy algorithm tries ﬁnd best sum  avgoccrratio calculate occrratio each document mary greedy algorithm rarely ﬁnds best  cluster average minpos ﬁnd position starting summary sense optimizing expected score   contentword appears ﬁrst each docu addition does explicitly consider max  ment cluster use smallest avgminpos sim length cutoff threshold causes end sen  ilar minpos use average instead content tence used scoring score ﬁnal sum  word does appear document position mary impacted developed  tal number contentwords document minrelpos complex algorithm explicitly search best  compute relative ﬁrst position minpos di combination sentences algorithm based stack  vided number contentwords document decoder jelinek  typical problem stack  return smallest cluster avgrelpos similar coders trouble comparing hypotheses dif  minrelpos use avearge instead              ferent lengths solved    each basic features create correspond search paul  requires ﬁnding admissible cost  ing log feature suppose basic feature cor function does exist instead using  responding log feature log  expand search chose use multiple stacks each stack rep  feature space introducing conjunction features each resenting hypotheses different lengths magerman   pair features use product values stack decoder method shown algorithm   new feature similar effect using degree takes input set sentences document clus  polynomial kernel                                    ter score array used weight sentences                                                        method uses maxlength stacks each length    optimization method                                maximum length summary each stack contains                                                        best summaries far exactly length  relatively novel aspect stack stackmaxlength contain summaries longer  moved algorithmic description common sys maxlength –butwordspastsizemaxlength consid  tems scoring description potential summaries given ered scoring stacksize  overall score based scores included content different hypotheses given stack  words goal ﬁnd summary best algorithm proceeds examining particular stack  score ﬁrst explain decide overall score looks solution stack solution set  summary section  section  sentences tries extend solution  scribe optimization procedure searches best sentence document cluster extensions  summary finally sec  variation placed stack appropriate length order  does sentence simpliﬁcation allow use avoid exponential blowup number solutions  sentences sentences certain useful phrases given stack use priority queue  removed                                              stacksize highest scoring solutions given stack                                                    ijcai                                                    algorithm  stack decoder multidocument summa   entirely summarization component detailed descrip  rization                                              tion approach sentence simpliﬁcation    input array sentences scores each term vanderwende et al      sentences    input maximum length maxlength                    experiments    input maximum stacksize    typedef  solution  variable length array sentence order evaluate performance systems use     ids                                                data sets used recent multidocument    let stackmaxlength priority queue solutions summarization shared tasks multidocument summarization     each queue stacksize solutions        task  duc multilingual multidocument    stack thesolution length                 summarization task mse ﬁrst show results    tomaxlength −                         purely extractive each tasks     sol ∈ stacki                        show effects variations systems       ∈ sentences                       perform experiments using sentence simpliﬁcation sys         newlen  minilengthsmaxlength         tem showing additional improvements         newsol  sol ∪s         score scoreofnewsol counting each word duc             maxlength words                 multidocument summarization task duc         insert newsol score queue stacknewlen participants given  document clusters each clus            pruning necessary                        ter  news articles discussing topic      end                                        asked generate summaries  words each    end                                          cluster task held duc   end                                           different documents  data develop   return best scoring solution stackmaxlength  ment especially training probabilities                                                          present results sumbasic                                                        ing different term scoring methods table  addition    notice did penalize words occur compare best peer  did truncate sentence baseline greedyline duc mentioned  scoring procedure problem previously greedyline simply takes ﬁrst  words  equivalent knapsack problem large score recent news article document cluster sum  pack maxlength word summary mary evaluation use rouge metric  duplication limitation sentence truncation stemming stopwords removed shown  using stack size  algorithm  devolves standard correlate human judgments lin hovy   exact solution using dynamic programming knapsack lin  best corre  problem                                              lations human judgments duc data    section  compare greedy algorithm sum yen  addition report performance  basic stack decoder algorithm show stack rouge bigram overlap rougesu skip bi  decoder leads reasonably large gains note algo gram metrics  rithm fast  seconds document cluster table stack systems use stack decoder algo  stack size  standard pc                    rithm basic systems use sumbasic’s iterative algo                                                        rithm sufﬁxes names indicate types    summarization simpliﬁed sentences           term scoring methods used discriminative training repre  goal summarization produce summaries sented train using frequencies denoted freq  content possible given length limit using frequencies ﬁrst  words pos ﬁnally  simplify sentences summary inter means score average document fre  moving phrases little expected value quencies frequencies ﬁrst  words  make room additional sentences provide value performed paired ttests comparing systems    each sentence document cluster sentence peer previous best performing sum  simpliﬁcation procedure eliminates various syntactic units basic basicfreq terminology three sys  based predeﬁned heuristic templates remov tems stacktrain stackinterandbasicinterwereallsig  ing noun appositives gerundive clauses nonrestrictive rel niﬁcantly better rouge peer  ative clauses intrasentential attributions unlike previ rouge rougesu three systems just  ous approaches deterministically shorten sentences slightly worse peer differences sig  fore sentence selection conroy et al  niﬁcant systems signiﬁcantly better  siddharthan et al  daum´e iii marcu  basicfreq improved  simpliﬁed sentence approach does replace orig baseline best previous  inal sentence instead added sentence pool  summarizer choose choice sentence rouge version  arguments      alternatives provided simpliﬁcation procedure left                                                        ijcai                                                           rouge    rouge   rougesu                    rouge     rouge   rougesu      stacktrain                         stacktrainsim              stackinter                         basictrainsim              basicinter                          stackfreqsim              basictrain                          basicfreqsim               stackfreq                             stacktrain                       stackpos                              basictrain                       basicpos                              stackfreq                         peer                                basicfreq                        basicfreq                   greedyline                                                                      table  duc rouge rouge rougesu  table  duc rouge stopwords removed      scores sentence simpliﬁcation  stemmed rouge rougesu stemmed scores                                                        discriminatively trained term scores does perform          rouge   rouge    rougesu         best fact model trained      stackinter                        duc data quite different data       stackpos                         mse      basictrain                     stackfreq                        sentence simpliﬁcation duc  mse       stacktrain                        look effects sentence simpliﬁcation ta      basicinter                        ble  shows performance different conﬁgurations         peer                          summarizer sentence simpliﬁcation sim       basicpos                      basicfreq                        number parentheses rouge score                                                        improvement comparing pairs conﬁg  table  mse rouge stopwords removed      urations stacktrainsim versus stacktrainetcwesee  stemmed rouge rougesu stemmed scores      sentence simpliﬁcation consistently raises rouge                                                         score  having essentially impact                                                        rouge rougesu difference rouge    looked component improvements score largest stacktrainsim versus stacktrainand  case stack decoding better basic greedy decod difference statistically signiﬁcant  ing differences statistically signiﬁcant dif pair stackfreqsim versus stackfreq appears  ferences pure frequency approach trained synergy using stack decoder sentence sim  approach stacktrain versus stackfreq basictrain ver pliﬁcation stack decoder allows better  sus basicfreq highly signiﬁcant job choosing candidate sentences  difference basicinter basicfreq highly sig include original sentences simpliﬁed ver  niﬁcant showing using position information sions sentences  does lead improvement  mse                                                  related work   different multidocument summarization task section compare work related work  conducted machine translation summa number aspects scoring method search method  rization workshop acl participating systems produced  word summary document cluster   sumbasic focus scoring individual  mixture english arabic news articles words contrast existing sentencebased  topic arabic documents translated eng sentencebased systems use variety features  lish automatic machine translation systems addition cluding sentence position document sentence length  major difference news articles generally shorter sentence similarity previously extracted sentences usu  used duc ignoring potential mis ally using maximal marginal relevance mmr frame  takes introduced machine translator ran sys work carbonell goldstein  explicit redun  tems speciﬁc modiﬁcations unusual setting dancy score daum´e iii marcu  sentence sim  counting frequencies ﬁrst  words instead ilarity document centroid cases individ   positionbased generative models  pos ual words considered during sentence selection impor  inter                                              tant words identiﬁed through graphbased analysis    shown table  data set mean rouge nodes graph represent words mani bloedorn   score best stackinter better  erkan radev  through proba  best participating peer  original version bilistic measures used work contrast  sumbasic rouge rougesu scores  complex systems features use fre  systems slightly lower three metrics quency positionbased  systems statistically signiﬁcant difference combine position frequency information    notice stackdecoding summarizer ing simple generative simple discriminative                                                    ijcai                                                    
