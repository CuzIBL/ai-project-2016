   approximating optimal policies agents limited execution resources                                        dmitri dolgov edmund durfee                             department electrical engineering science                                                   university michigan                                                   ann arbor mi                                               ddolgov durfeeumichedu                               abstract                               current resource amounts unobservable                                                                  easily estimated agent modeling        agent limited consumable execution re•                                                                 resource amounts state description computationally        sources needs policies attempt achieve good                                                                  infeasible aircraft scenario resources situa•       performance respecting limitations                                                                  tions methods arc beneficial include air•       agent plane fail                                                                  plane broken fuel gauge fuel unobservable        catastrophically crash runs re•                                                                 pilot fatigue attention resource easily es•       sources fuel wrong time midair                                                                  timated combination noncritical resources ex var•       present new approach constructing policies                                                                  ious refreshments exhausted        agents limited execution resources builds                                                                  explicit modeling unnecessarily complicates        principles realtime al research                                                                  optimization problem increases policy size rest        constrained markov decision processes specif•                                                                 paper use fuel example simply        ically formulate solve analyze pol•                                                                 intuitive instance consumable resource        icy optimization problem constraints im•                                                                   explain later standard riskneutral        posed probability exceeding resource                                                                  cmdp optimization techniques applicable prob•       limits empirically evaluate                                                                  lems violating constraints negative        solution technique show computation•                                                                 limit catastrophic  consequences main contribution        ally reasonable generates policies                                                                                            work extends standard cmdp techniques        sacrifice potential reward order make                                                                  handle types hard constraints naturally arise        kinds precise guarantees probability                                                                  problems involving critical resources particular for•       resource overutilization crucial mission                                                                 mulate optimization problem constraints im•       critical applications                                                                  posed improbability resource overutilization show                                                                  problem solved using standard linear pro•   introduction                                                 gramming algorithms formulation yields suboptimal                                                                  solutions constrained problem sacrifices po•  optimality gold standard rational decision making                                                                  tential reward make guarantees probability   russell subramanian  consequently                                                                  violating resource constraints later show   problem constructing optimal policies autonomous                                                                  violating constraints incurs dire costs guarantees   agents received considerable attention years                                                                  worthwhile musliner et al  references   traditionally problem viewed separately                                                                    introduce model section  review   problem actually carrying policies                                                                  markov models introduce notation specify assump•  real agents limitations execute                                                                  tions problem domain section  describes   clearly policy useful agent run                                                                  compares candidate solutions addressing aspects   resources carrying policy                                                                  problem present section  new method     paper present new approach construct•                                                                 empirically evaluate section  conclude   ing policies agents limited consumable re•                                                                 discussion strengths limitations results   sources running resources neg•                                                                 future directions   ative consequences ai research fo•  cused pomdp boutilier et ai  dean et ai   howard  puterman  methods formulating           model   policies agents emphasizing constraints   stochastic properties environments prob•  execution resources operations research literature    lems addressing lead formulate op  developed constrained mdp cmdp altaian  put•    erman  techniques account resource con•        term catastrophic course relative assume   straints cmdp methods particularly useful domains     designer willing accept certain risks receive payoff       resourcebounded reasoning                                                                                            timization problem stationary discretetime markov     justifiable hand problem does natu•   decision process section review       rally fit definition infinitehorizon model    known facts theory standard puterman       plane obviously flying forever    boutilier et  constrained altman  fully   leads make slightly different common    observable markov decision processes discuss     certainly novel assumption    assumptions particular class problems time agent spends executing policy assume    address work section provides neces•     predefined number steps agent spends    sary background subsequent sections dis•    optimal policies yield transient    cuss resourceconstrained optimization problems              markov processes decision problems type exten•                                                                 sively studied kallenberg  policy said yield     markov decision processes                                 transient markov process agent executing policy    standard markov decision process defined tuple  eventually leave corresponding markov chain                 finite set states agent spending finite number time steps given finite   finite set actions agent execute state space assumption implies                           defines transition function      leakage probability    probability agent state executes exist stateaction pairs   action state reward                        particular case assumption holds   function agent receives reward ria executing action  trajectories lead absorbing states     state                                                     agent enters absorbing state finished      clearly total probability transitioning    failed finish task   state given particular action greater  probability transitioning absorbing state zero                discuss actually interested                    planeflying example trajectories   domains exist states                  lead safe landing crash agent      policy defined procedure selecting ac•     enters states probability transitioning   tion each state policy makes choices accord•    states zero   ing probability distribution set actions     transient nature problems leads adopt   called randomized described mapping         expected total reward policy evaluation criterion given   states probability distributions actions              agent receives reward executes action                   deterministic policy chooses     total expected utility policy expressed   action state course special case   randomized policy seen similarly                                                                                                                            case standard cmdps described kallenberg    puterman  optimization criterion   constraints section  deterministic policies subop  number steps during agent accu•  timal work focus approximating      mulates utility transient bounded rewards   optimal policies class randomized ones              sum converges      time  agent initial probability distribution             state space markov follows     related work   following trajectory                                                                  section briefly survey approaches based                                                                  wellknown techniques solving problem finding                                                                                                                             optimal policies agents limited resources point                                                                  assumptions strengths limitations meth•  probability distribution time                  ods establishes landscape solution algorithms                                                                  compare method presented sec•   assumptions                                                                  tion terms complexity efficiency solution quality   typically markov decision processes divided   categories finitehorizon problems total number     fully observable mdps   steps agent spends finite                                                                  straightforward way handling resource con•  known priori infinitehorizon problems                                                                  straints mdp framework explicitly model re•  agent assumed stay forever puterman                                                                  sources making current amounts state    detailed discussion types models                                                                  description yields standard fomdp      work concentrate dynamic realtime domains                                                                  solved wide variety efficient methods benefit   agents tasks accomplish example con•  sider agent flying plane goal safely alternative way handling states treat   destination land example does naturally    infinitelyrecurrent agent gets infinitely tran•  correspond finitehorizon problem duration   sitions adopt model   executing various policies predetermined unless  natural domains leads unnecessary complications   artificially impose finite duration easily optimization problems                                                                                       resourcebounded reasoning  approach allows make use rele• munication domain  standard cmdp imposes    vant information construct best possible policy constraints expected amounts clearly method    agent able base action choice current state does work critical resources overutilization    resource amounts downside approach    negative consequences agent pilots aircraft    requires priori discretization resource amounts satisfied policy average uses    world model constructed   fuel does exceed tank capacity    additional burden specifying rewards state transi•   tions functions current resource amounts furthermore   sample path mdps    model size state space consequently just mentioned ross chen pointed weak•  policy size explodes exponentially number re•   ness cmdp approach constraints expected    sources compared state space resource    amounts possible solution ross varadarajan     amounts folded state description             propose approach constraints placed      fomdp approach relies fact agent    actual samplepath costs work space feasi•   observe exact amounts resources runtime how•   ble solutions constrained set policies proba•   hold especially multiagent domains bility violating constraints overutilizing resources    shared resources agent does know      long run zero work concentrates    agents doing shared resources   average perstep costs rewards inter•   consuming                                     ested total amounts distributions easily                                                                  calculable approach ross varadarajan    constrained mdps                                           benefits standard cmdp method explicit   alternative fomdp approach described     modeling resources required state space    treat problem constrained markov decision process    policies small addition unlike standard cmdp    altman  resources explicitly mod•   method suitable problems critical resources    eled treated constraints imposed weakness approach prob•  space feasible solutions policies                    lems restrictive allows possibility      constrained mdp resourcelimited agent differs      overutilizing resources policies produced    standard mdp agent getting re•    sample path method significantly lower pay•  wards executing actions incurs resource costs con•  compared policies probability   sequently constrained mdp cmdp described      resource overutilization furthermore domains   tuple                            desirable designer able con•  defines vector actions costs units resource           trol probability resource overutilization means               used action executed state      balancing optimality risk   vector amounts available   resources units resource initially available     mdps constraints variance      benefit cmdp does require     approach handling deviations expecta•  explicitly model resources affect world in•    tion markov models impose additional constraints   deed policies satisfy resource constraints does assign additional cost variance sobel    worry happens resources        proposed constrain expected cost maximize   overutilized consequently state space re•    meanvariance ratio reward huang kallenberg   sulting policies exponentially smaller ones  proposed unified approach handling variance   fomdp model standard cmdp formulation constrains          algorithm based parametric linear programming   expected usage resources certain limit approaches benefits standard cmdp   formulated following linear program         samplepath methods compared fomdp formula•                                                                 tion terms state space policy size                                                               complexity constructing initial world model addi•                                                                 tionally allow somewhat balance payoff   variables interpretation expected    deviation expected methods   number times action executed state               allow make hard guarantees probability      weakness approach yields policies overutilizing resources   suboptimal compared ones constructed   fomdp method agent does base de•       linear approximation   cision current resource amounts completely  hinted previous sections like able   ignores aspect state mentioned constrain feasible solution space set policies   previous section domains resources ob•   probability overutilizing resources   servable policy size vital importance exam• userspecified threshold words   ple agents architecture imposes constraints policies like able solve following math program   store approach prove fruitful      biggest problem approach                                                                   pointed example ross chen telecom      resourcebounded reasoning                                                                                            total resource used       reduce bias arise using small num•   policy upper bound resource  ber handcrafted test cases instead used large      trouble optimization space     number randomlygenerated constrained mdps    interpreted expected number times  generated problems shared common properties    executing actions corresponding states   interesting ones following values    difficult express simple function            main experiments given parentheses    optimization variables contain                   total number states actions re•   information probability distribution random       sources respectively       variable number times action actually executed                                                                                   maximum reward rewards assigned    corresponding state  expected values    section present linear approximation       uniform distribution    program based markov inequality                     mc  maxc maximum action cost resource costs                                                                      assigned stateaction pairs based                                                                   distribution described     using inequality fact expected resource                 correlation rewards resource   usage expressed opti•                                costs better actions typically costly   mization problem formulated linear program                                                                                               upper bounds resource                                                                      amounts assigned according uniform distribu•                                                                     tion range                                                                        dissipation probability  probability agent                                                                      exits each time step       potential weakness approach markov   inequality gives rough upper bound policies    parameter used ensure transient chain in•  correspond solutions lp conserva•   stead providing small number sink states   tive real probability overutilizing resources chosen use uniform dissipation probability order   significantly lower making hard        avoid additional randomization experiments   guarantees probability overutilizing resources choice provides stable domain   vital importance method prove valuable  main concern behavior approxima•  yields policies general higher payoff tion function probability threshold   ones obtained samplepath method  run number experiments various values   restrictive hand unlike standard            precise gradually increased   cmdps impose constraints expected resource us•         increments  each value gen•  age mdps constrain variance method     erated  random models solved using three   allows explicitly bound allowable probability re• methods  unconstrained mdp regard   source overutilization make precise guarantees   resource limitations  standard cmdp constraints   behavior respect                   expected usage resources  cmdp con•     worth noting unlike samplepath method straints probability overutilizing resources eq   methods constrain variance method re•      evaluated using montecarlo simulation each   lies solving standard linear program for•    three solutions policies terms expected reward   mer require solving quadratic parametric linear programs   probability overutilizing resources   formulation appears reasonable      figure  shows plot actual probability overuti•  approximation harder solve    lizing resources policies obtained three   standard cmdp section  experimental results   methods function probability threshold    providing means balancing solution quality    data points averaged runs particular value   precisely quantifiable risk resource overutilization  po curve corresponds method bounds                                                                 overutilization probability shows standard devia•   evaluation                                                  tion runs data similar variance                                                                 use plots means averaged runs   verify hypotheses properties approxi• given po analyses   mation described previous section performed      obviously po effect unconstrained   set numerical experiments compare behavior   standard cmdp maintain constant   standard cmdp constraints expected resource     probability overutilization does affect large ex•  amounts section  unconstrained mdp             tent solutions problem constraints overuti•                                                                lization probability overutilization prob•     generally speaking total cost sum random num•  ber differentlydistributed random variables calculating ability solutions produced approach   probability distribution nontrivial task                worth noting        inequality holds nonnegative random variables approaches  approximation yields results   transient make assumption methods good setting   costs nonnegative loss generality    constrain space feasible solutions                                                                                      resourcebounded reasoning        figure  probability resource overutilization                figure  average rewards penalties overutilization                                                                              large penalties iv   conservative                                                                            policy outperforms values  note                                                                            policy reevaluated postfactum section  briefly                                                                            discusses approach explicitly models penalties                                                                            optimization program                                                                               mentioned section  linear approximation                                                                            harder solve standard constrained                                                                            mdp formulated linear programs                                                                            number constraints experimentally verify                                                                            timed runs experiments figure                                                                             contains plot times took solve prob•                                                                           lems experiments running                                                                              times three methods appreciably different                                                                            particular average ratio running time stan•  figure  average rewards solutions three problems                                                                            dard cmdp running time unconstrained method                                                                             ratio running time linear approxi•                                                                           mation constraints overutilization probability      rewards obtained policies shown fig•                                                                           running time unconstrained method  slight   ure  actual rewards necessarily equal ex•                                                                           downward curvature plot running time   pected rewards used during optimization pro•                                                                           approximation method appears consequence   cess runs did overutilize                                                                            specific implementation linear programming algorithm   resources included average policies                                                                            used experiments   penalized violating resource constraints   explains total rewards received solutions   standard cmdps greater ones ob•                   discussion future work   tained solutions unconstrained problems                                                                            experiments substantiate claims approxima•     realistically agent incurs penalty          tion provides effective efficient method agents   overutilizing critical resource penalty           use formulate policies consider limita•  based consequences overutilization example               tions execution resources explicitly bound   agent flying plane resource question          probability resource overutilization new approxi•  fuel consequences trying use re•             mation achieves constraints overutilization es•  source catastrophic penalty high              sentially expensive use standard cmdp   account assigning fixed penalty poli•             unconstrained mdp methods method con•  cies overutilize resources update graph          structs policies careful avoiding resource   figure  realistic picture figure  shows             overutilization rewards associated policies   average rewards solutions obtained three meth•               resources overutilized tend re•  ods recalculated reflect penalties new rewards              wards methods policies resources                                   overutilization             overutilized illustrated figure    probability average reward successful runs          overutilization incurs penalties new method outper  given policy figure  penalty overuti•    lization penalty account           note used linear programming ap•  interval po conservative policy produced              proach solving unconstrained problem different algorithm   linear approximation outperforms policies              value policy iteration efficient       resourcebounded reasoning                                                                                                              
