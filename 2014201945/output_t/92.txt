                          decisiontheoretic model assistance              alan fern   sriraam natarajan      kshitij judah    prasad tadepalli                                 school eecs oregon state university                                         corvallis  usa                           afernnatarasrjudahktadepalleecsoregonstateedu                        abstract                          providing formal basis designing intelligent assistants                                                          ﬁrst contribution work formulate prob      growing intelligent assis lem selecting assistive actions assistant pomdp      tants variety applications organiz jointly models application environment      ing tasks knowledge workers helping peo   agent’s hidden goals key feature approach      ple dementia paper present  explicitly reasons environment agent      evaluate decisiontheoretic framework cap provides potential ﬂexibility assisting ways unfore      tures general notion assistance objec seen developer solving policies      tive observe goaldirected agent se typically intractable rely approximations      lect assistive actions order minimize second contribution suggest approximate solution ap      cost model problem assistant    proach argue suited assistant pomdp      pomdp hidden state corresponds   application domains approach based ex      agent’s unobserved goals formulation allows plicit goal estimation myopic heuristics online action      exploit domain models estimating   selection goal estimation propose modelbased      agent’s goals selecting assistive action bootstrapping mechanism important usability      addition formulation naturally handles uncer assistant early lifetime action selection      tainty varying action costs customization propose myopic heuristics based solving set      speciﬁc agents learning argue derived assistant mdps based simulation      domains myopic heuristics adequate se technique policy rollout contribution evalu      lecting actions assistant pomdp present ate framework novel gamelike environ      heuristics evaluate approach  ments using  human subjects results indicate      domains human subjects perform tasks    proposed assistant framework able signiﬁcantly decrease      gamelike environments results   user effort myopic heuristics perform      show assistant substantially reduces user computationally intensive method sparse sampling      effort modest computational effort     remainder paper organized follows                                                        section introduce formal problem setup    introduction                                       followed deﬁnition assistant pomdp  development intelligent assistants present approximate solution technique based goal es  tremendous impact potential domains variety timation online action selection finally em  ai techniques used purpose domains pirical evaluation approach domains  assistive technologies disabled boger et al clude discussion related future work   desktop work management calo  work ﬁnetuned particular  problem setup  application domains paper evaluate refer entity attempting assist  comprehensive framework intelligent assistants agent model agent’s environment    consider model assistant observes goal markov decision process mdp described tuple  oriented agent select assistive actions order w atciwherew ﬁnite set world states  best help agent achieve goals perform ﬁnite set agent actions a ﬁnite set assistant  sistant able accurately quickly infer goals actions w transition distributions rep  agent reason utility various assis resents probability transitioning state w given  tive actions achieving goals real applications action ∈ ∪ a taken state use  requires assistant able handle uncertainty denote random variable distributed ·  environment agent reason varying ac assume assistant action set contains  tion costs handle unforeseen situations adapt action noop leaves state unchanged compo  agent time consider decisiontheoretic nent actioncost function maps × ∪ a  model based partially observable markov decision pro realnumbers initial state distribution   cesses pomdps naturally handles features consider episodic setting beginning                                                     ijcai                                                    each episode agent begins state drawn lenges selecting assistive actions ﬁrst challenge  selects goal ﬁnite set possible goals gthe infer agent’s goals critical provide good  goal set example contain possible dishes sistance capture goal uncertainty including  agent prepare left unassisted agent exe agent’s goal hidden component pomdp state  cute actions arrives goal state effect belief state correspond distribution  episode ends assistant present able possible agent goals second challenge  observe changing state agent’s actions know agent’s goals reason uncertain  unable observe agent’s goal point environment agent policy action costs order select  agent’s state trajectory assistant allowed execute best course action pomdp capture infor  sequence actions a ending noopaf mation transition function cost model providing  ter agent perform action episode decisiontheoretic basis reasoning  ends agent assistant action leads goal given environment mdp w atci goal  state cost episode equal sum costs distribution agent policy π deﬁne cor  actions executed agent assistant during responding assistant pomdp  episode note available actions agent state space × each state pair  sistant need varying costs world state agent goal initial state distribu                                                               objective minimize expected total cost episode tion assigns state probability iwggwhich    formally model agent unknown stochas models process selecting initial state goal  tic policy πaw gives probability selecting ac agent beginning each episode action set  tion ∈ given agent goal state equal assistant actions a reﬂecting assistant  assistant historydependent stochastic policy pomdp used select actions                                            ∈                                π gives probability selecting action transition function assigns zero probability  given world state stateaction trajectory observed transition changes goal component state  starting beginning trajectory critical assistant change agent’s goal  assistant policy depend prior states action noop state transitions  actions serve source evidence agent’s goal wg probability wforthenoop action  critical selecting good assistive actions given                                                         simulates effect executing agent action selected  initial state agent policy π assistant policy π according πthatist w noop wg equal                                                                                        let π π denote expected cost episodes probability πw    begin state goal evolve according fol                                                                    cost model  reﬂects costs agent assis  lowing process  execute assistant actions according π tant actions mdp actions noop  noop selected  execute agent action according cw gacw  πifg achieved terminate step  cw noopecw awherea random vari    work assume disposal able distributed according π·w cost  environment mdp set possible goals ob                                                       noop action expected cost ensuing agent action  jective select assistant policy π minimizes                                                                                observation distribution μ deterministic  expected cost given ecigππ whereg                                                        ﬂects fact assistant directly observe  unknown distribution agent goals π unknown                                                        world state actions agent noop action  agent policy simplicity assumed fully observ                                                                                                state leading state observation  able environment episodic setting choices                                                        action executed agent immediately af  fundamental framework                                                        ter noop actions observation equal                                                                                                     assistant pomdp                                component state μ gw afor                                                        simplicity notation assumed component  pomdps provide decisiontheoretic framework deci state encodes preceding agent assistant action  sion making partially observable stochastic environments observations reﬂect states actions  pomdp deﬁned tuple s μwhere                                                       assume episodic objective episodes begin  ﬁnite set states ﬁnite set actions drawing initial pomdp states end arriving  transition distribution action cost function                                                                                  state satisﬁes goal policy π assis  initial state distribution ﬁnite set observations tant pomdp maps stateaction sequences assistant actions      distribution observations ∈ given                                    μ                                             expected cost trajectory π equal ob  current state pomdp policy assigns distribution                                                                              jective function ecigππ  previous section  actions given sequence preceding observations solving optimal assistant pomdp policy yields  tenusefultoviewapomdpasanmdpoveraninﬁnitesetof        optimal assistant problem setup  belief states belief state simply distribution                                                        sistant pomdp directly disposal  case pomdp policy viewed map                                                        given π given environment  ping belief states actions actions serve mdp set possible goals described  decrease uncertainty state observations                                                        section approach approximate assistant pomdp  produce andor make direct progress goals                                                        estimating π based observations select    use pomdp model address main chal sistive actions based model                                                     ijcai                                                      selecting assistive actions                        pomdp solved quite effectively just observing                                                        agent’s actions relate various possible goals  approximating assistant pomdp approach ap suggests domains little  proximating assistant pomdp observe agent act value selecting assistive actions purpose gath  ing environment possibly assisted ering information agent’s goal suggests  learn goal distribution policy π effectiveness myopic action selection strategies avoid  storing goal achieved end each episode explicit reasoning information gathering  set world stateaction pairs observed agent key pomdp complexities compared mdps note  during episode estimate based cases assistant pure information  observed frequency each goal laplace cor gathering actions disposal asking agent ques  rection likewise estimate πaw simply tion consider actions experi  frequency action taken agent ments mentioned believe actions  state having goal limit esti handled shallow search belief space conjunction  mates converge yield true assistant pomdp myopic heuristics motivation action  practice convergence slow slow convergence selection approach alternates operations  lead poor performance early stages assistant’s goal estimation given assistant pomdp agent  lifetime alleviate problem propose approach policy π initial goal distribution objective  bootstrap learning π                          maintain posterior goal distribution gotwhich    particular assume agent reasonably close gives probability agent having goal conditioned  optimal unrealistic applica observation sequence ot note assistant  tion domains beneﬁt intelligent assistants affect agent’s goal observations related  tasks conceptually simple hu agent’s actions relevant posterior given  mans require substantial effort complete given agent policy π straightforward incrementally update  assumption initialize estimate agent’s pol posterior got each agent’s actions  icy prior biased optimal agent ac beginning each episode initialize goal distribu  tions consider environment mdp tion timestep episode ot does  assistant actions removed solve qfunction involve agent action leave distribution            qfunction gives expected cost exe changed ot indicates agent selected  cuting agent action world state acting opti action state update distribution according  mally achieve goal using agent actions gotz · got− · πaw gwherez  deﬁne prior agent actions boltzmann dis normalizing constant distribution adjusted  tribution experiments prior pro place weight goals likely cause  vides good initial proxy actual agent policy al agent execute action  lowing assistant immediately useful accuracy goal estimation relies pol  date prior based observations better reﬂect pe icy π learned assistant reﬂects true agent  culiarities given agent computationally main ob scribed use modelbased bootstrapping approach  stacle approach computing qfunction estimating π update estimate end each  need given application domain episode provided agent close optimal  number algorithms exist accomplish includ experimental domains approach lead rapid goal  ing use factored mdp algorithms boutilier et al estimation early lifetime assistant   approximate solution methods boutilier et al  assumed simplicity actions  guestrin et al  developing specialized solutions agent directly observable domains    action selection overview let   ob natural assume state world observ                                                    able actual action identities cases  servation sequence beginning current trajec                                                                                              observing agent transitioning  tory time each observation provides world state use mdp transition function marginalize possi  previously selected action assistant ble agent actions yielding update  agent given ot assistant pomdp goal se                                                                                                                             lect best assistive action according policy π ot  gotz · got− · πaw gt waw     unfortunately exactly solving assistant pomdp                         a∈a  intractable simplest domains led action selection given assistant pomdp  heuristic action selection approach motivate distribution goals got address problem  approach useful consider special characteris selecting assistive action introduce idea  tics assistant pomdp importantly belief state assistant mdp relative goal  corresponds distribution agent’s goal denote mg each episode mg evolves drawing  agent assumed goal directed observed agent ac initial world state selecting assistant actions  tions provide substantial evidence goal noop agent executes action drawn  fact assistant does policy achieving goal optimal policy mg  agent’s goals rapidly revealed sug gives optimal assistive action assuming agent  gests stategoal estimation problem assistant acting achieve goal denote qfunction                                                     ijcai                                                    mg qgw expected cost executing  doorman domain  action following optimal policy    ﬁrst myopic heuristic simply expected qvalue doorman domain agent set possible  action assistant mdps heuristic value goals collect wood food gold grid  assistant action state given observations ot cells blocked each cell doors agent                                                       open door cell figure                                ·               aot      qg  ot              door closes timestep time                                                      door open goal assistant help user reach    select actions greedily according intuitively goal faster opening correct doors  hw  measures utility taking action                                                        state tuple s dwheres stands agent’s  assumption goal ambiguity resolved step                                                        cell door open actions agent  heuristic select actions purposes                                                        open door each  directions  information gathering heuristic lead assistant                                                        pickup cell total  actions  select actions make progress goals high                                                        assistant open doors perform noop  actions  probability avoiding moving away goals                                                        assistant allowed push agent through  high probability goal posterior highly ambigu                                                        door agent’s assistant’s actions strictly alter  ous lead assistant select noop                                                        nate domain cost − user    primary computational complexity computing                                                        open door cost assistant’s action trial  solve assistant mdps each goal technically                                                        ends agent picks desired object  transition functions assistant mdps depend  approximate agent policy π resolve each mdp af experiment evaluated heuristics hd hr  ter updating π estimate end each episode each trial chooses goal  using incremental dynamic programming methods heuristics random user shown goal tries  prioritized sweeping moore atkeson  al achieve starting center square  leviate computational cost particular user’s action assistant opens door does  deploying assistant solve each mdp ofﬂine based user pass through door open different door  default agent policy given boltzmann bootstrap user achieves goal trial ends new  ping distribution described earlier deployment prior begins assistant uses user’s trajectory update  itized sweeping used incrementally update agent’s policy  values based learned reﬁnements make π    results user studies doorman domain    practical solve assistant mdps presented figure  ﬁrst rows cummulative  resort various approximations consider ap results user study actions selected according  proximations experiments replace user’s hr hd respectively table presents total op  policy used computing assistant mdp ﬁxed timal costs number actions trials users  default user’s policy eliminating need compute assistant costs assistant                                                        average percentage cost savings  sistant mdp step denote approximation                                                                           trials users  seen hr appears  hd approximation uses simulation technique  policy rollout bertsekas tsitsiklis  approx slight edge hd  imate qgw expression  ﬁrst simulating effect taking action state  using π estimate expected cost agent  achieve resulting state approximate  qgw assuming assistant select  single initial action followed agent actions             ¯  formally let cnπ function simulates tra  jectories π achieving goal state andthenav  eraging trajectory costs heuristic hr identical                                     a ot replace qg expec                       · ¯      tation w∈w   π  com  bine heuristics denote hdrfi  nally cases beneﬁcial explicitly reason    figure  doorman domain  information gathering actions combine myopic  heuristics shallow search belief space assistant compare results sophisticated solution  mdp approach lines use sparse sam technique selected actions using sparse sampling kearns  pling trees kearns et al  myopic heuristics et al  depths   using  sam  used evaluate leaf nodes                      ples step leaves sparse sampling tree    experimental results                                                           gives pessimistic estimate usefulness assis  conducted user studies simulations domains tant assuming optimal user measure utility normalized  present results section              optimal utility aid assistant                                                     ijcai                                                                   total   user     average    time       contents bowl replace ingredient                                  −         heuristic  actions actions                 shelf assistant perform user actions                                         action                                    ±                   pouring ingredients replacing ingredient        hr                         shelf cost nonpour actions  experiments        hd                  ±                                       ±                  conducted  human subjects unlike doorman        hr                         domain necessary assistant wait ev    db               ±      db               ±         ery alternative time step assistant continues act    db                ±         noop best action according heuristic    db               ±           domain bigger statespace ﬁrst                                                        domain heuristics use default user’s policy  figure  results experiments doorman domain words compare hd hdr results user stud  ﬁrst half table presents results user stud ies shown half figure  hdr performs better  ies lower half presents results simulation hd observed experiments hdr                                                        technique aggressive choosing nonnoop actions  evaluated using hr experiments did hd wait goal distribution  duct user studies high cost studies highly skewed particular goal currently try  simulated human users choosing actions according ing understand reason behavior  policies learned observed actions results  presented half figure  sparse         total   user      average    time  sampling increased average run time order mag heuristic actions actions  −     nitude able produce reduction average cost                                action                                                                                            ±  user surprising simulated experi hdr                                                                                                         ±  ments sparse sampling able sample exact user hd                   policy sampling learned policy hdr           ±    used simulationsit remains seen db       ±    beneﬁts realized real experiments db           ±                                                                                              ±  approximate user policies                               db                                                                           db             ±      kitchen domain                                                        figure  results experiments kitchen domain  kitchen domain goals agent cook vari ﬁrst half table presents results user studies  ous dishes  shelves  ingredients each each lower half presents results simulation  dish recipe represented partially ordered plan  ingredients fetched order mixed compared use sparse sampling heuristic  heated shelves doors simulated user trajectories domain wellsee bot  opened fetching ingredients door tomhalfoffigure thereisno signiﬁcant difference  open time                                       tween solution quality rollouts sparse sampling                                                        simulations showing myopic heuristics perform                                                        ing sparse sampling computation                                                           related work                                                        prior work intelligent assistants did                                                        sequential decision making decisiontheoretic approach                                                        example email ﬁltering typically posed supervised                                                        learning problem cohen et al  travel planning                                                        combines information gathering search constraint                                                        propagation ambite et al                                                           personal assistant systems                                                        explicitly based decisiontheoretic principles                                                        systems formulated pomdps ap  figure  kitchen domain user prepare                                                        proximately solved ofﬂine instance coach  dishes described recipes right assistant’s                                                        helped people suffering dementia giving ap  actions shown frame                                                        propriate prompts needed daily activities boger et     different recipes state consists lo al  use plan graph track user’s  cation each ingredient bowlshelftable mixing progress estimate user’s responsiveness deter  state temperature state ingredient best prompting strategy distinct difference  bowl door open state includes approach single ﬁxed goal wash  action history preserve ordering plans ing hands hidden variable user respon  recipes user’s actions open doors fetch siveness formulation possible  gredients pour bowl mix heat bake goals current goal hidden assistant note                                                     ijcai                                                    
