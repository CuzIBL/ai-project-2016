      learning policies embodied virtual agents through demonstration                       jonathan dinerstein                   parris egbert     dan ventura                   dreamworks animation                cs department brigham young university               jondinersteinyahoocom                   egbertventuracsbyuedu                          abstract                            reinforcement learning rl powerful scheme                                                        obvious choice policy learning does meet      powerful ai machine learning    requirements — rl does scale contin      techniques exist remains difﬁcult quickly cre uous state spaces high dimensionality common      ate ai embodied virtual agents produces  evas challenging encode aesthetic goals      visually lifelike behavior important ap reward structure different approach      plications games simulators interactive dis present technique automatic learning policy      plays agent behave manner mimicking demonstrated human behavior technique      appears humanlike present novel technique  provides natural simple method      learning reactive policies mimic demon  struction policy allows animator      strated human behavior user demonstrates nontechnical person intimately involved      desired behavior dictating agent’s actions struction policy learn reactive policy      during interactive animation later  advanced behavior learning simple      agent behave autonomously recorded data nearly automatic allowing ai creation non      generalized form continuous statetoaction technical users approach empirically shown ef      mapping combined appropriate animation   fective quickly learning useful policies behavior ap      algorithm motion capture learned poli pears natural continuous state action spaces high      cies realize stylized naturallooking agent dimensionality      havior empirically demonstrate efﬁcacy      technique quickly producing policies   related work      result lifelike virtual agent behavior                                                        designing developing embodied virtual agents funda                                                        mentally multidisciplinary problem gratch et al     introduction                                       cole et al  study evas overlaps                                                        scientiﬁc ﬁelds including artiﬁcial intelligence com  embodied virtual agentorevaisasoftwareagentwith  puter graphics computerhuman interfaces                                           body represented through graphics badler et al graphics community interested                            dinerstein et al   given sufﬁcient intelligence evas reynold’s seminal work ﬂocking behavior  characters animate choosing actions reynolds  wellknown examples  perform each action motion applications ﬁlm lord rings trilogy duncan  depict  agents include games animation ing epicscale battle scenes humanoids largescale  virtual reality training simulators forth   animation implausible characters ani    despite success evas certain domains im mated manually notable techniques constructing  portant arguments brought current tech evas developed monzani et al  niques particular programming agent ai difﬁcult explicit programming agent ai remains difﬁcult task  timeconsuming challenging achieve natural gameoriented eva research includes development  looking behavior address issues learning pol action selection learning architectures laird   icy controlling agent’s behavior     computational models emotion proposed    scalable — policy learning scale problems gratch marsella  recent work examined      continuous highdimensional state spaces rapid behavior adaption eva better inter                                                        act given human user dinerstein egbert     aesthetic — virtual agent behavior appear dinerstein et al  work scheme      natural visually pleasing                    veloped nonembodied agent textbased com    simple — policy learning directable puter game learns through interaction multiple users      nontechnical user                               bell et al                                                     ijcai                                                      discussed introduction designing program  ming eva ai challenging authors ad  dressed through reinforcement learning survey given  dinerstein et al  rl natural choice  long used agent policy learning rl does  meet requirements paper example  quite challenging achieve naturallooking behavior  aesthetic goals integrated ﬁtness func figure  workﬂow technique simula  tion evas live continuous virtual worlds tor drives animation provides current state  require continuous state andor action spaces visualized demonstrator demonstrator  ﬂocking evas reynolds  traditional rl sponds action recorded used  tractible large continuous stateaction spaces date simulation stateaction examples pro  recent research begun address problem ng cessed eliminate conﬂicts generalized  jordan  wingate seppi  tinuous policy function policy used online control  learning computationally expensive limited agent  technique learns stylized eva behaviors quickly empiri  cally shown later                                         eliminate conﬂicts    agents robotics communities long recog                                           μ  nized need simpliﬁed programming agent ai      generalize stateaction pairs policy   interesting work performed pro   autonomous behavior  gramming demonstrationsuchaspomerleau           use μ compute decisions agent  van lent laird  mataric  kasper et al  ﬁxed time step Δt                                   angros et al  nicolescu   agent process learning approximate intelligent  structed through demonstrations user technique ﬁts cision making human example formulate decision  category speciﬁcally designed evas trained making policy  nontechnical users contrast existing tech                                                                             μ  →                    niques approach does require signiﬁcant effort  programmer domain expert demonstration ∈ rn compact representation current state  performed existing technique related character world ∈ rm action chosen  work “learning fly” sammut et al perform each component salient feature deﬁning  technique unique aspects particular important aspect current state character  performs conﬂict elimination operate ﬁnite repertoire possible actions quantized  tinuous action spaces operated nontechnical stateaction pair denoted sa demonstrator’s  users previous work conﬂict elim behavior sampled ﬁxed time step Δt equal  ination through clustering friedrich et al butthat rate character choose actions  approach quantizes training examples culls po autonomous observed behavior set dis  tentially useful cases                               crete cases  sa  saq each pair represents                                                        case target behavior ordering                                                                                 μ  contribution present novel scheme program  pairs set construct generalizing cases                                                        speciﬁcally ensure character’s behavior smooth  ming eva behavior through demonstration                                   μ  components scheme preexisting overall aesthetically pleasing construct interpolating                                                                                                 ∈ rn  application unique notable actions associated cases                                                        ∈ rm  critical components scheme novel speciﬁc     theoretically use continuous realvector  contributions paper include             valued interpolation scheme implementation ei                                                        ther use εregression svm continuous knearest neighbor    •      application agent programmingbydemonstra detailed later action set symbolic use voting      tion concepts policy learning humanoid evas knearest neighbor    • intuitive interface nontechnical users quickly generalization cases succeed critical      create stylized naturallooking eva behavior state action spaces organized programmer    •  novel conﬂict elimination method previous tech similar states usually map similar actions                                                        words si −s j  α ⇒ai −a j  βwhereα β      niques agentsrobotics usually ignored                        ·      blended conﬂicts                                small scalar thresholds euclidean metric cer                                                        tainly constraint need hold smoother                                                                                                  μ    eva programming demonstration                   mapping accurate learned policy  learning eva policy through demonstration involves  training  following steps figure                        training mode demonstrator programmer    train                                            need involved programmer integrates       observe record stateaction pairs       technique existing far “brainless” character                                                    ijcai                                                                                                           each recorded pair sa   integration involves designing state action spaces                         μ learnable integration complete closest neighbors sa                                                             compute median action avm neighbors using eq   demonstrator free create policies character                                                                     avm –   η  fact creation multiple policies agent         interesting achieve different stylized behaviors mark sa    demonstration proceeds follows character delete marked pairs  virtual world visualized realtime demonstra  tor interactive control actions character figure  conﬂict elimination algorithm  note continuous realtime presentation state  formation demonstrator critical making char                                                        experiments works use  acter training process natural possible analo                                                          use threshold η  possible  gous humans naturally perceive real world                                                        range component values  simulationvisualization character world pro  ceeds realtime demonstrator supplies character  autonomous behavior  actions perform information saved  form stateaction pairs stateaction adequate set stateaction pairs collected  examples collected conﬂicting examples construct continuous policy μ  → awe  automatically eliminated described through popular machine learning tech  discrete representative sampling entire policy func niques εregression svm haykin  knearest neigh  tion μ demonstrator control bor mitchell  through classiﬁcation scheme  character forced regions state actions symbolic chosen techniques  space — density sampling cor powerful wellestablished  responds importance each region state space trasting strengths weaknesses application    elimination conﬂicting examples important primary strength continuous knearest neighbor  human behavior deterministic nn strong guarantees accuracy  examples likely conﬂict important issue merely interpolates local cases robustly handle  machine learning schemes utilize “aver nonsmooth mappings outputs  age” conﬂicting examples result unrealistic vex hull local cases use knn  unintelligentlooking behavior note conﬂict resolution demonstrated policy rough knn does  possible augmenting state context large memory footprint ∼ mb policy  actions performed increase stateaction cases required svm weaker  statespace dimensionality make learning μ generalization  challenging require additional training examples support vector machine svm interesting alter  designed technique directly eliminate native knn compact global technique  conﬂicts                                             result performs powerful generalization struggle    conﬂicting examples formally deﬁned         highly nonsmooth mappings svm                                                        useful μ somewhat smooth especially   s −s   ν a −a   υ conflict                                                 continuous use popular εregression scheme  ν υ scalar thresholds eliminate conﬂicts gaussian kernel perform function approximation  arbitrarily delete cases involved conﬂict —  lead high frequencies policy goal  experimental results  remove examples represent high frequencies    pseudocode conﬂict elimination technique given experiments designed cover general fash  figure  complement pseudocode ion major distinguishing aspects popular uses  scribe technique brief each stateaction pair tested embodied virtual agents favorable experi  turn determine outlier neigh mental results results agentsrobotics litera  bors according state current example ture programmingbydemonstration related work  median action avm computed using follow section empirical evidence scheme  ing vectormedian method koschan abidi    viable tool creating popular types eva policies                                                        results achieved experiments summa                                                      rized table  comparisons reinforcement learn          avm  arg   min       ∑  ai −a j                        ∈a                      ing pegasus summarized table  dis                                                                               cussed later rl ap  words avm equal action neighbor proach technique believe valuable compare  closest actions neighbors according rl popular approach results                                         euclidean metric finally avm –   η informal user case study reported table  videos  case outlier marked deletion marked cases animation results available  retained testing cases deleted httprivitcsbyueduadg  batch conclusion conﬂict elimination algorithm publicationsphp                                                    ijcai                                                      pre  number stateaction pairs               time demonstrate pairs    min          time eliminate conﬂicts        sec    knn  compute target function values    μsec          storage requirements             ∼mb          total decisionmaking time cross ﬁeld  msec          total animation time cross ﬁeld  sec   svm    time train εregression svm    min     figure  spaceship pilot test bed pilot’s goal cross          compute new target function values  μsec    asteroid ﬁeld forward motion quickly possible          storage requirements             ∼ kb       collisions          total decisionmaking time cross ﬁeld  msec          total animation time cross ﬁeld  sec                                                        approximationbased rl using multilayer neural net                                                        failed learned policies poor type  table  summary average usage performance results learning typically considered stable employing  spaceship pilot case study  ghz processor                 ∈                                   linear function approximator limit learning   mb ram       similar results achieved tesauro  sutton barto  ap  experiments                                    plication demonstrationbased learning appealing                                                        effective highdimensional continuous state action    spaceship pilot                                  spaces                                                          tested nonstandard form rl designed specif  ﬁrst experiment virtual agent spaceship pilot ically continuous state spaces pegasus ng jor  figure  pilot’s task maneuver spaceship dan  ng et al  pegasus neural network  through random asteroid ﬁelds ﬂying end learns policy through hill climbing speciﬁcally small  ﬁeld quickly possible collisions set ﬁnitehorizon scenarios used evaluate policy  animation runs  frames second action determine ﬁtness landscape unlike traditional rl  computed each frame virtual pilot controls ori pegasus succeeded learning effective policy  entation spaceship                            case study table  interestingly performance    μ formulated follows inputs space policies learned through scheme pegasus sim  ship’s current orientation θφ separation vector ilar pegasus learning far slower  spaceship nearest asteroids ps − technique hours versus minutes ex  pa ps − pa complete state vector  tremely difﬁcult technical tune pegasus reward  θφΔxΔyΔzΔxΔyΔz outputs structure aesthetics pegasus powerful  determined change spaceship’s orientation useful technique does fulﬁll requirement given   ΔθΔφ                                          introduction providing nontechnical users    achieved good results experiment shown intuitive robust interface programming eva ai  table  accompanying video svm knn result rl does appear good ﬁt application  policies worked random asteroid ﬁelds tried analyzed explicit action selection through ta  animation naturallooking intelligent aes ble  empirically proven successful requires  thetically pleasing veriﬁed informal user case study signiﬁcantly cpu policies make decisions    gain point reference illuminate produces behavior does appear natural  ing computational properties technique analyzed requires programmer develop admissible heuris  learning policy through reinforcement learning table  tic  analysis helps validate nonrl approach im  portant fulﬁlling goals straightforward formu  crowd human characters  late case study mdp state action spaces experiment created policies control groups  deﬁned complete model human characters figure  characters milled  deterministic environment needed animation initial boulder fell sky ran away  state position side asteroid ﬁeld policies controlled decision making charac  reward structure positive safely crossing ﬁeld ters “turn left” “walk forward” nutsand  negative crashing using stateoftheart bolts animation carried traditional skeletal mo  tablebased techniques solving large mdp’s wingate tion policy speciﬁed small set  seppi  currently implausible perform rl discrete actions realvalued policy output quantized  continuous state space dimensionality greater  discrete created crowd animations each  size requires supercomputer mdp state space animation characters used policy showing  dimensionality  gross discretization intractable variety behavior achieved note each pol   divisions axis  states best result icy constructed required minutes capture  reported wingate seppi  dimensional necessary stateaction examples thousand ex  state space discretized approximately  states amples required use online  experimented learning policy through function train agents interact crowd performed sev                                                    ijcai                                                       rl   storage requirements             ∼g                test bed user        instruct time dem time          time learn policy             ∼ week          space ship author      na        min   peg   storage requirements             ∼kb                        artist      min        min          time learn policy             ∼ hrs                   game player min        min          time select actions            msec             crowd   author      na         min          total decisionmaking time cross ﬁeld  sec           designer    min        min          total animation time cross ﬁeld  sec                 student     min        min       average storage requirement      ∼m                                           ∼          max storage requirement                  table  summary informal user case study instruct          time select actions            sec     time long new user needed tutored use          total decisionmaking time cross ﬁeld  sec technique intuitive little instruction          total animation time cross ﬁeld  sec                                                        quired userdirected policies quickly constructed                                                        users study prefered aesthetics demon  table  summary average results using reinforcement strated policies traditional policies  learning pegasus spaceship pilot case  study compare table  purpose comparison  help validate selected demonstrationbased thetic value policies veriﬁed through informal  learning problem domain reinforcement learning user case studies table  use natural  takes prohibitive time static adaptive straightforward applicable nontechnical users  tablebased learning functionapproximationbased rl empirical evidence viable tool current  did learn effective policies experiments pega uses evas games animation ad  sus successfully learns useful policy case study dition approach nearly ﬁxed online execution time  requires signiﬁcantly learning time approach useful feature interactive agents  difﬁcult tune learning aesthetics takes improvements approach subject ongo  far cpu time make decisions policy learned ing research include following note demon  through technique requires admissible heuristic strator’s choice actions make character traverse  computational standpoint technique regions state space leaving gaps μ problem  interesting alternative traditional schemes agent automatically solved during training periodically  action selection                                     forcing character unvisited regions state                                                        space important issue perceptual aliasing                                                        result unanticipated character behavior                                                        ceptual aliasing minimized through effective design                                                        compact state space                                                          opted learning standard policies task                                                        structure rules nicolescu  allows                                                        work through programmingby          figure  virtual human crowd test bed       demonstration versus explicit programming                                                        scalability policies limited technique  eral brief demonstration iterations randomly placing augmented task learning method literature  static characters scene human trainer guided gain additional scalability planning behavior  learning agent through scene agent learned cost greater programmer involvement good  behave response agents arbi example crowd case study prepost conditions  trary positions                                      van lent laird  discrete contexts    created policies ﬁrst used learned determining switch policies  boulder impact second ﬁrst pol current technique learns deterministic poli  icy preboulder environment discretized cies interesting behaviors learn  grid formulated current orientation char example complex goaldirected behaviors  acter separation three clos broken subtasks technique lim  est characters decision making computed rate Δt ited policies smooth stateaction mappings  twice second boulder hit ground conﬂict elimination important demonstrator  switched policy continuous stateaction havior nondeterministic conﬂicts arise  spaces simply directed characters run straight varying delays demonstrator response time  away boulder modularity policy ﬂicts result unintended behavior temporal aliasing  learning easier                                      dithering behavior conﬂict elimination tech                                                        nique safely removes high frequencies stateaction                                                        pairs helping make μ smooth representative mapping    discussion                                         unlike elimination through clustering friedrich et al  technique number strong points  method does quantize eliminate examples  shown empirically policies learned rapidly aes alternative eliminating conﬂicts consider                                                    ijcai                                                    
