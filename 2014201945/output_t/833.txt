      transfer learning realtime strategy games using hybrid cbrrl        manu sharma michael holmes juan santamaria arya irani charles isbell ashwin ram                                           college computing                                      georgia institute technology                          manus mph jcs arya isbell ashwin ccgatechedu                          abstract                            introduce mechanism achieving transfer learning                                                        real time strategy rts games technical contribu      goal transfer learning use knowl tions      edge acquired set source tasks improve                                                          • novel multilayered architecture named case based      performance related previously unseen                                                            reinforcement learner carl allows capture      target task paper present multi                                                            separate strategic tactical aspects      layered architecture named casebased reinforce                                                            main leverage modular nature transfer      ment learner carl uses novel combina                                                            learning      tion casebased reasoning cbr rein      forcement learning rl achieve transfer   • novel combination casebased reasoning cbr      playing game ai variety       reinforcement learning rl particular use      scenarios madrtstm commercial real time         cbr instancebased state function approximator      strategy game experiments demonstrate       rl rl temporaldifference based revision      carl performs individual tasks       algorithm cbr      exhibits signiﬁcant performance gains      section brieﬂy introduce motivate      allowed transfer knowledge previous  choice rts games application domain section       tasks                                            details hybrid cbrrl approach transfer learning                                                        provide detailed experimental analyses demonstrat                                                        ing validity proposed techniques section  finally    transfer learning                                  related work ending discussion  transferring knowledge previous experiences new recommendations future research  circumstances fundamental human capability general  ai community focused attention generalization  real time strategy games  ability learn example instances task rts games family games involve sev  interpolate extrapolate unseen instances eral players competing real time typically goal  task recently growing kill players game dominate  come known transfer learning transfer learn territories game  ing somewhat like generalization tasks rts games popular created  seeing example instances function task transfer nonai developers nonai community rts  learning agent able show performance gains learn games designed synthetic domains  ing instances different task function exam ai techniques easily exhibit superior behaviors  ple imagine learning play checkers games interesting characterized enormous  allow learn play chess related unseen state spaces large decision spaces asynchronous inter  game faster                                         actions aha et al  rts games require reasoning    particular taxonomy transfer learning pro levels granularity productioneconomic facility  posed darpa  “levels” ranging usually expressed resource management technologi  simplest type transfer level  memorization cal development tactical skills necessary combat  complex form transfer level  differ frontation rts games used reallife applications  ing levels measure complexity transfer problem military training systems  terms speed quality reorganization scale note particular rts game used research  levels emphasize characteristics types madrtstm commercial rts game developed  problems transfer applies rep military simulations figure  shows source task left  resentations semantic content knowledge needed target task right achieving level  transfer “extend  solutions table  ﬁrst ﬁve transfer levels ing” madrts experimental results presented                                                    ijcai                                                     transfer level                                          description    memorization    new problem instances identical previously encountered during training new                      problems solved rapidly learning occurred    parameterization new problem instances identical constraints memorization different parameter val                      ues chosen ensure quantitative differences require qualitatively different solutions    extrapolating   new problem instances identical constraints memorization different parameter                      values cause qualitatively different solutions arise    restructuring   new problem instances involve sets components different conﬁgurations                      previously encountered during training    extending       new problem instances involve greater number components encountered during                      training chosen sets components                          table  transfer learning levels  taxonomy levels      figure  source task left target task right exhibiting transfer learning level  example simpler  actual experiments demonstrates changes scale characterizes “extending” experiments  number territories number troops increased—from three ﬁve respectively source task ﬁve  target task—necessitating qualitatively different set tactical decisions    paper obtained larger maps higher figure  each layer includes three representative  numbers troops buildings territories         modules                                                          • planner makes decisions action selection    approach                                               layer based current state  particularly interested applying transfer • controller reactive module provides percep  learning rts games designed architecture   tionaction interfaces layer  suited purpose particular implemented  multilayered architecture allows learn • learner modiﬁes representation used  strategic tactical levels abstraction section planner situ splitting merging actions updating  architecture approach features actions states  casebased reasoning reinforcement learning        experiments deﬁned three levels                                                        topmost strategic level currently simple based    architecture                              handcoded strategy middle tactical layer uses  casebased reinforcement learner carl realized hybrid casebased reasoner reinforcement learner  multilayered architecture similar spirit popular speciﬁcally layer makes tactical decisions ac  multitiered architectures albus  upper layers rea tion space attack explore retreat conquer nearest  son strategy lower levels concerned territory lowest layer incorporates reactive planner  ﬁner distinctions tactics granularity time scale scripted perform tasks predeﬁned manner speciﬁc  actions generally reduces lower layers actions madrtstm lower layer responds tasks del  upper layer ai treated goals layer immedi egated tactical layer goals  ately each layer conceptually identical shown reactive planner                                                    ijcai                                                           state feature                                  description          avghealth    average health agent’s troops currently alive             sp       agent’s alive troops percentage initial strength                   opponent’s troops killed percentage initial strength             tp       territories owned agent percentage maximum available scenario                   territories owned opponent percentage maximum available scenario    table  features representing state game tactical layer carl instance time territory  owned agent opponent playing sides side owns territory sends constant number  troops territory leaves smaller ﬁxed number troops patrolling                                                          rithms use experience learn value functions                                                        values map stateaction pairs maximal expected                                                        sum reward achieved starting state                                                        action pair learned value function used choose ac                                                        tions stochastically each state actions higher                                                        value chosen higher probability addition                                                        rl algorithms use form function approximation rep                                                        resentations complex value functions map state                                                        action features values map states distribu                                                        tions actions policy sutton barto                                                         extensive introduction rl                                                          casebased reasoning cbr provides approach                                                        tackling unseen related problems based past experi                                                        ence kolodner  leake  formalized                                                        fourstep process retrieve reuse revise retain                                                        aamodt plaza  view work                                                        uses cbr perform online lazy learning value function                                                        using rl techniques solve temporal credit assignment                                                        case representation                                                        individual cases represent information region                                                        generalized state space provide utilities executing                                                        different actions region state space general                                                        ith case ci represented tuple objects  figure  representative layer transfer learning ar         ci siaiqiei    chitecture carl architecture contains  levels conceptually similar planner learner  controller instantiated different ways depending • state si vector composed features representing  needs level                                  game state experienced                                                          • action set ai list possible actions agent    casebased reinforcement learning                    level architecture                                                          • utilities qi vector utilities each action ai  reinforcement learning rl natural approach                                                            available situation described si  building interactive agents rl problems decision  making agents interacting uncertain environments • eligibility ei real number reﬂecting cumulative  usually modeled markov decision processes mdps  contribution case previous time steps eligibil  mdp framework each time step agent senses  ities used support tdλ updates during revision  state environment chooses executes  ≤ ei ≤   action set actions available state important note agent need  agent’s action uncontrolled external formed particular action particular state corre  events causes stochastic change state environ sponding cases provide suggestions carrying  ment agent receives possibly zero scalar reward actions ∀ci ∈ case library ∀aij ∈ ai ∃qij   environment agent’s goal develop optimal   ai  policy chooses actions maximize expected table  enumerates features used experiments  sum rewards time horizon                ported features chosen represent    rl algorithms developed learning temporal changes behavior different sides  good approximations optimal policy agent’s game note each feature represented numeric                                                                                  experience environment high level algo value case ci ∈                                                     ijcai                                                    retrieval                                                evaluation analysis  features numeric use euclidean                                                        evaluate hybrid cbrrl transfer learning agent  distance similarity metric knnsqci  ∈                                                        madrtstmgame domain evaluation focuses examin  case library  di ≤ τkwheresq queried state di                                                        ing hypothesis agent learning  similarity euclidean distance state knowledge                                                        given task madrts able learn bet  case ci queried state sq τk taskdependent                                                        ter tasks transferring experience  smoothing parameter gaussian kernel function kdi                                                         quantify learning agent  exp−di τk  determines relative contribution individ  ual cases utilities queried state weighted single task compute performance metric end  average utilities retrieved cases      each completion single game particular map                                                        metric computed linear combination ﬁve features                                                       average health remaining agent troops time elapsed    ˆ                              kdi    qtsqaij                                uij     trial enemy troops killed agent’s troops alive terri                                                                           ∀c ∈k  sq                 ∀ci ∈ knnsq    nn                 tories owned agent percentage respective                                                        initial strengths given learning curve representing  reuse                                                        performance agent acting directly target  planner chooses action highest expected util task transfer learning curve ts representing  ity probability  −  encourage exploration                                                      formance agent transferring knowledge  begins large value decreases monotonically func source task use following three metrics deter  tion game trials number decisions quality transfer learning  casebased planner during trials    action passed goal lower layer achieve • jump start  difference mean value ﬁrst  noted earlier lowest layer scripted reactive planner data points trials mean value ﬁrst  commands series primitive actions directly af data points ts  fect madrts game engine example given tactic • asymptotic gain  difference mean value  explore lower planner ﬁnd subset idle troops                                                            data points trials mean value  instruct carry set exploratory actions                                                            ﬁrst data points ts  deﬁned game engine’s instructions                                                          • overall gain  difference mean value  revision                                                            data points trials mean value  revision phase uses rlstyle updates utilities qi data points ts  actions ai chosen agent particular tempo  ral difference λ sutton  update ensures previous fact measure different parameters  errors incorporated future decisions          transfer example possible curves                     ˆ      ˆ                           overall gain initial advantage transfer  qij  ←  αrt  γqt − qtei ∀ci  ∈ case library                                                        jump start vastly different        ˆ        ˆ  qt qt utilities actions chosen  decision cycle agent state representation si  experiments  time instances respectively rt reward ei                                                        section present results transfer levels   eligibility trace α gradientdescent based utility                                                         experiments carried large  ×   update factor constant γ qlearning discount rate  reward achieved each decision cycle linear combi tile maps territories conquer                                                        troops ﬁghting each side exhibit transfer learn  nation temporal changes state features eligibilities                                                        ing level  target scenario consisted increase  represent cumulative contribution individual state  action combination previous time steps factor map components number territories enemy troops                                                        buildings  experimental setup testing  ing  exploration factor ﬁnal action selected                                                        transfer learning level  involves mirroring initial  decision cycle eligibility increased cases                                                        ownership territories moving source scenario  combined form knn helps perform correspond                                                        target scenario difﬁculty  ing utility updates based case’s past contributions                                                       source scenario agent’s troops start initial posi                    kdi               γ            kdh  chosen action    tions adjacent territories large army target task       ei ←       ∀ch∈ knnsq                λγei                           new territories separated signiﬁcant distance coupled                                                        geographical obstructions forces agent                                                        velop new sequences tactics forces smaller  remainder cases library receive scaling battalions troops large army  eligibilities actions factor λγ                                                          trial terminates agent’s troops dead  retention                                             opponent troops dead remaining agent  new cases added case library distance troops conquered maximum number territories  closest neighbor dmin larger threshold param possible given current strength conquering territory  eter τd τd acts smoothing parameter requires troops need reach territory kill oppo  mechanism controlling density cases memory nent troops leave minimum number troops                                                    ijcai                                                                                                                   figure  transfer learning level   figure  transfer learning level  ts  respective performance learning curves obtained  tl metric     tl level   tl level   average ﬁve iterations agent solving target sce                        ∗           ∗  nario source scenario target scenario   jump start                                                                           asymptotic gain    ∗        transfer knowledge source scenario trend                       ∗           ∗  smooth ﬁtting points curve                    overall gain                                                                    table  results transfer learning performance metrics    experiments use value qlearning levels   ∗ indicates results statistically  discount rate constant γ   temporal difference signiﬁcant conﬁdence   stant λ   exploration factor    gradientdescent  based utility update factor α   number nearest  neighbors   task dependent smoothing parameter ﬁdence overall gain transfer learning level                                                           resultant values veriﬁed  tk maintained larger density parameter tden  abling cases contribute value query conﬁdence  exception  point time ensuring large portions asymptotic gain level  suggesting curves ts  domain covered fewer cases analyze converged  agent’s performance averaging ﬁve independent iter results show effective jump start transfer learning  ations each task executed trials level  appears agent learns  worst case running time  seconds trial      exploit attack tactic gave agent valuable                                                        method performing immediately    results                                          creased number enemy troops target scenario fur                                                        ther reusing conquering nearest territory tactic  results shown figure  tl level  fig existing case library worked despite larger number  ure  tl level  curves ts represent territories case tl level  agent beneﬁts  agent’s performance function completed game tri learned use explore tactic proved useful  als learning target scenario learning player’s troops starting geographically separated bat  source scenario learning target scenario af talions acting prior learning agent takes  ter learning source scenario respectively clarity trials discover use exploration tactic  show trend lines smooth ﬁts each curve unite smaller forces  trends trendt show continual increase  formance appearing easier vertical axis  figures   performance agent mea  related work  sured end each trial game just source target increasing transfer learning  task ∗  avghealth  ∗   ∗   sp ∗      recently example given task hierarchy mehta et  tp imeelapsed   table  timeelapsed al  transfer value functions family variable  quantity measured function time scale rts reward markov decision process mdps primary  game completion each task guaranteed objective speedup learning demonstrated dis   time units best agent cretized version rts game rosenstein et al   hardcoded agent random player each trial investigate transfer attempted  performance                                     prior guarantee source target tasks    figures   show ts signiﬁcant jump sufﬁciently similar best knowledge  start performs better overall table  sum existing casebased reasoning approaches address  marizes metrics using signed rank test problem transfer learning                                                    ijcai                                                    
