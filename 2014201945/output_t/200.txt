        discriminative learning bayesian network parameters easy                        hannes wettig peter griinwald teemu roos petri myllymaki henry tirri                  complex systems computation group cosco ° centrum voor wiskunde en informatica cwi                helsinki institute information technology hut po box              university helsinki  helsinki university technology nl gb amsterdam netherlands                      po box  fin hut finland petergrunwald®cwinl                          firstname lastname hiitfi                               abstract                               model likelihood surface known con•                                                                 cave cases parameters logis•       bayesian network models widely used dis•          tic regression model allowed vary freely        criminative prediction tasks classification      words bayesian network model corresponds subset        usually parameters determined using un•        logistic regression model model        supervised methods maximization             main result thm  provides general condi•       joint likelihood reason un•      tion network structure prove        clear parameters maximizing           bayesian network model mapped logistic regres•       conditional supervised likelihood show          sion model freely varying parameters        discriminative learning problem solved         new parametrization conditional loglikelihood        efficiently large class bayesian network         concave function parameters condi•       models including naive bayes nb tree                                                                 tion allowed vary freely convex set rk        augmented naive bayes tan models            global maximum conditional likelihood        showing certain general condition         surface simple local optimization techniques hill        network structure discriminative learn•       climbing        ing problem exactly equivalent logistic regres•                                                                   result leaves open possibility        sion unconstrained convex parameter spaces                                                                  network structures conditional likelihood sur•       hitherto known naive bayes mod•                                                                 face local nonglobal maxima make con•       els logistic regression models con•                                                                 dition superfluous second result thm  shows        cave loglikelihood surface global maximum                                                                  case simple network struc•       easily local optimization methods                                                                  tures satisfy condition con•                                                                 ditional likelihood exhibit local nonglobal maxima    introduction                                                   viewing bayesian network bn models subsets lo•                                                                 gistic regression models new earlier   recent years recognized discriminative papers heckerman meek ng jor•  prediction tasks classification use su•  dan  greiner zhou  concavity   pervised learning algorithm conditional likelihood    loglikelihood surface logistic regression known   maximization friedman et al  ng jordan       main contribution supply condition   kontkanen et al  greiner zhou  neverthe•     bayesian network models correspond logistic regression   bayesian network models parameters cus•      completely freely varying parameters   tomarily determined using ordinary methods maxi•       guarantee local maxima likelihood   mization joint unsupervised likelihood    surface direct consequence result show   main reasons discrepancy difficulty finding time supervised likelihood instance   global maximum conditional likelihood pa•  treeaugmented naive bayes tan model local   show problem solved long   maxima   underlying bayesian network meets particular additional        paper organized follows section  in•  condition satisfied existing bayesian      troduce bayesian networks alternative socalled  network based models including naive bayes nb tan           parametrization section  show allows   treeaugmented nb diagnostic models kontkanen et      consider bayesian network models logistic regression   al                                                     models based earlier results logistic regression     consider domains discretevalued random variables     conclude lparametrization supervised log  maximum conditional likelihood parameters       likelihood concave function section  present   logarithmic reparametrization way each conditional   main results giving conditions   bayesian network model mapped logistic regression      parametrizations correspond exactly conditional       learning                                                                                                              distributions conclusions summarized section     proofs main results given appendix      bayesian networks lmodel   assume reader familiar basics   theory bayesian networks ipearl       consider random vector   each variable xi takes values  ni let   bayesian network structure factorizes px                                                                   parent set variable xt   inb      interested predicting class variable xm   conditioned   loss generality assume     xo class variable children   instance   socalled naive bayes model — chil•  dren class variable ao independent given value   xq bayesian network model corresponding   set distributions satisfying conditional indepen•  dencies encoded usually parametrized vectors       components form defined                                                                 pai configuration set values par•  ents want emphasize   each pai determined complete data vector                   write patx denote configuration   given vector given data vec•  tor  need consider   modified vector replaced   entries remain write   configuration given        let mb set conditional distributions                                corresponding distributions                           satisfying conditional indepen•  dencies encoded conditional distributions   written                                                                   extended tv outcomes indenendence     given complete datamatrix  con•  ditional loglikelihood parameters   given                                                       lmodel viewed logistic regression                                                              lmodels closely related cases                                                                 formally identical bayesian network models                                                           think predictors combine information                                                              attributes using socalled softmax rule heckerman                                                                 meek ng jordan  statistics     note   models extensively studied lo•         standing nodes class variable gistic regression models mclachlan                                                                                                                  learning  precisely let let                                             future data addition discussed heckerman   realvalued random variables multiple logistic re•               meek used perform model selec•  gression model dependent variable xq covariates                 tion competing model structures using    yk defined set conditional distributions           bic approximate mdl criteria heckerman                                                                            meek stated general conditional bayesian                                                                           network models mb difficult deter•                                                                           global maximum gradientbased methods                                                                             used locate local maxima theorem  shows    allowed values                                                                            network structure models equiva•   defines conditional model parameterized                  lent  global maximum   pa                                        set parent                                                                          conditional likelihood reparametrizing mb  configurations xt let                                                model using local optimization method                                                                            question condition  cru•                                                                        cial question address section     indicator random variables obtained                         remark logtransformation continuous    lexicographically ordered renamed lfc                follows calculus      shows each lmodel corresponding bayesian net•                maxima concave conditional likelihood   work structure  formally identical logistic        parameterization global connected maxima    model  dependent variable ao covariates given              original parametrization likelihood sur•    network structures corresponding          face function unpleasant proper•   model ml standard multiple logistic model             ties wettig et al  concave general    input variables logistic model transformations           worse wrinkles mean con•   input variables lmodel transformation            vex subsets constraint   determined network structure                                                 likelihood surface does exhibit local non     turns conditional loglikelihood            global maxima suggests computationally pre  parametrization concave function parameters                 ferrable optimize  empirical evi•  theorem  parameter set convex con•                     dence reported greiner zhou    ditional loglikelihood concave   strictly concave                                                         main result                                                                            setting  follows each distribu•   proof obvious each parameter     values concavity ofis direct con•                                   tion mb ml thm  suggests    sequence fact multiple logistic regression models           doing reverse transformation    exponential families mclachlan                                                                               example showing conditional loglikelihood    strictly concave wettig et                          show distributions                                                                                  contains distributions violate                                                                            sumuptoone constraint    remark nonstrictness proven concavity pose                                                                                                       pa                         technical problem optimization avoided                                                                                 assigning strictly concave prior model parameters               corresponding   maximizing conditional posterior grunwald                parameterization redundant different index   et «  wettig et  instead likelihood            conditional distribution   prune model weakly supported parame•                 case distribution indexed   ters andor add constraints arrive strictly concave con•            turns network struc•  ditional likelihood surface experiments wettig et al              tures corresponding each distribution    suggest small data samples             expressed parameter vector    case order avoid overfitting section                                                                andmi   constraint added course leave parameter                                                                            case  mb — ml main result   space convex set subspace                      case satisfies following condition   corollary  local nonglobal maxima   likelihood surface lmodel                                        condition  exists      conditions global maximum exists                         discussed mclachlan  references   possible solution cases maximum exists               remark condition  implies class   introduce strictly concave prior discusssed                  moral node common child node      global conditional maximum likelihood parameters                  directly connected condition  demands   obtained training data used prediction                figures         learning                                                                                                                                                                                                implies example conditional likelihood sur•                                                                 face tan models local maxima global                                                                  maximum local optimization techniques                                                                    case condition  does                                                                  hold second result theorem  proven appendix    figure  simple bayesian network class variable    says case local maxima    denoted xo satisfying condition  left network   theorem  network    does satisfy condition right                  structure depicted figure right exist data sam•                                                                 ples conditional likelihood local nonglobal                                                                  maxima                                                                    theorem implies condition                                                                   superfluous ask condition                                                                  necessary having                                                                     mb network structures violate condition    figure  treeaugmented naive bayes tan model satis•     plan address intriguing open question future work    fying condition  left network tan right   cases class variable moral      concluding remarks   node network right does satisfy condition                                                                   showed effectively parameters max•  example  consider bayesian networks depicted fig•     imizing conditional supervised likelihood nb tan   ure  leftmost network   satisfies condition    bayesian network models did                                                                 showing network structure models satisfies   rightmost network does theorem shows   conditional likelihood surface local max•          condition  ensures conditional distri•                                                                 butions corresponding models equivalent par•  ima implying case                                                                  ticular multiple logistic regression model unconstrained      examples network structures satisfy condition  parameters arbitrary network structure   naive bayes nb treeaugmented naive bayes        satisfy condition  adding arcs   tan models friedman et  gen•     embed bayesian network model larger model   eralization children class   independence assumptions satisfies condition    variable allowed form treestructures figure       test runs naive bayes case wettig et al                                                                   shown maximizing conditional likelihood   proposition  condition satisfied naive bayes     contrast usual practice maximizing joint unsu•  treeaugmented naive bayes structures                 pervised likelihood feasible yields greatly improved                                                                  classification similar results reported greiner   proof naive bayes                        zhou  conclusions supported theo•               tan models children class vari•   retical analysis ng jordan  small   able parents children    data sets joint likelihood optimization   parent class variable use argument   outperforms conditional likelihood reason apparently be•  nb case child xj parents let xi   ing conditional method inclined  parent class variable xi fitting conjecture cases resorting   child class variable maximizing joint instead conditional likelihood                                                                  preferable use simpler model simplify      condition  automatically satisfied xo  prune restrict model hand choose param•  incoming arcs diagnostic models kontkanen et ai     eters discriminative fashion setting     bayesian network structures con•  model selection using lparametrization   dition does hold add arcs arrive    subject future research   structure condition does hold in•  stance add arc rightmost network               references   figure  mb subset larger   model condition holds ready          friedman et al  friedman gcigcr gold  present main result proof appendix           szmidt bayesian network classifiers machine learning                                                                        theorem  satisfies condition                                                                   greiner zhou  greiner zhou structural ex•    corollary  theorem  shows condi•         tension logistic regression discriminant parameter learning   tion  suffices ensure conditional likelihood sur•   belief net classifiers proceedings eighteenth annual   face local nonglobal maxima proposition           national conference artificial intelligence aaa pages                                                                     edmonton       easy case maximum conditional like• griinwald et al  griinwald kontkanen myllymaki   lihood parameters determined analytically           roos tirri wettig supervised posterior distri                                                                                                                 learning    butions  presented seventh valencia international          show possible vectors corre•     meeting baycsian statistics tenerife spain                      sponding parent configurations matter    hcckcrman meek heckcrman meek em•               chosen holds      bedded bayesian network classifiers technical report msrtr      microsoft research                                                                                                        heckcrman meek hcckerman meek mod•     els selection criteria regression classification      geiger shenoy editors uncertainty arificial intel­     derive  substitute terms      ligence  pages  morgan kaufmann publishers san            definition  clearly  ex•     mateo ca                                                                             actly term form appears sum    kontkanen et al  kontkanen myllymaki tirri       positive sign each ■ exists      classifier learning supervised marginal likelihood           exactly onewith mj  case      breese koller editors proceedings th in­                                                                            term form      ternational conference uncertainty artificial intelligence      uai morgan kaufmann publishers                            appears exactly sum negative sign                                                                              terms   mclachlan  gj mclachlan discriminant analysis              appear positive sign appear neg•     statistical pattern recognition john wiley  sons new york         ative sign follows terms                                                                                   cancel establishes  plugging     ng jordan  ay ng mi jordan discriminative         follows      vs generative classifiers comparison logistic regression                                                                              set pai      naive bayes advances neural information processing sys­     tems    pearl  pearl probabilistic reasoning intelligent sys­     tems networks plausible inference morgan kaufmann pub•           show determine constants      lishers san mateo ca                                          pa sum constraint                                                                            satisfied   lwettig et al wettig grunwald roos myl     lymaki tirri supervised learning bayesian net•     work parameters technical report hiit helsinki in•                                                                               stitute information technology hut  available      httpcoscohiitfiarticleshiit ps                                                                            achieve sequentially determining values   proofs                                                                 particular order                                                                              need additional terminology say ci deter­    proof theorem  introduce notation                 mined configurations pai pai                            let mj maximum number                  determined say undetermined de•                                                       termined configuration  say ci     mj exists condition  note                 ready determined ci undetermined     thementioned condition  lie set                          time mj  determined                                                                 note long ct  arc                            contradiction                                  undetermined exist ci ready de•     condition  implies completely determined                  termined      pairwe introduce functions                             ct undetermined ci ready determined     qj mapping corresponding paj                             case exists      andwe                                                       mj  cj undeter•                                                                           mined   ready determined oth•                                                                                                                                                              erwise repeat argument forward restricted      introduce each con•                               acyclic steps   figuration  constant define                              surely rind ready determined                                                                              algorithm sequentially assigns                                                                            values  satisfied start                                                                  id       undetermined repeat following steps                                                                            exists  undetermined                                                                              parameters constructed way combined                                                                               pick largest ci ready determined   vector clearly member                                                                               set configurations      having introduced notation show mat•    ter choose constants cipa corre•  sponding                                                               learning                                                                                                                                 
