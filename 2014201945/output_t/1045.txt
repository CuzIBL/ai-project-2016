                          efﬁcient robust independencebased                              markov network structure discovery                                 facundo bromberg      dimitris margaritis                                      department science                                           iowa state university                                              ames ia                                                                                                   abstract                                                             paper introduce novel algorithm                                       induction markov network structure                                                domain outcome conditional inde                                                                                            pendence tests data algorithms work                              successively restricting set possible struc figure  example markov network nodes represent variables      tures single structure consistent domain               conditional independence tests executed      existing independencebased algorithms mn data frequently challenging      known shortcomings rigidly ordering  tasks      sequence tests perform resulting poten structure mn encodes graphically set      tial inefﬁciencies number tests required ditional independencies variables domain      committing fully test outcomes result independencies valuable source information      ing lack robustness case unreliable tests number ﬁelds rely qualitative quanti      address problems through bayesian par  tative models social sciences markov networks      ticle ﬁltering approach uses population used physics vision commu      markov network structures maintain poste  nities geman geman  besag et al      rior probability distribution given historically called markov random ﬁelds      outcomes tests performed instead ﬁxed cently use spatial data min      ordering approach greedily selects each  ing applications geography agriculture clima      step optimally informative pool tology ecology shekhar et al       didate tests according information gain ad   motivation related work      dition maintains multiple candidate structures      weighed posterior probability makes  exist broad classes algorithms learning      robust errors test outcomes structure graphical models scorebased heckerman      sult approximate algorithm use  independencebased constraintbased spirtes      particle ﬁltering useful domains et al  scorebased approaches conduct search      independence tests uncertain applica space legal structures size superexponential      tions little data available expensive number variables domain attempt discover      cases large data sets andor dis model structure maximum score independencebased al      tributed data                                   gorithms rely fact graphical model implies                                                        set independencies exist distribution domain                                                        data set provided input algorithm    introduction                                       assumptions work conducting  paper focus task learning structure set statistical conditional independence tests data suc  markov networks mns subclass graphical mod cessively restricting number possible structures consis  els data discrete domains graphical models tent results tests singleton possible  include bayesian networks represented directed graphs inferring structure possible  mns consist parts undirected graph model work present algorithm belongs  structure set parameters example markov net category presents advantages domains  work shown fig  learning models data independence tests uncertain expensive  consists interdependent problems learning struc ﬁrst case occur applications data sets small  ture network given learned structure learning relative number variables domain case  parameters work focus structure learning occurs applications involving large data sets number                                                    ijcai                                                    data points andor domains data heteroge                              neously distributed columns data set                                                                                         tributes located geographically distinct locations                               sensor networks weather modeling prediction traf  ﬁc monitoring settings conducting conditional                                                                                             independence test expensive involving transfers                                      large amounts data possibly slow network figure  generative model domain left correct right  important minimize number tests       assumed    interesting work area structure learning undi  rected graphical models includes learning decomposable proofs correctness common assump  called chordal mns srebro karger  tion independencebased algorithms graphical model  general nondecomposable mns hofmann tresp    discovery assume faithfulness present work   scorebased approach independence described later main algorithm maintain pop  based approach mn structure learning gsimn al ulations structures each time step slightly abusing  gorithm bromberg et al  uses pearl’s infer notation denote populations xt denote se  ence axioms pearl  infer result certain inde quence tests yyt yt sequence value  pendence tests actually performing assignments tests independence dependence cor  gsimn disadvantages potential inefﬁciencies responding true false respectively yt  regard number tests required learn struc  ture relatively rigid predeﬁned order  generative model structures tests  tests performed ii potential instability cas input data set approach uses posterior prob  cading effects errors test results instead present ability structures prx dx ∈xto learn struc  paper takes bayesian approach maintains posterior ture underlying model explained  probability distribution space structures given section probability calculated princi  tests performed far avoid inefﬁciencies previous pled way need generative model involves variables  approaches greedily selecting each step optimally yyyt reﬂects independence  informative tests according information gain decrease straints variables constraint  entropy posterior distribution approach assumption tests sufﬁcient statistics  seen instance active learning tong koller structure information data set   addition approach robust errors value tests far structure concerned  test outcomes making use probability inde stems fact structure faithfully encodes  pendence instead making deﬁnite decisions corresponding independencies domain note assump  probability   way error independence tion particular approach implicit  test does permanently cause correct structure independencebased approach generative model  excluded lowers posterior probability   formalizes makes explicit    rest paper organized follows generative model encodes constraint shown  section present notation followed description fig  left dseparated tests  approach following present experimen yyt posterior structures prx   tal results conclude summary approach computed model accord                                                                                             yt                                                        ing model pr          yt pr            notation preliminaries                         yt dpryt  yt requires computa  markov network domain undirected model tion pryt  yt currently unsolved problem  used represent set conditional indepen assume model shown fig  right  dencies domain application domain set contains multiple data sets ddt abbreviated dt  random variables size  work use cap model shown tests through yt           ab                                      independent given data sets dt allows model  ital letters     denote domain random variables                                      t  bold letters sets variables space solved pryt  yt pryi   structures given denoted space yi di factors pryi  yi  di com  ditional independence citestsbyy conditional indepen puted known procedures discrete version  dence given denoted a⊥⊥b  sthe bayesian test margaritis  practice  set conditional independencies implied structure data set use  mn ones implied vertex sepa data set tests di  dj  model  ration a⊥⊥b  separated mn depicted fig  right used approximation  graph removing nodes edges adjacent overcome lack exact solution described  example fig  ⊥⊥  ifexactly show experiments section approximation  independencies hold actual probability distribu works artiﬁcial real world data sets  tion domain say domain graph modiﬁed model posterior probability  faithful faithfulness excludes certain distri structures computed given data sets dt  butions unlikely happen practice needed prx dt abbreviate prtx cal                                                    ijcai                                                    culations use conditional entropy hx  algorithm  particle filter markov network pfmn algo                                                                                     yt dt given set tests yt data sets rithm  pfmn nmqx                             similarly abbreviated                 ←− sample independent structure particles uniformly    finally parameters model follows distributed edgeset sizes text  prior prx assumed uniform each test yi com  ←−   pletely determined given  pryi  true    loop                                                                ←−                     score   ∈  direct consequence faithfulness  arg maxab arg maxs         assumption                                              yt ←− yt ∪yt                                                            pt ←− prdt  yt   perform test data     approach                                               pf ←− prdt  yt   perform test data                                                             update prtx pt pf using eq   learn mn structure domain data em        ←− pf       x   ploy bayesian approach calculating posterior proba           prt                                                                   htx  ytthen  bility prtx structure ∈xgiven data sets dtthe                                                                return unique structure prtx  problem learning structure framework sum  ←−   marized following steps ﬁnding sequence  tests yt minimum cost hx  yt dt each time during main loop algorithm lines                                  x                                                 ii ﬁnding unique structure pr              yt         ∈y                                                       – test                     optimizes   dt possible assump score function selected score function described  tion faithfulness guarantees existence sin each pair variables ∈v×vthe  gle structure consistent results possible tests space possible conditioning sets exponential equaling  domain                                          power set v−a optimization performed    practice procedure presents considerable dif heuristic search particular ﬁrstchoice hillclimbing  ﬁculties                                     used during procedure neighbors current point    • space structures superexponential   possible additions nonmember possible                                                              removals member possible replacements        exact computation entropy htx                                                         member nonmember score test yt      yt sum ∈x intractable                                                        deﬁned follows    • space candidate  tests exponential           htx   ytyt                      n−                                      scoretyt−                               size  tests a⊥⊥b                                yt      mandm   ranges  − moreover given                                                                              factor   denotes cost whichwetake      number tests tthereexist possible candidate proportional number variables involved      test sequences yt consider                  test factor used discourage expensive tests htx   address ﬁrst issue using particle ﬁltering approach ytyt entropy given performed test  discussed section  each step maintain pop yt equal                                ulation candidate mn structures purpose rep htx  ytythtx  yt − ig tyt   resenting posterior probability distribution structures derivation eq  makes use generative model  given outcomes tests performed far way fig  simple omitted lack space  required quantities posterior probability prtx                                                        term ig tyt denotes information gain candidate  conditional entropy htx  yt estimated sim                                                                                            test given information time  data sets  ple averaging particle population second equal  issue choosing test perform addressed using                                                                         ig       −                yx   yx  greedy approach each step algorithm choose           prt logprt                                                                            x∈x  test perform member minimizes         expected entropy penalized factor proportional yt denote value tests yt structure  cost exponential size minimization each test true false expression  performed through heuristic search approach        follows bayes’ rule generative model im    section explains algorithm  plies given mn structure xanytesty completely                                                        determined using vertex separation assuming faithfulness                                                                                                     pfmn algorithm                               implies prtyt  yt  xδytytwhere    algorithm called pfmn particle filter markov net δa kronecker delta function equals  iff  work structure learner shown algorithm  each equal say structure consistent assign                                                                                                   time step algorithm maintains set xt containing ment yt iff yt  yt quantity prtyt  yt                                                        eq  calculated  structure particles                                                                                                                                                             initially each structure generated randomly                   x∈yx yx  prtx                                                                yx    yx          uniformly selecting number edges         prt                                                                                                                prtx   randomly uniformly selecting edges pick                          ∈yt             ing ﬁrst pairs random permutation possible yt denote equivalence class structures                                                           pairs ensures edgeset sizes equal proba ∈xthat consistent assignment yt using  bility represented                                                    ijcai                                                                                                                                                       notation yt represents set structures imply algorithm  particle ﬁlter algorithm   thesamevaluesforthecitestsyt structure does pf mfq        class yt say equivalent ←−                                                         prt         wrt tests yt denote ∼t         particles ∈xdo    eq  interesting intuitive interpretation ←− prtx                                                                   prt−x           compute weights   each time posterior probability test true        ∈x                                                      particles  false given assignment tests equals ←−   wx                                                                        wx        normalize weights   total posterior probability mass structures equiv      x∈x                                                           resample particles using sampling probabili  alence class yt consistent  true     false                                               ties             normalized posterior probability mass  ←−                                                                  resample    class                                        each particle  times using metropolishastings    compute information gain eq  need distribution prtx  posterior prtxprx dt structure ∈xfor   ←− ∅  eq  explained section  use bayesian  ∈x                                                                       test described margaritis  test calculates  ←− ∪ mhx prtq                                                                     compares likelihoods competing multino  return  mial models different numbers parameters                                         x            ∈true  false                    algorithm    metropolishastings algorithm       pr                      according   −  generative model prdi  ytxprdi  yi                                                         ←−  pryt  xδytyt bayes’ law                                             t                 −                                                    prx dt ∝ prxprdt  xprx   prdi  yi      ∼u                                                                                                                               ∼ qx                                                                                    ˘        ¯                                                             ua   xix        px qx                                                                    min     pxiqxxi  constant proportionality independent            calculated sum structures asex       ←−   plained like equations section                                                                xi ←− xi  volve summations space structures  approx                                                            ←−  imated summation xt population particles                                                               return  previous iteration algorithm    completes description test score function prtx reﬂected “weight” weight used  eq  returning description alg  lines   probability resampling step line  bias selection                                           calculate data likelihoods optimal test twhich replacement particles xtintheﬁnal  used update posterior structures obtain step particles “moved” lines – through pairs         prt  using eq  updated distribution proposalacceptance steps metropolishastings                   new set particles computed line  using par mh algorithm shown alg  andrieu et al   ticle ﬁlter algorithm described section experiments use  algorithm                                                                                                  pfmn algorithm terminates entropy   quires prtx proposal distribution qx                                  estimated population  return provided parameters prtx addressed  ing unique structure particle posterior probability eq  particle ﬁltering applications  equals                                              proposal distribution key factor success    particle ﬁlter structures                    pfmn discussed section  particle ﬁlter andrieu et al  sequential markov  proposal distribution structures  chain montecarlo mcmc method uses set sam                                                        discussed previously use metropolishastings al  ples called particles represent probability distribution                                                        gorithm provide proposal distribution  change each observation sequence ob                                                        use mixture proposal  paqa − paqb consisting  servations each step observation performed                                                        global component qa local component qbweused  new set particles obtained set particles                                                        pa   experiments components qa qb  previous step new set represent new posterior dis follows  tribution given sequence observations far                                                          •                                 x   advantages sequential approach drastic global proposal random walk  gen                                                                          x      reduction cost sampling new distribution erates sample iteratively inverting each                                                                                 α  relative alternative nonsequential sampling approaches edge probability  experiments                                                                α                 case domain particles represent struc use    denotes hamming dis                                                                                    x       n−     tures observations correspond evaluation single tance structures                                                                        ci test data                                            number nodes struc                                                                              x   αm    − α m−h    particle ﬁlter algorithm shown algorithm   tureswehavethat                                                                                                                            used line  pfmn transform population xt xt • local proposal qb equivalenceclass moves qbx                                                                                                        −  change probability distribution prtx chooses remove edge set                                                    ijcai                                                          weighted cost pfmn vs gsimn           accuracy pfmn           number tests gsimn pfmn                                                                                            variables connectivity        variables connectivity   connectivity                                                                         nn                                                                                                                                      number tests gsimn                                                                   number tests pfmn                                                                                                                                                                                           variables                     variables                                                                                                             accuracy                                                                                                                                           number  tests      wcostpf   wcostgs                                                                                                                                                             number structure particles  number structure particles    number variables domain    figure  performance comparison pfmn vs gsimn artiﬁcial domains left ratio weighted cost   τ       middle´ accuracy   τ      right number tests conducted pfmn                    gsimn compared          described add edge set axor recovered network measure accuracy frac      remove edge add e e ∈ a±x tion unseen ci tests correct compare      each chosen probability  result each test resulting structure using vertex sep      sets ax a−x anda±x deﬁned        aration true value test calculated using vertex                                                        separation true model artiﬁcial domains statisti      a−    ∈   x   ve  −e  x ∼                                            cal tests data realworld domains purpose                                                    ∈ e¯x  vex ∪ex ∼t    procedure measure output network        ±                                ∈ exe ∈ ex ∪e       generalizes unseen tests                                                           ex −e ∪e ∼t        artiﬁcial experiments      ex set edges structure e¯x ﬁrst evaluated algorithm artiﬁcial domains      complement set ax a−xisthesetof structure underlying model called true networkis      edges missing existing added known allowed systematic study behavior      removed produce structure varying conditions domain sizes                                         ±                                                        equivalent class yttheseta dependencies reﬂect number edges true      set edge pairs e removed e network ii better evaluation quality output      added resulting structure equivalent networks true model known true networks                                                                                                      τ      result proposal generated randomly selecting ﬁrst        structure guaranteed equivalent pairs random permutation possible edges τ      hamming distance              connectivity parameter                                                          fig  shows results experiments ci tests    local proposal designed maximize accep                                                        conducted directly true network through vertex  tance proposed moves line  mh algorithm                                                        separation outcomes tests correct fig   evaluating true proposing moves independence                                                        left shows ratio weighted cost ci tests  equivalent structures provably posterior                                                       ducted pfmn compared gsimn domain sizes  probability welldesigned proposal qx                                                         number connectiv  ensures convergence posterior distribution prtx                                                        ities τ      increasing num  satisfy requirements aperiodicity irre                                                        ber structure particles weighted cost  ducibility andrieu et al  proposal satis                                                        pfmn lower gsimn small τ    ﬁes requirements aperiodic allows                                                        large τ   connectivities reaching savings  rejection irreducible support entire set                                                         τ  case τ   structures                                                         inconclusive—pfmn does better cases    experiments                                        explained fact number structures                                                        connectivity τ grows exponentially τ approaches   experimentally compare pfmn  gsimn  al   side clearly makes search task  gorithm bromberg et al  artiﬁcial pfmn difﬁcult fig  middle shows  realworld data sets gsimn stateoftheart exact pfmn approximate algorithm accuracy large  independencebased algorithm uses pearl’s axioms ci number particles exactly   tests infer certain tests actually conducting cases  τ  gsimn exact  report weighted number testsandbaccuracy algorithm accuracy  independence  weight each test used account execu tests correct case experiments  tion times tests different conditioning set sizes accuracy omitted ﬁgures fig  right  taken number variables involved each test— shows pfmn does fewer nonweighted test  bromberg et al  details obtain quality gsimn close optimal number tests                                                    ijcai                                                    
