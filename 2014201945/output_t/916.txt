   kernel carpentry online regression using randomly varying coefﬁcient                                                  model                    narayanan edakunni      ∗   stefan schaal   †   sethu vijayakumar     ∗†                 ∗ school informatics university edinburgh edinburgh eh jz uk     † department science university southern california los angeles ca  usa                nuedakunnismsedacuk sschaaluscedu sethuvijayakumaredacuk                        abstract                          model selection guarantees bayesian methods pro                                                        vide retaining ﬂexibility provided nonparametric      present bayesian formulation locally      localised learning      weighted learning lwl using novel concept     attractive characteristics lwprlike lo      randomly varying coefﬁcient model based     calised learning schemes independent learning rules      propose mechanism multivariate                                                        each individual local model combines blends      nonlinear regression using spatially localised lin outputs stage prediction addition avoid      ear models learns completely independent  ing negative interference schaal atkeson      each uses local information adapts                                                        property allows asynchronous learning local models      local model complexity data driven fashion leading improved efﬁciency preserve property      derive online updates model parameters model building generative probabilistic model      based variational bayesian em evaluation                                                        each individual local model derive corresponding learn      proposed algorithm stateof                                                        ing rules show novel formulation performs ro      theart methods reveal excellent robust gener bustly estimating local model complexity competes      alization performance surprisingly efﬁcient                                                        stateoftheart methods generalization capability      time space complexity properties paper                                                        extended learn truly incremental fashion      ﬁrst time brings computational storing data surprisingly efﬁcient computational      efﬁciency adaptability ‘noncompetitive’                                                        complexity space      locally weighted learning schemes mod      elling guarantees bayesian formulation                                                           randomly varying coefﬁcient model    introduction                                       modelling spatially localized linear models using proba                                                        bilistic framework involves deriving formulation allows  locally weighted projection regression lwpr vijayaku                                                        model ﬁt case linear ﬁt bandwidth                 mar et al  prime example recent developments particular location input space each local  area localised learning schemes resulted models combined provide prediction  powerful nonlinear regression algorithms capable operat                                                        novel data additionally order local models  ing realtime high dimensional online learning scenarios                                                        independent each capable modelling  proven work real world appli entire data learning correct bandwidth parti  cations including supervised learning sensorimo                                                        tions data parts – corresponds  tor dynamics multiple degree freedom anthropomorphic                                                        linear region does                                      robotic systems vijayakumar et al              paper accomplish formulating probabilis    locally weighted schemes including lwpr                                                        tic model called randomly varying coefﬁcientrvc model  determine region validity local models                                                        builds idea random coefﬁcient model  adaptive local distance metric data driven fashion longford   usually achieved minimising sort cross vali                                                          locally linear region centered xc generative  dation cost ﬁt using gradient descent methods model data points written  initialization local complexity parameter                                                                                  distance metric forgetting factor learning rates             yi  βi xi                   involved gradient method necessitate careful hand tun                                                                                ing multiple open parameters existing methods xi ≡ xi − xc   represents center sub                                                                                                  trivially achieved real world problems tracted bias augmented input vector βi ≡ βi βi   limited prior domain knowledge exists represents corresponding regression coefﬁcient  ∼  proper probabilistic formulation local weighted learn σ gaussian mean zero noise standard  ing framework – necessary development order exploit viation σ data assumed generated                                                    ijcai                                                                           region locality              higher conﬁdence larger regions data                                                        use gamma regularizer prior bandwidth param                                                        eters favors relatively small values hj leading                                                        localised models                                                                                                                                                                             hj ∼ gammaaj bj                                                                        shall assign noninformative normal prior                                                        μ parameter βˆ noninformative inverse                                                      gamma prior hyperparameters σweas                                                        sume uniform prior regularizer hyperparameters aj                                                        bj fig  summarizes resultant probabilistic model                                                        single local model model marginalize                                                        hidden variables βi obtain                                                                                figure  variation prior location input ˆ                               ˆ                                                         yiβσh hd yiβi xiσ βiβ idβi                                                                     ˆt                                                                     ⇒ yi ∼nβ   xi xi ixi  σ                                                                                                                                                                  interesting note form likelihood eq                                                         corresponds heteroscedastic regression used                                                        later sections prediction section deal                                                        computing parameter updates resultant en                                                        semble posteriors efﬁcient manner                                                          learning                                                        objective learn posterior parameters βˆ                                                        hj σ obtain point estimates hyperparameters –                                                        aj bj joint posterior given           figure  ‘local’ generative model                                      ˆ                                                              ˆ                    β hσa bcdμ                                                          βσy bcdμ                                                                                      bcdμ                                                                                                           iid fashion crucially allow regression coefﬁcient                                                                                            wherewehaveusedh   denote vector hd  random variable prior distribution given                                                                                                    denotes training data yn   ≡ ad                            ˆ                                                                  βi ∼nβ                   ≡  bd  posterior                                                        parameters rendered intractable difﬁculty  assumed each βi generated                        βˆ                              evaluating denominator eq  necessitates  gaussian centered conﬁdence repre use variational bayesian em evaluate posterior  sented covariance ci covariance deﬁned ˆ                                                       βσy bcdμ learn regulariser hyper  proportional distance xi center  effect points lie close center dis parameters                             ˆ  tribution βi peaked β resulting linear region  center illustrated schematically  variational approximation  fig  point center local model learn parameters model maximize  point lies close assign prior fairly tight marginal log likelihood respect parameters treat  mean point lies away ing βi hidden variables marginal log likelihood  prior broader consider various distance given  functions index variation covariance matrix                                                          lnya        μ  restrict diagonal version each diago zp    cd   nal element varying quadratically                                     ˆ                                                            ln    β βn  βσa μ scddβ dβn                                            cij jxi − xc xi − xchj  xi xihj                                                                 βˆ                                                               dσ  hj bandwidth parameter kernel deﬁning        extent locality th dimension                           ˆ                                                          ln        yiβ σp β βhhd  choice kernel parametrization allows use                                                                                                               jugate gamma prior hj higher values hj im       ply lesser variation coefﬁcients βi                 ˆ                                                                           hj aj bj βμ sp σ dβ dβn  larger regions linearity bandwidth modu      lates biasvariance tradeoff unconstrained likelihood                                                                           ˆ  maximization general favor large hj implies dh dhddβdσ                                                                            ijcai                                                                                                                                                                            ˜          −    − −  using jensen’s inequality objective function lower  ci                             bounds given                                                                                                                                                                 ˜        −      −                        ˆ                                μ˜   ci  ν  μ        qβ βn  βσ                                                                                                                                                                                         ˆ                                  ˜j             β βn  βσ μ scd                                                       ln                                 dβ dβn                                                     β    β   βˆ                          ˜                              ˜                          σ                          bj  bj    νij − μ˜ ij   gijj  sjj xi xi         ˆ                                                             dhdβdσ                                                     νij μ˜ ij denote jth element respective                                           ˆ                               ˜   optimal  value   qβ βn  βσ   vectors gijj sjj denotes jth diagonal element  makes  bound  tight given joint poste        β     β   βˆ                                      c˜                               rior       σ     posterior                                                                                                     ˜                      intractable make approximation assum                  yi − νi xi  xi gixi      ing posterior variables indepen                                                 β    β   βˆ    dent can expressed     σ                         ˆ                            need learn point estimates regulariser     qβiy    qhj yqβyqσ  form                                                     hyperparameters aj bj maximum likelihood value  approximation called ensemble variational ap hyperparameters aj bj maximiz  proximation details beal  ing bound fapprox given eq  respect  substituting factorised approximation eq                                                     hyperparameters keeping posterior distributions ﬁxed                                                        considering terms involving hyperparameters   fapprox     ln yiβiσq                             βi  σ                                                                                                                                                ˜                                                                                qhj a˜j  bj lnp hj aj bj dhj                    ˆ            ln βiβh hd                                qβ qh qh                                       βˆ                               ˙           ¸                        maximising   respect hyperparameters equiv              ln             ln βˆμ          alent minimising kl divergence distribu                  hj aj bj                                      hj                                                              βˆ       tions  posterior prior share                                                  parametric form kl divergence minimised           ˙         ¸           ln σ −     ln                   parameters distributions match leads sim                                  β                         σ            qβ                           ﬁ   ﬂ                    ple update rule hyperparameters given             ˙     ¸                                                                                       ˜          −    ln      −  ln       −ln                                ˜j                            ˆ         qσ                                                                       hj       β              σ                                 ˆ                                    β                                     μ   denotes expectation respect dis hyperparameters   initialised                                                      corresponding priors noninformative initialisation  tribution optimal values posterior probabili μ   −×i −     −  ties computed iteratively maximizing functional                      ensures  fapprox respect each individual posterior distribution condition hand regulariser hyperparameters  keeping distributions ﬁxed akin em procedure initialised encourages small ha  procedure shown improve factorised ap value sufﬁciently large value ensures  proximation actual posterior each iteration skipping bias settings used rvc  derivation procedure yields following posterior evaluations carried sec   distributions                  qβiy ∼nνi gi                    prediction using committee local models                   ˆ          ˜                 qβy ∼nμ˜                   dealt far building coherent probabilis                  ∼      ˜ ˜                    tic model each local expert derived inference                hj     gamma  aj  bj             procedures estimate parameters individual model                                    ˜                qσ ∼ invgamma˜c          given ensemble trained local experts order pre                                                        dict response yq new query point xqwetakethe                   ˙  ¸               xt      c −−                normalised product predictive distribution each lo                    σ                          cal expert close spirit paradigm product                                                           c i xix c i           experts hinton  bayesian committee ma                  c −                                                                                                    σ   c i xi              chines tresp   predictive distribution each local                                                      expert given  second derived making use                                             c                  shermanmorrison  woodbury  theorem                    ˆ        ˆ                  ˆ        xt                                           yqy   yqβσhqβyqσ yqhydhdβdσ  diag  hj  qh σ   expectation                                                                                                        spect qσ  furthermore˙ ¸ using results eq      βˆ         ν         c − μ˜                    yq  σ   form given eq              yi σ                                                   ˆ                              “         ”               integrate β eq                    c i xi                                                                                                    − μ˜  μ˜           thesameforσ    approximate qσ                 xt c                      σ                                  qhy delta function mode implies                                                    ijcai                                                             ≈           hy ≈                         algorithm  training local model  σ       δσmode       δhmode  ﬁnal predic  tive distribution kth local model          initialise hyperparameters Θ ≡μ scd                     ˜                        yqk ∼nμ˜ xqk xqk sk  kh xqk  σmode                                      mode                               xqk refers query point kth center sub  input yi  tracted augmented bias blending prediction  repeat                                                                                              ˜  different experts taking product normalising  estimate posterior hyperparametersΘi using Θi  results normal distribution given                                                              eq   eqs                                                                                                                                 αkμ˜ xqk                  estimate values hyperparameters   yq ∼nμ ζ   μ           ζ                                        αk           αk             regulariser prior using eq                                                             convergence posteriors  μ sum means each individual expert            ˜  weighted conﬁdence expressed each expert  Θi  Θi                     prediction αk ζ variance αk precision  end  each expert      xt  s˜            xt       αk     qk     qk σk     diag  qk qkhjk    complexity analysis    online updates                                   time complexity algorithm dominated                                                                                                    iterative learning rules estimate posteriors pa computation eq  equations use  rameters given appropriate prior data represented eq  eq  rewritten avoid ex                                                        plicit computation gi eq  requires diagonal  eqs  rewritten form online                               dates exploiting bayesian formalism batch mode elements computed  posterior evaluation                                                                                                              gij jcij j−cij jxij σ γi using eq                        yn                                ×                                        posteriorn    likelihood prior             γi  xi cixi computed od                                                      fact ci diagonal hand eq                                                                               expressed set online updates quires evaluation xi gixi turn written     posterior  likelihoodi × priori prior  posterior                                                                                                                                              xt   σ γi  transform batch updates                      derived earlier online updates given                                    σ  γi     ˜       −    − −                                                                  si ci   si                               computed  furthermore ma         ˜     −      −                               trix inverses eq  eq  computed    μ˜  sici  ν  si μ                                                                        od fact si ci diagonal matri    a˜ij  aij                                                                                 ces overall time complexity online update    ˜                             ˜                  odm number dimensions    bij  bij  ν ij − μ˜ ij   gijj  sijj xi xi                                                         number local models algorithm doesn’t require     c˜i  ci                                                                                                                              data points stored space com     ˜                                             plexity sufﬁcient statistics stored local models     di  di  yi − νi xi  xi gixi                                                                  independence local models means ef  repeat updates single data point xiyi fective time complexity brought od using  till posteriors converge – Θ˜ represents posterior parallel processors time complexity prediction  Θforthei th point use posterior ith odm including evaluation mean conﬁdence  step prior illustrated algorithm       bounds analysis algorithm                                                        efﬁcient respect time space fact matches  additiondeletion local models                     lwpr’s efﬁciency strong candidate situ  complexity learner adapted addition ations require real time online learning  deletion local models predictive likelihood  new data point sufﬁciently low conclude  complexity learner needs increased adding  evaluation  new local model leads simple heuristic section demonstrate salient aspects rvc  addition local model local model added model looking empirical test results compare  data point predictive probability particular accuracy robustness state art methods  training data ﬁxed threshold data point evaluate performance benchmark datasets  serves center added local model         fig shows local linear ﬁts selected test points    local models sufﬁcient overlap learned rvc noisy training data function  gion model redundant varying spatial complexity functions extremely  pruned overlap local models deter hard learn models high bias tends oversmooth  mined difference conﬁdence expressed nonlinear regions complex models tend  prediction common test point addition deletion ﬁt noise linear ﬁt roughly corre  heuristics used similar ones used sponds tangential line center each local model  schaal atkeson                         expected signiﬁcant result adaptation                                                    ijcai                                                                 data points                                                                true fn                             data points                        data points             linear fits                          true fn       rvc                 true fn        gp                                           learnt fn                      learnt fn                                             confidence bounds                  confidence bounds                                                                                                                                                                                                                                                                                                                                    bandwidth                                      −                               −                                                                                                                                                                                                                                                                  figure  alocal ﬁts bandwidth adaptation fit conﬁdence bounds learned rvc model gp model    local bandwidth section fig plots used rvc batch mode using updates  converged locality measure computed product derived sec cf eqs    subsequent eval  bandwidth parameters each input dimension  illustrat uations section make use online updates derived  ing ability adapt local complexity parameter sec   data driven manner illustration local centers  placed dense uniform grid input space using                                                                  target function compare ﬁts conﬁdence bounds           motorcycle data gp  learned rvc gaussian processes gp williams   fig important note     deliberately avoided using training data   conﬁdence bounds rvc nicely reﬂect                                                                                                                                              −                       motorcycle data rvc                                                               −                  data points                                                                                               learnt fn                                                                                     confidence bounds                                                               −                                                                                                                                                                                        −                                                        figure  fit conﬁdence bounds motorcycle                                                        dataset learned gaussian processes model        −                   data points                               learnt fn                 compare online learning characteristics trained                               confidence bounds        −                                            three candidate algorithms  data points                                                                                 ∼n                                                          sinc function corrupted output noise                                                             each training data presented learner er                                                        ror learning measured using set  uniformly  figure  fit conﬁdence bounds motorcycle distributed test points rvc model allowed  dataset learned rvc model local models cen single em iteration each data point ensure fair com  tered  uniformly distributed points input parison lwpr resulting error dynamics shown                                                        fig comparison gp exhibits sharply decreas    experiment aims illustrate ability rvc ing error curve surprising considering  model heteroscedastic data data varying noise essentially batch method stores away train  levels fig  illustrates ﬁt conﬁdence inter ing data prediction compare rvc lwpr  val learnt motorcycle impact data discussed ras ﬁnd rvc converges faster using roughly sim  mussen gharamani  notice conﬁdence ilar number local models attributed  interval correctly adapts varying noise bayesian learning rules rvc estimates posterior  data compared conﬁdence interval learnt gp parameters point estimates poste  squared exponential kernel shown fig  abil rior product likelihood prior event sparse  ity model nonstationary functions advantage data initial stages online learning prior en  rvc’s localised learning evaluations presented far sures posterior distributions assigned parame                                                    ijcai                                                     
