                         sequence labelling structured domains                         hierarchical recurrent neural networks                    santiago fernandez´    alex graves    jurgen¨ schmidhuber                            idsia galleria   mannolugano switzerland                      tu munich boltzmannstr   garching munich germany                                    santiagoalexjuergenidsiach                        abstract                          performance hierarchy nonetheless                                                        hmms efﬁciently represent information different      modelling data structured domains requires es scales attempt abstract information      tablishing relations patterns multiple hierarchical form nevillmanning witten       scales patterns arise sequential   paper known describes hierar      data multiscale structure contains dy chy crfs presented recently kumar      namic component modelled partic    hebert  classifying objects images hier      ularly case data unseg archy trained globally sequentially estimates      mented probabilistic graphical models pre parameters ﬁrst layer      dominant framework labelling unsegmented se  parameters ﬁxed second layer transi      quential data structured domains use tion matrices estimated      quires certain degree priori knowledge paper uses hierarchical approach extend appli      relations patterns patterns cability ctc sequence labelling structured domains      paper presents hierarchical sys hierarchical ctc hctc consists successive levels      tem based connectionist temporal classi  ctc networks level predicts sequence labels      ﬁcation algorithm labelling unsegmented se  feeds forward level labels lower lev      quential data multiple scales recurrent neu els represent structure data lower scale      ral networks experiments recognition error signal level backpropagated through      sequences spoken digits show lower levels network trained gradient descent      outperforms hidden markov models making    relative weight backpropagated error pre      fewer assumptions domain               diction error level adjusted necessary                                                        depending degree uncertainty target label    introduction                                       sequence level depend variability  assigning sequence labels unsegmented stream data extreme case error  data goal number practical tasks speech level used training network potentially dis  recognition handwriting recognition domains cover structure data intermediate levels results  structure multiple scales captured hierar accurate ﬁnal predictions  chical models assist process sequence labelling section brieﬂy introduces ctc algorithm    probabilistic graphical models hidden markov section  describes architecture hctc section  com  models rabiner  hmms conditional random   pares performance hierarchical hmms hctc  ﬁelds lafferty et al  crfs predominant speech recognition task section  offers discussion sev  framework sequence labelling recently novel algo eral aspects algorithm guidelines future work  rithm called connectionist temporal classiﬁcation graves et final conclusions given section   al  ctc developed label unsegmented  sequential data recurrent neural networks rnns  connectionist temporal classiﬁcation  like crfs contrast hmms ctc discrimi ctc algorithm labelling unsegmented sequential  nant algorithm contrast crfs hmms ctc data rnns graves et al  basic idea  general algorithm sequence labelling sense ctc interpret network outputs prob  ctc does require explicit assumptions statisti ability distribution possible label sequences condi  cal properties data explicit models patterns tioned input data sequence given distribution  relations                         objective function derived directly maximises    hierarchical architectures used hmms probabilities correct labellings objec  exist efﬁcient algorithms estimating parameters tive function differentiable network trained  hierarchies global way way maximises standard backpropagation through time werbos                                                     ijcai                                                     algorithm requires network outputs different  training  times conditionally independent given internal state objective function ctc derived principle  network requirement met long maximum likelihood attempts maximise  feedback connections output layer log probability correctly labelling entire training set  network                                              let training set consisting pairs input    ctc network softmax output layer bridle  target sequences length sequence  unit number labels required equal length input sequence  task activation extra unit interpreted prob express objective function minimised  ability observing “blank” label given time step                           activations units interpreted prob  omls−           ln pzx          abilities observing corresponding labels blank                    xz∈s  unit allows label appear consec  utively output label sequence trained ctc network network trained gradient descent dif  produces typically series spikes separated periods ferentiating equation  respect network outputs  blanks location spikes usually corresponds achieved calculating conditional probabil                                                                 position patterns input data algo ities pz dynamic programming algorithm similar  rithm guaranteed ﬁnd precise alignment   forwardbackward recursions used hmms graves                                                        et al     classiﬁcation                                      deﬁne  αts probability having passed  input sequence length require label se through sequence ls symbol time  quence ∈ l≤t  l≤t set sequences length                                                              αtsp  πt  bπtlsπt  sx  ≤  alphabet labels begin choosing                                                                                t  label blank timestep according probabil                                                                                                          yt                   ity given network outputs deﬁnes probability                        πt                                                                                                        t                                                π        distribution set length sequences           bπtls  extended alphabet l  ∪blank labels blank                                                        βts probability passing through rest  included disambiguate elements lt label                                                        label sequence lsl given symbol reached  sequences refer paths                 time    conditional probability particular path π ∈ lt                                                           β sp  π     bπ      lπ   given                                                             tt                                                                                                        t                                                                                    ∀  ∈  t                                          yt                                  pπ     yπt  π                                            πt                                                                                                                                               π                                                                      bπtt lsl         yk activation output unit time    paths mapped label sequences ﬁrst remov allow blanks output paths label se                                                                    ≤t                                      ing repeated labels removing blanks ex quence ∈ consider modiﬁed label sequence                                                         blanks added beginning end inserted  ample path aab path aaabb                               map labelling aab conditional probabil pair labels length                         ≤                                     ity given labelling ∈ sum probabilities  calculating αts βts allow  paths corresponding                 transitions blank nonblank labels                                                       pair distinct nonblank labels se                 plx        pπx               quences start blank ﬁrst symbol                        π∈b−l                        end blank symbol            t     ≤                                      equations     ﬁnd    →  manytoone map implied  process                                                           l    finally output classiﬁer hx proba       plx    αtsβts             ble labelling input sequence                                                        hx  arg max plx                                                                 noting label blank repeated                                                        times single labelling deﬁne set positions  general ﬁnding maximum intractable                                                                                      label occurs lablks    effective approximate methods graves et al                                                                                       differentiate equation  respect  used paper assumes probable                                                        network outputs  path correspond probable labelling                             ∗                                                                     hx ≈bπ                                   ∂pl                                                                                         αtsβts                            ∗                                            ∂yt     yt               π  argmaxpπ                                      s∈lablk                              π  π∗ just concatenation active outputs setting  differentiating objective function  timestep                                      obtain error signal received network during                                                    ijcai                                                                           seven                                                 seven          level                                                level                                                                                                   se ven                                         se ven        level                                                 level           level                                       level                                              unsegmented speech data                                unsegmented speech data    figure  schematic representation hctc network la figure  schematic representation error signal ﬂow  belling unsegmented input data stream outputs each hctc network network trained globally gra  level correspond probability having detected par dient descent external error signals injected intermediate  ticular label emitting blank label particular time levels considered optional applied contribution                                                        total error signal adjusted  training                                                        pair xz xxxt  external       ml                              ∂o    nw                               sequence zz  set target                         yt −           α sβ                                                                                     sequences ≤t ∀i            ∂uk               zt                                                                         s∈labzk               trained globally gradient descent fig                                                    ure  illustrates error signal ﬂow hctc network                   unnormalised    normalised  uk yk                      error signal target sequence  received  puts softmax layer respectively                                                                                                        network gi given equation  simplify nota                       l                            tion deﬁne                        α sβ                                     ∂omlv                                                                    target                                                                                     Δi                                                                                                 ∂uik  normalization factor                                                        input sequence vi external input sequence                                                        network output network gi− levels    hierarchical connectionist temporal                                                                                            target     classiﬁcation                                        error signal Δi   backpropagated net                                                        work gi lower levels  ji  hierarchy ctc networks series gggi  tribution term error signal unnormalised  ctc networks network receiving input ex                                                                                               activation uik kth output unit network gi    ternal signal networks gi receiving input                                    output network gi− architecture illustrated fig                                                                                 backprop                                                                                    −           ure                                                   Δi         yk     δm  wmk        wmk  yk                                                                                                note network gi hierarchy softmax           m∈m             ∈k  output layer forces hierarchical make                                                        set units level connected  decisions level use upper levels                                                        set softmax output units kinleveli δ  achieve higher degree abstraction analysis                                                                                              error signal backpropagated unit ∈  structure data facilitated having output probabili                                 mk                                                        weight associated connection units ∈  ties level                                        ∈                                                                 yk output activations softmax    modular design ﬂexibility layer level  allows using architectures feeding external                                                          finally total error signal Δi received network gi  inputs levels hierarchy long mathe sum contributions equations    matical requirements ctc algorithm met sec general contributions weighted depending  tion                                               problem hand important example    training                                         target sequences intermediate levels uncertain                                                        known                                                                    general hctc requires target sequences level       target  hierarchy given particular input sequence train    Δi                                                                                Δi        target   backprop                ing set each example training set consists         λiΔi     Δi                                                            ijcai                                                                   digit   phonemes                        delta acceleration coefﬁcients added giving vec                zero    ziirow                       tor  coefﬁcients total network coefﬁcients                    waxn                          normalised mean zero standard deviation                                               training set                three   thrii                    ow                         setup                    ay                        hmm     implemented  htk                    siks                         toolkit young et al  threestates lefttoright models                seven    ehven                  ey                           used each nineteen phonetic categories                    ay                       silence model “short pause” model allowed                oh      ow                              end digit estimated observation probabili                                                        ties modelled mixture gaussians grammar  table  phonetic labels used model digits exper model allowed sequence preceded followed si  iments                                               lence digits linguistic information                                                        probabilities partial phone sequences included        ≤     ≤                                             λi    extreme case λi     number gaussians insertion penalty op  target sequence provided network gi free                                           backprop     timised performance validation set selected  make decision minimises error Δi  ing training set number gaussians increased  ceived levels higher hierarchy train steps performance validation set stabilised  ing globally network potentially discover case decreased slightly  gaussians  structure data intermediate levels results ac time number gaussians increased parameter  curate predictions higher levels hierarchy estimation algorithm herest applied twice results                                                        collected validation set insertion penalties    experiments                                        varying   steps  best performance                                                        validation set obtained  gaussians  order validate hctc algorithm chose speech insertion penalty  total number parameters  recognition task problem known contain hmm   structure multiple scales hmms remain stateoftheart level hctc used second level  speech recognition hctc performance target sequence digits corresponding input  compared hmms                           stream ﬁrst level used target sequence    task ﬁnd sequence digits spoken phonemes corresponding target sequence digits  standard set utterances using intermediate level level each ctc network uses bidirectional  sequence phonemes associated word        lstm  recurrent neural network graves schmidhuber                                                         graves et al  primarily phoneme    materials                                        recognition task better results rnns  speakerindependent connecteddigit database tidig reported graves et al ctc formalism  leonard doddington  consists best realised bidirectional recurrent neural net   thousand digit sequences spoken  men women work schuster paliwal  network  children database recorded particular time depends past future events  dialectically balanced utterances distributed input sequence  test training set randomly selected ﬁve percent each level input layer fully connected  training set use validation set left  hidden layer hidden layer fully connected  utterances training set  validation  output layer ﬁrst level input layer  test set                                      size  forward backward layers  blocks    following digits present database each output layer size   phonetic categories  “zero” “one” “two” “three”  “nine” “oh” utter plus blank second level input layer size   ances consist sequence seven digits repre forward backward layers  blocks each  sentation digits phonetic level used exper output layer size  digits plus blank la  iments seen table  nineteen phonetic categories bel input output cell activation functions  used common digits hyperbolic tangent gates used logistic sigmoid func    samples digitized  khz quantization tion range   total number weights  range  bits acoustic signal transformed hctc network   mel frequency cepstral coefﬁcients mfcc htk  training hctc network gradient  toolkit young et al  spectral analysis carried scent weight updates training example   channel mel ﬁlter bank  hz  khz cases learning rate − momentum   preemphasis coefﬁcient  used correct spec weights initialized randomly range −   tral tilt mfcc plus th order coefﬁcient during training gaussian noise standard devia  computed hamming windows  ms long  ms tion  added inputs improve generalisation                                                    ijcai                                                                         ler                        partially trained network explore alternatives max              hmm                                  imise performance level hierarchy              hctc λ     ±                  future like explore ways im              hctc λ                          proving scalability example large vocabulary                                                        speech recognition requires systems work  table  label error rate ler tidigits results thousands words using output layers size  hctc λ  means  runs different random hctc currently practical instead assigning unique  weights ± standard error gives   conﬁdence output unit possible label methods ex  terval   tvalue   degrees plored assigning labels speciﬁc activation patterns  freedom twotailed test hctc λ  best group output units require modifying ctc’s  result obtained shown coincides best result training algorithm  obtained λ                                     aspect like investigate poten                                                        tial hctc word spotting tasks discriminant                                                        method hctc improve detection rates capa    performance measured normalised edit distance                                                        bility discriminating keywords nonspeech  label error rate ler target label sequence                                                        speech events hctc provides estimates  output label sequence given                                                        posteriori probabilities help directly assess    results                                          level conﬁdence predictions contrast                                                        generative approaches hmms use unnor  performance rates systems tested seen ta malized likelihoods  ble  best hmmbased achieved error rate   continuous digit recognition label error rate  hctc average     conﬁdence interval  conclusion    improvement   paper presented hctc algorithm uses  respect hmm approximately half hierarchy recurrent neural networks label sequences  number parameters                                 unsegmented data structured domains hctc capable    best results injecting error signal associ ﬁnding structure multiple levels using achieve  ated phoneme labels λ    accurate predictions hierarchy use  good best performance achieved λ neural networks offers ﬂexible way modelling domain  case pattern activations seven output time allowing trained glob  units instead encode dynamic ally experimental results demonstrated approach  structure data lower level order make ac outperformed hidden markov models speech recognition  curate predictions level sequence digits task  utterance figure  seven output  units activeinactive speciﬁc points time each acknowledgments  digit pattern active different digits  nonetheless associated directly phonetic research funded snf grant   categories encode type information  signal                                           references                                                        bridle  john bridle neurocomputing algorithms    discussion future work                            architectures applications chapter probabilistic  inserting levels hierarchy corresponding tar terpretation feedforward classiﬁcation network outputs  label sequence interesting target labels pages – springerverlag   known instance phonetic labels used sus graves schmidhuber  alex graves jurgen¨  pected invalid presence dialectal vari schmidhuber framewise phoneme classiﬁcation  ations dataset sources variability data bidirectional lstm neural network architec  speciﬁed accurately constraints tures neural networks –– junejuly   capable making effective decisions                                                                           experimentally means graves et al  alex graves santiago fernandez´  difﬁcult train particular goal hctc jurgen¨ schmidhuber bidirectional lstm networks  target label sequences phonetic level achieved similar improved phoneme classiﬁcation recognition pro  performance albeit different means hctc ceedings  international conference artiﬁ                                                           cial neural networks warsaw poland   λ suffered larger extent problem  local minima                                         graves et al  alex graves santiago fernandez´    assuming reasonable target label sequences faustino gomez jurgen¨ schmidhuber connectionist  speciﬁed priori intermediate levels possible solution temporal classiﬁcation labelling unsegmented sequence  train hctc error decreased signiﬁcantly data recurrent neural networks proceedings  remove contribution error signal rd international conference machine learning  target label sequences intermediate levels free pittsburgh pa usa                                                     ijcai                                                     
