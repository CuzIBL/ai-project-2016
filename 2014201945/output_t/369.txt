               word    sense   disambiguation         distribution    estimation                                       yee seng  chan   hwee  tou  ng                                      department   science                                      national university singapore                                     science drive  singapore                                      chanys  nghtcompnusedusg                        abstract                            work researchers showed different                                                        sense distributions domains important effect       word  sense disambiguation wsd        wsd  accuracy estimation target      trained domain applied different  main’s sense distribution crucial order build      domain show decrease performance   wsd  systems portable different domains      major reason different sense distributions partial solution determine predomi      different domains paper presents    nant frequently occurring sense each word      novel application distribution estimation al corpus research shown baseline heuristic se      gorithms provide estimates sense distribu lecting frequently occurring sense word gives      tion new domain data set  high wsd accuracy skewed distribu      training examples automatically gathered tion word senses mccarthy et al addressed      parallel corpora sense distributions estimated presented method automatically rank word      good achieve relative improvement senses corpus order determine predominant       incorporated wsd     sense each word obtained good results                                                        method applied senseval english allwords    introduction                                       task palmer et al  method used                                                                                       words multiple meanings process iden lated work mccarthy et al identify infrequently  tifying sense word particular context known occurring word senses  word sense disambiguation wsd wsd important machine learning literature algorithms estimate  task natural language processing nlp important class priori probabilities developed pa  applications machine translation information present novel application distribution estima  retrieval past research shown good approach tion algorithms provide estimates sense distribution  wsd  use supervised machine learning priori probabilities senses show ef  approach large text corpus each ambiguous word fective improving wsd accuracy systems trained  annotated correct sense collected applied different domains note determining  serve training data                            predominant sense special case trying estimate    corpusbased approach raises problem complete sense distribution best knowledge  domain dependence faced supervised wsd sys algorithms used nlp wsd  tems trained data domain exam recognize wsd scalable  ple sports show decrease performance ap problem faced current supervised learning systems  plied different domain example economics usually relied manually annotated data train  issue domain difference affecting wsd performance ing tackle problem prior workng et al   brought various researchers escudero et al exploited parallel texts wsd encouraging results   showed training wsd domain obtained evaluation nouns senseval en  applying lead drop  glish lexical sample task kilgarriff  used   wsd  accuracy shown rea wordnet  sense inventory miller  note  sons different distributions senses domains usage parallel texts training data represents natural   improvement accuracy observed domain difference test data senseval english  sensebalanced instances different domains lexical sample task  drawn british  cently agirre martinez  conducted experiments national corpus bnc paper chose draw train  training data wsd automatically gath ing data experiments parallel texts similarly  ered internet reported potential improve base evaluation nouns senseval english  ment  accuracy access sense dis lexical sample task  tribution test data                           paper structured follows section            parallel corpora                        size english texts size chinese texts                                                    million words mb million characters mb              hong kong hansards                                                   hong kong news                                                        hong kong laws                                                          sinorama                                                                xinhua news                                                             english translation chinese treebank                                  total                                                                                       table  size englishchinese parallel corpora    discuss process gathering training data par  distribution estimation  allel texts section  distribution es estimate sense distribution priori probabilities  timation algorithms used brief description different senses new data set use  predominant sense method presented mccarthy fusion matrix algorithm vucetic obradovic   et al section  evaluate effectiveness em based algorithm saerens et al   algorithms predominant sense method scribe section brief description predominant  improving wsd accuracy evaluation results nouns sense method mccarthy et al given  senseval  english lexical sample task presented discuss make use estimated priori proba  discussions section  conclude sec bilities improve classiﬁcation accuracy  tion                                                           confusion  matrix    training  data   parallel texts               let assume set labeled data sl classiﬁer                                                        used compute conditional probabilities plωj ωi   section brieﬂy parallel texts used estimate probability classifying  experiments process gathering training data stance class ω  fact belongs class ω                                                                                                                                             apply classiﬁer known conditional prob                                                        abilities plωj ωi set unlabeled data su obtain    parallel text alignment                          predictions predictions estimate                                                        probability predicting class ω   table  lists  englishchinese parallel corpora available                     linguistic data consortium used experiments note qωj   priori probabilities pωi su  sentences  corpora prealigned ei estimated solving following equation  ther manually automatically prepared af                         ter ensuring corpora sentence aligned tokenized qωj   plωj ωipωi                                                                               english texts perform word segmentation chi        nese texts word alignment parallel bn represents bthe numberb classes equation   corpora using giza software och ney  represented matrix form  ·                                                        priori probabilities su  represented  estimated                                                        solving    target translations selection                                              −                                                                               ·                  took approach described previous work                                                        obtain estimates bootstrap sampling efron  ng et al  select possible chinese translations                                                        tibshirani  employed ﬁrst  each sense english word instance wordnet                                                        basic idea bootstrap giving bootstrap algorithm   lists  senses noun nature senses lumped                                                          bootstrap sampling given original sample  translated way chinese                    ∗                                                        examples bootstrap sample obtained randomly sam  example sense   nature lumped                                                        pling examples replacement estimate  translated “ll” chinese                                                        conditional probabilities plωj ωi need split sl  word alignment output giza select                                ∗                                                        strain stest let deﬁne ωj  ωi  occurrences noun nature aligned                                test∗                                                        number examples bootstrap sample stest predicted  chinese translations chosen english side                                       ∗                                                        class ωj true class ωi deﬁne ωj   occurrences serve training data noun                                ∗                                                           number examples bootstrap sample predicted  nature considered disambiguated                                                                                             class ω  bootstrap algorithm  “sensetagged” appropriate chinese translations                                                                given stest su  classiﬁer repeat following    time needed perform manual target translations iterations experiments set   word senses relatively short compared effort needed  manually sense annotate training examples step • generate bootstrap sample ntest examples                                                               calculate  potentially automated use english test         ∗                                                             ∗           ntestωj ωi  chinese dictionary                                        ωjωi     ∗                                                                                      pj ntestωj ωi                                                               • generate bootstrap sample nu examples su  represents expectation estep  represents        calculate∗                                   maximization mstep represents number       ∗        ω      ω                       stances  note  probabilities ω               nu                                                                                                                                        plωi stay through iteration steps                        ∗                           each particular instance xk class ωi  observe    • useb  calculate ωi                                                                                                         new posteriori probabilities psω  step   iterations estimate priori probabilities                                                                                            simply posteriori probabilities conditions               ∗   bb  pωi    ωi                                labeled data plωixk weighted ratio new                                                                                                                                            priors ωi old priors plωi denominator  em  based balgorithm                              simply normalizingb factor                                                                                                   let assume train classiﬁer set eachb iteration step bothbthe posteriori ωixk                                                                                labeled data sl classes suppose priori probabilities ωi reestimated sequen  set independent instances independent realizations                                                        tially each new instance xk each class ωbi  stochastic variable       xn  new data                                                                                convergence estimatedb probabilities ωi  set likelihood instances deﬁned erative procedure increase likelihood  each                                                      step                                                                                                 lx     xn   pxk                                                                                                          predominant  sense                                  pxk ωi                                                                                                                   method automatically ranking wordnet senses deter                                  pxkωipωi     predominant frequent sense noun                                              bnc corpus presented mccarthy et al                               assume generation instances thesaurus ﬁrst acquired parsed  million words  classes withinclass densities pxkωi written english bnc corpus provide nearest  probabilities observing xk given class ωi neighbors each target noun distributional  change training set sl new data set similarity score dss target noun neigh                                                                                                           deﬁne pxkωi  plxkωi  determine priori bor wordnet similarity package pedersen et al   probability estimates pωi new data set max used wordnet similarity measure wnss  imize likelihood  respect pωi  ap used weight contribution each neighbor  ply iterative procedure em algorithm effect makes various senses target noun through                     through maximizing likelihood  obtain pri combination dss wnss prevalence score each  ori probability estimates byproduct            sense target noun calculated pre    show steps em algorithm dominant sense noun bnc determined  helpful deﬁne notations apply classi focus method determine pre  ﬁer trained sl instance xk drawn new data dominant sense obtain estimated sense distribu  set su  plωixk  deﬁne probabil tion normalizing prevalence score each sense  ity instance xk classiﬁed class ωi classiﬁer noted section   test examples senseval  trained  let deﬁne ω  priori                                                 english lexical sample task drawn bnc  probabilities class ωi sl  estimated reasonable expect sense distribution estimated                                            class frequency ωi sl willb deﬁne ωi predominant sense method applicable test ex     ωixk estimates new priori posteriori amples senseval english lexical sample task im  probabilities step iterative em procedureb assum plemented method order evaluate effectiveness                   ingb initialize ωi priori probabilities improving wsd accuracy implementation achieved ac  classes labeled data                          curacies close reported mccarthy et al                                                        mccarthy et al reported jcn measure predominant                 pω   ω                                                   sense accuracy  semcor corpus mea  each new instance xk su  each class ωi sured  using implementation senseval  em algorithm providesb followingb iterative steps  english allwords task  precision  recall                                                      reported implementation gave  precision                                    ωi                          plωixk                               plωi                recall differences minor                                             ωixk                                                             ωj            processing steps described mc                            plωj xk                                  plωj                                                                                  carthy et al  finally note predominant                                                    sense method effective large text corpus                                     psω      psω            thesaurus effectively generated                                                                                      large corpus                                                               maximum            em sample    true −       em  −       confm −    predominant −          number par          allel examples          noun                                          adjust sample adjust sample  adjust sample  adjust sample                                                                                                                                                                                                                                                       table   trials microaveraged scores  se nouns               maximum  number parallel examples noun em −    confm −    predominant −                                                    adjust sample  adjust sample  adjust sample                                                                                                                                                                                                                                                                        table  relative improvement percentage using priori probabilities estimated  methods      improving  classiﬁcation based priori         saerens et al  reported increasing training       estimates                                        set size results improvement priori estimation  classiﬁer trained estimate posterior class probabil investigate issue gradually increased maxi                                                        mum  number examples noun selected parallel  ities plωixk presented new instance xk   directly adjusted according estimated class texts each condition sampled training examples                                                      parallel text using natural distribution parallel  probabilities pω                                                    text maximum allowed number examples    denoting predictions classiﬁer ω  priori                                                  noun noted wsd accuracy achieved classiﬁer  probability class ω ω  estimated priori                                              adjustment column labeled table   probability class ω pω  adjusted predictions                                                provide basis comparison adjusted classiﬁer pos      ω  calculated   adjust                                         teriori probability output true sense distribution                                   pωi              test data using equation  increase wsd accuracy                            plωixk                                   plωi          padjustωixk                          obtained listed column adjust true−l                                       pωj                              plωj xk               example knew exactly true sense distribution                                    plωj                                                     test data achieved accuracy     note thebsimilarity  withp                    trained maximum  ex                                                                                    amples noun    evaluation                                           adjusting classiﬁer posteriori probability  experiments implemented supervised wsd ap resample training examples par  proach lee ng  reported achieve allel texts according true sense distribution  stateoftheart accuracy knowledge sources used crease wsd accuracy obtained using approach listed  clude partsofspeech surrounding words local colloca column sample true−l note scores listed  tions use naive bayes algorithm classiﬁer true−l indicate upperbound accuracy achiev  reported achieve good wsd accuracy able known true sense distribution test data  fast train test naive bayes classiﬁer increase wsd accuracy adjusting classiﬁer  naive bayes classiﬁer outputs posteriori probabilities using sense distribution estimated em algo  classiﬁcations used directly adjust rithm listed column adjust em−l correspond  predictions based                              ing increase resampling training examples par                                                        allel texts according sense distribution estimated    results                                          em algorithm listed column sample em−l  used approach outlined section  obtain training similar increases using sense distribution estimated  examples parallel texts senses noun confusion matrix algorithm predominant sense  lumped translated chi method listed columns adjust sample  nese word  nouns senseval der confm−l predominant−l corresponding rel  english lexical sample task senses  nouns lumped ative improvement various accuracy improvements  sense senses each  nouns comparing using true sense distribution achieved  translated chinese word limit evalu algorithms predominant sense method  ation remaining  nouns                      given table  example  parallel examples                                               performance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                wsd  accuracy                                                                                                                                                                                                                                                                                                                       figure  comparison parallelem’s performance senseval participating systems single shaded bar represents  parallelem white bars represent supervised systems pattern ﬁlled bars represent unsupervised systems    noun em adjust gives improvement  column obtained adding ﬁgures columns  improvement  using true adjust relative em−l sample  improvement       results show confusion matrix em algo  comparison se systems  rithms effective improving wsd accuracy compare wsd   accuracy achieved  algorithms effective large training data senseval english lexical sample task participating sys  set  examples noun experiments tems performed sense lumping publicly available  used results indicate potential parallel text predictions senseval participating systems  scaling achieve better wsd accuracy training wsd accuracy each calculated set  examples used shown increasing accuracy ﬁg  nouns senseval english lexical sample  ures column table  phenomenon task  supervised  unsupervised systems  explored ng et al  used number figure  shows relative ranking  parallel text examples ofﬁcial senseval training named parallelem various se systems  data time caused progressive note achieved better performance  duction absolute possible wsd accuracy im unsupervised systems ranked  provement indicated reduction columns  percentile list  including super  adjust sample true−l table  parallel vised systems  texts examples used experiments indicate  application algorithms achieved higher increases  discussion  wsd  accuracy compared predominant  sense method primary aim determining score  achieved parallelem competi  predominant sense word corpus              tive considering se supervised systems trained    note resampling training examples gives bet manually annotated ofﬁcial data  ter wsd improvements merely adjusting posteriori trained examples automatically extracted par  probabilities output classiﬁer number training allel texts following factors  examples sense affect accuracy classiﬁer considered  trained sense reliable estimate ofﬁcial training data similar sense distribution  sense distribution beneﬁcial test data  sample training examples results show sampling                                                        advantage participating senseval su  maximum  examples noun parallel texts                                                        pervised systems enjoy note results table  show  according distribution estimated em algorithm                                                        sample according true distribution  gives best achievable wsd accuracy                                                          maximum   examples noun achieve    column em sample table  show                                                        score   wsd  accuracy resulting sampling parallel text exam  ples according em estimated sense distribution  systems iit  resubmissions sys  different maximum number examples noun tem answers available 
