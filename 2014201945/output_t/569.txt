 combining structural descriptions imagebased representations image                                  object scene recognition∗                            nicolas huu† williams paquier raja chatila                                               laascnrs                        avenue du colonel roche  toulouse cedex   france                    nicolasdohuulaasfr williampaquierlaasfr rajachatilalaasfr                        abstract                          object represented collection volume parts                                                        need multiple viewtuned representations      object scene learning recognition ma                                                        models virtually rotated compared      jor issue vision robotics cog                                                        image use model make      nitive sciences paper presents principles                                                        recognition invariant illumination color      results approach extracts struc      tured viewbased representations multipurpose  identify certain properties modeling process      recognition structures hierarchical dis efﬁcient learning recognition firstly training      tributed provide generalization catego construction new representations sufﬁ      rization tracking process enables bind views ciently fast lack speed principal cutoff models      time link consecutive views scenes based structural descriptions building models      recognized using objects components il images remains non trivial task secondly efﬁcient      lustrative results presented                 modeling process organize knowledge structured                                                        way ﬁrst means organizing data build categor                                                        ical representations categorical structures make possible    introduction                                       obtain capacities class generalization low cost  object scene learning recognition major issue deed effectively exploit extracted data  vision robotics neuroscience cogni describable various levels speciﬁcity bottle  tive sciences main questions described container plastic object recycling  respect extract knowledge patterns light structure accelerate recognition deals  camera retina                          large collections stored objects reducing num    recognition objects models proposed ber candidate object models mode structure  literature ’imagebased’ ’viewbased’ model meanings decomposition representation set  object represented collection viewspeciﬁc lo parts thanks structural descriptions rbc offers  cal features poggio edelman  ullman  robustness particular respect noise occlu  tarr bulthoff¨  riesenhuber poggio  sion interesting share components  representations organized trees set view structural descriptions save memory use  tuned units constitutes weighted inputs higher level representation wheel car truck  objectinvariant unit each unit measures similarity bike thirdly learning knowledge extrac  tween input image stored view higher level tion structuring allow addition new repre  unit computes weigthed sum incoming connec sentations nonprohibitive memory cost ex  tions resulting value reaches given threshold plosion complexity openendedness property  learned object recognized riesenhuber’s hmax model approached using kinds data structures referred  object recognition ventral visual stream primates representation factorized cate  proposes similar grouping method higher level gories shared components  cells compute maximum response viewtuned cells  considering object recognition mention  riesenhuber poggio                        important results achieved templatebased approaches    second model based structural descriptions object classiﬁcation recognition lecun et al   important approaches ”recognitionby nearest neighbor methods support vector machines  components” rbc model biederman  each convolutional networks provide efﬁcient solutions    ∗the work described paper partially conducted needs structured knowledge reusability incremental  eu integrated project cogniron ”the cognitive compan autonomous learning points addressed  ion” funded european commission division fpist future best authors’ knowledge dealt  emerging technologies contract fp  precited methods    †supported european social fund               paper presents efﬁcient approach paquier partially implemented building structured rep weight kernels afferent maps units  resentations using primitives exhibiting map sharing set weights  properties mentioned paper organized fol tect learn pattern different positions  lows section  presents model choose extract input maps shown ﬁgure  coordinates unit  organize representations each essential property pre given map receptive ﬁeld afferent map  sented illustrated example produced imple position active unit corresponds  mented section  introduces viewbinding algo position detected pattern provides  rithm incremental building objectinvariant detec shiftinvariance property  tors use objects landmarks buidling structured type mapping previously introduced  representation scenes presented section  fukushima’s neocognitron successfully applied  clude section                                    handwritten digit recognition fukushima  figure                                                         illustrates intrinsic map feature experiment    architecture viewbased recognition            produce image containing set randomly dis                                                        tributed letters minute  ﬁrst map structure constitutes learned autonomously distributed representation  basic building block model discuss inherent input image each resulting map extract learned  shiftinvariant property learning process maps feature location input image obtain result  specialize certain features present way provide intermaps commu  associate order prevent redundancy special nication channel maps don’t learn feature  ization structure extracted features             times procedure called ”local competition”    maps  intermaps connectivity                introduced  map set local classiﬁers                                      weight kernels evolution                                                                map          weight kernel                   learning stream         map                                                                                                     integrate  stream                                                                map                        integrate max learn                       map                                                                                             final pattern representation                                                                 input image map map map map      layerc mapp layerc mapq layerc mapq unitxy    figure  map collection units sharing set  weights kernel each unit composed three stage cal figure  letter extraction noisy image input  ran  culation pipeline receives signals incoming previous dom noise  scale angle variation  steps  images  layers integration lateral signals maps situated second weight kernels evolution time input  layer competition                    image associated maps activations      order satisfy realtime constraints learning  recognition able update representations preventing redundancy competition  frequency compatible modiﬁcations  environment process images high frequency                        fps end view chose implement                                   blocking  signals  integrateandﬁre model local classiﬁers built calcu    lation pipelines ﬁgure                                                      map collection local classiﬁers called units  organized retinotopic way each map associated  afferent maps locations units’  input domains receptive ﬁelds receptive ﬁelds  each unit static each unit associated set layer layer layer layer layer layer  afferent units each afferent map denote Ωi set                                                        figure  hierarchical maps connectivity local compe  afferent units given unit ui                                                        tition letter ”l” detected relative positioning    time given unit ui information ﬂows                                                        horizontal vertical bars input image containing  receptive ﬁelds through set learning weights wit  learning weights organized kernels letters ”l” ”t” intermaps competition local inhi                                                        bition leads distinct specialisations    retinotopic mapping means stimuli adjacent  each visual world processed adjacent sets units previously mentioned requirement concerns struc                                                        tural decomposition components achieve    input    layer   layer    layer   layer  decomposition different maps observing                                                                                  ∈  puts ”decide” specialize distinct component              mapi    tectors undergoing process illustrated ﬁgures            zoom    zoom   example want train maps  tect letters ”l” ”t” input image ﬁrst step  extraction composed layer maps                               map  spectively detect vertical horizontal bars inputs  letter ”l” present image ﬁgure        map  pattern decomposed positions local ori zoom   entations relative positioning patterns                                  map  detectable learned dedicated map                                                                      map  layer layer                                                            point add letter ”t” input  image second learning map layer learn                           map  ﬁgure prevent new map learn  ing ”l” pattern time differentiation  achieved using local competition compare  map detection values choose best ﬁtted section   computation detection value    right calculating detection value unit                                                          each map broadcasts units maps map   map       map      map  layer coordinates unit input     input       input        input                                                                 map         map          map  coordinates correspond position letter receives                              map  incoming values ’max’ stage pipeline architec  ture ﬁgure  ’max’ stage indicates  computes maximum incoming value sends    map      map       map      map  ’learn’ stage learning process able com  pare local unit activation incoming ’integrate  figure  example hierarchical representation  fire’ stage distant activations allowing learning faces network connectivity showing weight kernels  best ﬁtted unit ensure map learn maps activities layer layer layer layer respec  pattern specialized detect illus tively extract contrasts oriented segments eyes mouth  trate intermaps competition figure dark face recognition robustness different subjects  discs represent blocking signal            variable postures  connecting maps build distributed hierarchical  representations                                       ages each  distinct subjects way  adopted layered hierarchical architecture rea previous example letter extraction layer learns  sons firstly need architecture categorization detect position each eye map position  units achieve linear separation input set complex mouth sharing segments extrac  features extracted single layer tion previous layer layer layer train  known limitation single layer perceptron map receives inputs detection values  simulate exclusive disjunction logical xor second mouth eyes specialized maps result map  reason reusability huge number extracted features learns representation face relative positioning  encountered different shapes oriented segments arcs eyes mouth example  circles color blobs building blocks complex sulting face recognition robust high variations subject  images extracted information shared network posture morphology kind recogni  avoid redundancy resulting computational tion applies images limited face orientations  head following idea similar meanings section  propose additional methods recognize  encoded using shared sets units example inter objects based pooling overlapping views  nal representations truck car intersect  intersection representing shared meaning  integration firing learning  tain internal representation wheels steering presented global mechanisms involving  wheel thanks distributed representation architec computing units present internal workings  ture easier pool similar objects cross categories precisely computation detection  wheeled vehicules                            values mentioned section     capability building hierarchical shared repre  sentations shown ﬁgure  experiment trained used faces database att laboratories cam  network recognize faces using set different im bridge httpwwwukresearchattcomfacedatabasehtmlintegration                                           learning output function converge binary  given unit ui denote βit ∈   calculated output like response corresponding strict linear separation  burst time burst detection value mentioned each step compute sample distribution input  fore let wijt ∈ wit learning weight associated burst discretizing output burst value  levels                                            connection units ui uj Ωi subset transfer function fi fact associative cumulative dis  afferent Ωi deﬁned                             tribution time                  wit  wij uj ∈ Ωi                                                                                                          fit      fit                                     Ωi  uj ∈ Ωi βj                                    b≤x  deﬁne three weight sums follows                                                         function fi sampling distribution burst levels                                                           updated each step ﬁgure obtain sigmoidal                              ij                      transfer function permits binarylike classiﬁcation                    t∈w                     ij                               behavior effective competition process                  β                       si        wij              object detection internally represented activa                                                       tion limited set computing units connectiv                         uj ∈Ωi                                                     ity scheme recognitions reﬂected positions               si          wij              located near centre learned pattern pat                      t∈w                       ij                             terns presence just center                                                       surface cover relevant increase pool ac  wi  wijt uj ∈ Ωi wijt        tivated units cover wider surface ﬁrst approach  integrated potential pi time expresses                                        level similarity learnt pattern inputs chose simple disc shape surface given si  given                                          learning                                       β               use model hebbianlike learning rule                     αp pit   − αp si           pit                                 units tend learn patterns responsible activation                              si                    words learning process occurs unit                                                        ﬁred won competition layer  αp  ∈   ﬁxed potential leak                                                        compute stochastic standard deviation σij each weight                                                        wij computed used coefﬁcient learning     burst level sampling distribution cumulative sampling distribution calculation follows                                    step                   step       step             step       step            step                             µij     − αw µij  αw  γij                                                                            σ     − α σ  α µ   − γ                                                     ij               ij       ij        ij                                                                          wij     − αw wij  αw  − σij  γij                                                                                                           µij stochastic mean αw learning rate                                                                    burst levels            burst levels      ensure weights corresponding noisy inputs                                                        high standard deviation tend  won’t  figure  burst sampling distribution cumulative distri representation γij function input burst  bution evolution burst sampling distributions computed unit takes values −   maps  steps    burst values discretized layer competition shown ﬁgure    levels corresponding cumulative distribution used time input burst unit received γijt    transfer function integration                    burst received afferent unit                                                        γijt  − finally burst present γijt      firing                                                   views objects  output bursts βi generated unit ui produced  integration processes situated layer section consider dobject recognition prob  ”learn” stage ﬁgure  ﬁrst case burst levels lem let consider camera rotating object  taken account positive bursts participate given orientation train dedicated map obtain  integration equation  second case previ robustness detect view say   ously explained need compare units activation degrees angle interval centered learnt prototype wider  order ﬁnd best ﬁtted map competition process  burst level computed follows              intervals obtained ”simple” objects balls                                                        paper cups views relatively invariant during                                                 pit  ti           viewpoint modiﬁcation turning ap            βit                                                    fit pit             proch pool overlapping view detectors view                                                        invariant objecttuned detector obtained simply   ti ﬁxed threshold fi adaptive transfer computing maximum viewtuned map response riesen  function variations time illustrated ﬁg huber poggio  provide continuous  ure underlying idea function units object recognition                                                                         orientation maps                input image                                               map                  tracker                                                 map                 map                                                                        map                 map                 map                                                   map                   map                                                   map                 map                                                                        map                 map                 map                                                   map    figure  object invariant detection viewtuned maps maps activities overlap produce continuous detection  resulting learning weights composed  kernels each map correspond  afferent oriented segment  detectors situated previous layer ﬁgure       assuming support external module producing module process ”oneshot learning”  initial object detection simply user’s selec use high learning rate frame thanks  tion input image module based robustness model trackermap detect ob  saliency motion resolve remaining ject started according hebbian  problems firstly objecttuned map track rule detection followed learning phase  object during point view modiﬁcations temporal bind learning occurs completly renews  ing secondly initial information stored prototype high learning rate  object’s complexity anticipate exact process restart allowing continous detection tracker  number required viewtuned maps ﬁnd method maps viewed shortterm memories reach  incrementally adds new maps network asso stable representations  ciate    maptracker  shortterm memory temporal binding  temporal binding  different views object tend occur close  gether time space tarr bulthoff¨  use temporal coherence track object  authors proposed advantage property stinger position resolve remaining problem building  rolls  wallis  particularly want men incrementally viewspeciﬁc representations tracked ob  tion stinger rolls’s hypothesis functional archi ject reach goal granted trackermaps capability  tecture operation ventral visual tested creating new maps during runtime  model called visnet  model each activation text moving objects andor moving point views  viewinvariant neuron maintained during short period shot learning learning high learning rate  time pooling achieved using associative used catch object’s views during movement spe  memory link temporal memory trace current view ciﬁc resulting prototype reﬁned viewtuned maps  tection method biologically plausible compute standard learning rates binding algorithm  difﬁcult apply realtime robotics viewto velops follows viewtuned map recognizes object  object membership priori known criteria position trackermap’s detection during short  decide newly specialized map corresponds period time use  frames timeout initially  view object preselection applied case tracker creates new map forces  size associative memory exceed computable learn unknown view oneshotlearning signal  capacities crucial restrict map specialization view detected map previously learnt  relevant views solution comes tracking approach standard learning process occurs map detec  domain temporal coherence used tion produces oneshot learning signal trackermap  way aim tracking extracting order focus tracking view process ensures  knowledge following position collection longterm memory speciﬁc view higher priority  pixels temporal coherence exploited consid during object detection tracker algorithm  ering persitent informations preserved used experiment ﬁgure  image  frames extracted propose use map rotating object used train threelayers network  architecture pixel tracking order follow position ﬁgure tracker detects object  object use information ﬁrst step thanks feedback longterm shortterm memory  incremental viewtuned map creation                  activity map  times longer    obtain desired tracking capability increasing map learned detect orientations  learning rate αw map section  invariant during rotation model adapt  trackermap value near  trackermap trained number required maps depending complexity  time ﬁrst object position produced external objects
