            unsupervised discretization using kernel density estimation        marenglen biba floriana esposito stefano ferilli nicola di mauro teresa ma basile                          department science university bari                                  orabona   bari italy                             bibaespositoferillindmbasilediunibait                         abstract                      vantages discrete values continuous ones point                                                   learning algorithms handle discrete      discretization defined set cuts                                                  attributes good discretization methods key issue      mains attributes represents important pre significantly affect learning     processing task numeric data analysis come different axes discretization      machine learning algorithms require discrete                                                   methods classified according different direc     feature space realworld applications tions followed implementation discretization tech     tinuous attributes handled deal niques different needs global vs local splitting     problem supervised discretization meth                                                  vs merging bottomup direct vs incremental      ods proposed little supervised vs unsupervised      synthesize unsupervised discretization methods local methods exemplified discretize lo     used domains class information                                                   calized region instance space subset     available furthermore existing methods stances side global methods use entire      equalwidth equalfrequency binning instance space chmielevski grzymalabusse        wellprincipled raising need                                                     splitting methods start list cutpoints      sophisticated methods unsupervised splitting intervals topdown fashion pro     discretization continuous features paper duce progressively cutpoints make discreti     presents novel unsupervised discretization                                                   zation contrary merging methods start      method uses nonparametric density es  possible cutpoints each step discretization     timators automatically adapt subinterval di finement eliminate cutpoints merging intervals      mensions data proposed algorithm                                                     direct methods divide initial interval sub     searches subintervals produce intervals simultaneously equalwidth equal     evaluating best cutpoint basis frequency need input user      density induced subintervals current                                                   number intervals produce incremental methods      cut density given kernel density esti cerquides mantaras  start simple discreti     mator each subinterval uses crossvalidated zation step progressively improve discretization      loglikelihood select maximal number                                                  needing additional criterion stop process      tervals new proposed method compared supervised discretization considers class information      equalwidth equalfrequency discretization unsupervised discretization does equalwidth      methods through experiments known ben                                                  equalfrequency binning simple techniques perform      chmarking data                              unsupervised discretization exploiting class                                                   information methods continuous intervals split     introduction                                 subintervals user specifying   data format important issue machine learning width range values include subinterval fre  ml different types data make relevant differ quency number instances each subinterval   ence learning tasks infinitely simple methods lead good results  values continuous attribute number discrete tinuous values compliant uniform distribu  values small finite learning classifi tion additionally outliers handled   cation treesrules data type important impact produce results low accuracy presence skew   decision tree induction reported dougherty et data usually deal problems class information   al discretization makes learning accurate used supervised methods   faster general decision trees rules learned using information available option exploiting unsu  discrete features compact accurate pervised methods exist supervised meth  induced using continuous ones addition ad ods literature work synthe                                                 ijcai                                                sizing unsupervised methods fact value point bin contributes    discretization commonly associated equally density matter close far away    classification task work supervised methods points    strongly motivated learning tasks class    information available particular domains    learning algorithms deal discrete values    learning settings cases class information    exploited unsupervised discretization methods    simple binning used      work presented paper proposes topdown    global direct unsupervised method discretization    exploits density estimation methods select cutpoints    during discretization process number cutpoints    computed crossvalidating loglikelihood  sider candidate cutpoints fall  stances attribute discretized space   possible cutpoints evaluate grow large datasets   continuous attributes instances   different values reason developed   implemented efficient algorithm complexity  figure  simple binning places block                                                         subinterval instance falls   nlogn number instances     paper organized follows section    scribe nonparametric density estimators special case restricting does real mir                                                   ror data principle points closer    kernel density estimator section  pre   sent discretization algorithm section  weighted points far    port experiments carried classical datasets uci step doing eliminating dependence bin                                                    origins fixed apriori place bin origins centered    repository section  concludes paper outlines future    work                                           point following pseudoformula                                                                                                                           instances  fall bin containing      nonparametric density estimation                binwidth   data available various distributions    straightforward construct density functions    given data parametric density estimation transformed following     important assumption available data density                                                               function belongs known family distributions    instances  fall bin    normal distribution gaussian having binwidth   parameters mean variance para   metric method does finding values parameters subtle important difference constructing binning    best fit data data complex density second formula permits place bin    assumptions distributions forced calculation density performed    data lead models fit data bin containing depending origin    se cases making assumptions difficult non bin center bin center allows    parametric density functions preferred     successively assign different weights points     simple binning histograms bin terms impact density   known nonparametric density methods consists pending distance consider intervals    signing value density function width centered density function given    instance falls interval –  formula    origin bin binwidth value    function defined follows symbol  stands    ‘number of’                                            instances fallin                                                           hn                                             instances  fall       case constructing density function box                                              width placed point falls interval                                                    centered boxes dashed ones figure     fixed origin bin instance falls added yielding density function figure      interval centered width block size  provides way giving accurate view    bin width placed interval figure  density data called box kernel density    important note wants density                                                   ijcai                                                 estimate weights points fall represent instances data decided cut    bin changed        middle points instance values advantage                                                    cutting strategy avoids need deciding                                                    point cut performed                                                   cluded left right subinterval                                                      second question subinterval                                                    cutsplit produced given step                                                    discretization process choice driven                                                    objective capturing significant changes density                                                    different separated bins proposal evaluate                                                    possible cutpoints subintervals assigning                                                    each score according method meaning                                                    follows given single interval split cut                                                   points produces bins induces initial                                                    interval densities computed using simple binning                                                    density estimation formula formula shown                                                    previous section assigns density value func                                                   tion instance bin ignores distance                                                    instances bin computing                                                    density subinterval produced averaged                                                    binned density binned density each point dif                                                   ferent density estimated kernel function                                                    difference subinterval fits     figure  placing box instance interval data better binning     adding                   reason split contrary idea underlying                                                    discretization algorithm splitting    order kernel density function introduced search worst subintervals produce                                                    “worst” means density shown each                                                  subintervals different dis                                                                         tances points intervals weighting func                    nh                        tion considered identified worst subintervals                                                    just split produce intervals   weighting function function does fit data way intervals density   providing smart way estimating density differs real data situation eliminated   counting frequency points xi bin replaced subintervals order achieve den  weighting differently depending dis sity computed kernel density function  tance contributions density value produce splitting main interval fig   points xi vary closer ure     weighted points away prop obvious question arises given sub  erty fulfilled functions called kernel interval cut anymore searching   functions kernel function usually probability den worst subintervals good candidates   sity functions integrates  takes positive values split true hand each step   domain important density estimation algorithms split subintervals   does reside kernel function gaussian subinterval case   epanechnikov quadratic used band split split scoring function   width selection silverman  motivate cutpoints allows choose subinterval split   choice bandwidth value case kernel   functions selection problem section  scoring function cutpoints   introduce problem cutting intervals based den each step discretization process choose   sity induced cut density given  different subintervals split subinterval   kernel density estimation                       identify candidate cutpoints middle points                                                   tween instances each candidate cutpoints     cut                        compute score follows    aim discretization produce subintervals                                                                                 induced density instances best fits avail px     px     able data problem solved cut scoret                                                                                             supervised topdown discretization method cut    exactly points main interval discretize                                                   ijcai                                                 refers instances fall left discretizeinterval   subinterval instances fall begin   right bin density functions respectively potentialcutpoints  computecutpointsinterval   kernel density function simple binning density func  priorityqueueintervalsaddinterval   tion functions computed follows     stopping criteria met                                                      priorityqueuecps                                                          foreach cutpoint cp potentialcutpoints              fxi                                       scorecp  computescoringfunctioncpinterval                                                      priorityqueuecpsaddcpscorecp                                                               end                                                         number instances fall left         bestcp  priorityqueuegetbest   right bin binwidth number         currentinterval  priorityqueueintervalsgetbest    stances interval split kernel density      newintervals  splitcurrentintervalbestcp    estimator given formula                leftinterval  newintervalsgetleftinterval                                                      rightinterval  newintervalsgetrightinterval                                                        xi                   potentialleftcps   computecutpointsleftinterval               pxi                                 potentialrightcps computecutpointsrightinterval                      hn                                                               foreach cutpoint cp potentialleftcps                                                        scorecp  computescoringfunctioncpleftinterval    bandwidth kernel function priorityqueuecpsaddcpscorecp    framework discretization remains clarified priorityqueueintervalsaddleftintervalscorecp    bandwidth kernel density estimator chosen   end     ways reported         foreach cycle potentialrightcps     silverman  fact context inter end      ested density computed classic kernel density end   estimator considers globally entire set available    instances classic way kernel density estimation works figure  discretization algorithm pseudo language    considers total number instances initial    interval chooses smoothing parameter              choice easy various techniques    investigated optimal proposal     text adapt classic kernel density estimator tak          ing equal binwidth specified follows    seen formula pxi instances figure  cut   distant xi contribute weight equal candidate cutpoints placed middle adja  zero density xi subinterval bin cent instances     subintervals pro   consideration binwidth instances fall duced cutpoint           contribute depending distance xi similarly cutpoints suppose    density xi interested knowing cur computing scoring function each cutpoint great   rent binned density induced candidate cutpoint est value indicating cutpoint produces    computed binwidth differs density worst subintervals reached cutpoint     bin computed weighting contributions xj subintervals       list    density xi basis distance xi – xj candidate cutpoints       useless consider function bandwidth greater  suppose scoring function evaluates follows                                            score   score   score                                                      score   score   algorithm selects     discretization algorithm                 best cutpoint splits corresponding inter   scoring function synthesized explain val shown figure     discretization algorithm works figure  shows    algorithm pseudo language starts list        cutpoints implemented priority queue    order maintain each step cutpoints ordered    value according scoring function          priority queue contains subintervals generated    far let through example suppose initial                                                                                 interval discretized figure  frequencies        instances shown                           figure  second cut                                                   ijcai                                                second cut produces new subintervals njtrain number training instances bin njtest  current discretization three subintervals number test instances fall bin total            candidate cut number instances width bin  points     suppose values regards kernel density estimator complexity   scoring function follows score   formula deduced complexity   score   score   score   evaluating kernel density points univari  best cutpoint  suggests cut discre ate data complexity problem solved   tization          algorithms proposed greengard strain        algorithm refines subintervals yang et al  compute kernel density esti  show worst fit data note worth ca mate onn instead context deal   ses happen split performed univariate data single continuous   subintervals left right attributes processed instances   produces shows good fit compared theoretical complexity algorithm onlogn  subintervals split future   strange scoring function evaluates overall    experiments   fit subintervals case cut   present example cutpoint  chosen order assess validity performance pro  left subinterval    shows good fit posed discretization algorithm performed  experi  data terms density right    ments datasets taken uci repository   shows bad fit case interval    classically used literature evaluate discretization   cut interval    remain algorithms past specifically dataset used   untouched till end discretization algorithm autos bupa wine ionosphere ecoli sonar glass heart   algorithm stop cutting stopping criterion hepatitis arrhythmia anneal cylinder autompg   maximal number cutpoints computed procedure datasets contain large set numeric attributes various   explained paragraph met        types  continuous attributes extracted                                                   random used test discretization algorithm    stopping criteria complexity              order evaluate discretization carried                                                   proposed algorithm respect algorithms   definition stopping criterion fundamental pre                                                  literature compared three methods equal  vent algorithm continuing cut each bin width fixed number bins use  experi  tains single instance reaching ex ments equalfrequency fixed number bins use   treme situation risk running overfitting                                                    experiments equalwidth crossvalidated   model real usual literature use log number bins comparison log  likelihood evaluate density estimators simple likelihood test data using fold crossvalidation   binning kernel density estimate solution                                                  methodology results test folds compared   stead requiring specific number intervals through paired ttest regards crossvalidated log  rigid based valid assumptions pro likelihood table  presents results ttest based   pose use crossvalidation provide unbiased es                                                  crossvalidated loglikelihood risk level      timation model fits real distribution shows number continuous attributes discretiza  experiments performed fold crossvalidation tion through method significantly better equal   used each fold algorithm computes stopping                                                   significantly worst compared methods   criterion follows supposing –  candidate   cutpoints each crossvalidated log  likelihood computed order optimize performance                                                     method                    method   each step structure maintains subintervals cur                                                              significanlty equal significanlty   rent discretization corresponding splitting values                                                               accurate       accurate   new values interval split                                                    equalwidth   computed each step algorithm computes                                                     bins                   loglikelihood –  cutpoints performed    times overall number cutpoints shows equalfreq   maximum value averaged loglikelihood test  bins            folds chosen best loglikelihood test equalwidth   data given following formula          cross                                                                     validated                                                                train           table  results paired ttest based crossvalidated       loglikelihood    test log                                             loglikelihood  folds                                                       clear majority cases new al                                                  gorithm shows difference performance respect                                                  ijcai                                               
