                     efﬁcient bayesian tasklevel transfer learning                                    daniel roy    leslie kaelbling                                   massachusetts institute technology                          science artiﬁcial intelligence laboratory                                         droy lpkcsailmitedu                          abstract                          evaluated real data end evaluate                                                        resulting model meeting domain learned      paper show using dirichlet pro decides based training data user accept      cess mixture model generative model data  reject request behalf model      sets provides simple effective method   shares data outperforms nosharing counterpart using      transfer learning particular present hierar data users inﬂuence predictions      chical extension classic naive bayes classi                                                          faced classiﬁcation task single data set      ﬁer couples multiple naive bayes classiﬁers                                                        wellstudied techniques abound boser et al  lafferty      placing dirichlet process prior parame                                                        et al  popular classiﬁer works practice      ters show recent advances approximate                                                        despite simplicity naive bayes classiﬁer maron      inference dirichlet process mixture model                                                         extend classiﬁer multitask setting      enable efﬁcient inference evaluate result                                                        training classiﬁer each cluster latent par      ing model meeting domain sys                                                        tition handle uncertainty number clusters      tem decides based learned model user’s                                                        membership deﬁne generative process data      behavior accept reject request                                                        sets induces clustering heart process      behalf extended model outperforms                                                        nonparametric prior known dirichlet process      standard naive bayes model using data                                                        prior couples parameters naive bayes classiﬁers      users inﬂuence predictions                                                        attached each data set approach extends applica                                                        bility naive bayes classiﬁer domain multitask    introduction                                       learning tasks deﬁned input space  machine learning confronted multiple bayesian inference clustered naive bayes model  related data sets asked make predictions example combines contribution partition data sets  spam ﬁltering typical data set consists thousands la weighing each partition’s posterior probability  beled emails belonging collection users sense sum partitions intractable  multiple data sets—one each user employ recent work heller ghahramani  combine data sets ignore prior knowledge dif implement approximate inference algorithm result  ferent users labeled each email combine data efﬁcient tasklevel transfer learning  group users roughly agree deﬁnition spam  increased available training data models  make predictions preferences  population users heterogeneous expect paper concentrate classiﬁcation settings                                                                                  simply collapsing data undifferentiated col features   labels drawn                                                                   lection make predictions worse              ﬁnite sets   respectively goal learn rela    process using data unrelated partially tionship input features output labels order  lated tasks known transfer learning multitask learn predict label given unseen combination features  ing growing literature baxter  guestrin et al consider tasks each nu training examples com   thrun  xue et al  humans effort posed label feature vector make dis  lessly use experience related tasks improve cussion concrete assume each task associated  formance novel tasks machines given precise different user performing common task                                                                                                ui  structions make connections paper write represent feature vectors                                                                            ui  introduce set instructions based statis corresponding labels  nu associated  tical assumption exists partition tasks uth user entire collection data  clusters data tasks cluster users duj xujyuj jth data point  identically distributed ultimately model sharing uth user                                                    ijcai                                                      let muy denote number data points labeled ∈l                                       dp  data set associated uth user let nuyfx  denote number instances uth data set                                                                    φu                   φu  fth feature takes value ∈vf parent label takes                                      value ∈lletθuyfx  pxf        yand             φuy  py    write general form           yuj    θuyf          yuj     θuyf    probability mass function data conditioned              lf                 lf  parameters θ φ naive bayes model                                                                 xujf                 xujf                                                                                                         pdθ φ     pd   θuφu       pd     θuφu                                                                                                                    ≤u≤u                                                                                        ≤n≤nu                                                 nu                                             unφ     xuny uθ            figure  graphical models nosharing clus                   uy              uyf        tered naive bayes right models each user parame                                            terization nosharing model parameters clustered     u          f                                   naive bayes model drawn dirichlet process                                  muy          uyfx                      intermediate measure integrated           φuy         θuyfx      y∈l      x∈vf                                                        assumption independence training entire collection  pdθ φ expanded product terms models identical training each model separately  each data set reﬂecting data sets independent data set model nosharing  ditioned parameterization step assumes data model  points exchangeable particular labelfeature pairs having speciﬁed prior factors parameter dis  independent given parameterization tributions each user specify actual parameter  step use naive bayes assump distributions each user reasonable tractable class  tion features conditionally independent given distributions multinomial parameters dirichlet  label finally step used fact distri distributions conjugate multinomials  butions multinomials maximum likelihood parame fore distribution φu takes values  terizations                                       simplex               muy                   nuyfx                                   φuy           θuyfx                                                                       muy                   nuyfx                         Γ  y∈l αuy              y∈l                    x∈vf                                                  αuy −                                                                 fφu                  φuy                                                                                  Γαuy    each data set parameterized separately             y∈l        y∈l  surprise maximum likelihood parameterization                                                                                   θ  each data set depends data data set similarly distribution uyf  takes values                                                          order induce sharing constrain pa simplex  rameterization users bayesian setting prior                                                                               Γ       βuyfx   distribution θ φ used enforce constraints θ       x∈vf            θβuyfx−                                                               uy                      uyfx  given prior resulting joint distribution data         x∈v  Γβuyfx                                                                                     x∈vf            pd        pdθ φ df θ φ           write resulting model compactly generative                     Θ×Φ                                process  models introduced section completely spec          φu ∼ dirαuy  ∈l            iﬁed prior distribution parameters naive                                                                         bayes model different priors result differ          φu ∼ discreteφu  ent types sharing                                                                    θuyf ∼ dirβuyfx  ∈vf     nosharing baseline model  seen ml parameterization naive                                                        xf         θuyf  ∀y ∈l∼discreteθuy unf   bayes model ignores related data sets bayesian set  ting prior density entire set parameters nosharing model function baseline  factors densities each user’s parameters result compare alternative models induce shar  sharing particular                            ing turn specifying model induces sharing                        u               θ φ         θuφu                        ∼                                                    symbol denotes variable left distributed                                                        according distribution speciﬁed right noted  equivalent statement parameters each user dirichlet distribution requires ordered set parameters  independent parameters users deﬁne arbitrary ordering elements vf                                                     ijcai                                                                                                                                   y∈l    clustered naive bayes model                               θu θuyf ff ∼  plausible assumption collection related data sets                                                                 θuyf  ∀y ∈l∼discreteθuy unf   clustered closelyrelated groups                                                                           make precise consider tasks discrete measure draw dirichlet process  space × related data associated practice random variable marginalized  tasks identically distributed cause tasks clustered named model  coarse model relatedness leads improved predictive clustered naive bayes model denote distribution                                                                                    performance limited training data               function parameters cnb explain    ﬁrst step specify distribution parti use model make predictions given training data  tions tasks properties like  distribution ﬁrst want exchangeability  approximate inference  tasks users probability depend                       ui ui                                                        given labelled training data      ≤i≤nu  ordering identity tasks users second want tasks ∈ unlabeled feature vec  exchangeability clusters probability ∗  vn                                                         tor        task like com  depend orderingnaming clusters ﬁnally                                  ∗     vn                                                         pute posterior distribution label    want consistency priori hypothetical existence                                                        using bayes rule ignoring normalization constants  unobserved task affect probability                                                           ∗  yx∗d   ∝ x∗y ∗  yd  ∗   yd  group tasks clustered                                                      chinese restaurant process crp stochastic pro                                                                           ∝      dθ φ df   θ φ   cess induces distribution partitions satisﬁes                        cnb    requirements aldous  following metaphor                   Θ×Φ                                                                  used deﬁne process imagine restaurant data set imagine nv   countably inﬁnite number indistinguishable tables th data point label bayesian inference  ﬁrst customer sits arbitrary table subsequent requires marginalize parameters including  customers sit occupied table probability propor latent partitions dirichlet process having cho  tional number customers seated table sen conjugate priors base distribution analytically  sit arbitrary new table probability proportional marginalized sum partitions makes  parameter α resulting “seating chart” parti exact inference dirichlet process mixture model  tions customers shown expectation intractable markov chain monte carlo varia  number occupied tables customers Θlog tional techniques popular approaches paper  toniak  navarro et al                   uses simple recentlyproposed approximation dpm    tasks each cluster partition share known bayesian hierarchical clustering bhc heller  parameterization extending generative model ghahramani approximates sum  imagine new user enters restaurant sits partitions ﬁrst greedily generating hierarchical clus  new table draw complete parameterization tering tasks efﬁciently summing ex  naive bayes model base distribution param ponential number partitions “consistent” hierar  eterization associated table user sits chy approach leads simple efﬁcient algorithm  occupied table adopt parameterization achieving tasklevel transfer  sociated table table consider rooted binary tree each task associ  uses rules predicting                   ated leaf convenient identify each internal    generative process corresponds known node tn set leaves descending node  dirichlet process mixture model dpm used treeconsistent partition tasks partition  successfully model latent groups ferguson  each subset corresponds exactly node graph fig  underlying dirichlet process parameters mix ure  seen given rooted tree  ing parameter α corresponds parameter objects set treeconsistent partitions strict  crp base distribution parameters subset set partitions exact inference  drawn each new table important specify dpm requires marginalize latent partition  draw complete parameterization feature distribu requiring sum superexponential number parti  tions θyf  each table decided share tions bhc approximation works efﬁciently comput  marginal distributions φ interested ing sum exponential number treeconsistent  knowledge relating features labels           partitions using divideandconquer approach combine    represent model compactly specify results each subtree intuitively tree chosen  ing generative process                           carefully set treeconsistent partitions cap                                                        ture mass posterior bhc tries ﬁnd                φu ∼  dirαuy  ∈l                                                        tree combining bayesian model selection greedy                                      φ ∼ discreteφu                heuristic                    f                                   just classic agglomerative clustering duda et al          ∼ dpα       dirβyfx  ∈vf          bhc starts objects assigned clus                   y∈l                              ter merges clusters implicitly                                                    ijcai                                                                                                           heller ghahramani show speciﬁc choice                                                                 π                                                          prior    leads approximate inference                                                        scheme dpm let α corresponding parameter                                                                      dpm calculate prior probability                                          each cluster tj tree built bhc                                                            initialize each leaf di  α πi                                        each internal node                                                                                   dk  αΓnk                                                    leftk rightk                                                                 αΓnk                                                             πk              inconsistent                                 dk                                                         end  figure  treeconsistent partitions represented sets having built tree approximates posterior distribu  nodes left collection leaves right partition tion partitions use tree compute pos  treeconsistent sets leaves  representable  internal node                                    terior probability unseen label assume                                                        unlabeled example xk associated kth task let ak                                                                                                  forming tree records merges performed set nodes path node root                                                        tree generated bhc algorithm figure   instead using distance metric merging                          ∈   nearest clusters bhc merges clusters maximize        note elements corre                                                        spond clusters task participates tree  statistical hypothesis test each step algorithm                                     determine pair set clusters tttm consistent partitions predictive distribution  merge consider particular clusters ti tj weighted average predictive distributions  let di dj set tasks each respectively each partition                                                                              bhc algorithm calculates posterior probability                     wi                                                     pykxkdt                 pykxkdi    clusters fact single cluster   specif                         wj                                                                           ∈a    j∈ak  ically hypothesis hk data dk  di ∪ dj                  identically distributed respect base model                                                             wk  rk           − ri pykxkdk  case naive bayes model probability           i∈ak                                               predictive distribution base model combining  data new cluster  simply                               marginal likelihood data                      data tasks cluster     alternative hypothesis h¯k data di dj computational complexity posterior computa                                                        tion quadratic number tasks heller ghahra  fact split clusters computing                            probability associated hypothesis normally mani proposed  log    random  require sum superexponential number partitions ized variants  associated tasks di dj clever  trick bhc algorithm restrict attention tree  results  consistent partitions probability data                                                        utility type sharing clustered naive  dk  di ∪ dj h¯k pdkh¯kpditi pdjtj                                                        bayes supports assessed real data sets  pditi probability data associated                                                        end evaluated model’s predictions meeting clas  tree tiletπk  phk prior probability                                                        siﬁcation data set collected rosenstein et al   cluster tk write pdktk recursively                                                        data set split  users multiple universities   pdktkπkpdkhk−πkpditipdjtj       industry lab military training exercise total                                                         labeled meeting requests  meeting  posterior probability hk                                                        quests user meeting acceptance task aim                           πkpdkhk                   predict user accept reject unseen               phkdk                                                     pdktk                    meeting request based small set features                                                        various aspects meeting  present bhc algorithm output sufﬁ                                                          evaluate clustered model assessed predictive  cient approximate bayesian predictions model                                                        performance transfer learning setting predicting labels                           input data                             user sparse data having observed labeled        model pxyθ prior density fθ           data remaining users particular calculated   initialize number clusters cn               receiveroperator characteristic roc curve having trained                             di                         training examples each user                                           conditioned knowledge labels remaining        pair di dj highest posterior users each curve generated according results             probability hk tk  ti  tj      random partitions users’ data training        merge dk ←  dk ∪ dj tk ← titj               testing sets figure  plots area roc curve        delete di dj ← −                      measure classiﬁcation performance versus number   end                                            training examples                                                    ijcai                                                           user  ab mil    user  ed mil user  td oregon user  tlp mit user  nh mil                                                                                                                                                                                                                                                                               ←                                                                                                ←                                                                          ←                                                       ←                                                                                                                                                                                                     ←                                                                                                     ←                                                                                                                                                                       ←                                                                                                         ←                              ←                                                                  ←                                                                                               ←                                                                                                                                                                                                           ←           ←                                                                             ←         ←                                                                                                                                                                                                                                                                                                                                           ←                                                                                                       ←   area  roc               clus                                                          ←                  ns                                                                              samples           samples           samples           samples          samples    figure  area curve auc vs training size ﬁve representative users auc varies  correct   wrong  chance each experiment label map cluster users user belongs cluster remains  experiments omit ﬁrst mention ﬁrst three examples illustrate improved predictive performance  examples demonstrate possible performance drop baseline model           examples  examples   examples  examples                                                                      nb                                                                                                                                   − auroc                                                               cnb                                                                                                                                                                                                                                                                 −       mmmmmmmmppppsspsspppp mmmmmmmmppppppppspsss mmmmmmmmppppppsppspss mmmmmmmmpppppppsspsps mean  difference auroc −                                                                                                                                                                            training samples  figure  progression trees bhc      examples user short vertical edges indicate tasks figure  clustered model area roc curve  strongly related long vertical edges indicate tasks standard model data available   unrelated key military professor sri researcher training examples standard model data match                                                        performance clustered model dotted lines standard error     users selected ﬁve representative  samples ﬁrst three examples users    show grouped military personnel pro  model performs able use related user’s fessors sri researchers grouped  data make predictions single labeled data point data warrant splitting apart  model groups user  military personnel figure  shows relative performance clustered  users   each step model makes pre versus standard naive bayes model clustered variant  dictions averaging treeconsistent partitions outperforms standard model faced  map partition listed ﬁgure largest contribution examples  examples models perform roughly  user  map partition changes each step provid equivalently standard model enjoys slight ad  ing superior predictive performance vantage does grow examples  user second ﬁgure model chooses sticks  map partition groups ﬁrst user  related work  example user  grouped user  initially earliest work related transfer learning focused  later roughly users witnessed sequential transfer neural networks using weights  improved initial performance tapered number networks trained related data bias learning net  examples grew                                     works novel tasks caruana  pratt     fourth example user  illustrates cases cently ideas applied modern supervised  initial performance user samples learning algorithms like support vector machines wu  improved good candidate related users dietterich  work understand  cluster finally example shows connection approaches kind sharing  cases predictions using clustered model expect clustered naive bayes model  leads worse performance speciﬁc case model work related large body transfer learning  groups user  user   samples research conducted hierarchical bayesian framework  model recovers mistake achieves equal common prior distributions used tie  performance                                          model components multiple data sets clustered    figure  shows trees corresponding partitions model seen extension model ﬁrst pre  covered bhc algorithm number training ex sented rosenstein et al  achieving transfer  amples each user increased inspecting partitions naive bayes model work ﬁt dirichlet dis  fall understandable lines military personnel tribution each shared parameter users unfortu                                                    ijcai                                                    
