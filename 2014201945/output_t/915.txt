                             learning partial observations∗                                                loizos michael                               division engineering applied sciences                            harvard university cambridge ma  usa                                          loizoseecsharvardedu                          abstract                            work employ learning means identifying                                                        structure domain given access certain ob      present general machine learning framework   servations subsequently utilize identiﬁed structure      modelling phenomenon missing informa  recover missing information new observations coming      tion data propose masking process model  domain note manner ac      capture stochastic nature information loss quired knowledge utilized draw conclusions      learning context employed means necessarily single step process valiant       recover missing information nonetheless focus examining      coverable extend probably approximately   dividual learned rules meaningfully applied partial      correct semantics case learning par observations given rules learned obser      tial observations arbitrarily hidden attributes vations partial studying multiple      establish simply requiring learned hypothe learned rules chained reasoned draw richer      ses consistent observed values sufﬁces conclusions presents challenges necessitates      guarantee hidden values recoverable lution fundamental problem examined      certain accuracy show sense                                                          present general machine learning framework      optimal strategy achieving accurate                                                        problem dealing missing information      covery establish number natural                                                        understood formulate notion masked attributes      concept classes including classes mono                                                        values learning examples patient records      tone formulas pac learnable monotone                                                        known agent masked attributes account      formulas classes conjunctions disjunc                                                        missing information given target agent      tions kcnf kdnf linear thresholds                                                        attempts learn presence particular disease      consistently learnable partial observations                                                        learning features agent’s hy      ﬁnally show concept classes parities                                                        potheses formed various pieces information      monotone term decision lists properly                                                        patient’s medical history masked attributes determined      consistently learnable partial observations                                                        arbitrary stochastic process induces each ex      rp  np implies separation                                                        ample possibly different ﬁxed distribution partial      sistently learnable partial observations versus                                                        observations example mapped schu      learnable complete noisy setting                                                        urmans greiner  intended capture situa                                                        tions probabilistic failure inability agent’s    introduction                                       sensors provide readings extend probably approx                                                        imately correct learning semantics valiant  apply  consider task predicting missing entries medical described situation salient feature extension  database given information available propose lack need specially prepared learn  does making predictions kind ing materials agent simply utilizes information  guarantees provide accuracy pre available through masking process  dictions problem faced type learning autodidactic emphasize  missing information data arguably universal multi agent employ supervised learning techniques                                               disciplinary problem standard statistical techniques schafer presence teacher explicitly providing                   graham  fail provide formal treatment agent “labelled instances” during learning phase  general case problem fact information  missing arbitrarily correlated actual value propose consistency intuitive measure success  missing information patients exhibiting certain learning process agent faced partial observa  symptom inclined disclose fact tions needs produce hypotheses contradict                                                        actually observed values masked attributes need    ∗this work supported grant nsfccf predicted correctly addition hypotheses                                                    ijcai                                                     sume deﬁnite value masked attributes need make deﬁnition  examples observations consider  prediction allow possibility “don’t know” nonempty ﬁnite set attributesanexample  predictions restrict predictions natural manner vector exm ∈ aanobservation vector  providing notion completeness prediction process obs ∈  ∗a observation obs masks exam    following presentation framework discuss ple exm obsi ∈exmi ∗ attribute xi ∈a  accuracy alternative measure success attribute xi ∈ais masked observation obs  agent expected correctly predict values masked obsi∗amasking process stochastic function  tributes show success agent stricter mask   →  ∗a maps each example  setting completely impaired depending exm observation obs masks exm  cealing masking process adversarially infor  mation hidden agent positive side examples deﬁne “truth” environment  show degree allowed masking process examples drawn underlying ﬁxed probability                                                                   agent perform optimally making accurate predictions distribution unknown agent unlike standard  simply making consistent predictions surprising pac learning agent does directly observe exam  lation measures success allows agent ples masked versions examples denote                                                        maskd  focus natural task learning consistently induced distribution observations  losing respect predicting accurately stochastic masking process understood    examine consistent learnability closely ways attributes correspond agent’s sensors masking  deﬁne notion reduction learning tasks es corresponds stochastic failure sensors pro  tablish concept class monotone formulas vide readings attributes correspond properties en  pac learnable hypothesis class monotone formu vironment masking corresponds agent’s inability si                                                        multaneously sense properties case masking  las consistently learnable partial observations             exm  result obtained reducing learning task duces each example possibly different ﬁxed dis                                                        tribution maskexm observations induced distribu  complete observations through second reduction show                                     maskd  concept classes conjunctions disjunctions kcnf tions remain unknown agent does   kdnf linear thresholds literals properly masked attributes values hidden  consistently learnable partial observations     connotations particular masked attribute    negative side show set consistently understood “nondeducible” rest attributes                                                        goal agent deduce masked attribute  learnable concept classes subset pac learnable         ∗  cept classes continue prove concept classes assigned value  deduce truthvalue  parities monotone term decision lists properly masked attribute according underlying masked ex  consistently learnable partial observations given ample nontrivial impossible  widely held complexity assumption rp  np true task depending masking process considered                                                                                                     intractability properly learning monotone term decision deﬁnition  formulas formula xi xik  lists partial observations provides partial answer function   →  arguments  question posed rivest  intractability results es                                ∈a                                                        associated attributes xi xik  tablish separations model consistent learnabil                          exm                                                        value xi xik given example deﬁned  ity partial observations existing models pac val        exm   exm      exm                                                              xi xik                ik   learnability valiant  learnability presence                               obs                                                        value xi xik given observation  random classiﬁcation noise angluin laird           val             obs                                                        noted     xi xik      deﬁned    assume reader familiar basic pac learning common value formula given examples masked  terminology kearns vazirani  proofs obs case common value exists ∗  brieﬂy discussed paper lack space                                                          agent’s task identifying structure environment                                                        precise problem learning certain    learning framework                             target attribute expressed formula  pac learning model valiant  set boolean attributes perceived through agent’s sen                                                              variables xxxn represents attributes en sors study learnability usually assumes tar  vironment concept boolean formula boolean attribute expressible formula called  variables example concept truthassignment target concept target attribute  boolean variables drawn underlying probabil sumes truthvalue according target concept  ity distribution paired induced truthvalue scribed setting captured following deﬁnitions    treatment distinguishes target attribute deﬁnition  formula equivalence formulas ϕ ϕ  attributes acting learning features target equivalent wrt probability distribution  natural better suited approach autodidactic learning prvalϕ  exmvalϕ  exm  exm ←d  target attributes externally “labelled”  sider examples treat attributes equally properties generally consider formula attributes  environment attribute acting learning target need expressed formula attributes approach  deﬁned learning task undertakes similar deﬁnitions results apply largely unchanged                                                    ijcai                                                     deﬁnition  supported concepts concept class hypotheses correctly predict values nonmasked  set formulas probability distribution tributes artiﬁcially purposes analysis “ob  supports attribute xt xt equivalent scured” observation observation drawn  formula ∈cwrt target concept xt sense best hope agent    supported concept classes essentially encode known gets observe parts environment form  assumed bias probability distribution ex hypotheses best case consistent obser  amples drawn imposes constraints examples vations agree underlying  simplest possible manner facil masked examples reminiscent developing physical  itates learnability assuming bias goal agent theories ﬁnding laws consistent ob  identify formula hypothesis class serve implying current past future  consistent target attribute high probability physical theories actually “correct” ones hypotheses                                                        developed manner course used make predic  deﬁnition  learning tasks learning task tions masked attributes world humans great  triple xt hwherext attribute concept lengths subsequently obtain values masked  class hypothesis class formulas tributes experimentally validate physical theory    omit writing set attributes learn context work study developed  ing task deﬁned does introduce ambiguities ories hypotheses consistent partial obser               −                                     vations agent actually make accurate predictions  deﬁnition   ε consistency hypothesis conﬂicts hypothetical validation experiment given ob  target attribute xt ∈awrt observation obs     obs                   exm  val   obs val   obs                      servation    masking example    attribute xt                 xt                hypothesis  masked obs wish examine pos   − εconsistent target attribute xt ∈aunder         exm                                          mask         sible predict   accurately simply  probability distribution masking process     consistently “ﬁllin” missing information obs     val   obs val   obs     pr                 xt                                           −                           −                     exm  ←d  obs ←  maskexm  ≤ ε   deﬁnition     ε accuracy hypothesis ε                                                         accurate wrt target attribute xt ∈aunder probability    recall formulas evaluate ∗ given observa distribution masking process mask  tion interpret value “don’t know” prediction                                                        prvalh   obs valxt  exm     prediction consistent target attribute                                                                           exm ←d  obs  ← maskexm  ≤ ε  similarly value ∗ attribute interpreted “don’t  know” sensor reading prediction consistent hypotheses evaluate ∗ given observation  sensor reading say long prediction accuracy requirement amounts asking  coming through hypothesis sensor reading hypothesis predicts   value value  directly conﬂict producing different   values accordance actual observed  inconsistency observational level          value target attribute identifying conditions    important note ability make “don’t know” form accurate hypotheses essential  predictions abused agent hypothesis agent’s actions yield utility based agent  necessarily formula assumes deﬁnite   value observes based actually holds agent’s en  sufﬁciently arguments speciﬁed vironment informed agent actual  evaluates ∗ given observation value state environment through observations accu  actual underlying example masked obtain rate predictions better decisions agent reach  observation determined framework ac clearly predictions accurate necessarily  counts implicit notion completeness imposing sistent holds obst ∈exmt ∗  natural restriction “don’t know” predictions  direction does hold general predic                                                        tions masked target attributes consistent  deﬁnition  consistent learnability algorithm                                                        evident reason accurate  consistent learner learning task xt h  probability distribution supporting xt theorem  indistinguishability adversarial settings  masking process mask real number δ δ≤  consider target attribute xt concept class  real number ε ε≤  algorithm runs time axt let ϕϕ ∈cbe ϕ  ϕthere  polynomial δ ε size target concept exist probability distributions ϕϕ  xt probability −δ returns hypothesis equivalent wrt ii ϕxt  ∈hthat −εconsistent xt maskdthe equivalent wrt iii ϕxt equivalent wrt  concept class consistently learnable xt exists masking process mask  exists consistent learner xt h maskdmaskd attribute axt                                                        masked drawn observation    consistent learners vs accurate predictors          theorem  shows examples masked  taken approach learned hypotheses ex way nonequivalent concepts indistinguish  pected consistent observations natural general able given set observations fact sufﬁces  ization respective requirements pac learning mask target attribute adversarially selected                                                    ijcai                                                     cases result through nonmasked attributes deﬁnition  accurate predictability algorithm  adversarially selected observations imply accurate predictor learning task xt h    value formulas axt excluding probability distribution supporting xt  possibility “don’t know” prediction clearly agent real number η η≤  masking process mask  means identifying probability distributions  − ηconcealing xt h real number  examples drawn equivalently δ δ≤  real number ε ε≤   formulas ϕϕ target concept xt im algorithm runs time polynomial η δ ε  possible agent conﬁdently return hypothesis size target concept xt proba  highly accurate wrt xt maskdmaskd bility −δ returns hypothesis ∈hthat −εaccurate  conﬁdence accuracy necessarily compromised wrt xt maskd concept class ac    note indistinguishability result imposes curately predictable xt exists accurate  mild restrictions probability distributions dand predictor xt h                 concept class  implies adversarial choice straightforward show following  masking process mask “almost always” prove dis  astrous algorithm attempting make accurate predic theorem  consistent learners  accurate predictors  tions algorithm computationally unbounded consider learning task xt h masking process  known bias probability distribution strong mask  − ηconcealing xt h algorithm  possible concept class cardinality consistent learner xt h algorithm  hypothesis class comprises formulas axt given η extra input allowed running time grows    established impossibility result suggests having polynomially η accurate predictor xt h  infrequently masked target attribute does sufﬁce learn established consistent learning  accurately important infrequently masked tar implies accurate predicting algorithm  attribute right context formalize used provision algorithm  deﬁnition   − ηconcealment masking process allowed running time achieve precision  mask   − ηconcealing learning task xt h determined ε running time dependence η  η maximum value example exm eliminated following true consistent learner  hypothesis ∈h                             uses observations mask                                                        target attribute ii induced predictor access  prvalxt  obs  ∗obs ← maskexm                                                        oracle returns observations distribution maskd            valh  obs valxt  exm    ≥ η                                                        conditioned observations masking    roughly speaking deﬁnition  asks hy target attribute use oracle exempliﬁes fact  pothesis inaccurate agent observe evidence predictor does require computation produce  fact probability generalizes case pac accurate hypothesis observations order  learning inaccurate hypothesis observed obtain “labelled instances” target concept  conﬂict target attribute masked intriguing implication results  note masking process mask existence sistent learner knowledge concealment  guaranteed theorem  necessarily concealing degree masking process able predict accurately  learning task xt h nontrivial concept class albeit “discounted” accuracy factor fact condi                                                        tion ii theorem  suggests consistent learner  theorem  relation consistency accuracy                                                        sense accurate predictor possible given  consider learning task xt h masking process                                                        result sufﬁces restrict attention consistent learning  mask   − ηconcealing xt h                                                        rest study learning partial observations  probability distribution hypothesis ∈h   − εηaccurate wrt xt maskd   − εconsistent xt maskd ii  consistently learnable concept classes  exists probability distribution hypothesis ∈h stronger learnability requirements impose compared   − εηaccurate wrt xt maskd pac learning render learnability impossible   − εconsistent xt maskd easy exercise show typical algorithm pac                                                        learning conjunctions valiant  analysis    assuming physical world does adversarially  hide information interpret result applied essentially unmodiﬁed partial observations  partial explanation possible humans theorem  concept class conjunctions literals  learn rules construct physical theories make accu axt properly consistently learnable xt  rate predictions situations known despite  fact learning takes place evaluated  onetomany reductions  observations inherently missing information     reductions learning tasks used establish    similarly case constructing learners noisy ex certain concept classes learnable standard  amples kearns  assume algorithm given reductions map examples learning task examples  bound concealment degree masking process different learning task case reductions map  allowed time depends bound during learning general partial observations partial observations                                                    ijcai                                                     deﬁnition  reductions learning task xt h kcnf formulas deﬁned exactly substitution                                       r−  reducible set xt    required each polynomially possible clauses  learning tasks ∈ polynomiallybounded theorem  reduction monotone classes learn  exists efﬁciently computable hypothesis mapping ing task xt h reducible learning task               r−                                                     × ×h     →h efﬁciently computable xt   exists set substitutions                                            stance mapping    ∗a →  ∗a  computable time polynomial   ∈       −                                        ii subformula substituted evaluated          following conditions hold                                                                                                    given observation time polynomial iii xt                     ∈h  ×    ×hr−                                                          tuple           obser  basisxt basiscm  basishm      vation obs ∈  ∗a holds gh conﬂicts                 obs                  ∈       −       immediate corollary theorems        xt wrt exists      concept class kcnf formulas properly consistently                                   jobs       conﬂicts xt wrt       learnable constant ∈  ii probability distribution maskd obs                                                          learning monotone formulas      drawn supports xt      ∈r −  exists induced probability employ reductions establish certain learnability results      distribution maskjdj  jobs drawn theorem  total selfreduction learning task                                            dj        cj                              xt h total reducible learning task       supports  xt  size target                                                      dj                           xt    xt  xt       concept xt polynomiallyboundedby    h         ·                                size target concept xt          identity mapping                                                        classes monotone formulas axt   ∈c    roughly conditions guarantee learned hy                                                        proof idea monotonicity formula xt∪c∪h  potheses meaningfully employed original task                                                        assumes   value given obs retains value  ii observations resulting tasks obtained                                                        masked attributes mapped valxt  obs ∈    masking examples drawn appropriate distributions                                                          theorem  establishes surprising fact mono  theorem  learning through reductions consider                                                        tone formulas consistently learning partial observations  learning task xt h reducible set                                                        reduces consistently learning concept class                  cj hj     aj r−  learning tasks xt        concept   complete observations equally intriguing fact hy                                       class  consistently learnable xt potheses learned complete observations result  ∈r− concept class cj consistently learn                                                    ing task apply unmodiﬁed making predictions partial  able xt                                      observations original task preceding facts nicely    following special case particular complement theorem  establishes consistently  observations resulting learning tasks complete learned hypotheses accurate possible                                                        theorems   imply concrete strategy predict  deﬁnition  total reductions reduction total                                                      accurately partial observations simply assign appro  ∈r−     ∗a →  priate default truthvalues masked attributes consistently                                                        learn resulting complete observations em    shallowmonotone formulas                        ploy learned hypothesis unchanged make predictions  establish reduction certain classes formulas technical point worth discussing encoding                                                        value target attribute certain attributes  deﬁnition  shallowmonotonicity aformulaϕ     sulting task observe agent learning resulting  shallowmonotone wrt set substitutions pro                                                       task agnostic fact uses “label” ex  cess substituting attribute xiψ subformula ψ ample involved manner standard use                 ϕ xiψψ ∈m produces monotone formula means test predictions hypothesis makes  denote basisϕ resulting monotone formula result established reduction nontrivial fact  set formulas shallowmonotone wrt set sub hypothesis does depend target attribute  stitutions formula ϕ ∈fis shallowmonotone wrt context original task sense allow  deﬁne basisfm    basisϕ   ϕ ∈f     agent use example’s “label” involved manner                                                        learning require hypothesis eventually employed    implicitly assume substitution process replaces making predictions does depend target attribute  distinct new attributes distinct subformulas ϕandthat corollary follows theorems    resulting formula entirely new attributes pac learnability certain monotone concept classes                      clearly set formulas shallowmonotone wrt                         c∈  set substitutions emphasis deﬁnition  corollary  each concept class conjunctions dis                                                    junctions kcnf kdnf linear thresholds literals  choice  corresponding basis wrt   note instance class kcnf formulas xt properly consistently learnable xt  constant ∈ basis comprises conjunc assuming  ∈cis loss generality sampling  tions basis wrt set substitutions used determine whp target concept tau  polynomially large number attributes tology reduction used case                                                    ijcai                                                     
