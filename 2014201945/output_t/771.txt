                                  interactive    feature   selection             hema   raghavan∗                    omid  madani                       rosie jones      university massachusetts               yahoo                       yahoo            governor’s drive          pasadena ave  pasadena     pasadena  ave pasadena       amherst  ma   usa              ca    usa                    ca    usa          hemacsumassedu               madaniyahooinccom              jonesryahooinccom                        abstract                          prior knowledge features used accelerate                                                        learning example ﬁnd documents topic cars       study effects feature selection hu traditional supervised learning user required      man feedback features active learning set  provide sufﬁcient examples cars noncars documents      tings experiments variety text catego way information      rization tasks indicate signiﬁcant po need user looking documents cars satisﬁed      tential improving classiﬁer performance fea information retrieval setting user asked      ture reweighting achieved selec issue query state words features indicating      tive sampling standard active learning information need feedback      access oracle point impor term document level incorporated fact      tant predictive features consistent pre document classiﬁcation user use keyword      vious ﬁndings ﬁnd feature selection based based search locate initial training examples      labeled training set little effect traditional supervised learning tends ignore prior      experiments human subjects indicate human knowledge user set training examples      feedback feature relevance identify suf obtained work try ﬁnd marriage      ﬁcient proportion  relevant fea approaches incorporating user feedback ma      tures furthermore experiments show   chine learning information retrieval show active      feature labeling takes th time learning dual process – term      document labeling propose algorithm   documentlevel applications email ﬁltering      interleaves labeling features documents  news ﬁltering user prior knowledge      signiﬁcantly accelerates active learning   willingness label possible documents                                                        order build suits needs show hu    introduction                                       mans good intuition important features text clas                                                        siﬁcation tasks features typically words  major bottleneck machine learning applications ceptible human human prior knowledge  lack sufﬁcient labeled data adequate classiﬁer perfor accelerate learning  mance manual labeling tedious costly tech  niques active learning semisupervised learning summary contributions  demonstrate  transduction pursued considerable success access feature importance oracle improve perfor  reducing labeling requirements standard active learn                                                        mance  signiﬁcantly uncertainty sampling   ing paradigm learning proceeds sequentially learn show naive users provide feedback features  ing algorithm actively asking labels instances  accuracy oracle  show  teacher objective ask teacher label relative manual costs labeling features th  informative instances order reduce labeling costs document feedback  show method simulta  accelerate learning little work neously soliciting class labels feature feedback im  supervised learning user teacher queried proves classiﬁer performance signiﬁcantly  aspects class assignment instances exper  iments paper study beneﬁts costs fea  ture feedback humans active learning end experimental setup sec  show  pick document classiﬁcation sebastiani  learn feature selection using oracle useful active learn  ing problem choice represents case super ing sec  sec  show humans iden  vised learning traditionally relies example docu tify useful features show humanchosen features  ments input training users sufﬁcient used accelerate learning sec  relate work                                                        past work sec  outline directions future    ∗this work author yahoo section sec   experimental    setup                              obtained plotting tact  crand correspond  test bed paper comes three domains ing curve using random sampling cm straight line    frequent classes reuters  deﬁciency ratio area  corpus  documents  newsgroups corpus cact cm area crand cm  lower   documents  usenet newsgroups  ﬁrst deﬁciency better active learning algorithm   topics tdt corpus  documents  aim minimize deﬁciency maximize   languages broadcast newswire sources  three corpora consider each topic versus  oracle feature selection experiments  classiﬁcation problem pick binary classi oracle experiments access labels  ﬁcation problems viz baseball vs hockey automobiles documents uses information return list  vs motorcycles newsgroups corpus important features assume parameter   classiﬁcation problems nonenglish stories input oracle oracle orders features  tdt corpus machine translated english decreasing information gain order given set features  features use words bigrams trigrams obtained perform active learning discussed previous  stopping stemming porter stemmer rain section plot cact each value  bow toolkit mccallum     use linear support vector machines svms  certainty sampling active learning scholkopf smola   lewis catlett  svms state art  text categorization fairly ro  bust presence redundant irrelevant  features brank et al  rose et al  uncertainty  sampling lewis catlett  type active learn  ing example user teacher queried  unlabeled instance classiﬁer uncertain  classiﬁer svm unlabeled instances  closest margin chosen queries tong koller   active learner access sub figure  average tact  different values  set unlabeled instances subset called pool number features number documents  use pool size  paper newly la  beled instance added set labeled instances figure  shows plot tact  number  classiﬁer retrained user queried total times features number labeled training examples    deﬁciency metric baram et al  quantiﬁes earnings category reuters axes denote  performance querying function given active learn  respectively number labeled training exam  ing algorithm originally deﬁciency deﬁned terms ples ranges  increments  number  accuracy accuracy reasonable measure performance features used classiﬁcation values    positive class sizeable portion total features dark dots represent maximum ft  case classiﬁcation problems each value dark band represents case  chosen modify deﬁnition deﬁciency deﬁne features used method learning dimen  terms  measure harmonic mean precision sion representative traditional active learning clearly  recall rose et al  using notation similar orig number documents performance bet  inal paper baram et al  let random set ter smaller number features num  labeled instances trand average  achieved ber documents increases number features needed  algorithm trained randomly picked ex achieve best accuracy increases ﬁgure obvious  amples tact  average  obtained using big boost accuracy starting fewer  actively picked examples deﬁciency deﬁned   features increasing complexity model                                                        number relevent features number labeled docu                         ptinitf rand  − tact           ments increase     dt                                              classiﬁcation problems exhibit behavior like                 rand   −  rand             tinit                                 figure raghavan et al  report aver                                                                          rand   obtained large number age deﬁciency  score  labeled examples  randomly picked examples   paper  fig  illustrate point column labeled            act shows performance using traditional active learning  positive negative example t• average features column labeled ora shows performance   computed  trials addition deﬁciency obtained using reduced subset features using oracle  port values intuitively cact curve intuitively limited labeled data little evi                                                        dence prefer feature featuredimension         httpwwwdaviddlewiscomresourcestestcollectionsreuters httpkddicsucieduda  reduction oracle allows learner “focus” di  tabasesnewsgroupsnewsgroupshtml httpwwwldcupenneduprojectstdt mensions matter “overwhelmed”                       class ↓                                                                      act    oracle act    ora    act   ora    act                         reuters                                         news                                          tdt                                             bas vs hock                                    auto vs mot               figure  improvements deﬁciency   using oracle select important features remember  objective minimize deﬁciency maximize  each three metrics ﬁgures bold statistically signiﬁcant  improvements uncertainty sampling using features corresponding columns denoted act  documents  labeled  using entire feature set leads better  scores    numerous dimensions right outset learning problem took  features ranked informa  number labeled examples increases feature selection tion gain entire labeled set case did stem  important learning algorithm data features remain legitimate english words  capable ﬁnding discriminating hyperplane  fea randomly mix features lower  ture weights experimented ﬁlter based methods ranked list show each user feature time  feature selection did work tiny options – relevant notrelevantdon’t  improvements expected given limited training know feature relevant helps discriminate pos  set sizes fig  consistent previous itive negative class measure time takes  ﬁndings sebastiani  determine humans user label each feature show user  identify important features                features list easier lists provide                                                        context serve summary method    human    labeling                                  provides upper bound time takes user judge                                                        feature compare time takes user  consider introductory example user wants judge document measure precision recall  ﬁnd documents discuss cars human perspec user’s ability label features ask user ﬁrst label  tive words ‘car’ ‘auto’ important features features documents feature labeling  documents discussing topic given large number process receives beneﬁt fact user  documents labeled ontopic offtopic given clas viewed relevant documents learning process  siﬁer trained documents classiﬁer ﬁnd proposed user labeling documents  features relevant little labeled data features simultaneously user inﬂu  say  labeled examples classiﬁer able enced documents reads method  determine discriminating features general stringent real case practice ask users  machine learning source labels important highlight terms read documents experiments  active learning scenarios expect labels direction conducted information retrieval croft  come humans valid questions pose  das   humans label features documents  labels  people provide noisy through inconsistent users graduate students employees  learn  features important company authors paper  classiﬁer perceptible human                     graduate students ﬁve science    concern paper asking people feedback public health users familiar use  features word ngrams entire documents computers users understood problem document   expect efﬁcient documents classiﬁcation worked corpora  contain redundancy results oracle experiments users native speaker english topics  indicate great potential hand know distributed randomly considering user expertise  synthetic examples composed combination real each user got average  topics  features difﬁcult label baum lang  overlapping topics users each topic                                                        labeled  users average feedback form asking    experiments  results                         users questions difﬁculty task  order answer questions conducted handed end  following experiment picked  classiﬁcation problems evaluated user feature labeling calculating av  thought perceptible average person erage precision recall identifying  features  street represented broad spectrum prob ranked oracle using information gain entire la  lems set  classiﬁcation problems took beled set fig  shows results comparison  binary classiﬁcation problems remaining  provided precision recall ora  oneversusall problems chose three earnings hurricane cle ranking  features obtained using  labeled ex  mitch talkpoliticsmideast given classiﬁcation amples picked using uncertainty sampling denoted  class        prec        rec      avg time secs   interactive learning algorithm   problem   hum      hum      feat docs   baseball                      let documents represented vectors xi  xixif    auto vs                   total number features each iteration   earnings                        active learner queries user uncertain   mideast                  document presents list features asks   mitch                   user label features considers relevant fea   average                    tures displayed user features ob                                                        tained ordering features information gain ob  figure  ability users identify important features pre tain information gain values labeled instances  cision recall oracle users hum trained classiﬁer labeled instances com  active learner seen  documents average pute information gain used  ranked farthest  labeling times features documents shown margin documents unlabeled set addition  numbers averaged users                      labeled documents using unlabeled data term                                                        level feedback common information retrieval                                                        called pseudorelevance feedback salton   precision recall humans high supporting user labels features considers  hypothesis features classiﬁer ﬁnds relevant af                                                        discriminative features let  ssf  vector  ter seeing large number labeled instances obvious taining weights relevant features feature number  human seeing little labeled data case presented user labeled relevant  ing true experiments additionally precision                                                        set si  si  parameters  recall  signiﬁcantly lower humans indi vector noisier real case  cating classiﬁer like svm needs cause addition mistakes user lose  data ﬁnd discriminatory features   features user considered relevant    column fig  shows time taken labeling presented feature collecting  features documents average humans require  relevance judgments features real life scenario  times longer label documents label features note correspond lazy user labels features  features easier label shown relevant leaves features unlabeled addition  context – lists relevant passages sev making mistakes user labeled feature relevant  eral metrics points discussion user ex past iteration don’t show user feature  pertise time taken label relevant nonrelevant features                                                           incorporate vector follows each xi  reserve future work impor                                                        labeled unlabeled sets multiply xij sj  tant consideration document length inﬂuences  words scale relevant features  document labeling time correlated ij                                                        nonrelevant features set         indicates small increase time  large increase length standard deviations precision scaling important features forcing  recall   respectively different users vary classiﬁer assign higher weights features  signiﬁcantly precision recall total number fea demonstrate following example consider lin  tures labeled relevant based feedback postlabeling ear svm     data points      survey inclined believe individual   labels  − respectively svm trained  caution exercised during labeling process        input learns classiﬁer  −     highlights postlabeling survey features equally discriminative feature                                                         considered discriminative user method  follows average users ease labeling features                           difﬁcult  easy         −  documents  general users poor prior knowledge assigning higher weight “soft” ver  feature labeling process hard sion feature selection mechanism sec   show labels extremely useful classi case oracle knew ideal set features view  ﬁer average expertise expert  indicating set experiments special case    users felt little domain knowledge tasks expect human labels noisy want  assigned proceed use fea zeroout potentially relevant features  tures labeled relevant naive users active learning                                                          experiments  results     human    loop                            make experiments repeatable compute average                                                        formance convenience simulate user interaction  saw sec  feature selection coupled uncer follows each classiﬁcation problem maintain list  tainty sampling gives big gains performance features user considered relevant  labeled examples sec  saw humans  discern discriminative features reasonable accuracy picked algorithm’s parameters based preliminary  approach applying term document test  topics baseball earnings acquisitions using oracle  level feedback simultaneously active learning     features sec presented feature lists used judg pool unlabeled data cohn et al  al  ments obtained sec  each  classiﬁcation querybased learning powerful theory  problems  lists user judged angluin  arbitrary queries difﬁcult answer  topic  tdt topics topic descriptions practice baum lang  popularity  provided ldc simulated explicit human feed poolbased methods motivation studying ef  feature relevance follows topic descriptions fectiveness ease predictive feature identiﬁcation  contain names people places organizations humans application area human prior knowl  key players topic addition keywords edge accelerate learning investigated paz  used words topic descriptions list rele zani kibler  work differs techniques  vant features given lists perform sim use prior knowledge generate hornclause rules  ulated hil human loop experiments  classiﬁ applications beineke et al  uses human prior knowl  cation problems each iteration features shown edge cooccurrence words improve classiﬁcation  user feature exists list relevant features set product reviews work considers  corresponding bit proceed active learn use prior knowledge active learning setting  ing sec  fig  shows performance hil work unique ﬁeld active learning extend  experiments report deﬁciency   query model include feature document level  baseline report results case feedback study human factors quality   features obtained information gain oracle feedback costs major differentiating theme  simulated hil experiments represents work previous work incorporating prior  user  precision recall obtain knowledge did address issue  method oracle expected better plain assumed experts machine learning taking role train  uncertainty sampling  measures reinforcing faith ing schapire et al  wu srihari   algorithm sec  performance hil ex godbole et al  assume knowledge  periments good oracle indicating user topic algorithmic techniques studied  input noisy help improve performance signiﬁ modes interaction differ worth comparison  cantly relative poor performance hil simu wu srihari  schapire et al  prior  lation average measure tdt categories knowledge given outset leads “soft” label  used words topic descriptions ing labeled unlabeled data incorporated  proxy explicit human feedback features plot training modiﬁed boosting svm training  right thil hurricane mitch compar scheme user labeling documents features si  ison tact  shown hil values higher multaneously expect proposed interactive mode  plain uncertainty sampling                  advantage requesting prior knowledge    observed relevant features usually spot outset easier user identifyrecall rele  ted early iterations auto vs motorcycles prob vant features labeling documents collection  lem user asked label  averaged mul presented candidate features work god  tiple iterations multiple users oracle features bole et al  puts emphasis issues  point informative words focuses multiclass training careful analy  termined oracle – car bike asked user sis effects feature selection human efﬁcacy  early iterations label car  proposed method attractive treats features sin  times asked  time label gle term documents labeled humans  word asked user ﬁrst iteration study labeling features documents  closely followed word bike user queried “oracle” setting using actual human annotators  ﬁrst  iterations  time relevant observe improvements using particu  features queried  iterations makes lar method standard active learning single domain  lieve stop feature level feedback  iterations reuters test  stop asking questions features  documents switch entirely documents remains area  conclusions future work  future work                                                        showed experimentally learning labeled    related  work                                      examples good feature selection extremely useful                                                        number examples increases vocabulary feature set  work related number areas including query size needs increase teacher  learning active learning use prior knowledge feature knowledgeable machine learning help accelerate  selection machine learning termrelevance feedback training early stage pointing poten  formation retrieval humancomputer interaction tially important words conducted user study  cite                         naive users performed compared feature ora    proposed method instance querybased learn cle used users’ outputs realistic human loop  ing extension standard “poolbased” active learn experiments signiﬁcant increase performance  ing focuses selective sampling instances paper raises question questions
