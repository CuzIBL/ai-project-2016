                          explanationbased feature construction                             shiau hong lim lilun wang        gerald dejong                                         dept science                                 university illinois urbanachampaign                                  shonglimlwangmreblcsuiucedu                          abstract                          reduce dimensionality input features                                                        highlevel features simplify task complex      choosing good features represent objects nonlinear combinations native features believe      crucial success supervised machine learn effective approach incorporate additional informa      ing algorithms good highlevel features tion form prior world knowledge direction      concentrate information classiﬁca approach gabrilovich markovitch  utilizes      tion task features constructed large repository open directory project aid natural      nonlinear combinations raw native input fea language classiﬁcation address different problem      tures pixels image using  handwritten chinese character recognition additional      nonlinear combinations svms dilute world knowledge derived relatively small imper      classiﬁcation information necessitating train fect abstract prior domain theory      ing examples hand searching     explanationbased learning ebl method dynam      modestlyexpressive space nonlinear functions ically incorporating prior domain knowledge learn      highinformation ones intractable  ing process explaining training examples charac      approach feature construction ter recognition task example know pixels      taskrelevant discriminative features automati input image equally important sun dejong      cally constructed guided explanationbased  used observation learn specialized feature ker      interaction training examples prior domain nel functions support vector machines embody      knowledge show challenging task specially constructed distance metric automatically      distinguishing handwritten chinese characters tailored learning task hand approach automati      automatic featureconstruction approach performs  cally discovers pixels images helpful      particularly difﬁcult complex distinguishing classes feature kernel function      character pairs                                  magniﬁes contribution                                                          know raw pixels intrin    introduction                                       sically distinguish character  sensitivity appropriate features crucial success pixels strokes writing implement  supervised learning appropriateness feature set strokes relation distinguish characters  pends relevance particular task various notions telling learner abstract relationships means  relevance proposed blum langley  perform moral equivalent inventing  kohavi john  statistical tools evaluate tion strokes repeated exposure patterns raw pix  select relevant feature sets available hypothesis els making learning task artiﬁcially difﬁcult  testing random probes crossvalidation unfortunately learns classiﬁer operates directly native features  feature construction strategies focus feature sub appreciates abstractions domain theory  set selection ﬁlters almuallim dietterich  illustrated training examples  kira rendell  wrappers kohavi john  section  makes good highlevel fea  embedded methods guyon et al  pre ture role explanations gen  determined set space deﬁned feature construc eral algorithm section  section  illustrate ap  tion operators markovitch rosenstein  require proach detailed feature construction algorithms chi  training examples evaluate                   nese character recognition section  show    challenging domains “native” observable features experimental results section  concludes  pixel values image highdimensional en  code large irrelevant information  high level features generalization  standard statistical techniques principal compo criteria feature construction usually based  nents analysis pca linear discriminant analysis lda particular set features retains information                                                    ijcai                                                     class discarding irrelevant input object regard particular task  dundant information possible informationtheoretic meth  ods based mutual information channel coding rate  explanationbased feature construction  distortion theorems applied problem battiti                                                        classical ebl “explanation” logical proof   tishby et al  torkkola let                                                    shows class label particular labeled example  random variables represent input label respec derived observed inputs version  tively underlying jointdistribution                                                     weaker require explanation identify potential  shown feder merhav  optimal                                                    low level evidence assigned classiﬁcation label  bayes error upper bounded   fea use training data calibrate evaluate strength  ture    retains information possible                                                    evidence  label     ≤                                                 prior knowledge includes ideal stroke models  represents information loss willing characters roughly sort obtains  tolerate hand discarding irrelevant infor                                                        vector font model stroke connected straight  mation achieved minimizing hf xy                                                    line ﬁnite width  satisfying condition  intuitively im feature construction performed following steps  plies information class label preserved                                                        state abstractly speciﬁcally  irrelevant information withinclass variations                                                        text chinese character domain  discarded show bounds actual risk  binary case appendix regardless hypothesis  explain each training example obtain associa  space used                                            tion assigned label observed na    comparing alternative features each satisfying    tive features mediated highlevel domain theory  hy ≤ hy  x prefer     concepts step each pixel associated  smallest hf xy  alternatively aim stroke line likely constrained  minimizing following functional                      line conﬁgurations match stroke require                                                           ments assigned character          jf hf   xy    hy               using prior knowledge identify  highlevel fea                                                           tures similar categories  high  unfortunately additional knowledge level features different cate  derlying probability distribution impossible accu gories ﬁrst set form reference strokes  rately estimate conditional entropy empirically conﬁdently new test images simi  data training set small example lar stroke present instance type  training examples different simply second set informationbearing character  ﬁne feature based nearest neighbor rule dependent  classiﬁer empirically naive estimator achieve  generated explanations evaluate each po  ˆ          reason believe feature tential similarity statistically using training set         resulting classiﬁer generalize unseen keeping ones detected efﬁciently  examples sense building right feature hard high conﬁdence strokes easily  building right classiﬁer does work unlabeled image form frame refer  inductive bias                                       ence guide high classdistinguishing infor    possible tasks exist fea mation  tures known apriorito jf  ≈  features  capture knowledge patterns unique par  using training examples optimize process  ticular class objects known ﬁnding detection features reference fea  liable way detect extract feature unlabeled tures identiﬁes high information image regions  input problem constructing good feature wrt reference strokes regions chosen  fore viewed problem building good detector larger training set greater variance  “highlevel” features based training data location detection strokes wrt reference  ensure detector produces right output strokes tighter  right reason detects intended highlevel feature generally ﬁnding lines image problematic  high conﬁdence resulting feature lines missed nonlines  generalize                                 process expensive la    verify detector produces right output beled examples during learning phase know  right reason doing statistically possible lines ﬁnd approximate geometrical  require training examples tasks conﬁguration greatly improves lineﬁnder’s perfor  stead available domain knowledge used build ex mance ﬁnd reference strokes  planations training examples used easilyfound similarities categories  verify feature’s output consistent prior classes different classiﬁcation task  knowledge show use idea automatically simple features algorithms  construct features focus informative able differentiate knowledgebased approach                                                    ijcai                                                     unnecessary examine process serve “reference strokes” finding allows target  chinese characters                               region accurately identiﬁed      classifying handwritten chinese                      building explanations     characters                                         prior model each character graph nodes                                                        represent stroke edges represent relationship  ofﬂine handwritten chinese character recognition remains tween strokes each stroke modeled line seg  challenging task large variability writing styles ment  parameters θ denoting center di  similarity characters approaches ei rection length thickness models  ther structural featurebased suen et al thefor handspeciﬁed obtained existing characterstroke  mer extract strokes new input image try match database model need highly accurate ex  extracted strokes known strokelevel models ex planation process relies primarily structural informa  tracting strokes unreliable consequently model tion model used explain each training example  matching process problematic featurebased approaches ﬁnding likely parameters each requisite character  utilize statistics welldesigned features proven stroke  robust features handcrafted general searching best set parameters  easy exploit prior knowledge during learning combinatorial problem general graph  process similar characters difﬁcult differentiate using acceptable size graph small pro  globallydeﬁned features                        cess each character during training    paper focus attention task distin efﬁciency structure graphs trees em  guishing pairs similar characters approach automat ploy dynamic programming approach produce expla  ically constructed features tailored best differentiate nations use algorithm based felzenszwalb  characters                                           huttenlocher  implementation uses automati    consider pair chinese characters figure  cally generated trees focusing horizontally vertically                                                        oriented strokes separately figure  shows character model                                                        example explanation            figure  similar chinese characters      identical leftmost radical ex figure  character model explained example  tracting global feature summarizes character  dilutes classrelevant information concentrated far  left image                                      identifying potential similarities    informative region identiﬁed given models particular pair characters  variability exact location informa form graph matching identify strokes similar  tive region figure  illustrates variability ﬁrst terms location direction length result pro  character pair                                cess identiﬁcation set strokes                                                        match characters refer set matching                                                        set candidates reference strokes figure                                                         shows example                                                          finding efﬁcient reference stroke detector                                                        efﬁcient feature extractor used step             figure  withinclass variability         concerned lines use hough transform                                                        forsyth ponce  particular perform hough    attempting deﬁne absolute set pixels say rd transform input image look local minimum  image left result noisy features speciﬁed region reﬂects variability match  small region risk missing important stroke ing set stroke training data refer  characters large region advantage “hough detector” stroke reliably  focused feature lost utilize knowl tected hough detector use following algorithm  edge similarities characters three  long roughly vertical strokes present characters example wen quan yi project httpwenqorg                                                    ijcai                                                        figure  strokes shown dotted lines                                                                   figure  ideal “target” rectangles  select matching set set reference strokes  reliably detected explanation each training  example used measure accurately hough detec feature point smallest score selected  tor detects particular stroke                      feature points qualiﬁes edge image used                                                        deﬁnition target window figure  illustrates    initialize set reference stroke      each stroke                                                  anchor       range directions offsets          stroke training examples          explanations ﬁnd smallest bounding                                ¯          rectangle center θ ρ¯ width Δθand          height Δρ       each α ∈     each β ∈                                                                                            right anchor          deﬁne bounded region hough space                               ¯            rectangle centered θ ρ¯ width αΔθ            height βΔθ                                 figure  actual “target” rectangles respect         ii each training example                  featurepoints note featurepoints                                                        left edge          search highest peak region          check peak threshold assume jointdistribution location ref             distance τ actual stroke orientation                                                      erence strokes target “window” deﬁned         iii record hit ratio α β percentage ref featurepoints each reference stroke parameterized            erence strokes correctly detected using spec direction offset ri ρiθi obtained            iﬁed parameters                            hough detector each target window parameterized                 ∗     ∗                          α β highest α β         parameters llll correspond left                ∗  ∗       hα β  add              right window example refer                                                        ence strokes target window form jointdistribution    range detector window α β                                                        rrrk joint distribu  algorithm chosen simplicity thresholds τ distance                                                        tion estimated training examples know  hough space optimized using crossvalidation                                                        explanations unlabeled examples    learning ﬁnal feature                        ﬁrst apply hough detector localize reference                                                        strokes rrk ﬁnd maximumlikelihood  reference strokes identiﬁed estimates location window according conditional proba  informative region relative parameters refer bility lr  assuming jointdistribution  ence strokes use simple deﬁnition “informative                                                                           gaussian mean  region” each character stroke                    μr  considered potentially informative stroke expla                  μl  nations know location strokes training  examples using locations ﬁnd smallest rectan covariance                                                                                   Σrr   Σrl  gle includes each stroke overlapping                             strokes rectangles combined single larger            Σlr   Σll  rectangle bounding strokes figure  illustrates conditional gaussian mean    feature points center endpoints                                                                            Σ     Σ−  −      reference stroke receive distance score respect each  μlr    μl    lr  rr   μr                                                     edge target window score deﬁne aδ covariance     combines mean distance   window edge  bσ                              δ                                Σ    Σ     − Σ   Σ− Σ  standard deviation given reference strokes        lr    ll    lr  rr   rl                                                      ijcai                                                      pair  svm    svmebl    pair  svm    svmebl       acknowledgements                                                                                       material based work supported national                                                                                       science foundation award nsf iis                                                                                        opinions ﬁndings conclusions recommendations ex                                                                                        pressed publication authors                                                                                       necessarily reﬂect views national science founda                                                                                        tion                                                                                                           appendix                                        show feature fx small hfxy                                         small hy fx leads better generalization bound                                        terms rademacher complexity speciﬁc case bi                                        nary classiﬁcation make use following theorem                                        theorem   bartlett mendelson  let                                        probability distribution x×±letf set ±                                                                                                                                                  valued functions deﬁned  let xiyi train       table  error rate   fold crossvalidation ing samples drawn according probability                                                         − δ function satisﬁes    experiments                                                                                                                                                          ˆ              rnf      lnδ                                                          fx ≤ pny  fx          evaluate pairwise classiﬁcations                                          difﬁcult pairs characters use etlb database  ˆ  popular database  chinese japanese pn empirical risk rnf  rademacher  characters each  examples ﬁrst learn  complexity  given  literaturestandard multiclass classiﬁer using linear discrim                                                                                                                                      inants identify  difﬁcult pairs pairs charac rnf eσn exn sup  σifxi xxn  ters result greatest confusion expected            f∈f  pairs similar characters use weighted direc  tion code histogram wdh kimura et al  features σ  σn independent ±valued random  features generally best best variables  handwritten chinese character recognition ding  following lemma shows rng bounded    each pair characters learn classiﬁer using hfxy  bounded  linear support vector machine observe support lemma  given discriminative feature ∗  vector machine performs signiﬁcantly better hf ∗xy  ≤ βwherey    takes values  linear discriminants use classi ± rademacher complexity set ±valued  ﬁer input svm wdh features extracted functions bounded follows  target feature window respect  detected reference strokes table  shows results                                                                                                   rng  ≤ β   experiment  challenging pairs characters                            svm generally robust presence                                                        proof concavity entropy shown  irrelevant features managed achieve signiﬁcant                                                        given label  exists  improvement pairs                                                                      ∗                          ∗                                                        pvy     prf xvyy     max   prf  xuy                                                                                    uf ∗xu  conclusion                                                        β                                                              ≥    −                                    believe key generalization incorporate                                                                                               ∗   available domain knowledge learning process vy “typical” value given                                                        given training samples xyxnynandthe  show tailored discriminative features constructed                                 −       comparisons generative models built according rademacher random variables σσnletn                                                                                                   −  prior domain knowledge approach particularly attrac note expected number examples                                                                                    tive problems training examples high     according     expected num                                                                               ∗   level domain knowledge available demonstrated ber examples typical given                                                         −    −                task classifying handwritten chinese characters   nt    pv− nt   pv respectively                                                          consider rademacher average deﬁned theorem                                                        suppose hypothesis space contains ±valued     note cases examples database                              ∗  mislabaled algorithm achieve better results functions hypothesis ∈gthat  corrected decided modify original database attains maximum rademacher average regard                                                                         ∗  retain integrity                                 function chosen label assigned                                                    ijcai                                                     
