  online learning exploiting relational models reinforcement learning              tom croonenborghs jan ramon hendrik blockeel            maurice bruynooghe                                  kuleuven dept science                                   celestijnenlaan leuven       tomcroonenborghs janramon hendrikblockeel mauricebruynooghecskuleuvenbe                          abstract                          vectors contrast relational setting concepts                                                        “on” “clear” intuitive work      recent years growing inter    studies shown learning model      est using rich representations relational world transition function reward function      languages reinforcement learning    beneﬁcial agent model learned      expressive languages advantages   used different ways help      terms generalization reasoning extending speed learning process generating training      existing approaches relational setting examples through simulation actions states happens      nontrivial problem paper present dyna architecture sutton  second allows      ﬁrst step online learning exploita agent reason actions way similar planning      tion relational models propose represen tesauro  allow achieve better rewards      tation transition reward function exploitation mode make better estimates qvalues      learned online present method ex exploration mode using lookahead tdleaf method      ploits models augmenting relational rein baxter et al  example note      forcement learning algorithms planning tech options complementary combined      niques beneﬁts robustness ap                                                          relational setting research far      proach evaluated experimentally                                                        focused modelfree setting recently growing                                                        terest extending methods learning exploiting mod                                                        els world relational setting van otterlo     introduction                                       recent overview nontrivial task use  reinforcement learning agent observe world expressive relational language inevitably implies  perform actions goal maximize obtained models world complex harder learn  reward small domains limited number states apply instance dyna strategy fairly  exact solution methods dynamic programming exist easy learn model keeping probability distribution  methods unable handle realworld states relational case probability distribu  mains large state spaces problems structur tion large space relational states necessary  ing world generalization essential recently lot harder shown ﬁeld statistical relational  strong relational reinforcement learn learning  ing tadepalli et al  framework providing work investigates feasibility beneﬁt using  expressive language describing world gen relational learned models lookahead study  eralization able handle “relational” state spaces strategy beneﬁcial complex worlds  easily represented using vector spaces possible learn perfect model present    example consider blocks world set marlie modelassisted reinforcement learning expres  blocks bbbn block bi stands sive languages ﬁrst learn relational transi  ﬂoor denoted onbifl block bj denoted tion reward function online contribution three  onbibj block bi exactly fold  propose representation transition func  block clear denoted clearbi tion facilitates efﬁcient incremental learning   scribe state set facts true state propose learning exploitation method contrast  clearbonbbonbflclearbonbfl  earlier approaches learning relational models offline  agent clear block bi clear zettlemoyer et al  partial model exploited  block bj ﬂoor denoted movebibj actions immediately avoiding possible initial period  arguments moveflb possible agent gains poor rewards trying learn good  effect number blocks limited model contrast work kersting et al   world clear states easily represented model does need complete note                                                    ijcai                                                     considering rl problem technique does learned online setting module learns func  require resets generative models fern et al tions form probability distributions ss                                                              experimentally evaluate efﬁciency ben rs agivenps pa rmdp  eﬁts approach examine inﬂuence quality  learned model results                  representation world model                                                        said state set ground state atoms  organization section  presents background  using binary random variable possible ground  section  shows transition function relational state atom each time point tadynamic bayesian network  mdp represented learned online section  dbn dean kanazawa  represent transi  scribes using model look steps ahead im tion function action taken time point represented  proves standard qlearning experimental evalua random variable ranges  tion shown section  section  discusses related work atoms represent valid actions reward time  conclude section                          represented realvalued random variable rt                                                          reward current state depends random vari    reinforcement learning mdps                    ables representing state action taken action                                                        taken depends current knowledge agent  reinforcement learning rl sutton barto  current state conditional probability distribution cpd  formulated formalism markov decision pro explicitly modeled chosen action result  cesses mdps need model relational domains agent’s reasoning process current state turn depends  led different formalizations relational mdps rmdps previous state action taken state  fern et al  kersting raedt  kersting speciﬁes layered network structure partial order  et al  use following simple form     random variables dependencies  deﬁnition  relational mdp rmdp deﬁned  variables state assume expert                                                        provides order random variables describing states  ﬁvetuple  pspactrwhereps   set state                                                        random variable depends preceding  predicates pa set action predicates set  constants ground state action atom form order avoid problem learning                                                        structure network problem especially  pccn pn ∈ ps pn ∈ pa ∀i  ci ∈  state state space set ground state atoms hard case online learning revision  action action state ground action atom structure interfere learning conditional    transition function  × × →    probability tables uninvestigated ways  ﬁnes probability distribution possible states cpds state random variables compactly rep  s denotes probability landing state s resented relational probability trees neville et al   executing action state reward function  fierens et al  setting main idea  × →  deﬁnes reward executing certain ac single relational probability tree predicate symbol                                                        ∈ ps tree used model conditional proba  tion certain state                                                                                                      bility distribution tpxs gives ground atom  task reinforcement learning consists ﬁnding predicate symbol probability true  optimal policy certain mdp initially state given current state action athis  known rlagent usual deﬁne func allows maximal generalizations using tree  tion discounted cumulative reward ﬁnd policy                                              π         atoms predicate symbol avoids prob  π  s → maximizes value function  lems arising generalizing predicates differ     ∞  γir  π        π           ≤   π               twhere      ent number arguments different types different  γ  discount factor indicates relative im semantics example figure  shows probability tree  portance future rewards respect immediate rewards cpd clearx blocks world relational    rrlsystem driessens  applies qlearning probability tree used represent cpd reward  relational domains using relational regression algorithm random variable note learned relational probability  approximate qfunction deﬁned               tree does necessarily use preceding random variables                                                                           network splits internal nodes   qs ≡ rs aγ    maxa qs                       s∈s                                learning model  knowing qvalues optimal policy π∗ mdp uninstantiated parts model cpd’s repre                     ∗  constructed π  argmaxa qs        sented relational probability trees learning                                                        model reduces online learning set decision trees                                                                                                online learning relational models               implementation use improved version                                                        cremental relational tree learning algorithm tg driessens et  section propose representation transition al   function reward function easily efﬁciently                                                           learns trees parallel space    van otterlo  overview           efﬁcient slightly timeefﬁcient                                                    ijcai                                                                         state movex cleara       tion indirectly states preconditions action                                                        introduction learning extra binary random vari                         clearstate                                                        able lt intended true state world changes                      yes                            allows prune away actions predicted illegal                               onstate      sampling width parameters                yes                yes              ﬂuence lookahead tree currently investigating                                                        use beamlike randomized searches         clearstate   clearstate          yes                                          empirical evaluation                         clearstate                                               section present empirical evaluation                         yes                         approach want evaluate incremental                                                  relational decision tree learner able build model                                                        world second want investigate perfor                                                        mance speed agent improves adding lookahead  figure  probability tree shows probability want evaluate robustness approach  block clear action movex  exe             state                                      evaluate lookahead beneﬁcial  cuted state   ﬁrst node tree checks block learner able build complete model  clear essentially frame assumption  case clear  domains experimental setup  block moved block clear                                                                                                  original state clear afterstate following experiments rrltg driessens et                                                                  block directly got moved block al  used estimate qvalues                                                        transition function domains learned                                                        easily sampling width used agent learns    learning examples easily created agents function modeling preconditions prune looka  experience note number possible facts head tree exploration policy consists performing  state large generate single step lookahead eliminate inﬂuence speciﬁc  possible examples apply suitable sampling strategy ordering random variables independence assumed                                                        random variables state ﬁgures show    online exploitation relational models           average fold run each test run consists  partially correct transition function ss  episodes average reward  episodes  learned enables agent predict future states following greedy policy percentage episodes  paper investigate use qlearning lookahead reward received used convergence measure  trees agent informed qvalues looking  steps future selecting action blocks world following experiments use  lookahead trees similar sparse lookahead trees used blocks world described introduction stack  kearns et al  obtain nearoptimal policies goal ona goal deﬁned way  large mdps                                           driessens  ona bgoal agent    transition function stochastic ceives reward iff block directly block bthe  uncertainty effect particular action objective stackgoal blocks  complete learning action needs sampled stack block ﬂoor  times obtain accurate value sampling width sw results unstackgoal agent rewarded iff  parameter algorithm starting current state blocks ﬂoor included behavior  root node generate possible action sw comparable stackgoal  directed edges using action label edge during exploration episodes maximum length  node tail edge represents state obtained  steps ones needed optimal policy dur  executing action head node contin ing testing optimal episodes allowed stack  ued tree reaches certain depth qvalues problem blocks world seven blocks ona  deepest level estimated learned qfunction goal number blocks varied each episode  backpropagating values level policy tween seven language bias used  constructed using level qvalues regu previous experiments rrltg algorithm  lar qlearning backpropagating qvalues blocks world driessens   different samples certain action certain state av                eraged value higher level determined using logistics second domain logistics domain contain  bellman equation eq                          ing boxes trucks cities goal transport cer    optimizations scheme possible im  plementation uses preconditions especially useful main difference agent execute  planning domains typically world remains unchanged possible action certain state instead legal ones  agent tries illegal action transition func makes problem difﬁcult                                                    ijcai                                                     tain boxes certain cities possible actions respectively logistics domain results plot  domain load box truck loads speciﬁed ted figure   box speciﬁed truck city  unload box truck takes box truck  moves depot city truck located  possible action moveaction moves  truck speciﬁed city state space ps consists  following predicates box truck box city  truck city predicates make lan  guage bias used experiments tree learning  algorithm test certain box certain truck    ﬁrst setting boxes cities three  trucks goal bring boxes speciﬁc  cities during exploration  steps allowed during  testing maximum length episode  steps  second setting domain extended boxes  cities trucks goal bring three speciﬁc  boxes certain locations  time steps      experiments                                             figure  blocks world ona goal        figure  errors learned transition function                                                                 figure  blocks world stack goal    test quality transition function  achieve apply learned model classiﬁer experiments show number episodes  randomly generate facts predict true needed obtain good policy smaller look  resulting state given state action figure  ing ahead learning lookahead takes additional  shows percentage errors step false positives computation time worlds performing  fp false negatives fn indicate number atoms actions expensive useful evaluate  incorrectly predicted true false respectively total time needed obtain good policy compares  blocks world seven blocks transition func learning figure  shows average reward func  tion learned rapidly optimal  tion time needed learn policy onab  episodes logistics domain ﬁve boxes trucks goal settings produce similar results omitted  cities appears reasonable transition function lack space computational costs learn  learned rapidly reaches optimal performance ing model using lookahead performs better sin    evaluate improvement obtained lookahead gle step lookahead current implementation  planning ran experiments standard learning model world computational bottle  rrltg experiments different amounts looka neck small levels lookahead mainly  head using learned model figure  figure  show sampling techniques create learning examples  results ona stack goals blocks world thing improve future work performing                                                        steps lookahead outperforms standard version    speciﬁc instantiations vary episode episode lookahead                                                    ijcai                                                                      figure  logistics                       figure  reward versus quality model                                                          uses approximations transition reward                                                        function perform hypothetical actions generate extra                                                        dates value function algorithms prior                                                        itized sweeping moore atkeson  extensions                                                        focus hypothetical actions interesting parts                                                        state space mentioned earlier approaches                                                        orthogonal approach explored future                                                        work                                                          learning model environment nontrivial                                                        task relational setting method focuses speciﬁ                                                        cally learning transition functions relational structure                                                        zettlemoyer et al  present method learn                                                        ing probabilistic relational rules given dataset                                                        state transitions applicable large noisy stochastic                                                        worlds main difference learning                                                        approach method directly applicable                                                        inforcement learning does work incrementally       figure  blocks world ona goal    limited domains actions small                                                        number effects    finally evaluate robustness approach set emerging ﬁeld statistical relational learning  experiment generating training examples seen lot work relational upgrades bayesian net  transition function step extending time learn works speciﬁcally sanghai et al  deﬁnes  good transition function  episodes test lational dynamic bayesian networks rdbns way  quality transition function number classiﬁ model relational stochastic processes vary time  cation errors average reward obtained learned combining search rl shown successful  policy using single step lookahead figure  plotted past instance context game playing baxter et  points different learning rates showing ob al indavies et al  online search used  tained reward function quality model approximate value function improve performance  horizontal line shows performance learning agent af continuousstate domains approach seen  ter  episodes using lookahead ﬁgure shows instance learning local search lls algorithm  learned model accurate bene scribed  ﬁcial use lookahead model performs new rrl algorithms proposed lately  inaccurately performance drop                   knowledge ﬁrst indirect rrl approach                                                        related sanner  ground relational                                                        naive bayes net learned estimation value func    related work                                       tion major difference work does  important related work indirect model consider aspects time consider game playing  based approaches propositional setting restrict undiscounted ﬁnitehorizon  ﬁt dyna architecture sutton  mentioned domains single terminal reward failure  introduction agent learns model world success                                                    ijcai                                                     
