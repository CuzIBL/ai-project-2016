             memorybounded dynamic programming decpomdps                           sven seuken∗                              shlomo zilberstein         division engineering applied sciences       department science                      harvard university                       university massachusetts                    cambridge ma                            amherst ma                    seukeneecsharvardedu                       shlomocsumassedu                        abstract                          singleagent control provably intractable ε                                                        approximations hard rabinovich et al       decentralized decision making uncertainty                                                        complexity results optimal algorithms      shown intractable each agent                                                        oretical signiﬁcance consequently researchers started      different partial information domain                                                        focus approximate algorithms solve signiﬁcantly      improving applicability scalability                                                        larger problems scalability remains major research      planning algorithms important challenge                                                        challenge main question – use available      present ﬁrst memorybounded dynamic pro                                                        memory time efﬁciently – remains unanswered      gramming algorithm ﬁnitehorizon decentral                                                        detailed survey existing formal models complexity results      ized pomdps set heuristics used iden                                                        planning algorithms available seuken zilber      tify relevant points inﬁnitely large belief                                                        stein       space using belief points algorithm      successively selects best joint policies each      horizon algorithm extremely efﬁcient hav  paper presents fundamentally new approach      ing linear time space complexity respect come time space complexity existing optimal      horizon length experimental results show  approximate algorithms previous algorithms      handle horizons multiple orders solve problems horizon larger       magnitude larger previously pos goal increase horizon barrier orders      sible achieving better solution magnitude approach based ﬁrst exact dy      quality results signiﬁcantly increase ap namic programming dp algorithm ﬁnitehorizon dec      plicability decentralized decisionmaking tech pomdps  hansen et al  original dp algorithm      niques                                           presented section  builds ﬁnal solution bottomup                                                        starting step policy time step succes    introduction                                       sively working ﬁrst time step iteration                                                        eliminates unnecessary strategies reduce total number   years researchers artiﬁcial intelligence policies kept memory unfortunately algorithm runs  operations research working decision making memory quickly key observation  uncertainty markov decision process mdp algorithm keeps policies memory  proved useful framework centralized deci actually necessary optimal  sion making fully observable stochastic environments nearoptimal behavior main idea identify  ’s partially observable mdps pomdps small set policies actually useful construc  troduced account imperfect state information tion good joint policies achieved using set  general problem results agents topdown heuristics identify tentative policies  cooperate optimize joint reward function hav necessarily optimal used explore belief  ing different local observations problem arises space section  describes set relevant belief states  application domains multirobot coordination manu identiﬁed using heuristics dp algorithm  facturing information gathering load balancing  select optimal joint policy trees belief     decentralized partially observable mdp dec   states leading bounded number good policy trees sec  pomdp framework way model problems  tion  presents details memorybounded dynamic  shown ﬁnitehorizon decpomdps   programming algorithm proves linear time space  nexpcomplete  bernstein et al  decen complexity respect horizon length ap  tralized control multiple agents signiﬁcantly harder plied algorithm set standard test problems    ∗this work author graduate student literature section  details results shows  science department university mas algorithm solve problems horizons multiple  sachusetts amherst                                  orders magnitude larger previously possible                                                    ijcai                                                      related work                                         • markovian transition probability table ssa                                                            denotes probability taking joint action a state  ﬁve years researchers proposed wide                                                                                        results transition state   range optimal approximate algorithms decentral  ized multiagent planning paper focus ﬁnite • Ωi ﬁnite set observations available agent                                                              horizon problems inﬁnitehorizon problems require  Ω⊗i∈i   Ωi set joint observations  different solution techniques important class ﬁnite o  o  on denotes joint observation  horizon algorithms called “joint equilibriumbased search • table observation probabilities ooa s  policies” jesp algorithms search denotes probability observing joint observation o  globally optimal solution instead aim local opti given joint action a taken led state s  mality nair et al  authors introduce idea                                                                                                  focusing reachable belief states approach •  × →is reward function ra  denotes                                                                                                      leads exponential complexity                          reward obtained transitioning state    approach closest work called point taking joint action a  based dynamic programming decpomdps  szer     • denotes total number time steps  charpillet  algorithm computes policies  based subset reachable belief states underlying state decpomdp  trast work approach does employ topdown available agents during execution time  heuristics identify useful belief states base actions beliefs current situation  memorybounded leads doubleexponential worst singleagent setting belief state distribution states  case complexity                                      sufﬁcient optimal action selection distributed set                                                        ting each agent base actions multiagent belief    decpomdp seen partially observ                                                        state – distribution states agents’  able stochastic game posg common payoffs emery                                                        policies unfortunately compact representation  montemerlo et al  approach posg ap                                                        multiagent belief state exists implicitly  proximated series smaller bayesian games interleav                                                        each agent joint policy known entire history  ing planning execution algorithm ﬁnds good solu                                                        local observations taken consideration  tions short horizons runs memory  horizon                                            deﬁnition  local joint policies decpomdp    way addressing high complexity dec                                                        local policy agent δi mapping local histo  pomdps develop algorithms problem classes                                                        ries observations oi  oi ···oit Ωi actions ai  exhibit certain structure becker et al  goldman                                                        joint policy δ  δ  δn tuple local policies                  zilberstein   techniques expo each agent  nential complexity unless interaction agents  limited model particularly useful solving decpomdp means ﬁnding joint policy  agents share payoff function ipomdp maximizes expected total reward policy sin                                         framework gmytrasiewicz doshi   ipomdps    gle agent represented decision tree qi  similar complexity barriers leading authors nodes labeled actions arcs labeled                                                                                                 troduce approximate algorithm based particle ﬁltering observations called policy tree let qi denote  doshi gmytrasiewicz  approach addresses set horizont policy trees agent solution  belief space complexity policy space complexity decpomdp horizon seen vec  remains high limits scalability              tor horizont policy trees called joint policy tree                                                                                                                        δ  qq  qn policy tree each agent                                                               solution techniques decpomdps                 qi ∈ qi policy trees constructed differ                                                        ent ways topdown bottomup goal optimal  formalize problem using decpomdp frame                                                        policy techniques lead ﬁnal policy  work bernstein et al  results apply                                                        goal approximate solution different characteris  equivalent models mtdp commtdp                                                        tics construction processes exploited  pynadath tambe   deﬁnition  decpomdp  ﬁnitehorizon decentralized topdown approach   ﬁrst algorithm used  partially observable markov decision process tuple approach maa∗ makes use heuristic search tech  isb aipΩiort                     niques szer et al  extension standard a∗    • ﬁnite set agents indexed       algorithm each search node contains joint policy tree                                                        example δ horizon joint policy tree expan    •        ﬁnite set states                       sion corresponding search node generates possible                                                                                                         δ    • ∈ Δs initial belief state state distribution joint policy trees horizon  use joint policy tree                                                        ﬁrst time steps using set heuristics    • ai ﬁnite set actions available agent                                                       suitable decpomdps parts search tree       ⊗i∈i ai set joint actions    pruned algorithm runs time quickly      a  a  an denotes joint action        search space grows double exponentially                                                    ijcai                                                    bottomup approach dynamic programming    ﬁrst                                        top−down  nontrivial algorithm solving decpomdps used                                        heuristics                                                                         bottomup approach hansen et al   policy trees              constructed incrementally instead successively com                                                                                       ing closer frontiers trees algorithm starts        frontiers works way roots using dy                                                                                        namic programming dp policy trees each agent                       identify relevant belief points  kept separately construction process  end best policy trees combined produce optimal  joint policy dp algorithm iteration step  horizon policies horizon  constructed baa                                                                     each consecutive iteration dp algorithm given set                       bottom−up                                                               horizont policy trees qi each agent set qi                                   dp  created exhaustive backup operation gener  ates possible depthtpolicy tree makes transi figure  construction process singleagent policy  tion action observation root node tree combining topdown bottomup approach  deptht policy tree exhaustive backup size                                                 topdown heuristics decpodmps  set policy trees qi   aqi  pol  icy trees generated step horizon total agents access belief state  number complete policy trees each agent order during execution time used evaluate           oao  double exponential blowup reason policy trees computed dp algorithm policy trees  naive algorithm quickly run memory good centralized belief state good                                                        candidates decentralized policy obviously belief    alleviate problem algorithm uses iterated elimi                                                        state corresponds optimal joint policy avail  nation dominated policies agents policy                                                        able during construction process fortunately set  tree qi ∈ qi dominated possible multiagent                                                        belief states computed using multiple topdown heuris  belief state mb ∈ Δs ×qj policy                                                        tics – efﬁcient algorithms ﬁnd useful topdown policies  tree qk ∈ qi  qi good better qi test                                                        topdown heuristic policy generated likely  dominance performed using linear program remov                                                        belief state computed obviously does lead  ing dominated policy tree does reduce value                                                        exactly belief state depending speciﬁc  optimal joint policy used iteration technique                                                        problem bottomup dp algorithm handle  signiﬁcantly reduces number policy trees kept mem                                                        dozen belief state candidates  experiments  ory eliminated policy tree set                                                        set reachable belief states identiﬁed using  policy trees containing subtree implicitly eliminated                                                        standard sampling techniques rest section  pruning technique number                                                        scribes useful heuristics identify belief states  policy trees grows quickly algorithm runs  memory small problems                  mdp heuristic   decpomdp turned    detailed analysis construction process shows fullyobservable mdp revealing underlying state af  policy trees kept memory useless ter each step joint policy resulting mdp assigns  policy trees eliminated early joint actions states policy used  reason policy tree eliminated real decpomdp during construction process  dominated belief state dec proven useful identify reachable belief states  pomdps small subset belief space actually likely occur  reachable reason policy tree inﬁnitehorizon heuristic inﬁnitehorizon dec  eliminated dominated possible belief pomdps signiﬁcantly different ﬁnitehorizon dec  agents’ policies obviously during construc pomdps instead maximizing expected reward  tion process agents maintain large set pol time steps agents maximize reward inﬁnite  icy trees eventually prove useless inevitably time period obviously optimal policies ﬁnitehorizon  policy trees considered test domi problems short horizon different  nance inhibits elimination large set policies inﬁnitehorizon problems longer    unfortunately drawbacks pruning process horizon similar solutions gen  avoided algorithm reaches root eral inﬁnitehorizon policies used topdown  policy trees predict beliefs state heuristics identify useful belief states problems  agents’ policies eventually useful sufﬁciently long horizons inﬁnitehorizon decpomdps  observation leads idea combining undecidable generally impossible  topdown approaches using topdown heuristics termine optimal policies ﬁnite inﬁnite horizon  identify relevant belief states dynamic program problems match rewards discounted  ming algorithm evaluate bottomup policy trees time bound difference value guaranteed  select best joint policy figure  illustrates idea recently approximate algorithms introduced                                                    ijcai                                                    solve problems efﬁciently bernstein et al algorithm  mbdp algorithm   resulting policies used simulate mul                                                         begin  tiple runs execution simply stopped  maxt rees ← max number trees backup  desired horizon corresponding belief state reached  ← horizon decpomdp                                                              ←                               ∈  random policy heuristic mdp heuristic          precompute heuristic policies each                                                             qq  ←  inﬁnitehorizon heuristic modiﬁed adding ε     initialize step policy trees  exploration – choosing random actions probability ε                                                                  qtqt ←          qt          qt  step helps cover larger area belief space       fullbackup fullbackup                                                                        particular heuristics suited  seli selj ←  problem reason completely random heuristic   maxtrees  used – choosing actions according uniform      choose ∈ generate belief state                                                                                      distribution provides set reachable belief states  foreach qi ∈ qi qj ∈ qj  dependent speciﬁc problem obviously          evaluate each pair qiqj  respect  random policies achieve low values problem                                                                                              add best policy trees seli selj  belief states policies used                                                                                                  delete policy trees qi qj                                                                           heuristic portfolio usefulness heuristics    qi  qj  ←  seli selj  importantly computed belief states highly                                                                                        select best joint policy tree δ qi qj   pendent speciﬁc problem problems        lution computed mdp heuristic simi  return δ  lar optimal policy accordingly computed belief  end  states useful problems heuristic  focus algorithm wrong parts belief  space consequently select poor policytree candi theorem  mbdp algorithm linear space com  dates decentralized case problem alle plexity respect horizon length  viated using heuristic portfolio instead  just using topdown heuristic set heuris proof variable maxt rees ﬁxed number  tics used compute set belief states each policy trees agent lower  heuristic used select subset policy trees learn limit equal maxt rees number trees  ing algorithms optimize composition heuris heuristics used select kbest pol  tic portfolio aspect scope paper icy trees upper limit equal amaxt reeso                                                        number policy trees backup    memorybounded dynamic programming                 choosing maxt rees appropriately desired upper limit                                                        preset iteration algorithm  mbdp algorithm combines bottomup topdown                                                        policy trees constructed construction  approaches algorithm applied decpomdps                                                        new policy trees algorithm uses pointers pol  arbitrary number agents simplify notation                                                        icy trees previous level able potentially  description algorithm  agents                                                        attach each subtree multiple times instead just il  policy trees each agent constructed incrementally                                                        lustrated figure  efﬁcient pointer mechanism  ing bottomup dp algorithm avoid double ex                                                        ﬁnal horizont policy tree represented ·  ponential blowup parameter maxt rees chosen                                                        cision nodes space grows linearly  backup number policy trees length                                                        horizon length agents onk ·t  −  does exceed available memory iteration  algorithm consists following steps  backup policies iteration performed     creates policy tree sets size amaxt reeso  topdown heuristics chosen portfolio used  compute set belief states best policy tree    pairs belief states added new sets policy  trees finally th backup best joint policy tree  start distribution returned                        t−      theoretical properties                                    t−  overcoming complexity barriers decpomdps  challenging double exponential number  possible policy trees each containing exponential num   ber nodes time horizon accordingly important  feature mbdp algorithm compute figure  exponential size policy represented using  represent policy trees using just linear space   linear space reusing policy trees maxt rees                                                     ijcai                                                      general idea pointer mechanism ap       mabc problem         tiger problem                                                                                               σ  plied algorithms improve horizon value   times  value        times  duces number nodes used ﬁnal policy tree                           oot  ok ·  grows double exponen                             tially – optimal dp algorithm – using pointer                         mechanism marginal effects improvement                                                                                                     space complexity necessary – number                  possible policy trees kept memory – sufﬁciently small                theorem   mbdp algorithm linear time com                  plexity respect horizon length                                                              table  performance mbdp algorithm  proof main loop algorithm lines  depends  linearly horizon length  inside loop oper algorithm entire computational process including  ations independent ﬁxed recursive calls best solution during trial  remaining critical operation line  heuristic run returned ﬁnal solution algorithm  policies precomputed currently heuristic partly randomized obvious average higher  guarantees linear time random policy heuristic recursion depth better solution value  heuristics higher polynomialtime com parameter controls selection heuristics portfo  plexity practice time used heuristics negli lio following experiments used uniform selection  gible topdown heuristic used computed method giving weight heuristics  linear time time complexity algorithm performed ﬁnetuning experimented  linear horizon length                         pruning technique described section                                                         little impact runtime solution quality    recursive mbdp                                   decided include following experiments  pointed earlier effectiveness topdown  heuristic depend speciﬁc problem alleviate  benchmark problems  drawback use heuristic portfolio al applied mbdp algorithm benchmark  gorithm computed complete solution joint problems literature multiaccess broadcast  policy deﬁnitely leads relevant belief states used channel mabc problem involving agents send  heuristic exactly idea recursive mbdp messages shared channel try avoid colli  algorithm applied recursively arbitrary sions hansen et al  multiagent tiger prob  recursiondepth ﬁrst recursion ﬁnished ﬁ lem agents open doors  nal joint policy used topdown heuristic leading dangerous tiger valuable trea  run                                   sure nair et al  table  presents performance results                                                        mbdp algorithm test problems shown    pruning dominated policy trees                   solution value computation time av                                                        eraged  trial runs tiger problem show  described section  exact dp algorithm uses iter             σ  ated elimination dominated strategies reduce size standard deviation  case algorithm  policy tree sets interestingly pruning technique achieved different solution values  trials                                                        mabc problem used  maxt rees recursion  combined mbdp algorithm different ways                                 maxt rees   backup performed pruning algorithm depth  tiger problem used  used eliminate dominated policy trees recursion depth   duces set remaining policy trees consequently evaluate mbdp algorithm compared op  dominated policy tree selected using topdown timal dp algorithm hansen et al  jesp algorithm  heuristics second pruning technique applied af nair et al  pointbased dynamic programming  ter heuristics used select policy trees algorithm pbdp szer charpillet  random  iteration pruning dominated policy trees step policy generating algorithm ofﬂine planning  leads efﬁcient use memory iteration algorithms computation performed centralized                                                        way ﬁnal solution complete joint policy tree                                                        desired horizon executed multi    experiments                                        ple agents decentralized way contrast bayesian  implemented mbdp algorithm performed  game approximation algorithm emerymontemerlo et al  intensive experimental tests algorithm three key  interleaves planning execution allows  parameters affect performance parameter decentralized replanning each step  maximum number trees maxt rees deﬁnes  perform consistent comparison created online  memory requirements runtime quadratically dependent version mbdp fact outperformed algo  maxt rees effect value solution analyzed rithm discussion online algorithms  section  second parameter depth recur scope paper following performance com  sion affects runtime linearly deﬁne trial run parison does include results online algorithms                                                    ijcai                                                    
