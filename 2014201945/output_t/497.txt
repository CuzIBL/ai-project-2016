               word sense disambiguation through sememe labeling                                xiangyu duan jun zhao bo xu                         institute automation chinese academy sciences                              national laboratory pattern recognition                            eastern road zhongguan cun                           xyduan jzhaonlpriaaccn xubohiticiaaccn                       abstract                      proposed including gambl cascaded mem                                                  orybased classifiers decadt et al  senselearner      currently word sense disambiguation wsd                                                   mihalcea csomai  naïve bayes methods yuret      systems relatively individual word sense ex        perts scarcely systems word sense   methods based individual word      transitions senses linearly consecutive                                                   experts motivation model word sense transi     words syntactically dependent words tions different view globally modeling word      sideration word sense transitions impor sense assignments sentence syntactic informa     tant embody fluency semantic expres                                                  tion word sense transitions form linear chain      sion avoid sparse data problem effectively transitions left right word word      paper hownet knowledge base used sentence dependency syntactic information word sense      compose word sense sememes                                                   transitions children parent      transition words’ senses linear chain word sense transitions studied      multiple transitions sememes using simulated annealing cowie et al  unsu     sememe transitions easier captured                                                   pervised graphbased algorithm mihalcea  num     word sense transitions se ber word sense transitions tremendous      memes sememes labeled wsd easy captured order circumvent short     paper multilayered conditional random                                                   coming adopt hownet httpwwwkeenagecom      fields mlcrf proposed model sememe   knowledge base decompose word sense      transitions experiments show mlcrf sememes usually  sememes hownet     performs better baseline                                                   fines closed set sememes word      maximum entropy model syntactic hypernym senses various combinations sememes      features enhance performance significantly various word senses practical model                                                   sense transitions through sememes through pure word    introduction                                  senses  word sense disambiguation wsd important   transition words’ senses  tasks natural language processing given context comes multiple transitions sememes paper   word wsd automatically assign correct propose multilayered conditional random fields   sense word wsd studied years mlcrf model sememe transitions know  current word sense disambiguation systems mainly ditional random fields crf label nodes structures   dividual word sense experts ide et al  sys like sequences trees consecutive labels crf   tems mainly categories target word’s constitute label transitions problem mod  contexts category words windows sur eling sememe transitions viewed sememe   rounding target word category relational labeling problem   information syntactic relation selectional prefer   researches hownetbased wsd wong   ences senses contextual words yang  similar work just like   need disambiguated attract little attention pa pos tagging regarded wsd word sense tagging   senses contextual word target word si adopted maximum entropy approach tag senses sec  multaneously considered try incorporate sense tran tion  comparisons experiments   sitions contextual word target word show mlcrf performs better baseline   framework target disambiguate wong yang’s maximum entropy model syntactic   words through determining word sense transitions hypernym features enhance performance sig     scholars relative researches glob nificantly  ally modeling word sense assignments methods                                                  ijcai                                                 introduction hownet                    sememes denoted yi yim  empirical                                                    parameter depending layers sememes    hownet zhendong dong et  bilingual com bound differentiate word senses each    monsense knowledge base important component                                                    note yi yim  ordered decreas   hownet knowledge dictionary covers ing importance example  sememe     words chinese close  english equiva                                                                  sense definition xi  simplification    lents use knowledge dictionary generate candi hownet knowledge dictionary word senses distinguish    date senses word hownet each candidate sense each  layers sememes sememes taken    word combination sememes example                                                    labels word xi   layers labels word sense    sense definition chinese word “research institute” disambiguation layered labeling problem se   follows                                     meme labeling carried flat sentences sequences       definstituteplace research knowledge    dependency syntactic trees illustrated figure        word sense def split sememes    commas sememes example “instituteplace”  yi     yi      yi    “research” “knowledge” symbols preceding sememes rep   resent relations entry word correspond   ing sememe example symbols “” “”      yi     yi      yi    symbol “” represents agentevent relation “” represents    corelation word sense def glossed “re   search institute” “instituteplace” acts xi       xi        xi    agent “researching” activity corelation             “knowledge”       sememes basic semantic units    nondecomposable feasible extract close set            yi   sememes chinese characters each    chinese character monosyllabic meaning bearing                                                                                       using closed set sememes hownet define               yi           words’ senses through various combinations se   memes                                                          word sense defined sememes according             xi          yk   order sememe word sense definition repre  sents main feature sense word                                                                                   main feature category sense                        example mentioned main feature word   “research institute” “instituteplace” sememes figure  graphical representation words sememe layers   word sense organized order importance sequence representation tree representa   hownet’s point view current symbols tion solid lines show sememe transitions dashed lines show    representing relations omitted                 related links note direction sememe transi      totally hownet contains  sememes tion different sequences trees sequences    classified  main classes including “entity” “event” direction left right trees direction    “attribute” “attribute value” “quantity” “quantity value” child node parent node sememes depend    “secondary feature”  main classes classi word higher layer sememes depend lower    fied hierarchically end  sememes layer sememes clarity show cross links    organized  trees each sememe root sememe detailed description feature set section    hypernym hypernym type    example “instituteplace” hypernym “organiza   sememes layer constitute label configuration    tion” “organization” hypernym “thing” “thing” sentence layer configurations sentence    hypernym “entity” hypernym feature shows usefulness                                                    denoted   aim    word sense disambiguation               ˆ  ˆ                                                           arg max          description                                           yym                                                   propose layerbylayer strategy “mlcrf” solve     task definition                             equation current layer sememe labeling use labeled                                                    prelayer sememes features order strategy    task disambiguate word senses through sememe st layer mth layer introduce mlcrf    labeling formal description task introduce conditional random fields following    follows denotes sentence ith word denoted section    xi  sentence’s length according   hownet sense each xi  decomposed layers                                                   ijcai                                                  introduction conditional random fields                                                                                                          conditional random fields crf lafferty et al                                                                 denotes pair mth layer labels words   undirected graphical models used calculate condi cm  tional probability set labels given set input val clique  ues cite definitions crf mccallum                                                                  …         represent   defines conditional probability proportional prod                uct potential functions cliques graph  … mth layer probabilities respectively each layer crf                                                   model trained individually  using seque ntial crf                                     tree crf                                                sutton et al  use dynamic crf perform mul  set input random variables set tiple cascaded labeling tasks perform ap  random labels           clique potential                                           proximate inference perform exact inference avoid   clique cyx set cliques graph crf                                                    exponential increase propose multilayered viterbi    clique edge node each end  repre   sents labels end nodes clique potential algorithm decoding    defined                                         illustrate multilayered viterbi let look                                                  single layer decoding single layer probable               exp        follows                                                                                arbitrary feature function argu arg max     arg max       ments    learned weight each feature function   total number feature functions clique                                                               denominator om   normalization factor output values                                                                                                                                               ted  sam ul tilayered                         dynamic programming algorithms employed case layer decod ing use pre layer labels   viterbi decoding variant forwardbackward features denominators longer   algorithm sha pereira  computing ex   ease explanation let consider linear chain   pectations normalization                    crf single layer denominator  omitted       kinds crf used sequential layer expressed   crf tree crf sequential crf like traditional linear   chain crf tree crf slightly different linear       sum    sum                                                             sum                       chain crf clique composed current word        sum   sum    preword tree crf clique composed current  denotes forward vector ith position    word child tree crf adopts sumproduct al   gorithm pearl  inference algorithm chain sum  denotes sum vector ’s   graphical models cohn blunsom  used tree                                                    elements crf sum forward vector ’s ele   crf solve semantic role labeling algorithm ments sum possible paths ith position    presented paper sumproduct algorithm                                                             equal     forwardbackward frame each node tree sum    sub forward backward vectors detailed   linear chain probability follows    scription algorithm included paper                                                                            sum   limit paper length                                log                                                                   logsum  logsum       multilayered conditional random fields                                                                                                     logsum  logsum     multilayered conditional random fields conditional                     probability follows                       equation  shows linear chain probability                                                    computed dynamically each time step prob                                                                          ability time based single layer equation                                                                    derive multilayered viterbi algorithm                                                        simplification hownet knowledge                                                                              dictionary word senses distinguish each  lay       denotes product clique potentials                                                ers sememes present layer viterbi sequential    ithlayer  denotes normalization factor ex crf figure  multilayered viterbi extended    ample                                          layer viterbi figure  forward vectors                                                  transition matrices second layer                                                                                                    exp     cm  cm    denotes combined label  layers ith position                                                                                                                                          ijcai                                                denotes second layer label ith position suppose defspace exdefspace following                                                    introduce apply methods layer   label set each layer contains elements si    kk kinds layer combinations              crf       layer viterbi need transition ma trices defspace                     st  forward vectors   layer  defspace candidate layer sememes word                 nd                                 layer sememes appeared definitions       layer normalizing factor                                                  word hownet defspace generates real space     st layer sememes related st                                                  includes labels sememes listed definitions    layer decoding forward vectors transition matrices entry word example chinese word fazhan  senses    neede calculate normalization factor nd layer hownet dictionary “causetogrow” “grow”              forward vector nd layer ith “include” senses single sememes candi         si   si                                         nd         date sememes layer  sememes    position     transition matrix   layer                                         defspace   ith position        logarithm value                                            exdefspace  st layer nd lay er’s ith clique potential respectively  exdefspace extended version defspace suppose                                                    words according hownet dic                                                   tionary  candidate layer sememes sem     initial valuevals    initial state                                                   sem  candidate layer sememes sem                                                         sem suppose appears training corpus    positio                         does appears considering training examples         candidate layer label si        sem sem defspace training procedure                                                   discriminates sem sem sem sem                                     discriminated discriminative training                                                    appears test data can’t disambiguated       valsi  maxvalsi  pairsi si              si                                   overcome shortcoming defspace candidate se                                                   memes word extended sememe        pairsi si                                                     hownet build candidate sememe list concretely       logsum  logsum                                    speaking sememe search diction                                                   ary entries sememes layer              argmaxvalsi  pairsi si        define common polysemous word sememes              si                                   candidate sememe list during training                                       candidate sememes word generated according                                   hand labeled sememe’s candidate list exam    till end node                              ple sem added candidate lists sem                                                    sem word provides positive example sem                                                  figure  layer iterbi algorithm sequential crf si  sem negative example sem       record optimal path                     sem tagged training corpus provides negative                                                    example sem sem       time complexity layer viterbi sen finally sememe’s candidate sememe list                                      st   tence   number   layer labels average  candidate sememes    number nd layer labels length candidate second layer sememes word gener  sentence figure  mainly deals sequence ap ated according word’s entries hownet   plying trees sub tree node layer sememes labeled layer se  introduced problem wsd  candidate meme like defspace used layer crf                                                      phase decoding multilayered viterbi    sense position layer combinations                                                    multiple senses word regarded candidate senses    entire labels                               word special measure like exdefspace taken     reducing number candidate labels     feature set    common crf input variable xi   mlcrf training taken layer layer layer   didate labels  sememes hownet use following factored representation features   wsd problem impractical treat sememes                                                                         candidate labels training single layer crf                        using hownet knowledge dictionary gen  binary predicate input   erate candidate sememes propose meth             ods reduce number candidate sememes current clique   binary predicate pairs                                                   ijcai                                                labels curre nt clique instance  annotations figure  corpus included                                                    chineseldc    “ word position de”                                                       comparison build baseline du      table  presents categories                                                    plicate wong yang’s method wong et al                                           baseline predicts word senses according            px        px      frequ ent sense word training corpus words        seq         seq    tree      tree   appear training corpus choose                                                    sense entry hownet dictionary imitate     si        true     sn       true         wong’s method method word level sense     si               sc                    word sememe optionally combined     sisi         scsn              second sememe defined kind word sense     si                sn                   categorical attribute semantic tag     si              sc                   offtheshelf pos tagger mxpost used tag                                                    senses    si        uniw    sn       uniw            named entities mapped                  biw                 biw         predefined senses example person named entities                  unip                unip        sense “human ” words                  bip                 bip         entries dictionary define sense referring                  trip                trip        synonyms entries end polysemous                             ff  words  corpus average                               cl cr    senses polysemous word                                                        following sections precisions polysemous         table  feature set sequ ential crf tr ee crf words used measure performance words                                                    entries dictionary mapped senses       layer label clique layer                                st                  recall precision note single layer    sememe table  si denotes  layer sememe ith sememe labeling precision rate correct sememes                                     st  word sequential crf sn denotes   layer sem eme corresponding layer sense precision rate cor   cu rrent node tree crf “” denotes hypernym uni bi rect senses sememes    tri denote unigrams bigrams trigrams respectively    denotes word denotes pos subscript seq tree   results mlcrf wsd    represent sequential crf tree crf respectively       st layer nd layer   sense   tree crf denotes child current clique cl cr   left right sibling node denotes parent                                                                            node ff denotes grandfather             seq def      second layer si sn replaced si                                                                            seq exdef   sn pxc excluding true appended si                                                                           sn true value si sn tree def  si  sc additional feature si sn added                                                      treeexdef                 pxc                                                   table  perf orm ance  ml cr ws nu mbers      experiments results                          precisions po lys emous ord wi th  resp ect                                                        layer second layer word sense     experimental settings                       table  shows performance mlcrf wsd “c”   sememe hownet based disambiguation widely used represents common features presented table     chinese wsd senseval  chinese lexical sample hypernym features “h” means common features  plus hy   task text disambiguation sememe level pernym features seq  seq  denotes sequential    scarcely considered furthermore text corpus tagged    def     exdef     wordnet like thesaurus publicly available crf using defspace exdefspace tree def  treeexdef    sentences used experiments chinese lin denotes tree crf using defspace exd efspace table    guistic data consortium httpwwwchineseldcorg  tree crf performs better sequentia   contains phrase structure syntactic trees generate crf shows sense transitions trees easie   gold standard dependency trees using head rules se                                                   captured sequences hypernym features en   lected dependency trees hand tagged senses     words training  hance performance tree crf partly avoids    words testing taggers tag corpus sparse data problem adds noise sequential crf    adjudicator decide tagging results agree exdefspace performs similarly defspace maybe    ment figure ratio matches total number relatively small corpus                                                   ijcai                                               
