                                                spectral learning              sepandar kamvar dan klein christopher manning                     sccm science dept science dept              stanford university stanford university stanford university          stanford ca  stanford ca  stanford ca           sdkamvarcsstanfordedu kleincsstanfordedu manningcsstanfordedu                             abstract  interested reader model          present simple easily implemented spectral          propose markov chain model similar spirit        learning algorithm applies equally          random surfer model page et al  descrip•       supervisory information pairwise link         tion motivated context text categorization        constraints labeled examples unsuper•         model depends notions pairwise data similarity        vised case performs consistently spec•     completely general model collection        tral clustering algorithms supervised case       documents each possibly unknown        approach achieves high accuracy cate•          topic reader begins document        gorization thousands documents given           continues read successive documents chooses        dozen labeled training documents          document read tries read document        newsgroups data set furthermore classifica•         topic prefer documents        tion accuracy increases addition unla•        similar current document mapping        beled documents demonstrating effective use           similarities transition probabilities cho•       unlabeled data using normalized affinity ma•          sen specific choice section         trices symmetric stochastic          transition probabilities define markov chain        obtain probabilistic interpretation    documents collection exist dis•       method certain guarantees performance             tinct topic areas document set generally                                                                  clusters data markov chain composed                                                                  subsets high intraset transition probabilities     introduction                                                low interset transition probabilities refer                                                                  subsets cliques each cliques corresponds topic   spectral algorithms use information contained eigen•    text clustering problem   vectors data affinity itemitem similarity matrix   course natural clusters data need per•  detect structure approach proven effective     fectly compatible document labels said   tasks including information retrieval deerwester     use supervision information sec•  et al   web search page et al  kleinberg  tion  use supervision override similaritybased   image segmentation lmeila shi  word class detec•    transition probilities example disallow transition   tion brew schulte im walde  data clustering     documents known differently  ng et al  spectral algorithms     labeled regardless pairwise similarity   useful unsupervised learning clustering little work   developing spectral algorithms super•      spectral clustering algorithms   vised learning classification      work consider adaptation spectral cluster• section discuss process turning affinity   ing methods classification present method   matrix pairwise document similarities normalized   combining item similarities supervision information    markov transition process eigenvectors   produce markov transition process data items     used detect blocks nearblocks cor•  markov process interested reader model ap•  respond clusters data   peal special case text clusteringclassification                                                                       note important difference way   algorithm incorporates supervisory information                                                                         models used random surfer model used   available form pairwise constraints la• left eigenvector transition matrix indicates relative   beled data empirically algorithm achieves high time process spends each data item   accuracy supplied small amounts labeled data    hand interested right eigenvectors transition matrix   section  small numbers pairwise constraints sec•     straightforwardly relate nearblock structure   tion                                                        transition matrix       learning                                                                                                                                                                                                    spectral learning kmeans                                                                          news                                                                            news                                                                           lymphoma                                                                           soybean                                                                        table  comparison spectral learning kmcans                                                                    details differ algorithm shown    clustering                                                                  figure  algorithm similar algorithm       treating each row point rk cluster clusters presented ng et al  njw au•       using kmeans sensible clustering algorithm thors fact difference type normalization       assign original point xi cluster row used differences algorithm        assigned cluster                              mncut meila shi  normalization    classification                                           different additionally mncut docs row                                                                  normalize step  table  describes different types      represent each data point row xi                                                                  normalizations mentions algorithms use      classify rows points rk using reasonable clas•       sifier trained labeled points                      noted data sets distant                                                                  outliers additive normalization lead poor perfor•     assign data point class  assigned                                                                 mance additive normalization out•               figure  spectral learning algorithm            liers clique clusters                                                                  represent outliers true clusters dataset   algorithm normalization va                                distant outliers divisive normalization likely   mncut divisive  di                                       lead better performance   njw symmetric divisive  ad   lsa                                                   parameter selection   sl normalized additive —             — dd                                             max        max      importance parameter selection overlooked           table  normalizations used spectral methods      presentation standard spectral clustering methods                                                                  different values results spectral clustering    calculating transition matrix                          vastly different ng et al  parameter   order fully specify datatodata markov transition   chosen based value gives distorted   matrix map document similarities transition prob•  clusters   abilities let affinity matrix documents     text experiments data termdocument   elements aij similarities documents       matrix similarity function gave pairwise cosine                                                                  similarities entry  set zero   given documents  points xi                                     ij                                                                 nearestneighbors reverse threshold•  distance function  common definition aij                                                                  ing affinity matrix manner useful spectral                  free scale parameter lsa       methods empirically work better zeros   deerwester et alt  given rownormalized       affinity matrix pairs items   termdocument matrix defined       class experiments chose     cosine similarity matrix salton                       learn optimal manner ng et al      map document similarities transition probabili•    learn optimal scale factor   ties ways define meila   shi  diagonal matrix elements       empirical results                  corresponds transitioning proba•                                                                 compared spectral learning algorithm figure    bility proportional relative similarity values alternatively                                                                  kmeans  data sets   define fiedler      chung  dmax maximum rowsum             •  newsgroups collection approximately    transition probabilities sensitive absolute sim•  postings each  usenet newsgroups   ilarity values example given document similar   •  newsgroups   newsgroups scicrypt   interested reader reading       talkpoliticsmideast socreligionchristian   document repeatedly docu•                                                                   • lymphoma gene expression profiles  normal   ment normalizations plausible                                                                      malignant lymphocyte samples  classes   chose slight empirical performance ben•                                                                     diffuse large bcell lymphoma  samples   efits data                                                                      nondlcbl  samples alizadeh      meila shi  shown probability   transition matrix markov chain strong cliques                                                                    httpwwwaimitedujrennienewsgroups total   piecewise constant eigenvectors suggest   documents documents stripped headers stopwords   clustering finding approximately equal segments  converted lowercase numbers discarded words   eigenvectors algorithm uses general method      occur   documents removed                                                                                                                  learning    • soybean soybeanlargh data set               define affinity matrix previous algo•       uci repository  classes                                   rithms   results shown table  numbers reported        each pair points    adjusted rand index values hubert arabie             class assign values atj  aj —    clusters output algorithms rand index fre•     likewise each pair points  differ•  quently used evaluating clusters based       ent classes assign values aij — aji —    pairs placed different clusters par      titionings adjusted rand index ranges —     key property expected value random     gives symmetric markov matrix describing   clustering  result spectral methods generally per• interested reader process uses supervisory infor•  form better kmeans consistent results ng mation present data similarities   et ai  brew schulte im walde          strength model lies fact incorporates unla•  cases poor performance kmeans reflects inabil•    beled data majority classification models deal   ity cope noise dimensions especially case   strictly labeled data benefit additive normal•  text data highly nonspherical clusters case  ization affinities adjusted samelabeled     composite negative cluster lymphoma how•         pairs higher equal mutual transition   spectral learning outperforms kmeans soybean     probability unlabeled pairs necessarily   datasct lowdimensional multiclass data  case normalization schemes   set                                                                   spectral classification algorithm    spectral classification                                      natural classes occur data markov                                                                  chain described cliques furthermore   previous section described clustering data set cliques stronger number labeled doc•  creating markov chain based similarities data  uments increases given model wish categorize   items analyzing dominant eigenvec•   documents assigning appropriate clique   tors resulting markov matrix section show  markov chain spectral clustering methods given sec•  classify data set making changes    tion  adapted classification replacing final   modify markov chain using class labels     steps clustering spectral space steps shown   known override underlying similarities second     figure  classify spectral space   use classification algorithm spectral space key differences spectral classifier   clustering algorithm                                        clustering algorithm transition matrix incor•                                                                 porates labeling information wc use classifier    modifying interested reader model                    spectral space clustering method novel   model described section  modified incor•     algorithm able classify documents   porate labeled data following simple manner     similarity transition probabilities known sub•  interested reader happens labeled document     sets model incorporates labeled   probability choose labeled document   unlabeled data improve addition   category high probability labeled data addition unlabeled data   choose labeled document different category low   observe empirically section    zero transition probabilities unlabeled documents   proportional similarity current source doc•  empirical results   ument current document labeled         cliques      wc wish create markov matrix reflects mod•   suggested section  markov chain de•  ified model propose doing following manner    scribed cliques subsets nodes   using normalization introduced section       graph internally fastmixing mutu•  similarity functions maximum pairwise similarity value    ally fastmixing figure  shows thresholded sparsity pat•   minimum similarity      tern affinity matrices  newgroups data set   like say points class maximally    labeled data added left matrix affinity matrix   similar points different classes minimally sim•  labeled data underlying similarities show   ilar                                                          blocklike behavior weakly extent unla•                                                                 beled data gives blocklike affinity matrix clusters natu•     thcrc arc  instances  features  different rally exist data basis spectral clustering   classes nominal hamming distance used             subsequent matrices increasing fractions data la•     sets kmeans numbers low  beled effect adding labeled data sharpen   partially illusory zeroed expectation adjusted                                                                  coerce natural clusters desired classes   rand index partially real consequence sparse high  dimensionality text data better kmeans results text typ• labels added blocks clearer cliques be•  ically require kind aggressive dimensionality reduction come stronger limit  labeled data   usually lsa spectral method careful feature selection interested reader accidently jump document                                                       topic       learning                                                                                                                                                                              documents incorporate                                                                     figure shows effect supplying increasingly                                                                  large fractions  newsgroups data set labeled                                                                  training instances using remainder data set                                                                  test instances spectral classifier outperforms naive                                                                  bayes substantially little labeled data                                                                  figures show graphs  news•                                                                 groups data set spectral classification performs                                                                  especially  data labeled                                                                  noticed data set naive bayes out•   figure  categorization accuracy  newsgroups task performs spectral classifier strongly supervised case    number unlabeled labeled points increases  labeled data strength spectral learning lies     labeled documents given number unlabeled docments incorporating unlabeled data strongly super•   used training set training set  news• vised case value    groups given fraction labeled cases test set    given run consisted documents  newsgroups    labels known during training run           spectral space                                                                  investigate behavior spectral classi•                                                                 fier wc performed following experiment took                                                                   newsgroups data labeled various fractions each                                                                   classes plotted each documents position                                                                  resulting dimensional spectral space space                                                                  rows matrix defined spectral learn•                                                                 ing algorithm figure  shows dimensions                                                                    plots labels data does tightly cluster                                                                  add labels circled points things happen                                                                  labeled points close samelabeled points away                                                                  differentlabeled points generally outside    figure  categorization accuracy  newsgroups task hubs markov process second    unlabeled data fraction labeled data pull unlabeled points radially    increases                                                                  effective pull classes apart                                                                  classes naturally strong clusters un•   accuracies                                                                  labeled data    validate utility spectral classification performed    following experiments  newsgroups data set      built batch classifiers standard     related work    multinomial naive bayes nb classifier standard add                                                                 yu shi  spectral grouping method    smoothing second spectral classifier de•                                                                 grouping bias presented allows    scribed used singlenearest neighbor classifier                                                                  toplevel bias labeled data formulate problem    perform final classification spectral space                                                                  constrained optimization problem optimal par•   affinity matrix thresholded cosine similarity be•                                                                 tition sought subject constraint normalized    tween documents   note thresholding im•                                                                cut values nodes preassigned   portant weakens effect outliers furthermore                                                                  class    saves space computation time resulting affinity    matrix sparse                                               main drawback algorithm yu shi      split data labeled training set unla•   constrains samelabeled data points   beled test set classification spectral classifier pro• algorithm present benefits zeros spar  cessed training test set evaluated sity pattern introduced differentlylabeled pairs further•  test set                                                 noted multiclass case labeled      figure shows effect using sample  docu•  sets combinatorialy tend embody differentlylabeled    ments  newsgroups data labeled training        pairs samelabeled pairs drawback us•   set increasing number unlabeled documents      ing information given differentlylabeled points   test set accuracy nb classifier course   trivial partition points cluster satisfy   constant sampling variation discards unlabeled constraints points labeled fact   data spectral classifier accurate nb     data labeled likely partition   classifier given sufficiently additional unlabeled   grouping bias algorithm trivial partition                                                                  figure shows spectral classifier outperforms        each document similar  documents grouping bias algorithm  newsgroups data   similarities appropriate row column set fact grouping bias started performing slightly   entries                                                  worse large fraction data labeled                                                                                                                  learning  figure  three classes  newsgroups data set spectral space increasing mounts labeled data classes scicrypt    pluses talkpoliticsmidcast dots socreligionchristian squares labeled points circled graphs show sparsity    patterns associated affinity matrices        constrained spectral clustering  mustlinks items known class                                                                     cannotlinks items different classes   recently constrained clustering constrained clustering allows exploratory data   wagstaff cardie  klein et al  analysis prior knowledge class la  volves clustering types pairwise constraints bels classifier presented section  easily modi      learning                                                                                                              
