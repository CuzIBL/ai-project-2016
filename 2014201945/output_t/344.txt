          managing domain knowledge multiple models boosting                            peng zang                                  charles isbell                     college computing                         college computing                         georgia tech                                 georgia tech                      atlanta ga                             atlanta ga                   pengzangccgatechedu                        isbellccgatechedu                        abstract                             mboost                                                        adaboost schapire singer  freund schapire      present mboost novel extension adaboost   ensemble learning technique iteratively      extends boosting use multiple weak learners structs ensemble hypotheses applying weak learner      explicitly provides robustness learning mod repeatedly different distributions data distributions      els overﬁt poorly matched data chosen focus “hard” parts data space      demonstrate mboost variety problems   hypotheses generated far perform poorly      compare cross validation model selection                                                        hhht set hypotheses generated adaboost                                                        ﬁnal boosted hypothesis hxsign                                                         sign∑t  α xwhereα  denotes weighting coefﬁ    introduction                                                                                                              cient ht  recent efforts ensemble learning methods shown em described single learner weak learner  pirically use methods models “bag” learning models example  powerful using single method example choosing decision tree perceptron  caruana  shows constructing ensemble classi weak learner use learner contained  ﬁer library  models ranging presumably choosing better given iteration  cision trees svms produces classiﬁers outperform using composite weak learner requires atten  best single model oddly boosting schapire — tion bag chooses best model  particularly popular ensemble technique strong theo combines models way introduces additional  retical empirical support—is rarely used fash inductive bias longer boosting decision tree  ion principle boosting able handle multi perceptron potentially complex ensemble  ple learning methods automatically simply using learner learner happens include decision tree perceptron  chooses multiple weak learners ensure additional inductive bias introduced  hand learner introduces additional complexity propose boosting algorithm arbi  compound overﬁtting effects                  trator acting mechanism choosing model    paper present new boosting algorithm use  mboost incorporates multiple models boosting ex mboost algorithm explicitly supports multiple weak  plicitly particular extend adaboost schapire learners formalizes notion boosting arbitra  singer  freund schapire  act arbi tor each round each weak learner proposes hypothesis  trator multiple models contributions mboost selects “best” “best” deter                                                        mined boosting reweight distribution    • principled technique extending boosting enable round round allowing mboost arbitrate weak      explicit use multiple weak learners           learners introducing inductive bias    • technique controlling weak learners overﬁt intrinsic boosting framework      poorly matched data embody poor boosting known susceptible overﬁtting      main knowledge                                  weak learner overﬁts imagine example weak                                                        learner rote learner hash table training er  sections follow ﬁrst present mboost ror zero generalization  comparing adaboost analyze theoretical matter rounds run rote learner gen  practical consequences mboost validate analysis erate hypothesis assuming noise  presenting empirical results including compar labels result ﬁnal boosting classiﬁer show  ison mboost model selection cross validation fi lack generalization capabilities using multiple  nally situate work literature conclude weak learners compound problem  discussion                                        boosting’s susceptibility kind overﬁtting lies                                                    ijcai                                                    algorithm  mboost                                    given coordinate adaboost taking largest                                                        possible lossminimizing step coordinate  require weak learners bbm     data  ∈ χy ∈−      boosting extended multiple weak learners                                              optimal weight chosen optimal hy    initialize din     −                             pothesis set proposed hypotheses cho     split data randomly parts dtrain sen words ﬁnd best coordinate                                                      largest component direction       dvalt                                                        gradient addition optimizing lossminimizing step     train learners bn dtraint generating hy                                                          know learning models aprioriwe       potheses hn                                                        precluded analytical optimizations ﬁnd best     choose hypothesis ht  argminz hihn                                                      hypothesis instead perform exhaustive search     choose αt ht usual                                  −α                  each hypothesis proposed different models ﬁnd     update dval  idval yiht xi                                                   optimal weight calculate resulting loss hypoth                                 ∑            normalize dt   dt  zt dt         esis lowest loss chosen number                                          end                                           weak learners typically small computation loss                                                      requires hypothesis predictions training cost   return final hypothesis hxsign∑ αt ht   step low                                                      applying technique adaboost        χ                                                                                            −       input space zt normalization                              ∑n      yi xi                                                        best hypothesis minimizing     stant                                                                               α                                                            yi true label ∑t ht hy                                                        potheses ensemble classiﬁer cost minimization  use training error hypothesis evaluation grows linearly number rounds run  training error overlyoptimistic poor measure rounds expensive fortunately minimizing  true performance avoid “stronger” learners  equivalent minimizing zt  normalization  mboost divides data round training vali stant adaboost round  dation sets using evaluate generated hypotheses  approach yields accurate measure hypothe proof particular round update rule gives  ses’ performance turn used choose best hy                    −  α                                                                                 yi ht xi   pothesis weight additionally mboost reweights       dt   dt        zt                                                                                         ∑ − α     distribution data validation set points               yi ht xi  training set suspect prediction                                                                                                                       m∏t  zt  overﬁtting causes reweighting points incorrectly                  −                                                                                 yi xi  imply learned reweighting val                                           idation set avoids making mistake note                 m∏t zt  vergence algorithm does change points                                                                                        eventually reweighted points training set zt normalize dt   round validation set                     − α                                                                                     yi tht xi    combination techniques means mboost                zt  ∑dt                       accurate measure hypotheses’ gener                                                                                                    alization errors “inside” bag compet           ∑dt  zt                   ing models mboost choose hypothesis directly                                                                                                −      minimizes loss function net effect users                 yi xi                                                                           ∑                             insert variety models boosting advantage                   t−                                                                               m∏    empirical power using multiple models mitigat                     ing effects poor weak learners overﬁt em  body incorrect domain knowledge                                                                                      algorithm  shows pseudocode mboost note                            −                                                                argming   argmin∑ yi xi         mboost shown derived adaboost boost                                                                                    ing scheme similarly adapted                                                                                              −                                                                              argmin         ∑e  yi xi                                                                                       ∏t−    analysis implications                                                                                                                                            using multiple weak learners                                         argmin zt                     boosting viewed coordinate gradient descent  method searches set hypotheses  combined weights minimizes loss each note mboost formulation zt deﬁned  round adaboost hypothesis ht coordinate given somewhat differently adaboost  adaboost optimizes weight αt coordinate mboost’s use validation set changes loss function  minimize loss function interpretation process difference slight proof applies                                                    ijcai                                                      finally worth noting mboost choosing validation sets chosen randomly determine  multiple hypotheses weak learner requirement learners “exhausted” consecutive rounds  pac sense relaxed ensemble learners hypothesis beats random required  act weak learner individual learner   note rounds distribution static                                                        mboost essentially performing internal montecarlo    using internal validation set                 crossvalidation each weak learners run  mboost needs accurate error estimate candidate hy rounds acquire tighter bounds each learner’s true error  potheses each round following langford  model eventually show weak  hypothesis error biased coin ﬂip bias learners does better random case mboost  true error number tails number correct pre continue weak learners better  dictions number heads number incorrect random conﬁdence case mboost halts  predictions    probability makes fewer mistakes  error bounds  predictions given true error rate ε given mboost approximately reduced adaboost  binomial  cumulative distribution function cdfknε consider weak learner trained data  ∑k   εi − εn−i                                    set round does following                                                       largest true error rate probability • splits data set training set validation set  having fewer mistakes predictions dtrain dval   δ                       ≥ δ                                         maxr  cdf   refer •                                                                         δ               trains each member learners bm dtraint  maximum reasonable true error mrte  bound                         true error formed pε ≤ mrteknδ ≥ generating hypotheses hm   − δ                                                  • simulates each hypothesis boosting reweighting    mboost hypothesis used mrteknδ dvalt chooses hypothesis hbestt low   hypothesis used upper est hypothetical normalization constant  bound true error  suggesting high •            ∈                ∈                                                            returns ht   hbestt dtraint wherei  conﬁdence hypothesis better random       indicator function returns  pred    recall view boosting coordinate gradient icate true  words return  descent method optimizing particular loss function use                                                                             ht copy hbestt asked  validation set hypothesis selection weighting dis predict data trained abstains  tribution reweighting essentially amounts different loss               −  ∑     ∈                            applying adaboost weak learner  function ∑n yi ht xi dtraint  mboost’s loss function                                                    struct learner  essentially mboost  reﬂects understanding real goal       ada                                                        sole difference mboost’s ﬁnal hypothesis hx  ensemble’s generalization error want avoid mis  α                     α                                                           sign ∑t hbestt instead sign ∑t ht   led atypical results training data use test set             evaluation allows mboost effectively apply weak learners ht hbestt differ predictions  prone overﬁtting “strong” learner   randomlydrawn ﬁnite set dtraint                                                        hx hx differ points union    easily imagine extending use single   ternal validation set perform fold crossvalidation dtrain dtraint  subset overall training data din  round boosting note treat limit size instance space approaches inﬁn  ment aims ﬁnding best weak learner best ity probability predicting data point seen train                                                        ing vanishingly small formally lim prx ∈  hypothesis building ensemble hypotheses goal                             χ→∞  ﬁnd best hypothesis round              lim hx−hx  whereχ instance    finally note internal validation mildly help  χ→∞  ful overﬁtting noise error side effect space point randomly drawn χfurther  boosting’s emphasis “hard” examples noisy situa • sum errors bounded size train                                                                          tions noise noise nonsystematic mboost deter ing set ∑ − hx   generated hypotheses predict noise      x∈χ  better random note section •      cause mboost halt                                       differ randomly drawn ﬁnite                                                            set errors hx − hx systematic                                                                                                  automatic stopping condition                         words ∑h − hx≈ hx  natural mboost stop weak learners represent decision boundary                                                              perform better random standard boosting uses training ≈ hx lada approximately mboost  data evaluation easier weak learners reduction allows inherit generalization  perform better random mboost running error bounds derived adaboost par  experiments mboost usually stops condition ticular vctheory based generalization error bound    mboost determine weak learners freund schapire aswellasthemargin  perform better random single round based generalization bounds schapire et al   generated hypotheses perform insufﬁcient training schapire singer  applied straightforwardly                                                    ijcai                                                      empirical evaluation                                                             table  cv accuracy mboost bestada  performed sets experiments examine mboost’s datasetlearner  mboost   bestada  pvalue  performance                                                         adult                             effects domain knowledge                       breastcancer                                                                         crx                             set experiments focus exploring mboost’s basic                                                         horsecolic                     ability exploit multiple domain knowledge sets encoded                                                         ionosphere                     weak learners validation set used used  synthetic dataset composed three gaussians  points labeled according three different lines  each gaussian weak learners used ﬁrst table  cv accuracy mboost bestcvada  simple perceptron second gaussian estimator com datasetlearner mboost bestcvada pvalue  bined perceptron represents accurate domain adult                     knowledge case                                        bcancer                         figures easy dataset crx                     adaboost mboost able approach perfect horsecolic                  classiﬁcation mboost leveraging additional ionosphere                domain knowledge second weak learner able  ﬁve times faster    explore effect inaccurate domain knowledge each weak learners turn total twentyﬁve  replace second weak learner learns vertical runs best adaboosted model according  bands width  hypothesis class yield help ported training error chosen used second  ful hypotheses simulating poor domain knowledge learner bestada note computational complexity  case mboost discards hypotheses generated sec learners equivalent onm  ond weak learner mboost adaboost perform identically number rounds number weak learners  round round                                    table  shows accuracy results ﬁve datasets                                                        reported accuracy scores use subsample crossvalidation     mboost versus adaboost individual             times each cross validation round randomly       learners                                         split data training validation sets   set experiments explore effect boost training set use variation cross validation through  ing multiple models versus boosting each model evaluation performed number  dividually want determine heterogeneous ensem times ﬁnd  rounds typically produce sam  bles superior homogeneous ones performed ples signiﬁcance tests  experiments ﬁve largest binary classiﬁcation uci used oneway anova  tests determine                                                                                                           datasets set twentyﬁve weak learners used reported accuracies statistically signiﬁcant pvalue                                                        datasets show mboost performing statistically    • naive bayes learners uses empirical probabil signiﬁcantly better      ities use mestimates    results quite surprising detailed analysis shows       used naive bayes learner provided bestada suffered signiﬁcant overﬁtting exam      orange data mining package                   ple adaboosted svm report higher accuracy rate    • knn learners      adaboosted fact adaboosted gener      used knn learner provided orange data min alized better      ing package                                        imagine using crossvalidation avoid                                                        problem end create learner bestcvada    • decision tree learners minob js min                                                        bestada uses tenfold cross validation      imum number examples leaf set                                                            select best adaboosted weak learner note       options set defaults used                                                        creases computational cost bestcvada factor       quinlan’s implementation                                                        present results comparing bestcvada mboost    • svm learners parameter cost table       misclassiﬁcation set        results line expectations commonly         svms used rfb kernel reported literature best singlemodel booster      options set defaults used lib quite effective mboost using      svm library chihchung chang chihjen lin tenth computational cost perform better three  compared learners ﬁrst mboost given ﬁve data sets good rest heterogeneous  twentyﬁve weak learners set run rounds ensembles appear perform better homogeneous ones  second learner runs adaboost  rounds mboost able advantage effect combine    adult data set quite large used subsample additional cv rounds performed ascertain statistical   examples                                        signiﬁcance                                                    ijcai                                                                                                                             figure  comparing adaboost mboost gaussian dataset left shows training accuracy theoretical bound right  shows training testing accuracy averaged  runs          table  cv accuracy mboost mboostacc        table  cv accuracy mboost bestcvind   datasetlearner mboost   mboostacc   pvalue          datasetlearner  mboost   bestcvind  pvalue   adult                                 adult                            bcancer                              bcancer                         crx                                   crx                             horsecolic                           horsecolic                      ionosphere                            ionosphere                        ensembles appropriately                             table  cv accuracy mboost mboostauto                                                         datasetlearner  mboost   mboostauto  pvalue    mboost weak learner arbitrator                 adult                           perform set experiments explore effects bcancer                  using mboost’s loss function weak learner arbitrator crx                      compare versions mboost ﬁrst standard horsecolic                    mboost using exponential loss function arbitrator ionosphere               second mboostacc using validation accuracy weak  learners boosters run  rounds    table  shows versions’ performance quite individual model fold cross validation mboostauto  similar dataset shows difference typically requires ﬁve times computational cost  statistically signiﬁcant mboost outperforming mboost table  shows mboost performing better  acc exploration ran experiment ﬁve datasets equivalently rest  allowed versions run exhaustion experi table  accuracy scores degrade  ment performance differences disappeared unsur run mboost exhaustion mboost quite resistant  prising performance difference terms overﬁtting factors combined mboost’s low com  speed convergence ﬁnal asymptomatic performance putational complexity lead suggest alternative  exponential loss function aggressive rea crossvalidation model selection instead trying  sonable show early gains          models ﬁnding best suggest boosting models                                                        synthesizing ensemble    mboost versus best individual learners  perform series experiments comparing mboost  related work  best individual model selected  fold cross mboost’s use multiple weak learners similar boost  validation bestcvind ran versions mboost ing extension known localized dynamic boosting mo  running  rounds automatic stop erland mayoraz  meir et al  avnimelech  ping condition  rounds mboost intrator  maclin whereαt —the weight  computational complexity selecting best hypothesis round t–is generalized function depen                                                    ijcai                                                    
