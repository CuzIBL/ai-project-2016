                   predicting preventing coordination problems                                  cooperative qlearning systems                                      nancy fulda     dan ventura                                       science department                                         brigham young university                                              provo ut                                     fuldabyuedu venturacsbyuedu                        abstract                          ber agents increases rely                                                        global perceptions agents’ actions require      present conceptual framework creating unique optimal equilibrium conditions ex      learningbased algorithms converge optimal ist realworld systems reinforcement learning      equilibria cooperative multiagent settings learning applied realworld problems realworld      framework includes set conditions suf constraints new algorithms need designed      ﬁcient guarantee optimal performance     objective paper understand algo      demonstrate efﬁcacy framework    rithms cited able work effectively use      using analyze wellknown multiagent understanding facilitate development algorithms      learning algorithms conclude employing  improve success isolating three factors      design tool construct simple novel multi cause behave poorly suboptimal indi      agent learning algorithm                         vidual convergence action shadowing equilibrium                                                        selection problem prove absence three    introduction                                       factors sufﬁcient guarantee optimal behavior cooper  multiagent reinforcement learning systems interesting ative qlearning systems algorithm effec  cause share beneﬁts distributed artiﬁcial tively addresses three factors perform sys  telligence including parallel execution increased autonomy tem designers select means addressing each problem  simplicity individual agent design stone veloso consistent constraints   cao et al  qlearning watkins  natu  ral choice studying systems simplicity  background terminology  convergence guarantees qlearning simplicity qlearning algorithm watkins   algorithm wellunderstood researchers able led frequent use reinforcement learning research  focus unique challenges learning multiagent permits clear concise study multiagent coordination  environment                                          problems simple environments multiagent coordination    coaxing useful behavior group concurrently problems direct consequence qlearning al  learning qlearners trivial task agents gorithm focus qlearning  constantly modifying behavior during learning paper analysis presented applicable  process each agent faced unpredictable environ reinforcement learning paradigms  ment invalidate convergence guarantees  qlearning agent described mapping  agents converge individually optimal policies state space action space agent maintains list  combination policies optimal expected discounted rewards called qvalues  behavior                                             represented function qs ∈ ∈    poor group behavior universal rule course current state chosen action agent’s objective  researchers observed emergent coordination learn optimal policy π∗  → maximizes  groups independent learners using qlearning similar expected discounted reward states each time  algorithms maes brooks  schaerf et al  step agent chooses action ∈ receives reward                 sen et al   failed coordination attempts oc rstat updates appropriate qvalue  cur frequently motivate host qlearning adap  tations cooperative multiagent environments claus                                                         Δqstatαrstatγ   max qsta − qstat  boutilier  lauer riedmiller  littman                        wang sandholm  algorithms critical  steps better understanding multiagent qlearning  α≤  learning rate  ≤ γ  multiagent reinforcement learning general discount factor point time agent’s  algorithms intractable num best estimate optimal policy learned policy                                                    ijcai                                                     πˆs  argmaxaqs learned policy differ    optimal policy based qvalue es ∗                                                           si  ari si   timates qlearning learned policy differs    exploration policy executed during learning offpolicy                                  ∗                                                                   γ psitsit−  at− ri sit Π sit  learning note approaches employ identi                                                                                                    cal learned exploration policies onpolicy learning        certain conditions tsitsiklis  qlearning    ∞ psitsit− at− prob  converge set optimal qvalues                 ability transitioning individual state sit given pre                                                        vious state action agent’s preferred joint policy                                                      described terms optimal joint qvalues  q∗          γtp     π∗       ∗                  ∗                        t−  t−      Πi si  argmaxaqi  si                                                                           deﬁnition  qlearning agents cooperative                                                                                             ∗                                                        joint action maximizes si  ai     ∞ pstst−at− probability                                                                                          agent maximizes agents  transitioning state st given previous state action  agent converged optimal qvalue set deﬁnition cooperation does require  optimal learned policies identical π∗sˆπs agents share reward signal joint actions  argmaxaqs                                     requires agents share set mutuallypreferred                                                        joint actions allows scenarios agents’    qlearning multiagent systems                 preferences differ agents’ needs                                                        simultaneously satisﬁed  let si ai represent state action space ith consider solution optimal  agent nagent state ex nash equilibrium paretooptimal cooperative set  pressed vector individual agent states  sn  ting deﬁnition optimality restricted subset  si ∈ si combination agents’ individual actions paretooptimal nash equilibria called coordination equi    ai ∈ ai resulting joint state action libria coordination equilibrium strict inequality  spaces                                      deﬁnition  strict inequality strict    combined  policy mapping    exactly joint action results agents receiving  Π  →   Πstπst  πnsntin max reward cases multiple joint  each time step joint action Πst executed each actions sort coordinated equilibrium selection  agent receives reward risi  updates correspond necessary              ing qvalue note rewards agents deﬁnition  joint action  a∗       coordi  ceive based joint action individual state nation equilibrium state  agents update qvalues corresponding individual           ∗     ∗    ∗    ∗     ∗                                                        ∀a  ai ∈ ai ∈ qi si  ai  ≥ qi si  ai   action state multiple joint actions aliased  single action perception agent                 factors cause poor    useful behavior terms  joint policy agent prefer maintained behavior  separate qvalue each joint action preferred joint identify three factors cause poor                     ∗  policy mapping Πi  si → represents joint havior ﬁrst factors represented various  policy provides maximum discounted reward agent guises literature second factor action shadowing  si agent’s joint qvalues qisi likely familiar readers section  deﬁned average joint states contain si show absence three factors sufﬁcient  expected reward received agent joint action guarantee perform optimally                   ∗  executed si Πi followed relationship  joint qvalues agent’s actual qvalue set  poor individual behavior  qisiai described                              agent learned  individually optimal policy                                                         ∗                    ∗                                                        π  si                siai                                                              argmaxai        behavior best                                                        response strategies players qlearning             qisiai    paai qi si                                                       guaranteed converge optimal qvalue set                                                        optimal policy individual case  paai probability joint action exe convergence guarantee breaks multiagent systems  cuted agent selects individual action ai note changing behavior agents creates  probability conditional nonmarkovian environment  contain ai probability  despite loss theoretical guarantees qlearning  contain ai probability depends ac agents converge optimal policies multiagent set  tions agents words paai function tings  agents necessarily need  joint exploration policy Πst agent’s optimal verge optimal qvalue set order execute optimal  joint qvalues deﬁned                          policy  agents playing optimally                                                    ijcai                                                     settle nash equilibrium nash equilibria tend depend complete reward structure task  selfreinforcing individual behavior qlearning agents exploration strategy used par  wellstudied literature follows focus tially exploitive exploration strategies proven particu  particularly following potential problems larly effective encouraging convergence set mutually                                                        compatible individual policies claus boutilier     action shadowing                                 sen sekaran   action shadowing occurs individual action appears  better second individual ac  achieving optimal performance  tion potentially superior occur typical  qlearning agents maintain qvalues individual ac cooperating qlearners behave optimally each agent  tions receive rewards based joint action executed learns individually optimal policy maximal action  consequence agent’s optimal pol shadowing equilibrium selection problem absent  icy preclude possibility executing coordination theorem  cooperative qlearning                                                           equilibrium                                          ∪iπˆisi optimal solution following  deﬁnition  joint action a† shadowed individ ditions hold                                     π∗                            ∗  ual action ˆi state ˆi        ∀i πˆisiπi si              ∗    †      ∗  ∀aaˆi ∈ qi si   qi si                           †          ∗                   ∗     †                                                             a  aˆi aˆi  πi si ∀aaˆi ∈ qi si                                                                 ∗    special case maximal action shadowing occurs qi si   shadowed joint action provides maximal possible                                    ∗                                                                a    ∀i ∈  aqi si   ≥  reward affected agent                              ∗                                                            qi si   deﬁnition  agent experiences maximal action shad                                     ∗  owing state exist aˆi        ∗                             ∗    ∗      ∗          proof let aˆ  ∪iπˆisi joint action selected  shadowed aˆi ∀a ∈ qi si   ≥ qi si                                                         learned joint policy ∀i aˆi ˆπisi                                                                               ∗    cause action shadowing problem lies condition  ∀i aˆi  πi si  value update function agents multiagent know condition  joint                                                          †                    ∗    †      ∗  each time step each agent receives reward based action ∀aaˆi ∈ si    si                                                                                         joint action space updates qvalue implies aˆi enables joint actions maxi  based individual action selection consequently agent mize agent i’s joint qvalue function ∃a  ∀j ∈                                                                             ∗         ∗   unable distinguish distinct rewards   aˆi ∈ qi si   ≥ qi si   aliased individual action action shadowing cooperative joint action  consequence wellknown credit assignment problem maximizes expected discounted reward agent  precisely deﬁned addressable  maximize agents    action shadowing prevented onpolicy set joint actions   ∀i ∈                                                                      ∗         ∗  learning – agents seeking maximize individual reward   qi si   ≥ qi si   tend gravitate coordination equilibria condition  know  policy learning does assure action shadowing joint action maximizes expected discounted reward  problem occur maximal action shadow agents follows unique                                                                                    ∗          ∗  ing likely occur despite onpolicy learning situations joint action ∀i qi si   ≥ qi si   failed coordination attempts punished penalty each agent’s individual action aˆi enables joint ac  games claus boutilier  maximal action shadow tion maximizes expected discounted reward  ing cause coordination problem agent case ˆa  maximizes expected dis  terests conﬂict                               counted reward agent ˆa strict coordination    equilibrium selection problems                   equilibrium deﬁnition  op                                                        timal solution  equilibrium selection problem occurs coordi  nation agents required selecting naturally possible sufﬁcient  multiple optimal solutions ditions guarantee optimal behavior  stricter deﬁnition standard gametheoretic term conditions make framework preferable  refers task selecting optimal equilibrium possibilities addressed  set possibly suboptimal potential equilibria modifying learning algorithm directly placing  deﬁnition    equilibrium selection problem oc   additional constraints cooperative learning environ  curs ∃a     a∀i ∈     ment    ∗         ∗  qi si   ≥ qi si     existence equilibrium selection problem does  improving performance  necessarily result suboptimal behavior given framework imposed theorem   equilibrium selection problem creates potential sider various approaches preventing coordination prob  agents miscoordinate happens lems topics grouped according factors affect                                                    ijcai                                                     behavior action shadowing equilibrium selec  taskoriented approaches equilibrium  tion factor suboptimal individual convergence selection  quite prevalent qlearning literature broad                                                        unique optimal solution simplest way prevent  topic examined                                                        equilibrium selection problem design    basic ways improve performance single optimal solution case  control task modify learning algorithm task agents need coordinate selecting mul  oriented approaches reward structures constrained tiple optimal equilibria premise  action shadowing equilibrium selection problem vergence proofs hu wellman  littman  present agent algorithmoriented approaches  possibility avoiding equilib  attempt design algorithms cope effectively rium selection problem using wolf bowling  note  abovementioned problems general algorithmoriented wolf variants focused adversarial general  approaches superior enable creation sum games cooperative ones  generalpurpose learners learn effective policies regard  reward structure useful acquainted  algorithmoriented approaches  taskoriented approaches algorithms designed  equilibrium selection  constrained environments need explicitly ad  dress issues implicitly resolved environment emergent coordination emergent coordination describes                                                        tendency set noncommunicating reinforcement    taskoriented approaches action              learners learn compatible policies each agent       shadowing                                        constantly seeking best response agents’ ac                                                        tions demonstrated example hexapedal  dominant strategies dominant strategy policy robot locomotion maes brooks  network load  maximizes agent’s payoff regardless actions balancing schaerf et al  cooperative box  agents task structured way creates pushing task sen et al   dominant strategies agents agent experience social conventions social convention prearranged  action shadowing                                     constraint behavior applies agents driv                                                        ing right side street premise    algorithmoriented approaches action         hind social learners mataric  homo egualis agents       shadowing                                        nowe et al  used coordina                                                        tion mechanism qlearning systems lauer riedmiller  joint action learning each agent able perceive   actions counterparts able distinguish strategic learners strategic learners agents model  high low payoffs received different joint ac counterparts select optimal individual strategy  tions indiscriminately attributing payoffs based model commonlyused model games  single individual action technique called joint agents each others’ actions ﬁctitious  action learning claus boutilier  exam play claus boutilier  variant adaptive play  ples joint action learning include friendorfoe qlearning wang sandholm  young  exam  littman  nash qlearning hu wellman  ple concurrent reinforcement learners mundhe  cooperative learners tan                  sen     optimistic updates optimistic exploration  terministic environments distributed reinforcement learning  applying framework incremental  lauer riedmiller  effect joint ac  tion learning giving agents extra infor policy learning  mation – agents optimistically assume agents section simple learning algorithm  act maximize reward store maxi signed addressing each conditions theorem  mum observed reward each action action’s utility  algorithm called incremental policy learning ad  variation stochastic domains uses weighted sum dresses issues optimal individual convergence action  actual qvalue heuristic select action ex shadowing equilibrium selection problem consis  ecution kapetanakis kudenko   heuristic tently learns play coordination equilibrium determin  results effective convergence optimal equilibria istic environments  stochastic climbing games does stochastic achieving optimal individual behavior incremental pol  environments                                         icy learning achieves optimal individual behavior using    variable learning rate approach addressing standard qlearning update equation estimate qvalues  problem minimize effect learning preventing action shadowing following example  agents given agent’s learning ap claus boutilier  incremental policy learning pre  proach taken wolf variants bowling  vents action shadowing learning qvalues entire  variable learning rate updating qvalues effect joint action space each agent perceive action selec  holding agents’ policies constant learn tions counterparts reward signal  temporarily stationary environment     uses information learn qvalues possible joint                                                    ijcai                                                     actions enables agents clearly determine  individual actions lead coordination equilibria    addressing equilibrium selection problem incre  mental policy learning uses sequence incremen  tal policy adjustments select multiple optimal  equilibria each agent maintains probability distribu  tion   pa  pam available action set      probabilities initialized  modiﬁed according algorithm described    incremental policy learning algorithm    • initialization      ∀i paivi arbitrarily chosen      n        vi ∀i vi      • action selection                                  figure  incremental policy learning performance func      each time step agent selects action ∈ tion α      according probability distribution  agent exe                                              cutes action receives reward signal   updates policy learned simultaneously agents                                     joint qvalues updates described achieve maximum expected rewards occurs    • probability updates                               cause agents’ policies settle qvalue      let rmax maximum qvalue stored joint estimates coordination equilibria large      action table                                      distinguished qvalues desirable actions      let  α≤                                       expected algorithm performs better lower values                                                          α             α      rt ≥ rmax ∀i                             smaller likely joint                          α   −       values converge correct values agents’                        policies settle turn enables agents easily learn            ai paipai − αpai     coordination equilibrium interestingly α ap    rmax reward target equilibrium proaches  performance algorithm degrades  ward preferred joint action intuitively gracefully    max received reward action selection probabil  discussion  ity distribution skewed somewhat action  resulted receipt                              methods used incremental policy learning simple    proof sketch ipl algorithm meets criteria principle demonstrated powerful algorithm  theorem  result optimal perfor successfully achieves individual optimal performance avoids  mance proceeds follows condition  met maximal action shadowing addresses equilibrium se  individual agents use standard qlearning condition  met lection problem learn optimal group behavior coop  agents allowed joint action space erative environments incremental policy learning satisﬁes                                                                                                       α  argument meeting condition  given fulda requirements deterministic environments  ventura                                        sufﬁciently small fact algorithm performs                                                        requirements violated    results                                            incremental policy learning particularly suited en  ﬁrst allow agents select random actions vironments small numbers interacting agents  joint qvalues converge use coordination number agents large method addressing  mechanism described results agents action shadowing problem joint action learning  sistently learning play coordination equilibrium required possible alternative represent                                                                                                    strictly controlled situations limited signiﬁcant subsets joint action space fulda                                                                        experiment agents learning qvalues ventura    coordination policy simultaneously agents repeat  edly play stateless singlestage game each agent  conclusion  ﬁve possible action selections each cell payoff identiﬁed set conditions sufﬁcient guarantee  matrix randomly initialized integer  optimal performance systems cooperative concurrently   different random payoffs assigned each agent learning agents each condition met multiple differ  exception ﬁve randomly placed coordination equi ent ways enabling creation learning algorithms  libria payoff  agents algorithm suited constraints particular environment  tested deterministic stochastic environments task example learning algorithm pre  each reward signal summed gaussian noise  sented addresses each conditions    figure  shows algorithm’s performance function major advantage framework  α averaged  trials qvalues ditions satisﬁed through algorithmoriented ap                                                    ijcai                                                     
