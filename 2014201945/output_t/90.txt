 evolino hybrid neuroevolution  optimal linear search                    sequence learning                      jurgen¨ schmidhuber daan wierstra       faustino gomez                                       juergen daan tinoidsiach                       tu munich boltzmannstr   garching m¨unchen germany                                idsia galleria   lugano switzerland                        abstract                            echo state networks esns jaeger deal                                                        temporal dependencies simply ignoring gradients      current neural network learning algorithms    sociated hidden neurons composed primarily large      limited ability model nonlinear dynami pool neurons typically hundreds thousands ﬁxed      cal systems supervised gradientbased recur random weights esns trained computing set      rent neural networks rnns suffer vanish weights analytically pool output units using      ing error signal prevents learning inputs fast linear regression idea ran      far past prob dom hidden units pool capable rich dynamics      lems numerous local minima     just need correctly “tapped” adjusting output      introduce general framework sequence learn weights simple approach currently title holder      ing evolution recurrent systems linear   mackeyglass timeseries benchmark improving      outputs evolino evolino uses evolution dis accuracy methods three orders      cover good rnn hidden node weights     magnitude jaeger      ing methods linear regression quadratic  drawback esns course truly      programming compute optimal linear mappings    computationally powerful nonlinear net does      hidden state output using long short learn means seemingly simple tasks      term memory rnn architecture method       generating multiple superimposed sine waves      tested three different problem domains  method fails according experience able      contextsensitive languages  multiple superim solve simple contextsensitive grammar task gers      posed sine waves  mackeyglass sys    schmidhuber  esns use      tem evolino performs exceptionally   large number processing units prone overﬁt      tasks methods show notable deﬁ  ting poor generalization      ciencies                                   method adapts weights succeeds                                                        ing gradient information learn longterm dependencies                                                        long shortterm memory lstm hochreiter schmid    introduction                                       huber  gers schmidhuber  lstm uses  real world nonlinear dynamical systems blackbox na specialized network architecture includes linear memory  ture possible observe inputoutput behavior cells sustain activation indeﬁnitely cells  internal mechanism generates behavior input output gates learn open close  unknown modeling systems accurately predict appropriate times let new information  behavior huge challenge potentially farreaching im side change state cell let activation  pact areas broad speech processingrecognition ﬁ potentially affect cells network’s output  nancial forecasting engineering                 cell structure enables lstm use gradient descent learn    artiﬁcial neural networks feedback connections dependencies arbitrarily long time spans  recurrent neural networks rnns werbos  robin cases gradient information little use  son fallside  williams zipser  numerous local minima lstm competitive  attractive formalism nonlinear modeling alternative approach training rnns neuroevolu  ability principle approximate dynamical sys tion yao  instead using single neural network  tem arbitrary precision siegelmann sontag  space network parameters searched parallel using  training rnns standard gradient descent algo principle natural selection population chromo  rithms practical short time window somes strings encoding instance network weight val   timesteps sufﬁcient predict correct ues connectivity evaluated problem each  longer temporal dependencies gradient vanishes chromosome awarded ﬁtness value quantiﬁes rel  error signal propagated through time ative performance highly ﬁt chromosomes  network weights adjusted correctly account combined exchanging substrings crossover ran  events far past hochreiter et al      domly changing values mutation producing new                                                      problem properties require nonlinearity recurrence                    yt yt                                                      dealt evolution                                                          figure  illustrates basic operation evolino net                                                                                                                                           linear output     work output network time yt ∈                                         layer         computed following formulas                                                                           yt  φt                                φ φt φ φt φ                                                             φt  fut ut −                                                              φt ∈ rn output recurrent neural network                     recurrent                          f· weight matrix note net                                                        works recurrent f· function entire                  neural network                        history ut ut −      case maximum                                                        margin classiﬁcation problems vapnik  com                                                        pute quadratic programming follows                                                                 focus mean squared error minimization problems                                                        compute linear regression  figure  evolino network recurrent neural network receives order evolve f· minimizes error  sequential inputs ut produce vector φ φ     φn ev correct output modeled  ery time step values linearly combined weight evolino does specify particular evolutionary algorithm  matrix yield network’s output vector yt rnn  evolved output layer weights computed using fast opti stipulates networks evaluated using  mal method linear regression quadratic programming following twophase procedure                                                          ﬁrst phase training set sequences obtained                                                        ui di  each length li presented  lutions hopefully improve existing population network each sequence ui starting time    approach effective solving continu each input pattern uit successively propagated through  ous partially observable reinforcement learning tasks recurrent network produce vector activations φit  gradient directly available outperforming conven                                                                             stored row × pi matrix Φ associated  tional methods qlearning sarsa difﬁ                              cult learning benchmarks moriarty miikkulainen  each φ target row vector containing  gomez miikkulainen  neuroevolution correct output values each time step sequences  rarely used supervised learning tasks time se seen output weights output layer ﬁg  ries prediction difﬁculty ﬁnetuning solution ure  computed using linear regression Φ  parameters network weights pre column vectors Φ values each outputs  vailing maxim gradient information used entire training set form nonorthogonal basis  available                                      combined linearly approximate    paper present novel framework called evo second phase training set presented net  lution recurrent systems linear outputs evolino work inputs propagated through  combines elements three aforementioned meth recurrent network f· newly computed output  ods address disadvantages each extending ideas nections produce predictions yt error predic  proposed feedforward networks radial basis functions tion residual error used ﬁtness measure  rbfs maillard gueriot  applied lstm minimized evolution  architecture evolino solve tasks esns neuroevolution normally applied reinforcement learn  achieves higher accuracy certain continuous function gen ing tasks correct network outputs targets  eration tasks conventional gradient descent rnns known priori use neuroevolution supervised  cluding gradientbased lstm                          learning circumvent problems gradientbased ap    section  explains basic concept evolino proaches order obtain precision required time  scribes speciﬁc implementation used pa series prediction try evolve network makes  section  presents experiments using evolino predictions directly instead network outputs set vec  three different domains contextsensitive grammars contin tors form basis linear regression intuition  uous function generation mackeyglass timeseries ﬁnding sufﬁciently good basis easier trying  section   discuss algorithm experimental ﬁnd network models accurately  results summarize conclusions                 study evolino instantiated using enforced sub                                                        populations evolve lstm networks sections                                                        scribe esp lstm details com    evolino framework                              bined evolino framework  evolino general framework supervised sequence  learning combines neuroevolution evolution  enforced subpopulations  neural networks analytical linear methods opti enforced subpopulations differs standard neuroevolu  mal sense linear regression quadratic pro tion methods instead evolving complete networks  gramming underlying principle evolino coevolves separate subpopulations network components  linear model account large number properties neurons ﬁgure  evolution esp proceeds follows                 time series                                                             peephole                                                                                                 output                                                                     gf                   fitness          esp                                                                                                   input                                                                      Σ         gi                                     output                   external inputs                                                        figure  long shortterm memory ﬁgure shows lstm                                 pseudo−inverse                                 weights                memory cell cell internal state forget                                                        gate gf  determines state attenuated each           lstm network                                 time step input gate gi  controls access cell                                                        external inputs summed Σ unit output gate  figure  enforced subpopulations esp population controls cell ﬁres small dark nodes  neurons segregated subpopulations networks formed represent multiplication function  randomly selecting neuron each subpopulation neuron  accumulates ﬁtness score adding ﬁtness each network  participated best neurons each subpopula functions needed form good networks members  tion mated form new neurons network shown different evolving subfunction types prevented mat  lstm network memory cells triangular shapes ing subpopulations reduce noise neuron ﬁtness                                                        measure each evolving neuron type guaranteed    initialization number hidden units net represented network formed allows      works evolved speciﬁed subpopula esp evolve recurrent networks sane      tion neuron chromosomes created each hidden performance esp does improve predeter      unit each chromosome encodes neuron’s input mined number generations technique called burst muta      recurrent connection weights string tion used idea burst mutation search space      random real numbers                              modiﬁcations best solution far burst                                                        mutation activated best neuron each subpopulation    evaluation neuron selected random each                                                        saved neurons deleted new neurons       subpopulations combined form recur                                                      created each subpopulation adding cauchy distributed      rent network network evaluated task                                                        noise saved neuron evolution resumes      awarded ﬁtness score score added cu                                                        searching neighborhood previous best solu      mulative ﬁtness each neuron participated                                                        tion burst mutation injects new diversity subpopu      network                                                        lations allows esp continue evolving initial    recombination each subpopulation neurons subpopulations converged      ranked ﬁtness quartile recombined      ing point crossover mutated using cauchy dis  long shortterm memory      tributed noise create new neurons replace      lowestranking half subpopulation         lstm  recurrent neural network purposely designed                                                        learn longterm dependencies gradient descent    repeat evaluation–recombination cycle suf                                                        unique feature lstm architecture memory cell      ﬁciently ﬁt network                                                        capable maintaining activation indeﬁnitely ﬁg    esp searches space networks indirectly sampling ure  memory cells consist linear unit holds  possible networks constructed sub state cell three gates open close  populations neurons network evaluations serve pro time input gate “protects” neuron input  vide ﬁtness statistic used produce better neurons gate open inputs affect internal state  eventually combined form successful network neuron output gate lets state parts  cooperative coevolutionary approach extension network forget gate enables state “leak”  symbiotic adaptive neuroevolution sane moriarty activity longer useful  miikkulainen  evolves neurons state cell computed  single population using separate subpopulations esp                                                                                  forget  accelerates specialization neurons different sub sit  netitgi  gi tsit −  gin gforget activation input training data gradient lstm evolino lstm  gates respectively net weighted sum                         external inputs indicated Σs ﬁgure                                                      cell              cell                                              netit  hx wij cj −   wik ukt                                                                                                                     table  generalization results anbncn language  usually identity function cj output table compares evolinobased lstm gradientbased lstm  cell                                            left column shows set legal strings used train each method                                          cj  tanhgj tsj          columns show set strings each method able                                                    accept training result lstm gradient descent   output gate cell each gate gers schmidhuber  averages  runs  gi memory cell open closed time calculated      type           type               type             useful continuous function generation tasks    gi   σx  wij  cj −   wik ukt                                                     interferes extent performance discrete  type input output forget σ contextsensitive language task  standard sigmoid function gates receive input  output cells cj  external inputs  experimental results  network                                                        experiments carried three test problems context    combining esp lstm evolino               sensitive languages multiple superimposed sine waves  apply general evolino framework lstm archi mackeyglass time series ﬁrst chosen  tecture using esp evolution regression comput highlight evolino’s ability perform discrete  ing linear mappings hidden state outputs esp continuous domains detailed description  evolves subpopulations memory cells instead standard setups used problems experiments  recurrent neurons ﬁgure  each chromosome string direct reader wierstra et al  mackey  containing external input weights input output glass selected compare evolino esns  forget gate weights total  ∗  weights reference method widely used time series benchmark  each memory cell chromosome number  external inputs number memory cells  contextsensitive grammars  network sets  weights learning recognize contextsensitive languages difﬁ  three gates equation  cell equation  receive cult intractable problem standard rnns  input outside cell cells esp require unlimited memory instance recognizing  scribed section  normally uses crossover recombine language anbncn strings number bs  neurons present evolino variant ﬁne cs equal entails counting number consecutive  local search desirable esp uses mutation bs cs potentially having remember quan  quarter chromosomes each subpopulation dupli tities string read gradientbased  cated copies mutated adding cauchy noise lstm previously used learn anbncn  weight values                           compare results gers schmidhuber     linear regression method used compute output evolinobased lstm  weights equation  moorepenrose pseudo  sets  simulations run each using different  inverse method fast optimal training set legal strings anbncn   sense minimizes summed squared error penrose     symbol strings presented  —compare maillard gueriot  appli networks symbol time networks  input  cation feedforward rbf nets vector φt consists units each possible symbol start  cell outputs ci equation  internal states input set  corresponding symbol  si equation  pseudoinverse computes observed  present time step  nection weights each memory cell refer network predicts symbols come  nections internal states output units “output termination symbol  activating  output  peephole” connections peer interior units output unit considered “on” activation  cells                                                greater     continuous function generation backprojection esp evolved lstm   networks  memory cells  teacher forcing standard rnn terminology used weights randomly initialized values −   predicted outputs fed inputs time cauchy noise parameter α mutation burst  step φt  fut yt −  ut −      mutation set   mutations kept    during training correct target values backprojected bound evolution terminated  gener  effect “clamping” network’s outputs right values ations best network each simulation  during testing network backprojects predictions tested  technique used esns esns results summarized table  evolinobased  change backprojection connection weights evolino lstm learns approximately  minutes average  evolves treating like input net importantly able generalize substantially better  work experiments described backprojection gradientbased lstm                        predicted                     mgs domain show capacity making precise                                                 predictions used setup experiments                                                        jaeger networks trained ﬁrst                                                         time steps series using “washout time”  steps                                                        during washout time vectors φt collected                                                        calculating pseudoinverse                                                          evolved networks  memory cells  gen                                                        erations cauchy noise α − bias input                                        added network backprojection values                       time steps  figure  performance evolino triple superimposed scaled factor  testing outputs  sine wave task plot show behavior typical network clamped correct targets ﬁrst  steps  produced  generations  evaluations ﬁrst  steps network backprojected prediction                                                                     datapoints left vertical dashed line used training  steps  cell input equation  squashed  data rest predicted network during testing time tanh function average nrmse evolino                                                                                        −             −   steps  show network predictions dashed curve during  cells  runs  ×  compared    testing plotted correct output solid curve esns  neurons jaeger evolino  inset magniﬁed clearly shows curves results currently secondbest reported far                                                          figure  shows performance evolino network                                                        mg timeseries fewer memory cells     multiple superimposed sine waves                 generations network fewer parameters  jaeger jaeger reports echo state networks unable achieve precision  neurons  unable learn functions composed multiple superim demonstrates evolino learn complex functions  posed oscillators speciﬁcally functions like sinx  quickly case approximately  minutes  sinx individual sines cpu time  plitude frequencies multiples each  esns difﬁculty solving problem dy  discussion  namics neurons esn “pool” coupled  truly solving task requires internal representa real strength evolino framework general  tion multiple attractors nonperiodic behavior ity different classes sequence prediction prob  function                                         lems able compete best known methods    evolved networks  memory cells predict convincingly outperform cases partic  aforementioned double sine sinx  sinx ular generalized better gradientbased lstm  network  cells complex triple sine contextsensitive grammar task solved super  sinx  sinx  sinx evolino used imposed sine wave task esns results  parameter settings previous section suggest evolino widely applicable model  backprojection used section  networks ing complex processes discrete continuous  tasks evolved  generations predict ﬁrst  properties speech  time steps each function tested data points evolino avoids problem vanishing gradient local  timesteps                              minima normally associated rnn training searching    average summed squared error training set space networks parallel through evolution   double sine  triple sine using lstm memory cells evolino searches  average error test set   respec weight space biased extracting retain  tively error levels barely visible timestep ing relating discrete events far apart   figure  shows behavior triple sine time borrowing idea linear regression  wave evolino networks timestep  magni esns evolino capable making precise predictions  ﬁed inset illustrates  times length tasks like mackeyglass benchmark  training set network makes accurate predic apart versatility advantage evolino  tions                                                esns produces parsimonious solutions                                                        esns large pools neurons likely    mackeyglass timeseries prediction              ﬁt data evolino networks smaller                                                        potentially general susceptible noise  mackeyglass mgs mackey glass  easily comprehensible instance rnn rule  standard benchmark chaotic time series prediction extraction techniques  produces irregular time series produced evolino template instantiated plugging  following differential equation y˙t  αyt−τ  alternative analytical methods computing optimal         β  yt − τ  − γyt parameters usually set linear mappings outputs given hidden state  dif  α   β   γ   chaotic ferent neuroevolution algorithms  various recurrent  delay τ   use common value network architectures particular implementation used  delay τ                                                                 mgs modeled accurately     normalized root mean square error nrmse  steps  ing feedforward networks timewindow input end training sequence standard comparison  compare evolino esns currently best method measure used problem
