              learning multiagent reasoning autonomous agents                               ijcai computers thought paper                                                peter stone                                     department sciences                                     university texas austin                                           pstonecsutexasedu                        abstract                          agents opposing uncorrelated goals                                                        cases dealing agents beneﬁcial      goal artiﬁcial intelligence enable points view robustness ﬂexibility      creation robust fully autonomous agents agent able learn adapt interacts      coexist real world agents learning interaction combination      need able learn order correct key necessary capabilities path creating robust      circumvent inevitable imperfections autonomous agents enable capabilities need      dynamically changing world   combine  basic algorithmic research machine learning      need able interact multiagent systems  applicationoriented research      share common goals pur threads aimed studying complete agents speciﬁc com      sue independent goals goals direct plex environments ultimate goal drawing general      conﬂict paper presents current research di lessons speciﬁc implementations      rections machine learning multiagent reasoning paper illustrates research methodology drawing      robotics advocates uniﬁcation heavily concrete examples research      concrete application domains ideally new theoret suggests research directions future aimed creating      ical results each separate area inform prac fully autonomous agents including robots capable learn      tical implementations innovations ing interacting represents perception      crete multiagent applications drive new theo important interesting research topics ﬁeld      retical pursuits synergistic  intended comprehensive – cer      search approaches lead goal tainly ai areas active interesting likely      fully autonomous agents                          fruitful ambition paper inspire                                                        current future colleagues    introduction                                       problems presented inspire  past research artiﬁcial intelligence formulate opposing arguments entirely different  focused highlevel reasoning abstract topics worthy pursuit advocated  grounded representations interpreting raw sensor data remainder paper organized follows  building grounded representations sections   learning multiagent reasoning  foci taken sufﬁcient deploying practical considered separately section  related issues  realworld ai systems recent years active contingent path robust physical agents robots learn  ﬁeld focused creating complete autonomous interact addressed  agents sense environment engage high sections – treat agent components essence  level cognitive decisionmaking execute ac creating complete autonomous agents putting  tions environment                             pieces daphne koller organized     ﬁeld progresses direction individual au computers thought lecture notion three  tonomous agents software physically embodied conceptual bridges connecting representation reasoning  capable prevalent multi learning way mitigating observation  agent systems consisting homogeneous similar agents ai communities  better understood success dency divide problem welldeﬁned pieces  fully interact real world agents able reason make progress each make  interactions heterogeneous agents widely progress problems tend away  varying properties capabilities                      each koller     agents similar identical section  argue creating fully functional agents  goals case advantage explic complex application environments excellent way  itly coordinating activities hand combat fragmentation finally section  concludes                                                    ijcai                                                     learning                                             sign choices result estimates diverge                                                        optimal value function baird  agents  machine learning active area ai form poorly reinforcement learning algorithms  years tom mitchell’s computers thought paper guaranteed convergence baird moore  lagoudakis  decades ago refers “a resurgence inter parr  achieving high performance practice  est machine learning” advances general prob quires ﬁnding appropriate representation function                                              lem solving knowledgebased expert systems mitchell approximator lagoudakis parr observe   supervised learning methods  classiﬁcation regression matured point crucial factor successful approximate al  enabling general purpose toolkit used produc gorithm choice parametric approxima  tively experts novices alike witten frank  tion architectures choice projection  similarly unsupervised learning algorithms data cluster parameter adjustment method lagoudakis  ing advanced nicely point view au parr   tonomous agents relatively recent development nonetheless representational choices typically  reinforcement learning rl algorithms designed learn manually based designer’s intuition  action selection delayed reward sequential decision                                                         despite early emphasis ﬁeld “tabula rasa”  making problems signiﬁcant             learning increasingly accepted process    unlike classical supervised unsupervised learning designing agents complex realworld domains al  learner supplied training data ways require manual input sort key suc  learn function features target set premise cess limiting requirements human knowledge  rl matches agent paradigm exactly learner gath tasks representations humans good  ers training data interacting environment tuitions referred existence general  learn policy mapping states actions rl purpose toolkit used expert knowledge  agent repeatedly takes actions new state underlying algorithms suggests supervised learn  environment lead immediate reward signal ing methods mature use robust complex  learner explicitly tradeoff exploration systems case rl despite ele  exploitation effort maximize longterm reward gant theory accompanies td methods notably  receive                                 qlearning converges optimal value function    common approach reinforcement learning relies state visited inﬁnitely watkins  despite  concept value functions indicate particular limited number successes reported  policy longterm value given state stateaction pair largescale domains tesauro  crites barto   temporal difference td methods sutton stone et al  crucial somewhat unintuitive decisions  combine principles dynamic programming statistical representations need based deep  sampling use immediate rewards received agent derstanding underlying algorithms application  incrementally improve agent’s policy es main  timated value function policy td methods remainder section outlines directions  enable agent learn during “lifetime” individ search help current reinforcement learning  ual experience interacting environment     machine learning methods scale point    small problems value function represented core components fully autonomous agents real  table large probabilistic domains world tasks sections   deal automatically  arise real world usually require coupling td methods adjusting knowledge representations used learning sec  function approximator represents mapping tions   address ways humans provide  stateaction pairs values concise parame intuitive knowledge learning agents providing  terized function uses supervised learning methods set subtask decomposition suggesting related tasks  parameters different methods function approx knowledge transfer  imation used successfully including cmacs ra  dial basis functions neural networks sutton barto  adaptive function approximation          using function approximators requires mak order address observation lagoudakis parr  ing crucial representational decisions number hid whiteson stone  automate search  den units initial weights neural network poor effective function approximators rl agents applying                                                         optimization techniques representation problem     say classical learning irrelevant agents research propose using evolutionary methods gold  autonomous agent incorporate learned predictions berg  optimizing representation  world model example autonomous bidding agent  use regression predict future closing price auction demonstrated ability discover effective representa  use information evaluate potential bids stone et tions gruau et al  stanley miikkulainen   al                                            synthesizing evolutionary td methods results new    td methods contrast policy search methods learn approach called evolutionary function approximationwhich  based results holistic policy evaluations automatically selects function approximator representations  individual action effects                            enable efﬁcient individual learning method                                                    ijcai                                                     evolves individuals better able learnthisbi gests knowledgetransfer framework analyze  ologically intuitive combination applied com learned policies environment discover abstractions  putational systems past hinton nowlan  improve learning similar environments  ackley littman  boers et al  french introduce efﬁcient algorithm discovers local  messinger  gruau whitley  nolﬁ et al gions state space action op   knowledge aid discovery timal regardless value given set state variables  good td function approximators                       example route work new    speciﬁcally use neuroevolution augmenting    york matter weather london algorithm  topologies neat  stanley miikkulainen    depends determining set actions optimal each  select neural network function approximators state statistical tests criterion trade  learning watkins  popular td method result computational sample complexity general  ing algorithm called neatq uses neat evolve topolo ize learned structure similar environment  gies initial weights neural networks better able agent ignore each set state variables correspond  learn backpropagation represent value estimates ing learned region state space  provided qlearning                                 care apply discovered abstrac    experimental evaluation test qlearning series tions criteria use discovery strictly weaker  manually designed neural networks compare given work safe state abstraction li et  sults neatq regular neat learns direct rep al  transferring abstractions domain  resentations policies results demonstrate evolu introduce generalization error preserve  tionary function approximation signiﬁcantly improve convergence optimal policy encapsulate state  performance td methods providing muchneeded abstractions temporal abstractionsoroptions  practical approach selecting td function approximators strue sequences primitive actions constituting single  automating critical design step typically performed abstract action sutton et al  contrast previ  manually                                             ous work temporal abstraction discover abstract ac    research takes step automating tions intended just simplify state representation  choice representations learning achieve certain goal state rl agents equipped  room future work including extending use dif abstract actions learn apply state abstraction  ferent policy search methods pegasus ng  way learn execute action  russell  policy gradient methods sutton et al fact abstractions building blocks hi   importantly optimizing function erarchical rl suggests recursive application ab  approximators neural networks cmacs straction discovery technique create hierarchies tempo  radial basis functions research feature selection ral abstractions explicitly facilitate state abstractions  adapt inputs representation fawcett  maxq task decompositions dietterich  pos  whiteson et al  space adaptive rep sibility highlights need robust testing optimal ac  resentations                                         tions each application method adds new poten                                                        tially optimal actions agent learning maxq hierar    learned abstractions                             chies based method natural direction future  approach adjusting problem representation state research  abstraction maps distinct ground states sin  gle abstract state agent treat ground states  layered learning  exactly way agent learn optimal behav hierarchies general powerful tools decomposing  ior environment satisﬁes particular condition complex control tasks manageable subtasks case  each action abstract outcome state point mammalian biology composition hierarchi  reward primitive states mapped cally organized components each able perform specialized  abstract state ground states grouped subtasks components span levels behavior  share local behavior abstract state space dean ranging individual cells complex organs culminat  givan  ravindran barto howeverthis ing complete organism purely behavioral  cited research applies planning context level organisms distinct subsystems including reﬂexes  domain model given user manually determines visual difﬁcult imagine monolithic  condition holds supplies corresponding state entity capable range complexity  abstraction rl algorithm                      behaviors mammals exhibit    jong stone  propose alternative condition covered previous section initial steps  state abstraction conducive automatic discov learning hierarchies automatically rel  ery intuitively agent behave optimally ignor atively simple domains complex tasks hi  ing certain variable state representation erarchies need deﬁned manually brooks   able learn successfully ignoring state vari gat  layered learning stone veloso   able recognizing discovering qualitative structure hierarchical paradigm similarly requires  tends require time learning optimal behav given task decomposition relies learning  ior policy thrun schwartz  approach sug various subtasks necessary achieving complete high                                                    ijcai                                                     level goal layered learning bottomup paradigm choice machine learning method depends sub  lowlevel behaviors closer environmen task  tal inputs trained prior highlevel behaviors                                                        principle     layered learning approach somewhat reminiscent  rodney brooks’ subsumption architecture summarized key deﬁning characteristic layered learning each  computers thought paper brooks  sub learned layer directly affects learning layer  sumption architecture layers control modules allowing high learned subtask affect subsequent layer  level controllers override lowerlever ones each control • constructing set training examples  level capable controlling robot                                                          • providing features used learning andor  speciﬁed level functionality order focus learning  quickly highlevel behaviors layered learning • pruning output set  abandons commitment layer completely layered learning originally applied complex  able control robot instead situationspeciﬁc multiagent learning task simulated robot soccer  general possible behaviors learned robocup soccer server noda et al anexten  managed higherlevel behaviors idea sion allows concurrent training multiple layers  building higher levels functionality lower levels implemented simulation whiteson stone   retained                                          described section  layered learning    table  summarizes principles layered learning cently applied successfully physical robots  paradigm described section cases subtask decomposition supplied manually                                                        relatively intuitive construct nonetheless discover     mapping directly inputs outputs ing ways automate decomposition leverag      tractably learnable                              ing abstraction discovery work described section     hierarchical task decomposition given       important future goal    machine learning exploits data train andor adapt  transfer learning      learning occurs separately each level         particularly topical area ai research  trans    output learning layer feeds fer learning leveraging learned knowledge source task      layer                                            improve learning related different target task                                                        transfer learning pertain classical learning       table  key principles layered learning particularly appropriate learning agents meant                                                        persist time changing ﬂexibly tasks envi                                                        ronments having learn each new task  principle                                            scratch goal enable agent advantage  layered learning designed domains com past experience speed new learning  plex learning mapping directly input reinforcement learning agent ways  output representation instead layered learning approach source target differ transfer prob  consists breaking problem task lay lem example source target tasks following  ers each layer concept needs acquired ma differences studied literature  chine learning ml algorithm abstracts solves local • transition function effects agents’ actions dif  conceptlearning task                                    fer selfridge et al   principle                                              • reward structure agents different goals singh  layered learning uses bottomup incremental approach   hierarchical task decomposition starting lowlevel sub • state space agents’ environments differ konidaris  tasks process creating new ml subtasks continues barto  fernandez veloso   til highlevel tasks deal domain com •  plexity reached appropriate learning granularity initial state agents start different locations  subtasks learned determined function time asada et al   speciﬁc domain task decomposition layered learning • actions agents different available actions maclin  automated instead layers deﬁned ml et al  taylor et al  soni singh   opportunities domain                            • state variables agents’ state descriptions differ maclin  principle                                                et al  taylor et al  soni singh   machine learning used central layered learning general case source target differ  exploit data order train andor adapt overall sys ways cases taylor et al   tem ml useful training functions difﬁcult troduce method transferring learned value function  ﬁnetune manually useful adaptation task source rl task seed learning target key  details completely known advance technical challenge mapping value function repre  change dynamically like task decomposition sentation meaningful value function typically                                                    ijcai                                                     larger representation despite fact stateaction values  inherently taskspeciﬁc    past research conﬁrms tasks closely related  learned policy task used provide good  initial policy second task example selfridge et  al  showed pole balancing task  harder time shortening length pole  increasing mass learner ﬁrst trained  longer lighter pole quickly learn  succeed difﬁcult task modiﬁed transition  function way learner able reﬁne initial  policy given task    consider general case tasks related  distinct state andor action spaces differ  use source policy πs initial policy td learner  target task transform value function  directly applied new state action  space introduce notion behavior transfer func  tional ρπsπt  allow apply policy  target task policy transform functional ρ needs  modify source policy associated value function  accepts states target task inputs allows  actions target task outputs depicted figure  ρ functional transforms value function  figure                                              task applicable second task    deﬁning ρ modiﬁcation way πt different state action spaces  good starting point learning target key  technical challenge enable general behavior transfer cur                                ρ                            appropriate example domains require mas  rent results indicate ’s exist agents represent people organiza  tasks taylor et al  automating discov tions different possibly conﬂicting goals propri  ery intertask mapping state variables etary information having multiple agents speed com  actions source target tasks remains open chal putation parallelization mas provide robustness  lenge does automatically selecting source target redundancy mas scalable result mod  tasks                                     ularity useful elucidation funda                                                        mental problems social sciences life sciences cao    multiagent reasoning                               et al  including intelligence decker                                                        garding point weiß  “intelligence  addition learning second essential capability robust deeply inevitably coupled interaction” fact  fully autonomous agents ability interact proposed best way develop intelligent ma  agents multiagent reasoning argued introduction chines start creating “social” machines daut  successfully interact real world agents able enhahn  theory based sociobiological  reason interactions heterogeneous agents theory primate intelligence ﬁrst evolved  widely varying properties capabilities need deal social interactions minsky   single complete agent able operate autonomously multiagent systems differ singleagent systems  extended periods time real world inevitable agents exist model each other’s goals ac  soon need tions fully general multiagent scenario  interact                            direct interaction agents communication    recent ﬁeld machine learning interaction viewed environmental stimuli  past decade multiagent systems mas begun present interagent communication separate  come forefront methodology impor environment  tant questions mas                           individual agent’s perspective multiagent systems    • advantages does offer alternatives differ singleagent systems signiﬁcantly                                                        environment’s dynamics affected agents    • circumstances useful               addition uncertainty inherent domain                                                        agents intentionally affect environment unpre  foolish claim mas used dictable ways multiagent systems viewed  designing complex systems like approach having dynamic environments figure  illustrates view  situations particularly appropriate  survey ﬁeld stone takes issue appear bond  veloso summarized circumstances mas gasser  sycara                                                     ijcai                                                     
