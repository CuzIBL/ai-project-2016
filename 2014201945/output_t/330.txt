                         simultaneous adversarial multirobot learning                                          michael bowling manuela veloso                                              science department                                                carnegie mellon university                                                pittsburgh pa                                abstract                               equilibria equilibrium simply policy                                                                  players each playing optimally respect        multirobot learning faces challenges                                                                  concept powerful solution games        robot learning challenges mul•                                                                 learning context agent learn better        tiagent learning great deal                                                                  policy agents playing equilibrium        recent research multiagent reinforcement learn•                                                                   multiagent learning stochastic games far        ing stochastic games intuitive ex•                                                                 applied small games enumerable state action        tension mdps multiple agents recent                                                                  spaces robot learning tasks continuous state        work general applied                                                                  action spaces typically just couple        small games hundreds states                                                                  dimensions discretizations space enumerable        hand robot tasks continuous of•                                                                 state set typically perform addition data        complex state action spaces robot learn•                                                                 considerably costly gather millions training        ing tasks demand approximation generaliza•                                                                 runs learn policies feasible typical solutions        tion techniques received exten•                                                                 robot learning problem use approximation make        sive attention singleagent learning paper                                                                  learning tractable generalization efficient use        introduce grawolf generalpurpose scal•                                                                 training experience        able multiagent learning algorithm combines                                                                    paper introduce new algorithm grawolf             gradientbased policy learning techniques                                                                                                                                  combines approximation generalization techniques        wolf win learn fast variable learning rate                                                                  wolf multiagent learning technique show em•       apply algorithm adversarial multi                                                                 pirical results applying algorithm problem si•       robot task simultaneous learning show re•                                                                 multaneous learning adversarial robot task section         sults learning simulation real                                                                  brief overview key concepts multiagent        robots results demonstrate grawolf                                                                  learning formal model stochastic games        learn successful policies overcoming                                                                  section  particular adversarial robot task        challenges multirobot learning                                                                  challenges learning section  present                                                                  main components grawolf policy gradient ascent    introduction                                                 wolf variable learning rate section  present                                                                  experimental results applying algorithm adver•  multirobot learning challenge learning act                                                                  sarial robot task conclude   environment containing robots robots   goals learning   adapting robots make environment longer sta•       multiagent learning   tionary violating markov property traditional single                                                                 multiagent learning focused game theoretic frame  agent behavior learning relies multirobot learning                                                                  work stochastic games stochastic game tuple   combines multiagent learning challenges                                                                  ani rn number agents   problems learning robots continuous state                                                                  set states set actions available agent   action spaces minimal training data                                                                  joint action space  transition      great deal recent work multiagent learn•                                                                 function axs   reward function   ing looked problem learning stochastic        ith agent —r looks similar mdp   games littman  singh et  bowling          framework multiple agents selecting actions   veloso greenwald hall  stochastic games     state rewards depend joint action   natural extension markov decision processes mdps    agents important difference each agent   multiple agents studied field     game theory traditional solution concept prob•        grawolf short gradientbased win learn fast   lem simultaneously finding optimal policies nash sound gradient       multiagent systems                                                                                                      separate reward function goal each agent    select actions order maximize discounted future    rewards discount factor       stochastic games natural extension mdps    multiple agents extension matrix games    multiple states each state stochastic game    viewed matrix game payoffs each joint action      determined matrices rts playing matrix    game receiving payoffs players transitioned    state matrix game determined joint    action stochastic games contain    mdps matrix games subsets framework      stochastic policies unlike singleagent settings deter•  ministic policies multiagent settings exploited   agents consider childrens game rockpaper                                                                 figure  adversarial robot task robot trying   scissors player play action determinis                                                                 inside circle robot trying stop   tically player win time selecting                                                                  lines show state action representation   action defeats fact requires considera•                                                                 described section    tion mixed strategies stochastic policies stochastic   policy function maps states   mixed strategies probability distributions players possibly changing policies approach   players actions later show stochastic policies   strong connection equilibria algorithms con•  useful gradientbased learning techniques            verge playing each                                                                  equilibrium bowling veloso                                                                    approaches scaled be•  nash equilibria concept stochastic poli•     yond games states games   cies optimal policies independent large number states games continuous state spaces   players policies define notion make state enumeration intractable previous algo•  bestresponse policy bestresponse play•  rithms stated form require enumeration states   ers policies optimal given policies major ad• policies value functions major limita•  vancement driven development matrix    tion paper examine learning adversarial robot   games game theory stochastic games notion    task thought continuous state stochas•  bestresponse equilibrium nash equilibrium nash     tic game specifically build idea bestresponse   jr                                                     learners using gradient techniques singh et ai  bowl•     nash equilibrium collection strategies each ing veloso robot task   players each players strategy bestresponse algorithm results   players strategies player bet•  ter changing strategies given players                                                                   adversarial robot task   dont change strategies makes notion equilib•  rium compelling matrix games stochastic games  consider adversarial robot task shown figure    equilibrium possibly having multiple equilib•    robot figure attacker trying reach   ria zerosum twoplayer games like adversarial task     circle center field robot closer   explored paper single nash equilibrium       circle defender trying prevent hap•                                                                 pening attacker reaches circle receives re•                                                                 ward defender receives reward negative   learning stochastic games stochastic games                                                                  rewards task at•  focus recent research area reinforcement                                                                  tacker reaches circle seconds elapses trial   learning different approaches explored                                                                  robots reset initial positions   algorithms explicitly learn equilibria                                                                  attacker meter circle defender halfway   through experience independent players policy                                                                  robots simultaneously learn environment   littman  greenwald hall  al•                                                                 each seeking maximize discounted future reward   gorithms iteratively estimate value functions use                                                                  experiments discount factor used    compute equilibrium game second approach                                                                  each second delay   bestresponse learners claus boutilier    singh et ai  bowling veloso       robots cmdragons robot   learners explicitly optimize reward respect  soccer team competes robocup smallsize                                                                  league details robot platform                                                                  relevant learning task learning algorithm      actually multiple equilibria equal   payoffs interchangeable                               situated large complex architecture                                                                                                    multiagent systems existing capability team employs global vision   stochastic policy according   mounted field input processed elab•  orate tracking module provides accurate positions   velocities robots positions comprise input   state learning algorithm team consists ro• sutton colleagues main result convergence proof   bust modules obstacle avoidance motion control    following policy iteration rule updates policys   actions learning algorithm involve providing tar• parameters gibbs distribution according   points obstacle avoidance module situating   learning context larger architecture focuses   learning having robot learn solve   understood problems like path planning object tracking               independent approximation qn°sa   learning directed heart problem multi parameters expected value taking ac•  robot interaction                                             tion state following policy rfc      second control loop partially described gibbs distribution sutton colleagues showed   inherent small latency specifically convergence approximation following   observed change world ms elapse   form   robots response executed latency overcome   each robots position velocity predicting through                                                             latency using knowledge past executed     actions actions opponent robots      point amounts approximation   known prediction possible robots la•   advantage function    tency effectively adds element partial observability  value following policy state   problem agents complete state   advantage function estimate use   world fact separate views state no• gradient ascent   tice adds tactical element successful poli• using basic formulation derive online version   cies robot fake opponent robot changing    learning rule policys weights updated   direction suddenly knowing robot able   each state visited summation states re•  respond change latency period           moved updating proportionately states contribution      task involves numerous challenges existing multi  policys overall value visiting states  agent learning techniques challenges include continu•   policy need weight later states discount   ous state action spaces elements partial observability factor account smaller contribution iff time   latency violation markov assump•     passed trial start turns equation    tion components memory   tracking obstacle avoidance modules fact   limitations make equilibria cease exist alto•                             gether bowling vcloso reason     algorithm online policy im•  exploring bestresponse learning techniques dont   provement step updating  simultaneously value   directly seek learn equilibrium present gra     estimation step updating value estimation using   wolf bestresponse learning algorithm handle   gradientdescent sarsa lsutton barto    challenges multirobot learning                            feature space policy requires maintaining                                                                  eligibility trace vector update rule                                                                  time state takes action transitioning    grawolf                                                      state time taking action update   grawolf gradientbased wolf combines key ideas        trace weight vector using   reinforcement learning policy gradient learning                                                            wolf variable learning rate policy gradient learning   technique handle intractable continuous state spaces                                                              wolf multiagent learning technique encourages   bestresponse learning algorithms converge situations  sarsa parameter appropriately de•  simultaneous learning briefly techniques    cayed learning rate addition raising power   combined                                     allows actions differing amounts time                                                                  execute semimarkov decision processes sutton    policy radicnt ascent                                    barto    use policy gradient technique presented sutton     policy improvement step uses equation    colleagues  specifically define policy gibbs state time actionvalue es•  distribution linear combination features candi• timates sarsa  used compute advantage   date state action pair let  vector policys  term     parameters let jsa identically sized feature vector   state  action gibbs distribution defines       multiagent systems                                                                                                     forms crux grawolf remains selec•    dimensional input space seven dimensions shown    tion learning rate wolf variable    white overlaid lines figure     learning rate used                                           chose use tile coding sutton barto                                                                   known cmacs construct feature vector tile cod•    win learn fast                                         ing uses number large offset tilings allow    wolf win learn fast method bowling         fine discretization large generalization    veloso changing learning rate encourage    use  tiles size mm distance dimensions    convergence multiagent reinforcement learning scenario  mms velocity dimensions simplify ac•   notice policy gradient ascent algorithm does   tion space requiring attacker select navigation    account nonstationary environment arises point perpendicular line through circles center    simultaneous learning stochastic games   shown dark overlaid line figure  line    agents actions simply assumed envi•     length three times distance attacker    ronment unchanging wolf provides simple way         circle discretized seven points evenly distributed    account agents through adjusting quickly     defender uses points actions nav•   slowly agent changes policy                          igates point bisects line attacker      rate learning changed algorithms      selected action point robots action included    guaranteed locally optimal policies non tiling eighth dimension tile size di•   stationary environments retain property us•    mension entire line actions distinguishable    ing wolf stochastic games simultaneous learning     generalization agents select actions tenth    wolf theoretical evidence limited twoplayer    second three frames unless feature    twoaction matrix games empirical evidence experi•     vector changed time keeps robots    ments games enumerated state spaces en•      oscillating during initial parts learning    courages convergence algorithms dont con•   verge intuition technique learner  results    adapt quickly doing poorly expected    doing better expected cautious presenting results applying grawolf prob•   players likely change policy lem consider issues related evaluation    implicitly accounts players learning    techniques try explicitly reason  evaluation    action choices                                       evaluation multirobot learning algorithms present num•     wolf principle naturally lends policy gradi•  ber challenges problem data gathering    ent techniques welldefined learning rate   real robot platform learning requires far    wolf replace original learning rate      trials practical execute physical world    learning rates used winning los•                believe demonstrate grawolf technique prac•   ing respectively determination winning losing    tical time constraints real robots re•  successful compare value cur•   search standpoint want thorough statistically signifi•  rent policv vs value average policy time evaluation requires far trials just                                  algorithm consid•   used learning solve problem using sim•  ered winning losing policy gradient ulator robot team robots   technique easily compute average pol•     show approach practical robots   icy instead examine approximate value using qw       providing extensive analysis results   current weight vector  average weight vector    second challenge problem evaluating suc•  time  specifically wc winning    cess simultaneous learning traditional singleagent                                                                  evaluation shows performance time converging                                                                  optimal value applicable multiagent domains                                                                  single optimal value independent agents   winning particular state update parameters   behavior optimal value changing time   state using                                 agents learn especially true selfplay                                                                  experiments agents using identical learn•   task                                                   ing algorithm performance success agent   returning robot task shown figure  order     necessarily performance failure   apply grawolf need select policy parameterization       hand want learning robots   specifically need mapping continuous     selfplay improve policy time paper   space states actions useful feature vector main evaluation compares performance learned   define given filtered positions ve•     policy performance initial policy   locities robots form available state information learning initial policy random selection avail•  available actions points field navigation able actions design available actions actually   observing radial angle attacker respect fairly capable policy agents use eval•  circle relevant task arrive seven uation technique challengers examined                                                                                                    multiagent systems figure  value learned policies compared random  figure  value learned policies playing chal•  opponent simulation lines right bars show   lengers simulation lines right bars show   standard deviations                                          standard deviations     littman  technique trains worstcase opponent  specifically measure policys worstcase performance   challenger particular policy generality better worstcase performance exploitable   learned policy paper present challenger results robust policy unknown opponents   demonstrating learned policies robust trained challenger policies learned    wolf variable learning rate plays critical role   trials averaging results used   keeping learning away easily exploited policies     experiment investigate robustness learned                                                                 policy affect wolf variable learning rate    experiments                                               grawolf algorithm left side graph shows   performance graphs section      worstcase performance learned defender policies   axis shows attackers expected discounted reward   right side shows worstcase performance attacker   roughly corresponds expected time takes at• policies wolf corresponds described grawolf   tacker reach circle right graph range algorithm slow does use variable learning rate   shown seconds measurements expected dis•        uses wolfs winning rate fast al•  counted reward gathered empirically course    ways uses losing rate    trials training occurred  trails takes  notice distance worstcase per•  approximately  hours training time robot plat•   formance defender worstcase performance   form simulation unless noted training   attacker fourth column table  respec•  simultaneously learning opponent      tively quite small demonstrates learned poli•  using gra wolf experiments simulation re•   cies quite close equilibrium policy exists   peated times averages shown graphs   means learned policies robust diffi•  examine performance policies learned   cult exploit   simulation examine performance learning      second notice wolf learned defender policy per•  robot                                                    forms better challenger keeps challenger     figure  shows results various learned policies lower reward learning rates   playing opponent following random policy      switches attacker learned policy per•  starting point learning middle bar    forms better challenger fast learning rate   corresponds expected value players   significantly different slow learning rate   following random policy corresponds      couple possible reasons expla•  value attacker following policy learned selfplay nation initialization values values   random defender corresponds value  intialized zero sides amounts opti•  random attacker defender policy learned   mistic initialization defender pessimistic   selfplay                                                    attacker rewards nonnegative attacker     notice desired learning does improve perfor•  mean attacker considers winning far   mance starting policy learned attacker policy   defender causing slower learning rate   random does far better random attacker pol•  employed time evidence effect   icy learned defender experiments demon•    examining percentage updates use versus   strate grawolf improves considerably starting     si during training attacker used slower winning rate   policy experiment explores robust learned    updates defender used winning                                                                 rate  effect value initialization gra•  policy effect wolf component                                                                 wolf interesting explored overall     figure  shows results challenger experiments poli                                                                results certainly dramatic unap  cies trained using simultaneous learning policy                                                                 proximated results bowling veloso wolf   fixed challenger policy trained  trails       multiagent systems                                                                                                    
