            svmc singleclass classification support vector machines                                                           wan jo yu                                             department science                                       university illinois urbanachampaign                                                  urbanail  usa                                                    hwanjoyuuiucedu                               abstract                                absence negative samples labeled data set                                                                  makes unfair initial parameters model        singleclass classification scc seeks distin•                                                                 leads unfair guesses unlabeled data        guish class data universal set                                                                    active learning methods try minimize labeling        multiple classes present new scc algorithm                                                                  labors construct accurate classification function dif•       efficiently computes accurate boundary                                                                  ferent approach involves interactive process        target class positive unlabeled data                                                                  learning users tong koller         labeled negative data                                                                    valiant  valiant  pioneered learning theory                                                                 positive examples based rule learning  de•   introduction                                                 nis defined probably approximately correct pac learn•  singleclass classification scc seeks distinguish     ing model positive unlabeled examples showed   class data universal set multiple classes kdnf disjunctive normal form learnable pos•  distinguishing apples fruits identifying waterfall pic• itive unlabeled examples denis    tures image databases classifying personal home      experimental attempts learn using positive unlabeled   pages web paper target  data tried using kdnf letouzey et al   class positive complement set samples negative     decomite et al  rule learning methods      natural collect noninteresting ob• simple efficient learning nominal features tricky   jects negative data train concept interest• use problems continuous features high dimensions   ing objects positive data scc problems prevalent sparse instance spaces   real world positive unlabeled data widely       positive examplebased learning pebl framework   available negative data hard expensive acquire   proposed web page classification yu et ai    yu et al  letouzey et al   decomite et      method limited web domain binary features     example text web page classification training efficiency poor using svm iter  personal homepage classification collecting negative train•  atively training time quadratic   ing data sample nonhomepages delicate   size training data set problem critical   arduous manually collected negative data      size unlabeled data set large   easily biased persons unintentional prejudice     probabilistic method scc problem re•  detrimental classification accuracy   cently proposed text domain liu et ai    example diagnosis disease positive data easy   specified paper method  revision   access patients disease unlabeled em algorithm  performs badly hard problems   data abundant patients negative data ex• fundamental limitations generative model assump•  pensive detection tests disease expensive tion attribute independence assumption results   patients database assumed negative  linear separation requirement good estimation   samples tested applications   prior probabilities   pattern recognition image retrieval clas• osvm oneclass svm distinguishes class   sification data mining rare class classification data rest feature space given pos•  paper focus scc problem positive unla•    itive data set tax duin  manevitz yousef   beled data labeled negative data                     based strong mathematical foundation osvm                                                                  draws nonlinear boundary positive data set fea•   previous approaches scc                                ture space using parameters  control noise   traditional semisupervised learning schemes suit•   training data control smoothness   able scc labeled negative data     boundary advantages svm   portions positive negative spaces seriously unbal•  efficient handling high dimensional spaces system•  anced known prp « prp           atic nonlinear classification using advanced kernel functions       learning                                                                                                                     figure  boundaries svm osvm synthetic data set big dots positive data small dots negative data      osvm requires larger positive       lems nominal continuous attributes linear nonlin•   training data induce accurate class boundary ear separation low high dimensions section     support vectors svs boundary comes    positive data set small number positive svs    notation    hardly cover major directions boundary espe•   use following notation paper    cially high dimensional spaces svs coming                                                                    • data instance    positive data osvm tends ovcrfit undcrfit    easily tax proposed sophisticated method uses artif   • subspace positive class    ically generated unlabeled data optimize osvms pa•         positive data set sampled    rameters balance ovcrfitting undcrfitting    • unlabeled data set uniform sample univer•   tax duin  optimization method           sal set    infeasibly inefficient high dimensional spaces                                                                    • feature space universal set    best parameter setting performance lags far                                                                        number dimensions    original svm negative data short•          age svs makes incomplete boundary descrip•       example web page classification universal    tion figure show boundaries svm      set entire web uniform sample web    osvm synthetic data set twodimensional space      collection web pages € rm    used lbsvm version  svm implementation        instance web page    lowdimensional space data ob   stensibly smooth boundary osvm result   mapping convergence mc framework   good generalization instead poor expressibility                                                                   motivation   caused incomplete svs   worse highdimensional spaces svs         machine learning theory optimal class boundary   boundary needed cover major directions high function hypothesis hx given limited number train•  dimensional spaces increase number svs      ing data set label considered   osvm ovcrfits accurate shown     gives best generalization performance de•  figure lcandd                                          notes performance unseen examples                                                                  training data performance training data     contributions paper layout                            regarded good evaluation measure hypothesis be•                                                                 cause hypothesis ends overfitting tries fit   discuss optimal scc boundary moti•                                                                 training data hard problem easy classify   vates new scc framework mappingconvergence mc                                                                  boundary function complicated needs   algorithms mc framework generate                                                                  boundary likely overfitted problem   boundary close optimum section  section                                                                   hard classifier powerful boundary   present efficient scc algorithm support vector mapping                                                                  undcrfit svm excellent example super•  convergence svmc mc framework prove                                                                  vised learning tries maximize generalization   svmc iterates mc framework                                                                  maximizing margin supports nonlinear separa•  nearoptimal result training time independent                                                                  tion using advanced kernels svm tries avoid   number iterations asymptotically equal                                                                  overfitting underfitting burges    svm empirically verify analysis svmc   extensive experiments various domains real data sets       optimal scc classifier labeled negative data   text classification web page classification pat• needs maximize generalization   tern recognition letter recognition bioinformatics highly expressive power avoid ovcrfitting undcrfitting   diagnosis breast cancer shows outstand•  illustrate example nearoptimal scc bound•  ing performance svmc wide spectrum scc prob        ary labeled negative data consider synthetic data                                                                  set figure  simulating real situation          httpwwwcsientuedutwcjlinlibsvm                  universal set composed multiple groups data                                                                                                                   learning                                                                           figure  example spaces mc framework                                                                              boundary positive data set feature space   figure  synthetic data set simulating real situation big                                                                            maximizes margin   dots dots big small dots     positive class supposing data             input  positive data set unlabeled data set                                                                            output  boundary function   group center  positive data set sam•  ple assuming big dots sample                    algorithm identifying strong negatives   osvm draws tight boundary shown                         supervised learning algorithm maximizes margin   figure overfits true class area   absence knowledge distribution              algorithm   nearoptimal scc classifiers locate boundary               use construct classifier ho classifies   outside figure maximize                   strong negatives negative positive   generalization mc framework using systemati•                   classify   cally draws boundary figure                                            examples classified negative ho                                                                                     examples classified positive ho    negative strength                                                     set tv                                                                                loop   let hx boundary function positive class                                                                                 nun   outputs distance boundary instance                                                                                use construct      tv                                                                                                                                                                      classify                                 positive instance                                 examples pi classified negative hi                              negative instance                                  examples px classified positive hi                                                                                                                located farther                   repeat                              boundary                        return hi     definition  strength negative instances neg•  ative instances hx   hx                                    figure  mc framework   ifhx  hx stronger     example  consider resume page classification function                  assume subspace tightly subsuming   hx web suppose negative data               class boundary function   objects nonresumepages hx                                                                              algorithm svm figure  let          negative   hx   write resume page                                                                                                                                               space positive space divided  write article  page considered                                                                                 distant boundary resume class             boundary drawn  let ni negative space   relevant features resume class word           positive space divided hi   resume  text true resume page                  boundary drawn induce follow•                                                                           ing formulae mc framework figure  figure     mc framework                                                         illustrates example spaces framework   mc framework composed stages mapping   stage convergence stage mapping stage                                                                                                                                               algorithm uses weak classifier  draws initial   approximation strong negatives  negative data lo•  cated far boundary positive class steps                                                                                                                                                 figure  based initial approximation   convergence stage runs iteration using second base classi•  fier  maximizes margin make progressively              number iterations mc framework   better approximation negative data steps  through               theorem  boundary convergence suppose uni•  figure  class boundary eventually converges              formly distributed algorithm does generate       learning                                                                                                                                 false negatives algorithm maximizes margin      validity component algorithms   class boundary mc framework converges   boundary maximally separates outside          generate false negatives     number iterations logarithmic margin                                                      classification methods threshold control                                                                  tradeoff precision recall adjust   proof  classifier constructed                threshold makes near  recall sacrific•  algorithm does generate false negative classifier    ing precision violations handled   constructed algorithm trained separated        soft constraint svm determining threshold                                                                  intuitive automatic concerning pre•  space divides rest space                                                                  cision quality precision quality does   equal classes boundary                                                                  affect accuracy final boundary far approxi•  maximizes margin           mates certain negative data boundary   repeatedly                    converge eventually figure  visualizes boundary af•  classifier constructed algorithm trained ter each iteration svmc mapping stage identi•  separated space divides rest               fies strong negatives covering wide area   space equal                                  positive data figure used osvm algorithm                                                                     mapping stage intuitively set parameters   margins margin half                                                                  osvm covers positive data   logarithmic margin                                                                  concern false positives precision quality                                                      mapping poor boundary each iteration converges     iteration stops exists               figures final boundary close   sample outside final boundary         true boundary drawn svm figure   located outside maximizing        experiments section  show   margin                                           final boundary accurate initial                                                                  boundary mapping stage rough loose     theorem  proves certain conditions final    setting threshold    boundary located outside how•  example figure framework generates     maximize margin   better boundary located outside                                                                  svm boosting currently popular supervised   theorem  somewhat strong assump•        learning algorithms maximize margin strong   tion uniformly distributed guarantee bound• mathematical foundation svm automatically finds opti•  ary convergence realistic situation  mal boundary validation process   distance classesfigure  shows gaps      parameters tune small numbers theoretically moti•  classesif margin              vated parameters work intuitive setting   smaller iteration convergence stops be•      reasons use svm research prac•  cause margin betweenand                    tice soft constraint svm necessary cope      reduces half each iteration boundary ap•      noise outliers soft constraint svm affect    proaches boundary likely stop con•  accuracy final boundary   verging far unless severely sparse      likely lot noise practice usually                                                                 carefully collected users experiments low setting   following claim                                                                   parameter control rate noise   claim  boundary mc located        training data performs cases reason   outside severely sparse exists used isvm semantically meaningful parameter   visible gaps                                  chang lin                                                                                                                   learning  support vector mapping convergence                           iteration positive svs determined depending        svmc                                                    negative svs hard determine positive data                                                                  svs independent negative svs svm parame•   motivation                                                 ters   classification time final boundary smc sim•      surprisingly adding following statement step   ple mc  svm equal svm              original mc framework figure  com•  final boundary boundary function training      pletes svmc algorithm   time smc long large          reset negative svs   training time svm highly depends size data set            smc runs iteratively                             theorem  training time svmc suppose   assuming number iterations andtsvm    training time classifier   tsvm known quadratic linear      proof simplicity proof approximate each   number dimensions refer chang lin       value follows   discussion complexity svm de•  creasing sampling density reduce training time   hurts accuracy final boundary density   directly affect quality svs final   boundary      svmc   svmc prevents training time increasing dramat•  ically sample size grows prove   svmc iterates mc framework near  optimal result training time independent number   iterations training time asymptotically equal   svm     approach svmc use minimally required data   set each iteration data set does degrade   accuracy boundary saves training time    theorem  states training complexity svmc   each svm maximally illustrate svmc achieves         asymptotically equal svm experiments   consider point starting iteration section  show svmc trains faster smc          smc step  figure  merge     remains accuracy figure  visualizes       need data order   boundary each iteration svmc data set                                                                 figure    construct data far contribute   svs set negative svs representative   data set negative svs           empirical results   newly induced data set support negative                                                                 section show empirical verification anal•  side                                                                 ysis svmc extensive experiments various domains   claim  minimally required negative data minimally         real data sets  web page classification letter recognition   required negative data th iterationthat diagnosis breast cancer  show outstand•  makes accurate boundary constructed    ing performance svmc wide spectrum scc prob•                                                                lems nominal continuous attributes linear nonlin•          negative support vectors                                                                 ear separation low high dimensions   rationale negative svs fromand   negative svs hi closest data set      datasets methodology   iji directions supported      space limitations reports main results   feature space supported negative svs     evaluation based measure   representing data set precision recall used liu et al   negative svs hi excluded constructing    recent works scc positive              suffer negative svs hi need   unlabeled data report accuracy                                                                    used letter recognition breast cancer data sets   support direction tvj does support feature                                                                 uc machine learning repository    direct com•  space negative support vectors hi                                                                                                                     parisons osvm osvm used letter digit   minimally required negative data set  lth iteration                                                             □        refer liu et al  justification using fl     minimally required data set positive side  measure scc   definitely exclude data object each       httpwwwicsucicdumlcarnm lrepositoryhtml       learning                                                                                                              
