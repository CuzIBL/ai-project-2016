aems anytime online search algorithm approximate policy reﬁnement                                          large pomdps                                    stephane´  ross  brahim chaibdraa                        department science software engineering                                laval university quebec canada gk                                      rosschaibdamasiftulavalca                        abstract                          subset belief states acting environment                                                        strategy online pomdp algorithms tries exploit satia      solving large partially observable markov deci   lave  washington  geffner bonet       sion processes pomdps complex task   mcallester singh  paquet et al sincewe      intractable lot effort    need plan current belief state acting      develop approximate ofﬂine algorithms  line needs compute best action      solve larger pomdps state    belief state considering subset belief states      oftheart approaches fail solve large pomdps  reached ﬁnite planning horizon      reasonable time recent developments      line pomdp search suggest combining        drawback online planning generally needs      ﬂine computations online computations meet hard realtime constraints face      efﬁcient considerably reduce large pomdps recent developments                                                                                                               error approximate policies computed   line pomdp search algorithms paquet et al        ofﬂine vein propose new anytime suggest combining approximate ofﬂine online solv      online search algorithm seeks minimize  ing approaches efﬁcient way tackle large      efﬁciently possible error ap pomdps effectively generally compute ap      proximate value function computed ofﬂine ad  proximate policy ofﬂine using standard ofﬂine value itera      dition show previous online computations  tion algorithms use approximate value function      reused following time steps order heuristic function online search algorithm      prevent redundant computations preliminary   ing combination enables online search algorithm plan      results indicate approach able tackle shorter horizons order respect online realtime      large state space observation space efﬁciently straints retain good precision furthermore doing      realtime constraints                  exact online search certain horizon reduces er                                                        ror approximate value functions consequently                                                        does require precision value function    introduction                                       efﬁcient  pomdp framework provides powerful model se   paper propose new anytime online search al  quential decision making uncertainty gorithm aims reduce efﬁciently possible  real world applications huge state space obser error approximate ofﬂine value iteration algorithms  vation space exact solving approaches com algorithm combined approximate ofﬂine  pletely intractable ﬁnitehorizon pomdps pspace value iteration algorithm reﬁne improve approx  complete papadimitriou tsitsiklis  inﬁnite imate policies computed algorithm  horizon pomdps undecidable madani et al  used simple online search algorithm    recent research area focused applied stationary dynamic environments  veloping new ofﬂine approximate algorithms ﬁnd ﬁrst introduce pomdp model ofﬂine  approximate policies larger pomdps braziunas online approximate solving approaches present  boutilier  pineau et al  poupart  smith new algorithm experimental results show  simmons  spaan vlassis  suc efﬁciency  cessful application pomdps real world problems  limited fact approximate al  gorithms intractable huge state space real world  pomdp model  applications main drawbacks ofﬂine ap  proaches need compute policy section introduce pomdp model present  belief state space fact lot computations different approximate ofﬂine online approaches solve  generally necessary agent visit small pomdps                                                    ijcai                                                      model                                              similarly deﬁnition optimal value function                                                                                 π∗  partially observable markov decision process pomdp deﬁne optimal policy equation   model sequential decision making uncertainty                                                                                ∗                                   ∗  using model agent plan optimal sequence π  arg max rb aγ ob av τb                                                                    a∈a  action according belief taking account                     o∈Ω  uncertainty associated actions observations                                                   pomdp     generally  deﬁned     tuple   troγ                                     problem formulation      Ω             state space  inﬁnite number belief states consequence  action set Ω observation set ss ×a×s →                                                       impossible compute policy belief    transition function speciﬁes probability states ﬁnite time shown  ending certain state s given state                  ×  →                  optimal value function pomdp piecewise lin    did action                   reward  ear convex deﬁne optimal value function  function speciﬁes immediate reward obtained                                                                  s   × × →         policy ﬁnitehorizon pomdp using ﬁnite set   doing action state      Ω                  dimensional hyper plan called αvector belief state  observation function speciﬁes probability space exact ofﬂine value iteration algorithms  observing certain observation given did action          ∗                 s    γ                                able compute ﬁnite time  andendedinstate   discount factor         exact value iteration algorithms applied small    pomdp agent does know exactly problems   states high complexity  state currently observations current state                                                                                refer littman cassandra littman   uncertain instead agent maintains belief state cassandra et al   probability distribution states speci  ﬁes probability agent each state  approximate ofﬂine algorithms  agent performs action perceives observation  agent update current belief state using belief contrary exact value iteration algorithms approximate                                                                                                         α  update function τb speciﬁedinequation       value iteration algorithms try subset                                                        vectors each iteration algorithm order limit                                               ηoo   bs     complexity algorithm pineau pineau et al                             s∈s                         pineau  developed point based value iteration al                                                        gorithm pbvi bounds complexity exact value         b                         new belief state belief state iteration number belief points set instead  agent summation speciﬁes expected proba keeping αvectors exact value iteration pbvi                        s  bility transiting state  given performed action keeps maximum αvector belief point                  belief state  afterward expected probability maximizes value precision algo                                              weighted probability agent observed state rithm depends number belief points loca  s               η    doing action  normalization constant tion chosen belief points spaan spaan vlassis  new probability distribution states sums   adopted similar approach perseus instead                                                  π∗    solving pomdp consists ﬁnding optimal policy updating belief points each iteration perseus                                                    speciﬁes best action belief state  dates belief points improved  optimal policy depends planning horizon previous αvector update current iteration  discount factor used order ﬁnd optimal policy perseus generally updates small subset belief points  need compute optimal value belief state each turn converge rapidly approximate  planning horizon inﬁnite horizon optimal policy use larger sets belief points improves  value function ﬁxed point equation        precision recent approach shown                                                       interesting efﬁciency hsvi smith simmons      ∗                                ∗                         bmaxrb aγ       ob av τb    maintains upper bound deﬁned set           a∈a                                                                           α                         o∈Ω                            points lower bound deﬁned vectors hsvi uses                                                        heuristic approximates error belief points    equation rb expected immediate reward order select belief point value iteration  doing action belief state ob proba updates selects belief update updates  bility observing doing action belief state bthis upper bound using linear programming methods  probability computed using equation                                                           approximate online algorithms                                                                                                                        ob   oo  bs   satia  lave satia lave  developed ﬁrst online                  s∈s         s∈s                      algorithm solve pomdps heuristic search algorithm                                                        uses upper lower bounds computed ofﬂine value    equation similar belief update function function conduct branchandbound pruning search  needs sum possible resulting tree pomdp represented andor graph  states s order consider global probability observ belief states ornodes actions andnodes  ing state space                       root node andor graph represents                                                    ijcai                                                    current belief state presented figure  authors straints needs value iteration αvectors  suggested solving underlying mdp upper bound online high complexity  use value function reasonable policy  lower bound heuristic proposed guide aems  search algorithm compared proposed heuristic  section                                        present new  online search algorithm called                                                        anytime error minimization search aems algorithm                                                        aims determine best action current belief                                                                                   state doing lookahead search search                                                        exploring tree reachable belief states current                 aa                                                                      belief state considering different sequence actions                                                        observations tree belief states represented                                                        ornodes choose action child node actions                                                                  represented andnodes consider belief                                                  state child nodes associated different possible obser                                                                              vations presented figure                                                           tree structure used determine value cur                                                        rent belief state best action belief state                 figure  search tree               values actions belief states tree eval                                                        uated backtracking fringe belief state values according    bipomdp algorithm  washington  uses equation  search conducted  classic ao algorithm nilsson  online search inﬁnite horizon use approximate value function  andor graph author slightly modiﬁed ao use  fringe tree approximate inﬁnitehorizon                                                        value fringe belief states tree expanded  lower upper bounds value function lower                                      bound bipomdp precomputes ofﬂine min mdp  low estimate current belief state  guar  est possible value state uses approximation anteed precise discount factor  fringe tree upper bound use aems conducts search using heuristic pro                                                      vides efﬁcient way minimize error current  qmdp algorithm  littman et al  solve         lying mdp use bound heuristic direct belief state  handle large observation space aems  search ao promising actions use able reuse computations previous time  difference lower upper bound guide steps order prevent redundant computations finally  search fringe nodes require precision aems anytime algorithm able exploit                                                        bit time available each turn    rtbss algorithm paquet et al  sim                                                          key idea aems consists exploring search tree  ilar algorithm uses branch bound technique                                                        expanding fringe node highest ex  search andor graph current belief state                                                        pected error contribution current belief state bex  line search tree depthﬁrstsearch                                                        panding belief state reduce error lead  fashion certain predetermined ﬁxed depth                                                        better precision current belief state  reaches depth uses lower bound heuristic evaluate                                                        planning  long term value fringe belief state rtbss uses  upper bound heuristic order prune branches  expected error evaluation  tree pruning possible upper bound value  doing action lower lower bound three key factors inﬂuence error introduced  action belief state                      fringe belief state current belief state ﬁrst    techniques proposed conduct actual error committed using lower bound value  online search pomdps geffner geffner bonet  function instead exact value function evaluate   adapted rtdp algorithm pomdps ap  value fringe belief state order evaluate error  proach requires belief state space discretized compute difference upper lower  generally needs lot learning time performs bound maximal possible error introduced                                                        lower bound function fringe belief state refer  online search algorithm uses observation sam                            pling proposed mcallester mcallester approximation function ˆ deﬁned equation               singh   instead exploring possible observations              −  approach samples predetermined number obser                ˆ                         vations each andnode generative model ub upper bound ∗b lb  environment recent online approach called sovi lower bound ∗b real error b deﬁned  shani shimony  extended hsvi online bv ∗b − lb lower equal  value iteration algorithm authors proposed im approximation ˆb  provements speed upper bound updates evalua error multiplied different factors  tions main drawback approach hardly backtracked search tree taken  applicable online large environments real time account good evaluation impact looking                                                    ijcai                                                    equation  notice value child belief furthermore want know probability  state multiplied discount factor γ certain fringe belief state reached sequence  probability ob reaching child belief state given optimal actions use product rule combine  action taken parent belief state factors probabilities optimal action each depth  value interval   reduce contribution combining factors ﬁnd probability  error parent belief state’s value        reaching certain fringe belief state bd depth denoted    implicit factor considered max bd computed using equation   operator indicates need consider  values belief states reached doing sequence          d−                                                                                     optimal actions words know optimal                  action certain belief state need              pursue search action’s subtree equation oi ai bi denote observation action  action values considered value bin                                                        belief state encountered depth leads belief state  case approximate value function                                                         depth  generally sure certain action optimal                                                          consequently compute expected error intro  uncertainty ac                                                                                    duced certain fringe belief state depth  count considering probability action                                                        current belief state using equation   optimal action given current bounds particular                                      upper bound value certain action lower           bd    γdp bd  bd  lower bound value action a belief state                  ˆ                wearesurethata optimal action probabil use equation  heuristic choose  ity optimal action  fringe node contributes error  time encounter cases upper bound propose different deﬁnitions term  higher highest lower bound handle case ab refer ebd using equation  heuris  assume actions lower bound ﬁxed tic aems ebd using equation  heuristic  assume exact value parent belief state evenly aems  distributed current lower upper bounds intuitively ebd sound heuristic guide  consider types distributions particular search desired properties fa  know certain bound precise vor exploration nodes loose bounds loose bounds  using assumptions evaluate probability generally indicate ﬂawed  certain action best action future exploring node generally important  using equation                                      precision make better decisions ad                                                        dition tight bounds value belief                         ua − lb                ab                             state need search belief state longer                          ub − lb                   low impact quality                                                                                              ua upper bound value action solution belief state eb  favors explo  lb ub  corresponds current lower ration probable belief states encounter  upper bound belief state obtained future good reasons firstly belief  maximum lower upper bound actions state really low probability occurring future  belief state basically computing need high precision value  ua ≥ ∗bv ∗b ∼ uniformlbubie better precision belief state small  probability ua greater ∗b assuming impact value actions consequently  ∗b follows uniform distribution lb action choice secondly exploring proba  ub formula valid ua lb ble belief states increases chance able  mentioned earlier case ab reuse computations belief state fu  sure action optimal ac ture improve precision future finally  tion probability interpreted prob ebd favors exploration actions look promising  ability prune action belief state behavior desired multiple reasons generally  ∗b ∼ uniformlbub  ab hesitate actions best action  probability distribution gives measure likely choice actions highest probability  certain action pruned future remain ing optimal concentrating search actions  optimal action                                   better position decide    alternative way approximate max operator best reason promising actions opti  consider current action highest upper bound mal ﬁnd pretty quickly better  optimal action case use alterna precisions upper bounds  tive deﬁnition ab presented equation      compare heuristic differs                                                       heuristics proposed guide bestﬁrstsearch                                                               argmaxa∈a ua          pompds satia  lave actually proposed similar heuris       ab                                                        otherwize                         tic suggested exploring each iteration fringe                                                    ijcai                                                                                                                 db     db−    nodes maximize term γ ˆb  resulting possible action observation combina  term differs heuristic fact tions computes lower upper bounds  consider term ab actually belief states using lower bound upper bound  makes big difference terms performance practice functions actions lower upper bounds  hand bipomdp explore fringe simply computed using equation  ∗ replaced  node reached sequence actions maximizes respectively notice reach belief                          bound maximizes ˆ  is equivalent choos state tree du                                     db−         plicated current algorithm does handle cyclic  ing fringe node maximizes ˆb   ing equation  ab heuristic does graph structure possibly try use technique  account probability certain belief state going proposed ao lao algorithm hansen zilberstein  reached discount factor applies value  handle cycle investigated fur  belief state explore belief nodes tree ther affects heuristic value  lot impact bounds bagain node expanded need backtrack  experiments show affects performance new upper lower bounds tree order update  bipomdp approach                             probabilities each action optimal reconsider                                                        best action choices backtrack func    anytime error minimization search                tion recursively recomputes bounds using equa                                                        tion  ∗ replaced probabilities  mentioned expected error eb term ab best actions each ancestor nodes leads  seek minimize efﬁciently expanded node b∗ notice bounds certain  anytime fashion exploring fringe belief state ancestor node change need pursue  highest eb term includes prob backtracking process subsequent ancestor  ability ob possibly handle large observation node bounds remain  space generally environment  observations high probabilities  search conducted probable  empirical results  tree furthermore probability ab eb implic present empirical results aems algorithm  itly does pruning non optimal actions limits tested algorithm rocksample environ  search parts search tree actions ment smith simmons  modiﬁed version  small probabilities optimal detailed description environment called fieldvisionrocksample fvrs  algorithm presented algorithm          fvrs new environment introduce test al                                                        gorithm environment big observation space  algorithm  aems   anytime error minimization search observation space size exponential number rocks                                                        presented    function aemst    static  andor graph representing current search each environments ﬁrst computed ap    tree                                               proximate policy pbvi limiting computation time    ← currenttime                                  number belief points small value    currenttime − ≤                     evaluated policy empirically compared im     b∗ ←                          arg maxb∈fringeg                          provement yielded different online approaches using              ∗                                                                   ∗      expandb                                         policy lower bound  upper bound                 ∗      backtrackb                                      used qmdp algorithm solved underlying mdp    end                                           provided resulting value function online al                          return bestactroot                             gorithm online time available decision making                                                        constrained  second action different online    aems algorithm takes time allowed search heuristics presented satia bipomdp aems aems  tree parameter returns best action cur implemented bestﬁrstsearch algorithm  rent belief state current belief state stored root search heuristic affect perfor  node andor graph graph kept mance different implementations com  memory resume search fringe time pared heuristic search performance depth  steps action executed environment ﬁrst search algorithm rtbss using pbvi  graph updated new belief state qmdp value functions lower upper bounds  root simply setting root node  reach following actionobservation path  rocksample  old root node value eb computed quickly rocksample rs environment robot explore  information needed compute value stored environment sample good rocks each rock ei  belief state nodes created updated ther good bad scientiﬁc value robot receives    expand  function simply does onestep lookahead rewards accordingly robot receives rewards leav  fringe belief state given parameter construct ing environment going extreme right en  ing action andnodes belief state ornodes vironment beginning agent knows position                                                    ijcai                                                    
