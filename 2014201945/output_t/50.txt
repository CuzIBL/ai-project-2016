              measuring semantic similarity latent relational analysis                                                 peter turney                                     institute information technology                                      national research council canada                          montreal road ottawa ontario canada ka                                         peterturneynrccnrcgcca                         abstract                          example word pair masonstone analogous                                                         pair carpenterwood relation mason stone        paper  introduces  latent  relational  analysis highly similar relation carpenter wood        lra   method   measuring  semantic  similar                                                          past  work   semantic  similarity  measures   mainly       ity lra measures similarity semantic rela                                                         concerned   attributional  similarity   instance       tions pairs words pairs  latent semantic analysis lsa measure degree       high degree relational similarity                                                         similarity words relations       analogous   example   pair  catmeow                                                          landauer dumais         analogous    pair  dogbark    evidence recently vector space model vsm information        cognitive  science   relational  similarity                                                          retrieval adapted task measuring relational       fundamental cognitive linguistic tasks                                                         similarity achieving score  collection         analogical  reasoning    vector  space collegelevel multiplechoice word analogy questions tur      model  vsm  approach    measuring  relational                                                         ney littman  vsm approach represents       similarity  similarity pairs cal                                                        relation pair words vector frequencies       culated cosine angle vec predefined patterns large corpus        tors   represent    pairs   elements                                                            lra  extends   vsm  approach   three  ways          vectors based frequencies manu                                                        patterns derived automatically corpus       ally constructed patterns large corpus lra ex  predefined     singular  value  decomposition       tends vsm approach three ways  patterns                                                         svd used smooth frequency data used       derived automatically corpus  sin                                                         way   lsa     automatically  generated  syno      gular  value  decomposition   used   smooth  nyms used explore reformulations word pairs       frequency  data     synonyms   used                                                          lra  achieves        analogy  questions  statisti      formulate  word  pairs   paper  describes                                                          cally equivalent average human score        lra algorithm experimentally compares lra     related problem classifying nounmodifier relations lra       vsm tasks answering collegelevel mul                                                        achieves  similar  gains    vsm    problems       tiplechoice  word  analogy  questions   classify                                                        lra’s performance stateoftheart       ing  semantic  relations   nounmodifier  expres motivate research section  briefly outlines        sions lra achieves stateoftheart results reach                                                        possible applications measure relational similarity       ing humanlevel performance analogy ques                                                        related work vsm approach relational similar      tions   significantly  exceeding  vsm  perform ity   described   section     lra  algorithm   pre      ance tasks                                                          sented   section    lra   vsm   experimentally                                                         evaluated performance  word analogy questions     introduction                                       section  classifying semantic relations noun  paper introduces  latent  relational  analysis lra modifier expressions section  discuss interpreta  method measuring relational similarity lra poten tion    results  limitations   lra   future  work    tial  applications    areas  including  information  ex section  paper concludes section     traction  word  sense  disambiguation  machine  translation   information retrieval                              applications relational similarity     relational similarity  correspondence relations                                                          problems   text  processing    solved     contrast attributional similarity  correspon  greatly  simplified      black  box     dence   attributes  medin et  al                                                                input   chunks   text   produce   output    words high degree attributional similarity say                                                         measure degree similarity  meanings   synonymous  pairs  words high chunks use information retrieval ques  degree   relational  similarity   say   analogous  tion  answering  machine  translation  using  parallel corpora                                                                                             ⋅r  information  extraction  word  sense  disambiguation text                 ƒ        ⋅r  summarization measuring lexical cohesion identifying sen cosine θ                                                                                                                 ⋅  timent   affect   text     tasks  natural          ⋅                                                                                      ƒr    ƒ     language  processing     vision   motivates       ii          search paraphrasing barzilay mckeown   make   vector   characterize   relationship   textual entailment dagan glickman  topics tween  words counting frequencies   lately attracted             various short phrases containing turney litt    absence black box current approaches man    use   list     joining  terms   “of”   problems typically use measures attributional simi “for”  “to” form  phrases contain   larity example standard bagofwords approach “x  y” “y  x” “x  y” “y  x” “x  y”    information  retrieval   based   attributional  similarity “y  x”  phrases used queries   salton mcgill  given query search engine search engine number hits matching documents   produces   ranked  list   documents    rank   recorded each query process  yields vector   document  depends    attributional  similarity    numbers number hits query   document    query   attributes   based  word corresponding element vector log       frequencies relations words ignored       turney littman  evaluated vsm approach      attributional  similarity  measures    use   performance     collegelevel  multiplechoice   ful believe limited supple sat analogy questions achieving score  sat   mented   relational  similarity  measures  cognitive  psy analogy  question  consists    target  word  pair  called    chologists    argued   human  similarity  judge stem    choice   word  pairs   answer   analogy   ments  involve   attributional   relational  similarity question  vectors   created    stem  pair   each   medin et al                                  choice  pair    cosines   calculated    angles     consider word sense disambiguation example iso   stem  vector   each  choice  vector   best   lation word “plant” refer industrial plant guess choice pair highest cosine use    living  organism  suppose   word  “plant”  appears  set analogy questions evaluate lra section     text near word “food” typical approach dis best previous performance sat questions   ambiguating  “plant”   compare   attributional  simi achieved combining thirteen separate modules turney et   larity   “food”   “industrial  plant”    attributional al   performance lra significantly surpasses   similarity   “food”   “living  organism”  lesk   combined real contest   banerjee pedersen  case decision  approaches     simply  add  lra    clear industrial plants produce food combination fourteenth module vsm mod  living organisms serve food help ule best performance thirteen modules tur  ful know relation “food” “plant” ney et al   following experiments focus com  example    text  “food    plant”   relation  paring vsm lra   tween  food   plant  strongly  suggests    plant   vsm evaluated performance dis  living organism industrial plants need food tance  measure    supervised  nearest  neighbour  classifier    text  “food    plant”   relation  strongly  suggests  nounmodifier  semantic  relations  turney   littman     plant    industrial  plant   living  organisms  problem classify nounmodifier pair   usually considered locations               “laser  printer”  according    semantic  relation      measure   relational  similarity   potentially  im tween head noun printer modifier laser   prove   performance    text  processing  application evaluation  used    nounmodifier  pairs      currently uses measure attributional similarity manually labeled  classes semantic relations nas  believe  relational  similarity     step   attribu tase szpakowicz  example “laser printer”   tional similarity black box envisioned classified instrument  printer uses laser                                                        strument printing testing pair classified search    related work                                       ing    single  nearest  neighbour    labeled  training   let  semantic relation pair words data best guess label training pair                                                        highest cosine training pair analo   let semantic relation   pair   wish measure relational similarity gous  testing pair according vsm lra evalu                                                        ated set nounmodifier pairs section     relations given     task    infer   hidden  latent  relations    compare                                      latent relational analysis     vsm approach turney littman  lra takes input set word pairs produces  create vectors represent features  measure relational similarity   measure similarity cosine                                                             input  pairs  lra  relies   three  resources                                           angle           search engine large corpus text  broad                                                        coverage  thesaurus   synonyms      efficient  implementation svd lra does use labeled data struc column frequency jth pattern step    tured data supervised learning                   phrases contain ith word pair step      lra proceeds follows                              calculate  entropy   apply  log   entropy  transforma   alternates  each word pair ab  input set tions    sparse  matrix  landauer   dumais     look thesaurus numsim  words fol each  cell   replaced    logarithm  multiplied     lowing experiments numsim    similar weight based negative entropy corresponding   each a′  similar make new word pair column vector matrix gives weight pat  a′b   likewise  look    numsim   words   terns vary substantially frequency each pair   similar each b′  make new word pair  apply svd  log entropy transformations ap  b′  ab    called  original   pair   each a′b   ply svd  svd decomposes  product three   b′  alternate  pair intent alternates matrices uΣv    column orthonor  semantic relations original   mal  form    columns   orthogonal    unit     filter  alternates    each  original  pair ab   filter  length  Σ     diagonal  matrix  singular  values     × numsim   alternates   follows   each  alternate  pair svd golub van loan   rank                                                                 Σ                     Σ            send query search engine frequency   rank  let     phrases begin member pair end diagonal matrix formed  singular values        phrases     maxphrase  let   vk     matrices  produced   selecting    words use maxphrase    sort alternate pairs corresponding  columns       matrix                                                            Σ    frequency phrases select numfilter  vk  matrix rank  best approximates   frequent  alternates   discard   remainder   use original  matrix     sense    minimizes   ap  numfilter     alternates dropped step tends proximation  errors  golub   van  loan                                                                                  Σ    eliminate alternates clear semantic relation think    matrix vk     “smoothed”   “com     phrases    each  pair  originals   alternates pressed” version original matrix svd used  make   list   phrases    corpus   contain   pair duce noise compensate sparseness                                                                                   Σ  query search engine phrases begin  projection  calculate  use    rec  member pair end minimum ommended   landauer   dumais     matrix   mininter  intervening words maximum maxinter     number   rows       columns   intervening  words   use mininter     maxinter       instead     × numpatterns   columns    experiments   maxphrase  –     ignore  suffixes   searching      columns  instead        use    phrases   match   given  pair   phrases  reflect  want calculate cosines row vec  semantic relations words each pair    tors proven cosine                                                                         Σ     patterns    each  phrase     previous row vectors  cosine                                                                                        Σ    step build patterns intervening words pattern corresponding row vectors vk    constructed replacing interven  evaluate alternates  let ab  cd  word   ing  words wild cards wild card replace pairs input set step  numfilter       word   each  pattern  count   number   pairs versions ab  original numfilter  alternates like  originals alternates phrases match pattern wise   numfilter      versions  cd      wild  card   match  exactly   word    numfilter     ways compare version ab                                                                                                         Σ  numpatterns  frequent patterns discard rest version cd  look row vectors    use numpatterns    typically millions  correspond    versions  ab     versions    patterns feasible  cd    calculate  numfilter       cosines    ex   map pairs rows  preparation building matrix periments  cosines     suitable svd create mapping word pairs row  calculate relational similarity  relational similar  numbers   each  pair ab  create   row  ab    ity  ab   cd     average    cosines   row ba  make matrix symmet  numfilter       cosines   step       rical reflecting knowledge relational similarity greater equal cosine original pairs ab    ab  cd  relational cd    requirement    cosine    greater   similarity ba  dc  mason stone car equal original cosine way filtering   penter wood stone mason wood carpen poor analogies introduced step    ter intent assist svd enforcing symmetry slipped through  filtering step   averaging    matrix                                        cosines opposed taking maximum intended    map patterns columns  create mapping provide resistance noise   numpatterns  patterns column numbers each pattern   experiments   input  set  contains       create column “ word  word ” column  word pairs steps   repeated each    “ word   word ”        × numpatterns  input pairs compared    columns                                           following experiments use local copy     generate   sparse  matrix generate   matrix   waterloo multitext wmts search engine   sparse  matrix  format   value    cell   row  corpus  ×    english words corpus gathered web crawler academic web sites ence    average  human  score    score   clarke et  al      wmts    distributed  multi lra statistically significant   processor  search  engine  designed  primarily   passage    questions     word  pairs   question    retrieval document retrieval possible spe stem choices  pairs input set   cial  case   passage  retrieval   local  copy  runs   step  introducing alternate pairs multiplies number   cpu beowulf cluster                               pairs resulting  pairs step  each     wmts suited lra scales pair  ab  add ba  yielding  pairs    large  corpora   terabyte    case   gives  exact pairs   dropped    correspond   zero  vectors   frequency  counts  unlike   web  search  engines  appear window words   designed   passage  retrieval    document   wmts  corpus     words    appear    trieval powerful query syntax         lin’s  thesaurus    word  pairs  appear  twice       source synonyms use lin’s  automati sat questions lioncat sparse matrix step    cally generated thesaurus lin’s thesaurus generated   rows  word  pairs     columns  patterns   parsing corpus  ×   english words consisting density  percentage nonzero values   text wall street journal san jose mercury table    compares  lra   vsm      analogy   ap  newswire  lin     parser   used   extract questions  vsmav  refers    vsm  using  altavista’s   pairs words grammatical relations words database corpus vsmav results taken   clustered synonym sets based similarity turney   littman     estimate   altavista    grammatical  relations   words   judged  search index contained  ×    english words   highly similar tended kinds time   vsmav  experiments  took  place  turney    grammatical relations sets words    littman  gave estimate  ×    english words     given word speech lin’s thesaurus pro   believe   estimate   slightly  conservative   vides list words sorted order decreasing attribu vsmwmts  refers    vsm  using   wmts    tional similarity sorting convenient lra contains     ×      english  words   generated    makes possible focus words higher attributional vsmwmts results adapting vsm wmts    similarity ignore rest      use rohde’s svdlibc implementation singu    table  lra versus vsm  sat analogy questions   lar  value  decomposition    based   svdpackc            vsmav  vsmwmts       lra   berry                                                correct                                                                                    incorrect                       experiments word analogy questions                   skipped                           table  shows  sat analogy questions    total                           relational  similarities    stem   each score              choice calculated lra choice highest three pairwise differences three scores table   relational similarity correct answer ques  statistically significant  confidence using   tion quart volume mile distance     fisher exact test using corpus vsm lra   table  relation similarity measures sample sat question achieves score  vsm achieves score                                                             absolute  difference       relative  im      stem        quartvolume  relational similarity  provement  vsm corpus times lar      choices   daynight                  ger lra’s corpus lra ahead absolute                 miledistance               difference  relative improvement                  decadecentury                comparing  vsmav   vsmwmts   smaller  cor                frictionheat               pus reduced score vsm drop                 partwhole                  larger number questions skipped                                                            vsmwmts  versus     vsmav       lra  correctly  answered        analogy  ques smaller corpus   input  word pairs simply   tions   incorrectly  answered    questions  ques appear short phrases corpus lra   tions skipped stem pair alternates able answer questions vsmav   did appear phrases corpus  uses    corpus   vsmwmts   lin’s   choices    relational  similarity   zero     thesaurus  allows  lra   substitute  synonyms   choices each question expect answer words corpus    questions correctly random guessing vsmav  required    days   process     analogy   fore score performance giving point each questions turney littman  compared  days   correct  answer     points   each  skipped  question  lra    courtesy   altavista  turney   littman   lra attained score   sat questions    inserted    second  delay   each  query      average  performance   collegebound  senior  high   wmts   running  locally     need    school  students   verbal  sat  questions  corresponds   delays vsmwmts processed questions day   score  turney littman  differ  experiments nounmodifier relations           vsm accuracies measures significant                                                         using corpus vsm lra’s accuracy    section describes experiments  nounmodifier higher absolute terms  higher relative terms   pairs  handlabeled     classes   semantic  relations   nastase szpakowicz  experiment table  comparison lra vsm class problem   class problem class problem  classes            vsmav  vsmwmts      lra   semantic  relations  include cause     “flu  virus”                                                                 correct                       head noun “virus” cause  modifier “flu” loca                                                               incorrect                     tion  “home town” head noun “town” lo                                                               total                         cation  modifier “home”  “printer tray”                                                                accuracy                 head noun “tray”  modifier “printer”                                                                precision                topic     “weather  report”   head  noun  “report”                                                                 recall                   topic  “weather” list classes nas                                                                                      tase szpakowicz  turney littman       classes  belong     general  groups   relations   causal   relations temporal   relations spatial   relations par table  comparison lra vsm class problem   ticipatory  relations “student protest” “student”       vsmav  vsmwmts      lra   agent  performs “protest” agent  partici correct                     patory  relation qualitative  relations “oak tree” incorrect              “oak” type  “tree” type  qualitative  relation total                          following  experiments  use  single  nearest  neighbour accuracy               classification   leaveoneout  crossvalidation   precision                leaveoneout  crossvalidation   testing  set  consists   recall           single nounmodifier pair training set consists                        remaining  nounmodifiers   data  set   split     times each nounmodifier gets turn testing   word pair predicted class testing pair class   discussion     single  nearest  neighbour    training  set    experimental  results   sections       demonstrate   measure nearness use lra calculate relational lra performs significantly better vsm   similarity testing pair training pairs clear room improvement accu    following  turney   littman     evaluate  racy      adequate   practical  applications   performance accuracy macroaveraged past work shown possible adjust   measure  lewis      measure    harmonic tradeoff   precision  versus  recall  turney   littman   mean precision recall macroaveraging calculates       applications    information   precision  recall     each  class  separately   extraction lra suitable adjusted high   calculates average classes            precision expense low recall       word pairs input set lra step limitation speed took days    introducing alternate pairs multiplies number pairs lra answer  analogy questions pro  resulting  pairs step  each pair  ab  gress    hardware  speed   gradually     add ba   yielding   pairs   pairs   dropped concern software optimized   correspond zero vectors words  speed     places    efficiency   appear lin’s thesaurus sparse matrix step  increased operations parallelizable    rows  columns density  possible precompute information     table  shows performance lra vsm lra require substantial changes   class problem vsmav vsm altavista cor algorithm   pus vsmwmts vsm wmts corpus        difference   performance   vsmav    results   vsmav   taken   turney   littman vsmwmts shows vsm sensitive size    three pairwise differences three  meas corpus   lra   able   surpass  vsmav    ures statistically significant  level according wmts corpus tenth size av   paired ttest accuracy lra significantly corpus likely lra perform better   higher    accuracies   vsmav   vsmwmts larger corpus wmts corpus requires terabyte   according    fisher  exact  test    difference  hard disk space progress hardware likely make   tween vsm accuracies significant using      terabytes  affordable    rela  corpus vsm lra’s accuracy  higher tively near future   absolute terms  higher relative terms         nounmodifier  classification   labeled  data     table  compares performance lra vsm   yield  performance  improvements     noun   class  problem   accuracy    measure  lra modifier pairs  classes average class    significantly higher accuracies measures examples expect accuracy improve sub  vsmav vsmwmts differences 
