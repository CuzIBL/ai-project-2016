                     unsupervised learning backward inhibition                                                        tomas    hrycej                                          pcs systeme gmbh                       pfaelzerwaldstr  doo  muenchen  west germany                              abstract                            al  analogical mathematical concepts                                                              discriminant regression analysis        backward inhibition twolayer                    kohonen  symbolic supervised learning        connectionist network used               winston  quinlan         alternative enhancement             kodratoff ganascia         competitive model unsupervised learning          situations data available sufficient        feature discovery algorithms based             supervised learning        backward inhibition presented                  correct outputs responses        shown superior                        known cases merely overall        competitive feature discovery algorithm                 criterion response quality given        feature independence controllable grain               survival criterion living organisms reaching        representation feature                positive biological emotional state        layer distributed certain                        humans lead modification        classification hierarchy defined               supervised learning  instead set correct        features discovered                                       responses reinforcement criterion                                                                   given                                                                 weaker reinforcement direct     introduction                                                 learning correct responses fail                                                                   high complexity input illustrated    connectionist neural network models                                                                   backpropagation model rumehart et al    models consisting networks simple processing              shown limited class    nodes shown possess cognitive                    inputoutput encodings materialized    capabilities particular capabilities shape         twolayer connectionist    recognition hinton lang  fukushima                 complex multiplelayer systems     von der malsburg bienenstock               corresponding learning schemes    development concepts dalenoort  learning            developed multiplelayer systems    grossberg   ackley et al                    scale poorly  systems layers    rumelhart et al  le cun  storing              computationally intractable    patterns hopfield  generalization anderson           obvious limitations supervised learning                                                       complex inputs require layers      connectionist models provide important                  layers turn intractable    advantages symbolic models                                                                proposal solution problem      • automatic development representation                                                              ballard  partition network      • correction noisy input                            modular way network      • graceful degradation cells erroneous     autoassociation form unsupervised learning      • generalization                                    ballards model distributed highly invariant      • inherent parallelism                                 representation input supervised learning                                                              applied discovered representation      connectionist models typically transparent                                                              instead original input    symbolic ones preferable                                                                general learning models capable    possible form knowledge input learning                                                              performing unsupervised classification discovering    examples                                                              characteristic features regularities input data      straightforward form learning                                                              presented rumelhart zipser     supervised learning associating inputs                                                              grossberg  perform essentially    known outputs bestknown algorithms                                                              clustering algorithm network form    backpropagation algorithm rumelhart et al                                                               symbolic analogy conceptual clustering    boltzman machine learning algorithm ackley et                                                              stepp michalski  hybrid model        parallel distributed processing lebowitz  models suffer                  rule special case adaptation law studied   deficiencies argued section                 kohonen  page  case  shown   model rumelhart zipser suffers                       kohonen weight vector converges dominant   random uncontrollable grain feature discovery              eigenvector eigenvector corresponding    constructing redundant local feature                        largest dominant eigenvalue input correlation   representation  difficulties building feature            matrix exx   hierarchies details section                       equally true weight vectors     paper presents novel type feature                      feature units discover feature dominant   discovery model backward inhibition model                 eigenvector model candidate   does suffer deficiencies                       feature discovery modified   mathematically related variance                   ways perform satisfactorily   analysis cluster analysis                                                                       competitive correlation model    existing models                                                                      possible modifications competitive   recent past models feature discovery               model proposed rumelhart zipser    presented classified                general model grossberg    competitive models wellknown representant                 feature layer model consists multiple feature   class critically examined section                nodes instead activating each feature      introduce mathematical concepts                         proportionally weighted input feature   necessary explanation backward inhibition                   cell feature unit maximal activation   concept rudimentary onefeature model                        maximal similarity  inner vector product   presented section                                           weight vector input vector fires                                                                      weights modified    simple correlation model                                          obvious interpretation learning process   basis models treated paper                model given input vector feature   simple noncompetitive twolayer correlation model                 maximal value similarity measure inner   referred basic adaptive model                 vector product input currents   nodes input layer connected single node            weights learns moving weights input   feature layer figure  feature                vector competition causes each input vector   represented vector weights assigned                 attract nearest feature results   connections feature node activation level             partitioning input exclusive clusters   given wx input vector activation                  competition features   rule model learns widely used correlation              different inputs probably different features   learning rule continuous form                           similar ones single common      dw  ayxdt                                                  converge stable state each                                                                      feature corresponds cluster input stimuli      ala constant                                                                      similarity members single cluster                                                                      significantly higher similarity                                                                      different clusters formal description                                                                      learning process given rumelhart                                                                      zipser                                                                          shortcoming model feature                                                                      cells remain inactive                                                                      outperformed                                                                         direct consequence behaviour                                                                      typically coarse features way                                                                      refine grain discovered features                                                                      increasing number feature cells                                                                      feature cells higher probability                                                                      remain inactive                                                                        proposals remedy rumelhart                                                                      zipser  analysis shown                                                                      general case exhibit better                                                                      performance basic competitive model                                                                      hrycej                                                                          additional deficiencies                                                                      competitive model                                                                          features competitive model                                                                           disjoint number substantially higher                                                                           features                                                                           independent satisfying condition                                                                                                                         hrycej       guaranteed  contrary finding        features frequent        section  fact      original input xx  corrected input        independent feature  features simply        feature unit suppressed feature unit        logical complements each                   activated       representation input feature layer        local each input activates single feature       suppression simulated propagating        node representation does           inhibitory signal size feature activation        possess advantageous properties error backwards input units feature node        correction efficient coding automatic activated activation level  connections        generalization hinton et al  strengths   input nodes        distributed representations                         respectively inhibits nodes amounts       features discovered  respectively        competitive model represents finer partitioning       properties backward inhibition        results directly          used sections constructing feature        disjointness features        discovery algorithms        hierarchical level hierarchical modification        possible additional         eigenvectors features        layers added clusters discovered                                                             adaptive rule section  discovers        lower levels partitioned                                                             dominant eigenvector input correlation matrix        higher levels arrangement requires                                                             use backward inhibition principle previous        priori structuring network tree                                                             section furhter features converge        structure reasons structure                                                             eigenvectors eigenvectors input correlation        inefficient successor nodes                                                             matrix successively applying        inactive feature node remain                                                             backward inhibition        inactive                                                                eigenvectors correlation matrix       additional drawback algorithm    properties make intuitively good        maximum finding parallel operation            candidates meaningful features        parallel alternatives       athey extract highly correlated input lines        maximization reggia  chun et al                                                                input lines mutually independent                                                                          eigenvectors unit vectors dominant      note criticism concerns                     eigenvector corresponds input line   featurediscovery properties competitive model         largest variance important input   valuable properties competition          feature   noise reduction contour enhancement                                                                input lines completely correlated   covered alternative backwardinhibition model                                                           dominant eigenvector corresponds vector                                                                  input variances correlation matrix                                                                  diagonal    backward inhibition                                                                correlation matrix partitioned   drawbacks competitive model result            diagonal submatrices corresponding   lack capability discover finer features         mutually independent groups highly correlated   simultaneously coarse ones          input lines eigenvectors corresponding   nature competition section presents            each groups   concept backward inhibition copes            eigenvectors correlation matrix   problem                                                  symmetric matrix orthogonal                                                                  independent    backward inhibition concept                              define linear transformation                                                                  recodes input completely possible   basis reflections idea                                                                  given dimensionality words   subtle features discovered strong                                                                  explain variability input   features overshadowing suppressed                                                                  compact way sense main components   way      implementation idea simple           set eigenvectors   strong feature successfully learned  appropriate feature   suppressed input time   optimal way suppress feature input        latency time algorithm   vector isolate component orthogonal      stated previous section eigenvectors   suppressed feature subtract orthogonal      input correlation matrix successively   projection input vector feature weight        applying backward inhibition simplest method   vector wj feature suppressed          use sequential algorithm   represented wj wj                     learns dominant eigenvector lets        parallel distributed processing weight vector feature node converge                  discovered latency time long week   dominant eigenvector inhibits dominant                    features discovered   feature input orthogonal projection                   representation input feature layer   eigenvector second largest                   distributed following example   eigenvalue dominant eigenvector                                                                          eigenvectors ordered   modified input turn discovered                                                                            eigenvalues represent certain implicit   feature node                                                                            hierarchy illustrated following   eigenvectors discovered fact                                                                            example    gramschmidt process evaluation orthogonal   projections kohonen  page  ordering                   example  let  input patterns figure                                                                       represented  binary input lines   features corresponds descending ordering                                                                      symmetry  substituted zeros    eigenvectors absolute value corresponding                                                                      theoretically  features    eigenvalues                                                                      satisfied  features features discovered     number features                                                                      backward inhibition algorithm presented table      learn fth feature basic adaptive rule                                                                        contained patterns activation      backwardinhibiting input features fff                                                                      value each feature multiplied  given each      obvious drawback cognitive inadequacy               feature column easier orientation abridged    algorithm sequentiality certain        code given middle column symbol meanings    degree intrinsic characteristics problem                feature activation    feature    adaptation rule learns                  activation      dominant feature words small features    enabled come shadow large    features filtering large features occur    sequentially control    parallel better say local each feature node    following extended activation rule applied    simultaneously features    step  activate feature nodes basic adaptive      model activation rule    step  weights certain feature node      reached temporarily stable state feature      node performs backward inhibition input nodes      certain number cycles latency time      learning rule remains identical    basic adaptive model learning latent feature node    suspended during latency time      algorithm criterion weight stability    needed simple test weight increment    zero small appropriate    input inherently varying weights    completely stable average recent    periods used used exponentially    weighted average weight increment vv past    cycles edwi edwabs ratio average increment    average absolute value increment edw     pedw  pw edwabs  edwabs     vpw ratio converges zero positive    increments periodically tradedoff negative                   classification hierarchy formed descending    increments absolute values                     order eigenvalues given figure                                                                       dominant feature  partitions paterns    relatively large depending acoefficient                                                                       diagonally oriented  marginoriented     adaptive rule                                                                      diagonally oriented partitioned feature       feature discovery model superior                                                                      right  left  left     competitive aspects                                                                      right  features   measure balancedness        eigenvectors correlation matrix                                                                      diagonal patterns         orthogonal correlation matrix                                                                         hierarchy explicit feature vector         symmetric independent                                                                      implicit encoding needed        grain feature discovery                       input associative network         controlled latency time parameter latency             associations depend strong features like         time short strong features rapidly learned           diagonal vers marginoriented encoded         time weak features             single feature cell weaker ones like diagonal                                                                                                                         hrycej  orientation encoded   special features adding feature cell    competitive algorithm   actual encoding corresponds                                                             backward inhibition used improve grain   path hierarchy tree                             control competitive model section                                                              feature unit won competition                                                             weights incrementally learned feature                                                             represented weight vector wj partially                                                             suppressed time                                                                xx   byivi                                                                    parameter controls extent                                                             suppression grain feature discovery                                                             competition features correspond                                                             eigenvectors                                                                model difficult treat                                                             mathematically supposed                                                             verified experimentally example                                                              weak features chance attract weight                                                             vector grain feature discovery                                                             refined     feature used classify novel           example  illustrate grain control set   patterns illustrated following example        input vectors taken       example  novel patterns                     experimentally classified figure      correlation matrix   table  note pattern    classified right  left balanced   undecided marginoriented diagonal   pattern  marginoriented pattern    diagonal balanced undecided direction                                                                   learning trials                                                             feature units                                                                  equivalent original model backward                                                             inhibition average numbers active features                                                             given table                                                                   unit case showed poor convergence                                                             corresponding results presented                                                             seen basic competitive algorithm     note short latency times partitioned patterns typically three groups   approximations eigenvectors       matter feature units   residue suppressing strong eigenvectors     backwardinhibitionbased algorithm   substantially differ exact features    contrast three groups maximum   corresponding weak eigenvectors different input vectors   substantially differ exact eigenvectors        depending backward inhibition strength   cases tested showed high   orthogonality good properties features         conclusion   discovered preserved                                                             backward inhibition twolayer connectionist                                                             network used alternative                                                             enhancement competitive feature discovery                                                             model        parallel distributed processing 
