        networked distributed pomdps                        synergy distributed                         constraint optimization pomdps               ranjit nair       pradeep varakantham milind tambe               makoto yokoo      knowledge services group           science dept          dept intelligent systems        honeywell laboratories       university southern california        kyushu university      ranjitnairhoneywellcom         varakanttambeuscedu           yokooiskyushuuacjp                        abstract                                        loc    loc                                                                           realworld multiagent applications                                        distributed sensor nets network                      loc             loc      agents formed based each agent’s limited                              loc      interactions small number neighbors      distributed pomdps capture real                                             world uncertainty multiagent domains      fail exploit locality interaction dis figure  sensor net scenario present target      tributed constraint optimization dcop cap      loc loc loc target loc      tures locality interaction fails cap loc      ture planning uncertainty paper      present new model synthesized distrib     domain unlikely work domain      uted pomdps dcops called networked           cause geared advantage local      distributed pomdps ndpomdps        ex        ity interaction result consider      ploiting network structure enables present  possible action choices noninteracting agents      distributed policy generation algorithm    trying solve distributed pomdp distributed      performs local search                            constraint satisfaction distributed constraint opti                                                        mization dcop applied sensor nets                                                        approaches capture uncertainty     introduction                                      domain introduce networked distributed  realworld multiagent applications dis pomdp ndpomdp model hybrid pomdp  tributed sensor nets network agents formed based dcop handle uncertainties domain  each agent’s limited interactions small num advantage locality interaction ex  ber neighbors instance distributed sensor nets ploiting network structure enables present novel  multiple sensor agents coordinate neigh algorithm ndpomdps – distributed policy gen  boring agents order track individual targets mov eration algorithm performs local search  ing through area particular consider  paper problem motivated realworld challenge    ndpomdps  lesser et al  each sensor node scan  directions — north south east west deﬁne ndpomdp group    ag agents  figure  track target sensors tuple hs Ω bi  ×≤i≤nsi ×  overlapping scanning areas coordinate scanning su set world states si refers set  area simultaneously assume local states agent su set unaﬀectable  independent targets each target’s  states unaﬀectable state refers world  ment uncertain unaﬀected actions state aﬀected agents’ actions  sensor agents additionally each sensor receives obser environmental factors like target locations agent  vations area scanning obser control  ×≤i≤nai set joint actions  vation false positives false negatives ai set action agent  each agent pays cost scanning target    assume   transition independent distributed  present cost turned oﬀ existing dis pomdp model transition function                                                                       ′          ′                      ′  tributed pomdp algorithms  montemerlo et al   ﬁned   pusu su·q≤i≤n pisi su ai si  nair et al  becker et al  hansen et al  aha     ani joint action performed                                                                                   ′   ′     ′  ′  rich capture uncertainties state shs     sn sui hs     sn suiis resulting state agent i’s transition function deﬁned contain agent              ′        ′  pisi su ai si  prsisi su ai unaﬀectable                                                                    ′       ′        vπni    busubni sni bisi sl    slk su hi  transition function deﬁned pusu su  prsusu                          πl  becker et al  relied transition indepen     sisni su          l∈e st i∈l  dence goldman zilberstein  introduced                                                    possibility uncontrollable state features                                                        trying ﬁnd best policy agent given  works authors assumed state collectively                                                        neighbors’ policies need consider non  observable assumption does hold                                                        neighbors’ policies property locality  mains                                                        interaction used following section    Ω   ×≤i≤nΩi set joint observations  Ωi set observations agents make      locally optimal policy generation  assumption observational independence  deﬁne joint observation function os ω   locally optimal policy generation algorithm called                                                        lidjesp   locally interacting distributed joint equi            ω   hs        q≤i≤n                                  librium search policies based dba algo        ω  ω      ω       ω       ni           ni        rithm yokoo hirayama  jesp   nair et     ω    pr                                         al  algorithm algorithm  each    reward function  deﬁned  rs     agent tries improve policy respect neigh  pl rlsl     slk su hal     alki each bors’ policies distributed manner similar dba  refer subgroup agents   initially each agent starts random policy  sensor grid example reward function expressed exchanges policies neighbors evalu  sum rewards sensor agents ates contribution global value function  overlapping areas   reward functions initial belief state respect current policy  individual agent’s cost sensing   based neighbors’ policy function evaluate agent  reward function construct interaction hyper tries improve current policy calling  graph hyperlink exists subset function getvalue returns value agent  agents rl comprise interaction hypergraph i’s best response neighbors’ policies agent  deﬁned  ag agents ag computes gain make local neigh  vertices  ll ⊆ ag ∧ rl component  borhood utility exchanges gain neighbors  edges neighborhood deﬁned ni  ∈ i’s gain greater neighbors  agj  ∧ ∃l ∈ ∈ ∧ ∈ sni  ×j∈nisj refers changes policy sends new policy  states i’s neighborhood similarly deﬁne neighbors process trying improve local  ani  Ωni  pni oni                              policy continued termination based    distribution initial state deﬁned maintaining exchanging counter counts                                                        number cycles gaini   omitted  bs  busu · q≤i≤n bisi bu bi refer  distributions initial unaﬀectable state i’s article order simplify presentation                                                          algorithm computing best response  initial state respectively deﬁne                                         qj∈ni       dynamicprogramming approach similar used  assume available agents jesp deﬁne episode agent time  possible reﬁne model make available agent et  st  st st  ωt  given neighbors’ poli                                                       ni  ni       ni  goal ndpomdp     cies ﬁxed treating episode state results                    π    π      π  compute joint policy      ni maximizes  single agent pomdp transition function                                            team’s expected reward ﬁnite horizon starting  observation function deﬁned follows  πi refers individual policy agent                                                          ′                   mapping set observation histories ei ai ei pusu su  · pisi su ai si  ·  ai πni πl refer joint policies agents                                                                                     pni sni  su ani  sni  · oni sni  su  ani  ωni   ni hyperlink respectively    ndpomdp thought    nary dcop              ′                                                                              ω  ois       ai ωi  variable each node individual agent’s                          policy reward component rl    function getvalue   returns optimal policy  thought local constraint reward com agent given deﬁnitions transition  ponent rl   corresponds nonlocal observation functions policies ni pa  straint constraint graph section getvalue implemented value iteration  push analogy taking inspiration single agent pomdp algorithm  dba algorithm  yokoo hirayama  algo   used  rithm distributed constraint satisfaction develop  algorithm solving ndpomdps                      experimental results    deﬁne  local neighborhood utility agent experiments ran lidjesp algorithm  expected reward accruing hyperlinks  sensor domain figure  ﬁrst benchmarkalgorithm   lidjespagent                                                                      lidjesp   lidjespnonw jesp    πi ← randomly selected policy prevv al ←     exchange πi ni                                          termination detected                              si sni  su                                                                      prevv al   ←    busu · bisi · bni sni  ·         evaluateagent si su sni  πi πni  hi  hi         gaini ← getvalueagent πni    − prevv al            exchange gaini ni                                                                      maxgain  ← maxj∈ni∪igainj                                                               run  time secs     winner ← argmaxj∈ni∪igainj                                 maxgain    winner                                                                     initialize πi                                                                              findpolicyagent hi  πni                                    horizon      communicate πi ni    maxgain                                        lidjesp    lidjespnonw jesp        receive πwinner winner update πni                return πi                                                                                                                                    jesp nair et al’s jesp algorithm  uses  centralized processor ﬁnd locally optimal joint pol         icy does consider interaction graph sec   value                                                                  ond benchmark lidjespnonw lidjesp  fully connected interaction graph figure shows        run time seconds logscale yaxis                                                                   creasing ﬁnite horizon xaxis figure                                     shows value policy yaxis                           horizon  creasing ﬁnite horizon xaxis run times  values obtained averaging  runs each figure   agent fconﬁg run time secs  diﬀerent randomly chosen starting policies  seen value  figure values obtained lidjesp jesp  lidjespnonw quite similar lid  jesp lidjespnonw converged higher   hansen et al  ea hansen ds bernstein  local optima jesp comparing run times  zilberstein dynamic programming partially  noted lidjesp outperforms lidjesp     observable stochastic games aaai   nonw jesp run        lesser et al  lesser ortiz tambe   secs                                     distributed sensor nets  multiagent perspective                                                          kluwer academic publishers      acknowledgments                                   montemerlo et al  montemerlo gor  material based work supported       don schneider thrun approximate  darpaipto coordinators program air             lutions partially observable stochastic games  force research laboratoryunder contract fa–     common payoﬀs  aamas   –c–the views conclusions contained nair et al  nair pynadath yokoo  document authors tambe marsella taming decentralized  terpreted representing oﬃcial policies ex pomdps eﬃcient policy computation  pressed implied defense advanced research    multiagent settings ijcai   projects agency government               yokoo hirayama  yokoo     hi                                                          rayama distributed breakout algorithm solving  references                                              distributed constraint satisfaction problems ic                                                          mas   becker et al  becker zilberstein lesser     cv goldman    solving transition indepen    dent decentralized markov decision processes jair    –   goldman zilberstein  cv goldman      zilberstein decentralized control cooperative    systems  categorization complexity  analysis    jair – 
