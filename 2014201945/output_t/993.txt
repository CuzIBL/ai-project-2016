                             marginalized multiinstance kernels                                    james kwok        pakming cheung                             department science engineering                       hong kong university science technology hong kong                                       jameskpakmingcseusthk                          abstract                          trieval chen wang  each image bag                                                        each local image patch instance      support vector machines svm highly      successful machine learning problems       following dietterich et al ’s seminal work number      recently used multiinstance mi new mi learning methods diverse density dd                                                                                                  learning employing kernel deﬁned di algorithm maron lozanoperez  haveemerged      rectly bags bags focus methods based support vec      stances known labels mi kernel implic tor machines svm highly successful      itly assumes instances bag equally machine learning problems main ap      important fundamental property mi  proaches extend standard svm mi data                                                                                             learning instances positive bag modify svm formulation andrews et al                                                                                     necessarily belong positive class cheung kwok   unlike standard      different instances bag dif svm lead nonconvex optimization problems      ferent contributions kernel paper suffer local minima approach design ker                                                                                                     address instance label ambiguity using nels directly bags g¨artner et al        method marginalized kernels ﬁrst   called mi kernels used standard svm      sumes instance labels available instance labels unavailable mi kernel implicitly      deﬁnes labeldependent kernel instances sumes instances bag equally important      integrating unknown instance labels central assumption mi learning      marginalized kernel deﬁned bags   stances positive bag necessarily positive      obtained desirable property ker instances small contributions      nel weights instance pairs consistencies sumption mi kernel crude      probabilistic instance labels experiments recall major difference mi learning      classiﬁcation regression data sets show traditional singleinstance learning lies ambiguity      marginalized mi kernel used  instances labels instance labels available      standard svm performs consistently better   mi problem solved easily      original mi kernel outperforms num related idea explored emdd algorithm zhang      ber traditional mi learning methods           goldman  each bag instance                                                        sistent current hypothesis selected represen                                                        tative bag converts multipleinstance    introduction                                       data singleinstance data leads improved speed  supervised learning each training pattern known accuracy dd algorithm implicitly  class label applications weak la sumes positive instance each pos  bel information formulated supervised itive bag practice dd emdd  learning problems classic example discussed diet ferior kernelbased mi algorithms andrews et al   terich et al  drug activity prediction task cheung kwok   predict drug molecule bind targets paper address ambiguity instance labels  enzymes cellsurface receptors molecule considered design mi kernels using method marginal  useful drug conformations bind ized kernels tsuda et al  similar expectation  targets biochemical data tell binding maximization em algorithm transforms incomplete  capability molecule particular conformation data problem observed data complete data  words class labels associated sets problem observed hidden data  patterns bags instead individual patterns instances easier solve method successfully  dietterich et al called multiinstance mi learning used deﬁne marginalized kernels strings tsuda et al  wellknown mi application contentbased image  trees graphs mah´e et al                                                     ijcai                                                       rest paper organized follows section   marginalizing joint kernel  ﬁrst gives brief introduction marginalized kernel sec obtain marginalized kernel expectation  tion  describes proposed marginalized kernel mi joint kernel  wrt hidden variables cas  classiﬁcation followed extension mi regres                                                                                           sion section  experimental results number clas bb          kz        siﬁcation regression data sets presented section        cc  section gives concluding remarks   computation conditional probability cibi                                                        postponed section  note cibi    marginalized kernels                               known direct computation  computationally                                                                                                like em algorithm data assumed generated infeasible takes Ω    time ci ∈    latent variable model observed variable hid joint kernel deﬁned  kbb   den variable θ task deﬁne marginalized kernel simpliﬁed  observed variables help                   xn xn  hidden variables ﬁrst deﬁne joint kernel   cbp cb    kccicj kxxi xj   kzzz pairs xθ xθ    cc                hidden information unobserved poste   xn xn           rior distribution θ ad θ obtained probabilistic  kxxi xj   kccicj              model θ  turn estimated data          cicj  marginalized kernel obtained taking expectation                                                                                  ·           joint kernelx wrt hidden variables                                                                                                   cci            ccj   kxx       θxp θxkzzzdθ dθ                                                                                                                 θ θ ∈Θ                                                                                                                                where  cij    cicij−cijcini using        Θ                                                         ij ci  ij   ij      domain hidden variables cicij           conditional  hidden variable continuous summation replaced independence assumption cij bip cij xij   integration note computing  intractable kbb reduced       Θ    large important issue designing xn xn  marginalized kernels                                            kxxi xj kccicj cixip cj xj                                                          cicj    marginalized kernels mi classiﬁcation           shown section  computed  mi classiﬁcation given set training bags polynomial time particular using  instance                                          bmym   wherebi       ini   label kernel kbb simply  ith bag containing instances xij ’s yi ∈ n n  each bag bi observed information associated     ci xip cj xjkxxi xj  xij ’s hidden information unknown instance                           ∈    labels  cicini wherecij                           n n                                                                                         joint kernel                                               ci     cj     kx                                                                  joint kernel deﬁned combined variables   utilize information intuitive finally avoid undesirable scaling  input xij ’s instance labels ci note problems normalize kernel g¨artner et al                                                                          kbibj                                                             ← √       √  traditional kernels polynomial gaus  kb  kb    sian kernels deﬁned input recent results                                                                   note  reduces mi kernel kbibj  show use label information lead better ker                                                                   kxxi xj g¨artner et al  kc· ·  nels cristianini et al                       paper deﬁne joint kernel        constant g¨artner et al  assumes                   n n                              instance pairs bi bj equally impor        kzz       kccicjkxxi xj   tant kc  weights differently according                                                sistency probabilistic labels compared  kx· · kc· · kernels deﬁned input emdd chooses representative instance  label parts instances respectively simple rea each bag during inference perform marginalization  sonable deﬁnition kc                           possible instance labels consistent               kccicjici  cj             bayesian framework  i· returns  predicate true   deﬁning conditional probabilities  wise case  equal alignment section consider obtain conditional  kernel kx instance labels cristianini et al                                                        probability cibi mentioned tsuda et al   high alignment implies high kernel value similarity advantage marginalized kernel approach                                     deﬁnitions joint kernel probabilistic model    deﬁnitions possible completely separated lot freedom pick  kccicj ci  cj andotherwise          ing good probabilistic model                                                    ijcai                                                     using probabilistic model diverse density     algorithm  marginalized mi kernel classiﬁcation  motivated success dd algorithm mi learning input training set pair bags                              maron lozanoperez   ﬁrst consider using output kbb  probabilistic model estimating cij xij  dd algo                                                       run dd algorithm different initializations  rithm ﬁnds hypothesis instance space                                       maximizing following dd function                  store hypotheses obtained array                                                         ∈hdo                  “            ”       “            ”                                                  compute hd using   store value                      −h−xij           −h−xij   ddh     −    −e                   −                array             xij              b− xij                                                      end               −                                         xij ∈ ∪  bi bi  index positive negative bags                                                            compute cij xij  using    training set respectively intuitively high dd                                                          end  value positive bags instances close neg                                                          compute kxxi xj instance pairs ×  ative instances negative bags far away                                                          compute kbb   using precomputed  model deﬁne                                                            values array kx cij xij                   hdddhz                                                       normalizing factor hd                                                      using training accuracy improve hd                                                                      −xij −h               cij xij                     mentioned earlier dd algorithm suc                                                        cessful note use hypotheses  cij xij − cij xij                                                         dd values classiﬁcation recall obtained    dd function highly nonlinear                                                        hypotheses local minima dd function  local minima instead using obtained                                                        comparable classiﬁcation accuracies  optimization process advantageous use                                                        demonstrated section  hypotheses  multiple hypotheses chen wang wethenhave                                                       dd values higher         cij xij     hxij cij xij    surprising dd value drop signiﬁcantly hy                       h                               pothesis close negative instance consequently                          hdp cij xij   summation  dominated hypotheses                                                        alleviate problem retaining merits                                                        dd instead deﬁne hd each dd hypothesis  summation set hypotheses obtained                                                        proportional training accuracy let pred bi  note  automatically weights each hypothesis                                                                                                 label predicted bi equal   likelihood contrary ddsvm chen                                                                            max       −xij −h ≥    wang  weighting rely addi xij∈bi                                                                                         tional heuristics ﬁlter away important hypotheses               substituting equations  ﬁnally     hd    ipredhbiyiz                    xn xn                                                     kb                                                        number training bags                                                            normalization superiority deﬁnition              hdp hdp cixihp cj xj  experimentally veriﬁed section   complete algorithm shown algorithm          modular design marginalized kernel    recently rahmani goldman  proposed easily plug good estimators cij xij   graphbased mi semisupervised learning method example embased approach used  edge weight bags roughly equal                                                          time complexity          xn xn                ddxiddxj                                                 let nh number dd hypotheses algorithm                                                                                         computing cij xij ’s step  takes                                                                                     quite similar  general  nhd time assuming ’s obtained  positive semideﬁnite valid kernel function constant time precomputed data                                                        dimensionality assuming each evaluation kernel    searching large prohibitive computa kx takes od time computing kx· ·’s step   tionally efﬁcient performs gradientbased optimization takes onnd time summation involved kbb                                                                            dd function initializing instance pos step  takes onh nn time cij takes    itive bag chen wang  maron lozanoperez  total time complexity computing kbb                                                                 note probabilities implicitly conditioned onh  dnn  nnhd polynomial  training data clarity  write hd                                        posterior probability given data instead    marginalized kernels mi regression    rahmani goldman  dd values   ﬁrst normalized nonlinearly transformed ﬁltered regression assume each bag output yi normal  weight deﬁned positive bags    ized range   subsequently each hidden instance                                                    ijcai                                                      label cij   realvalued setting follow multiple hypotheses used kbb sim   extended dd model amar et al  ilarly generalized                                                                                                                                                                                                −h −x   −h −x                              −h−xij                       dp dkx  ge             cij xij   −cij −      zxij                                                                                                                                  zxizxj                                                         ijhh                                   ij ensures  cij ij dcij                                                          −h−x                probability  follow ex   easily shown zxij  −  ij −        tended dd model obtain     proceeding section  employ joint ker                                                                                                                                                                                                                                  −h−xij    nel  summations ci cj replaced hd  −yi − max                                                                                        xij ∈bi   tegrations   marginalized kernel bb           bi    avoid integrating    dimensional space                                    alternatively  use train                                                         ing accuracy each hypothesis following dooly et     probability instances share real                                                                                                                max       −h−xij    valued label zero  poor measure al weuse   xij ∈bi        h’s pre                                                         dicted output bag bi error eh    stance label similarity intuitively larger difference                                                                             m                              ci cj smaller kccicj noting                −h−xij                                                                 yi − maxxij ∈bi         deﬁne     −    ∈                                            ci  cj       natural choices kc fig                                               ure                                                                      eh − minh eh                                                             hd    −                                                                                                     max     − min             linear kccicj−ci − cj                            eh      eh                                                                                               gaussian kccicjexp  −βci − cj         experiments                                                         section compare performance proposed                                    β                     marginalized kernels mi kernel g¨artner et al                              exp                                                        number mi data sets kernel deﬁned                                                                                                                     input parts use gaussian kernel kxxi xj                                                                                                                                    exp −γ xi − xj  whereγ width parameter                                                                                                                                      mi classiﬁcation                                                                         simplicity marginalized kernels deﬁned                                                        denoted mgddv mgacc respectively                                                                drug activity prediction                                                                               ﬁrst experiment performed popular musk   figure  possible deﬁnitions label kernel musk data sets task predict drug musky             kc cicj regression setting                 mentioned section  each drug considered bag                                                         each lowenergy conformations instance     putting long straightfor use fold crossvalidation each fold dd algorithm   ward calculations shown marginalized mi applied training data dd hypotheses   kernel bags given            capture information test set compari                                                 son ddsvm chen wang  run                         −h−xi  −h−xj                                          seen table  mgacc kernel   bb                                                                            performs better mi kernel outperforms                                                         ddsvm using multiple dd hypotheses   gu equal                             assumed equally important demonstrates                                                   importance weighting instance pairs based         −ci − cj −ci − −cj − vdci dcj   sistency underlying labels section         used equal                        table  testing accuracies  musk data sets                                                                                musk   musk                   ´                           exp  −βci −cj   −ci − u−cj − vdci dcj    ddsvm                                                                                 mi kernel              used closed form expressions        mgddv kernel            integrals appendix clas   mgacc kernel            siﬁcation deﬁnitions kbb computed   polynomial time                                        mgacc kernel better mgddv                                                         kernel figure compares corresponding hd           experiments set β kccicj                                                                ci − cj                                      ftpftpicsuciedupubmachinelearningdatabasesmusk                                                     ijcai                                                      values  mgddv  mgacc  dd hypotheses obtained typical fold musk  table  testing accuracies  image data set  data seen mgddv kernel small subset                  repetitions  repetitions                                                                 ddsvm        ±        hd values high summation                                                                  histsvm      ±        dominated hypotheses hand                    ±                                                            misvm                 mgacc kernel  values vary gradually         mi kernel     ±   ±   figure hypotheses utilized mgddv kernel   ±                                           computing cij ij  recall     mgacc kernel    ±   ±   proportional h’s dd value  propor  tional h’s training accuracy linear trend  figure shows higher dd value higher website three lj lj lj  training accuracy hypothesis provides  used recent study cheung kwok   evidence success dd algorithm obtained adding irrelevant features                                                        data sets keeping realvalued outputs intact                                                                                     mgddv                             demonstrated section  mgacc kernel su                   mgacc                            perior mgddv kernel exper                                                                                                    iment mgacc kernel deﬁned                                                      dooly et al  report percentage error        phd                              phd                                                                                err  mean squared error mse                                                  training  accuracy                                                          table  shows results dd em                                                        dd citationknn wang zucker  reported                                                   dd hypothesis           dd hypothesis      cheung kwok  seen proposed      hd          values closer look   mgacc kernel gaussian instance label kernel                            hd                                                                      mgddv mgacc ker          values mg    consistently outperforms superiority   nels                    acc kernel                 mgacc kernel linearly decaying instance label  figure  hd values obtained typical fold kernel indicates smaller weight  musk data xaxis shows different hypotheses ob signed instance pairs labels quite different fig  tained dd algorithm sorted increasing hd val ure  explains relatively inferior performance  ues obtained mgacc kernel                mi kernel weights instance pairs equally    image categorization                                  conclusion  second experiment image categorization task paper show design marginalized kernels  ing data set chen wang thereare multipleinstance learning pretend hid  classes beach ﬂowers horses each class contain den instance labels known deﬁne joint kernel using  ing  images each image regarded bag each instance inputs labels integrating  segment instance follow exactly setup hidden data marginalized kernel obtained kernel  chen wang  data randomly divided differs existing mi kernel different instance  training test set each containing  images each cat pairs weighted consistency probabilistic  egory multiclass classiﬁcation problem labels experimentally marginalized mi kernel  employ standard onevsrest approach convert performs better mi kernel outperforms  number binary classiﬁcation problems experiment traditional mi learning methods  repeated  times average testing accuracy reported note proposed kernel straightforwardly    table  shows results dd used regularization framework recently proposed  svm histsvm  chapelle et al  misvm cheung kwok  mod  ported chen wang  seen mg ularity marginalized kernel explore  acc kernel superior mi kernel better deﬁnitions joint kernel probabilistic model  ensure improvement mi kernel statisti  cally signiﬁcant repeat experiment  times instead acknowledgments   difference conﬁrmed signiﬁcant   level signiﬁcance using paired ttest research partially supported research                                                        grants council hong kong special administrative    mi regression synthetic musk molecules          gion                                                             perform mi regression data set used dooly et httpwwwcswustledu∼sgmultiinstdata data  al  goal predict binding energies sets available easier variations three  musk molecules use three data sets lj used dropped study  lj lj downloaded author’s httpwwwcsusthk∼jameskpapersicml datazip                                                          err classiﬁcation error obtained thresholding         httpwwwcsunoedu∼yixinddsvmhtml            target output predicted output                                                     ijcai                                                     
