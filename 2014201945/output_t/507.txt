                approximate policy iteration using largemargin classifiers                                        michail lagoudakis ronald parr                                                      duke university                                                    durham nc                                              mglparrcsdukeedu                               abstract                              finite set actions markovian transition model                                                                  probability making transition        present approximate policy iteration algo•                                                                state sf taking action state reward cost        rithm uses rollouts estimate value each   function rs expected reward taking        action given policy subset states    action state  discount factor future        classifier generalize learn improved        rewards initial state distribution        policy entire state space using multi       states drawn process initialized        class support vector machine classifier          reinforcement learning assumed learner        obtained successful results inverted pendu•       observe state process immediate reward        lum bicycle balancing riding domains        step completely unknown                                                                 paper make assumption learning al•   introduction                                                gorithm access generative model process                                                                 black box takes state action inputs   reinforcement learning provides intuitively appealing      outputs state drawn reward note   framework addressing wide variety planning con•  having model   trol problems significant success tackling    policy mdp mapping    largescale problems through use value function andor states actions action agent takes   policy approximation success methods how•      state value state policy   contingent extensive careful feature engi•  expected total discounted reward process begins   neering common problem machine learning meth•      state decisions steps according   ods                                                          policy     modern classification methods mitigated feature   engineering problems through use kernel methods   little exploit recent advances   purposes reinforcement learning course                                                                 goal decision maker compute learn op•  note potential benefits modern classifica•                                                                timal policy maximizes expected total discounted   tion methods reinforcement learning example yoon et                                                                 reward initial state distribution   al  use inductive learning techniques including boost•  ing generalize similar problems dietterich   wang  use kernelbased approximation method                                                                 known mdp exists opti•  generalize similar problems novelty ap•                                                                mal deterministic policy   proach application modern learning methods   single noisy problem inner loop policy iteration  approximate policy iteration   algorithm using rollouts avoid prob•  lematic step value function approximation tech• policy iteration pi method discovering policy   nique aims address critiques value function methods iterating through sequence monotonically improving   raised proponents direct policy search avoid• policies improvement each iteration typically achieved   ing confines parameterized policy space             computing learning stateaction value function                                                                 current policy rt defined    definitions assumptions   markov decision process mdp defined tuple    state space process improved policy        fcrn yoon givan pursuing similar approach                                                                                                         poster papers practice policy iteration terminates surprisingly small   number steps relies exact representation   value functions policies      approximate methods frequently used state   space underlying mdp extremely large exact   solution learning methods fail general framework   using approximation policy iteration known ap•  proximate policy iteration api general form   api assumes approximate representations value functions   policies result api guarantee mono  tonic improvement convergence optimal policy   performance bound outcome api constructed   terms bounds approximation errors bert  sekas tsitsiklis  practice api finds         figure  api rollouts classification   good policies iterations normally makes big   steps space possible policies contrast   policy gradient methods despite acceleration methods  ing policy iteration algorithm terminating   forced small steps                    optimal policy largescale problems choosing                                                                   sf dealing imperfect classifiers poses challenges    practical api value functions                          consider number alternative choices dis•  dependence typical api algorithms approximate       tribution uniform distribution state space   value functions places continuous function approximation      good choice lowdimensional state space   methods inner loop methods typically mini•    result poor coverage highdimensional spaces bet•                                                                ter choice distribution favors certain impor•  mize error poor match bounds                                                                 tant parts state space classification   api problem just theoretical efforts im•                                                                widely assumed classifier trained examples   prove performance adding new features some•                                                                drawn distribution tested   times lead surprisingly worse performance making feature                                                                 representative states learning policy it•  engineering somewhat tedious counterintuitive task                                                                 eration drawn future state      important observation noted fern yoon                                                                 distribution jaakkola et al  unknown policy   givan  montecarlo technique called roll                                                                   policy unknown possible   outs used api avoid problematic                                                                           closely approximate distribution  starting state   value function approximation step entirely rollouts estimate                                                                                                     drawn trajectory simulated     sa generative model executing action          nt                                                          using rollouts repeatedly each visited state determine   state following policy recording                                                                 actions trajectory terminates proba•  total discounted reward obtained during entire trajectory                                                                 bility states visited trajectories   simulation repeated times results                                                                 viewed samples desired distribution   averaged large number trajectories obtain accu•  rate estimate rollouts used                           main contribution paper particular em•  tesauro galperin  online improvement    bodiment algorithm described previous section   backgammon player purposes choose rep•     rollouts used basic statistical hypothesis testing                                                                sample ftest determine statistically significant differences   resentative set states sp distribution state   space perform rollouts determine ac•     rollout estimates different actions   tion maximizes current policy            action determined clear winner state   fitting function approximator values obtained treat positive training example state                                                                 negative examples   rollouts instead train classifier spy each state   labelled maximizing action state al• clear winner action state negative examples   gorithm summarized figure                             extracted clearly bad actions allow•                                                                ing classifier choose freely remaining good      learn supervised learning algorithm trains clas• actions winner case examples   sifier given set labeled training data termination  generated actions equally good   condition left somewhat open ended                                                                    significant contribution effort use sup•  performance current policy does exceed                                                                 port vector machines svms learn implemen•  previous subsequent policies similar                                                                 tation used svmtorch package collobert ben  notion similarity depend learner used                                                                 gio  multiclass classifier kernel trick   cycle policies detected learner dependent                                                                 svms able implicitly automatically consider clas•  assume fortuitous choice sp sufficiently pow•                                                                sifiers complex feature spaces   erful learner correctly generalize sp en•  tire state space algorithm each iteration learns im•  proved policy previous effectively implement       special thanks alan fern sharing observation       poster papers                                                                                                         figure  positive negativcx examples support vcctorso                 figure  successful trajectories bicycle      experimental results                                                    right side input svm simply                                                                            sixdimensional state description value    inverted pendulum problem goal balance pen•           performance sensitive svm parameters   dulum applying forces left lf right rf         currently doing experimentation larger numbers   force nf actions noisy state space          rollout states different kernel parameters   continuous consists vertical angle angu•                                                                           research supported nsf grant    lar velocity pendulum transitions governed   nonlinear dynamics time step                references   seconds reward  angle exceeds   absolute value end episode  dis•            bcrtsekas tsitsiklis  bertsekas tsitsiklis   count factor  using  states perform              neurodynamic programming athena scientific belmont mas•  rollouts algorithm consistently learns balancing policies              sachusetts    iterations starting random policy           collobert bengio  ronan collobert samy bcngio   choice kernel polynomialgaussian did affect              svmtorch support vector machines largescale regres•  results significantly figure  shows training data             sion problems journal machine learning research jmlr                                                                                    lf action uniform distribution num•  ber support vectors normally smaller number              dietterich wang  dietterich wang batch   rollout states constant tradeoff train•             value funtion approximation support vectors advances   ing error margin set                                          neural information processing systems   mit press      bicycle balancing riding problem randlov              fern et al  fern yoon givan approxi•  alstrom  goal learn balance ride bi•               mately policy iteration policy language bias learning   cycle target position located  km away starting              control policies relational planning domains submitted                                                                               nineteenth international conference machine learning   location state description sixdimensional vector                                                                               icml july                        angle handlebar   vertical angle bicycle angle               jaakkola et al  tommi jaakkola satinder singh                                                                               michael jordan reinforcement learning algorithm partially   bicycle goal actions torque applied                                                                               observable markov decision problems tesauro touret  handlebar   displacement rider                                                                               zky leen editors advances neural information pro•   actions restricted                   cessing systems  cambridge massachusetts  mit press       giving total  actions noise                                                                            ng et al  andrew ng daishi harada stuart russell   uniformly distributed term    added                                                                               policy invariance reward transformations theory ap•  displacement dynamics bicycle based                                                                               plication reward shaping proc th international conf   model randlov alstrom  time step set                machine learning pages  morgan kaufmann san    seconds typical problem used                 francisco ca    shaping reward ng et al  reward       given each                                                                          randlv alstram  randlav alstrom learn•  time step rt — dt distance                                 ing drive bicycle using reinforcement learning shaping   wheel bicycle goal position time             fifteenth international conference machine learning    discount factor set                            morgan kaufmann      using polynomial kernel degree  able                tesauro galperin  tesauro galperin on•  solve problem uniform sampling  roll•                 line policy improvement using montecarlo search advances   states sampling distribution                               neural information processing systems nips     target policy problem solved              yoon et al  yoon fern givan inductive   rollout states shown figure  shows sam•                   policy selection firstorder mdps proceedings eigh  ple trajectories bicycle twodimensional plane                teenth conference uncertainty artificial intelligence    initial position  left side goal position            morgan kaufmann                                                                                                                          poster papers 
