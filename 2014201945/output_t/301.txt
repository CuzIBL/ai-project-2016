              decision tree estimate class probabilities                                    using decision boundary          isabelle alvarez             stephan bernard              guillaume deffuant           lip paris vi university          cemagref lisc                 cemagref lisc              place jussieu              aubiere france             aubiere france            paris france         stephanbernardcemagreffr      guillaumedeffuantcemagreffr         isabellealvarezlipfr                        abstract                          smoothing induce drastic change fundamen                                                        tal properties tree structure tree      paper proposes new method estimate  model modiﬁed main objective intelligibility      class membership probability cases classiﬁed method propose aims improving class      decision tree method provides smooth   probability estimate modifying tree      class probabilities estimate modiﬁca der preserve intelligibility use      tion tree data numerical ap attributes cases consider new feature dis      plies posteriori doesn’t use additional train tance decision boundary induced dt      ing cases relies distance deci boundary inverse image different class labels      sion boundary induced decision tree   propose use new feature seen      distance computed training sample   margin dt estimate posterior probabilities      used input simple   expect class membership probability closely      dimension kernelbased density estimator   related distance decision boundary      provides estimate class membership prob case geometric methods like support vector ma      ability geometric method gives good results chines svm svm  deﬁnes unique hyperplane      pruned trees intelligibility feature space classify data original input space      tree fully preserved                          corresponding decision boundary complex                                                        distance hyperplane used estimate    introduction                                       posterior probabilities platt  details  decision tree dt algorithms popular widely twoclass problem case dt decision boundary  used classiﬁcation purpose provide relatively consists pieces hyperplanes instead unique  easily intelligible model data contrary hyperplane propose compute distance deci  learning methods intelligibility desirable property sion boundary training cases adapting idea                                                                         artiﬁcial intelligence considering interactions smyth et al   train kernelbased density es  enduser enduser expert timator kde attributes cases  hand enduser classiﬁcation needs addi single new feature  tional information just output class order paper organized follows section  discusses  asses result information consists generally lated work probability estimate dt section  presents  fusion matrix accuracy speciﬁc error rates like speciﬁcity distancebased estimate posterior proba  sensitivity likelihood ratios including costs com bilities section  reports experiment performed  monly used diagnosis applications context numerical databases uci repository comparison  cision aid valuable information class distancebased method smoothing methods  membership probability unfortunately dt provide section  discusses use geometrically deﬁned subsets  piecewise constant estimates class posterior probabil training set order enhance probability estimate  ities cases classiﬁed leaf share make comments use distance  posterior probabilities consequence concluding section  main objective separate different classes  raw estimate leaf highly biased contrary  estimating class probabilities dt  methods highly suitable probability estimate pro decision trees dt posterior probabilities piecewise  duce generally intelligible models lot work aims constant leaves inaccurate  improving class probability estimate leaf smooth limited use ranking examples evaluate  ing methods specialized trees combined methods decision risk decision reason lot work  tree combined algorithms fuzzy methods ensem improve accuracy posterior probabili  ble methods section  actually methods ties build better trees concern                                                    ijcai                                                       traditional methods consist smoothing raw condi convention twoclass problem choose                                                                             Γ  tional probability estimate leaf  class positive class orient  case stands  number training cases class label classiﬁed positive class area algebraic distance negative  leaf total number training cases classiﬁed deﬁnition  algebraic distance dt class problem  leaf main smoothing method laplace cor          rection variants like mcorrection correction algebraic distance hx−dx Γ cx          nc  number classes shifts positive class hxdx Γ  probability prior probability class cest                                                          deﬁnition extends easily multiclass problems  nik  zadrozny elkan  improves                                                        each class consider Γ  decision boundary induced  accuracy posterior probabilities tree struc                                                                    tree class Γ inverse image class  ture unchanged probabilities constant                                                                 area Γ ⊂ Γ  leaves order bypass problem smoothing methods           used unpruned trees great number leaves deﬁnition  classalgebraic distance dt                                                                                          −       Γ   allows tree learn posterior probability calgebraic distance hc                                                                         Γ   accuracy provost domingos  intelligi hc  bility model reduced case specialized calgebraic distance measures distance case  building pruning methods developped pet’s class area actually algebraic distance particular case  instance ferri hernandez             calgebraic distance positive class    order produce smooth casevariable estimate deﬁnitions apply decision tree  posterior probabilities kohavi  deploys naive case axisparallel dt adt operating numerical data  bayes classiﬁer nbc each leaf using specialized simple algorithm computes algebraic distance  duction algorithm partition space nbtree adapted distance algorithm alvarez   different classical partition objective consists projecting case set leaves  better verify conditional independance assumption each class label differs cx class problem  leaf idea zang su  use nbc eval multiclass problem each class considered turn  uate choice attribute each step structure cxc set contains leaves labels  tree essentialy different                  differs contains leaves class    methods obtain casevariable estimates poste label nearest projection gives distance  rior probabilities propagating case through paths algorithm  algebraicdistancexdtc                                          each node mainly fuzzy methods like umano et al   ∞                                            quinlan   recently ling  gather set leaves class cf veriﬁes            yan   methods aim managing uncertainty  cf xor cx   input training database        each ∈     smyth gray fayyad  propose struc                                                           compute pf xprojectionontoleafxf  ture tree use kernelbased estimate each leaf  compute xdx  consider training examples use                                                                            df  df   tributes involved path leaf dimension  return x−signcxc ∗  subspace length path                                                                                                 resulting dimension far high kind algorithm  projectionontoleafxf ti i∈i   technique method used practice                                                                                    propose reduce dimension  kde foritosizei                                                                                                  effective low dimensions preserve  doesn’t verify test ti yu   structure tree method theoretically applies soon ti involves attribute threshold value  possible deﬁne metric input space  return                                                        projection leaf straightforward case    distancebased probability estimate dt         adt area classiﬁed leaf hyperrectangle                                                        deﬁned tests complexity onn worst    algebraic distance new feature              case number tests tree  consider axisparallel dt adt operating numer number different attributes tree  ical data each test tree involves unique attribute distance decision boundary presents main  note Γ decision boundary induced tree Γ advantages continuous function relatively  sists pieces hyperplanes normal robust uncertainty value attributes new  axes assume possible deﬁne metric cases noise training data assuming  input space possibly cost utility function thresholds tests modiﬁed attributes    let case cx class label assigned  tree  dx Γ distance decision bound  kernel density estimate kde  ary Γ decision boundary Γ divides input space algebraic distance  different areas possibly connected areas la kernelbased density estimation nonparametric tech  beled class assigned tree nique widely used density estimation constantly im                                                    ijcai                                                     proved researches algorithms variable bandwidth  bandwidth selection venables ripley   references    univariate kde basically sums contribution each  training case density kernel function kto  estimate density fx given sample                                             i∈n              ˆ           x−xi  computes          kernel function bandwidth vary    methods used framework compute  distancebased kernel probability estimate class  membership use kernel regression estimate  used basic kde simplicity    consider algebraic distance hx  tribute kde performed compute  density estimate distribution algebraic distance  set observations hxx∈                                                                         −              ˆ                     xi          fhx                                                  nb                                figure  variation laplace gdk probability estimate                       xi∈s                             test sample wdbc database dt errors highlighted    order estimate conditional probabilities  sider set training cases subset sc cases  experimental study  cxc estimate density populations    ˆ    density estimate distribution algebraic  design remarks  distance decision boundary                    studied distancebased kernel probability esti    ˆ                                                   mate databases uci repository blake merz    fc density estimate distribution algebraic  distance points class                         numerical attributes missing val    ˆ                    ˆ                              ues simplicity treated multiclass problems    computed  derive                                                    class problems generally chose positive class  bayes rule pˆc estimates prior probability class                                                        class group class lowest frequency                          ˆ    ∗ ˆ                database               ˆ     fc                        ˆ                        each database divided  bootstrap samples                             hx                                                        separate training test sets proportion    deﬁnition  distancebased kernel probability estimate specting prior classes estimated frequency    pˆchx called distancebased kernel dk proba total database certainly best way build  bility estimate                                       accurate trees unbalanced datasets different error costs    algorithm straightforward note training interested building accurate  set used build decision tree dt               trees just want study distancebased kernel proba                                                        bility estimate reason grow trees  algorithm  distancebasedprobestxdtcs                                                        default options weka’s witten frank  im   compute algebraic distance                                                        plementation cases different op                                            algebraicdistance dt               tions build better trees unpruned trees disabled   compute subset   probability                                                  collapsing function used laplace correction smooth  density estimated default value                                                     ing method correct raw probability estimate leaf   select xx ∈ sxcxc                                                    pruned trees unpruned trees built nbtree            ˆ                 y−hxi    compute       xi∈sx                        samples                   nb                           ˆ                   y−hxi                  used different metrics order compute dis   compute                                      nb  xi∈sc                         tance decision boundary minmax mm metric   compute return pˆchx equation     standard metric metrics deﬁned    possibilities considered set sx basic information available data estimate  used compute kernel density estimate discuss range each attribute estimate mean ei  section  simplest method consists using standard deviation si new coordinate  sample algebraic distance taken account glob deﬁned   ally consideration concerning location          −                      −                                                              mm      xi  mini            xi  ei  cases corresponding conditional probabil yi                    yi                                                                                  maxi  − mini               si  ity estimate pˆg global distancebased kernel gdk prob  ability estimate figure  shows gdk probability es parameters standard metric estimated each  timate varies test sample compared laplace training sample each bootstrap sample minmax  probability estimate piecewise constant     metric avoid multiple computation attribute                                                    ijcai                                                     outside range practice choice metric          normal vs   dk vs  guided data source instance accuracy dataset rederror normal unpruned nbtree  sensors function range suggests                                                           bupa   ±  ± ± ±  use adapted version minmax coordinate glass ± ± ± ±    compute kernel density estimate algebraic dis iono ± ± ± ±  tance choose simplicity used standard algorithm iris ±  ± ± ±  available venables ripley  default op thyroid ± ± ± ±  tions setting kde parameters choice pendig ± ± ± ±  kernel optimal bandwidth specialized algorithms pima ±  ± ± ±  available dedicated packages certainly sat ±  ± ± ±                                                                     ±          ±        ±         ±  better results systematically used default option segment                                                                   sonar  ±  ± ± ±  bandwidth selection inappropriate      ±          ±        ±         ±  using bayes rule equation  used vehicle                      ˆ      ˆ                                     vowel  ±  ± ± ±  bandwidth fc set fraction τ range wdbc ± ± ± ±  algebraic distance  parameter used wine ± ± ± ±  control kde algorithm sophisticated methods  obviously used bayes rule equation  table  mean difference auc obtained global dk  reformulated variable bandwidths used probability estimate standard metric τ  laplace                                                        correction gdk pruned tree versus laplace pruned    results global distancebased kernel     pruned tree mean values standard deviations ×       probability estimate                             signiﬁcant values italic bad results bold  order compare method methods prob  ability estimate used auc area auc remains better easily comprehensible  ceiving operator characteristic roc curve bradley τ increases kernel estimate tends erase sharp varia   auc widely use assess ranking ability tions consequence probability estimate reach  tree method distinguish extremes   easily log loss metric shown  good scores scores good probability estimates gives useless results inﬁnite  doesn’t make hypothesis true posterior probability  useful true posterior probability unknown  local estimate partition space  clear good probability estimates pro  duce average better auc used mean squared order local estimate ﬁrst idea                                                        use leaves reﬁne deﬁnition sets sx used  error logloss methods make strong hy                                               pothesis true posterior probability           run kernel density estimate step  algorithm     table  shows difference auc global simply leaf classiﬁes we’ll ar  distancebased kernel gdk probability estimate gue simple example option shows severe draw  laplace correction apart cases gives better backs deﬁnition based geometry dt  values laplace correction  conﬁdence boundary generally preferred  terval intelligibility viewpoint interresting  note gdk probability estimate pruned tree gener           gd vs laplace  gd vs nbtree  ally better smoothing method unpruned tree dataset  mse    pvalue   mse   pvalue  better smoothing method pruned tree             ±                ±  formed wilcoxon unlateral pairedwise test each batch bupa                                                                   glass   ±  ±     samples tests conﬁrm exactly signiﬁcant results           ±                ±                                                           iono         table  pvalues                      iris   ± ±     choice metric limited effect result thyroid ±  ±  term auc difference different order pendig ± ±  vehicle pima especially glass pima ± ±   reach −                                                 sat    ±  ±    used bandwidths τ     segment ±  ±   range algebraic distance results vary smoothly sonar  ± ±                                                               vehicle ± ±   bandwidth table  present completely similar           ±                ±  results                                                    vowel                                                                     wdbc   ± ±    table  shows comparison using mean squarederror     wine   ± ±  mse metric measure quality the probability                                             −  estimate error test point se                                                       table  mean mse difference pvalue corresponding  pˆxc  pcx true conditional probability unilateral wilcoxon paired test gdk probability estimate  pcx set  cxc sex summed standard metric τ  laplace correction normal  each test sample performance gdk proba pruned tree nbtree values × pvalues  bility estimate diminishes quickly τ increases signiﬁcant values italic bad results gdk bold                                                    ijcai                                                       dt boundarybased partition  rationale partition consider groups  points distance deﬁned normal  decision surface points share  axis deﬁning distance relate linear  piece decision surface    let hh  hk separators used nodes  tree learning sample partition total sam  ple associate each separator hi subset si  comprises examples projection  decision boundary Γ belongs hi generically  projection px Γ unique px belong  separators case associate separator  hi deﬁning px largest margin deﬁne sx  si kde run hxjxj ∈ si explained  previously    advantage partition  illustrate simple example advantage  partition compared partition generated leaves figure  partition deﬁned three leaves  suppose learning examples uniformly distributed tree partition  subsets light gray gray  square space probability class  deﬁned separators  depends axis following pattern figure     • growing −  probability class       grows smoothly   sigmoid function      centered −                                  table  shows results comparison local    • growing   probability class  distancebased kernel estimate smoothing method      decreases sharply        leaf databases partition space                                                        results auc comparison mse nec    typical decision tree obtained sample generated essarily correlated mse globally better  distribution represented ﬁgure  global dk probability estimate  three leaves yield class  deﬁned single  separation yields  deﬁned separa  tions gray area ﬁgure partition deﬁned  leaves generates  subsets    figure  shows partition obtained  method includes subset each separation  figure  shows results obtained kernel estimation  simple parzen window width  leaf partition  partition compared actual probability  function used generate sample  notice partition gives result close  obtained kernel estimation  axis case leaf partition  introduces strong deviations original probability    main explanation deviations side effect  artiﬁcially increases probability   border case leaf partition actually  bias obtained estimating directly  probabilities leaves leaf partition intro  duces artiﬁcial symmetry sides leaf  averaging examples similar distance  boundary problems met partition  claim problems illustrated example good  chances occur frequently leaf partition  particular side effect border leaf al figure  results kernel estimator width  parzen  ways induce distortion avoided partition window leaf partition  artiﬁcial symmetries introduced leaf separator partition black true probability gray  includes separators                          estimate                                                    ijcai                                                     
