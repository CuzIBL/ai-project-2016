                                    recursive random fields                                      daniel lowd    pedro domingos                             department science engineering                         university washington seattle wa  usa                                    lowdpedrodcswashingtonedu                          abstract                            example mln formula rx ∧ sx                                                        treat worlds violate rx sx proba      formula ﬁrstorder logic viewed  ble worlds violate mln acts      tree logical connective each node soft conjunction groundings rx sx simply      knowledge base viewed tree    appear distinct formulas mlns convert knowledge                                            root conjunction markov logic richardson    base cnf performing learning inference                             domingos  makes conjunction prob   possible disjunction rx ∨ sx distinction      abilistic universal quantiﬁers directly satisfying rx sx satisfying      rest tree remains purely log just universally quantiﬁed formula effectively      ical causes asymmetry treatment   conjunction groundings existentially      conjunctions disjunctions universal quantiﬁed formula disjunction leads      existential quantiﬁers propose   quantiﬁers handled differently      come allowing features markov logic                                                          asymmetry avoided “softening” disjunction      networks mlns nested mlns                                                        existential quantiﬁcation way markov      representation recursive random ﬁelds rrfs                                                        logic softens conjunction universal quantiﬁcation      rrfs represent ﬁrstorder distributions                                                        result representation mlns nested mlns      exponentially compactly mlns                                                        features recursive markov logic networks      form inference rrfs using mcmc icm                                                        recursive random ﬁelds rrfs short      weight learning using form backpropa      gation weight learning rrfs power      rrfs desirable properties including abil                                                        ity represent distributions like noisy dnf rules ex      ful structure learning mlns applied                    ﬁrstorder knowledge bases provides ﬂex ceptions ofall quantiﬁers compactly      ible form theory revision evaluate rrfs mlns rrfs allow ﬂexibilty revising ﬁrst      problem probabilistic integrity constraints order theories maximize data likelihood standard methods      databases obtain promising results          inference markov random ﬁelds easily extended                                                        rrfs weight learning carried efﬁciently using                                                        variant backpropagation algorithm    introduction                                         rrf theory revision viewed ﬁrstorder prob  recent years seen development increasingly pow abilistic analog kbann algorithm initializes  erful combinations relational probabilistic representa neural network propositional theory uses  tions inference learning algorithms propagation improve ﬁt data towell shavlik  general representations date markov  propositional rrf predicates zero  logic attaches weights ﬁrstorder formulas arity differs multilayer perceptron output  views templates features markov random ﬁelds joint probability inputs regression  richardson domingos  markov logic variable probabilistic version condi  language choice applications uniﬁca tional probability propositional rrfs alternative  tion logic probability incomplete boltzmann machines nested features playing role  treats toplevel conjunction universal quanti hidden variables nested features determinis  ﬁers knowledge base probabilistic principle tic functions inputs learning does require em  logical combination viewed limiting case inference does require marginalizing variables  underlying probability distribution markov logic remainder paper organized follows  disjunctions existential quantiﬁers remain deterministic begin discussing mlns limitations  symmetry conjunctions disjunctions troduce rrfs inference learning algorithms  universal existential quantiﬁers lost ex compare mlns present preliminary  cept inﬁniteweight limit                    experimental results                                                    ijcai                                                       markov logic networks                              different standard markov random ﬁeld fea  markov logic network mln consists set ﬁrstorder tures built subfeatures arbitrary                                                        number levels speciﬁcally each feature  formulas weights wifi serve template                                                                constructing markov random ﬁeld each feature    base⎛ case ⎞  markov random ﬁeld grounding formulas                                                                                          ⎝            ⎠  joint probability distribution         fix     exp      wijfjx    recursive case                                                                  zi                                                                                                                                                                                          exp                       recursive case summation features                                                      referenced “parent” feature fi child feature fj                                                        appear parent feature rrf  ni number true groundings ith formula                                                        viewed directed acyclic graph features  given assignment normalization constant                                                        attribute values leaves probability  probabilities worlds sum richardson                                                        conﬁguration given root note probabilistic  domingos  show ﬁnite domains gen                                                        graphical model represented rrf undirected  eralizes ﬁrstorder logic markov random ﬁelds                                                          overall distribution simply recursive feature    way think markov logic network                                                        write probability distribution follows  “softening” deterministic knowledge base ﬁrstorder                                                                                 knowledge base represented conjunction                          groundings formulas mlns relax conjunc normalization root feature  tion allowing worlds violate formulas assigning perfeature normalization constants zi absorbed  pergrounding penalty each violated formula worlds corresponding feature weights wki parent fea  violate formulas possible likely tures fk user free choose convenient  violate fewer way inconsistent knowl normalization normalization  edge bases useful                         easy show generalizes markov random    mlns soften conjunctions universals ﬁelds conjunctive disjunctive features each fi ap  disjunctions existentials remain deterministic just proximates conjunction weights wij large  mlns probability world decreases gradually limit fi  iff conjunct true fi  number false groundings universally quantiﬁed represent disjuncts using large negative weights  mula probability increase gradually negative weight parent feature fk wki nega  number true groundings existentially quantiﬁed tive weight wki turns conjunction disjunction just  mula rrfs accomplish                           negation does morgan’s laws                                                        conjunction disjunction represent    recursive random fields                            mofn concepts complex distributions                                                        different features different weights  recursive random ﬁeld rrf loglinear model note features small absolute weights little  each feature observable random variable effect instead using heuristics search  output recursive random ﬁeld build intu determine attributes appear feature  ition ﬁrst propositional case generalize include predicates let weight learning sort  interesting relational case concrete example attributes relevant feature sim  given section  illustrated figure  ilar learning neural network initializing small                                                        random values network represent logical    propositional rrfs                               formula need commit speciﬁc structure  primary goal solving relational problems rrfs ahead time attractive alternative tradi  interesting propositional domains propo tional inductive methods used learning mrf features  sitional rrfs extend markov random ﬁelds boltzmann  rrf seen type multilayer neural net  machines way multilayer perceptrons extend work node function exponential  singlelayer ones extension simple principle sigmoidal network trained maximize joint like  allows rrfs compactly represent important concepts lihood unlike multilayer perceptrons ran  mofn allows rrfs learn features dom variables inputs outputs rrfs  weight learning effective current variables inputs output joint probability  featuresearch methods markov random ﬁelds       ways rrf resembles boltzmann machine    probability distribution represented propositional greater ﬂexibility multiple layers learnable  rrf follows                                    ing variant backpropagation algorithm rrfs                                                                                               hidden variables sum nodes network                                        exp      wifix              deterministic values making inference efﬁcient                                                                              relational rrfs  normalization constant ensure prob relational case relations arbitrary number  abilities possible states sum  makes objects place ﬁxed number variables allow                                                    ijcai                                                     parameter tying different groundings use parame interesting belief richardson domingos  terized features parfeatures represent parameter  people tend smoke friends omit  tuple vector g size depends arity ted simplicity demonstrate represent  parfeature note g vector logical variables beliefs ﬁrst converting ﬁrstorder logic  arguments predicates opposed random boolean converting rrf  variables ground atoms represent state world represent ﬁrst belief “smoking causes  use subscripts distinguish parfeatures dif cer” ﬁrstorder logic universally quantiﬁed implica                                                     ∀      ⇒  ferent parameterizations ig  ig   represent tion smg cag implication rewritten  different groundings ith parfeature           disjunction ¬smg ∨ cag morgan’s laws    each rrf parfeature deﬁned ways   equivalent ¬smg ∧¬cag rep                                                        resented rrf feature                ig rigi    gik  base case                  ⎛                  ⎞                                                                                                            exp  smg  cag                 ⎝                  ⎠                                      fix    exp      wij      recursive case                           jgg                                                                                             positive  negative feature                           g                                                        weight negative shown general  base case straightforward simply represents rrf features model conjunction disjunction  truth value ground relation speciﬁed cnf knowledge base represented rrf sim  grounding each possible combination param ilar approach works second belief “friends people  eters arguments parfeature recursive case sums friends”  weighted values child parfeatures each parameter ﬁrst beliefs handled markov logic  gi child parfeature parameter parent fea networks key advantage recursive random ﬁelds  ture gi ∈ g parameter child feature summed representing complex formulas belief “ev                                                      eryone friend smokes” naturally rep  does appear parent feature gi ∈                                                        resented nested quantiﬁers ∀g∃hfrg ∧ smh  g parameters analogous parameters appear                                                        best represented rrf feature references sec  body head horn clause just sums                                                      ondary feature  child features act conjunctions summations                                                                                                                     parameters act universal quantiﬁers markov logic se               mantics fact generalized quantiﬁers represent   fgx    exp     wfghx                                                                           mofall concepts just simple feature sums repre                     sent mofn concepts                                                                                       relational version recursive random ﬁeld gh exp frg  smh  fore deﬁned follows                                                                                                        note rrfs feature represent distribution                    xfx                      number smoking friends each person depend                                                        ing assigned weights it’s possible  set ground relations ra sa                                                        smoking friend people  assignment truth values ground relations                                                        three rrf actually learn  root recursive parfeature root                                                        distribution data  parameters recursive parfeature normal                                                          belief problematic mln  ized constant ensure valid probability distri                                                        mln purely logical there’s change prob  bution propositional case zi’s                                                        ability number smoking friends number  absorbed weights parent features                                                        exceeds secondly mlns represent belief ef  normalized convenient way                                                        ﬁciently mln existential quantiﬁer converted    relational rrf converted propositional                                                        large disjunction  rrf grounding parfeatures expanding summa  tions each distinct grounding parfeature dis frg ∧ sma ∨ frg ∧ smb ∨···  tinct feature shared weights                                                         objects database disjunction    rrf example                                       conjunctions mln convert                                                                                      clarify ideas let example knowledge base dnf cnf form leading  cnf clauses each  richardson domingos  domain consists grounding rule  three predicates smokesg smoker cancerg features deﬁne   joint distribution follows                                                                        cancer friendsg friend  exp      wf                                                                                   abbreviate predicates smg cag frg hre                                                                                                        ghi  gh   spectively                                                                                                                                                        wish represent three beliefs smoking causes                    cer ii friends friends friends transitivity friend figure  diagrams ﬁrstorder knowledge base contain  ship iii friend smokes ing beliefs corresponding rrf                                                    ijcai                                                                                                                                                                                                                                                                                                                                                           ghi                                                                                                                                                    fg                                                                                                                                               ghi ghi                                                                                                                                                                                                                                                   gh                                                                                                                                                   smg   cag  frgh frhi frgi       ¬smg cag ¬frgh ¬frhi frgi                                         frgh smh                                      frgh smh    figure  comparison ﬁrstorder logic rrf structures rrf structure closely mirrors ﬁrstorder logic  connectives quantiﬁers replaced weighted sums      inference                                          ﬁrst term evaluated chain rule  rrfs generalize mlns turn generalize ﬁnite          ∂ logf  ∂ logf ∂fi                                                                                ﬁrstorder logic markov random ﬁelds exact inference            ∂wij        ∂fi   ∂wij  intractable instead use mcmc inference particu                                                        deﬁnition fi including normalization zi  lar gibbs sampling straightforward sample each                                  unknown ground predicate turn conditioned          ∂fi              ∂zi                                                                            fi fj −  ground predicates conditional probability particular      ∂wij           zi ∂wij  ground predicate easily computed evaluating                                                          repeated applications chain rule  relative probabilities predicate true                                                        ∂ logf∂fi term sum derivatives  false                                                        paths through network fi given path    speed signiﬁcantly caching feature sums                                                        feature graph ffafkfi derivative  predicate updated notiﬁes parents                                                        path takes form fwafawbfb ···wkfkwiwe  change necessary values recomputed                                                        efﬁciently compute sum paths caching    current implementation map inference uses                                                        perfeature partials ∂f∂fi analogous backpropagation  erated conditional modes icm besag  simple                                                          second term ∂ logz∂wij expected value  method ﬁnding mode distribution starting                                                                                             ﬁrst term evaluated possible inputs   random conﬁguration icm sets each variable turn                                                        complete partial derivative  likely value conditioned variables                                                                                                                                              procedure continues singlevariable change fur ∂ log ∂ logfx   ∂ logfx                                                                                    − ex  ther improve probability icm easy implement fast ∂wij         ∂wij              ∂wij  run guaranteed converge unfortunately  guarantee converging likely overall conﬁgura individual components evaluated  tion possible improvements include random restarts simu computing expectation typically intractable  lated annealing                                 approximated using gibbs sampling efﬁcient    use icm ﬁnd initial state gibbs sam alternative used richardson domingos                                                                                           ∗            pler starting mode signiﬁcantly reduce burn instead optimize pseudolikelihood   besag                                                                              n  time achieve better predictions sooner                  ∗                                                                 xx      xt  xtmbxxt    learning                                                                                                                            mb                                       given particular rrf structure initial set weights state markov blanket  learn weights using novel variant backpropagation data pseudolikelihood consistent estimator  algorithm traditional backpropagation goal little known formal properties  efﬁciently compute derivative loss function form poorly long chains inference required  respect each weight model case loss worked quite test domain  function error predicting output variables expression gradient pseudolog  joint log likelihood variables likelihood propositional rrf follows                                                                               consider partition function root feature zfor ∂ ∗                                                                log                   ¬x mb    computations extract term                                                                                           ∂wi  use refer unnormalized feature value                                                                                                                                              ∂      begin discussing simpler propositional case                   ∂ log  log xt¬xt                                                                       ×          −  abbreviate  arguments derivative                     ∂wi          ∂wi  log likelihood respect weight wij consists  terms                                                compute iterating query predicates    ∂ log  ∂ logfz  ∂ logf  ∂ logz     toggling each turn computing relative likeli                                     −       ∂wij         ∂wij         ∂wij       ∂wij        hood unnormalized likelihood gradient permuted                                                    ijcai                                                     state note compute gradient unnormalized  experiments probabilistic integrity  log likelihood subroutine computing gradient constraints  pseudologlikelihood longer need  approximate intractable normalization term    integrity constraints statements ﬁrstorder logic                                                                                                   learn relational rrf use domain instanti used detect repair database errors abiteboul et                                                                  ate propositional rrf tied weights number al   logical statements work errors  features number children feature critical increasingly impractical noisy  pend number objects domain instead databases arise integration mul  weight attached single feature attached tiple databases information extraction web  set groundings parfeature partial derivative want make constraints probabilistic sta  respect weight sum partial deriva tistically infer types errors sources date                                                                                                      tives respect each instantiation shared weight little work problem andrit                                                        sos et al  ilyas et al  early approaches                                                        natural domain mlns rrfs use    rrfs vs mlns                                      ﬁrstorder formulas construct probability distributions  rrfs mlns subsume probabilistic models   worlds databases  ﬁrstorder logic ﬁnite domains trained gen common types integrity constraints  eratively discriminatively using gradient descent inclusion constraints functional dependencies inclusion  optimize log likelihood pseudolikelihood constraints form  optimizing log likelihood normalization constant        ∀x∃yrx ⇒ ∃zsx  approximated using probable explanation  mcmc                                              example company   relation    mln converted relational rrf trans “projectlead”x charge project  lating each clause equivalent parfeature sufﬁ relation “managerof”x manages employee  ciently large weights parfeature approximates hard constraint says project leader manages  junction disjunction children worker course employees manage  weights sufﬁciently distinct parfeature employees lead project  different value each conﬁguration children evaluate mlns rrfs inclusion constraints  allows rrfs compactly represent distributions generated domains consisting  people  projects  require exponential number clauses mln   probability  person project leader leads    rrf converted mln ﬂattening coleads each project probability  each  model typically require exponential number project leads manages employee probability   clauses mln intractable learning additional managing relationships generated prob  inference rrfs better modeling soft ability vary   actual  disjunction existential quantiﬁcation nested formulas leadership management relationships unobserved    addition “softer” mln clause rrf noisy versions corrupted probabil  parfeature represent different mln clauses simply ity vary    adjusting weights makes rrf weight learning converted constraint formula mln  powerful mln structure learning rrf rrf described previous sections added im                                                      plications projectleadx ⇒ projectleadx    recursive parfeatures root represent                           mln structure clauses managerofx ⇒ managerof predi  distributions nclause mln represent  cates primes observed ones    leads new alternatives structure learning mlns rrfs worked better given directions  theory revision domain little background knowl integrity constraint project leaders manage people  edge available rrf initialized small ran managers lead projects learned weights optimize  dom weights converge good statistical model pseudologlikelihood models results shown  potentially better mln structure learning figure  each data point average  traintest  constructs clauses predicate time set pairs rrfs show consistent advantage mlns  adjust weights evaluate candidate clause    cause better represent fact employee    background knowledge available begin manages people probably project leader  initializing rrf background theory just employee manages people  mlns addition known dependen    second type integrity constraints functional depen  cies add dependencies parfeatures dencies form  predicates small weights weight learning ∀x  ∃z  rx   ∧ rx   ⇒   learn large weights relevant dependencies negligi                                      ble weights irrelevant dependencies analagous functional dependency each determines unique  kbann does using neural networks tow equivalence set ys example suppose com  ell shavlik  contrast mlns theory pany table parts suppliers represented  revision through discrete search                     lation suppliertaxid companyname parttype sup                                                    ijcai                                                     
