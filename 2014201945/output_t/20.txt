                        experts algorithm transfer learning                                       erik talvitie satinder singh                                           university michigan                                     science engineering                                       etalviti bavejaumichedu                          abstract                          number experts size problem                                                        order enforce restriction algorithm present      longlived agent continually faces new tasks makes use state observations actions taken      environment agent able use experts information available      knowledge learned solving earlier tasks pro  expect agent let πb denote      duce candidate policies current task                                                        “best” expert largest expected asymptotic      multiple reasonable policies sug average reward objective agent’s      gested prior experience agent    actual return time step near asymptotic      choose potentially apri                                                                turn π  clearly unreasonable      ori knowledge applicability cur agent knew identity πb begin mediator      rent situation present “experts” algorithm need time close unknown mixing time πb      efﬁciently choosing candidate policies                                                        achieve return intuitively mixing time policy      solving unknown markov decision process                                                        time takes guarantee return      task conclude results experiments close asymptotic return need reason      domains generate candidate                                                        able objective paper provide mediator algorithm      policies solutions related tasks use                                                        time polynomial  accomplishes actual return      experts algorithm choose         close asymptotic return best expert mix                                                        ing time  mediator given time    introduction                                       run competes favorably larger subset experts  agent sufﬁciently complex environment likely  face tasks related solved given good  relationship existing work  policy related task agent determine reason idea using state mappings known mdps gener  able policy current task mapping current situa ate knowledge mdps entirely novel  tion analogous task knows instance homomorphisms mdps used  taking action situation generate abstractions problems allowing compact rep  reasonable ways agent apply resentations induce policies problem  experience new situation knowledge work restrict mappings special class  new problem way evaluate seek optimal mapping consider  apriori                                              optimal mapping difﬁcult impos    particular represent agent’s sequential decision sible calculate set “reasonable” mappings  making problem markov decision process mdp heuristically generated guarantee  assume learned optimal policy mdp quality policies use “experts”  faces new unknown mdp  agent algorithms efﬁciently choose ﬁnd good  group candidate policies  generated albeit suboptimal policy quickly  mappings states  work learning use advice  agent’s policy following terminology used team “experts” perform task traditionally  supervised learning settings think policies focused supervised learning setting   “experts” advise agent agent   sequential nature prob  mediate experts order leverage lem clearly related multiarmed bandit prob  knowledge learning solution new task     lem  long stood canonical example    agent simply ignore expert advice learn “explorationexploitation” tradeoff online learning  new task scratch ideally experts agent presented slot machine arms  provide signiﬁcant savings learning time each pull arm yields reward drawn  desire algorithm sample complexity dependent known ﬁxed distribution agent’s goal minimize                                                    ijcai                                                    regret difference reward got policy π mdp probability distribution  pulling best arm reward actually actions conditioned state time write πts  receives lai robbins provided algorithm pas probability taking action state time                                                                                            pulls achieves regret olog  →∞ policy stationary πts aπt ∀t tforthe  similarity setting clear results relied remainder policies mentioned assumed station  fact each arm ﬁxed reward distribution time ary unless stated  case “arms” policies shared path sequence states                                                              π  mdp                                                  write pm mean probability traversing    important generalization multiarmed bandit following policy π policy called ergodic  problem removed statistical assumptions se number steps approaches inﬁnity probability  quence rewards assigned each arm allowing adver ing particular state approaches ﬁxed limiting value  sary select reward distribution√ each arm time mdp called unichain stationary policies  step  auer et al provide  bound regret ergodic restrict attention unichain mdps  ing algorithm exp adversarial setting let mdp π policy mandp   analysis auer et al assumed adversary creates path expected undiscounted return                                                                            priori ﬁxed sequence reward distributions      expected return  affected actions decision maker choosing state expected undiscounted step return state                                                                                  π             π  different sequences experts result different dynam following policy π um  pm pum  ics underlying mdp bounds exp apply asymptotic expected undiscounted return state                                                             π                π  setting clear similarity um  limt →∞ um  note ergodic                                                                  π                                      π  setting compare algorithm exp policy π um independent write um  empirical work                                   asymptotic return drop explicit    algorithm present closely related dependence unknown ﬁxed mdp  family algorithms collectively called “exploration ex problem setting agent acting unknown  ploitation experts methods” eee  algorithms mdp provided set stationary policies  select experts adversarial setting “experts” each step agent choose  environment “chooses” observations bt depending experts act behalf does perceive current  agent’s past actions aa  at− giving agent reward state action taken does receive reward signal  ratbt each time step say expert goal agent achieve undiscounted return  achievable τvalue μ exists constant cτ ≥  close expected asymptotic return best expert                                     step  possible history num efﬁcient time  berofstepst                                            central problem agent does know                                                      experts’ mixing times sure following                st                                          ≥ μ −  τ             expert ﬁnite number steps provide                             sτ             good estimate expert’s asymptotic quality               ss                                   present algorithm explicitly addresses issue    eee algorithms achieve return close highest  achievable τvalue time polynomial cτ             atease policymediating algorithm    setting special case considered farias  megiddo assume environment governed section present stylized algorithm facil  mdp allowing depend entire itates analysis experiments introduce  tory algorithm quite similar practicallyminded alterations algorithm  eee family considering special case atease ”alternating trusting exploration suspicious                                                                                                       characterize direct dependence mixing times exploitation” proceeds iterations indexed                                                              policies abstract quantity cτ     each iteration involves trusting exploration  allows formally understand algorithm phase followed suspicious exploitation phase                                                                                                        form during entire runtime gives strong intu trusting exploration phase atease tries expert exp                                                                                            ition algorithm useful     steps exp ﬁxed polynomial  regardless                                                        previous disappointments expert caused                                                        regardless poorly doing during    preliminaries                                      ﬁxed time suspicious exploitation phase atease ranks  markov decision process mdp agent perceives experts according performance exploration  state world ﬁnite set decides phase tries using best expert constant num  action set given state action ber batches shall each texp steps long  world probabilistically transitions new state agent return batches lower ex  receives reward drawn distribution associated pert’s return exploration phase atease stops using  new state assume rewards bounded expert proceeds exploit best process  nonnegative holds easy elimination continued experts  obtain adding minimum reward reward signals expert lasts long exploitation phase                                                    ijcai                                                                     arguments   δ   rmax   sete experts                   initialize ←                    trusting exploration phase                    run each expert texp polynomial steps recording texp–step returns                    set e ←                                                          sort experts texp–step returns u˜                   suspicious exploitation phase                    e  ∅ set ← goto                                                                                          let expert highest texp–step return                     run batch texp steps                                                                                               return batch u˜ −  remove goto                     run ltexp steps ← goto  goto                                      table  pseudocode atease algorithm                                                                   t  eliminated phases repeated optΠe  probability −δ time polynomial                                                                    increasing durations order allow possibility    δ  rmax  experts long mixing times perform note mixing time asymptotically best ex  long run pseudocode provided table  pert ∗ actual return atease compete    note reach iteration associ expert time polynomial ∗ asymptoti  ated exploration time greater mixing time cally best expert mixes quickly performance atease  experts problem choosing best expert precisely compare favorably expert quickly  stochastic multiarmed bandit problem unfortunately experts longer mixing times  way agent know reached ﬁrst glance theorem  imply sample  point each iteration conceptually like bandit al complexity atease completely independent num  gorithm trying choose unknown set experts ber states actions mdp environment  mixing times exploration time ex case mixing time experts  perts rejected during exploitation order minimize general dependent size mdp state space  effect experts mixed            mixing time asymptotically best expert    atease input conﬁdence parameter δanap                                                       exponential size state space  proximation parameter  bound maximum reward pointed algorithm avoid  rmax set experts show high                                                    running best experts mixing time  probability  time polynomial actual pendence atease complexity mdp entirely  return atease compare favorably expected unavoidable dependence mixing time  return best policy mixes time     use standard concept mixing time  use weaker directly applicable notion   empirical illustrations  expectedreturn mixing time  related section use toy problems study applica  standard deﬁnitions mixing time                  bility atease algorithm comparison experts                                                        algorithms despite existence  deﬁnition  let mdp let π ergodic                                                        sophisticated techniques simple greedy algorithm  policy mtheexpectedreturn mixing time π denoted                                            π          chooses arm looks best far  tπ smallest ≥ tmaxi −   π                                                    probability  chooses action random difﬁcult   ≤                                                        beat practice  use greedy repre  deﬁnition  let set stationary ergodic experts sentative bandit algorithms compare greedy                     t  mdp  mthenΠe       denotes set experts previously discussed exp algorithm slightly prac  expectedreturn mixing time deﬁne tical version atease denoted ateasel ateaselite       t              π  optΠe maxπ∈Πt                                 contains modiﬁcations designed help speed                                                      convergence                                         appropriate selection exp  time poly ateasel differs atease increases ex  nomial mixing time  atease achieve return              t                                       ploration time number exploitation batches  close optΠe  high probability formalize chooses experts eliminated exploitation phase  claim theorem  similarity each iteration exploration time multiplied  farias megiddo’s results present proof constant incremented larger  theorem    given input parameters  δ jumps exploration time help expand number mixing  rmaxtheatease  algorithm’s actual return  experts quickly large constant num                                                        ber exploitation batches set equal exploration time    kearns  singh called return mixing time     reducing impact earlier iterations finally during                                                    ijcai                                                                     −armed bandit problem mixing problem                                                                              mixing problem                    ε−greedy                                                                                                                                                                                                           ateasel                              ateasel                  ε−greedy                                                                              exp                                                                                                                                                                                                                                                                                                                 expert   expert                                                                                                                                                                                                                 exp                                           average  reward time step                 average  reward time step                                                                      time step                time step                             figure  results toy problems text descriptions results three algorithms averaged  runs    suspicious exploitation phase experts abandoned robocup keepaway  delivery domain  performance batch falls best expert’s         ball                                                                                                        queue   estimate minus constant  ensures                            best expert mixed continue exploited keeper                                                                      taker                       conveyor  performing better alternatives                                 belts  sake clarity simple experiments set     ∞ exploitation phase suspicious                         queue                                                                                                     ﬁrst problem figure  standard armed bandit  problem underlying mdp state actions  giving reward  reward  each expert figure  robocup soccer keepaway domain  ∈     chooses rewarding action probabil delivery domain text descriptions      ity  andsoexperti expected asymptotic return                                               expectedreturn mixing time   applications transfer learning  algorithms perform similarly beginning  tually ateasel surpassed illustrates aspect demonstrate utility experts algorithms trans  ateasel algorithm continues explore low fer settings described introduction imagine  return experts longer longer periods time hopes agent applies knowledge task  perform better long run neces mapping original statespace new  sary case underlying domain mdp mapping combined policy old problem  long mixing time case result induces policy new state space agent  slower convergence time comparison bandit algo able identify optimal mapping  rithms “sawtooth” pattern seen shows clearly advised multiple “experts” provide different state  effects alternating exploration exploitation phases mappings problem automatically discovering small                                                        set “reasonable” mappings deep outside    second problem figure  state action mdp scope paper experiments mappings  shown  experts choses heuristically created hand  action state  action state  section consider transfer learning problems  expected asymptotic return  mixes quickly ﬁrst agent presented task complex  expert choses action state  learned mappings rep  state  choses probability  probabil resent different ways discard state information  ity  second expert expected asymptotic return order make use knowledge simpler space   takes longer mix problem highlights second imagine agent loses use  strength ateasel greedy exp likely sensors agent’s mappings educated guesses  stay expert long allow mix add state information obtain advice  receive good estimates experts ’s quality contrast policy depends richer observations  ateasel discovers second expert quickly adopts  subsequent iteration accounts superior  robocup soccer keepaway  return seen                                     ﬁrst experiment used modiﬁed version                                                        stone’s robocup keepaway testbed  domain sim    results simple exeriments intended high ulates teams robots keepers takers  light strengths weaknesses atease particu figure  keepers attempt ball takers  lar seen atease effective solving long possible intended multi  stochastic bandit problem comparison algorithms specif agent learning problem considered simpler problem  ically designed purpose mixing time ﬁxing agent’s policy provided handcoded  experts unknown signiﬁcantly outperform algo policy modiﬁcation reward signal  rithms mixing time account     originally posed reward action number                                                    ijcai                                                           −  robocup experts                − ateasel sarsa               −     ε−greedy                                                               −                                 −                                  −       −                                 −                                  −       −                                 −                                  −       −                                 −                                  −       −                                 −                                  −       −                                 −                                  −     avg  reward simulation step   avg  reward simulation step                                                                           avg  reward simulation step     −                                 −                                  −                                                                                               training time hours              training time hours              training time hours    figure  results keepaway figure shows performance typical set experts ateasel       shown solid lines sarsa α      dashed lines greedy       steps agent recieved ball case       delivery domain results                                                                average reward  instead used reward                      h−learning  signal agent recieved reward action       qh          ateasel  end episode recieved reward  inci                                                                      qh         ε−greedy  dentally reinforcment learning agents learned        qh  faster reward signal                                                 exp                                                                  qh    following  used sarsa tilecoding  linear function approximator train agents   away  episodes asked use  resulting policies keepaway gener                                                            average  reward time step                                                                 ated  experts each mapped keepaway ignoring                                                                                                                   time steps             keeper using different criterion closest furthest                                 “most open” typical spectrum performance   experts keepaway shown figure                                                         figure  results experiments delivery domain    figure  performance  representative averaged  runs comparing ateasel     runs ateasel compared  representative runs linear  hlearning ρ α   expp  sarsa learning keepaway scratch domain δ   greedy   label qxhy represents  best expert longest mixing time expert assumes queue  contains job pick  surprise ateasel does approach performance queue  assumes agent holding job  best expert time shown  cases able quickly avoid “bad” experts  delivery domain  note unless optimal policy experts pro                                                                               vided ateasel achieve optimal return delivery domain   robot deliver jobs  expected learning algorithm eventually queues conveyor belts figure  queue  pro  surpass ateasel learner spends signiﬁcant duces jobs type  destined conveyor  time performing poorly transient period belt  provide reward  delivered queue  poor performance transfer learning attempts avoid  produces jobs type   equal probability jobs  ateasel appears sidestep effectively      type  destined conveyor belt  provide reward                                                         delivered obstacle open circle moves    note episodic nature equal probability agent incurs penalty   domain return each episode unbiased es collides obstacle world fully observable  timate expert’s expected return each ex  sensors agent  actions available  pert’s mixing time episode thinking switch lanes pick  time terms episodes problem expressed drop pick action available agent  stochastic bandit problem compare ateasel job queue similarly drop action  greedy each “pull” chooses expert available appropriate conveyor belt agent  episode figure  shows  representative runs greedy holds job  expect toy examples greedy following tadepalli trained agents using hlearning  perform better ateasel average reward reinforcement learning algorithm  like toy experiment ateasel does perform compet ρ α   asked policies  itively greedy provides theoretical perfor used agents lose  sensors  mance guarantees greedy                dicate jobs held queues job                                                    ijcai                                                    
