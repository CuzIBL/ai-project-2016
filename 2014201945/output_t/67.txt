         semantic smoothing document models agglomerative clustering                               xiaohua zhou xiaodan zhang xiaohua hu                                             drexel university                               college information science  technology              xiaohuazhoudrexeledu xzhangischooldrexeledu thuischooldrexeledu                          abstract                          analyze underlying reasons poor performance                                                        propose solution      paper argue agglomerative                                                          steinbach et al  argue agglomerative      clustering vector cosine similarity measure                                                        hierarchical clustering perform poorly nearest      performs poorly reasons                                                        neighbors document belong different classes      nearest neighbors document belong different      classes cases pair documents cases according examination data each class                                                        “core” vocabulary words remaining “general”      shares lots “general” words second sparsity                                                        words similar distributions different classes      classspecific “core” words leads grouping                                                        documents different classes share      documents class labels different                                                        general words stop words viewed similar      clusters problems resolved suitable                                                        terms vector cosine similarity solve problem      smoothing document model using kullback      leibler divergence smoothed models      “discount” general words “emphasize”                                                        importance core words vector think      pairwise document distances inspired recent                                                        poor performance agglomerative clustering      work information retrieval propose novel                                                        attributed sparsity core words document      contextsensitive semantic smoothing method                                                        document short contains number      automatically identifies multiword phrases                                                        core words documents class      document statistically map phrases      individual document terms evaluate new    share core words falsely grouped different                                                        clusters using vector cosine similarity metric solve      modelbased similarity measure three datasets                                                        problem assign reasonable positive counts      using complete linkage criterion agglomerative                                                        “unseen” core words related topical terms occur      clustering significantly improves                                                        document      clustering quality traditional vector cosine      measure                                            discounting seen words assigning reasonable counts                                                        unseen words exact goals probabilistic                                                        language model smoothing paper view   introduction                                        calculation pairwise document similarity process                                                        document model smoothing comparison usual  document clustering algorithms categorized use kullbackleibler divergence distance function  agglomerative partitional approaches according measure difference models word probability  underlying clustering strategy kaufman rousseeuw distributions problem reduced obtaining good   agglomerative approaches initially assign each smoothed language model each document corpus  document cluster repeatedly merge pairs language modeling approach information retrieval  similar clusters cluster left ir received attention recent years  partitional approaches iteratively reestimate cluster mathematical foundation empirical effectiveness  model cluster centroid reassign each document nugget language modeling approach ir  closest cluster document moved smooth document models lafferty zhai   longer comparison partitional approaches best knowledge document model smoothing  agglomerative approach does need initialization studied context agglomerative clustering  gives intuitive explanation set documents paper adapt existing smoothing methods used                                                     grouped suffers  language modeling ir context agglomerative  clustering time performs poorly general terms clustering hypothesize document model smoothing  cluster quality steinbach et al  paper                                                      ijcai                                                    significantly improve quality agglomerative  adopted compound terms text classification  hierarchical clustering                              compound terms used smoothing    ir simple effective smoothing strategy purpose work instead compound terms directly  interpolate document models background collection working features conjunction singleword  model example jelinekmercer dirichlet absolute features previous work zhou et al   discount zhai  lafferty   twostage    proposed contextsensitive semantic smoothing method  smoothing zhai lafferty  based language modeling ir method decomposes  strategy document clustering tfidf score used document set weighted contextsensitive topic  dimension values document vectors effect signatures statistically maps topic signatures  tfidf scheme roughly equivalent background individual terms topic signature defined pair  model smoothing potentially significant concepts semantically syntactically related  effective smoothing method referred each topic signature similar compound  semantic smoothing context sense information term sense constant unambiguous  incorporated model lafferty zhai  meanings cases instance “star” “movie”  trial semantic smoothing dated forms topic signature context highly related  latent semantic indexing lsi deerwester et al  “entertainment” rarely “war” extraction  projects documents corpus reduced space concepts concept pairs relies domain  document semantics clear lsi explores ontology impractical public domains  structure term cooccurrence solve synonymy overcome limitation propose use  brings noise reducing dimensionality multiword phrases “star war” “movie star” topic  unable recognize polysemy signatures paper concept pair multiword  term different contexts practice criticized phrase unambiguous furthermore multiword  lack scalability interpretability         phrases extracted corpus existing statistical    berger lafferty  proposed kind semantic approaches human knowledge documents  smoothing approach referred statistical translation multiword phrases robust smooth  language model statistically mapped document terms document model through statistical translation multiword  query terms                                     phrases document individual terms                                                          evaluate new modelbased document similarity                    pq  tq  pw                    metric three datasets using agglomerative clustering                                                      complete linkage criterion kaufman rousseeuw       probability translating document        tq                                          experiment results show kldivergence  term query term pw  maximum similarity metric performs consistently better vector  likelihood estimator document model term cosine metric kldivergence metric  translations document containing “star” returned semantic smoothing significantly outperforms simple  query “movie” likewise document  background smoothing result agglomerative  dimension “star” “movie” merged clustering semantic smoothing comparable  cluster “entertainment” document   kmeans partitional clustering three testing datasets  containing “movie” “star” like lsi    approach suffers contextinsensitivity  vt                     vd             vw  problem unable incorporate contextual information                                                             model resulting translation fairly                                  general contain mixed topics example “star”  class “entertainment” movie star                                                                                                class “military” star war                                                                                                                       unlike berger lafferty  estimated word       translation probabilities purely based word distributions                                 corpus cao et al  constrained word relationships    human knowledge relationships defined   figure  illustration document indexing vbtb vbdb vbwb  wordnet order reduce noise combined phrase set document set word set respectively  linearly semanticconstrained translation model  smoothed unigram  document model     document model smoothing  model did solve contextinsensitivity problem  essence                                               semantic smoothing document model    compound terms play important role   suppose indexed documents given  machine understand meaning texts collection terms individual words topic  usually constant unambiguous meanings bai et al signatures multiword phrases illustrated figure                                                       ijcai                                                    translation probabilities topic signature tk experiment parameters set     individual term denoted pwtk given  respectively  easily obtain document model                                                        table  examples phraseword translations three phrases                    pt   pw  tk  pml tk            automatically extracted collection newsgroup                                                      xtract list  topical words each phrase  likelihood given document generating topic  arab country     nuclear power     gay people                                                           term    prob     term    prob   term     prob  signature tk estimated                                                         arab       nuclear      gay                               ct                           country    power        homosexual                                   pml tk                                israel     plant        sexual                           cti                          jew        technology   church                                                            israeli    air          persecute                                                           jewish     fuel         friend       ct frequency topic signature tbib                                                       palestine  fossil       abolitionist   given document                                             reactor      parent         refer model translation model syria    steam        society      berger lafferty’s work  discussed expel   contaminate  lesbian      introduction translation multiword phrase terror     water          individual term specific translation iraq  cold       lover        model weakens effect “general” words davidsson  cool      lifestyle                                                           war        tower        emotion      relieves sparsity classspecific “core” words homeland  industry  thier        topics document expressed egypt   radioactive  repress      topic signatures multiword phrases translation zionist  boil  affirm       model used information loss legitimism  site      ministry     natural extension interpolate translation model kaufman  built  straight     unigram language model                        rejoinder  temperature  preach                   pw          each phrase bkb set documents bkb                    ml                                containing phrase intuitively use document     coefficient accounting background set kb estimate translation probabilities tbkb                                                          determining probability translating given phrase tb  collection model pw  pml  maximum                                                   likelihood estimator experiment  set  terms vocabulary terms appearing  refer unigram model simple language model document set center subtopic represented tb kbwe                                                        simply use maximum likelihood estimator  baseline language model use jelinekmercer smoothing                                                        problem simple term frequency counting  purpose discounting “general” words                                                        terms address issue subtopics    final document model clustering use described                                                        background terms collection use  equation  mixture model components                                                        mixture language model remove noise assuming set  simple language model translation model                                                          documents containing tbkb generated mixture                                 pbt    pb  pt         language model terms document set                                                        translated given topic signature model pw    translation coefficient  control influence                                       tk  components mixture model training data generated background collection model pw    translation coefficient trained optimizing  clustering quality                                       pw      pw    pw                                                                     tk                tk   topic signature extraction translation         coefficient accounting background noise                                                         denotes parameter set translation probabilities  zhou et al  implemented topic signatures concept  tk    pairs developed ontologybased approach extract tbkb mixture language model log likelihood    concepts concept pairs documents generating document set dbkb  domains ontology available reason                                                            log pd   cw log pw     propose use multiword phrases topic signatures       tk                     tk  employ xtract smadja  identify phrases                     documents xtract kind statistical extraction tool cw dk  document frequency term kb    syntactic constraints able extract noun phrases cooccurrence count kb  frequently occurring corpus external collection translation model estimated using    knowledge xtract uses parameters strength kbb peak em algorithm dempster et al  em update    zscore kbb spread ubb percentage frequency formulas  control quantity quality extracted phrases                                                      ijcai                                                                                                   dataset normalized arithmetic mean maximum                               pˆ                                  possible entropies empirical marginals                     pw                              tk                                                                         ix                      cw  pˆ                       nmix                                                                                                          tk                                                   logk  logc                      cwi   dk  pˆ wi                                                       random variable cluster assignments  experiment set background coefficient  random variable preexisting labels data  truncate terms extremely small translation number clusters number pre  probabilities purposes smaller number existing classes regarding details computing ix  translation space document smoothing refer banerjee ghosh  nmi ranges  efficient second assume terms extremely   bigger nmi higher quality  small probability noise semantically related clustering nmi better common extrinsic  given topic signature disregard terms measures purity entropy sense does  translation probability  renormalize necessarily increase number clusters  translation probabilities remaining terms increases                                                          complete linkage criterion agglomerative   kldivergence distance metric                 hierarchical clustering document similarity metrics  estimating language model each document traditional vector cosine kullbackleibler  corpus contextsensitive semantic smoothing use divergence proposed paper cosine similarity  kullbackleibler divergence language models try three different vector representations term frequency  distance measure corresponding documents tf normalized term frequency tf divided vector  given probabilistic document models pwd   length tfidf kldivergence metric use  pwd kldivergence distance pwd pwd document models semantic smoothing described  defined                                           equation  test  translation coefficients  ranging                                                           actually uses simple background                                pw              smoothing           pw   log                               wv             pw                order compare partitional approach  vocabulary corpus kldivergence implement basic kmeans using cosine similarity metric  distance nonnegative score gets zero value three vector representations tf ntf tfidf  document models exactly  calculation cluster centroid uses following  kldivergence symmetric metric formula  define distance documents minimum                       kldivergence distances                               centroid   d                                                                                                  dc                                   distd   min                                                              corpus result kmeans    calculation kldivergence involves scanning clustering varies initialization run times  vocabulary makes solution computationally  random  initialization average results  inefficient solve problem truncate terms various vector representations each run  distribution probability  estimating initialization  document model using equation  renormalize  probabilities remaining terms terms  datasets  high probability values document models makes conduct clustering experiments three datasets tdt  difference clustering results           la times trec newsgroups ng                                                        tdt corpus  document classes each   experiment settings result analysis             reports major news event la times news labeled                                                         unique section names financial entertainment   evaluation methodology                            sports newsgroups dataset collected   cluster quality evaluated three extrinsic measures different usenet newsgroups  articles each  purity zhao karypis  entropy steinbach et al index  documents tdt unique   normalized mutual information nmi banerjee class label  documents sections la  ghosh  space limit list times  documents newsgroups  result nmi increasingly popular measure cluster each document index title body content                                                        multiword phrases individual words ignore  quality measures consistent nmi                                                        sections including meta data list  stop words  runs nmi defined mutual information                                                        used testing stage  documents randomly  cluster assignments preexisting labeling                                                        picked each class given dataset merged                                                      ijcai                                                    big pool clustering each dataset create general nmi increase increase translation  random pools average experimental results coefficient till peak point  case  classes selected tdt      downward experiment consider          phrases appearing  documents topic  sections selected la times entertainment signatures order obtain good estimate translation  financial foreign late final letters metro national probabilities topics document  sports calendar view  classes ng expressed multiword phrases phrasebased  selected testing                                 semantic smoothing cause information loss                                                        interpolate translation model unigram language  table  statistics three datasets                 model make loss easy understand   dataset          tdt     la times  ng        nmi goes downward influence    indexed docs                   semantic smoothing high actually lsi causes    words                        information loss dimensionality reduction    phrases                         aggressive mechanism recover loss   avg doc length word                      sense semantic smoothing approach flexible   avg doc length phrase                       lsi      classes                                                                                                      experiment results analysis                         dragon toolkit zhou et al  used conduct                                                                                                                        nmi  clustering experiments translation coefficient    equation  trained tdt dataset maximizing   nmi clustering optimal value  applied   datasets nmi result agglomerative   hierarchical clustering complete linkage criterion                                                                                      translation coefficient  tdt  listed table  vector cosine measure used                                    ng  pairwise document similarity tfidf scheme performs      modelbased agglomoerative clustering latimes  slightly better tf scheme discussed  heuristic tfidf weighting scheme discount    figure  variance cluster quality translation  “general” words strengthen “specific” words  coefficient  controls influence semantic smoothing  document vector improve agglomerative  clustering quality kldivergence similarity measure steinbach et al  reported kmeans performed  background smoothing document models  good better agglomerative approaches  consistently outperforms cosine measure tf experiment repeated finding using vector cosine  tfidf schemes expected kldivergence measure similarity measure complete linkage algorithm performs  contextsensitive semantic smoothing significantly significantly worse kmeans table    improves quality agglomerative clustering semantic smoothing document models  three datasets semantic smoothing class  result complete linkage clustering comparable  independent general words dramatically weakened kmeans three representation schemes tf norm  classspecific “core” words strengthened tf tfidf kind indication  does appear document semantic smoothing document models effective  distance intraclass documents decreased improving agglomerative clustering approaches  distance interdocuments increased  improve clustering quality                       table nmi results regular kmeans clustering                                                        number true classes listed table   table  nmi results agglomerative hierarchical clustering dataset tf     ntf          tfidf  complete linkage criterion                                                         tdt                                            cosine           kldivergence    dataset                                              la times                                   tfntf    tfidf   background semantic                                                         ng                            tdt                         la times                          conclusions future work   ng                                                                              quality agglomerative hierarchical clustering    robustness semantic smoothing method highly depends pairwise document similarity measures  show performance curve figure  density classindependent “general” words  point  semantic smoothing improve sparsity classspecific “core” words documents make  cluster quality simple background smoothing traditional vector cosine poor similarity measure                                                      ijcai                                                    
