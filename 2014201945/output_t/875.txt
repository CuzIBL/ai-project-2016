          continuous nonlinear dimensionality reduction kernel eigenmaps                                                       matthew brand                                       mitsubishi electric research laboratories                                               cambridge ma  usa                               abstract                                       minimal distortion visavis weights                                                                      sense larger stipulates shorter embedding        equate nonlinear dimensionality reduction                  distance formally embedding        nldr graph embedding side information        vertices derive solution                                                                      problem form kernelbased mixture        affine maps ambient space target        space unlike spectral nldr methods        central eigenproblem relatively small                                                                       result continuous mapping defined        entire space just datapoints demon­       stration visualizing distribution           integer  norm constraint         word usages proxy word meanings                sets scale embedding causes vertices        sample machine learning literature                    high cumulative weight embedded nearer                                                                      origin    background graph embcddings                                   rigidly rotated changing dis­                                                                     tortion distortion measure invariant rigid   consider connected graph weighted undirected edges          translations eigenproblem   specified edge matrix let posi­                       unwanted degree freedom dof solution   tive edge weight connected vertices zero           stochasticity dof isolated uniform   let  diagwl diagonal matrix             eigenvector  suppressed embedding                cumulative edge weights vertex         error  addingto   following points known easily derived spectral      rigidly translates embedding   graph theory fiedler  chung                                                                     premultiplying  rearranging equates equa­     generalized eigenvalue decomposition evd                tion  evd graph laplacian                             wvdva                                                                                            real eigenvectors eigenvalues                       premultiplying connects equation                                                                       symmetric kvd normalized laplacian        premultiplying equation  makes general­                                                                    ized eigenproblem stochastic eigenproblem                                                                                                                                   summary equation  gives optimal embedding       stochastic transition matrix having            graph rc eigenvectors eigenvalue Ωiis       nonnegative rows sum largest eigen­      stochastic corresponding eigenvector  uniform        value equation  stochastic        important property evd solution        paired eigenvector uniform                        isolates problems unwanted translational degree free­                                                                dom single eigenvector leaving remaining eigenvec­     expanding collecting terms wij reveals geo­   tors unpolluted       metric meaning eigenvalues                           embedding algorithms derived anal­                                                                ysis including fiedler vector fiedler  locally lin­                                                                                                                          ear embeddings lle roweis saul  lapla­      eigenvectors paired eigenvalues through cian eigenmaps belkin niyogi  example di­            embedding vertices      rect solution equation  gives laplacian eigenmap       learning                                                                                                               historical note symmetrized formulation proposed   affine guarantees uniform    fiedler used heuristic graph force eigenvectors model additional variance    layout mohar                          problem                                                                     working backward desiderata leading col­    transformational embeddings                                 umn uniform let                                                                  zk modified representation vertices val­   consider general problem given                                                                  ues reweighted pervertex basis    information vertices matrix  • • •                                                                 clearly uniform column each row       columns generated applying vector    jxn                                                          divided element    valued function —► £  each vertex graph                                                                  follows immediately related eigenproblem    seek linear operator transforms optimal    graph embedding gz —♦ transfor­                                                           mational embedding distinguish direct em­  bedding discussed                                      stochastic  embedding      natural candidate algebraic statement trans­ unwanted translational degree freedom totally re­  formational embedding problem generalized evd           moved note raw stochastic approximations                                                                  orthogonal metric diagonal matrix                    zwztv  zdztva  methods                                                                    noted that—when scaled equal norm   setting  vtz makes equivalent orig­                  — approximations uniformly   inal direct embedding problem equiva­      superior distortion scores monte carlo trials ran­  lent symmetric eigenproblem make cholesky decomposi­         dom graphs wc clear ordering lowest highest   tion rtr  zdzt uppertriangular rdxd let        distortion reweighted affine stochastic raw figure                     zwz eruxd                                bv  va                                                               gives embedding  computational   advantage short matrix origi­  nal eigenproblem reduced small xd     problem matrix multiplications scale dn     sparsity    correcting problematic eigenstructure   generally case —there lin­  ear combination rows giving desired lin­ figure  comparison methods modifying row  ear mapping gz — does exist equations       space graph shows distortion optimal   optimal leastsquares approximation                        embedding averaged  trials node matrices   approximation flaw having random edge weights random rx   eigenvector vi uniform dis­  carded unwanted translational dof worse       raw approximation suboptimal information   eigenvectors variously contaminated un­   ddimensional embedding spread     wanted dof resulting embedding polluted arti­      eigenvectors subset optimal stochas­  facts reason direct solution equation  tic approximation suboptimal—it optimizes differ­  raw approximation                                             ent measure implied equation  practice com­    options remedy limited modify      puting embeddings graphs embedding structure   rowspace reintroduce uniform eigenvector     known priori wc reweighted stochastic   reasons obvious restrict   approximations results clearly similar   operations applied column   superior approximations   knowing column                              need correction stems fact     simplest operation append uniform row    that—the literatures spectral graph theory nldr    makes relation                 notwithstanding—equation  completely correct                                                                  statement embedding problem show         gramlike factorization work example given evd forthcoming paper statement embedding                               cholesky especially attrac­ problem equation  algebraically underconstraincd   tive numerical stability sparsity easy invertibility numerically illconditioned particular point                                                                                                                   learning strictly true stochastic eigenvalue      monotonically decreasing relatively insensitive   paired uniform eigenvector leads patholo•        noise small lead exact re•  gies ruin embedding obtained    constructions data sampled manifolds   basic derived formulations nldr algorithms        flat straightforward calculus shows equation    derived equation  roweis saul       desired minimum  gen•  belkin niyogi  teh roweis  re•     erally multiplicative inverse distance mea•    mediate problem                                           sure appropriate ambient space contrast      forthcoming paper makes analysis is•      lle weightings correlated distances make   sues identifies correct problem statements equa• problem scale invariant scale largest   tions    gives closedform optimal solutions   nonzero offdiagonal value  consequently every•  problems approximation methods discussed sec•      computed   tion useful faster reasonably let situate gaussian kernels px    highquality embeddings nldr method datasets                  manifold paper   considered result reweighted approxima•      random subset data points kernel centers set   tion numerically indistinguishable optimal            kernels radial basis functions let vector   embedding requires substantially calculation   reweighting method justified pade approxi•                                                                                                                          mation optimal solution      nonlinear dimensionality reduction                           kth local homogeneous coordinate xi scaled                                                                  posterior kth kernel optional local   let    set points sampled                    dimensionalityreducing linear projection let representation   lowdimensional manifold embedded highdimensional        vector   ambient space reduceddimension embedding                                                                                                          set lowdimensional   points local neighborhood structure de•      vertical concatenation local coordinate vec•  sire instead mapping general•                     tors collect column vectors basis matrix   ize correspondence continuum rea•  sonable interpolation extrapolation expected    summarize far goal linear   neighborhood data spectral methods nldr typ•       transform basis kernelweighted coor•  ically require solution andor large eigenvalue   dinates maximally consistent local distance   generalized eigenvalue problems kruskal wish     constraints specifically   kambhatla leen  tcnenbaum et al  roweis   saul  belkin niyogi  ex•                                                                                                                          ception teh roweis  brand  offer em  beddings points mappings spaces      show leverage transformational embed•     isomorphic graph embeddings section    ding section  continuous nldr algorithm specifi•   methods developed apply directly con•  cally kernelbased mixture affine maps ambient   tinuous mapping ambient embedding space immedi  space target space show      atly follows continuity smoothness   edge weight matrix vertex matrix specified let                   iff xj satisfy locality criterion                       stated                                                                  evd determines transformation    embedding satisfy                                                                                 continuous kernel representation de•                                                                 fined entire ambient space     larger wy penalize large distances                                                              wij computed  measure similar•  ity graphtheoretic literature usually takes •     nldr methods typically                                 proof consider three pointson              gaussian kernel analogy heat dif•    manifold similarity measure causes   fusion models belkin niyogi  uninforma        distortion global minimum   tive setting wij   usable large loss generality fix global location                                                                  scale embedding fixing endpoints   number points edges connectivity informa•                                                                 solving unique zero distortions derivative   tion suffices determine metric properties em•                                                                 obtain optimum  har•  bedding gaussian setting complementary weak•        monic relation unique continuous satisficing measure   ness sensitive small variations distance              sets    neighbors introduced curvature   simple algebra confirms optimum   data manifold measurement noise ambient space      induction multiple points multiple dimensions direct       learning                                                                                                                  matter numerical prudence recommend using         show kernel eigenmaps computed using    reweighted approximation                                 transformational embedding section  embedding                                                                  methods given inputs                                                                                                figure  shows raw                                                                                                  kernel eigenmap embedding                                                                                                  computed using basis    blush reweighting                                                                                                  matrix created     necessary construction  —                                                                                                  gaussian unita kernels    denominator—should uniform datapoints                                                                                                  placed random points    mentioned algebra predicts                                                                                                  required solving    structure numerical eigensolvers                                                                                                  manageable      obtain approximate inverse mapping map                                                                                                    eigenproblem    means covariances each kernel target                                                                                                   trials performed    space obtain kernels                                                                                                  different sets ran­   breaking                  blocks corresponding each             fc                                                                                domly placed kernels    kernel moorepenrose pseudoinverse each                                                                                                  trials reweighted    set using reweighted map ap­                      figure  kernel eigenmap                                                                                                  stochastic maps gave    proximate inverse map                                      embedding raw result                                                                  raw affine maps exhibited substantial folding                                                                  edges corners embedding                                                                    figure  shows                                                                  reweighted kernel eigen­                                                                 map computed     illustrative example                                        figures                                                                     result smoother                                use variant      actually exhibits                              swiss roll standard test man­  folding original                              ifold nldr community     laplacian eigenmap                              illustrate arguments meth­                                                                   problem reg­                             ods developed paper                                                                  ularized putting positive                              sampled twisted version                                                                  mass diagonal                              manifold regularly                                                                    ww                               grid added small                                                                  making recovered ker­                             gaussian isotropic noise fig­                                                                 nel eigenmap isomet­                             ure  shows ideal  param­                                                                ric figure                               eterization views                                                                  regularization appropri­                             ambient   embedding points                                                                 ate believed                              shown joined line aid                                                                  neighborhoods    figure  swiss roll visual interpretation em                                                                 roughly size                              beddings experiments    section use matrix generated using  near­    recently proposed   est neighbors each point inverse distance function locality preserving projec­                                                                 tion lpp niyogi      laplacian eigenmap                                                                   figure  kernel eigenmap                                                                   essentially raw   embedding figure  shows                                                                  embedding reweighted                                                                  approximation direct solu­  embedding specified                                                                  regularized results                                                                  tion equation    matrix note   exhibits fold­                                                       giving linear pro­  ing corners                                         jection ambient space target space best   edges                                      preserves local relationships   partly problems                                                                      lpp admirably simple   uniform eigenvector                                                                        shown   exacerbated fact                                                                        affine approximation   spectral embeddings                                                                       section    tend compress near                                                                      distortion lpp   boundaries laplacian                                                                      suffer loss uni    eigenmap requires solution figure  laplacian eigenmap        figure  lpp embedding form eigenvector figure    large   eigen embedding                         affine upgrade show embeddings   problem offers mapping points kernel eigen   swiss roll produced lpp affine modification   maps approximations embedding                 equivalent method trivial single uniform                                                                                                                 learning density kernel upgrading lpp affinc projection cap•     gx—there need compute new global embedding   tures datas structure affine revise evd   view manifold avoids folding                     reweighting scheme theoretically mooted                                                                  subsequent discovery better problem formulation    visualizing word usages                                      closedform solution practically viable fast ap•                                                                 proximation large problems postconditioning   statistical analyses natural language similar usage pat•                                                                 step unavoidable numerical error nldr algorithm   terns words taken indicate sim•                                                                 based eigenvalue decompositions   ilar meanings strongly related meanings latent semantic                                                                    paper used random kernels nu•  analysis lsa linear dimensionality reduction term                                                                 merous avenues discovering stronger methods investi•  document cooccurence matrix principal components                                                                  gating placement tuning kernels stability   matrix embedding similarly used words                                                                  embedding topological structure sample com•  similarly located literally colocation proxy col•                                                                 plexity short issues proved fertile ground   location propensity words used                                                                  research classification regression studied   synonymy expect kernel eigenmap offers                                                                  anew context estimating geometry topology   powerful nonlinear analysis                                                                  manifolds      nips  corpus features matrix counting occur•  rences  words  documents mod•             references   eled  words  documents   matrix roughly corresponds years papers       belkin niyogi  mikhail belkin partha   reasonable snapshot everchanging terminology         niyogi laplacian eigenmaps dimensionality re•  field stemmed words combined counts           duction data representation technical report   roots determined distance word         tr university chicago science   roots cosines angles logdomain         transformed occurrence vectors — logl  xij   brand  matthew brand charting manifold proe   matrix generated adding edge each word          nips     closest neighbors cosinespace representation   using  random words kernel centers fig•        chung  fan rk chung spectral graph theory vol•  ure  discusses resulting embedding techni•      ume  cbms regional conference series mathe•  cal terms arc clearly grouped field       matics american mathematical society    common english words arc tightly clustered common se•       fiedler  miroslav fiedler property eigenvectors   mantics lsa dimensions shown fig•         nonncgative symmetric matrices application   ure  data arc reveal significantly semantic    graph theory czech math journal     structure                                                                  niyogi  xiafei partha niyogi local•                                                                    ity preserving projections technical report tr    discussion                                                      university chicago science october    kernel eigenmap generates continuous nonlinear map•        kambhatla leen  kambhatla todd leen   ping functions dimensionality reduction manifold re•      dimensionality reduction local principal component   construction suitable choices kernels reproduce       analysis neural computation     behavior nldr methods ker•                                                                 kniskal wish  kruskal wish mul•  nel local group points perform local dimensional•    tidimensional scaling sage publications beverly hills   ity reduction pc each kernel obtain   ca    equations   nldr algorithm like chart•  ing brand  automatic alignment teh roweis      mohar  mohar laplacian spectrum graphs    demonstrations kernel eigen•      alavi editor graph theorv combinatorics ap•  map simultaneously determine local dimensionality        plications pages  wiley new york    reductions global merger                            roweis saul  sam roweis lawrence     kernel eigenmap typically substitutes small dense         saul nonlinear dimensionality reduction locally lin•  cvd large sparse lvd graph embedding prob•         ear embedding science  december    lems sparse case specialized power method             compute desired eigenvectors significantly                                                                  teh roweis  yee whye teh sam roweis    time required evd kernel setting                                                                    automatic alignment hidden representations proc   similar efficiencies apply typically     nips    sparse allowing fast construction reduced evd prob•                                                                 tenenbaum al  joshua tenenbaum vin   lem zwz amenable fast power methods   course important efficiency kernel method        silva john langford global geomet•  ability embed new points quickly function       ric framework nonlinear dimensionality reduction                                                                     science  december          courtesy roweis available toronto website       learning                                                                                                               
