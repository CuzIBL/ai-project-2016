                       semisupervised gaussian process classiﬁers              vikas sindhwani                       wei chu                     sathiya keerthi    department science                 ccls                        yahoo research         university chicago              columbia university             media studios north        chicago il  usa          new york ny  usa            burbank ca  usa        vikasscsuchicagoedu            chuweigatsbyuclacuk          selvarakyahooinccom                        abstract                            utilize graphbased construction semisupervised                                                        kernels sindhwani et al  data geometry      paper propose graphbased construc modeled graph vertices labeled unla      tion semisupervised gaussian process classi  beled examples edges encode appropriate neigh      ﬁers method based recently proposed    borhood relationships intuitive terms limit inﬁ      techniques incorporating geometric proper nite unlabeled data imagine convergence graph      ties unlabeled data globally deﬁned ker underlying geometric structure probability dis      nel functions machinery standard    tribution generating data smoothness enforced      supervised gaussian process inference brought  regularization operators graph laplacian      bear problem learning labeled   functions vertices graph transferred      unlabeled data approach provides nat producing kernel hilbert space rkhs functions deﬁned      ural probabilistic extension unseen test exam entire data space gives rise new rkhs      ples employ expectation propagation proce    standard supervised kernel methods perform semi      dures evidencebased model selection  supervised inference paper apply similar ideas      presence labeled examples approach gaussian processes aim draw beneﬁts      signiﬁcantly outperform crossvalidation bayesian approach small labeled set avail      techniques present empirical results demon   able semisupervised kernel sindhwani et al       strating strengths approach           motivated through bayesian considerations used                                                        performing gaussian process inference apply expecta                                                        tion propagation ep rasmussen williams     introduction                                       references approximating posterior processes  practitioners machine learning methods frequently en calculating evidence model selection  counter following situation large amounts data point aspects proposed method  cheaply automatically collected subsequent label method abbreviated ssgp  ing requires expensive fallible human participation seen providing bayesian analogue laplacian sup  recently motivated number efforts design semi port vector machines lapsvm laplacian regularized  supervised inference algorithms aim match squares laprls proposed sindhwani et al   formance welltrained supervised methods using small belkin et al  semisupervised logistic regression  pool labeled examples large collection unlabeled proposed krishnapuram et al  paper  data scarcity labeled examples greatly exaggerates empirically demonstrate labeled examples  need incorporate additional sources prior knowledge scarce model selection evidence ssgp signiﬁcant  perform careful model selection deal noise la improvement crossvalidation lapsvm laprls  bels bayesian framework ideally suited handle graphbased bayesian approaches incorporat  issues paper construct semisupervised gaussian ing unlabeled data recently proposed methods  processes demonstrate discuss practical utility designed transductive bayesian learning  number classiﬁcation problems                   ing graphbased prior zhu et al  kapoor et al    methods based geometric intuition  core probabilistic model methods  real world problems unlabeled examples iden deﬁned ﬁnite collection labeled  tify structures data clusters low dimensional man labeled inputs outofsample extension unseen test data  ifolds knowledge potentially improve inference requires additional followup procedures contrast  example expect high correlation class approach possesses gaussian process model entire  labels data points cluster nearby input space provides natural outofsample prediction  manifold respectively cluster manifold focus paper derive ssgp ver  assumptions semisupervised learning             ify usefulness binary classiﬁcation tasks wish                                                    ijcai                                                                                                                                                                           comment potential following respects  ssgp yi fxi   Σllp yl use Σll                                                                                                   provides general framework learning tasks shorthand Σxlxl  normalization factor yl  regression multipleclass classiﬁcation ranking pylflpfldfl known evidence model   ssgp produces class probabilities conﬁdence es parameters parameters covariance function                                                                           timates used design active learning schemes noise level σn   efﬁcient gradientbased strategies designed posterior distribution nongaussian pre  choosing multiple model parameters regularization serve computational tractability family inference tech  kernel parameters                                niques applied approximate posterior gaus    begin brieﬂy reviewing gaussian process classiﬁca sian popular methods include laplace approximation  tion section  propose evaluate ssgp meanﬁeld methods expectation propagation ep  following sections                                   paper use ep procedure based empiri                                                        cal ﬁndings outperforms laplace approximation    background notation                            ep key idea approximate nongaussian                                                        posterior form unnormalized gaussian  standard formulation learning examples pat l                                                             pyifxi  ≈ μ parameters μ  terns drawn space  typically subset                                                      obtained locally minimizing kullbackliebler diver   associated labels come space  ease presentation paper focus bi gence posterior approximation                                                          gaussian approximation posterior hand  nary classiﬁcation taken  −we                                                        distribution latent variable test point tas  assume unknown joint probability distribution                                                                                            given following integral tractable quantity   x×y  space patterns labels set pat                         ≈n                 lu                                            fxt yl     fxt fl fl yldfl       μtσt   terns xi drawn iid marginal              −                    −     − −                                                      μt ΣltΣll  μ σt  kxt xt  Σlt Σll     x×yxy label yi associated xi drawn −     −                                                     Σll   ΣllΣlt    column vector Σlt    conditional distribution xi ﬁrst pat                terns interested leveraging unlabeled patterns kxt xkxt xl Σll gram matrix    lu                                               labeled inputs conditional distribution   xj jl improved inference                                                              −                                                          fxt fl multivariate gaussian mean ΣltΣllfl    set notation discussion ahead denote                         −                                                      covariance matrix kxt xt − Σ Σ Σlt  xl  xii set labeled examples associated                          lt  ll                               lu                      finally compute bernoulli distribution  labels yl  yii xu   xiil set                                                        test label yt probit noise model  labeled patterns denote given patterns                                                                                                       xd xd  xl ∪ xu  patterns held pytylΦμt    σn  σt    details  test purposes collected denoted gaussian processes point reader rasmussen  xt  set patterns labeled unlabeled test williams  references  denoted  xd ∪ xt  use utilize unlabeled data gaussian  denote generic dataset                        process inference    standard gaussian process setting supervised  learning proceeds choosing covariance function  semisupervised gaussian processes    x×x   → point ∈x  sociated latent variable fx given dataset latent  kernels semisupervised learning  variables fx  fxx∈x treated random variables symmetric positive semideﬁnite function k· · serve  zeromean gaussian process indexed data points covariance function gaussian process  covariance fx fz fully determined kernel function deterministic reproducing kernel hilbert  ordinates data points given kx space rkhs functions x→ rkhs  prior distribution variables multi gp associated function k· · closely  variate gaussian written pfx  Σxx lated through classical isometry hilbert  Σxx  covariance matrix elements kx space random variables spanned gp random  ∈                                                        variables form αkfxk mean square lim    gaussian process classiﬁcation model relates vari section review construction rkhs  able fx label through probit noise model adapted semisupervised learning deterministic                                  ∼n          signfx  ξ ξ     σngiven     classiﬁers sindhwani et al  kernel rkhs  latent variables fl associated labeled data points used covariance function ssgp  class labels yl independent bernoulli variables context learning deterministic classiﬁers                                         joint likelihood  given yl fl yi fxi  choices kernels norm  interpreted  l                xi                                         smoothness measure functions norm    Φ  σ    Φ cumulative density function                                                      used impose complexity structure  normal distribution    combining likelihood term gaussian prior rkhs function notation fx means function  latent variables associated labeled dataset evaluated gp fx refers latent random  xl obtain posterior distribution pflyl   variable associated                                                    ijcai                                                    learning algorithms developed based minimizing instantiation interpreted realization certain geom                                           functionals form fxlylγ hwherev  etry conditioning geometry unlabeled data  loss function measures ﬁts data through variables bayesian update gives posterior  markably loss functions involve through point latent variables ssgp resulting posterior pro  evaluations representer theorem states minimizer cess incorporates localized spatial knowledge                   l  form fx  αikx xi αi data bayesian learning  main computed provides algorithmic basis ways deﬁne appropriate likelihood  algorithms like regularized squares rls support evaluation geometry variables simple formu  vector machines svm squared loss hinge loss lation pgfd given  spectively semisupervised setting unlabeled data                                                                                                                         suggest alternate measures complexity smoothness   pgfd     exp  −   pd mpd             respect data manifolds clusters space                        contains functions smooth respects                                                                                       pd     fx    fxlu      required restructure reﬁning norm using                    labeled labeled data xd general procedure Φfx Φfxlu  column vector conditional la  form operation follows deﬁne h˜ space bel probabilities latent variables associated xd  functions modiﬁed datadependent inner graphbased matrix graph laplacian sec                                                      tion  normalization factor pgfd  product  fg h˜   fg  mgwheref                                                          interpreted measure fd corroborates  vectors fxx∈xd gxx∈xd respectively   symmetric positive semideﬁnite matrix norm given geometry computed terms smoothness  induced modiﬁed inner product combines origi associated conditional distributions respect unla  nal ambient smoothness intrinsic smoothness mea beled data implements speciﬁc assumption  sure deﬁned terms matrix deﬁnition connection true underlying unknown conditional                                                        marginal distributions – points ∈xare  based construction data adjacency graph                          acts empirical substitute intrinsic geometry close intrinsic geometry  conditional                                                        distributions pyx pyx similar func  marginal px  derived from graph lapla                                                      tion pyx smooth respect px   cian example      βpl pop  ular choices families graph regularizers lapla likelihood form eqn  leads nongaussian  cian matrix graph implements empirical version posterior natural substitute use unnormalized  laplacebeltrami operator underlying space gaussian form commonly used ep approximations                                                        posterior approximated  riemannian manifold operator measures smoothness                                                                  h˜  respect manifold space shown                                                                 h˜               fdg ≈ exp −  fd  fd  pfdp     rkhs new datadependent norm                         better suited compared original function space  semisupervised learning tasks clustermanifold form captures functional dependency  assumptions hold form new kernel k˜ associated latent geometry variables rendering subsequent  h˜ derived sindhwani et al interms computations tractable value inconsequential  kernel function using reproducing properties deﬁning approximate posterior distribution                                                                                     rkhs orthogonality arguments given    cels normalizing term   impor                                                        tant evidence computations section      ˜               −               −    kx zkx    Σdxi   mΣdd    mΣdz        comment role propose use partial  Σdx similarly Σdz denotes column vector evidence model selection matrix approximated                          kx xkxlu  laplacian svm lapla  laplacian matrix dataadjacency graph cor                                                        respond deterministic algorithms introduced sind  cian rls solve regularization problems rkhs h˜                                                        hwani et al  belkin et al  note  hinge squared loss respectively                                                        alternatively computed ep calculations leading    datadependent conditional prior                 novel graph regularizer tractable computations                                                        evidence outline details direction  ssgp uses semisupervised kernel function k˜ deﬁned                                                        chu et al   covariance function gaussian process learn                                                          proceed make assumption given fd  ing ssgp covariance fxi fxj                                                        independent latent variables points  depends ambient coordinates xi xj                                                        data set contains xd set unseen test points xt   geometric properties set xd section                                                         discuss derivation k˜ bayesian considerations                                                                         pgfx pgfd                   general approach summarized follows  gin standard gaussian process given unlabeled assumption allows sample extension  labeled data xd deﬁne joint probability distribution need recompute graph new dataset  associated latent process variables fd andanab posterior distribution fx given pfx ∝  stract collection random variables denoted pgfx pfx pgfdpfx                                                     ijcai                                                      prior distribution fx gaussian  Σxx table  datasets features training examples la  given standard gaussian processes form beledunlabeled test examples  block matrices   prior p fx    written   fol                                                          dataset    dn                    domain                fd                  Σdd    Σdt  lows fx           ∼n                              moons                    synthetic                                                   Σdt   Σtt            vs                   usps digit  whereweusetheshorthandd      xd  xt        set              image recognition  posterior distribution fx conditioned                                              ∝       pcmac               newgroups  written zeromean gaussian distribution fx       reuters               reuters         ˜ −  exp−  fx Σxx fx                                −                       kernel eqn  needs recomputed multiple times          −      Σdd    Σdt                     Σ˜ xx                                      crossvalidation based counts                          tt                             Σdt    Σ                              able uniquely identify good set parameters    proposition given eqn  ﬁnite collection trast evidence maximization needs suppress expen  data points random variables fx  fxx∈x    sive labels needs hold labeled examples  conditioned multivariate normal distribution graph continuous function model parameters   Σ˜ xxwhereΣ˜ xx covariance matrix el precise parameter selection demonstrated sec  ements given evaluating following kernel function tion  amenable gradientbased techniques  k˜  x×x→                                          addition beneﬁts note novel possibility em                                     −               ploying unlabeled data model selection maximizing    k˜ zkx − Σdxi  mΣdd    mΣdz                                                             evidence pyl gφpylgφ pgφ  ∈xhereΣdx        denotes column vector                                                                                        term computed   φ         df   kx xkxn                              given eqn  immediately derive log pgφ                                                                      proposition shows gaussian process condi log −  log detmΣdd  identity  tioned geometry variable gaussian process matrix simplicity paper focus compar  modiﬁed covariance function k˜  proof result uses ing standard evidence maximization ssgp standard  straightforward matrix algebra omitted brevity crossvalidation laplacian svmrls    ssgp posterior process obtained conditioning  original gp respect note form  experiments                 ˜  variance function eqn  derived performed experiments synthetic twodimensional  properties rkhs                              dataset real world datasets binary classiﬁcation    model selection                                  problems statistics datasets described                                                        table   model selection ssgp involves choosing kernel pa  rameters noise variance σn section  def synthetic data  inition k˜ based choice covariance function  graph regularization matrix masinsindhwani  toy dataset moons meant visually demonstrate                                                        ssgp shown figure  collection unlabeled  et al  paper restrict attention fol                                                       examples small black dots labeled examples                                x−z           γi                              −       lowing choices kx zexp     σ        γa    class large colored points classiﬁcation boundary                                       ˜                              contours mean predictive distribution  use kernel γa parameters γ γ balance  ambient intrinsic covariance parameters related unit square shown standard gp ssgp fig  computation graph laplacian – number ure ac plots demonstrate ssgp utilizes unla                                                        beled data classiﬁcation tasks uncertainty predic  nearest neighborsnn the graph adjacency matrix                                                  xi−xj                            tion figure respects geometry data increas  set wij exp   −        ifxi xj adjacent                        σt                             ing anisotropically moves away labeled ex  nnnearest neighbor graph zero    amples figure demonstrates evidencebased model    denote φ collection model parameters opti selection  mal values φ determined maximizing evidence  pylgφ available ep computations    real world data sets    model selection particularly challenging semi real world data sets vs generated  supervised settings presence labeled examples ﬁxed trainingtest split randomly drawing onefourth  popular model selection techniques like crossvalidation original data set test set unseen examples  deterministic methods like laplacian svmrls require sub retaining rest training set labeled unlabeled  sets alreadyscarce labeled data held examples learning curves plotted varying  training graphbased semisupervised learn labeled data randomly chosen training set  ing proper crossvalidation exclude held vs chose exact data splits used lawrence  subset adjacency graph instead simply sup jordan  facilitate comparison algorithms  pressing labels order ensure good outofsample exten reduce complexity model selection ﬁx nn   sion adds major expense semisupervised  vs setandnn                                                     ijcai                                                      mean predictive distribution supervised learning logrithm evidence semi−supervised learning vs set                                                                                                                                                       ssgp                                                                               ssgp                                              −                                                                                                            gp                                                                      gp                                                    −                                                                                                                                         σ                                                                                                                                                                                       −                    −log                                                           error  rate test set                −                                                                   error  rate test set           −              −       −     −                                                                             − −                                    −                                                                                                                                                                                                                                                                                                                                 prob label present  −                                                                                         prob label present                                              −                                        −                             −log σ                           pcmac                      reuters  − − −                                                                        mean predictive distribution semi−supervised learning entropy predictive distribution semi−supervised learning                  −                                                                    −                                                                                                                                                                                                                                                                                                  −                                                −                                                                                                                                                                                                                                                                                           error  rate test set  error  rate test set                              −                                                                                     −                                                                                                     − −                                                         −                    −                                                                         −                                                                          prob label present    prob label present                                                                         −  −                                                                                                         figure  comparison ssgp gp test data                         −  −  − − −       − −                                                              datasets pcmac gap converge  figure  moons mean predictive distribution faster performance based parameters chosen ev  supervised gp shown graph based maximum idence based model selection closer optimal  evidence best settings log σt  −          −                                             performance ssgp gp performance curves  log σ   shown cross graph results ssgp lesser variance  ssgp best settings shown graph                                                                       vs                      set                                                                                                                                                                                                                                        ssgp     pcmac reuters values based experimen                       ssgp                                                                         gp                                                                             gp  tal experience sindhwani et al  image                                                                                                                                                                        text data sets used weighted graphs euclidean dis                                                                                       tance graph similarity measure gaussian weights                                                           error  rate unlabeled set                                                                                     error  rate unlabeled set  width σt set mean distance adjacent                                                                                                                                                                                                                                                            prob label present  vertices behavior evidencebased model selection                               prob label present    ssgp crossvalidation cv based model selection          pcmac                      reuters                                                                                        laplacian svm investigated range values                                                                                          parameters σ γaγi  each dataset computed                                                                                   mean length feature vectors σ training set                                              σ σ                                                                                              probed σ range    σ σ σ   range                                      −   −   −                           γi      error  rate test set   error  rate test set                                                                                    γ           choices ratio γa                                                                                                                                                                      note choice  ignored      prob label present    prob label present  labeled data reduces algorithm standard super                                                           figure  ssgp vs gp unlab data transduction  vised mode noise parameter σn probit model                      −  class labels set  experiments note  evidence maximization versus cv  parameter allows ssgp potentially deal label noise compare evidencebased model selection ssgp cv     benefit unlabeled data figure  plot  based model selection laplacian svm figure   mean error rates test sets datasets plot mean error rates test sets datasets  function number labeled examples expressed function number labeled examples ssgp  terms probability ﬁnding labeled example train laplacian svm utilize unlabeled data  ing set ssgp standard supervised gp ignores through kernel figure  shows correspond  unlabeled data figure  shows corresponding curves ing curves performance unlabeled data solid  performance unlabeled data solid curves show curves show mean standard deviation mini  mean standard deviation minimum error rates mum error rates ssgp laplacian svm set  ssgp gp set parameters σ γaγi γi  parameters σ γaγi  dashed curves show corre  gp dashed curves show mean standard sponding curves parameter setting chosen evidence  deviation error rates parameters chosen maximiz maximization cv ssgp laplacian svm respec  ing evidence mean standard deviations com tively mean standard deviations computed  puted  random draws choice  random draws choice labeled  labeled data make observations utiliz data used fold cv each labeled set  ing unlabeled data ssgp makes signiﬁcant performance im  fewer examples case leaveoneout cross  provements standard gp test unlabeled validation used important note cv pro  sets gap larger unlabeled set holds true tocol laplacian svms suppresses labels does  entire span varying amounts labeled data exclude labeled examples graph make                                                    ijcai                                                    
