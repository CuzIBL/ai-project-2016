           simple training dependency parsers structured boosting               qin iris wang                      dekang lin                     dale schuurmans   department computing science              google            department computing science          university alberta           lindekgooglecom                  university alberta      wqincsualbertaca                                                 dalecsualbertaca                          abstract                          shown application areas structured                                                        prediction models directly capture relationships      recently signiﬁcant progress                                                        tween output components perform better models      learning structured predictors coordinated train                                                        directly enforce relationships lafferty et al       ing algorithms conditional random ﬁelds                                                        tsochantaridis et al  altun et al  taskar et al      maximum margin markov networks unfor                                                          particular ideas started      tunately techniques based specialized                                                        signiﬁcant impact ﬁeld natural language pars      training algorithms complex implement                                                        ing mcdonald et al  mcdonald pereira       expensive run present simpler ap                                                        corstonoliver et al  state art results      proach training structured predictors apply                                                        recently achieved through use structured      ing boostinglike procedure standard super                                                        training algorithms parsing largescale structured pre      vised training methods idea learn lo                                                        diction problem multiple predictions coordi      cal predictor using standard methods lo                                                        nated achieve accurate parse given input sentence      gistic regression support vector machines                                                        parsing particularly challenging important task      achieve improved structured classiﬁcation                                                        machine learning involves complex outputs parse      ”boosting” inﬂuence misclassiﬁed compo                                                        trees limited training data manually parsed treebanks      nents structured prediction retraining lo                                                        machine learning methods proved provide      cal predictor repeating improvement                                                        best approach obtaining robust parsers real data      structured prediction accuracy achieved      incorporating ”dynamic” features—ie ex     drawback current structured prediction training      tension features predicted    algorithms involve new specialized      component depend predictions   parameter optimization algorithms complex non      components                   trivial implement usually require far computa                                                        tion standard classiﬁcation learning methods lafferty et      apply techniques problem learn                                                        al  taskar et al  main reason increased      ing dependency parsers annotated natural lan                                                        complexity fact form structured prediction      guage corpora using logistic regression                                                        algorithm parser viterbi decoder      efﬁcient base classiﬁer predicting dependency                                                        sidered underlying training principle causes      links word pairs able efﬁciently                                                        structured inference output predictions tightly cou      train dependency parsing model structured                                                        pled parameter optimization process during training      boosting achieves state art results en      glish surpasses state art chinese   paper demonstrate somewhat surprising                                                        sult state art performance natural language pars                                                        ing achieved through use conventional local    introduction                                       classiﬁcation methods particular show simple  recently signiﬁcant progress form structured boosting used improve train  developing training algorithms learning structured predic ing standard local classiﬁcation methods structured  tors data tsochantaridis et al  altun et al  case modifying underlying training method  taskar et al  structured prediction learning extends advantage approach use offtheshelf  standard supervised learning framework uni classiﬁcation techniques support vector machines  variate setting single output variable considered logistic regression achieve competitive structured pre  multivariate setting complex nonscalar pre diction results little additional effort achieve  dictions yˆ produced inputs challenge through use simple ideas introduce  each component yˆi yˆ depend simple form “structured boosting” structured  input instead account correla predictor parser used modify predictions  tions yˆi neighboring components yˆj ∈ yˆ local weak learning algorithm inﬂuences                                                    ijcai                                                                                                                                                                                           vp                                                                               investors continue pour cash money funds                                                                           vp               figure  dependency tree                                     vp                                                              np  example weightings subsequent hypotheses implic                        np    pp  itly encouraging accurate structured predictions sec                                   np  ond exploit old idea natural language parsing  highlight generally “dynamic” features nns   vbp   vb  nnin     nn    nns  used local classiﬁers account investors continue pour cash money funds  previous classiﬁcations restricted set neighboring     figure  constituency tree  examples    demonstrate ideas concretely problem cherry lin  ding palmer  coreference  learning natural language dependency parsers labeled resolution bergsma lin  information extrac  treebank data dependency parsing com tion culotta sorensen   plex problem able achieve state art results consider problem automatically learning  training local “link predictor” merely attempts pendency parser given language treebank col  predict existence orientation link lection manually parsed sentences treebanks usually  words given input features encoding context—without wor sist constituency parses dependency parse  rying coordinating predictions coherent global automatically extracted constituency parse  parse instead wrapper approach based structured simple rules bikel  training data consists  boosting used successively modify training data set sentences each annotated directed spanning  training algorithm implicitly encouraged facili tree words figure  languages like english  tate improved global parsing accuracy                chinese tree planar called “projective”    remainder paper organized follows dependency tree complex object parsing  section  brieﬂy dependency parsing prob conceptually reduced set local classiﬁcations  lem introduce notation terminology each word pair sentence classiﬁed three  explain relationship structured prediction categories link left link right link aspect  learning traditional classiﬁcation learning section  problem local given pair words given sentence  point opportunities connecting problems context link label aspect  section  brieﬂy explain general terms problem global local link predic  idea “dynamic” features classiﬁcation learning tions coordinated globally produce consistent ac  used improve structured prediction curate dependency tree aspects problem—local  section  introduce main technique proposed link prediction versus global link coordination—are crucial  paper structured boosting explain relation standard achieving state art dependency parser locally  boosting approaches finally sections   problem amenable standard classiﬁcation learning ap  scribe experiments learning dependency parsers proaches globally problem requires coordination—that  treebank data show competitive results ob interaction parsing algorithm  tained through use standard learning methods fact  results surpass state art accuracy chinese pars  local coordinated training  ing competitive state art english sec  tion  concludes paper discussion future work problem learning structured predictor generally                                                        formulated follows given set annotated                                                        objects  st  tt  each object si consists    dependency parsing                                 composite observation sentence consisting  main application involves learning dependency word string each annotation ti complete labeling  parsers labeled data brieﬂy introduce problem relevant subcomponents si link label  issues creates natural language pars pair words sentence typically single composite  ing dependency tree speciﬁes words sentence example si ti broken coordinated                                                                                    directly related dependency structure set local examples siti siti  natural language sentence directed tree nodes each label tij ∈ ti atomic classiﬁcation sub  words sentence links represent direct component sij ∈ si taken context si ti example  dependency relationships words figure  asentencesi  wiwin dependency tree labeling  generally speaking dependency tree easier ti  titin−n decomposed local examples                                                                           derstand annotate constituency trees involv siti in− in−n siti  ing speech phrase labels figure  consisting arbitrary necessarily adjacent word pairs  sequently growing dependency link label left right context si ti  parsing recent years dependency relations context important accurately predicting  playing important roles machine translation fox  component label locally requires consideration                                                    ijcai                                                    just subcomponent word pair hind training algorithms explicitly incor  surrounding components possibly labels porate effects structured predictor directly  surrounding components discuss training algorithm parameter optimization lo  point                   cal predictor performed directly considering im    decomposition facilitates purely local approach plied effects structured global local pre  learning problem given original compos   diction error extension structured training loss  ite data  st  tt  sentences developed large margin training princi  parses ﬁrst break data local exam ple support vector machines tsochantaridis et al                           ples      st  ij ik ijk siti  altun et al  taskar et al  maximum  ignore relationships examples use stan ditional likelihood principle logistic regression lafferty et  dard supervised learning algorithm learn local predictor al  subsequently training algorithms based  wij wik → tijk context si ti example principles applied parsing taskar et al   strict attention linear predictors support vector machines wang et al  recently resulted state art  logistic regression models need learn weight accuracy english dependency parsing mcdonald et al  vector θ set features deﬁned local examples  mcdonald pereira  unfortunately main  fwij wiktijk si ti each feature fm computes drawback structured training techniques  value based component wij wik label tijkin specialized nontrivial implement require  context si ti case multiclass support vector great deal reﬁnement computational resources ap  machine crammer singer  logistic regression ply signiﬁcant task like parsing mcdonald et al   model hastie et al  trained conventional corstonoliver et al   manner achieve accurate local prediction model   paper pursue simpler general approach    question remains perform valid applied local prediction learning strategy  structured prediction composite test objects using lo requiring underlying training algorithm  cal prediction model problem structured clas modiﬁed ensuring training outcome di  siﬁcation requires constraints respected rectly inﬂuenced resulting accuracy global struc  classiﬁcations different local components example tured predictor parser  dependency parsing predicted word pair labels  link left link right link form valid directed span  dynamic features  ning tree case languages like english  chinese planar tree form describing general structured boosting method  global consistency achieved practice simply combin ﬁrst brieﬂy simple useful idea improving  ing local link classiﬁer parsing algorithm structured classiﬁcation accuracy local predictors  pendency parsing algorithm effect dynamic program idea use socalled “dynamic” features features  ming algorithm goal producing maximum account labels surrounding  weight spanning tree subject constraints eisner  components predicting label target component  output local predictor used numerical particular predicting label target component  weight score potential link context parser sij ∈ si composite object si assume  use make decisions link labels labels components say sij− ∈ sihave  choose sense role local predictor just computed constraint neigh  supply learned scoring function preexisting parsing boring labels used available attempting  algorithm exactly approach combining local link pre label sij  supervised training problem  dictors dependency parsing algorithms tried labels available component  success researchers—using support vec real issue arises during testing—that during  tor machines yamada matsumoto  logistic regres structured classiﬁcation test object labels  sion aka maximum entropy models ratnaparkhi  each component determined systematic  charniak  generative probability models collins order ensures required features available   wang et al —to learn local scoring functions component needs labeled    unfortunately simple local learning strategies easiest way illustrate concept sequential  obvious shortcoming problem training labeling task like speech tagging given sentence  loss minimized during local parameter optimization  wwn goal predict corresponding tag se  directly parser true quence  ttnherethepreceding tags used  accurate local predictor prerequisite ac features current word consideration—eg  curate parse prediction parameters local model maximum entropy markov model memm mccallum et  trained directly optimize global accu al —while permitting efﬁcient viterbi decod  racy parser far better choice parame ing algorithm used structured prediction alterna  ters exist given space deﬁned fea tively use following tags features use  tures leads better global parsing accuracy preceding following tags toutanova et al   advent recent training algorithms learning idea dynamic features general  structured predictors helpful main idea sequence labeling maximum conditional likelihood train                                                    ijcai                                                    ing                                                    • global structured predictor used classify    dependency parsing dynamic features eas data using current weak local predictor ex  ily employed example considering possible link ample parsing algorithm used repredict train  label connects head word subordinate word ing labels coordinated global fashion using  access standard parsing algorithm learned link predictor internal scoring function  existing children head occur • based resulting misclassiﬁcations struc  words consideration case number tured predictor parser output ensemble  types preexisting subordinate children valid features weight current weak local predictor calculated  used predict new headsubordinate local example weights updated according  link occur turns informa  standard boosting method example ex  tive feature link prediction parsing collins  ponential loss adaboost freund schapire   wang et al                                        schapire singer  logistic regression loss    idea using dynamic features new boosting collins et al                                                   ﬁelds like parsing collins  magerman  •  widely appreciated dy steps repeated number  namic features trivial way improve structured boosting rounds  predictors requiring modiﬁcation resulting ensemble weak local predictors provides  lying training methods fact possibility combined local predictor used subsequent  used parsing mcdonald et al  global structured prediction composite test examples  yield immediate improvements subsequently reintro advantage approach simplicity gener  duced mcdonald pereira belowweﬁndthat     ality applied standard local training method  simple dynamic features easily improve structured prediction requiring modiﬁcation underlying algo  performance                                          rithm structured boosting local learning algo                                                        rithm forced respond behavior global pre    structured boosting                                dictor effect simple training wrapper local                                                        examples reweighted based predictions  dynamic features signiﬁcantly improve current hypothesis instead predictions lo  structured prediction performance local training algo cal hypothesis forces global structured predictor make  rithms local training myopic parameter opti ﬁnd structured boosting method form  mization process remains uninﬂuenced global behav improve quality dependency parsers learned  ior structured predictor fact dynamic treebank data note boosting rounds  features capture weak aspects global prediction feasible application each round requires  expect proper structured training algorithms yield sig entire corpus reparsed local prediction model  niﬁcant global accuracy improvements structured prob retrained witness useful im  lems like parsing example simpler sequence labeling provements achieve state art results  tasks observed structured train  ing algorithms like conditional random ﬁelds lafferty et  experimental design dependency parsing  al  outperform myopic counterparts maximum  entropy models mccallum et al  memm  applied ideas learning structured predictors  training fails consider global prediction accuracy laf challenging problem learning dependency parser  ferty et al  attempting incorporate treebank data particular considered lan  global predictor directly local parameter optimiza guages english chinese  tion inevitably led design new complex training data sets used english penn treebank  mar  algorithms require standard local training methods cus et al  chinese treebanks  palmer et al  replaced problem seek avoid complicate   palmer et al  experiments  training process manner                      english converted constituency structures depen                                                                                         introduce main proposal structured boost dency trees using rules yamada matsumoto                                                         adopted standard trainingdevelopmenttest  ing provides straightforward way combine global                                                        split used literature development  structured prediction parsing local parameter opti                                                                                                       mization modifying underlying local training test sets tagged speech tagger ratna                                                        parkhi  chinese used rules bikel   algorithm fact procedure trivial variant                                                        conversion created data split wang et al  standard boosting algorithms freund schapire                                                               schapire singer  collins et al  altered  chinese treebank  data set data                                                        split corstonoliver et al  chinese treebank  incorporate structured prediction algorithm during clas                                                         data set chinese treebank  contains chinese tree  siﬁcation phase procedure follows                                                        bank  subset adds approximately  sentences    • standard predictor trained local labeled  words taiwanese chinese text      components discussed sec  produce “weak” static features english chinese used      local predictor local link predictor parsing common set feature templates particular static                                                    ijcai                                                    features used set features described mc    table  boosting static features  donald et al  “in pos features”                                                                      english        chinese treebank   given target word pair context static fea iter  tures consisted indicators individual words da  ra     cm     da     ra      cm  speech tags speech tags words          surrounding context addition indicator features used           mcdonald et al  added distance feature               simply measures far apart words               sentence highly predictive link existence  links dependency parse short range    dynamic features  dynamic features used each misparsed local example simply increased  number previous children candidate head word additive constant weights kept  indicator speech previous child hypothesis kept fact experiments  word side candidate head word en obtain state art results just using simpliﬁed pro  glish used special dynamic feature try capture cedure focus initial results compar  prepositional phrase attachment preference candidate isons standard boosting algorithms adaboost  child tagged pp use feature indicates freund schapire  logistic regression  tag word ﬁrst grandchild ﬁrst child child form boosting described collins et al     local training local training algorithm used progress  standard logistic regression model aka maximum entropy  model hastie et al  attempt learn predictor  three word pair labels link left link right link  results  given features described regularize parame determine effectiveness generality approach  ters model used linear nonnegative regularizer conducted number experiments each data  described goodman  regularization param sets english chinese results achieved  eter λ set  experiments parameter ing simpliﬁed boosting procedure mentioned  selected tuning english development additive weight updates keeping hypothesis  set used modiﬁcation data sets determine effectiveness basic structured  unfortunately number features number local boosting idea started simple local prediction model  examples large training logistic regres static features measured parsing accuracy  sion model took day accelerate held test set function number boosting  training process employed trick parti rounds table  shows parsing accuracy improved  tioned set local examples determined word pairs each round boosting static features english  each sentence according speech tags chinese using chinese treebank  explain  pair each equivalence class number features improvements carefully note da dependency ac  reduced dropping features curacy indicates word pairs correctly linked  constant class partitioning dropped ra root accuracy measures sentence roots  overall training cost hours computers correctly identiﬁed cm complete match number  separate partitions trained parallel sentences entire tree correct focus  interestingly quality learned model signif work improving dependency accuracy scores da  icantly affected training procedure suggests root accuracy complete match scores  speech tags word pair used fact reﬂected boosting procedure instance  create partitions essential piece informa reweighting based each candidate link  tion deciding existence orientation dependency predicted correctly root labeled correctly  link                                                 complete sentence matched correctly    parser dependency parsing algorithms   surprisingly table  shows dependency accu  available differing computational cost range racy da improves each round boosting english  eisner’s time parser length sen improves rounds improves overall chi  tence time cky parser mcdonald et al  nese ra cm results ﬂuctuate somewhat note  difference algorithms improvements appear small observed  global constraints enforce types features da differences statistically signiﬁcant english  use during dynamic programming faster algorithms test corpus consists  instances word pairs oc  enforce fewer global constraints need use curring sentence differences   stricted class features experiments used stan centages shown tables statistically signiﬁcant  dard cky parser jurafsky martin  allowed greater  conﬁdence chinese test corpus  use features described en consists  instances differences   forcing planarity constraint                     percentages shown tables statistically signiﬁcant    boosting method experimented boosting greater  conﬁdence  methods including simpliﬁed variant weights second determine effectiveness dynamic fea                                                    ijcai                                                    
