             revisiting output coding sequential supervised learning                                       guohua hao      alan fern                          school electrical engineering science                                          oregon state university                                         corvallis  usa                                    haog aferneecsoregonstateedu                      abstract                          moore  approaches demon                                                        strated good performance number sequencelabeling      markov models commonly used joint     problems satisfy assumptions models      ference label sequences unfortunately infer      ence scales quadratically number labels  recent approach focus paper      problematic training methods   sequential errorcorrecting output coding secoc cohn et      inference repeatedly preformed pri al  motivated success errorcorrecting output      mary computational bottleneck large label sets coding ecoc nonsequential problems dietterich      recent work used output coding address bakiri  idea use output code “reduce”      issue converting problem labels multilabel sequencelearning problem set binarylabel      set problems binary labels models problems secoc solves each binary problem independently      independently trained each binary problem combines learned models make predictions      reduced computational cost com  original large label set computational cost      bined joint inference original labels learning inference scales linearly number binary      revisit idea show through exper models independent number labels experi      iments synthetic benchmark data sets  ments data sets showed secoc signiﬁcantly      approach perform poorly critical reduced training labeling time little loss accuracy      explicitly capture markovian transition struc initial secoc results encouraging      ture largelabel problem study did address secoc’s general applicability      simple cascadetraining approach show potential limitations nonsequential learning ecoc      improve performance problems     shown formal reduction multilabel bi      negligible computational overhead                nary label classiﬁcation langford beygelzimer                                                         contribution paper show result does    introduction                                       hold sequential learning sequence                                                        labeling problems output code secoc  sequence labeling problem assigning class label forms poorly compared directly learning large label  each element input sequence study supervised set assuming optimal learning binary problems  learning sequencelabeling problems number  labels large markov models widely used se given theoretical limitations secoc prior  quence labeling time complexity standard empirical success main goals paper  bet  inference algorithms viterbi forwardbackward ter understand types problems secoc  scales quadratically number labels suited  suggest simple approach overcoming  number large problematic predictions limitations secoc evaluate performance  quickly problematic fact present experiments synthetic benchmark data sets  number recent approaches training markov mod results show originally introduced secoc  els lafferty et al  tsochantaridis et al  forms poorly problems markovian transition  collins  repeatedly perform inference during learning structure important transition information  number labels grows approaches quickly captured input features results suggest  come computationally impractical                     critical explicitly capture markovian transi    problem led recent work considers various tion structure secoc good choice  approaches reducing computation maintaining high response introduce simple extension secoc  accuracy approach integrate approximate called cascaded secoc predictions previously  inference techniques beam search learning learned binary models used features later mod  process collins roark  pal et al  sec els cascading allows richer transition structure  ond approach consider learning subclasses encoded learned model little computational  markov models facilitate subquadratic inference—eg head results show cascading signiﬁcantly im  using linear embedding models felzenszwalb et al  prove secoc problems capturing markov  bounding number distinct transition costs siddiqi ian structure critical                                                    ijcai                                                      conditional random fields                            idea recently applied sequence labeling  study sequencelabeling problem goal problems sequential errorcorrecting output  learn classiﬁer given observation se coding secoc cohn et al  motivation  quence xxt  returns label sequence  reducing computation time large label sets secoc  yyyt  assigns correct class label each instead training multiclass crf train binaryclass  observation work use conditional random crf hk each column bk code matrix specif  ﬁelds crfs sequence labeling sequential crf ically training data crf hk given xibkyi  markov random ﬁeld geman geman  bky bkybkybkyt  binary la  label sequence globally conditioned observation se bel sequence given set binary classiﬁers code ma  quence hammersleyclifford theorem states                                                     trix observation sequence compute multi  conditional distribution  following form label output sequence follows ﬁrst use forward                                                                                   backward algorithm each hk compute probabil               exp    Ψyt−yt                    zx                                ity each sequence position labeled                                                                                      each sequence element xt form probability vector                                                            zx normalization constant Ψyt−yt tp  pn pk probability computed                                                                                       potential function deﬁned clique yt−ytfor hk let label class label yt  simplicity consider pairwise cliques yt−yt codeword closest hxt based distance  chainstructured crfs assume potential complexity inference process just ·   function does depend position tasinlafferty et codeword length typically  al  potential function usually represented smaller number labels squared secoc  linear combination binary features                signiﬁcantly speed inference time large label sets                                                                                  Ψ                                        prior work cohn et al  demonstrated     yt−yt        λαfα yt       γβgβ ytyt−      problems secoc signiﬁcantly speed training time                  α               β                     signiﬁcantly hurting accuracy work  functions capture relationship label yt did address potential limitations secoc ap  input sequence boolean functions capture proach key characteristic secoc each binary  transitions yt− yt conditioned crf trained completely independently    crf training algorithms proposed each binary crf sees coarse view multi  training parameters λ γ class label sequences intuitively appears difﬁcult repre  methods involve repeated computation partition sent complex multiclass transition models yt−  function zx andor maximizing label sequences yt using independent chains raises fundamen  usually using forwardbackward  tal question representational capacity secoc  viterbi algorithms time complexity algorithms model following counter example gives partial answer  ot ·lkwherel number class labels question showing secoc model unable  order markov model sequence length represent relatively simple multilabel transition models  ﬁrst order crfs training inference scale quadratically secoc counter example consider simple markov                                                        model three states     deterministic tran  number class labels computationally  →      →         →  demanding large label sets use sitions       diagonal code                                                        matrix sufﬁcient capturing nontrivial codewords  output coding combating computational burden label set—i nontrivial binary partitions ybe                                                        low show example label sequence correspond    ecoc sequence labeling                         ing binary code sequences                                                                                nonsequential supervised classiﬁcation idea               errorcorrecting output coding ecoc success                                                                              fully used solve multiclass problems dietterich            bakiri  multiclass learning problem train  ing examples xiyi reduced number binary given label sequence        aﬁrst  class learning problems assigning each class label abi order markov model learned  converge  nary vector code word cj length code matrix yt yt− yt yt−    built taking code words rows each column shown sequence length grows model  bk regarded binary partition origi make iid predictions according stationary distribution  nal label set bky ∈  learn binary predicts  probability  true  classiﬁer hk using training examples xibkyitopre iid predictions binary crfs  dict label new instance concatenate pre independent using models make predictions  dictions each binary classiﬁer vector hx secoc yield substantial error rate  hxhxhnx predicted label yˆ given sequence perfectly predictable independent ﬁrstorder bi  yˆ argminj Δhxcj  whereΔ  distance  nary transition models simply unable capture tran  measure hamming distance implementa sition structure problem experimental results  tions hx stores probabilities binary labels show deﬁciency exhibited real data                                                    ijcai                                                      cascaded secoc training crfs                    transition model model allows assess accuracy  each binary crf secoc limited knowledge   attained looking window observa  markovian transition structure possibility tion features second denote “isecoc” use  improve situation provide limited coupling isecoc train ﬁrstorder binary crfs denote  binary crfs way include observation “csecoch” use csecoc train ﬁrstorder bi  features each binary crf record binary predictions nary crfs using cascade history length  previous crfs approach cascaded secoc  summary results results presented justify  csecoc opposed independent secoc isecoc ﬁve main claims  isecoc fail capture signiﬁcant                                                transition structures leading poor accuracy obser    given training examples   iyi letyi   vations original evaluation isecoc  prediction binary crf learned jth binary par                                                cohn et al   csecoc signiﬁcantly improve  tition bj yit tth element yi  train isecoc through use cascade features   binary crf hk based binary partition bk each training ex                                                      formance csecoc depend strongly base crf  ample xiyi extended bkyi each                                                   algorithm particular appears critical algorithm  union observation features xit predictions  previous binary crfs sequence positions t−l able capture complex nonlinear interactions ob    experiments position servation cascade features  csecoc improve                                                        models trained using beam search gtb used     x    k−     k− k−    k−         ityit−l yit− yit yitr  base crf algorithm  using weaker base learning              k−h     k−h k−h    k−h            methods vp beam search outperform csecoc          yit−l yit− yit yitr   refer cascade history length use  nettalk data set                                            x  previous binary predictions position itsince nettalk task sejnowski rosenberg   features signiﬁcant autocorrelation eas sign phoneme stress symbol each letter word  ily lead overﬁtting predict given observation se                                                      word pronounced correctly observations  quence   make predictions ﬁrst binary crf correspond letters yielding total  binary observa  feeding predictions later binary crfs appro tion features each sequence position labels correspond  priate use decoding process isecoc phonemestress pairs yielding total  labels use    use previous binary predictions csecoc standard  training  test sequences  potential capture markovian transition structure comparing isecoc figures show  secoc experiments show impor sults training various models using gtb window  tant problems transition structure critical sizes   window size  isecoc  sequence labeling reﬂected observation able signiﬁcantly improve iid indicates  features computational overhead csecoc isecoc able capture useful transition structure im  secoc increase number observation features prove accuracy increasing cas  typically negligible impact overall training cade history length csecoc able substantially improve  time cascade history grows po isecoc accuracy improves  tential csecoc overﬁt additional features  percentage points indicates indepen  discuss issue section dent crf training strategy isecoc unable capture                                                        important transition structure problem csecoc    experimental results                               able exploit cascade history features order cap  compare crfs trained using isecoc csecoc   ture transition information particularly critical  beam search label set consider existing problem apparently just using information  base crf algorithms gradienttree boosting gtb diet observation window length  sufﬁcient make  terich et al  voted perceptron vp collins  accurate predictions indicated iid results  gtb able construct complex features primitive results window size  similar im  observations labels vp able combine provement isecoc iid csecoc  observations labels linearly gtb secoc smaller expected larger window  expressive power require substantially compu size spans multiple sequence positions allowing model  tational effort cases use forwardbackward al capture transition information using observations  gorithm make labelsequence predictions measure ac making need explicit transition model impor  curacy according fraction correctly labeled sequence tant secoc methods capture useful  elements used random code matrices constrained transition structure iid csecoc beneﬁting  columns identical complementary class la use cascade features window sizes  bels code word                         csecoc performs best particular cascade    consider nonsequential baseline model denoted tory length increasing length decreases ac  “iid” treats sequence elements independent curacy small indicates csecoc  examples effectively using nonsequential ecoc se suffer overﬁtting cascade history grows  quence element level particular train iid using figures show corresponding results vp  secoc zerothorder binary crf crfs observe including cascade features allows csecoc                                                    ijcai                                                                                                                                                                                                                                                                                                                                                                                  iid                                                                                  isecoc                                                                                 csecoc                                                                             csecoc                       prediction  accuracy                prediction  accuracy  csecoc                                                                                 csecoc                                            iid                                                      isecoc                                          csecoc                                        csecoc                                                  csecoc                                          csecoc                                                                                                                                                           length code words                   length code words                        window size  gtb             window size  gtb                                                                                                                                                                                                                                                                                                                                                                                                                                           prediction  accuracy              prediction  accuracy  iid                                              iid                                                                     isecoc                                           isecoc                    csecoc                                          csecoc                  csecoc                                        csecoc                                                                    csecoc                                          csecoc                 csecocall                                          csecoc                                          csecocall                                                                                                                                                           length code words                   length code words                         window size  vp              window size  vp                      figure  nettalk data set window sizes   trained gtb vp    improve isecoc compared gtb longer believe gtb requires forwardbackward infer  cascade histories required achieve improvement ence during training vp does  overﬁtting observed cascade history length  adversely affected beam search csecoc using  able observe overﬁtting code gtb performs signiﬁcantly better vp beam search  length  including possible cascade history bits  noted csecocall overﬁtting results suggest  noun phrase chunking  practice limited validation process used consider conll  noun phrase chunking npc  select cascade history csecoc large label sets shared task involves labeling words sentences  trying small number cascade history lengths problems used original evaluation  practical alternative training label set isecoc  class labels combi    gtb versus vp csecoc using gtb performs signiﬁ   nations partofspeech tagging labels npc labels  cantly better using vp explain noting  observation features each word sequences  critical feature csecoc binary crfs  sequences training set  se  able exploit cascade features order better capture quences test set large number observa  transition structure vp considers linear combinations tion features good results gtb using  features gtb able capture nonlinear current implementation present results vp  relationships inducing complex combinations fea comparing isecoc shown figure win  tures capturing richer transition structure dow size  isecoc outperforms iid incorporating cas  dicates using csecoc important use cade features allows csecoc outperform isecoc  training methods gtb able capture rich small margin overﬁtting csecoc  patterns observations cascade features larger numbers cascade features moving window size    comparing beam search compare   changes story large observation  formance csecoc multilabel crf models trained formation included current adjacent sentence  using beam search place fullviterbi forward positions result iid performs  backward inference common approach achiev secoc approaches large observation infor  ing practical inference large label sets beam search mation each sequence position appears capture  tried various beamsizes reasonable computational transition information secoc gets little beneﬁt  limits figure  shows accuracytrainingtime tradeoffs learning explicit transition model suggests  best beamsearch results best results achieved performance secoc methods domain pri  secoc  codeword bits various cascade marily reﬂection ability nonsequential ecoc  tories graph shows test set performance versus interesting given isecoc results cohn et al  training time cpu seconds notice  domains involved large local  gtb beam search signiﬁcantly worse vp observation information iid results reported                                                    ijcai                                                                                                                                                                                                                                                                                                                                                                                                                                           prediction  accuracy                prediction  accuracy                                                                                                      isecocgtb                           isecocgtb                                       csecocgtb                         csecocgtb                                          beamgtb                              beamgtb                                           beamvp                               beamvp                                                                                                                                                  cpu seconds                             cpu seconds                            window size                       window size                            figure  nettalk comparison secoc beam search                                                                                                                                                                                                                                                                       prediction  accuracy              prediction  accuracy                                                   iid                                           isecoc                                   iid                                        csecoc                            isecoc                                          csecoc                             csecoc                                          csecoc                             csecoc                                          csecoc                            csecoc                                                                                                                                                          length code words                   length code words                            window size                       window size                            figure  npc data set window sizes   trained vp      comparing beam search  compare accuracy  increases transition model importance  versus trainingtime models trained secoc “transition” data set use po  ko   beam search using described experimental setup pl  kl  transition structure im  figure  shows beam search performs better portant observation features quite uninformative fig  csecoc methods training time using ure shows results gtb training using window size  vp base learning algorithm believe poor  isecoc signiﬁcantly better iid indicat  formance csecoc compared beam search ing able exploit transition structure available  ing vp does allow rich patterns captured iid including cascade features allows csecoc  observations include cascade features hypoth improve performance showing isecoc  esize observed nettalk csecoc perform unable fully exploit information transition model  better beam search given base crf method previous experiments observe csecoc does  capture complex patterns observations overﬁt largest number cascade features win                                                        dow size  graph shown results similar    synthetic data sets                              relative improvements observa                                                        tion information available making transition model  results suggest isecoc does poorly critical                                                        critical results mirror nettalk data set  transition structure captured observation features                                                          “both” data set use              csecoc improve isecoc cases                                                                                              transition structure observation  evaluate hypotheses                                                                     features informative figure shows results    generated data using hidden markov model hmm                                                        gtb window size  observation features provide   labelsstates ll  possible observa                                                        large information performance iid similar  tions oo specify observation distribution                                                        secoc variants csecoc  each label li randomly draw set oi ko observations                                                        unable improve isecoc case suggests  including oi current state li hmm gen                                                        observation information captures bulk tran  erates observation oi probability po                                                        sition information performance secoc meth  generates randomly selected observation oi ob                                                        ods reﬂection nonsequential ecoc  servation model important increasing po                                                        ability explicitly capture transition structure  decreasing ko transition model deﬁned similar  way each label li randomly draw set li kl labels  including li current state li hmm sets  summary  state li probability pl generates uncovered empirical theoretical shortcomings  random state li increasing pl decreasing kl dependently trained secoc independent training binary                                                    ijcai                                                    
