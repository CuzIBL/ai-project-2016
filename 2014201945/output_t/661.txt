                           computing semantic relatedness using                          wikipediabased explicit semantic analysis                                 evgeniy gabrilovich     shaul markovitch                                      department science                        technion—israel institute technology  haifa israel                               gabrshaulmcstechnionacil                        abstract                            propose novel method called explicit semantic                                                        analysis esa ﬁnegrained semantic representation      computing semantic relatedness natural lan    unrestricted natural language texts method represents      guage texts requires access vast amounts    meaning highdimensional space natural concepts      commonsense domainspeciﬁc world knowl      rived wikipedia httpenwikipediaorg      edge  propose explicit semantic analysis      largest encyclopedia existence employ text classi      esa novel method represents mean   ﬁcation techniques allow explicitly represent      ing texts highdimensional space concepts meaning text terms wikipediabased concepts      derived wikipedia use machine learning   evaluate effectiveness method automatically      techniques explicitly represent meaning computing degree semantic relatedness frag      text weighted vector wikipediabased  ments natural language text      concepts assessing relatedness texts     contributions paper threefold      space amounts comparing corresponding present explicit semantic analysis new approach rep      vectors using conventional metrics cosine resenting semantics natural language texts using natural      compared previous state art using concepts second propose uniform way computing      esa results substantial improvements corre relatedness individual words arbitrarily long text      lation computed relatedness scores human  fragments finally results using esa computing      judgments   individual                                             semantic relatedness texts superior existing state      words           texts impor     art using wikipediabased concepts makes      tantly use natural concepts esa model easy interpret illustrate number      model easy explain human users          examples follows      introduction                                          explicit semantic analysis  related “cat” “mouse” “prepar approach inspired desire augment text rep  ing manuscript” “writing article” reasoning resentation massive amounts world knowledge  semantic relatedness natural language utterances rou represent texts weighted mixture predetermined set  tinely performed humans remains unsurmountable natural concepts deﬁned humans  obstacle computers humans judge text relatedness easily explained achieve aim use  merely level text words words trigger reasoning cepts deﬁned wikipedia articles sci  deeper level manipulates concepts—the ba enceindiaorlanguage important advantage  sic units meaning serve humans organize share approach use vast amounts highly orga  knowledge humans interpret speciﬁc wording nized human knowledge encoded wikipedia furthermore  document larger context background wikipedia undergoes constant development breadth  knowledge experience                             depth steadily increase time    long recognized order process nat opted use wikipedia currently  ural language computers require access vast amounts largest knowledge repository web wikipedia avail  commonsense domainspeciﬁc world knowledge    able dozens languages english version  buchanan feigenbaum  lenat guha  largest  million words million  prior work semantic relatedness based articles compared  million words  articles  purely statistical techniques did make use encyclopaedia britannica interestingly open editing  ground knowledge baezayates ribeironeto  approach yields remarkable quality—a recent study giles  deerwester et al  lexical resources incor  wikipedia accuracy rival britannica  porate limited knowledge world budanitsky                                                             hirst  jarmasz                          httpstorebritannicacom visited                                                      ijcai                                                                                                             building semantic interpreter    use machine learning techniques build semantic                           word  interpreter maps fragments natural language text    weighted sequence wikipedia concepts ordered          uilding weighted wordi  relevance input way input texts represented       inverted index                                                                                           weighted list  weighted vectors concepts called interpretation vectors wikipedia                concepts                                                                                            ikipedia  meaning text fragment interpreted terms                        wordn articles  afﬁnity host wikipedia concepts comput  ing semantic relatedness texts amounts comparing                            weighted  vectors space deﬁned concepts exam                          inverted index  ple using cosine metric zobel moffat  semantic analysis explicit sense manipulate si ng semanti nter pr eter  manifest concepts grounded human cognition                                                                                        vector  “latent concepts” used latent semantic analysis                                   comparison                                                          text     semantic    observe input texts given form         interpreter                    elatedness  wikipedia articles plain text use                               estimation  conventional text classiﬁcation algorithms sebastiani  text  rank concepts represented articles according                                                                               eighted  relevance given text fragment key ob            vector                                                                               ikipedia  servation allows use encyclopedia directly               concepts  need deep language understanding precataloged  commonsense knowledge choice encyclopedia arti            figure  semantic interpreter  cles concepts quite natural each article focused  single issue discusses                 input “equipment”     input “investor”    each wikipedia concept represented attribute vec  tool                   investment                                                                digital equipment corporation angel investor  tor words occur corresponding article entries  military technology equipment stock trader  vectors assigned weights using tfidf scheme      camping                mutual fund  salton mcgill  weights quantify         engineering vehicle    margin ﬁnance                                                                weapon                 modern portfolio theory  strength association words concepts           original equipment manufacturer equity investment    speed semantic interpretation build inverted   french army            exchangetraded fund                                                                electronic test equipment hedge fund  index maps each word list concepts  distance measuring equipment ponzi scheme  appears use inverted index discard insignif  icant associations words concepts removing table  concepts sample interpretation vectors  concepts weights given word low    implemented semantic interpreter centroid able features consequently work used wikipedia                                    based classiﬁer han karypis   given text concepts augment bag words hand  fragment ranks wikipedia concepts relevance computing semantic relatedness pair texts essen  fragment given text fragment ﬁrst represent tially “oneoff” task replace bag words  vector using tfidf scheme semantic interpreter iter representation based concepts  ates text words retrieves corresponding entries illustrate approach show highestscoring  inverted index merges weighted vector wikipedia concepts interpretation vectors sample  concepts represents given text let  wi                    v                               text fragments concepts each vector sorted  input text let tfidf vector decreasing order score concepts  weight word wiletkj  inverted index entry                                                    relevant ones input text table  shows  word iwhere  quantiﬁes strength association relevant wikipedia concepts individual words “equip  word wi wikipedia concept cj cj ∈ ccn                                                       ment” “investor” respectively table  uses longer    total number wikipedia concepts passages examples particularly interesting jux  semantic interpretation vector text vector                                                    tapose interpretation vectors fragments contain  length  weight each concept deﬁned ambiguous words table  shows ﬁrst entries vec   ∈t vi · kj  entries vector reﬂect relevance                                                    tors phrases contain ambiguous words “bank”  corresponding concepts text  compute seman ”jaguar” readily seen semantic interpreta  tic relatedness pair text fragments compare tion methodology capable performing word sense dis  vectors using cosine metric                      ambiguation considering ambiguous words context    figure  illustrates process wikipediabased seman neighbors  tic interpretation implementation details avail  able gabrilovich preparation    earlier work gabrilovich markovitch   empirical evaluation  used similar method generating features text cat implemented esa approach using wikipedia snap  egorization text categorization supervised learning shot march   parsing wikipedia xml  task words occurring training documents serve valu dump obtained  gb text  articles                                                    ijcai                                                              input “us intelligence say conclu input “the development tcell leukaemia following oth             sively saddam hussein weapons erwise successful treatment three patients xlinked se             mass destruction information gap vere combined immune deﬁciency xscid genetherapy tri             complicating white house efforts build sup als using haematopoietic stem cells led reevaluation             port attack saddam’s iraqi regime approach using mouse model gene therapy             cia advised administration ofﬁ scid ﬁnd corrective therapeutic gene ilrg             cials assume iraq weapons act contributor genesis tcell lymphomas             mass destruction agency given onethird animals affected genetherapy trials             president bush “smoking gun” according scid based assumption ilrg             intelligence administration ofﬁcials” minimally oncogenic pose risk patients”            iraq disarmament crisis               leukemia            yellowcake forgery                    severe combined immunodeﬁciency            senate report prewar intelligence iraq cancer            iraq weapons mass destruction  nonhodgkin lymphoma            iraq survey group                     aids            september dossier                     icd chapter ii neoplasms chapter iii diseases blood                                                   bloodforming organs certain disorders involving                                                   immune mechanism            iraq war                              bone marrow transplant            scott ritter                          immunosuppressive drug            iraq war rationale                   acute lymphoblastic leukemia           operation desert fox                  multiple sclerosis                    table  concepts interpretation vectors sample text fragments                           ambiguous word “bank”                     ambiguous word “jaguar”              “bank america”            “bank amazon” “jaguar car models” “jaguar panthera onca”             bank                         amazon river     jaguar car       jaguar             bank america              amazon basin     jaguar stype      felidae             bank america plaza atlanta amazon rainforest jaguar xtype  black panther             bank america plaza dallas amazoncom     jaguar etype      leopard             mbna                         rainforest       jaguar xj          puma             visa credit card           atlantic ocean   daimler            tiger             bank america tower       brazil           british leyland motor panthera hybrid              new york city                                 corporation             nasdaq                       loreto region    luxury vehicles    cave lion             mastercard                   river            engine          american lion            bank america corporate center economy brazil jaguar racing kinkajou                  table  concepts interpretation vectors texts ambiguous words    removing small overly speciﬁc concepts having textual descriptions concepts urls amounted  fewer  words fewer  incoming outgo  mb text order increase train  ing links  articles left processed text ing information populated odp hierarchy  articles removing stop words rare words crawling urls taking ﬁrst  pages en  stemming remaining words yielded  distinct countered each site eliminating html markup  terms served representing wikipedia concepts truncating overly long ﬁles ended  gb ad  attribute vectors                                    ditional textual data removing stop words rare    better evaluate wikipediabased semantic interpreta words obtained  distinct terms used  tion implemented semantic interpreter based represent odp nodes attribute vectors   largescale knowledge repository—the open  informative attributes selected each odp node  directory project odp httpwwwdmozorg     ing document frequency criterion sebastiani  odp largest web directory date  centroid classiﬁer trained training set  cepts correspond categories directory each concept combined concatenating crawled  topcomputersartificial    intelligenceinthis       content urls cataloged concept fur  case interpretation text fragment amounts computing ther implementation details available gabrilovich  weighted vector odp concepts ordered afﬁnity markovitch   input text                                      using world knowledge requires additional computation    built odpbased semantic interpreter using odp extra computation includes onetime preprocess  snapshot april  pruning topworld ing step semantic interpreter built  branch contains nonenglish material obtained actual mapping input texts interpretation vectors  hierarchy  concepts  urls formed online standard workstation throughput                                                    ijcai                                                    semantic interpreter words second algorithm                      correlation                                                                                              humans    datasets evaluation procedure                  wordnet jarmasz                –  humans innate ability judge semantic relatedness roget’s thesaurus jarmasz      texts human judgements reference set text pairs lsa finkelstein et al         considered correct deﬁnition kind “gold wikirelate strube ponzetto   –   standard” algorithms evaluated esawikipedia                          studies measured interjudge correlations esaodp                               consistently high budanitsky hirst   jarmasz  finkelstein et al   −        table  computing word relatedness  ﬁndings expected—after consen algorithm                 correlation  sus allows people understand each                                        humans    work use datasets  bag words lee et al  –  best knowledge largest publicly available collec lsa lee et al            tions kind assess word relatedness use     esawikipedia                                                                      wordsimilarity collection finkelstein et al   esaodp                          contains  word pairs each pair – human  judgements averaged each pair produce        table  computing text relatedness  single relatedness score spearman rankorder correlation  coefﬁcient used compare computed relatedness scores  human judgements                                common web pages hand wikipedia ar    document similarity used collection  docu ticles virtually noisefree qualify standard  ments australian broadcasting corporation’s news written english  mail service lee et al  documents paired  possible ways each  pairs –  related work  human judgements human judgements av ability quantify semantic relatedness texts  eraged each pair collection  relatedness scores lies fundamental tasks computational linguistics   distinct values spearman correlation ap including word sense disambiguation information retrieval  propriate case used pearson’s linear word text clustering error correction budanitsky  correlation coefﬁcient                               hirst  prior work ﬁeld pursued three main                                                        directions comparing text fragments bags words vec    results                                          tor space baezayates ribeironeto  using lexical  table  shows results applying methodology resources using latent semantic analysis lsa deer  estimating relatedness individual words wester et al  technique simplest  esa techniques yield substantial improvements performs suboptimally texts compared  prior studies esa achieves better results share words instance texts use synonyms  wikipediabased method recently introduced strube convey similar messages technique trivially  ponzetto  table  shows results computing inappropriate comparing individual words  relatedness entire documents                      techniques attempt circumvent limitation    test collections wikipediabased semantic inter lexical databases wordnet fellbaum   pretation superior odpbased fac roget’s thesaurus roget  encode relations  tors contribute phenomenon axes multi words synonymy hypernymy quite met  dimensional interpretation space ideally orthog rics deﬁned compute relatedness using vari  onal possible hierarchical organization ous properties underlying graph structure  odp deﬁnes generalization relation sources budanitsky hirst  jarmasz  baner  cepts obviously violates orthogonality requirement jee pedersen  resnik  lin  jiang  second increase training data build conrath  grefenstette  obvious drawback  ing odpbased semantic interpreter crawled approach creation lexical resources requires  urls cataloged odp allowed increase lexicographic expertise lot time effort  textual data orders magnitude consequently resources cover small fragment  brought nonnegligible noise language lexicon speciﬁcally resources contain                                      ˜                proper names neologisms slang domainspeciﬁc tech      httpwwwcstechnionacil    gabr            nical terms furthermore resources strong lexical  resourcesdatawordsim                                                        orientation mainly contain information individual    despite test collection designed testing  word relatedness merely similarity instructions hu words little world knowledge general  man judges speciﬁcally directed participants assess degree wordnetbased techniques similar esa  relatedness words example case antonyms approaches manipulate collection concepts  judges instructed consider “similar” “dis important differences wordnetbased  similar”                                             methods inherently limited individual words                                                    ijcai                                                    adaptation comparing longer texts requires extra level larity words using rg rubenstein goodenough  sophistication mihalcea et al  contrast  list  word pairs mc miller charles  method treats words texts essentially  list  word pairs similarity  way second considering words context allows ap lation considered using lexical resources suc  proach perform word sense disambiguation table  cessful reaching correlation –  using wordnet achieve disambiguation infor human judgements budanitsky hirst  jarmasz  mation synsets limited words gloss  case lexical techniques slight edge  odp wikipedia concept associated huge esa correlation human scores   amounts text finally individual words esa mc  rg entire lan  provides sophisticated mapping words guage wealth considered attempt capture  cepts through analysis large bodies texts associ general semantic relatedness lexical techniques yield sub  ated concepts allows represent meaning stantially inferior results table  wordnetbased tech  words texts weighted combination concepts niques consider generalization “isa” rela  mapping word wordnet amounts simple lookup tion words achieve correlation –  weights furthermore wordnet senses human judgements budanitsky hirst jar  each word mutually exclusive approach concepts masz  szpakowicz’s elkb jarmasz  based  reﬂect different aspects input tables – roget’s thesaurus achieves higher correlation   yielding weighted multifaceted representation text use richer set relations    hand lsa deerwester et al  sahami heilman  proposed use web  purely statistical technique leverages word cooccur source additional knowledge measuring similarity  rence information large unlabeled corpus text lsa short text snippets major limitation technique  does rely humanorganized knowledge applicable short texts sending long text  “learns” representation applying singular value query search engine likely return  composition svd wordsbydocuments cooccurrence results hand approach applicable  matrix lsa essentially dimensionality reduction tech text fragments arbitrary length  nique identiﬁes number prominent dimensions strube ponzetto  used wikipedia com  data assumed correspond “latent puting semantic relatedness method called  cepts” meanings words documents com wikirelate radically different given pair  pared space deﬁned concepts latent semantic words wikirelate searches wikipedia arti  models notoriously difﬁcult interpret com cles respectively contain  puted concepts readily mapped natural titles semantic relatedness computed using various  cepts manipulated humans explicit semantic analy distance measures measures ei  sis method proposed circumvents problem rep ther rely texts pages path distances  resents meanings text fragments using natural concepts category hierarchy wikipedia hand  ﬁned humans                                       approach represents each word weighted vector    approach estimating semantic relatedness words wikipedia concepts semantic relatedness com  somewhat reminiscent distributional similarity lee puted comparing concept vectors   dagan et al  compare mean differences esa wikirelate  ings words comparing occurrence patterns  wikirelate process words actually occur  large collection natural language documents titles wikipedia articles esa requires  compilation documents arbitrary word appears text wikipedia articles  documents aligned encyclopedia articles each  focused single topic                    wikirelate limited single words esa    paper deal “semantic relatedness” compare texts length  “semantic similarity” “semantic distance”  wikirelate represents semantics word  used literature extensive survey text article associated node  relatedness measures budanitsky hirst  argued category hierarchy esa  notion relatedness general sim phisticated semantic representation based weighted  ilarity subsumes different kind speciﬁc vector wikipedia concepts  relations including meronymy antonymy functional associ  ation maintained computational shown previous section richer  linguistics applications require measures relatedness representation esa yields better results  narrowly deﬁned measures similarity  example word sense disambiguation use related  conclusions  words context merely similar words bu proposed novel approach computing semantic relat  danitsky hirst  argued notion se edness natural language texts aid large  mantic distance confusing different ways  used literature                      wikirelate strube ponzetto  achieved relatively    prior work ﬁeld focused semantic simi low scores – domains                                                    ijcai                                                    
