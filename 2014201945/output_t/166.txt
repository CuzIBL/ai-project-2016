                              searching interacting features                                          zhengzhaoandhuanliu                             department science engineering                                          arizona state university                                      zhengzhao huanliuasuedu                          abstract                          intrinsic character feature interaction irreducibil                                                        ity jakulin bratko  feature lose      feature interaction presents challenge feature relevance absence interacting features      selection classiﬁcation feature      little correlation target concept existing efﬁcient feature selection algorithms usually                                                                                                               combined features sume feature independence dash liu  hall        strongly correlated target concept irreducible nature feature interactions      unintentional removal features result algorithms select interacting features      poor classiﬁcation performance handling fea  monk attempt explicitly address feature      ture interaction computationally intractable interactions ﬁnding loworder interactions                                                                                             recognizing presence feature interaction way jakulin bratko   authors suggest      propose efﬁciently handle feature interaction use interaction gain practical heuristic detecting      achieve efﬁcient feature selection present ex tribute interaction using interaction gain algorithms      tensive experimental results evaluation       detect datasets way feature class                                                        way features class interactions fur                                                        ther provide jakulin bratko  justiﬁcation    introduction                                       interaction information replace notion ‘high’  high dimensionality data poses challenge learning ‘low’ jakulin bratko  statistical sig  tasks classiﬁcation presence irrele niﬁcance illustrate signiﬁcant interactions form  vant features classiﬁcation algorithms tend overﬁt train interaction graph apply feature selection  ing data guyon elisseeff  dash liu  algorithms synthetic data known interaction ob  features removed performance dete serve fare fcbf yu liu cfshall  rioration giladbachrach et al  feature selection  relieff kononenko andfocusalmuallim  effective means remove irrelevant features blum dietterich  available weka witten  langley  optimal feature selection requires frank   exponentially large search space wheren  motivating examples synthetic data known fea  number features almuallim dietterich ture interaction synthetic data sets used exam  searchers resort various approximations determine ine various algorithms deal known feature interac  relevant features relevance determined correla tions feature selection ﬁrst data set corral john et  tion individual features class hall  al  having boolean features aabbir  yu liu  single feature class deﬁned ∧ ∨ ∧  sidered irrelevant based correlation class features aabb independent each fea  combined features rele ture irrelevant values uniform ran  vant unintentional removal features result dom distribution feature correlated   loss useful information cause poor clas time redundant three training data sets  siﬁcation performance studied jakulin bratko monks data target concepts  monk   attribute interaction example monk data    monk exactly  set involving feature interaction features                                                                                                     monk target concept monk     monk        ora          herea   interacting features  class noise added training data  considered individually correlation results presented table  corral algo  class similarly zero measured mu rithms remove irrelevant feature focus  tual information irrelevant each moves redundant feature features aa bb  individually evaluated combine interact each determine class label  strongly relevant deﬁning target concept stance cfs fcbf relieff remove                                                    ijcai                                                           table  features selected each algorithm artiﬁcial data   indicates missing relevant feature                         relevant features fcbf         cfs         relieff       focus                  corral aabb  aabb    aabb aabb                  monk   aaa     aa         aaa      aaa                  monk  aaaa              aaaa  aaaa                            aa         aa        aa        aa         aa                  monk   aaa             aaa    aaaa  strongly correlated   three monks directly applied identify relevant interact  data sets relieff ﬁnd true relevant features seen ing features dimensionality data set high  table  fcbf cfs perform similarly fcbf efﬁcient feature selection algorithms identify relevant  ﬁnds features focus handle feature interaction features based evaluation correlation  selecting features exhaustive search class feature selected feature subset rep  algorithm focus ﬁnds irrelevant feature monk resentative measures used evaluating relevance include   noise data sense overﬁts training distance measures kononenko  robniksikonja  data focus impractical ﬁnding moderately  kononenko  information measures fleuret                                             m     highorder interactions expensive consistency measures almuallim dietterich   large dimensionality large     using measures feature selection algo    work design implement efﬁcient ap rithms usually start set successively add  proach deal feature interactions feature interactions “good” features selected feature subset socalled  implicitly handled carefully designed feature eval sequential forward selection sfs framework  uation metric search strategy specially designed framework features deemed relevant mainly based  data structure account interactions individually high correlations class rele  features performing feature selection     vant interacting features high order removed hall                                                         bell wang  irreducible nature    interaction data consistency                   feature interaction attained sfs                                                          recall ﬁnding highorder feature interaction using rel  goal feature selection remove irrelevant fea evance deﬁnitions   entails exhaustive search  tures deﬁne feature relevance john et al feature subsets order avoid exponential time complex  letf   set features fi feature ity derive feature scoring metric based  si  −fi  denote conditional probability sistency hypothesis proposed almuallim dietterich  class given feature set                           approximate relevance measure deﬁni  deﬁnition  feature relevance afeaturefi relevant iff tion  metric design fast ﬁlter algorithm                                                        deal feature interaction subset selection       ∃   ⊆                                si   sisuchthatpc    fisi    si            let data set instances  dddm                                                        feature space features   feature fi said irrelevant                                                        fn following    deﬁnition  suggests feature relevant                                                        deﬁnition  inconsistent instances instances di  removal feature set reduce prediction                                                        dj values class la  power deﬁnition  shown feature                                                        bels di dj inconsistent instances matching  relevant reasons  strongly correlated                                                        instances inconsistent  target concept  forms feature subset  features subset strongly correlated target deﬁnition  inconsistentinstances set ⊆  concept feature relevant second reason inconsistentinstances set iff ∀ didj ∈  di  exists feature interaction feature interaction charac dj inconsistent duplicate maximal  terized irreducibility jakulin bratko akth inconsistentinstances set iff ∀d ∈ d∈ ∪d  feature interaction formalized             inconsistentinstances set                                                                                               deﬁnition  kth order feature interaction feature deﬁnition  inconsistency count let                                                                                        subset features fkletc denote metric inconsistentinstances set elements ddk                                                                                              measures relevance class label feature ccct class labels  partition                                                         feature subset features  fk said interact subsets ssst class labels                                                                                                   each iff arbitrary partition  si dj dj label ci  inconsistency count                 ≥             wherel       φ                inconsistencycountdk   −  max  si                                                                                            ≤i≤t               ∀i ∈  fi                                                        deﬁnition  inconsistency rate let  dp    identifying relevant features kth order feature note maximal inconsistentinstances sets incon                                                        sistency rate icr  interaction requires exponential time deﬁnitions                                                                                        ≤i≤p inconsistencycountdi     interestingly using data sets relieff missed aa icrd  monk monk                                                                                                                   ijcai                                                    deﬁnition  consistency contribution ccontribution ranked features eliminated slist initialized  let π denote projection operator retrieves sub set features instances hash  set columns according feature subset key insert entry hash table  ccontribution cc feature fi deﬁned information labels recorded each en                                                        try hash table corresponds maximal inconsistency                           −              cc  fi     icr  πf −fi    icr  πf                                                                                                                 set πslist  inconsistency rate πslist    easy verify inconsistency rate monotonic obtained scanning hash table property says  terms number features ∀si sj si ⊆ sj order generate entry hash table new                                                        slist eliminating feature necessary scan  ⇒  icrπsi ≥ icrπsj ccontribution  feature nonnegative number zero data current hash table property ii  meaning contribution ccontribution feature fi suggests each iteration elimination number                                                                                    list  function −fiwheref set features entries hash table new decrease  ccontribution feature indicator signiﬁ fore hashing data structure allows efﬁcient update  cantly elimination feature affect consistency ccontribution iterative feature elimination  ccontribution irrelevant feature zero ccontribution  dealing feature order problem  considered approximation metric def  inition  using inconsistency rate approximation consider feature order problem applying   conditional probability class given feature set contribution removing current irrel    monotonic property inconsistency rate suggests evant feature likely retain relevant ones                                                        remaining subset selected features assuming set  backward elimination search strategy ﬁts ccontribution                               best feature selection start features divided subset including relevant                                                        features subset containing irrelevant ones consid  feature set successively eliminating features time                                  based ccontributions backward elimination allows ering remove features ﬁrst features  feature evaluated features inter likely remain ﬁnal set selected features  act backward elimination ccontribution apply heuristic rank individual features using symmet  ﬁnd interacting features backward elimina rical uncertainty su descending order  tion using inconsistency rate ccontribution prob heuristically relevant feature positioned begin  lems ﬁrst problem costly needs ning list su used fast correlation measure  calculate inconsistency rate each potentially remov evaluate relevance individual features hall   able feature work focus almuallim  yu liu  heuristic attempts increase                                                       chance strongly relevant feature remain se  dietterich   focus relies exhaustive search                           impractical dimensionality reasonably lected subset let denote entropy                                                        joint entropy respectively mx chchx−  large separates work focus      sign speciﬁc data structure achieve efﬁcient calcu mutual information measuring common  lation ccontribution algorithm interact formation shared variables su                                                        class label feature fi  second problem ccontribution measure sensitive                                 feature selected compute ﬁrst socalled                                                                                                              fic  feature order problem features evaluated ﬁrst   su  fic                                                                                  hfihc  consistency likely eliminated ﬁrst    solutions problems enable ccontribution ranking heuristic guarantee interact  used building efﬁcient algorithm backward elimi ing features ranked high monk example  nation present solutions algorithm suacsuac evaluated                                                        ﬁrst ccontribution cca cca    eliminating irrelevant features                    eliminated experimentally examine rank                                                        ing effect section   ﬁrst present solutions form pillar components  algorithm interact discuss details  interact  algorithm                                                        solutions pave way ccontribution    efﬁcient update ccontribution                used feature selection present algorithm inter  ccontribution relies calculation inconsistency rate act searches interacting features ﬁlter al  monotonicity inconsistency rate following gorithm employs backward elimination remove  properties true feature fi eliminated features low ccontribution details shown  asetf  fi  fn    inconsis figure  given set features class  tent instances sets inconsistent ii each maximal tribute ﬁnds feature subset sbest class concept  inconsistent instances set equal size big algorithm consists major parts ﬁrst  ger based properties implement hashing lines  features ranked descending order based  mechanism efﬁciently calculate ccontribution each su values second lines  features  stance inserted hash table using values evaluated starting end ranked  features slist hash key slist contains feature list function getlastelement returns feature                                                    ijcai                                                         input         feature set features      table  summary benchmark data sets number            fn                                  features number instances number classes         class label         δ predeﬁned threshold                          data set         data set                                                                   lungcancer       vehicle                output                                             zoo              krvskp                sbest best subset                            wine                                                                     soylarge       internetads                                                                                      ×         slist null                                      cmc                                                                                                                         calculate sufic fi                    relevant features δ    evaluate            append fi slist                         interact using benchmark data sets comparison         end                                           representative feature selection algorithms fol       orderslist descending values suic        lowing aspects  number selected features  predictive          getlastelementslist                     accuracy feature selection  run time ex         repeat                                        amine effective solutions given sections             null                            through lesion study removing               ccf slist  ccontribution              ≤ δ                           time interact                 remove slist              end                                       experiment setup           end                                        experiments choose representative feature se            getnextelementslistf                lection algorithms comparison fcbf yu         null                               liu cfshall  relieff kononenko         sbest  slist                                 focus  almuallim dietterich  avail        return sbest                                  able weka environment witten frank                                                         interact implemented weka’s framework             figure  algorithm interact               available request experiments  end list slist ccontribution feature conducted weka environment  δ feature removed selected func  data sets jakulin  identiﬁed  data sets                             tion getnextelement slistf returns unchecked having feature interactions selecting interacting fea  feature just preceding ranked feature list line  tures focus discussion  data sets  algorithm repeats features list checked datasets uci ml repository blake                                  δ predeﬁned threshold  δ  features merz  include datasets  ccontribution δare considered immaterial removed experiment ‘internetads’ data uci ml repos  alargeδ associated high probability removing itory ‘×c’ data alizadeh et al  relevant features relevance deﬁned ccontribution  information  data sets   higher value ccf slist indicates rele monks data sets summarized table  each data          vant δ       mentioned parameter set run  feature selection algorithms obtain se  tuned using standard cross validation lected feature subsets each algorithm data sets  time complexity interact ﬁrst algo taining features continuous values needed apply                                          rithm linear time complexity nm wheren mdl discretization method available weka  number features number instances index features order evaluate  given data set second algorithm selected features good apply effec  calculation feature’s ccontribution using hash table tive classiﬁcation algorithms linear support vector  takes onmforn    features time complexity                                                                     machine svm  available weka af  interact   worst case analy ter feature selection obtain prediction accuracy fold  sis average time complexity  crossvalidation cv evaluates features  use hash table current slist calculation time performance sensitive given sets  contribution  number entries hash features accuracy rates monk   table decreases each iteration assume decreases  aaanda  features respec                                               percentage initial size  tively focus’s exponential time complexity  iterations overall time complexity interact provide results focus obtained  hours          −    −    nm                proof omitted dedicated use pc interact fcbf cfs  words interact expected comparable  heuristic algorithms fcbf yu liu      data sets monks data consider syn                                                        thetic data discussed earlier separately    empirical study                                       naive bayes classiﬁer nbc assumes conditional inde                                                        pendence features irina rish  selecting interacting  empirically evaluate performance interact features limited impact experimental results  search interacting features synthetic data sets nbc conform analysis presented  known feature interaction table  interact ﬁnds space limit                                                    ijcai                                                    table  number selected features each algorithm      interact fc fcbf relieff fo focus fs set  na denotes available                                                                                                               data set      fc   cfs      fo    fs      lungcancer                                              zoo                                     wine                                                     soylarge  na      cmc                                                         vehicle                               krvskp                     na                            internetads               na         ×c                 na                          average                                                                                                                                               zoo lungcancer wine soylarge cmc vehicle krvskp xc internetad  lieff complete runs seconds consistent  understanding expectation algorithms    results discussion    number selected features table  presents numbers      figure  tinteractd tinteract  features selected ﬁve algorithms algorithms sig  niﬁcantly reduced number features cases    average numbers selected   table  run time second each algorithm  features  data sets  interact    data set      fc     cfs       fo  fcbf  cfs  relieff  set lungcancer          data sets indicated na table focus did zoo                                                                             wine                   ﬁnish  hours  synthetic data sets soylarge           na  known relevant features table  interact selected  cmc                    relevant ones examine effect reduc vehicle              tion accuracy                                          krvskp             na                                                             internetads     na    predictive accuracy feature selection  ×c        na   data sets known interactions obtained pre average           dictive accuracy rates fold cross validation using  svm results shown subtables ta  ble  classiﬁers reduction features fcbf employs su observe table  fcbf does  teract obtains results comparable using features perform interact selecting interacting  average accuracy  interact vs    features results suggest combination su  set  interact vs  ccontribution helps interact achieve design goal  set svm comparing interact feature se clearly using su heuristic alter  lection algorithms interact performs consistently better natives rank features studying effects different  better average accuracy svm inter ranking algorithms line ongoing research  act comparable feature selection algorithms effect data structure devised hashing  exception ‘soylarge’ data result svm data structure speed time consuming calculation  notice data set  features  instances ccontribution during feature selection examine   classes table  interact identiﬁes  features ta effective data structure comparing run time  ble   smallest number selected features fcbf                                                        interact interactd   does  selected  features surmise easy employ hashing data structure data size deter  inconsistency measure satisﬁed small feature mining factor run time ﬁrst reorganize  data sets  subset each class small number instances according sizes approximated ∗  number  sum classiﬁers interact help achieve bet features multiplied number instances  ter similar accuracy interact effective sidering feature types nominal continuous fig  search interacting features                       ure  shows ratios using time interact base    effect feature ranking interact ranks features                                                        tinteract     divided tinteract shows  backward elimination features begins                                                               run time difference interact interactd  lesion study remove ranking component pronounced larger data sets  interact form version interactr summa  rize results space limit interact run time comparison table  records run time  better equivalent interactrtheaver each feature selection algorithm focus al  age fold cv accuracy svm interact gorithms ﬁnished runs given time algo  interactr      respec rithms ordered relieff fcbf interact cfs  tively noticing interact ranks features using su focus relieff fastest focus slowest                                                        ﬁnishes run  hours                                                    ijcai                                                    
