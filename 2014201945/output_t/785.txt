                  bounded policy iteration decentralized pomdps             daniel bernstein                eric hansen                  shlomo zilberstein       dept science        dept cs engineering       dept science      university massachusetts       mississippi state university      university massachusetts          amherst ma              mississippi state ms            amherst ma            berncsumassedu              hansencsemsstateedu             shlomocsumassedu                        abstract                          problems various ways extend inÔ¨Ånite                                                        horizon case cases suffers fact      present bounded policy iteration algorithm memory requirements grow quickly each itera      inÔ¨Ånitehorizon decentralized pomdps policies    tion practice used solve small      represented joint stochastic Ô¨Ånitestate problems likely optimal algorithm suf      trollers consist local controller each fer problem Ô¨Ånitehorizon decpomdps      agent let joint controller include cor shown nexpcomplete just agents bern      relation device allows agents correlate stein et al       behavior exchanging information dur  paper present memorybounded dynamic      ing execution show leads improved programming algorithm inÔ¨Ånitehorizon decpomdps      performance algorithm uses Ô¨Åxed     algorithm uses stochastic Ô¨Ånitestate controller rep      memory each iteration guaranteed pro resent joint policy agents straightforward ap      duce controller value high proach use set independent local controllers      previous possible initial state distribu each agent provide example illustrate higher      tions case single agent algorithm value obtained through use shared randomness      reduces poupart boutilier‚Äôs bounded policy deÔ¨Åne joint controller set local      iteration pomdps                             trollers correlation device correlation                                                        vice Ô¨Ånitestate machine sends signal                                                        agents each time step behavior determined prior    introduction                                       execution time does require agents  markov decision process mdp framework proven exchange information receiving local observations  useful solving problems sequential decision mak algorithm generalizes bounded policy iteration  ing uncertainty problems agent pomdps poupart boutilier  multiagent  base decision partial information case each iteration node chosen  state case better use gen local controllers correlation device parameters  eral partially observable markov decision process pomdp updated through solution linear program  framework pomdps difÔ¨Åcult solve generalization theoretical guarantees  worst case progress development pomdp case iteration guaranteed produce  practical dynamic programming algorithms smallwood new controller value high possible  sondik  cassandra et al  hansen  initial state distribution  poupart boutilier  feng zilberstein  experiments applied algorithm idealized    general problems team deci networking robot navigation problems problems  sion makers each local observations act large exact dynamic programming  domains types problems arise handled approximation algorithm  include networking multirobot coordination ecommerce addition correlation device gives rise better solutions  space exploration systems model problems addition larger controllers lead better solu  use decentralized partially observable markov deci tions  sion process decpomdp framework model   number approximation algorithms devel  recognized decades witsenhausen oped previously decpomdps peshkin et al    little work efÔ¨Åcient algorithms nair et al  emerymontemerlo et al                                                     previous algorithms guarantee bounded    recently exact dynamic programming algorithm memory usage monotonic value improvement ini  proposed decpomdps  hansen et al  tial state distributions furthermore use correlated  algorithm presented context Ô¨Ånitehorizon stochastic policies decpomdp context novel theimportance correlation recognized game                                                                 ab                          aa  theory community aumann  little     ba     aa                      ab  work algorithms Ô¨Ånding correlated policies             bb                             ba    background                                             ‚Äìr                                       ‚Äìr                                                                                            section present formal framework                                         multiagent decision making decentralized partially  observable markov decision process decpomdp tu  ple hi ai oi ri                                                                                         bb    ‚Ä¢ Ô¨Ånite set agents indexed                                    ‚Ä¢ Ô¨Ånite set states      ‚Ä¢ ai Ô¨Ånite set actions available agent figure  Ô¨Ågure shows decpomdp                                                        optimal memoryless joint policy requires correlation        √ói‚àài ai set joint actions       ha     ani denotes joint action      ‚Ä¢ oi Ô¨Ånite set observations agent  taken agents‚Äô controllers determine                                                                                     √ói‚àài oi set joint observations  ditional distribution denoted inde      ho     oni denotes joint observation      pendent joint controller following subsection                                                        show independence limiting    ‚Ä¢ set markovian state transition observation                                probabilities  osa denotes probability  utility correlation      taking joint action state results transition      state joint observation              joint controllers described allow agents                                                        correlate behavior shared source randomness                  ‚Ä¢  √ó ‚Üí  reward function                use simple example illustrate utility cor    paper consider case process relation partially observable domains agents  unfolds inÔ¨Ånite sequence stages each stage limited memory example generalizes given  agents simultaneously select action each receives singh et al  illustrate utility stochastic poli  global reward local observation objective cies singleagent partially observable settings  agents maximize expected discounted sum rewards consider decpomdp shown figure  prob  received denote discount factor Œ≥ require lem states agents actions agent   ‚â§ Œ≥                                              agents each observation                                                        distinguish states example    finitestate controllers                           consider memoryless policies                                                          suppose agents independently randomize  algorithm uses stochastic Ô¨Ånitestate controllers rep                                                        behavior using distributions consider  resent policies section Ô¨Årst deÔ¨Åne type policy each agent chooses accord  troller agents act independently provide ing uniform distribution yields expected reward  example demonstrating utility correlation show ‚àí time step results expected longterm  extend deÔ¨Ånition joint controller allow                                                         reward  ‚àír   straightforward show  correlation agents                                      ‚àíŒ≥                                                        dependent policy yields higher reward    local finitestate controllers                   states  decpomdp each agent select action based let consider larger class policies  history local observations finitestate controllers pro agents act correlated fashion words                                                        consider joint distributions consider policy  vide way represent local policies using Ô¨Ånite                                            memory state controller based ob assigns probability  pair aa probability   servation sequence agent‚Äôs actions based pair bb yields average reward  each time  state controller allow stochastic transitions step expected longterm reward  dif  stochastic action selection help make ference rewards obtained independent  limited memory type controller correlated policies arbitrarily large increasing  used previously singleagent context platzman   meuleau et al  poupart boutilier     formally deÔ¨Åne local Ô¨Ånitestate controller agent  correlated joint controllers  tuple hqi œài Œ∑ii qi Ô¨Ånite set previous subsection established correlation  troller nodes œài  qi ‚Üí ‚àÜai action selection function useful face limited memory subsection  Œ∑i  qi √ó ai √ó oi ‚Üí ‚àÜqi transition function extend deÔ¨Ånition joint controller allow correla  functions œài Œ∑i parameterize conditional distribution tion agents introduce additional          ai qiqi oi                                    Ô¨Ånitestate machine called correlation device provides                                variables  xc ai xc ai oi qi   objective maximize    improvement constraints                                          ‚àÄs q‚àíi   ‚â§ a‚àíic q‚àíixc airsa                                                                                                                                                                  Œ≥       xc ai oi qip q‚àíic q‚àíi a‚àíi o‚àíip  osap cv                                       soq   probability constraints                                                                                               ‚àÄc     xc ai   ‚àÄc ai oi  xc ai oi qi  xc ai                                                                                                                 qi                                                                                                       ‚àÄc ai xc ai ‚â•  ‚àÄc ai oi qi xc ai oi qi ‚â•     table  linear program used Ô¨Ånd new parameters agent i‚Äôs node qi variable xc ai represents aiqi                                        variable xc ai oi qi represents ai qic qi oi    extra signals agents each time step device op lated joint controller change correlation  erates independently decpomdp process vice local controllers improvements  does provide agents information bounded backup involves solving lin  agents‚Äô observations fact random numbers necessary ear program following improvement controller  operation determined prior execution time reevaluated through solution set linear equa    formally correlation device tuple hc œài tions bounded backup works  set states œà  ‚Üí ‚àÜc state transition func prove produces new controller value  tion each step device undergoes transition each high initial state distributions  agent observes state    modify deÔ¨Ånition local controller  improving local controller  state correlation device input Ô¨Årst improve local controller  local controller agent conditional distribution choose agent node qi                  form ai qic qi oi correlation device search new parameters conditional distribution                                                                local controllers form joint conditional distribu ai qic qi oi  tion ca refer correlated search new parameters works follows  joint controller note correlated joint controller sume original controller used second    effectively independent joint controller step try replace parameters qi better    value function correlated joint controller ones just Ô¨Årst step words look  computed solving following linear equa best parameters satisfying following inequality  tions each ‚àà ‚àà  ‚àà                                                                                  ‚â§   ac qrs                          ac qrsa                                                                                                                                                                               Œ≥       qa op osa                                       Œ≥        osap qa           soq                    soq                                                ¬∑ ccv                            ¬∑ ccv                                                         ‚àà q‚àíi ‚àà q‚àíi ‚àà note inequality  refer value controller initial satisÔ¨Åed original parameters  state distribution distribution Œ¥ deÔ¨Åned possible improvement                                                          finding new parameters using linear program                                      Œ¥  max   Œ¥sv           ming shown table  note linear program                      qc                                                      poupart boutilier  pomdps                                                        nodes local controllers correlation  assumed given initial state distribution                                                        device considered hidden state size polyno  troller started joint node maximizes value                                                        mial sizes decpomdp joint controller  distribution                                                        exponential number agents    bounded policy iteration                             improving correlation device  bounded policy iteration algorithm procedure improving correlation device  improving correlated joint controllers improve corre similar procedure improving local controller variables  xc   objective maximize    improvement constraints                                                               ‚àÄs   ‚â§ ac qrsa  Œ≥  qa op osaxcv                                                     soq   probability constraints                                                                                  ‚àÄc     xc   ‚àÄc xc ‚â•                                                table  linear program used Ô¨Ånd new parameters correlation device node variable xc represents  cc                                                                                           Ô¨Årst choose device node consider changing param tn vo ‚â• tn vo vn  limk‚Üí‚àû tn vo  eters just Ô¨Årst step look best parameters vn ‚â• vo value new controller higher  satisfying following inequality                  original controller possible initial state                                                      distributions     ‚â§   ac qrs               argument changing nodes correlation device                                                     identical given                                         Œ≥      qa op osa                                                          local optima                    soq                                                        bounded backups nondecreasing values                           ¬∑ ccv                                                         initial state distributions convergence optimality  ‚àà ‚àà                            guaranteed couple factors contributing    previous case search parameters fact local controller corre  formulated linear program shown table lation device improved possible   linear program polynomial sizes algorithm stuck suboptimal nash equilibrium  decpomdp joint controller exponential each controllers correlation device  number agents                                     optimal held Ô¨Åxed open problem                                                        linear program updating    monotonic improvement                            controller time                                                          course bounded backup does Ô¨Ånd optimal pa  following theorem says performing rameters controller held Ô¨Åxed  updates lead decrease value sequence updates converge local optimum  initial state distribution                   reaching nash equilibrium pomdps  theorem  performing bounded backup local poupart boutilier  provide characterization  troller correlation device produces correlated joint local optima heuristic escaping  controller value high initial state applied case address  distribution                                         suboptimal nash equilibrium problem    proof consider case node qi agent  i‚Äôs local controller changed let vo value function  experiments  original controller let vn value function implemented bounded policy iteration tested  new controller recall new parameters different problems idealized networking scenario          ai qic qi oi satisfy following inequality problem navigating grid ex  ‚àà q‚àíi ‚àà q‚àíi ‚àà                          perimental methodology speciÔ¨Åcs problems                                                      results    vos ‚â§    ac qrs                                                        experimental setup                                        Œ≥       qa op osa algorithm guarantees nondecreasing value                    soq                       initial state distributions chose speciÔ¨Åc distribution                           ¬∑ ccv    focus each problem experiments different dis                                                      tributions yielded qualitatively similar results  notice formula right bellman opera deÔ¨Åne trial run algorithm follows  tor new controller applied old value function start trial run size chosen each local  denoting operator tn inequalities implies trollers correlation device action selection  tnvo ‚â• vo monotonicity ‚â•  transition functions initialized deterministic                                                                                                                                                                                                                                                                                                                                                  independent                                            independent                                                          value    value                                                                               correlated                                 correlated                                                                                                                                                                                                                                                                                                                                                                  size local controllers                             size local controllers                                                                                   figure  average value trial run plotted size local controllers multiaccess broadcast channel  problem robot navigation problem solid line represents independent controllers correlation device  node dotted line represents joint controller including twonode correlation device    outcomes drawn according uniform distribution buffer agent  containing message buffer  step consists choosing node uniformly random agent   correlation device local controllers  forming bounded backup node  steps  meeting grid  run considered practice values usu problem robots navigating  ally stabilized  steps                      bytwo grid obstacles each robot sense    varied sizes local controllers   walls left right goal  agents‚Äô controllers sizes each robots spend time possible  varied size correlation device   square actions left right  number joint nodes ranged   memory stay square robot attempts  limitations prevented using larger controllers open square goes intended direction  each combination sizes performed  trial runs probability  goes direction  recorded highest value obtained runs stays square wall results  average value runs                      staying square robots interfere                                                        each sense each    multiaccess broadcast channel                     problem  states each robot  Ô¨Årst domain idealized model control multi  squares time each robot  observations  access broadcast channel ooi wornell  bit sensing wall left right total  problem nodes need broadcast messages each number actions each agent  reward   channel node broadcast time agents share square  discount  collision occurs nodes share common goal max factor  initial state distribution deterministic  imizing throughput channel                placing robots upper left corner grid    start each time step each node decides  send message nodes receive reward   results  message successfully broadcast reward  each combination controller sizes looked  wise end time step each node observes best solutions trial runs values  buffer previous step contained collision solutions controller sizes  successful broadcast attempted           smallest    message buffer each agent space instructive compare average values  message node unable broadcast message mes trial runs figure  shows graphs average values plotted  sage remains buffer time step node controller size  able send message probability buffer average value increases increase size cor  Ô¨Åll step pi problem nodes relation device node nodes essentially mov       states  actions ing independent correlated  agent  observations agent discount fac small controllers average value tends increase  tor  start state distribution deterministic controller size controllers larger
