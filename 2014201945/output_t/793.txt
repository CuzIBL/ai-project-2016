                   general game learning using knowledge transfer                                     bikramjit banerjee peter stone          department sciences university texas austin austin tx                                      banerjee pstonecsutexasedu                          abstract                          oftentimes speciﬁc skills required target tasks ac      present reinforcement learning game player   quired specially designed source tasks sim                                                                                                           interact general game playing sys ilar targets asgharbeygi et al       tem transfer knowledge learned game    consider challenging scenario skills      expedite learning games      general source target pairs bear little resemblance      use technique valuefunction transfer speciﬁcally consider genre player al      general features extracted state space ternate complete information games require      previous game matched com      knowledge acquired game transferrable                                                        game genre      pletely different state space new game                      λ      capture underlying similarity vastly disparate develop td  based reinforcement learner      state spaces arising different games use automatically discovers structures gametree      gametree lookahead structure features  uses features acquires values features      show featurebased value function trans learned valuefunction space uses values      fer learns superior policies faster reinforce learned game initialize parts valuefunction      ment learning agent does use knowledge   spaces games genre intention reuse      transfer furthermore knowledge transfer using   portions valuefunction space independent      lookahead features capture opponentspeciﬁc   game chosen genre order learn faster new      valuefunctions exploit opponent’s   games accomplished focusing exploration      weaknesses learn faster reinforcement   complementary regions value function space      learner uses lookahead minimax pes    foresight informative gameindependent way      simistic search opponent         use gametree lookahead generating features                                                        show features acquired way oppo                                                        nent indicative play opponent    introduction                                       new games assume transfer learner  general game playing ggp domain introduced  identify played given opponent  pell  allows description wide range games fore different game retrieve  uniform language called game description language feature values learned opponent reuse  gdl genesereth love  challenge simple lookahead search player additionally  velop player compete effectively arbitrary games need know learn opponent’s strategy select  presented gdl format paper focus effective heuristic right heuristic show  problem building learning agent use knowledge lookahead search player perform  gained previous games learn faster new games transfer learner given opponent  framework    knowledge transfer received signiﬁcant attention  reinforcement learning  cently machine learning research asgharbeygi et al                                                        reinforcement learning rl sutton barto    taylor stone  ferns et al  instead                                                        machine learning paradigm enables agent make se  developing learning systems dedicated individual appli                                                        quential decisions markovian environment agent’s  cations each beginning scratch inductve bias trans                                                        goal learn decision function optimizes  ferred previous learning tasks sources new                                                        future rewards learning techniques emanating rl  lated learning tasks targets order                                                        successfully applied challenging scenarios    •      offset initial performance target tasks compared game playing particularly champion backgammon      learning scratch andor                  player tdgammon tesauro  involving delayed    • achieve superior performance faster learning wards rewards termination leading credit      scratch                                          signment problem actions goodbad rl                                                    ijcai                                                     problems usually modeled markov decision processes  ggp learner  mdps sutton barto anmdpisgivenbythe    developed complete ggp learner caters            tuple            set environmental states ggp protocol ggp  genesereth love                                      agent given time set ac protocol deﬁnes match instance game played                                   × →  tions choose state        start state terminal state players                      reward function   speciﬁes reward connect game manager henceforth just man                                               ∈  environment agent gets executing action  ager network connection each match starts         ∈    × ×  →    state                        state tran players receiving gdl ﬁle specifying description  sition probability function specifying probability game going play respective roles  state markov chain consequential agent’s game  game manager waits predeﬁned  selection action state agent’s goal learn time startclock players allowed analyze                              π  →  policy action decision function    maximizes  game possible players signal                                              sum discounted future rewards state  ready end startclock phase case   π                                             manager terminates phase proceeds   set  rs πsγrs πs γ rs πs                                                           phase manager asks ﬁrst mover  ssare samplings distribution fol waits playclock  lowing markov chain policy π                submitted time manager selects    common method learning valuefunction random behalf player moves player  deﬁned through online interactions environ continues submitted end  ment learn actionvalue function given playclock manager moves player early                                                                      π            manager senses terminal state returns appropriate     qs ars amaxγ                                π                               rewards players terminates match paper                             s                         consider games player unambiguously knows    learned online dynamic programming using current state  following update rule                               measure learning performance learner plays se                                                       ries matches game given opponent    qs ← qs aαrsa maxγqs   − qs                                                      notes cumulative average rewards gets    playing action argmaxb qs state manager computations transfer learning  α ∈   learning rate rsa actual envi expensive perform qupdates                        ronmental reward ∼  actual state states visited course match during start  resulting agent’s choice action state sthe clock match sequence afterstates  qvalues guaranteed converge equation  σσσk σk terminal memory use  limit inﬁnite exploration each ensured fast tdλ sutton barto  update  suitable exploration scheme sutton barto                    t−p                                                              Δqσpαλ      rt  γqσt − qσt      rl ggp                                                               λ  ∈                                                           time                         general game playing game manager acts rk potentially nonzero qσk   environment learner needs interact batch update qvalues effectively different  way mdp outlined im online updates player face afterstate  portant differences  game manager returns rewards match unless game allows noop  end game  win  draw relegating bulk computation start   loss intermediate rewards  agent’s ac clock period transfer learner makes rapid moves involv  tion followed opponent’s action decides ing simple qvalue lookup useful large games  state agent faces opponent chooses ac chess derivatrives movetime  tion following stationary necessarily deterministic exceed playclock times  policy learner faces stationary mdp deﬁned  fore opponent adaptive distribu  features value space  tion effectively nonstationary technique                                                        rl feature usually means property states  value function learning longer guaranteed  verge paper focus stationary nonadaptive  instance gps location feature mobile                                                        robot set features union  opponents                                                                         let σ ≡ ∈ Σ state resulting learner’s joint values partitions  each state                                                    scribed uniquely terms features work  execution action state  actually state                                 opponent faces decision making state σ called use gamespeciﬁc features simply state space inor  afterstate reached different states der enable detailed learning each game                                                   learner result different actions usually Σ general game agent seen  × popular game playing systems learn paper consider smaller variants popular  values afterstates instead stateactions accordingly games tictactoe othello connect that’s mainly  learn qσ                                        ease experimentation presentation                                                    ijcai                                                     purpose transfer constrain identify game  independent features feature space nonetheless  correlated value function features abcd  transition structure afterstate gametree  certain depth gameindependent way    purpose feature game tree template efgh  lookahead state matches template feature  said active state feature generatedmatched  starting afterstate generated  learner current position opponent’s state shown ijkl  red square root each subtree figure  expand  ing game tree fully moves figure   features discovered learner tic  opponent followed learner tactoe game circlesquare terminal states  classiﬁes each node subtree win loss draw square circle meaning win loss learner crossed  nonterminal tree expansion determination square draw considered feature  node classes enabled game simulator using terminal node level  prolog based theorem prover learner generates                                                        terminal consequently identify winning moves  given game description nodes subtree                                                        step ahead  classiﬁed siblings class lowermost level                                                          training runs complete source game  coalesced step siblings higher                                                        case tictactoe extract feature information  level mid level figure  subtree                                                        acquired valuefunction space involves matching each  structure coalesced resulting structure                                                        afterstate subset Σ actually visited dur  feature does incorporate gamespeciﬁc infor                                                        ing source learning each discovered features  mation number moves available player                                                        using simulator lookahead afterstate σ matches  state semantics state figure  illustrates                                                        feature note value qσ feature  process extending scheme arbitrary number                                                        value feature fi calculated weighted aver  lookahead levels straightforward                                                        age valfi  avgwqσσ matches fiwherew                                                         weight associated σ specifying number times                                                        σ visited during source game experience                                   originalsubtree                                                        abstract features gametree space associated                                                        values source task assumption                                                        similar values target task                                                          feature values computed use                                                                   σ                        σ                                   lowestlevelcoalescing initialize   target game each matches                                   intermediatestep   fi qinitσvalfist σ matches fi each                                                        new σ encountered target game during startclock                                                        match look afterstates visited during pre                                                        ceding match afterstate visited pre                                                        vious match matched set features discov                                                        ered source game initialized                                   midlevelcoalescing                                                   λ                                   finalstep          match initialize default value nextthetd                                    producingafeature    updates according equation                                                           idea transfer mechanism save cost                                                        valuebackup steps near terminal states                                                        states gain predictive potential guide exploration  figure  illustration actual subtree rooted focus regions foresight usually  given afterstate matchinggenerating feature cir available way transfer learner behaves like  cular green nodes represent learner’s states solid red human learners  square nodes opponent’s states learner’s  states squares stand win learner characteristics feature transfer                                                        features depend exact game long    figure  shows  features discovered trans genre chosen games speciﬁcally size  fer learner tictactoe game note board number available actions each level  features distinct associated semantics semantics states actions winloss criteria  overlapping instance figure  jk really effectively abstracted away exploiting gdl  variants concept “fork opponent” learner’s sider diverse natures games aspects tic  results state opponent matter  makes learner win default initialization value δ average win  transfer learner needs check starting afterstate loss rewards case                                                     ijcai                                                     tactoe number available moves steadily diminishes  experimental results  connect diminishes intervals othello section report empirical results isolate im  actually increase winning criteria widely varying pact general gametreefeaturebased transfer scheme  games similar tictactoe connect variety games consider method  completely different othello key motivation success lead quicker andor better asymptotic  research develop simple techniques learning new games compared learning  transfer knowledge effectively game markedly new games scratch  different game focused high extracted feature values tictactoe game  level abstraction                                 source tested transfer learner  different tar    distinct leaftypes used features figure  games connect capturego othello connect  pend possible outcomes games variant connect board size  ×   acquired paper assumed games goal make line  instead  pieces capturego   possible outcomes viz win loss draw identiﬁed variant gomoku board size  ×   distinct rewards    respectively game match terminates player captures opponent’s piece  fers different set rewards −    following usual rules player  transfer learner create distinct leaftype each capture player larger terri  outcomes acquire features game tory wins just regular version othello follows  apply features previous games game rules regular game played smaller  needs provided equivalence relation board size  ×   maps rewards previous reward sets  games compared learning speeds base   game corresponds  previous games line learner transfer learner using feature knowledge                                                    acquired tictactoe baseline learner uses    worthwhile note games tic state tdlearning equation  value function ini  tactoe connect terminal player tialized uniformly default value comparison pur  cause win draw loss player poses isolate effect knowledge transfer  games othello player’s lookahead search compare lookahead learner  cause immediate defeat features discovered uses depth lookahead transfer learner  tictactoe naturally capture aspect figure  minimax search estimate value new afterstate  shows “win” nodes midlevel “loss” search nonterminal states leaf level eval  nodes lowest level transfer learner treat uated default value terminals level  games source consequently capture evaluated actual values value estimate  variety possible types features fact treat ev afterstate reached used initialize qvalue  ery game application domain previously ac tdlearning using method  learners  quired features end source new features equation   carry forward future games paper use three different types opponents  focus speciﬁc sourcetarget pairs learn spe  learners compete ggp framework  ciﬁc opponents study effects transfer controlled  experiments                                          greedy opponent uses small ﬁxed probability     concern using complex feature spaces trans exploration uses following policy  fer time overhead computing transfer knowl looks ahead turn seeks terminal nodes  edge overwhelm learning time having  takes winning moves avoids losing moves  small number features limiting depth lookahead wise plays randomly similar shortsighted  ensuring low computational complexity trans novice player  fer knowledge single source game serves                                                        random   opponent picks actions using uniform prob  target games time spent acquiring features                                                            ability distribution set available actions  amortized consider added complexity                                                            turn  target learning limited lookahead depth serves  features somewhat indicative outcome weak opponent opposite greedy player  subsequent moves note indication explores manner picks worst moves  unambiguous outcome figure decisive turns effect opponent plays randomly  speciﬁed knowing opponent’s disposition time vicinity terminal state  ambiguity justiﬁes transfer learning merely looking makes particularly poor decisions  ahead concrete idea ultimate outcome purpose considering weak opponent study  playing state irrespective opponent’s style fast different learners learn exploit certain  play initialized corresponding weaknesses opponent table  shows feature val  qvalue target game known value ues  features figure  computed trans  come minimax search experiments fer learner tictactoe game competing  actually show transfer learner learning faster rl each  types opponents note minimax  minimaxlookahead opponents            lookahead learner initialize afterstates                                                    ijcai                                                                                                                          table  values features figure  acquired  tictactoe game various opponents                                                                                                       feature id        figure  greedy  random   weak                                                                                                                                                                                                              withtransfer                                                                               withouttransfer                                                             withlookahead                                                                                                                    meancumulativeaverageperformanceoverruns                                                                                                                                                                                                           matchesplayed                                                                      figure  learning curves transfer learner baseline                                      learner rl lookahead  ×  othello                                     greedy opponent                                                                                                      matched features values              words initializations minimaxlookahead    learner accurate true values        states transfer learner assuming oppo  nent perfectly rational experiments learners’       parameter values α  γ  task  episodic λ  ﬁxed exploration probability                                                                                            withtransfer                                                                      meancumulativeaverageperformanceoverruns withouttransfer                                                                               withlookahead                                                                                                                                                                                                                                                       matchesplayed                                                                    figure  learning curves transfer learner baseline                                                                                             ×                                                    learner rl lookahead   capturego                                                        greedy opponent                                                                     learners converge othello game terminal                         withtransfer                 states game shallow games                        withouttransfer                         withlookahead           meancumulativeaverageperformanceoverruns  order verify learning rates weak ran                                                        dom opponent pitted transfer learner looka                                                         head learner each opponents othello                         matchesplayed                  game game challenging learners                                                        depth terminal states transfer learner used  figure  learning curves transfer learner baseline                                                        feature values learned each opponent matches  learner rl lookahead  ×  connect                                                        opponent learning curves shown fig  greedy opponent                                                        ures   opponents quite unlike    figures    show learning curves  learn minimaxlookahead learner assumes learning rate  ers greedy opponent transfer learner uses poorer transfer learner transfer learner  feature values learned player tictactoe learns values features learns  table  cumulative average reward  context opponent reuse pit   matches averaged  runs plotted ted opponent future note looka  number matches ﬁgures trans head learner used maxmax heuristic instead  fer learner outperforms baseline learner minimax learn faster weak opponent  lookahead learner ultimate winner assumption similarly avgmax heuristic random opponent  rational opponent realized case uses su random policy opponent learners  perior initializations compared transfer learner typically deal enormous sizes afterstate space  learners use fast td methods afterstate learn learn slower figure   ing learning rates high typically crossing  opponents previous experiments  formance level  matches thing experiments demonstrate knowledge transfer  note figure   matches insufﬁcient beneﬁcial addition baseline learner                                                    ijcai                                                     
