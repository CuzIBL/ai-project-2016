                               learning    coordination      classiﬁers                          yuhong   guo       russell  greiner       dale schuurmans                                     department  computing   science                                            university alberta                                   yuhonggreinerdalecsualbertaca                          abstract                          explicitly represented directed graphical model getoor et                                                        al   approaches learning multivariate      present new approach ensemble classiﬁca  classiﬁers include conditional random ﬁelds crfs lafferty      tion requires learning single base clas et al  relational markov networks rmns taskar et      siﬁer idea learn classiﬁer simulta al  maximum margin markov networks mn      neously predicts pairs test labels—as opposed taskar et al  methods led sub      learning multiple predictors single test labels— stantial progress learning classiﬁers make dependent      coordinating assignment individual la predictions test labels explicitly related      bels propagating beliefs graph data      argue approach statistically mo learning multivariate predictors exciting      tivated independent identically distributed problem focus classical iid case      iid data fact present experimental results paper demonstrate believe      show improvements classiﬁcation accuracy  surprising counterintuitive connection learning mul      singleexample classiﬁers range tivariate dependent predictions beneﬁcial idea      iid data sets set base classiﬁers like iid setting particular develop relational learn      boosting technique increases representational ing strategy classiﬁes test patterns connecting la      capacity controlling variance through prin bels graphical model—hence correlating subsequent      cipled form classiﬁer combination             predictions—even explicitly assumed train                                                        ing test examples iid                                                          explaining rationale approach    introduction                                       explaining dependent prediction makes sense  supervised learning far studied task iid setting note standard relational learning ap  machine learning research problem ﬁnite set proaches prms crfs rmns mns  observed training examples  xn yn pro naturally correlate predictions iid data tech  duce classiﬁer  → achieves small misclassiﬁ niques consider label dependencies explicitly  cation error subsequent test examples research serted hold true underlying model domain  tended adopt standard “iid” assumption training iid case dependencies asserted test  test examples independent identically distributed labels standard relational approaches reduce single  fact assumption fundamental theo label learning techniques univariate logistic regres  retical research topic anthony bartlett  sion support vector machines  characterizes standard learning methods—as exem proposing different intentionally add dependencies  pliﬁed fact machine learning methods clas tween test labels labels explicitly assumed  sify each test pattern isolation independently test independent underlying data generation process  patterns                                             surprisingly demonstrate correlating predictions    recently increasing attention paid advantageous  problems training test labels indepen introducing basic approach mo  dent instead strongly related example domains tivate justify technique three separate ways  speech tagging webpage classiﬁcation show predicting correlated test labels statistically  each wordtag webpagelabel depends tag label justiﬁed iid setting independence  proximal words webpages addition just features sumptions explicitly taken account fact show  immediate word webpage various forms “rela incorrect conclude learned predictor suf  tional” learning models developed handle ﬁciently treat test cases independent simply  kinds problems years notable example come iid source second show pro  work probabilistic relational models prms posed relational learning technique viewed nat  correlation class labels different instances ural generalization similaritybased learning techniquesmoreover viewed simple form ensem  xn yn given construct set  ble learning method advantages standard pairs set xixj  yiyj supply  approaches show empirically proposed conventional training set learning predictive model  method achieve improvements classiﬁcation accuracy yiyjxixj φ data experiments  range iid domains different base learning al ignore duplicate pairs include orderings  gorithms                                             each distinct pair ensure learned model sym                                                        metric training data constructed parame    learning   coordinated   label  predictors         ters model φ estimated way                                                        univariate case using maximum condi  begin simply introducing learning method tional likelihood maximum posteriori estimation prin  attempt motivate thoroughly initially ciple example given linear logistic representation  focus using probabilistic classiﬁers yixi θ use analogous linear logistic representa  brieﬂy consider extension nonprobabilistic classiﬁers tion yiyj xixj φ joint feature space xixj  section                                      training models estimation prin    iid setting standard approach probabilistic ciple using different related data sets  classiﬁcation learn univariate model yx θ coordination model learned way generally  serts conditional probability distribution single clas make independent predictions yi yj ex  siﬁcation variable given input pattern θ rep tended parameters φ constrained enforce indepen  resents parameters model given repre dence expect model learn make  sentation key steps building univariate pendent coordinated predictions labels  classiﬁer ﬁrst learn speciﬁc predictive model corresponding input patterns interestingly learning coor                                    θ given training data     yn based dination classiﬁer advantage potentially squaring  using principle maximum conditional likelihood number available training examples  maximum posteriori estimation given set test advantage mitigated subsampling increase         x∗    x∗                  x∗  patterns   classiﬁes each independently complexity model learned  computing label yˆi maximizes estimated condi                                    x∗  tional probability yˆi  arg maxy  θ natural ex  classifying test data coordination  amples approach learning naive bayes classiﬁers given coordination classiﬁer require principle  friedman et al  logistic regression classiﬁers hastie classifying individual test patterns x∗ fact problem  et al  kernel logistic regression classiﬁers zhu classifying test patterns involved  hastie  sigmoid network classiﬁers neal  case approach consider set training  bayesian network classiﬁers greiner zhou                                      x∗    x∗                                                        examples     yn test patterns      approach different instead learning univari                                         x∗                                                        classify each test pattern  ate classiﬁer predicts single label instead isolation instead seek classify test patterns depen                                              propose learn pairwise label classiﬁer yiyj φ dent manner perform classiﬁcation proceed three                                             takes arbitrary pair input patterns  stages reminiscent conditional random ﬁelds  asserts conditional joint distribution pair la construct graph test training labels  bels yi yj example classes say graph constructed use learned coordi    pairwise classiﬁer assert condi nation classiﬁer yiyjxixj φ assign “potentials”  tional probability possible pair labelings yi yj  ∈ possible labelings each edge   potentials                                                                                            given input patterns used deﬁne markov random ﬁeld test label      general pairwise predictor does assume assignments establishing joint probability distribu                                  yi yj independent given tion labelings finally compute joint labeling  assumed independent true model ∗ ∗                                                          ym test examples maximizes ap  present justiﬁcation section  refer proximately maximizes joint label probability  pairwise label classiﬁer form “coordination scribe each three steps  classiﬁer” highlight fact attempts model                                                          deﬁning graph construct graph  coordination appear labels yi yj                                     ∗  ∗  given input patterns xi xj given alternative rep consider edges connect pair test labels yi  yj                                                                                    ∗  resentation yiyjxixj φ main test label training label yi  yj  training  processes ﬁrst training coordination classiﬁer data make use edges training  using label test patterns             labels                                                          classify test patterns simplest approach concep    training coordination classiﬁer                tually consider complete graph connects each    coordination classiﬁer doubles number input fea readers probably objecting point given  tures squares number output classes orig iid assumption new information gained  inal univariate classiﬁer despite increase model com example pairs present original  plexity training coordination classiﬁer remains concep examples section  argues conclusion gener  tually straightforward assume standard training sample ally incorrect machine learning context         ∗                                                                          test examples  test label yi test training labels  usually impractical consider     true  mm  −    test pairs ignoring duplicate pairs     conditional                   reduce number edges adding        model  restrictions natural alternatives consider  connecting each test label training labels  analogous standard similarity kernel  based learning methods ii connecting each test label   learned  test labels surprisingly gives best results conditional               experiments iii connecting each test label model  training test labels reduce overall  number edges uniformly subsample edges sub                                                        figure  iid setting true test labels  ject different restrictions                 independent given true conditional model    deﬁning potentials graph   independent given learned estimate model  structed assign potentials conﬁgurations  each edge cases depending   ∗  x∗x                                                        yi yj φ plays role generalized similarity  edge connects test labels test label training              ∗                                                                           measure classifying yi terms     yn  label                                               ∗  ∗     difference coordination model learned    edge connects test labels yi  yj  earlier training phase ﬁxed                              ∗  ∗       ∗ ∗ x∗x∗  simply assign potential ψyi  yj   yi yj   φ hand  given learned coordination classiﬁer            remaining cases ii iii difﬁcult    edge connects test training label introduce edges test labels causes la    ∗                                              ∗   yj  assign unit potential singleton node  bels dependent surprisingly                                  ∗               given conditional probability yi given yj exploiting test label dependence actually improve clas  assign                                             siﬁcation accuracy test data known                                        ∗    ∗          iid main points paper       ∗          ∗     ∗            φ  ψ         φ              models computing maximum probability assignment   yj                              x∗x                                     yyj φ  hard graph contain loops cope                                                                                          problem performing probabilistic inference  potential assigned singleton y∗                      ∗                               complex graphical model use loopy belief propagation  remove edge yi  yj  graph efﬁciently compute approximate solution murphy et al  sulting graph edges test labels possibly  ﬁnd gives adequate results  combination singleton potentials nodes y∗ pair                        ∗  ∗                wise potentials edges yi  yj     potentials assigned  rationale discussion  ﬁne joint probability distribution node labelings presenting experimental results important  manner markov random ﬁeld taking product explain rationale technique suggest  form                                                  coordinated classiﬁcation makes sense iid setting                                                          given assumption training test data                           ∗    ∗                     ∗        ∗  ∗     dependent proposing predict test labels building               ψ        ψy                        yj               graph asserting joint potentials pairs labels                             ji             learned coordination classiﬁer using belief propaga                                                        tion make dependent predictions does make sense  normalizing appropriate constant                                                        make dependent predictions iid labels turns    computing labeling finally given joint probability approach justiﬁed taking independence  distribution deﬁned markov random ﬁeld goal assumptions account figure  illustrates basic argu  compute joint test pattern labeling maximum ment standard machine learning setting true  probability interested computing given correct model generating iid data label  maximum  probability assignment ignore normal input pattern independent label  ization constant depending edge model pattern note requires knowledge  use different implications                correct model correct structure    assuming model test labels connect train rarely case classiﬁcation learning instead given  ing labels pairwise potentials markov estimate true model obtained training data  random ﬁeld completely factored case remain dependent figure  clearly shows  computing maximum probability assignment easy context supervised learning generally case  determined independently each test pattern es test labels dependent given learned model fact  sentially removing testtest edges reduces technique obvious supervised learning algorithms correlate  classical method each test pattern classi labels training data observation simply  ﬁed independently learned coordination model principle applied test data  using relational technique iid problem propagation order label test instances  appear awkward known precedent method robust approximations  machine learning research transductive learning vapnik like running single iteration loopy belief propaga   zhu et al  transduction learner knows tion just taking local votes products  set test patterns exploits knowledge  make predictions ultimately dependent fact  experimental results  idea exploited recent approaches semi  supervised learning using markov random ﬁelds zhu et al implemented proposed coordination classiﬁcation   proposing general framework ex technique different forms probabilistic classiﬁers  tending standard probabilistic learning algorithms trans using various standard iid data sets intent  ductive similar fashion                         determine approach merit    method motivated noting robust alterations classiﬁers data sets ex  natural extension standard ideas supervised iid clas periments conducted standard twoclass benchmark  siﬁcation observed learning coordination classiﬁer data sets uci repository data sets used  yiyjxixj φ natural generalization learning meth  australian  breast  chess  cleve  corral   ods use similarity measure kxi xj  classify test crx  diabetes  ﬂare  german  glass  heart          x∗                    x∗        x∗         hepatitis  mofn  pima  vote  examples based similarities      training patterns fact corresponds graph experimental results obtained fold cross vali  choice connects test labels training dation repeated  times different randomizations  labels coordination classiﬁcation extends standard sim graph structures tables plots report averages  ilarity based approach ﬁrst learning patterns predict results standard deviations included tables  dependencies labels using standard methods ap table  figure  show results ﬁrst exper  plied novel way correlating test predictions iment case implemented standard logistic  graph recent work gression model using unaltered input features learn base  learning kernels classiﬁcation lanckriet et al  coordination classiﬁer yiyjxixj φ classiﬁcation  transductive learning kernels xu et al  performed running loopy belief propagation test  far formulations remained hard extend labels stabilized usually   iterations ﬁrst  apply practice                                    experiment used graph test labels deter    interesting view coordination classiﬁcation introducing label dependency  novel form ensemble method label beneﬁcial effect “edge” results subsampled                x∗                                      testtest edges uniformly random overall density  test pattern computed combination votes  multiple predictors associated different test  edges test example table  figure  show                  ∗                                   sulting misclassiﬁcation error obtained coordination clas  training patterns xj  fact remotely connected pat siﬁcation comparison learning standard logistic regres  terns inﬂuence classiﬁcation belief propagation                                                        sion model yixi θ notable reduction    ensemble method coordination classiﬁcation overall misclassiﬁcation error → signiﬁ  useful features requires training improvement data sets breast  diabetes  single base classiﬁer yiyjxixj  φ multiple  mofn  pima  minor increase  base classiﬁers trained perturbed data second data sets cleve  corral   boosting bagging coordination classiﬁcation increases table  compares error coordination clas  representational capacity original univariate classi siﬁcation boosting base logistic regression model  ﬁer given classiﬁer representation single label  θ used  rounds adaboost freund                                                           yi θ mentioned previously coordination classi schapire   combining approximately             ﬁer yiyj φ doubles number input features number votes test pattern coordination  squares number output classes addition pre classiﬁcation experiment shows ensemble  diction test label principle depend training method coordination classiﬁcation performs competitively  test patterns course simply increasing representa case advantage coordination classiﬁcation  tional capacity base classiﬁer increases risk overﬁt needs learn single base classiﬁer opposed  ting advantage ensemble methods multiple training episodes required boosting need  resulting classiﬁer complex “smoothed” run loopy belief propagation output labels disad  principled form model combination helps avoid vantage  overﬁtting exploiting added representational complex investigate robustness method repeated  ity process model combination used previous experiments using different base classiﬁer ta  reduce variance learned predictor case ble  figure  show results experiment using  base model combination principle inference naive bayes instead logistic regression base clas  markov random ﬁeld fact coordi siﬁcation method results strong  nation classiﬁcation competitive ensemble technique ﬁrst case tried credible note    biggest drawback coordination classiﬁcation boosting obtains larger improvements larger  need perform probabilistic inference loopy belief losses classiﬁcation coordination appears fairly stabletable  comparison average misclassiﬁcation error  table  comparison average misclassiﬁcation error   uci data using logistic regression base model ∆  uci data using naive bayes base model ∆  aver  average improvement base                        age improvement base              base  boosted ∆  std  edge   ∆  std                 base  boosted ∆  std  edge   ∆   std   australian                australian                 breast                     breast                        chess                            chess                    cleve                      cleve                      corral                           corral                       crx                       crx                       diabetes                   diabetes                      ﬂare                        ﬂare                      german                     german                    glass                    glass                   heart                     heart                      hepatitis                    hepatitis                  mofn                    mofn                 pima                      pima                      vote                            vote                        average                   average                                        base vs boosted                                        base vs boosted                                                                                                                                                                                                                                                                                                                                                                                                                                                                               classification  error base                         classification  error base                                                                                                                                                                                                                            classification error boosted                       classification error boosted                          base vs edge                                           base vs edge                                                                                                                                                                                                                                                                                                                                                                                                                                                                               classification  error base                         classification  error base                                                                                                                                                                                                                             classification error edge                          classification error edge    figure  comparison average misclassiﬁcation error figure  comparison average misclassiﬁcation error  uci data sets using logistic regression plot base model uci data sets using naive bayes plot base model  versus boosted logistic regression plot base model versus boosted naive bayes plot base model versus  versus “edge”based coordination classiﬁcation       “edge”based coordination classiﬁcation
