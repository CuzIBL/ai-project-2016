          ensembles partially trained svms multiplicative updates                                       ivor tsang      james kwok                             department science engineering                       hong kong university science technology hong kong                                         ivorjameskcseusthk                          abstract                            simpler approach use multiplicative updates                                                        ﬁrst explored cristianini et al      training support vector machines svm convergence sensitive learning rate      volves quadratic programming problem  ﬁxed manually learning rate large      optimized complicated numerical solver shooting oscillations occur small      paper propose simpler approach convergence slow      based multiplicative updates idea ﬁrst second problem multiplicative update rule      explored cristianini et al  work hardmargin svm      vergence sensitive learning rate noisy data hardmargin svm poor performance      ﬁxed manually update rule  softmargin svm trade complexity      works hardmargin svm known     training error usually preferred choice      poor performance noisy data pa tradeoff parameter zero inﬁn      show multiplicative update svm ity critical various procedures pro      formulated bregman projection prob   posed determination using crossvalidation      lem learning rate adapted au generalization error bounds evgeniou et al       tomatically connection  typically computationally expensive      boosting bregman distance show      possibility avoids parameter tuning com      multiplicative update svm pletely using ensemble approach kim et al       garded boosting weighted parzen window   combines multiple svms different parameters ob      classiﬁers motivated success boosting tains improved performance lot svms      consider use adaptive ensemble  trained approach expensive      partially trained svms extensive experi                                                          paper ﬁrst address learning rate problem      ments show proposed multiplicative update                                                        formulating multiplicative update svm bregman      rule adaptive learning rate leads faster                                                        projection problem collins schapire       stable convergence pro                                                        shown learning rate adapted automatically      posed ensemble efﬁcient training compa                                                        based data connection      rable better accuracy besttuned                                                        tween boosting bregman distance collins schapire      softmargin svm                                                         kivinen warmuth  show mul                                                        tiplicative update regarded boosting algorithm                                                        weighted parzen window classiﬁers base    introduction                                       hypotheses each base hypothesis added ensemble  kernel methods support vector machines   partially trained hardmargin svm boosting  svms highly successful machine learn literature wellknown ensemble performs  ing problems standard svm training involves quadratic better single base hypothesis address  programming qp problem solved com possibly poor performance hardmargin svm  plicated numerical solver generalpurpose sider using ensemble partially trained svms  qp solver used inefﬁcient particu prediction note comes little extra cost mul  lar type qps consequently lot specialized optimiza tiplicative updating procedure experimentally ensemble  tion techniques developed popular ap observed comparable better accuracy  proach using decomposition methods sequential besttuned softmargin svm  minimization optimization smo platt whichin  rest paper organized follows learn  volves sophisticated working set selection strategies advance ing rate problem multiplicative update addressed sec  caching schemes kernel matrix gradients tion  connection update process  heuristics stepsize prediction              boosting proposed ensemble partially                                                    ijcai                                                    trained hardmargin svms discussed section   bregman projection  experimental results presented section  section assume access optimal  section gives concluding remarks                value ρ∗ return issue determining ρ∗                                                        section                                                                                                multiplicative updating rule svm                  α  ∈ pm   α  ∈    α ≥  α                                                         dimensional probability simplex natural choice breg  paper focus particular variant hard man distance unnormalized relative entropy  margin svm called ρsvm sequel th iteration minimize bregman distance  introduced section  develop sections                                                                                           new α current estimate αt α αm                                                                               ∂uα ρ∗   iterative multiplicative updating procedure based  α          α         α k˜ α − ρ∗  bregman projection collins schapire  requiring satisfy   ∂α                 kivinen warmuth  convergence similar constraint                                                         following entropy projection problem  erative procedure shown section                                                                  xm “     α            ”                                                            min      α ln  − α   αt  ρ∗  αk˜ α     hardmargin   ρsvm                                                                                                                              α∈pm           α                                                                    given training set xiyiiwherexi ∈ yi ∈                                     ρsvm ﬁnds plane fxw   ϕx kernel   introducing lagrange multipliers constraints ρ∗   induced feature space feature map ϕ separates αk˜ α α                               ρw                             shown remaining  classes maximum margin                      constraint α ≥  inactive lagrange multiplier                                                      needed setting derivative the lagrangian   max ρ −w      yiw ϕxi ≥ ρ          wρ                                                          αi            ∗     ˜                                                                  αi ln −αi αi −ηt ρ − α kαt   −λα  −                                                                  αi  corresponding dual                                                                                                                   wrt α zero αi  αi exp−ηtyiftxi                                                        λ                                           α              αk˜ α    α ≥   α                      using   substituting               minα                                                     λ     −    η           η                                                         obtain       ln  twhere                                                            αt    −η     vectors zero respectively α  exp                                                ˜  ααm vector lagrange multipliers                                                                        α        α      −ηtyift xi zt ηt        yiyjkxi xj  easily shown                        exp                        xm                    xm                     update commonly known multiplicative           αiyiϕxi fx   αiyikxi   date cristianini et al  exponentiated gradient                                                kivinen warmuth  ηt playing role                                                        learning rate notice initializing α   zero duality gap objectives   α                                                               subsequent iterations mentioned  equal optimality                        earlier α ≥   inactive constraint finally                      ∗    ∗   ∗                       shown ηt obtained dual                      ρ  α   k˜ α                                                                                                         ∗                                                                     max  − lnztηtexpρ ηt          denote                                                              ηt                         ˜                    uα ρ¯ α kα  − ρ¯α  −                mentioned section  similar multiplicative updat                                                                              ing algorithm standard hardmargin svm ex       ρ  ¯ lagrangian multiplier dual constraint plored cristianini et al  learning rate  α       karushkuhntucker kkt condi η adaptive performance sensitive  tions lead                                                       η                        ρ                                                       manual choice  hand svm vari                       ∂u α ρ∗                        ant value ηt automatically determined                ≤ α∗⊥       ¯    ≥                           ∂α                        improved convergence properties adaptive scheme                                 α∗                     experimentally demonstrated section   ⊥ denotes vectors orthogonal                                                          estimating value ρ∗   ∂uα ρ¯                                                                                   ∗            k˜ α − ρ¯ fx  − ρy¯ fx  − ρ¯ return question estimating ρ  recall     ∂α                                                                                ∗                                                        initialize α α     ρ equal                                 ρ                      optimal dual objective minimized  using  optimal ¯ dual variable    ∗  dual equal optimal primal variable ρ optimal α yield smaller objective                                                                                    ˜                                                        α equal ρ¯                          ρ∗   ρ∗                                 ∗                     ∗                        ¯                              ρ ≥    ρ ∈  ρ¯                                                          following consider using decreasing sequence    ρ∗   ρ∗α∗   α∗k˜ α∗  ρ∗                                                   ∗  ¯ ¯                    using    ρt’s ρ¯ ≥ ρt ≥ ρ  consequently constraint                                                 ∗                             α          variables optimality denoted superscript  paper initialize                                                      ijcai                                                                                    ∗      ˜   replaced ρt ≥ ρ  α kαtandthe    algorithm  training ρsvm  entropy projection problem                      input  xiyiim tolerance           xm “                  ”                                                             αi                       min      α  ln    − α  α       ρ ≥ α k˜ α          initialize  αi mand                                      α∈pm           α                                                ˜                                                      ρ¯  αkα             ˜                                                             ft xi  let ρ¯t ≡ αtkαt ﬁrst consider feasibility   use  compute                                                            set ρt ¯ρt  t  proposition   t  ifρ¯t satisﬁes                                                          set ηt argminη≥  ztηexpρtηifηt                        − t   ∗                   ρ       ≥ ρ                            t  t goto step  test t                   ¯t                                                                                  small ηt thensett  −  terminate                                   ρ¯t     ˜                  α ∈ pm              ≥  α kαt            update lagrangian multipliers αi’s using   exists    t                                                                   αt   αt    −η    η              ˜               ˜                                   exp    compute  proof  wehavev  kv  ≥  vector               ˜                ∗                                           ρ¯t  αtkαt   αt − α                                                          ρ¯t ≥ ρ¯tthent  t ← goto           α k˜ α ≥ α∗k˜ α − α∗k˜ α∗                                                 step  t  t check t    condition                                 t ≥  goto step  set  −          − t    ∗        ˜    − t   ∗ ˜ ∗             terminate      ρ¯t     ≥  ρ   ⇒   αtkαt       ≥  α  kα                                   t                    t                      output svm         summing                     ρ¯t     ∗ ˜                       ≥ α  kαt                      convergence                 t               ∗                                        section design auxiliary function  α  α satisﬁes condition                                                        used lower bound loss decreases                 −t    ∗    ρ¯t  ≥ ρ  feasible solution  ex each iteration collins schapire  denote rela                                                                                                     · ·            αt              α                         tive entropy bregman distance Δ   ists set   optimal  notice                      ∗  differs  equality constraint   optimal α solution  intersec  inequality constraint optimization tion hyperplanes deﬁned linear constraints                                                               ˜   exactly section  ρt ≥ α kαt t’s using     optimization ηt extra constraint η ≥               ˜       ∗ ˜                                  ˜                                  αtkαt  ≥ α  kαt                   subsequently set ρ¯t  αtkαt ρt ac  cording                                            decrease loss consecutive iterations                            ρ¯t                      ρ                                        α∗ α   −   α∗ α                                                      Δ       Δ                                                                                                                       ˜                         kkt condition  ηtαtkαt − ρt              ∗                                                                                   αi  ln αi   − ln αi   αt  αt                     ηt   leads                                               ˜                                                           αtkαt    ρt                                                                                         −η     α∗y   −    η  using                                                    ln using                                                                                    α  k˜ α                                ρ¯t                        m       cos ωtt  √ √                                                            ρ¯t ρ¯t  t   ρ¯t               ≥−ηt       αi   yiftxi − ln ztηtusing   ωtt angle wt wt recall                                                                   m                         entropy projection procedure used solve dual            starts converge angle ωtt tends   αi     ln αi   − ln αi    using   zero cos ωtt → ast  ifρ¯t  ρ¯t         optimization progress deteriorated  implies current t large overshot  solution discarded case  ∗     ∗                                                         Δα   αt − Δα  αt ≥ Δαt αt ≡ aαt   infeasible instead relax constraint   setting t ← t obtain larger ρt repeated a· auxiliary function a· increases                                   ∗  ρ¯t ≤ ρ¯t note ρ ≤ ··· ≤ ρ¯t ≤ ρ¯t bounded zero Δ· · ≥  sequence                ρ∗    α∗ k˜ α∗    α∗                                aαt’s converges zero α ∈ pm compact                 optimal optimization                                    ∗                        αt    ρt                    continuity uniqueness α  limit  resumes new      process                                                  ∗             t                                        sequence minimizers converges global optimum α   iterates smaller termination threshold                                         complete algorithm shown algorithm         using   αt k˜ αt                                                                                                                            ρ¯t      ˜                      ˜     tρ¯t                                                           asα   kα   ¯ρtthenαt − αt kαt         ηt  process achieve global t                               t                                            ∗                                               ˜  optimal terminate algorithm αt  α  special case ftxi ∈ −  kαt∞ ≤                                                                                                      tρ¯t                                                                          αt − αt ≥     experiments use   initialize       ing h¨older’s inequality         t                                                     ijcai                                                                                                                                            apply pinsker’s inequality fedotov et al  identical αi playing role di compare   obtain                                             ηt role ct compare                                                        consequence fact boosting regarded                              t ρ¯t                αt αt                                                                                      Δ                                      entropy projection kivinen warmuth                                                  boosting perspective effectively using base hy             tρ¯t                                      pothesis form  α ∈ pm necessarily equal  let ν min   summing  each step                                                      α∗ base hypothesis regarded variant                                  t                    parzen window classiﬁer patterns weighted         ∗           ∗     Δα   α − Δα  αt  ≥     Δαt αt       α                                                                                                   note edge base hypothesis th itera                                                                                     ˜                                                        tion αiyi  αi yikxi xj   α kαt         ⇒     ≥    α∗ α   tν                 ln     Δ                                constraint  requiring edge                                                                                                   ∗  number iterations required  base hypothesis wrt αi distribution ρ    lnm                                                                                              ρ    ν  ﬁxed  gives faster ﬁnite convergence correspondence holds svm                                                                                               ∗           result linear asymptotic convergence smo al algorithm algorithm  adaboostν algorithm                                                       additionally ρt ρsvm analogous   gorithm lin                                             ∗                                                      adaboostν  ρsvm similar ν    assuming  satisﬁed following    ∗  convergence proof solution ρsvm’s dual adaboostν controlling approximation quality         ∗                                ˜            maximum margin note ν quite difﬁcult  αt → α   iterations αt kαt  →   ∗                                                                                     ρ  ﬁnal solution desired ρsvm classiﬁer set practice hand set adaptively                                                          wellknown connections    ensemble partially trained svms                 tween svm  boosting r¨atsch                                                         typically interested relating combined hypoth  close resemblance update rules esis svm fact boosting uses  ρsvm  variants adaboost algorithm partic norm svm uses norm hand  ular show section  algorithm sec                ∗                                       focus showing using weighted parzen  tion  ρ assumed known similar window classiﬁer base hypothesis hy  adaboost algorithm algorithm  section         ∗                                 ∗             pothesis svm  ρ unknown similar adaboostν algorithm  r¨atsch warmuth  inspired connection  using ensemble prediction  section  consider using output boosting mentioned section  hardmargin svm  ensemble improved performance time complexity hypothesis poor performance  considered section                     noisy data inspired connection boosting                                                        use boosting ensemble’s output prediction  algorithm  adaboost r¨atsch warmuth                                                                          t                                                                                ηt    input  xiyiim number iteration                                                                                                       t                                                                                      ηr    initialize di                                                                 convex combination partially trained     train weak classiﬁer wrt distribution di each svms note takes little extra cost evidenced       pattern xi obtain hypothesis ht ∈ −   boosting literature use ensemble     set                                              lead better performance experimen                  ct argminntcexp                                  c≥                           tally demonstrated section                                                                          computational issue       ntc    di exp −cyihtxi                                                                                                   update distributions di                          each step update takes om  time                                              excessive large data sets alleviated           di        di  exp −ctyihtxi ntct  performing lowrank approximations kernel matrix                                                        fine scheinberg whichtakesomr time    end                    t                                rank update computation ρt                                                                                            ¯    output                                                                                                          omr instead om  time                                                           experiments    ρsvm training vs boosting                       section experiments performed ﬁve small                                                                                                                                     αt   η                 breast cancer diabetis german titanic waveform  comparing update rules section                                                                                                            γ     dty   di ct adaboost edge hypothesis                                                                        ∗                                                           adaboostν algorithm   algorithm  replaced        −t    ∗     ∗      ˜     ∗     t       ρ¯t   ρ       ρ  ≤ α  kαt ρ                 minrt γr − ν  γt           ht      t                      −t                       edge   satisﬁes loose kkt condition ρsvm          httpidaﬁrstfraunhoferdeprojectsbenchbenchmarkshtm                                                    ijcai                                                                                                                                     seven mediumtolarge realworld data sets astroparticles                                                                       β     xi −   xj  each small data set  adult adult adult adult web web table                                                         table  comes  realizations performance                                                        obtained averaging realizations          tableasummaryofthedatasetsused             each large data set realization available                                                          ensemble compared following       data set dim    training patterns  test patterns       cancer                                        svm best bestperforming svm       diabetis                                      csvms obtained regularization path algorithm       german                                       hastie et al        titanic                                    svm loo csvm best leaveone      waveform                                    outerror bound evgeniou et al  ob        astro                                                                                  tained regularization path algorithm       adult                          adult                                parzen window classiﬁer ﬁrst hypothe       adult                                 sis added ensemble        adult                               hardmargin ρsvm hypothesis        web                            web                                 proposed ensemble                                                        implemented matlab run ghz                                                        pentium– machine gb ram    adaptive learning rate  ﬁrst demonstrate advantages adaptive learning table  testing errors  different data sets  rate scheme section  lack space                                                                    svm     svm    parzen    ρ   proposed  report results breast cancer waveform similar                                                          data set best  loo   window  svm    ensemble  behavior observed data sets comparison cancer                 train ρsvm using multiplicative updating               η                                          diabetis                    rule ﬁxed incristianini et al           german                        seen figure  performance cris titanic                   tianini et al  sensitive ﬁxed choice ηwhen waveform            η small η  smaller objective value astro                   creases gradually η slightly larger say adult                   estimate ﬁnally overshoot indicated adult               vertical lines convergence longer obtained adult               hand proposed adaptive scheme web                       verges faster                                      web                                                                                                                 seen table  ensemble performs                 ρsvmadaptive          ρsvmadaptive better hardmargin ρsvm accuracy               svmη             svmη                     svmη            svmη                                                      comparable better svmbest                 svmη              svmη                                                                                           svmη              svmη   table  compares time obtaining ensemble                                   objective                objective                running regularization path algorithm                                                            seen obtaining ensemble faster takes small                                                    number iterations illustrate performance large                                                                                            iterations              iterations      data sets experiment adult data set table            cancer              waveform         regularization path algorithm run large  figure  comparing update ﬁxed η data use instead νsvm using libsvmfor  adaptive scheme                                      comparison ν unlike parameter csvm                                                          alternatively ﬁnd besttuned svm performing                                                        grid search softmargin parameter each grid    accuracy speed                               point requires complete retraining svm regular  show proposed ensemble partially trained ization path algorithm usually efﬁcient hastie et al   hardmargin svms section  comparable perfor   inner qp regularization path algorithm solved  mance besttuned softmargin svm csvm use using optimization package mosek httpwwwmosekcom                                                           gaussian kernel kxi xjexp−xi − xj β   includes time computing kernel matrix                                                        expensive large data sets time computing    astroparticles         downloaded         leaveoneout bound included timing svmloo  httpwwwcsientuedutw∼cjlinlibsvmtoolsdatasets version  implementation   httpresearchmicrosoftcomusersjplattsmohtml httpwwwcsientuedutw cjlinlibsvm                                                    ijcai                                                    
