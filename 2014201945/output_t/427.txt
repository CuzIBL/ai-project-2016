        solving pomdps using quadratically constrained linear programs                   christopher amato      daniel bernstein    shlomo zilberstein                                      department science                                        university massachusetts                                            amherst ma                                    camatobernshlomocsumassedu                        abstract                          obtained inspecting manufactured products numer                                                        ous pomdp applications surveyed cassandra      developing scalable algorithms solving par        tially observable markov decision processes         developing effective algorithms mdps pomdps      pomdps important challenge ap       thriving ai research area cassandra      proach effectively addresses intractable feng hansen  feng zilberstein  hansen      memory requirements pomdp algorithms         littman et al  meuleau et al  poupart      based representing pomdp policies ﬁnite    boutilier  thanks new algorithms im      state controllers paper illustrate provements computing power possible solve      fundamental disadvantages existing techniques  large realistic mdps contrast current pomdp      use controllers propose new       exact techniques limited high memory requirements      approach formulates problem quadrat toy problems      ically constrained linear program qclp                                                          pomdp exact approximate solution techniques      deﬁnes optimal controller desired size                                                        usually ﬁnd nearoptimal solutions limited      representation allows wide range powerful                                                        memory optimal solution concise      nonlinear programming algorithms used                                                        current exact algorithms use dynamic programming      solve pomdps   qclp optimization                                                        require intractable space work      techniques guarantee local optimality                                                        make process efﬁcient feng zil      results obtain using existing optimization                                                        berstein  time space complexities pomdp      method show  signiﬁcant solution improvement                                                        algorithms remain challenges pomdp approxima      stateoftheart techniques results                                                        tion algorithms operate limited mem      open promising research directions solving                                                        ory consequence provide weak theoretical      large pomdps using   nonlinear programming                                                        guarantees contrast new approach      methods                                                        bounds space usage permitting principled search                                                        based optimal solution    introduction                                         current techniques used ﬁnd optimal ﬁxedsize                                                        trollers rely solely local information meuleau et al   early markov decision processes mdps poupart boutilier  present formulation  partially observable counterparts pomdps optimal ﬁxedsize controller represents optimal  widely used ai community planning pomdp policy given controller size illustrate  uncertainty pomdps offer rich language sit beneﬁts employ standard nonlinearly constrained  uations involving uncertainty domain stochastic optimization technique nonlinearly constrained optimiza  actions noisy observations variety possible objec tion active ﬁeld research produced wide  tive functions pomdp applications include robot control range techniques quickly solve variety large  simmons koenig  medical diagnosis hauskrecht problems bertsekas  quadratic constraints  fraser  machine maintenance eckles  linear objective function formulation belong spe  robots typically sensors provide uncertain cial class optimization problems robust  complete information state environment efﬁcient algorithms developed  factored planning process medi techniques guarantee locally optimal solutions new  cal setting internal state patient known formulation facilitate efﬁcient search  certainty machine maintenance problem lution space produces highquality results  earliest application areas pomdps seeks ﬁnd cost optimization algorithms employ advanced tech  effective strategy inspection replacement parts niques previously used meuleau et al   domain partial information internal state poupart boutilier  avoid convergence certain                                                    ijcai                                                    suboptimal points                                     given node variables xa xqao    rest paper organized follows ﬁrst maximize   overview pomdp model explain solu improvement constraints  tion represented stochastic controller brieﬂy                                                                     ∀svq s  ≤      xars  discuss previous work solving pomdps using              a                                                                                                                               controllers show represent optimal γ s oos q xqaov  controller using quadratically constrained linear program  qclp conclude demonstrating set large probability constraints  pomdps formulation permits higher valued ﬁxed   xa ∀a    q xqao  xa  size controllers generated previ                                                                    ∀axa  ≥  ∀q aoxqao ≥   ous approaches maintaining similar running times  results suggest using qclp formulation small                                                                                                     highvalued controllers efﬁciently large table  linear program bpi variables ao                                                                aq   qaq              assortment pomdps                                 represent                given node     background                                                                                                                    aq rs                                                                                                         pomdp deﬁned following tuple                                                                                                             γ  s oos q ov      s Ωo                                                        equation referred bellman equation    • ﬁnite set states designated initial state dis                   tribution                                          previous work    • ﬁnite set actions                                                        pomdp approximation algorithms    •                        ss         state transition model   transition developed discuss controllerbased approaches                      s                          probability state action taken state  focus work techniques seek    • reward model rs expected immediate determine best action selection node transition pa      reward taking action state             rameters ﬁxedsize controller    • Ω ﬁnite set observations                      poupart boutilier  developed method                                                        called bounded policy iteration bpi uses step    • observation model oosa probability                                             s     dynamic programming lookahead attempt improve      observing action taken resulting state pomdp controller increasing size approach    consider case decision making process alternates policy improvement evaluation  unfolds inﬁnite sequence stages each stage erates through nodes controller uses linear  agent selects action yields immediate reward program shown table  examine value proba  receives observation agent choose action bilistically taking action transitioning old  based history observations seen note controller improvement states  state directly observed beneﬁcial action selection node transition probabilities updated  agent remember observation history objective accordingly controller evaluated cycle  agent maximize expected discounted sum continues improvement bpi  wards received consider inﬁnite sequence guarantees maintain value provided  problem use discount  ≤ γ maintain ﬁnite troller likely ﬁnd concise optimal controller  sums                                                   heuristic proposed incorporating start    finitestate controllers used elegant way state knowledge increasing performance bpi  representing pomdp policies using ﬁnite mem practice poupart boutilier  extension  ory state controller based observation se termed biased bpi improvement concentrated certain  quence turn agent’s actions based state node state pairs weighing each pair unnormal  controller controllers address main ized occupancy distribution solving  reasons intractability pomdp exact algorithms set linear equations  remembering observation histories allow                                                        oqsbp qs  stochastic transitions action selection help    make limited memory singh et al  ﬁnite                                                                                               γ  qsao oq sp ap aqoos ap  state controller formally deﬁned tuple q ψ η  ﬁnite set controller nodes ψ  → Δa states nodes variables used previ  action selection model each node mapping nodes ously deﬁned probability bp beginning node  distributions actions η  × × → Δq repre state pair factor δ included allows  sents node transition model mapping nodes actions value decrease each node state pair  observations distributions resulting nodes makes changes parameters likely small  value node state given action selection node value lost note state pairs  transition probabilities aq qq given sult value higher start state node                                                    ijcai                                                                                                          taken calculating true value updating                                                        action transition probabilities bpi requires                                                        distribution nodes increases value                                                        states make improvements biased bpi sets                                                        equal weights each state preventing improvement                                                        allowing value lost using predetermined δ does                                                        guarantee controller improvement chosen                                                        values quality instead decrease                                                          likewise gradient calculation ga dif  figure  simple pomdp bpi ga fail ﬁnd ﬁculty ﬁnding optimal controller meuleau et  optimal controller                                 al formulate problem unconstrained heuristic                                                        adjust gradient ensure proper probabilities                                                        maintained example problem heuristics  instead decrease value pair heuris improve controller remain stuck general  tics increase performance difﬁcult adjust heuristic guarantee ﬁnding globally optimal solu  applicable domains                        tion essentially controller represents local maximum    meuleau et al  proposed approach methods causing suboptimal behavior  improve ﬁxedsize controller authors use gradient premise work general formulation  cent ga change action selection node transition problem deﬁnes optimal controller facilitates  probabilities increase value crossproduct mdp cre design solution techniques overcome  ated controller pomdp considering limitation produce better stochastic controllers  states mdp combinations states existing technique guarantees global optimality experimental  pomdp nodes controller actions results show new formulation advantageous  mdp based actions pomdp determin   general linear program used bpi allow  istic transitions controller observation seen controller improvement easily stuck local max  value resulting mdp determined ma ima authors suggest heuristics  trix operations allow gradient calculated unfortu stuck nonlinear approach offers advantages using  nately calculation does preserve parameters single step multiple step backup improve  probability distributions complicates search troller generally allow optimal controller  space likely result globally optimal solu set parameters appear better  tion gradient followed attempt improve short term inﬁnite lookahead deﬁned qclp  controller complex incomplete gradi representation predict true change value  ent calculation method time consuming error ga gets stuck local maxima meuleau et al  prone                                                construct crossproduct mdp controller                                                        underlying pomdp complex procedure calculate    disadvantages bpi ga                      gradient representation does ac  bpi ga  fail ﬁnd optimal controller count probability constraints does calculate  simple pomdps seen state true gradient problem techniques advanced  pomdp actions observation figure  gradient ascent used traverse gradient  transitions deterministic state alternating shortcomings remain  action taken state  action taken        state  state changes positive reward given  optimal ﬁxedsize controllers  negative reward given  formative observations given single node initial unlike bpi ga formulation deﬁnes optimal  state distribution state equal likelihood troller given size creating set vari  best policy choose action equal probabil ables represents values each node state pair  ity modeled node stochastic controller intuitively allows changes controller probabilities  value equal  notice example shows reﬂected values nodes controller  limited memory node stochastic controller ensure values correct given action selection  provide arbitrarily greater total discounted reward node transition probabilities quadratic constraints  deterministic controller size       bellman equations each node state added    initial controller deterministic chooses results quadratically constrained linear program al  action say bpi converge optimal controller difﬁcult solve qclp exactly  value initial controller state  r−γr−γ robust efﬁcient algorithms applied qclp  −r − γ state forγ common  simple gradient calculation intuitive represen  value negative each state based step lookahead tation matches common optimization models  assigning probability action raise sophisticated optimization techniques used solve  value state  lower state  qclps require resources commonly produce  node assumed value new action better results simpler methods                                                    ijcai                                                                                               variables xq aqo yq                     maximize                                       bsyqs                                                                          given bellman constraints                                     ⎡⎛               ⎞                                                        ⎤                                                                                                           ∀q yq  ⎣⎝    xqaqo⎠ rs aγ     ss oosa   xqaqoyqs⎦                                       q                        s                    q                                                                          probability constraints           ∀q    xqaqo                                                            qa                                                                                                                                                                                                 ∀q   xq aqo    xq aqok                                                       q               q                                                       ∀qaqoxqaqo ≥                 table  qclp deﬁning optimal ﬁxedsize controller variable xqaqo represents qaq variable yq              represents initial controller node ok arbitrary ﬁxed observation                  experimental results suggest pomdps table  describes qclp deﬁnes optimal              small optimal controllers approximated concisely ﬁxedsize controller value designated initial node              unnecessary use large maximized given initial state distribution neces              memory order ﬁnd good approximation sary constraints ﬁrst constraint represents bellman              approach optimizes ﬁxedsize controllers allows al equation each node state second              gorithm scale large problems using straints ensure variables represent proper probabili              tractable space rest section ties constraint guarantees action selection              formal description qclp prove optimal does depend resulting observation              solution deﬁnes optimal controller ﬁxed size seen                                                                    theorem   optimal solution qclp results                qclp formulation                                 optimal stochastic controller given size initial              unlike bpi alternates policy improvement state distribution              evaluation quadratically constrained linear program proof optimality controller follows bell              improves evaluates controller phase value man equation constraints maximization given node              initial node maximized initial state distribution initial state distribution bellman equation              using parameters action selection probabilities each straints restrict value variables valid amounts based              node aq node transition probabilities qq                                                           chosen probabilities maximum value              values each node each state  ensure initial node state represents opti              value variables correct given action node mal controller              transition probabilities nonlinear constraints added              optimization constraints bellman equa              tions given policy determined action selection  methods solving qclp              node transition probabilities linear constraints used constrained optimization seeks minimize maximize              maintain proper probabilities                    objective function based equality inequality                reduce representation complexity action selec straints objective constraints linear              tion node transition probabilities merged called linear program lp formulation                                                               linear objective contains quadratic constraints                qaq op aqp q               quadratically constrained linear program unfortunately                                                                                      problem nonconvex essentially means                   aq op aq                                                                  multiple local maxima global maxima                results quadratically constrained linear program ﬁnding globally optimal solutions guaranteed              qclps contain quadratic terms constraints wide range nonlinear programming al              linear objective function subclass gen gorithms developed able efﬁciently ﬁnd              eral nonlinear programs structure algorithms solutions nonconvex problems variables              exploit produces problem dif constraints locally optimal solutions guaranteed              ﬁcult linear program possibly simpler gen times globally optimal solutions              eral nonlinear program qclp formulation permits example merit functions evaluate current solution              large number algorithms applied           based ﬁtness criteria used improve convergence                                                                ijcai                                                                figure  hallway domain goal state designated star    problem space convex approximation  domain information methods ro  bust gradient ascent retaining modest efﬁciency  cases quadratic constraints linear objec  tive problem permits better approximations  representation likely convex problems  higher degree objective constraints    paper used freely available nonlinearly  strained optimization solver called snopt gill et al  figure  hallway domain mean values using bpi  neos server wwwneosmcsanlgov algo  qclp increasing controller size  rithm ﬁnds solutions method successive approxima  tions called sequential quadratic programming sqp sqp  uses quadratic approximations solved conducted different computers expect affects  quadratic programming qp solution gen solution times small constant factor experi  eral problem qp typically easier solve ments mean time biased bpi delta  quadratic objective function linear reported time each slightly higher  straints snopt objective constraints combined bpi  approximated produce qp merit function                                                          hallway benchmark  used guarantee convergence initial point                                                        hallway domain shown figure  introduced    experiments                                        littman cassandra kaelbling  frequently                                                        used benchmark pomdp algorithms consists grid  section compare results obtained using new world  states  observations  actions  formulation snopt solver bpi biased  squares robot face north south east  bpi ga implemented produced signiﬁcantly west goal square robot begins random  worse results required substantially time location orientation make way goal state  techniques saving space omit turning going forward staying place start state  details ga focus competitive tech goal state observations consist  niques bpi biased bpi implemented accord different views walls domain  ing original descriptions poupart boutilier observations transitions extremely noisy   poupart boutilier  choices loss goal reward  discount factor used            δ  parameter  biased bpi decreased performance results domain shown table  figure  second domain studied best value  controller sizes mean value produced     δ   experimentally trial error qclp greater produced each version  search centered value suggested poupart bpi biased bpi particular biased bpi δ             max    rs a−min   rs a−γ  boutilier  sa            sa                    improves bpi quality remains limited  note goal experiments demonstrate value optimal pomdp policy known  beneﬁts formulation used conjunction value optimal policy underlying mdp  “off shelf” solver snopt formulation represents upper bound optimal pomdp policy  general solvers applied fact  currently developing customized solver  advantage inherent structure qclp size qclphal  bpihal   qclpmac     bpimac  increase scalability                                           min     min      min     mins    domains used compare performance       min      min      min     mins  qclp versions bpi each method ini          min     min     mins   mins  tialized random deterministic controllers   mins  mins   mins   mins  port mean values times convergence slightly      mins  mins    mins   mins  increase quality qclp produced controllers    mins  mins    mins  mins  lower bounds added represent value  taking highest lowest valued action respectively table  mean running times qclp bpi  inﬁnite number steps experiments hallway machine problems                                                    ijcai                                                    
