   flexible unsupervised ppattachment method using semantic information                         srinivas medimi                          pushpak bhattacharyya               deptt sc  engg               deptt sc  engg            indian institute technology bombay        indian institute technology bombay                   mumbai india                         mumbai india                        msrcseiitbacin                           pbcseiitbacin                        abstract                          pps make difference example sen                                                        tence ate rice saladspoon attachments rice      paper revisit classical nlp prob  salad ate spoon approach consider      lem  prepositional phrase attachment pp                               −np   −p −np           contribution noun pp ppattachment fur      attachment given pattern                  ther previous methods focused      text verb np noun phrase  −  −  −                          np                                          structures consider        preposition  noun      attachments far away pps adjuncts      phrase question asked does − np               np                                     deal situation unsupervised approach      attach      question typically    large corpora biggest challenge data spar      swered using word world knowl   sity propose simple effective method data spar      edge word sense disambiguation wsdanddata      sity reduction dsrusingwordnet dsr helps smooth      sparsity reduction dsr requirements generalize low probability counts correct pp      ppattachment resolution approach    attachment basically determined semantic property      scribed paper makes use training data ex lexical items context preposition use      tracted raw text makes unsuper                                   − −            iterative graphbased unsupervised word sense disam      vised approach unambiguous                                                                − −                                        biguation wsd method simlar mihalcea                  tuples training corpus teach   exploits global semantic dependency interaction      resolve attachments                −   − −                         word senses ranks senses each word helps      ambiguous                 tuples test    correct ppattachment      corpus graph based approach word sense dis   remainder paper organized follow sec      ambiguation wsd used obtain accurate  tion  describes graph based iterative unsupervised word      word knowledge data sparsity prob  sense disambiguation method section  explains data      lem addressed detecting synonymy using  sparsity reduction process section  details unsupervised      wordnet ii doing form inferencing                                                    ppattachment method section  presents experimental      based matching sand unam      results section  concludes paper      biguous patterns −p −np np −p −np      experimentation brown corpus provides      training data wall street journal corpus test  word sense disambiguation      data accuracy obtained ppattachment res    ppattachment      olution close  novelty section word sense disambiguation      lies ﬂexible use wsd dsr phases    wsd method context prepositional phrase attach                                                        ment ppattachment approach iterative graph    introduction                                       based algorithm performs random walk sense                                                        dependency graph words involved context  correct ppattachment essential syntactic conse approach word sense disambiguation ppattachment  quent semantic analysis sentence example based hypotheses  prepositions semantic car  sentence lifted girl crane crane riers semantic relations derived associa                    pp  prepositional phrase   ppattachment ei tion prepositions  exploitation global semantic  ther lifted crane machine sense craneorgirl dependency words preferably proximity  crane bird sense crane word sense disambigua preposition help wsd during completion  tion ppattachment mutually affect each correct module work similar work generic graphbased  unambiguous assignment senses ppattachment approach sequence data labeling application  respectively evident example previous wsd published mihalcea   researches ratnaparkhi  donald hindle rooth   considered properties nouns inside wordnetprincetonedu                                                    ijcai                                                      graph based algorithm wsd                    observed graphbased iterative algorithm performing  perform random walk graph each context random walk context sense dependency graph assigns  sequence words supposed disambiguate each highest weight sense  writesenseforbook                                                        sense  literature process assigning weights  sequence − − − nsincep                                                        vertices given subsection   content word interaction basically   typical graph − − − sequence observed differences ﬁrst  shown ﬁgure  formally deﬁne graph follow sense write senses word literature                                                        slight example sense case literature  let given sequence words  www  wn                                                        interchangeably used  let each word wi nwi senses assume senses                                                                            wi          wi    sw         sw   word         wi  wi  wi            sense similarity  struct labeled graph ve each vertex rep  resents word sense labeled sense number similarity words computed using                                                        following criteria motivated variations  swi  vertex possible sense       sj                            original lesk algorithms lesk   longer  word   wi                  wi  dependency edges  pair vertices labeled sense depen sequence words match similarity   dency representing sense similarity concepts stop words shall ﬁl  explained later word senses word tered prepositions ﬁltered words  connected connected sense disambiguated associated prepositions  labeled sense vertices words preposition appears deﬁnitions words  labeled pairs related dependency           match extra weight assigned semantic similar                                                        ity sequence words match preposi                                                        tion similarity gets weight  normalization                                                        weights based length deﬁnition                                                        counter effect long deﬁnitions  pronouns similar                                                        forms treated sin                                                        gle entity  words having different derivational morpholog                                                        ical forms published publication considered                                                        similar                                                          labeling process graph vertices                                                        basic idea motivated pagerank algorithm brin                                                        page                                                            number links connected vertex                                                            important vertex                                                           incoming link important vertex                                                            link carries weight accordingly vertex                                                            receiving link highly weighted                                                        labeled graph given set weights wab associated                                                        edge connecting vertices va vbthe weighted                                                        page rank score determined given equation                                                                                                                                                                          wba  figure  partial labeled graph senses assigned words wpva− dd ∗                  wpvb                                                                                                  bc                                                                                       vc∈outva  sentence wrote book literature                              vb∈inva                                                                                                                                                                  equation  wpva value indicate stationary    labeling graph edges                         probability particular sense word  consider sentence wrote book literaturethe                                                          iterative algorithm vertex ranking  −n −p  −n  tuple wrote book literature words  disambiguated write book literaturethela algorithm consists three main steps  building  beled graph vnpntuple constructed described labeled dependencies graph  scoring vertices using  section  figure  shows labeled graph structure graphbased ranking algorithms  label assignment  sentence words sentence multiple erative algorithm vertex ranking using equation  sim  meaning words write bookandliterature ilar described mihalcea  difference  wordnet senses respectively simplic way ﬁnd similarity senses words  ity presentation sample labeled sense depen steady state weights relative weights senses  dency edges shown graph sense dependency graphbased algorithm ranks weights  similarity zero edge placed ﬁgure  experimented assigning multiple senses each word                                                    ijcai                                                      data sparsity reduction dsr process                  each having weight empirical count original                                                            tuple update appropriate frequency counts  words general polysemous data spar            ∗  sity context attributed nonexploitation counts   newly generated tuples  paradigmatic relationships words infeasi  inferencing step involves inferencing  ble collect possible combinations training examples tuples − − syntagmatic context based  large corpus paradigmatic contextual matching partly fully generate  lations exploited consider following sentences new tuples available training corpus  sample corpus                                   increase frequency count existing tuples    painted wall colour va meaning verb example − − − − exist      achment                                              − − − − nthenifv − − ni                                                                                               pn    paints wall red color va               exists       infer existence                                                              frequency count vpni added    coated wall colours va                 appropriately    paint room medieval scenes va                                                        three steps applied given order    coated building cracks na meaning observed signiﬁcant reduction dsr      noun attachment    coloured house distemper va                                                           unsupervised prepositional phrase  corresponding ppattachment tags given parenthe  sis                                                      attachment method    given data set words variants                                                        section propose unsupervised ppattachment  inﬂectional morphology painted paints paint                                                        method does require annotated data pro  synonyms colour color build                                                        posed method directly collects training examples  ing house room exploit observations                                                        text approach based hypothesis unam  establish relation verbs                                                        biguous attachment cases training data teach  painted coated given preposition attva                                                        solve ambiguous attachment cases test dataour  possibly ﬁnd higher probability equation                                                         approach motivated work ratnaparkhi ratna  instance available training                                                        parkhi  extent similar terms  corpus                                                        statistical modelling extraction training examples    nscenev    coated  att  va    introduced graph based word sense                                                        disambiguation data sparsity reduction point  data sparsity reduction dsr procedure described difference point difference employing                                                  training data reﬁnement process described subsection                                                            data sparsity reduction dsr                                                          resolve attachment ambiguous −n −p −n  use lemmatisation synset replacement paradig test instances using extracted unambiguous − −  matic substitution inferencing based syntagmatic − − cases training data example  context updation process verb attachment ambiguous ate rice spoon interpreted cor  given similar process followed noun attach rect unambiguous ate spoon incorrect unambiguous  ment substituting updation process verb rice spoon instances extracted − − nsare  attachment                                           reliable − − ns reliable particularly  following steps applied original training npnsinwhichp  − nsaretheppadjuncts  corpus each preposition modeling prnp va appear far away verbs actually attached verbs  attachment  independent nand  statitics changed                                                          collecting training data raw text    lemmatisation morphing  alemmatisation      dependent noun lemmatising tuples ﬁrst annotate text partofspeech tags using      similar add frequency counts tuples lt pos noun phrases verb phrases chunked      bmorph verb morphing tuples simi using simple chunker chunking replace      lar add frequency counts tuples   each chunk head words extraction heuristics                                                        applied extract unambiguous − − nsand    synset replacement each tuple corpus  − −      create new tuples weights proportional em           training instances given subsection       pirical counts each word synsets ﬁrst process tagging chunking extraction training ex                                                        amples given table       senses suppose       synw contv   number synonymous words        synw contn   number synonymous        product language  technology group  ltg      words ∗ number tuples generated edinburghttp  wwwltgedacuk                                                    ijcai                                                                                                          right word left nouns  table  process extracting training data raw text verbs disambiguated three strategies used                                                        ﬁne data         tools                 output                     set heuristics reliable unambiguous − −        raw text      professional conduct doctors ns based syntactic heuristics pick                      guided indian medical association reliable npns example  − −        pos tagger   dt professional jj conduct nn                    dt doctors nns vbz guided vbn subject like tube through doorway sentence                    indian nnp medical nnp association nnp   tube through doorway disturbs people         chunker     conduct nn doctors nns guided vbn  − − predicate sentence                             association                                          extraction heuristic  conduct ofn doctors form form ’be’ basinitem program                       guided byn  association  sentence important item program        morphology      conduct ofn  doctor                        guide byn association  tuples step  reﬁned using strong       synset addition  conduct ofn  doctor    ditions use wordnet ﬁnd semantic properties                      behaviorp ofn physician                    similarly   combinations words place time group                        guide byn association  finally slightly weaker conditions applied through                        direct byn association                     similarly   combinations limited statistical inferencing set highly cor                                                            rect − − − − tuples    heuristic extraction unambiguous training       training method       data                                             goal resolve ambiguous ppattachment instances  extraction heuristic exploits idea attachment using learnt knowledge unambiguous ppattachment  site preposition usually words left ambiguous tuple form vnpn  preposition heuristic following parame ambiguous training tuples form − −  ters                                                 − − deﬁne classier equation   window size maximum distance words                                                          vnpnarg    max   prvnpna  tween preposition nweusewin                           a∈nv   experiments extract                                                                                  − − parsed segments satisfy       factor prvnpna follows        •            preposition                               prvnpna        • form verb                     prv prn pravn  prp                                                                                                             • ﬁrst verb occurs words      np          left        •                                           factors prn prv  independent          noun occurs                                    •                                           tachment   need computed  estimation             ﬁrst noun occurs words pravn  prp    prn                                                                                                     right                                                                                  •                                           difﬁcult training data           verb occurs                   occur reason factors computed    − − parsed segments satisfy      using approximation equation           • preposition                                                          prann                                                                   pra  nvn ≈                                                                                        zvn        •  ﬁrst noun occurs words                          prav                                                                            pra  vn ≈          left                                                               zvn        • verb occurs words left                                                         zvnpra            nnpra                appears ensured preposition                                                         similarly approximate prp          subordinating conjunction whtype conjunc prn                                                                       given equations   respec          tion appears new verb seen    tively reasons approximations avoid        •                                           ﬁrst noun occurs words using counts vn seen                             right                                  extracted data        • verb occurs                                                               prp  nvn ≈ prp  nn                               − −         −                                                      unambiguous instances        sand                prp  vvn ≈ prp  vv   − ns extracted using heuristics particularly  − −  ns correct prnp  nvn ≈ prnp  nn  reﬁnement                                               prnp  vvn ≈  prnp  vv                                                                                                             reﬁnement training data                       approximated probabilities computed train  ﬁlter incorrect − − − − ing data medimi bhattacharyya weuseda  instances applying graph based wsd algorithm dis variant backedoff technique order smooth proba  cussed  features considered word bility computation                                                    ijcai                                                      experiments results analysis                           training datawe used brown corpus collecting    ambiguous training examples corpus size mb                                         syninf                                                                                                    wnsyn  sisting  sentences nearly  million  thousand   words frequent prepositions                                                            ppattachment  precision  preposition highly biased   noun attachment considered extracted unambigu      gwsrnk gwsrnk gwsrnk gwsrnkc gwsrnkc  ous distinct − − − − tuples number    respectively                               figure  performance summary flxuppattch dsrwith  testing data testing used penn treebank wall wsd combination senses assigned words  street journal data ratnaparkhi ratnaparkhi et al   standard benchmark data pp attachment used makes tradeoff coverage precision  groups  ratnaparkhi et al  collins case gwsrnkc increases precision  brooks  stetina nagao                decreases coverage proportionately ob  baseline consider unsupervised approach rat served inferencing consistently increases precision  naparkhi ratnaparkhi  baseline  performance evaluation baserpfurtherwe compared performance  tested performance extracted unam baseline using wsd applying dsr stages figure  biguous samples process basems      shows performance variation    proposed flexible unsupervised  ppattachment short flxuppattch experimented  performance flxuppattch stages dsr  wsd dsrwowsd ii dsr  wsd dsrwithwsd stages names  appear table       table  naming flxuppattch systems utilizing differ  ent dsr stages graph based wsd                        stages data sparsity reduction               morphing inferencing synset synset                                           inferencing   dsrwowsd  morph    infer    wnsyn    syninf   dsrwithwsd morphws inferws  wnsynws  syninfws                                                        figure         comparison      baselines                                                         flxuppattchgwsrnkc different stages dsr    wsd method provides ranks  senses each word experimented different schemes  sense assignment                                     performance morphing better                                                        performance ratnaparkhi ratnaparkhi     gwsrnk assign ﬁrst ranked sense             baserp gives indication better accuracy    gwsrnk ﬁrst highest ranked senses           extraction heuristics comparative performance                                                        best performing stateoftheart systems    gwsrnk ﬁrst three highest ranked senses         shown table     gwsrnkc ﬁrst sense  times ran      dom assignment second sense  times      random assignment sense              conclusion    gwsrnkc  times ﬁrst sense  times presented unsupervised method ppattachment      second sense  times sense       compares favourably existing state art  different senses assigned observed approaches method makes use lexical semantics  formance respect different stages inferencing through use wordnet employ  dsr process particulrly synsets synsets wsd limited way make use unambiguous  inferencing performance comparision given fig ppattachments learn resolution ppattachment  ure  case gwsrnk precision low case ambiguous − − − tuples  cause coverages tuples increases  starting point raw corpora method usable  introduces noise through wrong lexical entries annotation available clearly efﬁcacy method  net negetive effect precisison best performing depends richness presence − −  combination gwsrnkc fact − − tuples obvius future work consists reﬁning                                                    ijcai                                                    
