             feature selection kernel design linear programming                                 glenn fung romer rosales bharat rao                 siemens medical solutions  valley stream parkway malvern pa usa             glennfungsiemenscom romerrosalessiemenscom       bharatraosiemenscom                          abstract                          appropriate task dependency                                                        noted number approaches seek build      deﬁnition object data point similar functions particular classiﬁcation regression model      ity critical performance machine given training data cohen et al  xing et      learning algorithms terms accuracy al  schultz joachims  lanckriet et al       computational efﬁciency athitsos et al       case similarity function unknown chosen                                                          paper interested automatically ﬁnd      hand paper introduces formulation                                                        ing good similarity functions basic information      given relative similarity comparisons triples                                                        task provided      points form object like object                                                        expert user unlike approaches rely class labels      object constructs kernel function                                                        lanckriet et al  building sets points      preserves given relationships approach                                                        globally similar dissimilar xing et al       based learning kernel combination                                                        wagstaff et al  explore simplertospecify      functions taken set base functions                                                        form user supervision provided statements like ob      kernels formulation based                                                        ject similar object object remark      deﬁning optimization problem                                                        interested building functions      solved using linear programming instead semi                                                        consistent information provided user      deﬁnite program usually required kernel learn                                                        interested obtaining functions efﬁcient      ing show construct convex problem                                                        compute addition focus functions      given set similarity comparisons                                                        evaluated looking features input      arrive linear programming formulation                                                        dimensions implying form feature selec      employing subset positive deﬁnite matrices                                                        tion      extend formulation consider represen                                                          formally let represent objects      tationevaluation efﬁciency based formulating                                                                                             points xk ddimensional space         novel form feature selection using kernels                                                           like obtain function       expensive solve using pub                                                                  ×     →that satisﬁes similarity com      licly available data experimentally demonstrate                                                        parisons addition approximated      formulation introduced paper shows k˜  d ×d →                 dd      excellent performance practice comparing                   uses     components                                                        xk rely mercer kernel representations      baseline method related stateofthe art                                          approach addition efﬁcient cristianini shawetaylor  deﬁne similarly                                                           k˜           kx     α      α ∈      computationally                                   asakernel                                                                               mixture kernel functions pa    introduction related work                      work representation similarity func                                                        tions deﬁne convex problems obtaining later ex  deﬁnition distance similarity function tend idea focus k˜  motivation  input data space fundamental performance ma framework paper related earlier work metric  chine learning algorithms terms accuracy ef learning rosales fung  focus  ﬁciency easily veriﬁed looking role different problem kernel design  distance similarity plays common algorithms  kmeans nearest neighbors support vector machines example objects include database records user opin  kernel method concept ions product characteristics  similarity depends task objects note afﬁne combination kernels just  similar dissimilar ways depending appli vex combination restricting kj ker  cation similarity function appropriate task nel                                                    ijcai                                                       paper ﬁrst show relative constraints involv et al  cohen et al  schultz joachims   ing triples points form kxi xj kxi xk rosales fung  relationships  used learn think ﬁrst formulation ﬁned respect distances rankings use rela  parallel approaches learning rely super tive similarity constraints form kernel design offers  vised learning class labels unlike presented different challenges note interested pre  classiﬁcationalgorithm dependent formu serving absolute similarities general  lations involve solving semideﬁnite programming problem difﬁcult obtain absolute similarities imply relative similar  sdp graepel  lanckriet et al butweshow ities converse true  different linear programming formulation introduced important observation kernel design ker  paper sufﬁcient practice nel conditions implied relationships linear rela  efﬁcient computationally use solve tionships individual kernel entries kernel matrix  larger problems faster addition extend formu deﬁned points expressed  lation consider issue representationevaluation efﬁ linear constraints addition unlike meth  ciency based form feature selection formulation ods referred restricting kernel family  leads linear programming approach solving ker kernels depend original feature time  nel design problem optimizes succinct feature able learn kernels depend minimal set  representations                                      original features seen implicitly perform                                                        ing feature selection respect original data space    kernel design                                    combination beneﬁts new linear program  recent years idea kernel design learning algo ming approach formulation proposed paper  rithms received considerable attention machine unique set attributes signiﬁcant advantages  learning community traditionally appropriate kernel cent approaches kernel design  choosing parametric family wellknown ker  nels gaussian polynomial learning gen  notation background  erally suboptimal set parameters tuning procedure following vectors assumed column vec  procedure effective cases suffers tors unless transposed row vector superscript   important drawbacks  calculating kernel matrices scalar inner product vectors  large range parameters computationally pro dimensional real space d denoted xythe  hibitive especially training data large                                                         norm norm denoted x x  little prior knowledge desired similarity respectively column vector ones arbitrary dimension  easily incorporated kernel learning procedure                                                        denoted e zeros denoted bya                                    recently authors bennett et al  kernel matrix given set points denoted  lanckriet et al  fung et al  considered                                                        individual components kij  kernel functions  use linear combination kernels belong fam simply  ily superset different kernel functions parameters  transforms problem choosing kernel model  ﬁnding optimal linear combination members  linear programming formulation  kernel family using approach need kernel design  predeﬁne kernel instead ﬁnal kernel constructed ac  cording speciﬁc classiﬁcation problem solved  using linear combination kernels                                                                                                     paper use mixture model representation let say given set points xk ∈  approach does depend attached    appropriate similarity function  speciﬁc machine learning algorithm classiﬁca unknown expensive compute addition given  tion regression inference instead inspired fact information relative similarity comparisons  kernels seen similarity functions rely mally given set  kkxi xj    explicit conditions similarity values subset kxi xk kernel function kthekernelk  points original training dataset formulation known explicitly instead user able provide  allows user explicitly ask conditions similarity relationships sparsely example  satisﬁed reﬂected desired kernel function interested ﬁnding kernel function satisﬁes  learned                                              relationships    relative similarity constraints                    rest paper let suppose instead                                                        kernel deﬁned single kernel mapping  explained concentrate examples proxim gaussian polynomial kernel instead com                                                   ity comparisons triples objects type object posed linear combination kernel functions kjj   like object object choosing type rel kasbelow  ative relationships inspired primarily athitsos                                                                                k                                                                      α     note elements mixture functions   αjkjx            kernels                                                                                                                 ijcai                                                                                                                                                                                        α                                                Ω                       αikixi xj      pointed lanckriet et al theset     ij     ki∈Ω              set train  kx ykkx seen predeﬁned set ing points points used deﬁne triples                                                                                                 initial guesses kernel matrix note set Ω finally deﬁning auxiliary variables rij ∈  γ ≥  contain different kernel matrix models different γ ≥  formulation  rewritten linear program  parameter values goal ﬁnd kernel function ming problem following way  kα  satisﬁes relative kernel value constraints speciﬁed                                                                                                                                        α                                                                  min         t  γ  si     γ    user set  general formulation achieving      αsr                      ii  given                                                   st                                                                               α     α          min          γhkα                          ∀t ∈t     kij − kik  t ≥                α                                        ∀i   ji ∈s      −r   ≤ kα   ≤               st                                                                  ij    ij      ij                    α                   α                        ∀i ∈s     kα −         ≥       ∀i ∈t  xi xj t kxi     xk                             ii    ji ij              ∀tt                ≥                        ∀j ∈  k−sj         ≤ αj  ≤   sj                             kα                                     ∀tt                   ≥                                                                                                                                                                               t slacks variables indexes  function ∈ dimensionality αandsj ≥ note      α                                    α  hk   regularizer kernel matrix alter simplify notation overloaded variables                            α  natively kernel function  capacity control meaning clear context  parameter γ ≥  controls tradeoff order better understand motivation formulation  straint satisfaction regularization strength usually ob  important note  tained tuning note formulation sufﬁ                                                                      minimizing    kα   equivalent minimizing  cient optimize set values αi order obtain        ii                                             α                              α      positive semideﬁnite psd linear combination       rij kii ≥ rij  ∀i  k                                                          ji               ji    αjkjx suitable speciﬁc task hand                                                                                     ii implicitly minimizing rij  op  formulation requires solving relatively expensive                              ji  semideﬁnite program sdp enforcing αi ≥  ∀i fung et timal solution α∗r∗∗s∗ problem   al  lanckriet et al  restricting                                                         α                   kernels set Ω psd results psd             ∗     α∗                                                                      ≥ rij  kij   ∀i ∈si   strongly limits space attainable kernel          kα  functions                                            iii combining iandii obtain    instead using restrictions explore alterna                                                                                                    α∗       ∗          α∗  tive general deﬁnition allows consider kernel       ∀ikii ≥    rij     kij   function Ω αk ∈without compromising compu                                 tational efﬁciency requiring solve sdp                                                                               ∗  explicitly focus characterized sub implies kα diagonal dominant  family psd matrices set diagonal dominant  positive semideﬁnite  matrices order provide better understanding  motivation formulation present following  learning kernels depend fewer input  theorem stated golub van loan            features k˜   theorem  diagonal dominance theorem   suppose   modify formulation  order learn kernels         d×d   ∈       symmetric each  depend small subset original input features                                               far know existing direct objective opti                                                       mization methods feature selection kernels perform                    mii ≥    mij                     feature selection implicit feature space induced                          ji                          kernel respect original feature space                                                        ing using traditional kernels leads nonlinear noncon  positive semideﬁnite psd furthermore                                                        vex optimization problems difﬁcult solve  inequalities strict positive deﬁnite                                                        nonlinearity relations original features introduced  based diagonal dominance theorem matrices kernel mappings  positive diagonal elements simplifying nota order overcome difﬁculty propose simple              α      α  tion letting kij ≡ xi xj arrive following effective idea instead using kernels depend  alternative formulation                              features consider set Ω¯ comprised weak ker                    t                                  nels depend feature time example            minα    t  γ α                                                                                                                  xi   xj                  st                                                                                                  α     α                         vectors d weak gaussian kernel   ∀t ∈t   kij − kik  t ≥                                     α                 α   depending feature deﬁned            ∀i ∈t               kii  ≥     j∈sji kij                 ∀t≥                                                                                                                                                                                                                                  kfxi yj exp−μ  xi − xj                                                                                                                                                        ijcai                                                                                                      ¯  let denote set indices kernels ki ∈ Ω  depend feature ∈d          table  benchmark datasets  linear combination weak kernels Ω¯ written             pts ndimsd      classes                           d            α     α        kij   xi xj        αpkpxi xj                                 p∈if                        housingboston                                                                               ionosphere                          note αp  ∀p ∈ given fthisimpliesthat    α                                                      iris                                 kij does depend original feature motivates  formulation feature selection uses weak  wine                                                                                  balance scale                        dimensional kernels                                                                       α       breastcancer wisc                             min        t  γ   sf     γ                 αs                        ii      soybean small                                      st                                        protein                                                     α     α   ∀t ∈t     kij − kik  t ≥                  pima diabetes                                                          α            ∀i          −rij ≤ kij ≤   rij                            α                ∀ikii         −   rij ≥          ∀f∀p ∈           −sf ≤ αp  ≤   sf             evaluation settings                                    t ≥              datasets employed experiments generally                                                     used classiﬁcation class labels available  indexed feature number various methods compared  kernel number interesting note quire explicit class labels method introduced pa  each feature formulation  minimizing mf  requires relative similarity information subset  max αp ∈  appropriate       points clearly class labels provide information    mf     ⇒αp≤     ∀p ∈                       use available class labels generate set triples             ⇒   αp  ∀p ∈                         similarity comparisons respect classes explic                        α                               itly given randomly chosen set three points             ⇒∀i kij  does depend featuref                                                     training set belong class                                                        belongs different class place triple                                                                                                     numerical evaluation                               set  points class                                                        remaining point case xing et al  experimental evaluation used collection                                                       supervision form sets called similar set  publicly available datasets uci repository dissimilar set order identify sets  summary datasets shown table  datasets use class labels build similar set  commonly used machine learning benchmark pairs likewise dissimilar set pairs given level  performance evaluation choice datasets motivated supervision method attempts ﬁnd optimal ma  primarily use evaluating competing approach halanobis distance matrix sameclass points closer  xing et al  aimed learn distance functions each differentclass points xing et al     evaluate approach single kernels com details  paring standard gaussian various widths lin triple ∈tused approach  ear polynomial kernels kernels ones learning use ∈sand ∈dfor learning  used basis mixture kernel design rea xing et al wheres similar dis  sonable baseline comparison compare formu similar sets believe provides fair level supervi  lations formulation attempts perform implicit feature sion algorithms roughly information  selection deﬁning weak kernels uses provided possible obtain superset  kernel matrices depend input dimensions construction obtained     chosen compare formulation order evaluate performance various methods  proposed xing et al  obeys various rea use  split data training testing  sons addition stateoftheart method pri methods training required train  mary reason choice uses similar ing portion generate  triples explained  identical type supervision explained unlike actual training information provided appropri  related approaches code data ate representation algorithms testing repeat                            public method  addition method edly choose three points random class labels  performed constrained version kmeans wagstaff et al imply points similar each   task ﬁnding good clusterings         points class    httpwwwicsuciedu∼mlearnmlrepositoryhtml  different class label check correct    data experiments code xing et al  relationships learned points  downloaded httpwwwcscmuedu∼epxingpapers class similar closer each  class dataset  obtained thresholding median value points chosen random point  attribute                                     measure used algorithms deﬁne                                                    ijcai                                                     percentage correct simply proportion points     test set sampled random respect classimplied       similarity distance relationship                              method requires setting balancing parameters  ×  set using cross validation splitting training   set halves values tested parameters             −   −   −                     parameters       effect number dimensions employed                                                                 higher value γ favors using fewer kernels dimen  sions likewise larger values γ favors kernel matrices    gauss μ                                                                      gauss μ                                                                 smaller trace                                               poly degree                                                                     correct distance relationship  mix kernels                                                                   discussion results                                         mix weak kernels                                                                                                                                                            fig  shows performance approach compared                        dataset index  various gaussian polynomial kernels linear  kernel performed identically polynomial kernel figure  performance single kernels approachs  degree omitted graph mean uci datasets show instances approach  standard deviation performance measure each ing kernel matrices weak kernels bars show performance  dividual kernel computed  random samplings results  random splits trainingtest points training ap  dataset order consistent approaches plicable kernel mixtures performance measured terms  number samples used testing set  triples percentage randomly chosen points  test set    expect optimal mixture kernels provide distance relationship respect class labels number triples  higher performance cases weak kernel mix used training runs  error bars show standard  tures compared single kernels generally deviation  case seen ﬁgure cases single  predetermined kernels suboptimal case weak method effectively reduces number features  kernels performance better single kernels dataset  case mixtures kernels  cases single kernel provides higher performance fig  shows comparison present formulation                                                                                                  average reasons  simply using weak kernels xing et al  weshow  unlucky selection sampling test points datasets   percentage correct averaged  random splits data   standard deviation large justify onestandarddeviation bars each   possibility interesting reason  im splits  triples test set randomly chosen                                                        comparing performance methods note  posed restrictions solution space mixture kernel             dataset  recall concentrated solution dataset  method clearly outperforms  subspace psd matrices diagonal dominant competing approach interestingly dataset  matrices cone psd matrices incor optimal number dimensions determined  porated solution space kernel mixture perform equal original dimensionality  good base kernel mixture single kernel addition implicit nonlinear representations im  valid solution note possi plied kernels employed believe key reason  ble required pay higher computational superior performance mixture weak kernels  costs large datasets number constraints automatic identiﬁcation relevant dimensions    interestingly performance mixture weak ker duction dimensionality appears provide important ad  nels superior larger number vantage time generalization generally accepted  degrees freedom number α mixing parameters simpler representation preferable blumer et  larger feature selection effect overﬁtting al  reduce overﬁtting practice  case datasets considered computational efﬁciency perspective test time  paper results suggest able represent original data succinctly  case remark result theoretically guaran especially advantageous particular similarities  teed nonlinear interactions multiple dimensions calculated directly using lowdimensional representa  features representable using linear combina tion computational time savings signiﬁcant online  tion single features                              applications projection step approach pre    fig  shows average optimal number dimensions computed offline retrieval applications queryby  process fold cross validation experiment example objects stored lowdimensional  corresponding onestandarddeviation error bars representation conceptual point view formu  number dimensions identiﬁed counting number lation advantage providing effective  α’s larger  automatic choice dimension tool understanding data identify  ality using crossvalidation explained variables dimensions high low relevance task  valuable property method presented note                                                    ijcai                                                     
