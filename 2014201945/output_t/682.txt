        training conditional random fields using virtual evidence boosting                lin liao         tanzeem choudhury†             dieter fox         henry kautz                    university washington                            † intel research         department science  engineering                   ne th st                        seattle wa                               seattle wa                         abstract                          eral domains general guidance given                                                        mpl safely used mpl      conditional random ﬁelds crfs   observed overestimate dependency parameters      applied successfully variety domains experiments geyer thompson       training remains challenging task paper addition ml mpl performs feature selec      introduce novel training method crfs    tion explicitly able adequately han      called virtual evidence boosting simulta  dle continuous observations limitations make      neously performs feature selection parameter  unsuitable tasks activity recognition based      estimation achieve extend standard   real sensor data identifying set features      boosting handle virtual evidence ob useful classiﬁcation alternatively boosting      servation speciﬁed distribution successfully used feature selection context      single number extension allows classiﬁcation problems viola jones       develop uniﬁed framework learning local application relational data remains unsolved problem      compatibility features crfs experiments assumes independence hidden labels      synthetic data real activity classiﬁ paper show seamlessly integrate boost      cation problems new training algorithm  ing crf training combining capabilities      performs training approaches including max paradigms integration achieved cutting      imum likelihood maximum pseudolikelihood   crf individual patches mpl using      recent boosted random ﬁelds             patches training instances boosting key difference                                                        mpl framework neighbor labels    introduction                                       treated observed virtual evidences beliefs  conditional random ﬁelds crfs undirected graphical approach seen “soft” version  models developed labeling relational data laf mpl able avoid overestimating neighborhood  ferty et al  directly modeling conditional dependencies happens mpl  distribution hidden states given observations crfs paper three main contributions extend  make assumptions dependency structure standard boosting algorithm handle input features  observations crfs especially suitable classiﬁca virtual evidences form likelihood values  tion tasks complex overlapped observations deterministic quantities second based extended  training crfs large numbers features espe boosting algorithm present general approach training  cially continuous features challenging standard crfs approach able  training algorithms based maximum likelihood ml • perform feature selection parameter estimation  quire running inference each iteration optimization uniﬁed efﬁcient manner  expensive exact inference • select compatibility features learn dependency  easily intractable large dense networks structures relational data  people resort approximate inference tech •  niques loopy belief propagation markov chain  handle discrete continuous observations  monte carlo consequence ml learning procedure perform experimental validation algorithm  converge suboptimal results diverge real world activity classiﬁcation tasks synthetic    alternative maximize pseudolikelihood data using crfs different degrees complexity  training data mpl besag  essential idea comparison approach consistently outperforms  mpl convert crf set independent patches training techniques  each patch consists hidden node true values rest paper organized follows discuss  direct neighbors applying ml simpliﬁed model related work section section  extend  mpl usually efﬁcient successful sev boosting virtual evidence section  apply                                                    ijcai                                                    extension training crfs finally show experimen computed                                                                          tal results conclude                                                   ef xi                                                                               ef xie−f xi                                                              pyi  xi      −f                                                                                                                  related work                                                          ef xie−f xi     techniques presented perform feature ef xi  e−f xi                                                                                         represent potentials   selection during crf training                                                          discrete features mccallum  suggested ef  respectively  refers  ﬁcient method feature induction iteratively increasing sum feature functions ensemble weak learners log                                                        itboost minimizes objective function  respect  conditional loglikelihood dietterich et al  applied                                            gradient tree boosting select features crfs al stepwise way speciﬁcally given current ensemble                                                         logitboost selects weak learner fmx using new  gorithm combines boosting parameter estimation              linearchain models                                  ton step adds  ensemble shown    work closest boosted random  ﬁelds   computing newton step equivalent solving  brfs torralba et al  developed speciﬁcally vi following weighted leastsquareerror wlse problem                                                                                 n  sion tasks graphs densely connected                                                                                                   fmx     argmin    wi fxi − zi      brfs approach combine boosting belief                      propagation select compatibility dependencies                                                                                                                     yi−  local features key differ wi yi  xi − yi  xi   zi                                                                               pyixi  ences learning compatibility dependencies brfs weight working response sample  use linear combinations beliefs weak learners instead  combinations crf features models com  extension virtual evidence  patible standard inference techniques second brfs section extend logitboost handle virtual ev  sume denselyconnected graphs weak pairwise connec idences beliefs each feature fea  tions show experiments depen ture vector xi input boosting proba  dencies strong brfs perform poorly bilistic distribution feature’s domain opposed  brfs formulate inductions local compatibility fea single observed value  denote distribution  tures differently specify number each type crossproduct feature values vexi  features separately contrast approach treats                                                          main         aim minimize negative  types features consistent manner algo        −                    rithm determine type better given iteration loglikelihood log  ve  ve  experiments algorithm automatically picks reli represents posterior probability true label conditioned                                                        virtual evidence compute term follows  able local features beginning starts selecting com     ⎧                                                                           px                                                                                    xi  patibility features accumulating local evidence      ⎪        vexie                                                                     ⎨⎪p                                                                                               xi         −f                                                                                xie                                                                            xi ve                                                                    px                                                          ve               −f xi                boosting virtual evidence                                  ⎪       vexie                                                                     ⎪p                                                                                                ⎩   xi         −f                                                                                xie     boosting general approach supervised learning fre           xi ve                                             xy            und schapire   given training data          xi         ±f                                                                       xi      xn yn  xi feature vector yi la xi ve  computes expected poten  bel boosting works sequentially learning set weak tials replace potentials  helpful  classiﬁers combining ﬁnal decisions think vexi message random ﬁeld models  traditional boosting algorithms assume feature values  consistent belief update belief propagation  terministic section extend virtual ev determine weak learner fmx modify  idence pearl  feature distribution logitboost error criterion  taking expectation  domain single observed value specif wrt virtual evidence  ically generalize logitboost algorithm friedman et              n                                                                                               al  directly handles probabilities closely fmx  argmin wie fxi − zi  related random ﬁeld models simplicity ex              plain logitboost extension binary classi           n  xi                 ∈                                                                                 ﬁcation case    easily extended    argmin           wi vexifxi − zi    multiclass problems friedman et al                                                                                                xi      logitboost algorithm                             wi zi computed logitboost using  logitboost minimizes negative loglikelihood      pyivexi obtained                                   n                      algorithm described alg  constructs                                                           −log pyyn  xxn −   log pyi  xi   iterations each iteration algorithm ﬁrst                                                    mulates wlse problem line   solves  right term follows independence paper virtual evidence discrete distribution  labels using logistic regression model pyi  xi training crfs discrete hidden states                                                    ijcai                                                      inputs  training data vexiyi yi ∈   inputs  structure crf training data xiyi            ≤ ≤                                    yi ∈   ≤ ≤     output approximately minimizes eq         output learned     ···m                                  ···m      ···n                                run bp using virtual evidences vexinyi     compute likelihood pyivexi using eq         ···n     compute weight wi  pyivexi − pyivexi   compute likelihood pyivexinyi using eq                                     yi−      compute working response zi                        compute weight                                 pyivexi                                                            wi  pyivexinyi − pyivexinyi    end                                                                                    yi−                                                            compute working response zi                  obtain “best” fmx solving eq                                            pyivexinyi    update xf xfmx                            end                                                                         end                                                   obtain “best” solving eq                                                           update     fm     algorithm  extending logitboost virtual evidence                                                                     end                                                                 algorithm  training crfs using veb  obtain weak learner line  vexi deter  ministic value eq   eq    exactly original logitboost algorithm note virtual evidences provided inputs  extension generalization original logitboost alg  veb update vexinyi each iteration  able handle deterministic evidence messages nyi changing veb  shown property standard logitboost dates  speciﬁcally compute virtual evidence yi  minimizes objective function using newton steps neighbor yk veiyk distinguish messages                                                        multiplying compatibility potentials ef ykyi    virtual evidence boosting training             denote messages λk→iyk μk→iyi     conditional random fields                          spectively messages computed iteratively during                                                        belief propagation pearl     boosting algorithm previous section assumes indepen                ykxk                                                             λk→iykαe                      μj→kyk    dence training examples applied                          j∈ny ji  crfs labels dependent similar                               mpl algorithm ﬁrst convert crf set individual                                                                                                ykyi  patches each patch consists hidden node direct neigh μk→iyiβ           λk→iyk       bors observations key difference mpl                       yk  instead using true labels neighbors use α β used normalization seen  messages neighbors virtual evidences λ–messages contain information distribution  apply extended boosting independent patches sending node yk μ–messages contain information  feature selection parameter estimation based values recipient node yi prefer  ideas develop new training algorithm crfs called μ–messages correspond exactly messages sent reg  virtual evidence boosting veb veb general tech ular belief propagation use each λk→iyk message  nique handle continuous discrete observa virtual evidence veiyk end belief propagation  tions used crfs arbitrary structures generate combined virtual evidence vexinyi  explain veb context binary classiﬁcation node yi “stacking” observations xi received  algorithm readily extended multiclass labeling virtual evidence messages veiyk combined virtual ev  experiments                idence used compute posterior distribution                                                             veb algorithm                                  ve  using  equivalently belief prop                                                        agation                       veb algorithm described alg  crucial dif                       yixi                                                           pyivexinyi   γe             μk→iyi   ference veb extended logitboost algorithm                                                                                          k∈ny   lies way virtual evidences handled veb                                  siders types evidences node yi ﬁrst type γ used normalization  hard evidence given input algorithm cor summarize each iteration veb updates vir  responds observations xi training data sec tual evidences belief propagation using lo  ond type soft evidence corresponding messages cal features compatibility potentials contained  neighboring nodes nyi messages obtained  initially  posterior distributions  running belief propagation current ensemble pyivexinyi uniform proceeds exactly  linear combination feature functions  extension extended logitboost algorithm order add  boosting allows treat types virtual evidence weak learner fm experiments sufﬁcient  noted vexinyi learn crf’s local run belief propagation iteration learning  features compatibility features uniﬁed framework cycle greatly increases efﬁciency approach                                                    ijcai                                                      feature selection veb                           unify three cases computing optimal feature  key step alg  step  ﬁnds “best” weak weights follows                                                                                learner solving wlse problem note                                                                                                 α          di  weak learner crfs certain kind combination                 n                        features algorithm essentially performing feature se                    wicdi  lection parameter estimation paper                                                        cdi count feature data instance assume  sider weak learners linear combinations features                                                                                    cut crfs individual patches di    involve single type local attribute neighbor  local features real number   com  nutshell determine “best” weak learner ap patibility features greater  allow  proach enumerates types local attributes neighbor parameter sharing instance example  relations each type computes optimal parameters node connected neighbor  weak learner square error picks type approach parameter estimation solved  overall square error opti simply performing feature counting makes  mal parameters section discuss formulate algorithm efﬁcient  weak learners crfs binary labels esti important notice algorithm typically ﬁrst  mate optimal parameters efﬁciently speciﬁcally picks reliable local attributes messages neighbors  sider three different cases weak learner involves close uniform beginning iter  continuous attribute discrete attribute neighbor ations starts picking compatibility features mes  lationship ﬁrst cases treated just like sages provide information  regular logitboost apply extended boosting  neighbor relationships handle virtual evidences  evidences provided belief propagation                experiments    continuous attribute xk weak learner evaluate performance veb compare  linear combination decision stumps              alternatives boosted random ﬁelds brf maximum                                     fx   αδx       ≥  hαδx             likelihood ml maximum pseudolikelihood mpl                            α      α                  perform experiments using synthetic data     threshold   feature different activity recognition datasets experi  weights approximately optimal values solv  ing wlse problem  speciﬁcally determined ments run veb brf  iterations  using heuristic maximize information gain each iteration run iteration belief propagation  compute optimal α α analytically set ml mpl learning use shrinkage prior  ting ﬁrstorder partial derivative square error equal zero mean unit variance ml mpl optimization  zero                                  implemented using quasinewton procedure ml       pn                      pn                 wiziδxi ≥           wiziδxi   evaluate likelihood using bethe method yedidia et  α                       α                                         al  gradient using belief propagation            wiδx   ≥                wiδx                                              learned models tested using map belief propagation                                   second given discrete attribute ∈ ···d use speciﬁc inference algorithm  weak learner expressed                          brf described torralba et al  accuracies                         d             xk         α δ xk                 calculated based map labels experiments                                                run standard desktop pc ghz cpu                                                    gb memory                                   αd weight feature δx  indicator  function  xk   optimal  synthetic data  weights calculated similarly               veb versus brf                      pn                                wiziδxi                veb brf similar brf assumes graphs              αd                                                       densely connected each individual message                            wiδx                                                    informative veb does make    given certain type neighbor correspond sumption connectivity structure difference  ing virtual evidence veiyk weak learner weighted signiﬁcant assumption true  sum indicator functions compatibility features vision applications discussed torralba et al                                                                                invalid applications experi               fyk       αdδyk                                                     ment examine performance veb brf                                                        dependencies nodes stronger    solving wlse problem virtual evidence  synthetic data generated using ﬁrstorder markov  optimal weights                     n                                 chain binary labels emphasize difference                           wiziveiyk               learning compatibility features intentionally use weak ob             α        i                                                   servation models each label connected  binary obser                            wiveiyk                                                     vations conditional probabilities observation    complex weak learners decision trees involving differ models uniformly sampled range    ent attributes learned similar ways  adjust transition probabilities label                                                      ijcai                                                                                                               training algorithm  average accuracy         veb      brf                                                           veb                                                                                                                  brf                                                                            ml  observations         accuracy                               accuracy                              ml  boosting                                                                          mpl  observations                                                          mpl  boosting                     transition prob    veb brf mpl ml                                                                 table  average accuracy indoor activities  figure  classiﬁcation accuracies experiments using synthetic  data error bars indicate  conﬁdence intervals  veb vs brf transition probabilities pairwise dependen  real activity recognition data  cies turn weak strong comparison different learning crfs wellsuited tasks activity recognition  algorithms feature selection                     ing real sensor data overlaps mea  label     each given transition surements strong relationships activities  observation model generate labels chains section compare different training approaches  perform leaveoneout crossvalidation using linear crfs veb brf handle continuous  chain crf additionally run experiments times sensor measurements directly doing ml mpl  randomly generating different observation models  straightforward performance ml mpl ter    running durations algorithms similar rible simply use continuous measurements fea  compare accuracies average accuracies ture values fact features cor  using veb brf conﬁdence intervals shown respond zeromean gaussian assumption  fig clear compatibility dependen far truth try tricks circumvent  cies strong transition probabilities range  difﬁculty learn decision stumps observa   methods similar accuracies tions using heuristics veb brf  dependencies stronger   veb dra use boosting logitboost experiment select  matically outperforms brf mainly weak inter set decision stump features decision stumps  action assumption underlying brf does hold fed ml mpl weight estimation                                                                            feature selection complex models                    friedman et al    real sequential estimation problems longrange  dependencies modeled using highorder   indoor activity recognition  markov models practice impossible ﬁrst experiment person collected audio accelera  know exact order people use markov tion light sensor data stayed indoors using small  models longer dependencies actual data wearable device total length data set  experiment simulate scenario generating  minutes recorded period  days goal  synthetic data using highorder markov model tran recognize person’s major indoor activities including  sition probability pyn  yn−pyn  yn−k usage meal meeting tv watching sleeping  constant observation model similar segmented data oneminute chunks manually  previous experiment label yn depends labeled activity each minute purpose super  past label yn−k value unknown vised learning testing each chunk data com  crf model speciﬁcally pick   set puted  feature values included energy various  transition probability pyn  yn−k  yn  yn−k frequency bands log linear signal autocorrela   given generate long tion different entropy measures features fed  chains perform leaveoneout crossvalidation repeat crf observations linear chain crf cre  experiment different k’s compute average ated day evaluate algorithm using leaveoneout    exact value unknown crf model crossvalidation person performs different ac  generate denselyconnected crf connections tivities different days accuracies vary signiﬁcantly  each pair nodes distance day day activities meals hard  equal  crf trained using different algo recognize activities sleeping rela  rithms experiments veb reliably identify tively easy recognize result leaveonedayout  correct values picking pairwise features crossvalidation accuracies different days vary signif  distance multiples brf performs icantly standard deviation large overall  feature selection structure learning does perform average accuracies using different methods shown  veb average classiﬁcation accuracies shown table  veb   better ml  fig veb robustly extract sparse mpl matter incorporate continuous obser  structures signiﬁcantly outperforms approaches vations error bars different techniques  running time veb brf mpl quite efﬁ overlap approach signiﬁcantly  level better  cient each training takes tens seconds contrast competitors evaluate combination dif  training using ml takes  minutes         ferent experiments reported paper                                                    ijcai                                                    
