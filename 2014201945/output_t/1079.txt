    tighter error bound decision tree learning using pac learnability                       chaithanya pichuka raju bapi chakravarthy bhagvati                                    arun pujari deekshatulu                             department information sciences                                          university hyderabad                                   gachibowli hyderabad  india                  pssgrkyahoocom     bapics chakcs akpcs bldcsuohydernetin                        abstract                          svm bayesian ml knn increasingly                                                        complex decision regions richer hypothesis      error bounds decision trees generally based representation capability noted lim      depth breadth tree paper ited empirical characterization classiﬁers possible      propose bound error rate depends picted figure  theoretical characterization classiﬁers      depth breadth speciﬁc decision tree general difﬁcult paper shall consider dts      constructed training samples bound theoretical empirical analysis      derived sample complexity estimate based      pac learnability proposed bound com     techniques proposed estimating                                                                                                   pared traditional error bounds sev future error rate decision tree classiﬁer kaariainen                                                                                          eral machine learning benchmark data sets langford  mansour   primary      image data set used content based im classes error estimation methods available ﬁrst class      age retrieval cbir experimental results demon methods utilize empirical error training samples      strate proposed bound gives tighter estima predicting future error empirical error based      tion empirical error                      training set test set example                                                        use kfold crossvalidation dataset trans                                                        form crossvalidation estimate estimate heuris    introduction                                       tic conﬁdence interval error ﬁnal decision tree  computational learning theory colt deals char learned examples approach sample  acterization difﬁculty learning problems compression bound considers test set labelling  capabilities machine learning algorithms vapnik  estimates label based training set  vidyasagar  probably approximately correct labelled previous test set langford thereare  pac framework formulated characterize classes hy methods microchoice bound langford  potheses reliably learned reasonable poly blum  test set bound occam bounds lang  nomial number randomly drawn training examples ford  microchoice bound inherently depends  reasonable computation sample complexity speci structure dt calculating choice spaces  ﬁes number training examples needed classiﬁer node dt test set bound test setbased bound  converge high probability successful hypothesis entirely characterized errors labelled test set  pac framework possible derive bounds test set bound incorporates test set error directly es  sample complexity combining expression timate usually good occam bound assumes underlying  size hypothesis space empirical error mitchell distribution binomial computes estimate based   characterization hypothesis space im empirical errors observed training dataset  portant issue colt                                  second class methods utilize structural aspects    order understand representational capability classiﬁer order arrive estimate example  various classiﬁers ran algorithms percep case dts consider depth breadth  tron linear polynomial support vector machines svm dt constructed training samples estimation  bayes maximum likelihood ml decision tree dt oblique future error  decision tree knearest neighbor knn classiﬁers paper propose structurebased error bound  standard iris benchmark dataset iris fourdimensional dts using pac learning framework new error  dataset comprising three classes classes bound considers depth breadth dt learned  linearly separable figure  shows twodimensional projec training examples conducted experiments  tion decision regions described trained classi benchmark datasets compare results various error  ﬁers benchmark dataset perceptron linear svm bound estimation methods proposed esti  dt deﬁne elementary decision regions comprising linear mation method works rest paper organized  axisparallel rectangular surfaces oblique dt polynomial follows firstly structurebased                                                    ijcai                                                    figure  decision regions classiﬁers comparison hypotheses learned various classiﬁcation methods iris  machine learning benchmark dataset    empirical errorbased estimation methods experimental cc distribution ε                                                                                           sults presented subsequently discussion results ε  andδ  δ   learner  followed conclusions                              probability  − δ output hypothesis ∈                                                                    ≤                                                                                       errort   ε time polynomial ε  δ     classiﬁer structurebased error bounds             sizec represented mathematically                                                        follows  section introducing classiﬁer structure based  bounds pac learning explore                  ≤                                                                   errort ε     δ  generalization bounds formulated based                                             pac learning framework                                                      ≥  −                                                                    errort ε         δ    pac learning error bounds                      paclearnability concept used derive general                                                        lower bound size training set required  section review relation probably ap                                                                       learner possible scenarios ﬁrst sce  proximately correct pac learning framework general                                                        nario assume learner consistentthatisthe  ization error bounds valiant  mitchell pac                                                        learner outputs hypotheses consistent tar  tackles questions related number examples                                                        concept training set probability single  computation required learn various classes                                                        hypothesis having true error greater ε consistent  target functions pac learning framework major                                                        training set  −  probability  assumptions assumes classiﬁcation error                           ε                                                        hypothesis consistent individuals  bounded constant ε arbitrarily                                                            − ε  instead hypotheses  small second assumption requires classiﬁer’s                                                                                       true error greater ε consistent  probability failure bounded constant δthat                                                        training set upper bound probability  arbitrarily small short require                                                        hypotheses consistent  classiﬁer probably learns hypothesis approximately                                                                                        randomly drawn individuals ∗−   using fact  correct – called probably approximately cor                         ε                                                         ≤   write following inequality  rect pac learning framework deﬁning error entire                                                          equation   instance distribution  data true error errort                                                   error training set empirical error errord                      ≤  ∗  − ≤  pac learnability deﬁned formally follows            errort ε           ε     δ          deﬁnition mitchell  consider concept class rewrite shown equation  solving  deﬁned set instances length learner number training examples needed learning  using hypothesis space paclearnable using algorithm results inequality shown equation                                                     ijcai                                                                                                            substitute equation  expres                                  −mε           perrort ε ≤ ∗    ≤  δ          sion error bound decision tree using depth feature                                                        shown equation                                                   ≥   lnh  ln                                   ε           δ                                                                   ≤            second scenario learner agnosticthat errort  errord  true error training set necessarily zero      use chernoff approximation estimate er            ln −             ror bound pac learning framework mitchell                          logn      ln δ  analogous equation  error bound agnostic learner                                           derived shown equation                                                         breadthbased error bound dt pac ii                ≥     lnh  ln                                 ε           δ    use equation  derive generalization method computation based breadth  error bound shown equation                    tree approach akin shown lecture                                                       notes guestrin guestrin  assume dts                                                      binary trees use idea kleaves decision               ε ≥       lnh  ln                                            δ                   tree − leaves decision trees                                                 write terms true error errort                      training error errord shown equation        approximation pac ii                                                                                  given attributes    error ≤ errord      lnh  ln                                           δ                                                          hk      number decision trees leaves    coming original problem deriving error     bounds dts equation  used know            k−                                                                                                       size hypothesis space  pac based hk        hihk−i  error bounds need estimate three                                                                            k−      k−  ways estimating dts ﬁrst approach es hk      timate based depth dt second method  estimate based breadth dt possibility  use measures propose bound based step equation  rough crude approxi  approach recursive estimation mation hk previous step substitute  subsequent subsections three equation  expression error bound  structurebased error bounds dts                 decision tree using breadth feature shown equa                                                        tion  crude approximation replaced better    depthbased error bound dt pac           approximation obtained recursive estimation paciii  computation based depth tree  approach akin shown lecture notes  guestrin guestrin  assume dts bi                                                          error ≤ errord  nary trees use idea kdepth decision tree           −                                                   depth decision trees children                                                                                                               − lnnk − lnk ln δ   approximation   pac                                                                                                       given attributes     hk           number decision trees depth                                                     depth breadthbased error bound dt   hk      choices root attribute∗                   pac iii                  possible left subtrees∗                                                          possible right subtrees              propose new approach calculating error bounds                  ∗    ∗   hk           hk   hk                             dts approximation similar pac ii                    −   hk        ∗                                      resulting bound tighter show   lk        loghk                        empirically simple idea approach   lk         logn lk                             important consider structural features dt                     lk    −        logn                  closer approximation whichinturnleadstoa                                                     tighter estimate error bound                                                    ijcai                                                    approximation pac iii                        test set bound  given attributes                                   bound assumptions error distribution                                                        particular classiﬁcation error distribution mod   hk          number decision trees leaves    elled coin ﬂips distribution binomial distribution                                                   shown equation             ∗ ∗            ∗ ∗                                                  k                                                                                          m−j                                                               bin  cd          − cd                                                                                                                                                                                     k−                                k−                     expression equation  computes probability                    k−i       ∗  ∗                                         examples coins error rate cd produce fewer                                                    errors interpret binomial tail probability                                                                                                      k−                                    empirical error greater equal butweare                                                        interested error bound classiﬁer given probability   fk              fifk−i                                                        error δ deﬁne binomial tail inversion given                                                                       equation  gives largest true error         initial condition                                                                                                   probability observing errors δ                                                                                                                                      ≥     substitute expression equation  bin  errort maxp   bin      δ     error bound dt using approximation                                                                                              given number test instances test set bound                                                        formulated shown equation     empirical errorbased bounds                       test set bound                                                                        ≤              ≥  −  section generalization bounds errort  bin  errortestδ      δ       mulated based empirical error observed dataset drawback test set bound  unlike structurebased error bounds discussed pre possible test set training sets incompatible  vious section bounds reviewed section explicitly introduce inaccuracies error bound estima  corporate empirical errors observed datasets — train tion kaariainen langford   ing test sets                                                          occam’s razor bound    microchoice bound                                termed training setbased bound takes                                                        training set performance consideration rea  microchoice approach tries estimate aprioribound sonable learning algorithms implicitly assume  true error based sequential trace algo train set accuracy behaves like true error addition                               rithm langford blum   learning algo ally bound need know prior probability  rithm applied training set instances algorithm hypotheses  successively makes sequence choices ccn  sequence choice spaces ccn ﬁnally producing occam’s razor bound  hypothesis ∈ speciﬁcally algorithm looks priors classiﬁers hforallδ ∈    choice space produce choice choice turn                                                                    ≤                 ≥  −  determines choice space noted errort bin errord δp         δ    stage previous choices eliminated obtain occam’s razor bound negating  consideration stage algorithm looks equation   data make choice ∈ choice  determines choice space choice       ≥                  spaces thought nodes choice tree errort    bin errord  δp   δ       each node tree corresponds internal state important notice prior  learning algorithm node belonging choice selected looking instances relax  space ci                                             occam’s razor bound entropy chernoff bound    apply microchoice bound decision tree somewhat tractable expression langford   taking account choice spaces available node chernoff occam’s razor bound  decision tree using pac bound technique priors ph classiﬁers hforallδ ∈    error bound shown equation                                                                                                                                                                                                                    err ≥ errdh      ln    ln    δ                                                                                    δ  error ≤ errord h               ci  ln                                             δ                                                                                    i∈nodesdt                application occam’s razor bound somewhat                                                    complicated application test set bound                                                    ijcai                                                                     table  results experiments error bound calculating  different data sets           data set         pi   pii   piii  mc      occ    test    emp   att                     weather                                                 yellowsmall                                                adult                                                      adult                                                   yellowsmall                                             contact lenses                                            labor                                               monk                                                  monk                                                monk                                             votingrecords                                            crx                                               tictactoe                                          segment                                             cbir                                         size pi paci pii pacii piii paciii mc microchoice occ occam’s razor test test set emp empirical        error att number attributes average breadth dts average depth dts average count                                             misclassiﬁcation errors                                                          lustrate various error bounds computed case                                                        breadth depth each choice space size                                                        each node indicated figure  number                                                        attributes nis                                                            value computed various error bounds                                                        equation   substituted equation  ob                                                        tain ﬁnal theoretical estimates error bound                                                        different schemes   taking                                                           δ    obtain    following values er                                                        ror bounds  paci    pacii                                                           paciii   microchoice bound need                                                        equation  substitute choicespace sizes  figure  example dt corresponding adult dataset dt figure  microchoice error works  breadth depth  quantity parenthesis   denotes size choice space node                                                           empirical results    illustrative example                               experiments  different machine learning                               −                  benchmark datasets uci repository results     pac  −     hk   ∗         eq                                 −                   ported averaged  experiments conducted ran                     ∗                                      k−       k−             domly chosen subsets datasets training     pac  − ii    hk             eq                                                                            ∗−                     data testing remaining   case                              ∗            image dataset content based image retrieval cbir                             k−                       complexity experiment run     pac  − iii   hk         hihk−i eq          conducted results summarized table  ta                                                                               ble reports observed error rates training set test                               init cond       set empirical error goal experimentation     microchoice                   ci               theoretical bounds estimated various methods                            i∈nodesdt                 come close observed empirical error boldfaced                                                 entries table  correspond best estimate true                                                    error table expected paciii    consider  example experimental dataset  gives better estimate paci pacii depth  adult dataset dt structure obtained based measures overestimate size tree  experiments adult dataset chosen il assume complete binary tree reality dt                                                    ijcai                                                    
