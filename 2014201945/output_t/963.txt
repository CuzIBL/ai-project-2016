     skewing efficient alternative lookahead decision tree induction                                david page                                         soumya ray                         page  biostat wise edu                             sraycswiscedu                  department biostatistics  medical informatics         department sciences                           university wisconsin                             university wisconsin                          madison wisconsin                             madison wisconsin                              abstract          paper presents novel promising approach        allows greedy decision tree induction algo•       rithms handle problematic functions par•       ity functions lookahead standard approach        addressing difficult functions greedy decision                                                                  table  truth table drosophila fruitfly survival based        tree learners approach lim•                                                                 gender sxl gene activity        ited small problematic functions subfunc       tions   variables time complex•       ity grows exponentially depth                                                                  variety data analysis approaches employ informa•       lookahead contrast approach presented                                                                  tion gain kullbackleibler divergence filter variable        paper carries constant runtime penalty                                                                  selection control computation time example        experiments indicate approach effective                                                                  sparse candidate algorithm learning baycsian networks        modest amounts data problematic                                                                  friedman et al  inability approaches        functions subfunctions seven vari•                                                                 learn functions like  frequently noted example        ables examples contain                                                                  context analyzing gene expression microarray data ifried•       numerous irrelevant variables                                                                  man etai  szallasi                                                                     tdidt algorithms myopia search re•      introduction                                               duced cost increased computation time stan•                                                                 dard approach through depthfc lookahead norton    algorithms topdown induction decision trees        default tdidt algorithms depth looka•  tdidt widely used algorithms          head time perform split grows exponen•  machine learning data mining statistical classifica•      tially problematic functions remain matter   tion tdidt implementations id quinlan        large chosen   quinlan  wwwrulequestcom cart          purpose paper introduce alternative ap•  ibreiman et al  easy use produce      proach problematic functions parity does   humancomprehensible models tdidt algo•         incur high computational cost lookahead ap•  rithms wellknown myopic greedy     proach relies observation functions   strategy choosing split variables internal node labels actually problematic distribution data signif•  myopia worst data labeled accord•  icantly different uniform cases functions   ing parity functions exclusiveor bit odd parity problematic tdidt algorithms func•  denoted © related problematic functions   tions exclusiveor relatively easy   naturally arise realworld data example fruitfly ex• observe tdidt algorithm performance data set   periments flies male active sxl   improved algorithm access second sig•  gene female inactive sxl gene survive nificantly different distribution data course   flies die survival exclusiveor func• second distribution unavailable   tion gender sxl activity table                      show second distribution simulated     course parity paritylike functions prob•   skewing data distribution note   lematic machine learning statistical algorithms   present paper focuses entirely classification ignoring re•  employ explicitly implicitly linear assumption  gression trees trees predict continuous outputs   order gain efficiency models include perceptrons     leaves furthermore limit entire discussion tdidt   logistic regression linear support vector machines fischers algorithms perform binary splits data   linear discriminant naive bayes include  use variables values       learning                                                                                                                table  subfunctions each       fraction assignments positive      zero gain    nonzero gain according entropy gini                                                                          figure  trees representing     review lookahead hard functions    assume reader familiar approach tdidt   algorithms assumed familiarity includes knowledge    commonlyused functions measure node purity   entropy quinlan  gini index ibreiman et al    includes familiarity greedy heuristic choos•   ing split variable maximizes improvement   gain node purity score review notions   problematic functions lookahead ease discus•  sion section limits twoclass problems   classes positive negative      various node purity functions employed differ•   table   functions three variables   ent tdidt algorithms splitting variable yield        problematic using depth lookahead   nonzero gain class distribution changes  problematic functions inverses   values ranges   distribution classes value xt  cause labels selected three nodes   range zero gain according node pu•   variables furthermore functions require   rity measure common usage example table         higher lookahead example suppose examples   variable nonzero gain according gini  constructed variables target   entropy variables zero gain             functions table  involving       consider data set drawn uniform distribution  depth lookahead tdidt highly likely choose incor•  binaryvalued variables labeled according                   rect variables problems solved depth   target function fortunate                   lookahead time choose split     complete data set—one occurrence each      problematic targets remain   truth assignment —it clear   variable class distribution exactly  motivation skewing   xi   regardless large uniform sample   choose draw variable nonzero gain     consider target function discussed previous   chance probability cor•  section  suppose data distributed     rect variables xioo higher gain differently uniform example introduce   incorrect variables extremely low     dependencies present uniform distribution ev•  learning task virtually impossible tdidt algorithm   ery odd number  proba•     depth lookahead preceding task triv•     bility   suppose variables   ial depth lookahead given node chooses    independent uniform distribution variable   split variable split variables probability taking value  case   level tdidt algorithm augmented way con•       large sample expect class distribu•  sider possible depth trees shown fig•   tion examples differ significantly   ure  each maximum possible gain      class distribution examples    reasonably large data set high probability     examine preceding claim closely consider   depth trees gain marginally different   second distribution each variable probability   zero depth lookahead © be•      draw sample  examples expect   comes easy depth lookahead repeated     roughly  roughly    step tree construction functions   variable  subfunction easy                          reduce superexponential growth lookahead      course depth lookahead comes price    mere exponential growth address paritylike functions                                                                  require tree leveled—all nodes level labeled   number variables number examples                                                                  variable nonstandard lookahead proce•  time choose split goes                    dure imposes high computational burden                                                                                                                  learning     expect roughly                   weight each example xi takes value        belong             multiplying weight constant sake illustra•   positive class  expect           tion suppose double weight     belong positive class end process each example weight be•              fraction positive examples quite  tween   likely each variable signif•   different values examples          icantly different weighted frequency distribution previ•         positive examples            ously desired guaranteed example    positive result nonzero             suppose original data set consists  truth assignments    gain instance information gain roughly   variables  suppose half    maximum  possible gini gain roughly       examples     half     maximum  possible hand vari•       favored setting each variable happens    able likely nearly zero gain            examples assigned weight  new    unless highly unlikely sample drawn tdidt al•   frequency distribution each variable   gorithm choose split                original frequency distribution addition potential   point remainder learning task trivial           difficulty second difficulty process magnify      notice preceding discussion moving sec• idiosyncrasies original data instance suppose   ond distribution changed marginal distribution   data set  simplicity favored   variable just target re•  setting each variable  happen ex•  vealed correct variables target function   ample variables set  inordinately             problematic functions three    high weight compared examples potentially giv•   variables table  notice important aspect   ing insignificant variable high gain mitigate   second distribution changed frequency   potential problems skewing procedure   distributions variables specific change     difficulties preceding paragraph occur   variable gone way—to probability      data sets combined choices favored settings   taking value —and second distribution    selections favored settings data set   given nonzero gain exactly variables target    leave variables frequencies unchanged rela•     preceding discussion conclude   tively unlikely leave variables frequencies   access distributions different  unchanged furthermore selections favored   choosing good variables split relatively easy   settings magnify idiosyncrasies data   realworld problems rarely access   unlikely magnify idiosyncrasies there•  different distributions data ability request fore instead using skewing create second dis•  data according second distribution choose in•    tribution use create number additional   stead section discusses practice simu• distributions different distributions come   late second distribution different randomly replacement selecting different combi•  procedure skewing simulation approach tends mag•       nations favored settings variables according   nify idiosyncrasies data set example introducing  uniform distribution ensure tree construction   dependencies present original dis•   thrown course single bad distribution orig•  tribution experiments indicate inal skewed tree construction process modified   data set large magnification idiosyncrasies  described following paragraph   major problem                                          suppose weightings data origi•                                                                 nal data set plus reweighted versions data set    skewing algorithm                                            considering split score each variables                                                                  each weightings data variable   desired effect skewing procedure skewed target function nearly zero   data set exhibit significantly different frequencies    gain weighting noted   original data set draw new ex•     occur weightings variables   amples change frequency distributions variables    achieve high gain variables appear tar•  attaching various weights existing examples     significantly nonzero gain   procedure initializes weight example      weightings necessarily   present details reweighting procedure     set gain threshold variable exceeds gain   binaryvalued variables nominal variables con•    threshold greatest number weightings selected   verted binary variables discuss extensions continu•  split variable expectation selected variable   ous variables section                                     highly likely correct sense actually      assume variable takes value       target function time choosing split   example takes value      remains mn contrast lookahead increased   example—otherwise variable carries information     runtime small constant   removed each variable ran•                        pseudocode algorithm shown algorithm    domly uniformly independently each variable select    actually doubling tripling weights algo•  favored setting   increase       rithm takes parameter weight ex      learning                                                                                                               algorithm  skewing algorithm                                 following section describes experiments designed test    input matrix data points boolean             preceding hypotheses        variables gain fraction number trials        skew                                                       experiments    output variable xi split — variable        sufficient gain                            section discuss experiments synthetic real      entropy class variable                            data designed test hypotheses preceding para•     variable max gain                               graph addition question arises problematic      gain                                            functions subfunctions occur high frequency                                                        justify additional work skewing experiments                                                                section address question begin                                                    discussion experiments using synthetic data target                                                                functions examples drawn randomly uni•       begin skewing loop                                      formly replacement experiments compare                                                simple id id skewing wc selected id                                                  eliminate issues sophisticated pruning     randomly chosen favored value                         parameters input skewing algorithm algorithm      —                                                                         parameters cho•                                                       sen experiments held constant     fori                                            experiments improved results obtained     lthen                                                 tuning     zei  vi                                      set experiments synthetic data exam•                                                  ples generated according uniform distribution                                                           binary variables target functions drawn randomly                                               generating dnf formulae subsets        entropy class variable                  variables number terms each target drawn ran•                                               domly uniformly   each term    gain distribution                             drawn choosing each variable appear    ife                                               negated unnegated equal probabilities    fi                                                   targets ensured satisfiable examples        end skewing loop                                         variables satisfy target labeled positive    argmaxfi                                                 examples labeled negative figures  show learn•   itfj                                             ing curves different target sizes each point each curve       return                                                  average runs each different target                                                          different sample specified sample size    return                                                      general figures fit expectations algo•                                                                 rithms perform skewing provides slightly consis•                                                                 tently better results wc note differences sta•    ample multiplied xi takes preferred value vi tistically significant probably difference skewed id   example multiplied  — il•   likely ordinary id include irrelevant variables     lustration  weight example xi particularly faced problematic functions sur•  takes value vi effectively doubled relative examples  prise figures indicate ordinary tdidt al•  xi does value                             gorithm outperforms skewing average sample      hypothesis practical experiments new al• size small relative target size sample size grows   gorithm run somewhat slower ordinary tdidt        crossover point reached skewing consistently   algorithm constant factor rarely produce   outperforms ordinary tdidt algorithm furthermore   trees lower accuracy ordinary tdidt      sample size required effective skewing grows   algorithm produce trees slightly moder• number variables target results   ately higher accuracy—when target contains     make clear order growth ex•  problematic subfunctions produce        ponential target complexity grow exponentially   trees higher accuracy—when target    number variables target experi•  problematic function target problematic func•   ments needed explore order growth ob•  tion variables skewing gain   servation implies limitation skewing—that skewing   individual variable target likely small there• undesirable learning tasks small samples target   fore hypothesize unless data set large concepts potentially employ variables   benefits skewing approach apply problem•      set experiments focuses problematic   atic target functions variables note functions methodology   large number variables target reduces potential following exception targets drawn randomly   gain number variables examples does    functions described entirely variable                                                                                                                 learning                                                                          figure  threevariable hard targets                     figure  fourvariable targets                             figure  fourvariable hard targets                     figure  fivevariable targets                             figure  fivevariable hard targets                  —i                        figure  sixvariable targets                             figure  sixvariable hard targets     learning                                                                                                              
