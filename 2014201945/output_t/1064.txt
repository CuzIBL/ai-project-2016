        instancebased amn classiﬁcation              improved object recognition                                  laser range data            rudolph triebel      richard schmidt      oscar´ mart´ınez mozos     wolfram burgard                          department science university freiburg                            georgeskoehleralle   freiburg germany                      triebel rschmidt omartine burgardinformatikunifreiburgde                        abstract                          task use technique called associative markov net                                                        works amns combines concept relational      paper present algorithm iden   learning collective classiﬁcation      tify diﬀerent types objects       far amns used based loglinear      laser range data method combination     model means linear relationship      instancebased feature extraction similar extracted features weight parameters      nearestneighbor classiﬁer nn collec learned algorithm results classiﬁcation algo      tive classiﬁcation method utilizes associative rithms learn hyperplanes feature space separate      markov networks amns compared previ        object classes realworld applications      ous approaches transform feature vectors  assumption linear separability features      better separable linear hyper justiﬁed way overcome problem      planes learned amn classi      extend loglinear model subject ongo      ﬁer present results extensive experiments  ing research paper propose diﬀerent approach      evaluate performance algo combining ideas instancebased classiﬁcation      rithm recorded indoor scenes com  amn approach obtain classiﬁer ro      pare standard amn approach   bust error introduced linearseparability      nn classiﬁer classiﬁcation rate obtained sumption present way compute new feature vec      algorithm substantially exceeds tors given set standard features transform      amn nn                               original features show features better                                                        suited classiﬁcation using amn approach                                                          paper organized follows section gives    introduction                                       overview work presented far  paper consider problem identifying objects area summarize concepts collective clas  range measurements believe seg siﬁcation using associative markov networks  mentation data known object classes use shortly learning inference amns  fulinmanydiﬀerent areas map registration robot lo detailed description approach presented finally  calization object manipulation humanrobot interaction present results experiments illustrate  example robot able classify data ac method provides better classiﬁcations previous ap  quired range sensor better capability ﬁnding proaches  corresponding data points diﬀerent measurements  way scan registration carried reliably  related work  application areas object recognition impor problem recognizing objects range data  tant include seekandrescue tasks aimed studied intensively past approaches  robocup rescue initiative                        distinguished according features used pop    makes object detection especially hard fact ular approach spin images johnson  alarc´on  involves segmentation classiﬁcation et al  frome et al  rotationally  segments simultaneously solve segmentation translationally invariant local descriptors spin images  classiﬁcation problem recently collective classiﬁcation used features work presented types  approaches popular approach based features include local tensors mian et al  shape  assumption realworld domains spatial maps wu et al  multiscale features li  neighbors data tend labels ex guskov osadaet al  proposed object  ample anguelov et al  formulate problem seg recognition technique based shape distributions  menting range data known classes supervised approach requires complete view objects  learning task apply collective classiﬁcation solve method deal partially seen objects additionally                                                    ijcai                                                    huber et al  present approach partsbased object presented anguelov et al  authors propose  recognition method provides better classiﬁcation use associative markov networks amns  cause nearby parts easier identify help special type relational markov random fields described  guide classiﬁcation similar idea detecting object taskar et al following brieﬂy  components presented ruizcorrea et al  scribe amns  introduced symbolic surface signatures ap  proach object recognition proposed boykov hut  associative markov networks  tenlocher  based markov random ﬁelds mrfs associative markov network undirected graph  classify objects camera images incorporating sta nodes represented random variables  tistical relationships nearby object parts rela                                                        yyn  case random variables dis  tional approach using mrfs proposed limketkai crete correspond labels each data points  et al  idea exploit spatial relation                                                        ppn each node yi each edge yi graph  ship nearby objects case consist                                                        associated nonnegative value ϕxi yiandψxij yi  line segments work related approach respectively known potentials  described paper presented anguelov et al  nodes edges node potentials reﬂect fact  amn approach used supervised learning                                                        given feature vector xi labels likely  setting classify range data paper combine                                                        signed pi edge potentials encode  approach techniques instancebased classiﬁca interactions labels neighboring nodes given  tion obtain improved classiﬁcation results                                                        edge features xij potential node edge                                                        high given label yioralabelpairyi    range scan classiﬁcation                           ditional probability labels given features  suppose given set data points ppn high conditional probability represented  taken scene set object classes network expressed         ck data point context scan                n          point grid cell occupancy grid following                                                                        py      ϕx    ψx        formulations identical cases                                    ij                                                                               ij∈e    each data point pi given feature vector  xi later features denotes the partition function deﬁnition                                                                                              ﬁned context paper task ﬁnd label given    ϕx   ∈ ψx       ∈                                                          ij ij   yi         each pi labels yi yn  remains node edge potentials ϕ  optimal given features notion optimality ψintaskaret al potentials deﬁned using  context depends classiﬁcation method choose loglinear model model weight vector wk  standard supervised learning approach deﬁne like introduced each class label  node po  lihood function pωy  labels given features ϕ                   ϕ        ·                                                        tential deﬁned log xi yi wn xi  classiﬁcation problem subdivided                                         ∗                  yi accordingly edge potentials deﬁned  tasks ﬁrst need ﬁnd good parameters ω likeli            kl                                                ∗       log ψxij yi  · xi  yi  jnote  hood function pωy  seek good labels                                                                                                            ﬀ                   ∈ rdn    ∈ rde  maximize likelihood assuming given training di erent weight vectors wn  data set labels yˆ assigned hand nodes edges                                                                                                     ﬀ  formulate classiﬁcation follows          purpose convenience use slightly di erent                       ∗                                notation potentials    • learning step ﬁnd ω  argmaxωpωyˆ                        ∗                                                          k    •                              ∗        inference step ﬁnd argmaxypω                           ϕ              ·                                                                    log xi yi      wn  xiyi              collective classiﬁcation                                                    standard classiﬁcation methods bayes clas                     k k                                                                   ψ                   kl ·   siﬁcation nearest neighbor adaboost classiﬁcation  log xij yi         xijyi   data point depends local features                   labeling nearby data points practice                                                        yi indicator variable  point pi  observes statistical dependence labelings asso label   ciated neighboring data points example                                                          reﬁnement step model introduce  sider local planarity scan point feature       kl                kk ≥  happen likelihood py  class label ‘wall’ constraints  thisre                                                        sults ψx   fork  ψx   λk  higher class label ‘door’  ij                      ij       ij                                                        λk ≥  scan points vicinity point belong class ij  idea edges nodes dif  ‘door’ methods use information neighboring data ferent labels penalized edges equally  points denoted collective classiﬁcation techniques labeled nodes speciﬁcation amns makes pos  chakrabarti indyk recently collective clas sible run inference step eﬃciently using graph cuts  siﬁcation approach classify range scan data boykov et al  taskar                                                     ijcai                                                                                                                                        learning inference amns                               training set class           class                                                                  training set class           class                                                                   test set class                                                                      test set class     section learning inference    carried amns reformulate equation                                                                                                                                                conditional probability pwy parameters    ω expressed weight vectors  wn                                                                                                                                                                         feature                                                         feature   plugging equations   obtain log pwy                                                                                                                                                     equals                                                      n k               k                                                                                                                                                kk                                                               · xiy        · xijy − log zwx                                                          feature               feature           ij∈e  note partition function depends figure  example feature transform τ twoclass  labels                              problem features left training data test data                                                        ground truth labeling right test data applying τ    learning  mentioned standard supervised learning task introduced variables yk representing labels                                                                                  ij  goal maximize pwy problem points connected edge inequality  arises partition function depends                                   ∧                                                        conditions linearization constraint yij yi  weights means maximizing log pwyˆ   intractable calculation needs each current implementation perform learning  instead maximize margin inference step solving quadratic linear program  optimal labeling yˆ labeling deﬁned equations   implementation based                                                        library ooqp gertz wright asmen             log pωyˆ  − log pωy          tioned inference step performed ef  term zwx cancels maximization ﬁciently using graphcuts application  eﬃciently method referred maximum margin data sets comparably small linear pro  optimization details formulation omitted gram solver turned fast  sake brevity note problem  duced quadratic program qp form           instancebased extension              min   w  cξ                                 main drawback amn classiﬁer based                                                       loglinear model separates classes lin                   n                                   early assumes features separable hyper               ξ −   α ≥        ≥      st wxyˆ                               planes justiﬁed applications does                                                   hold instancebased classiﬁers nearest         α −      αk −  · ≥−  ∀                   neighbor nn classiﬁer nn classiﬁcation query data                 ij wn  xi   yˆi              ij ji∈e                                   point p˜ assigned label corresponds train                                                        ing data point features closest features         αk  αk −  ·  ≥  αk αk ≥   ∀  ∈            ij   ji  xij     ij ji     ij        x˜ p˜ learning step nn classiﬁer simply stores  variables solved qp entire training data set does compute reduced  weights  wn slack variable ξ additional vari set training parameters called lazy  ables αi αij α ji refer taskar et al classiﬁcation  tails                                                  idea combine advantage instance                                                        based nn classiﬁcation amn approach obtain    inference                                        collective classiﬁer restricted linear separa  optimal weights calculated bility requirement presented section  perform inference unlabeled test data set  ﬁnding labels maximize log pwy  men  transformed feature vector  tioned does depend maximiza suppose given general kclass classiﬁcation prob  tion equation  carried considering lem each training data point pˆ given fea  term constraints imposed variables ture vector xˆ length labely ˆ ∈know    yi leads linear program form         assume correct label new query feature                                                                                     ∗             n  k             k                     vector x˜ corresponds labely ˆ closest training ex                     ·           ·                    ∗         max       wn  xiyi       xijyij      ample feature space nn algorithm yields                      ij∈e                  optimal classiﬁcation course assumption valid                                                                                                   ∗                          k                            general say labely ˆ              ≥  ∀            ∀                   likely label example consider          st yi      yi                                                        twoclass problem features depicted left              ≤  ≤  ∀  ∈                     fig  twodimensional feature space deﬁned             yij  yi yij   ij                   training data shown boxes true labels                                                    ijcai                                                        training data test data ground truth nn          amn             iamn                                 figure  results data occupancy grid map    arbitrary test data set shown triangles related  experimental results  labels closest training feature points performed series experiments data  example probability labely ˜ ∈  corresponding                                         μ σ         compare instance–based amn iamn algorithm  x˜ proportional gaussian distribution   nn amn classiﬁer results experi  μk  dx˜ xˆ kandk ∈  xˆk denotes training ex                                       · ·             ments demonstrate iamn outperforms  ample label closest x˜  distance algorithms independent features used  feature space variance σ set     left side fig  attempt  implementation details  separate classes using hyperplanes case lines  lead severe classiﬁcation errors speed learning process need represent                         τ  rl →  rk                    feature vectors nearest neighbor search  troduce feature transform       way                              ﬃ  τ                                             feature space performed ciently end   x˜ dx˜ xˆ dx˜ xˆ transformed features        ˜  τ                                                 use kdtrees       store training feature vectors    x˜ easily separable hyperplanes                                     ﬀ  illustrated right side fig  reason each class ck way computational ort                                                        nearest neighbor lookup logarithmic  nn classiﬁer chooses label corresponding                                                        number stored instances  smallest component ˜t means set                                                        training step consists computing features  transformed feature points  ttk  assigned label described              training data transforming features according                                                        eq  assigning transformed features nodes     ∈ rk    ∧···∧      ∧       ∧···∧              tk      tk  tk− tk  tk     tk tk     amn edge features amn consist constant  seen equation border set tk scalar value described anguelov et al  described −  hyperplanes passing through ori solving quadratic program obtain weight vectors  gin case twoclass example separating wk inference step use transformed fea                                                             τ  hyperplane line described equation  tures mx test data node features amn                                                        edge features constant     nearest neighbors  problem nn classiﬁer assignment feature computation depending input data used  label query point p˜ depends labeling computed diﬀerent types features data points  instance training set feature case grid data each point map rep  vector closest x˜ possible features resented set geometrical features extracted  training instances close x˜ laser range scan covering ﬁeld view each  labeled diﬀerently example suppose                                                      laser simulated centered each point map rep  distances x˜ closest training features xˆ xˆ                                                                               resenting free space mozos et al   num  similar corresponding labelsy ˆ andy ˆ ber geometrical features huge  select   ﬀ                                       di erent decision assigning labely ˆ p˜ best ones using adaboost algorithm  wrong especially presence noise pro data set computed spin images johnson  ceed follows feature vector x˜ corresponds p˜  size  ×  bins spherical neighborhood  compute nearest training instances each computing spin images radius                                               classes   ck corresponding distances dx˜ xˆk  cm depending resolution input data    used  deﬁne transformed feature vector τmx˜as           τ                         data reduction   classifying data sets            x˜   dx˜ xˆk  dx˜ xˆ  points scans   scan points time  experiments show higher values increase clas memory requirements amn classiﬁer grows  siﬁcation rate large improvement small large cases perform data set reduction  experiments   turned good choice using kdtrees inserting data points tree                                                    ijcai                                                                             ground truth                             nn                                  amn                                   iamn                            figure  classiﬁcation results scene multi data set    prune tree ﬁxed level λ points subtrees single set training oc  level λ merged mean point spin cluded objects classiﬁcation performed  image features computed data complete scans  set provide high density feature extraction fig  shows typical classiﬁcation result scan                                                        multi test set nn assigns wrong labels    map annotation                                diﬀerent points neighbors classiﬁed correctly  classiﬁcation experiment used occupancy amn results show areas points tend clas  grid map interior building map annotated siﬁed label restriction  three diﬀerent labels ’corridor’ ’room’ linearly separable data object parts misclassi  ’lobby’ map divided nonoverlapping ﬁed especially complex objects like chairs fans  submaps training testing fig shows results obtained iamn better complex  submap used training fig ground truth objects transformed feature vectors computed  classiﬁcation results obtained nn iamn better suited classiﬁcation based separat  standard amn approach shown figs ing hyperplanes table  shows resulting classiﬁcation  fig shows result iamn algorithm rates iamn classiﬁer outperforms  seen iamn approach gives best result three data sets  classiﬁcation rate nn  higher statistical analysis using tstudent test signiﬁ  standard amn  classiﬁcation cance level  shown fig  seen  noisy contrast iamn result consistent multi set algorithm performs signiﬁcantly better  wrt neighboring data points highest classiﬁcation nn standard amn detailed analysis  rate                                       classiﬁcation results depicted fig  wchich demon                                                        strates iamn yields highly accurate results    scan point classiﬁcation                      threedatasets  furthermore evaluated classiﬁcation algorithms  three diﬀerent data sets overall number          data set  nn    amn    iamn  scanned scenes scans obtained laser             uman  range scanner ﬁrst data set called human                          sists  recorded scenes humans varying poses      single            scans labeled classes ’head’ ’torso’      multi             ’legs’ ’arms’ second data set named single             ﬀ  sists  di erent scans object classes ’chair’ table  classiﬁcation results three data sets  ’table’ ’screen’ ’fan’ ’trash can’ each scan sin  gle data set contains just object each class apart  tables screens standing  data set named multi consists seven scans  conclusions  multiple objects types single data set paper proposed algorithm combines    human single data sets evaluated using cross sociative markov networks instancebased approach  validation multi data set used object instances similar nearest neighbor classiﬁer identifying ob                                                    ijcai                                                    
