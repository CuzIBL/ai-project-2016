   selective supervision guiding supervised learning decisiontheoretic                                            active learning                                ashish kapoor eric horvitz sumit basu                                            microsoft research                                microsoft way redmond wa  usa                         akapoor horvitz sumitbmicrosoftcom                        abstract                            given cost labeling previously unlabeled cases                                                        cost misclassiﬁcation—which different dif      inescapable bottleneck learning large ferent classes—we seek quantify expected gain ex      data sets high cost labeling training data pected value associated seeking information unla      unsupervised learning methods promised    beled data point expected gain corresponds      lower cost tagging leveraging notions value information provided labeling guid      similarity data points assign tags ing principle active learning framework shall      unsupervised semisupervised learning   show employ value information learn      techniques provide poor results errors ing gaussian process classiﬁcation framework      estimation look methods guide  applicability approach supervised      allocation human effort labeling data semisupervised scenarios      greatest boosts discriminatory power review key work active learning      increasing amounts work focus ap   scribe active learning framework uses expected value      plication value information gaussian pro ofinformation voi criterion triage labeling      cess classiﬁers explore effectiveness present concepts computational issues use      method task classifying voice messages gaussian process classiﬁcation show meth                                                        ods extended handle semisupervised learning    introduction                                       highlight effectiveness framework task  increased sensing decreased storage costs leading classifying voice messages conclude experimen  growing availability large data sets data tal results discussion  sources unavailable machine learning rea  soning high cost labeling cases ex  background  ample access thousand voice mes growing recent years active learn  sages stored server wish build classiﬁcation ing numerous heuristics schemes proposed  automatically classify voicemail messages choosing unlabeled points tagging example fre  different categories unfortunately performing super und et al  propose disagreement commit  vised learning data set require manual ef tee classiﬁers criterion active learning tong  fort listening voice messages applying labels koller  explore selection unlabeled cases query    unsupervised learning shows promise reducing ef based minimizing version space support  fort required tagging use preparing data vector machines svm formulation gaussian  sets supervised learning pure unsupervised process framework method choice look  learning based notions clusters similarity expected informativeness unlabeled data point  fraught labeling errors                         mackay  lawrence et al  speciﬁcally idea    believe rich space opportunities choose query cases expected maximally  realm complementary computing horvitz paek inﬂuence posterior distribution set possible   machine learning focus ideal coupling classiﬁers additional studies sought combine ac  human supervision unsupervised methods dis tive learning semisupervised learning mccallum  cuss selective supervision use valueofinformation nigam  muslea et al  zhu et al   triage human tagging efforts activelearning method methods inherently focus minimizing  considers cost required tag data misclassiﬁcation rate focus value moving  costs associated use classiﬁer realworld set decisiontheoretic framework active learning  ting show minimize total cost associated consider costs risks realworld currencies em  construction use classiﬁcation systems ploy computations expected value information bal  costs measured currencies monetary quantities ance cost misdiagnosis costs providing la  valuable resources                          bels                                                    ijcai                                                       decisiontheoretic active learning                 data points                                                                                                                                                       ∗              ∗  linear classiﬁer parameterized classiﬁes test point ju       − pi ·   rpi ·  −                                                                                            according signfx fxw given set          i∈u  training data points xl   xn class labels ∗                                                          pti xi true conditional density  tl   tnwhereti ∈ − goal learning   algorithm learn parameters classiﬁcation class label given data point                                                        true conditional compute expression exactly  techniques focus minimizing classiﬁcation error                        p∗      preferences relative numbers false positives approximate approxi                                                        mate total risk unlabeled data points  negatives produced classiﬁcation vary            person task preferences expressed terms     ju ≈     − pi · pi       realworld measures cost monetary value           i∈u  seek minimize expected cost use                                                        let ci denote cost knowing class label xi  classiﬁer time additionally cost tag                                    ging cases training vary cases different assume costs risks    classes problemspeciﬁc variables      measured currency assumption does    aim quantify values acquiring labels differ impose signiﬁcant constraints different currencies  ent data points use computations values transformed single utility using appropriate real                                                        world conversions  guiding principle active learning intuitively knowing                             label currently unlabeled points reduce given risks   approximate expected                                                                                     j¯   jlju  total risk classiﬁcation task hand la misclassiﬁcation cost point  nm  assuming  bels acquired price difference reduction closed world encounters   total expected cost use classiﬁer points xl ∪xu  expected cost sum total                                                        risk jall  mj¯ cost obtaining labels  shall refer risk cost acquiring new label                           expected value information learning label                                                                  jall    ci  jl  ju     ci        realworld cost associated usage classiﬁer                                                                           i∈l               i∈l  function number times classiﬁer  used real world probability distribution usage querying new point reduction  considered computation expected cost    total risk cost incurred query label    simplicity shall focus discussion computing difference quantities triages  class discrimination problems methods discussed selection cases label formally deﬁne voi  paper generalized straightforward manner han unlabeled point xj difference reduction  dle multiple classes note work presented total risk cost obtaining label  makes myopic assumption seek label                                                                                            voixj    −  jall − jall − cj     data point time generalized applying                                                                    lookahead procedures consider acquisition labels jall denote total expected cost  different sets points                         tal misclassiﬁcation risk respectively consider xj la                                           ×    let deﬁne risk matrix rij  ∈ ir  beled voi quantiﬁes gain utilities terms  rij denote cost risk associated classifying data realworld currency obtained querying point  point belonging class use index  strategy choose labeling  note class  assume diagonal elements point highest value information results  zero specifying correct classiﬁcation incurs cost minimization total cost consists total risk  given labeled set xl labels tl train misclassiﬁcation labeling cost note  classiﬁer fx compute total risk labeled data approach differs earlier methods active learn  points                                            ing focus minimize classiﬁcation                                                      error           jl       − pi     rpi            let consider xj querying note need                  i∈l             i∈l−                   compute expression voi know label                                                                                                                                 xj total risk computed know                                                                       denotes probability point classiﬁed actual label tj similarly cj computed                                        class  sign    ifurther    costs labels different different classes approxi       − indices positively negatively labeled              jth                                                      mate terms data point expectation  points respectively note predictive distribu                       j−                                                        empirical risk jall ≈ pjj   − pjhere  tion depending classiﬁcation technique  j−  available predictive distributions available denote total risks xj labeled  gaussian process classiﬁcation section  proba class  class  respectively risks written  bilistic classiﬁers including probabilistic mappings sum risks labeled unlabeled data following                                                                        j−   j−    j−  puts svms platt                                                              labeled cases set unlabeled data                                points xu  xn  xnmthatwewishtoclassifywe     approximation used case previ  seek include total risk associated unlabeled ously seen unseen provided density px does change                                                    ijcai                                                                                                                          equal risk        equal label cost       asymmetric risk        equal label cost       equal risk        asymmetric label cost                                                                                                                                                                                                                                                                                                                                                                                   −                               −                               −      −                                  −                                 −         −   −                  −  −                  −  −                                                                                              figure  selection points query label based value information circles represent unlabeled cases radii  correspond voi labeling each case squares class  triangles class  represent previously labeled cases  different ﬁgures correspond following situations symmetry costs risks labeling asymmetric risk  asymmetric label costs cross corresponds selection query curve denotes decision boundary based  currently available labels    calculate risks ﬁrst compute pj resulting marked cross figure shows voi  posterior probability adding xj positively labeled unlabeled data points case selected query  example active set using similar expressions risks cost labelings equal                                      equations   compute jl ju therisks  classes situation cases nearest deci  labeled unlabeled data points xj sion boundary associated highest voi choosing  sumed positively labeled corresponding computa cases minimize objective overall cost corresponds                 j−    j−                             selection queries minimize classiﬁ  tions follow jl ju similarly use  expectation cj costs labeling vary class cation error points decision boundary    strategy select cases labeling ones informative figure illustrates  highest voi                                      situation far expensive misclassify                                                        point belonging class  asymmetry risks                jsel argmaxvoixj                  points likely belong class  lay                          j∈u                                                        close decision boundary highest voi figure                     voi   note   jsel  zero depicts situation obtaining label point  condition knowing single label does reduce class   times expensive obtain label  total cost situation employed stopping point belonging class  voi highest  criterion stopping criteria openworld situations points likely belong class   include computation gains accuracy classiﬁer close decision boundary sample data set illustrates  multiple uses based probability distribution voi used effectively guide tagging supervision  expected cases lifespan note minimizes operational training costs  greedy policy indicate stopping classiﬁer  potential reduction overall cost querying point assumed closed  set points                                      set labeled unlabeled data avail    demonstrate approach starting illustra able prior work active learning includes stud  tions toy data set shall employ gaussian process ies assume open data points arrive  classiﬁcation section  active learning frame time methods discussed paper  work figure  shows selection unlabeled points extended handle open case using aver                                                                         jl  query based voi criterion sample data consists age empirical risk   guiding principle earlier used  half moons multidimensional space zhu et al  note transductive learn  half belongs class  class  ing framework ﬁnal classiﬁcation boundary depends  simulation start cases labeled labeled data labeled unlabeled data  represented squares class  triangles points used determine cases query  class  different graphs ﬁgure correspond dif trained classiﬁer applied novel test points  ferent settings risks labeling costs yond original set labeled unlabeled points  assume costs querying points shall section  extensions handle trans  belong class  − respectively unlabeled points ductive case using semisupervised learning algorithm  displayed circles radii correspond voi train classiﬁer pause review  labeling cases case selected queried gaussian process classiﬁcation                                                    ijcai                                                                                     table  features extracted voice messages                                   prosodic features                 metadata information                          ∗ includes max min mean variance                                  duration silence∗                    weekend                               duration voiced segment∗            work day                                    absolute pitch∗                   pm work day                              length productive segment∗        hours work day                                    length pause∗                      size bytes                         change pitch during productive segments∗     size seconds                   rate features syllable silence productive segments pauses external caller      gaussian process classiﬁcation                     psignfxx  work explore active learning use gaus                                                                                                                     w¯  sian process gp classiﬁers advantages using sign                                                                                                    Ψ               gp classiﬁcation directly model predictive                         Σwx    ditional distribution ptx consequently making easy  compute actual conditional probabilities cal unlike classiﬁers gp classiﬁcation models pre                                                                                  tx  ibrations postprocessing gp methods provide bayesian dictive conditional distribution   making easy com  interpretation classiﬁcation approach goal pute actual conditional probabilities calibra  infer posterior distribution set possible tions postprocessing probabilistic interpretations  classiﬁers given training set                      kernel classiﬁers svm sollich                                                        attempts map output classi            pwxl tlpw     ptiw xi         ﬁers directly probability platt  approach                              i∈l                       use predictive distribution selectivesupervision                                                    framework compute expected risks quantify    corresponds prior distribution clas value information  siﬁers selected typically prefer parameters  small norm speciﬁcally assume spherical  computational issues  gaussian prior weights ∼n prior im  poses smoothness constraint acts regularizer mentioned earlier adf ep used approximate  gives higher probability labelings respect inference gp classiﬁcation proposed scheme  similarity data points likelihood terms selecting unlabeled points computationally expensive                                                                                                     ptiw xi incorporate information labeled data note computational complexity ep  different forms distributions selected popu size labeled training set proposed method  lar choice probit likelihood ptw xΨt · wt compute voi unlabeled data point  Ψ· denotes cumulative density function quiring perform ep twice point consid  standard normal distribution posterior prefers pa eration  rameters small norm consistent faster alternative use adf approximating  training data                                        new  posterior classiﬁer com    computing posterior pwx   nontrivial puting ep speciﬁcally compute new posterior                                                          approximate inference techniques assumed density wxl∪j tl ∪ compute gaussian pro  filtering adf expectation propagation ep typ jection old posterior multiplied likelihood term                                                               th                   ically required idea adf approximate data point wxl∪j tl ∪  ≈              wx                                                      posterior     gaussian distribution w¯  Σw wherew¯  Σw   respectively  wx   ≈n                                                                                                ¯ Σw similarly ep approx mean covariance pwxl tl · Ψ · xj  imate inference technique ep generalization adf equivalent performing adf starting old  approximation obtained adf reﬁned posterior pwxl tl incorporating likelihood term                                                                                         ing iterative message passing scheme refer readers Ψ · xj  does require  operations com  minka  details                          pute voi unlabeled data point use similar                                      wx     ∼                              j−    given   approximate posterior                 computations approximate wxl∪j tl ∪−  w¯  Σw frequent practice choose mean   ¯ distribution point classiﬁer mean  supervised semisupervised learning  called bayes point classiﬁes test point according  signw¯ relatively straightforward generalize underlying classiﬁer proposed framework based  nonlinear case using kernel trick gaussian processes easily extended  idea ﬁrst project data higher dimensional semisupervised case kapoor  sindhwani et al   space make separable evgeniou et al    speciﬁcally core gp classiﬁcation kernel    byproducts using  gp  classiﬁca  matrix kwhereentrykij  encodes similarity  tion framework obtain predictive distribution ijth jth data points using                                                    ijcai                                                                               caller close  close               mobile  non−mobile                                              voi                                   voi                                                                                                    random                                random                                                                                                      uncertainity                          uncertainity                                                                                                      entropy                               entropy                                                                                                                                                                                                                                                                                                                                                                                                                                                     error  unlabeled points          error  unlabeled points                                                                                                                                                                                                                          number labeled points               number labeled points                                                                                                caller close  close               mobile  non−mobile                                                               voi                                   voi                                                                                                 random                                random                                           uncertainity                      uncertainity                                           entropy                            entropy                                                                                                                                                                                                                                                                                                                                     total  cost incurred                  total  cost incurred                                                                                                                                                                                                                        number labeled points               number labeled points                                                                        figure  comparison different active learning schemes graphs show error unlabeled points versus number  labels classifying voicemails graphs show total cost incurred versus number labels voi criteria provide good  classiﬁcation performance signiﬁcantly lower costs results averaged  runs error bars represent standard error    similarity matrix gp classiﬁcation use inverse routing email messages statistical classiﬁers horvitz  transformed laplacian                         et al          rΔ  Δ   σi     Δd    −              given set voice messages ﬁrst extract features                                                        promise value discriminating target  the diagonal matrix diagonal elements classes speciﬁcally look prosody metadata  dii   kij σ  added remove zero accompanies messages  eigenvalue spectrum rΔ intuitively prosodic features consider multiple prosodic features  computing similarity directly kernel kij including syllable rate pause structure pitch dynamics  inverse transformed laplacian computes similar employ pitch tracker extract prosodic fea  ity manifold unlabeled data points help tures summarized table   classiﬁcation populating manifold using message metadata extract metadata voice  similarity manifold guide decision bound messages speciﬁcally extract features indicate  ary extension gp classiﬁcation handle semi day time additionally consider  supervised learning recently studied kapoor  size voicemail bytes length  sindhwani et al  related graphbased message seconds extract features indicate  methods semisupervised learning rest active caller calling outside recipient’s orga  learning framework used semi nization metadata features shown table   supervised gp classiﬁcation framework                                                          explore selective supervision concepts applied                                                        voicemail classiﬁcation challenge data set    sample challenge classifying voicemail            sists  labeled voice messages received single user  shall challenging classiﬁcation task period  months explore use methods  highlights value employing selective supervision guide labeling efforts supervised classiﬁcation  methods speciﬁcally goal build notating voicemail tedious shorter voicemails  classify voice messages ways including labeled quickly longer ones use  messages urgent vs nonurgent caller person asymmetric cost criterion cost label scales  ally close vs close person called length voicemail speciﬁcally assume  detect caller calling mobile phone classi cost labeling voicemail  dollars second  ﬁcation task related prior work prioritization message length experiments assume                                                    ijcai                                                     
