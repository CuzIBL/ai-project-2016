                    subtree mining question classiﬁcation problem              minh le nguyen                  thanh tri nguyen                    akira shimazu       japan advanced institute      japan advanced institute       japan advanced institute        science technology            science technology            science technology          nguyenmljaistacjp               tthanhjaistacjp              shimazujaistacjp                          abstract                          common words like ”what” ”else” considered                                                        ”stopwords” omitted dimension reduction step      question classiﬁcation putting questions process creating features document      semantic categories important words important question classiﬁcation      question answering paper introduces   word frequencies play important role document      new application using subtree mining ques classiﬁcation frequencies usually equal       tion classiﬁcation problem formulate   question signiﬁcantly contribute      problem classifying tree certain la performance classiﬁcation furthermore shortness      bel set labels present use question makes feature space sparse makes      subtrees forest created training data feature selection problem difﬁcult different      tree classiﬁcation problem maximum   document classiﬁcation      entropy boosting model used classi      ﬁers experiments standard question classiﬁ     main approaches question classiﬁca      cation data show uses subtrees  tion ﬁrst approach does question classiﬁcation                                                                                                        maximum entropy boosting models    ing handcrafted rules reported voorhees                                                                                           promising results indicate method  voorhees  voorhees   pos      achieves comparable better performance  sible manually write heuristic rules task      kernel methods improves testing efﬁ usually requires tremendous tedious work      ciency                                           correctly ﬁgure various forms each speciﬁc type                                                        questions order achieve reasonable accuracy                                                        manually constructed classiﬁcation program    introduction                                         second approach applies machine learning ques  current information retrieval just tion classiﬁcation problems radev et al   ”document retrieval” given keywords machine learning tool ripper utilized  turns relevant documents contain keywords problem author deﬁned  question categories trained    user really wants precise tested question classiﬁcation trec dataset  swer question instance given question ”who reported accuracy   ﬁrst american space” user really wants high question classiﬁcation li li  answer ”alan shepard” read through lots roth  presented use snow method  documents contain words ”ﬁrst” ”american” good performance depends fea  ”space”                                          ture called ”relwords” related word constructed    order correctly answer question usually needs semiautomatically automatically author  understand question asked question classi ported useful features question classiﬁcation  ﬁcation impose constraints plausible task including words parts speech chunking labels  answers suggest different processing strategies named entity labels addition indicated hierar  instance understands question ”who chical classiﬁcation methods natural question classi  ﬁrst american space” asks person ﬁcation problem question classiﬁed classiﬁers  search space plausible answers signiﬁcantly ﬁrst classiﬁes coarse category second  duced fact opendomain question answer classiﬁes ﬁne category  ing systems include question classiﬁcation module zhang leezhang lee  employed tree ker  accuracy question classiﬁcation important nel svm classiﬁer indicated syntactic  overall performance question answering formation tree kernel suitable problem    question classiﬁcation little different document authors showed performance  classiﬁcation question contains small number coarse category report classifying ques  words document large number words tions ﬁne categories                                                    ijcai                                                      hand connection data mining tech  niques machine learning problem recently shows  data mining help improve performance classiﬁer  morhishita morhishita  showed use associ  ation rule mining helpful enhancing accuracy  boosting algorithm berzal et al berzal et al    presented family decision list based association  rules mined training data reported good result  uci data set li roth     mining subtrees helpful reranking  syntactic parsing problem kudo et al  en  semble learning semantic parsing nguyen et al  figure  labeled ordered tree subtree relation  success natural question  subtree mining helpful question classiﬁcation prob  lem paper investigates connection subtree mining decision stump functions observed training data  maximum entropy boosting model formu set trees labels functions  late question classiﬁcation tree classiﬁcation prob incorporated maximum entropy model boosting  lem present efﬁcient method utilizing sub model  trees forests created training data max  imum entropy model subtreemem boosting model   boosting subtree features  subtreeboost tree classiﬁcation problem   decision stumps classiﬁers trees inaccurate    remainder paper organized follows sec applied real applications ﬁnal decision  tion  formulates tree classiﬁcation problem relies existence single tree accu  maximum entropy model boosting model using subtree racies boosted boosting algorithmschapire  features presented section  discusses efﬁcient subtree  boosting repeatedly calls given weak learner ﬁ  mining methods section  shows experimental results nally produce hypothesis linear combination  section  gives conclusions plans future work hypotheses produced prior weak learners                                                                   k                                                        fxsgn    αkhtkykx                                                          weak learner built each iteration differ    classiﬁer trees                                                                                                                       ent distributions weights di   dl   assume each sentence question parsed tree                                                                tree sequence word dependency tree di di  ≥  weights calculated  syntactic tree                                          problem question classiﬁcation equivalent clas way hard examples focused easier ex  sifying syntactic tree set given categories amples                                                                                     deﬁnition                                                                                                                                   gainty       yidihtyxi  tree classiﬁcation problem induce mapping fx                      →     given training examples            xiyiiwherexi ∈  labeled ordered tree exist boosting algorithm variants  yi ∈   class label associated each train original best known algorithm  ing data important characteristic input exam adaboostschapire  reason used ad  ple xi represented numerical feature vector bagof aboost algorithm decision stumps serving weak  words labeled ordered tree                    functions    labeled ordered tree tree each node asso  ciated label ordered siblings  maximum entropy model subtree  ﬁrst child second child child    features    let labeled ordered trees say matches maximum entropy models berger et al   uort subtree ⊆  exists onetoone make unnecessary independence assumptions  function ψ nodes satisfying conditions able optimally integrate sources  ψ preserves parentdaughter relation ψ preserves knowledge believe potentially useful transfor  sibling relation ψ preserves labels      mation task maximum entropy model max    let labeled ordered trees class label imum entropy model able exploit features  yi ∈   decision stump classiﬁer trees beneﬁcial effectively ignore ir  given                                            relevant model probability class given vector                             ⊆                     features according formulation              htyx                                                                       n                                                                              exp  λific    figure  shows example labeled ordered tree                                                                                      pcx                              subtree nonsubtree                                                          zx                                                    ijcai                                                      zx normalization constant fic fea di normalized weight assigned xigivent ﬁnd  ture function maps each class vector element optimal rule ty  maximizes gain value  binary feature total number features λi naive exhaustive method ﬁrst  weight given feature function weights λi enumerate subtrees calculate gains  features fic estimated using optimization tech subtrees usually impractical number subtrees  nique lbfgs algorithm liu nocedal  exponential size method ﬁnd optimal rule    current version maximum entropy model using modeled variant branchandbound algo  subtrees deﬁne each feature function fic using sub rithm summarized following strategies  trees class label value feature function • deﬁne canonical search space set  received  observed training data subtrees set trees enumerated  received  current framework                                                          •  maximum entropy model subtrees mined incorporated optimal rule traversing search space  mem feature functions mentioned  • prune search space proposing criterion respect                                                            upper bound gain    efﬁcient subtree mining                              order deﬁne canonical search space applied                                                                                                           goal use subtrees efﬁciently mem boosting efﬁcient mining subtree method described zaki    models number subtrees corpus used branchandbound algorithm  large method generating subtrees corpus deﬁned upper bound each gain function pro  intractable                                          cess adding new subtree deﬁne bound each    fortunately zaki zaki  proposed efﬁcient subtree based calculation following theorem                                                        morhishita   method rightmostextension enumerate subtrees      ⊇       ∈                         given tree algorithm starts set trees     gain ty   sisting single nodes expands given tree size bounded μt    attaching new node tree obtain trees                                                                                                                        l                  l  size inefﬁcient expand nodes                                                        μtmax           di −  yidi        di    yidi  arbitrary positions tree duplicated enumeration                                                                   iyit∈xi    iyi−t∈xi  inevitable algorithm rightmost extension avoids                                              duplicated enumerations restricting position attach  ment subtrees used parameters bellow   note equation used binary classiﬁca    • minsup minimum frequency subtree data tion adapt multi classiﬁcation problem    •                                                   use common strategies onevsall onevsone      maxpt maximum depth subtree             error correcting code consider multi classiﬁcation    • minpt minimum depth subtree             problem combination binary classiﬁcations                                                        paper focus use onevsall multiclass    table  shows example mining results                                                        problems subtrees selected using                                                        bound mentioned binary classiﬁcation          table  subtrees mined corpus       ing boosting            frequency        subtree                      efﬁciently prune search space spanned right                      vpvbzfeaturespp            extension using upper bound gain μtdur                     vpvpvbnconsidered           ing traverse subtree lattice built recursive                      vpvpvbncredited            process rightmost extension maintain tem                       vpvpvbndealt              porally suboptimal gain δ gains calculated previ                                                                                                                          vpvpvbninvented            ously μt delta gain supertree ∈                                                        greater delta  safely prune                                                        search space spanned subtree    subtree mining using right extension used prune branch algorithm selecting subtrees  maximum entropy model boosting model listed algorithm   following subsection subtrees note means length given subtree addi  used mem boosting                             tion algorithm  presented algorithm classes                                                        current work applied binary class prob    subtree selection boosting                   lem  subtree selection algorithm boosting based training model using subtrees used  decision stump function eliminate subtree vsall strategy obtain label given question  gain value appropriate boosting process  borrow deﬁnition kudo et al  kudo et al  subtree selection mem   shown bellow                                count cutoff method    let  xyd    xlyldl  training subtree selection maximum entropy model  data xi tree yi labeled associated xi ducted based right extension way each subtree                                                    ijcai                                                      input let  xyd    xlyldl   experiments previous work collected    training data xi isatreeandyi labeled associated sources  english questions published usc    xi di normalized weight assigned xi  manually constructed questions rare classes                             output optimal rule                         trec  trec  questions  questions    begin                                               trec  standard data used previous    procedure projectt                                work question classiﬁcation zhang lee li    begin                                               roth  labels each question followed         ≤    μt δ                                      layered question taxonomy proposed li roth        return     end                                                 contains  coarse grained category  ﬁne grained                                                       categories shown bellow each coarse grained category    argmaxy∈k gainty                 gainty  δthen                                 contains nonoverlapping set ﬁne grained categories       ty ty                                     • abbrs  abbreviation expansion       δ  gainty                                    • desc  deﬁnition description manner reason    end                                                         •    each ∈set trees rightmost extension  enty  animal body color creation currency dis        single node added right extension        easemedical event food instrument language letter       μt ≤ δ continue                                 plant product religion sport substance symbol                     projectt                                          technique term vehicle word    end                                                   • hum description group individual title    end projectt                                                                     •    ∈ tt ∈∪i tt ⊆ xi                  loc city country mountain state                     projectt                                        • num code count date distance money order    end                                                     percent period speed temperature size weight                return                                           order obtain subtrees proposed methods    end                                                 initially used chaniak parser charniak  parse            algorithm  optimal rule              sentences training testing data obtained                                                        new training data consisting set trees                                                        labels mined subtrees using right extension                                             mined using algorithm described zaki  algorithm zaki  table  shows running example  frequency maximum length minimum length using maximum entropy models subtrees train  subtree used select subtrees frequency sub maximum entropy model used lbfgs algorithm  tree suitably used count cutoff feature selec liu nocedal  gaussian smoothing  tion method maximum entropy models cur  rent framework maximum entropy model used  simple feature selection method using count cut method table  running example using maximum entropy  results gained question classiﬁcation model subtrees  promising                                                         features                                    weight  using boosting                                         enty adjpadvp                            order ﬁnd effective subtree features maximum en enty adjpinof                        tropy model applied subtree boosting selection enty adjpjjcelebrated                 presented previous section obtained subtree hum adjpjjcommon                     features running subtrees boosting algorithm hum adjpjjdead                    performing data set vs strategy fact entysubstance npadjprbsmostjjcommon   obtained  data set  class labels question classi numcount adjpppinfor        ﬁcation data subtree features applying boost numcount npdtthennnumber      ing tree algorithm each data set combined  set features maximum entropy models advantage evaluate proposed methods question classiﬁca  using boosting subtrees maximum entropy models tion used evaluation method presented li  ﬁnd optimization combination subtree roth zhang lee  formula  features maximum entropy principle using ef                 correct predictions  ﬁcient optimization algorithm lbfgs algorithm     accuracy   liu nocedal  opinion combination                    predictions  subtree features using maximu entropy models better conducted following experiments conﬁrm ad  linear combination mechanism adaboost vantage using subtree mining classiﬁcation                                                        maximum entropy model boosting model experi                                                        ments using signiﬁcant test  conﬁdent    experimental results                               interval table  shows accuracy subtree maxi  experiments conducted pentium iv  ghz mum entropy model stmem subtree boosting model st  tested proposed models standard data similarly boost combination subtree boosting maximum                                                    ijcai                                                    entropystmb tree kernel svm svmtk  coarse grained category respectively shows boost table  question classiﬁcation accuracy using subtrees  ing model achieved competitive results svm tree der ﬁne grained category deﬁnition total  labels  kernel stmb outperforms svmtk achieve     parameters         subtreeboost subtreemem  best accuracy                                       maxpt minsup                                                                           maxpt minsup                                                                          maxpt minsup                 table  question classiﬁcation accuracy using subtrees  der coarse grained category deﬁnition total  labels         stmem    stboost  stmb   svmtk             boosting maximum entropy model mathe                                        matical formulation relation worth investigating                                                          subtree mining approach question classiﬁcation  table  question classiﬁcation accuracy using subtrees relies parser syntactic trees  der ﬁne grained category deﬁnition total  labels parsers including charniak’s parser used       stmem    stboost  stmb    mem    svm          experiments targeted parse questions                                    parsers usually trained penn treebank                                                        corpus composed manually labeled newswire    table  shows performance subtreemem subtree text surprising parsers  boost subtreemb question classiﬁcation achieve high accuracy parsing questions  shows result using word features maximum entropy questions training corpus herm  models mem support vector machine models svm jakob  accuracy question parsing dramatically  result using svm tree kernel reported improves complementing penn treebank corpus  original paper zhang lee  author did marcus et al  additional  labeled ques  report performance svm using tree kernel ﬁne tions training believe better parser beneﬁcial  grained category paper reported perfor approach  mance ﬁne grained category using subtrees max summarily experiments show subtree mined  imum entropy boosting model corpus useful task question classi  using semantic features relation words obtain ﬁcation indicated subtrees used feature  better result comparison previous work table  functions maximum entropy model weak function  dicates subtreemb obtained highest accuracy boosting model addition paper al  comparison methods utilizes ad ternative method using subtrees question classiﬁcation  vantage subtree features selection using boosting results showed experiment conclude  maximum entropy principle                            mining subtrees signiﬁcantly contribution    subtree information semantic features formance question classiﬁcation  named entity types relation words paper investigate use syntactic tree repre  overlap mixing features improve sentation overlap named entity types  performance question classiﬁcation task           relation words tree structure representation    results indicate boosting model outperformed combination syntactic semantic information bene  maximum model using subtrees computational time ﬁcial approach future like investigate  mem faster boosting model         use semantic parsing semantic role labelling    training time maximum entropy models approx task  imately  times faster boosting model needed  approximately  hours ﬁnish training process  conclusions  boosting models  training data examples  ing maximum entropy model needed approxi paper proposes method allows incorporating  mately  hours computational times combination subtrees mined training data question classiﬁ  subtree boosting features maximum entropy models cation problem formulate question classiﬁcation  comparable subtree boosting model             problem classifying tree preﬁxed categories    testing using mem approximately  times proposed use maximum entropy booting  faster comparison boosting model guess model subtrees feature weak functions consid  reason boosting model ﬁnd optimal rules ering subtree mining subtree feature selection process  during training process addition larger number experimental results show boosting model  classes  labels affect computational time achieved high accuracy comparison previous work  strategy onevsall method         using semantic features boosting model    table  depicted results boosting model maxi performed maximum entropy model using subtrees  mum model using subtrees minsup maxpt computational times slower  times com  mean frequency maximum depth subtree parison proposed maximum model ad  respectively table reported relation dition combination using boosting maximum en  tween subtree mined parameters performance tropy models subtree features achieved substantially bet                                                    ijcai                                                    
