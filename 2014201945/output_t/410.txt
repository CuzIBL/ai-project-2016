                                   learning restart strategies                            matteo gagliolo∗†     jurgen¨ schmidhuber∗†‡                                         matteojuergenidsiach                          ∗ idsia galleria   manno lugano switzerland                               † university lugano faculty informatics                                  bufﬁ   lugano switzerland                                       ‡ tu munich boltzmannstr                                      garching m¨unchen germany                        abstract                          used determine optimal uniform strategy based                                                        constant cutoff luby et al  paper      restart strategies commonly used minimiz authors argue strategy little practical inter      ing computational cost randomized algo    est model performance usually unavailable      rithms require prior knowledge runtime present universal restart strategy performance      distribution order effective propose worse optimal strategy logarithmic factor      portfolio strategies ﬁxed prov strategies based opposite hypotheses      able bound performance based     availability rtd alternative run      model runtime distribution updated  time sample set problem instances collected      strategies run sequence problems andusedtolearnamodel  underlying rtd      computational resources allocated probabilis potential obstacles approach gathering      tically strategies based perfor meaningful sample runtime data computationally      mances using wellknown karmed bandit prob   expensive especially case restart strategy      lem solver present bounds performance  useful runtime exhibits huge      resulting technique experiments variations gomes et al  second given problem      satisﬁability problem solver showing rapid conver set contain instances signiﬁcantly different rtds      gence nearoptimal execution time           obtained model capture overall behavior      keywords randomized algorithms restart strate  algorithm set corresponding restart strategy      gies performance modeling heavytailed distribu suboptimal each instance problems met      tions algorithm portfolios satisﬁability lifelong initial training phase captured poorly      learning adversarial multiarmed bandit problem estimated rtd leading suboptimal strategy                                                          obstacles difﬁcult circumvent training    introduction                                       cost reduced using small censored sample                                                        runtime values obtained aborting runs exceed  trying communicate lossy communication cutoff time obviously impact model’s  channel unbounded delays long wait precision requirements sense strict  resending unanswered message multimodal expect coarse model allows  function optimization using gradient descent steps evaluate nearoptimal strategy gagliolo schmidhuber  starting new random initial second problem unavoidable worst case  point randomized search tree deep setting practical situations expect rtd  abandoning hopes ﬁnding goal each instance problem set uncorrelated  starting search anew similar situations desirable suboptimal uniform strategy based set rtd  minimize time solve given problem instance great advantage large bound universal  given algorithm runtime random variable paper show luby’s universal uniform  possibly unknown distribution restart strategy restart effectively interleaved solve set problem  used generate sequence “cutoff” times instances uniform strategy based nonparametric  algorithm restarted unsuccessful               model rtd problem set updated    proved knowledge runtime distribu                                                       time new problem instance solved online fashion  tion rtd given algorithmproblem combination resulting exploration vs exploitation tradeoff treated                                                        bandit problem solver auer et al  used    following sake readability refer  rtd problem instance meaning rtd different allocate runs strategies  runs randomized algorithm instance model converges uniform strategy favored rtd  rtd problem set meaning rtd different runs  randomized algorithm each run different instance uniform randomly picked replacement set                                                    ijcai                                                     set close rtd instances worst cases obvious tradeoff faced ex  case bounds performance preserved   ploration performance various akandexploita    following restart strategies sect  brieﬂy tion solvers estimated fastest wellknown  troduced followed discussion “adversarial bandit” aged paradigm addressing tradeoff  approach algorithm selection sect  used multiarmed bandit problem basic form robbins  strategy sect  references related  faced greedy gambler playing sequence  work sect  sect  presents experiments random trials karmed slot machine each trial consisting  ized satisﬁability problem solver sect  discusses potential choice available arms rewards ran  impact viable improvements presented approach domly generated different stationary distributions                                                        gambler receive corresponding reward    optimal restart strategies                         gret gained pulled                                                        arms aim game minimize  restart strategy consists executing sequence runs                                                        regret deﬁned difference expected cumu  randomized algorithm order solve problem                                                        lative reward best arm cashed  instance stopping each run time solu                                                        gambler generally speaking bandit problem solver bps  tion restarting algorithm different                                                        described mapping entire history ob  random seed operationally deﬁned function                                                        served rewards xk each arm probability distribution   → producing sequence thresholds em                                                            pk  choice successive  ployed luby et al  proved optimal restart                                                           trial picked  strategy uniform  constant rt  used bound each run show case recent works original restricting assumptions                                                        progressively relaxed partial information case  expected value total runtime tt evaluated                                                        reward pulled arm revealed gam                                                      −                           bler nonoblivious adversarial setting rewards                            τdτ                ett                               given trial arbitrary function entire                                                  history game allowed deceptive    cumulative distribution function cdf set current choice based  runtime unbounded run algorithm pessimistic hypotheses auer et al  devised  function quantifying probability problem probabilistic gambling scheme exp regret  solved time                                 ﬁnite number trials bounded high probability                                                ∗    distribution known optimal cutoff time om log number arms  evaluated minimizing  authors suggest number trials alg  algorithm features ﬁxed  universal nonuniform restart strategy cutoff sequence lower bound γ exploration probability parame           proving performance tu ter α controlling exploitation set  bounded high probability respect expected based obtain optimal regret rate  runtime ett ∗  optimal uniform strategy                                                        algorithm  expm auer et al            tu  ett ∗ logett ∗                                                                 set α  log km     algorithm selection bandit problem              set γ min log km                                                          initialize sk      consider sequence   bm  problem                                                        each trial    stances set algorithms  aa  ak               sk                                                              set pk ∝   α    pk   each bm solvable ak loss                                                                                 pick arm probability pˆk   − γpk  γk  generality assume execution each ak                                                            observe reward xk ∈    subdivided sequential steps indicate tkr                xkγ                                                            update sk  sk      duration rthstepofthekth algorithm             pˆkk  current problem each ak mechanism end  determining tkr iterative algorithm tkr  cost rth loop  setting generally selection best algorithm set problems  wants solve incoming problem instances fast pos naturally described karmed bandit setting  sible mean identifying single best algorithm  set each instance algorithm selection original formulation based ﬁnite upper bound  rice  general setting algorithm cumulative reward best arm each  portfolios huberman et al  optimal schedule reward  avariantexp algorithm proposed  interleaving execution different algorithms each unknown authors present better bound  forming different subsets               expected reward                                                           cases selection perinstance basis algorithm                                            j−    sequence composed powers used portfolios difﬁcult treat bandit problem solvers usually                                              j−  twice  precisely    provide bounds regret respect single arm best                       j−      j−         −  rt −     ≤  −         algorithm different each problem                                                    ijcai                                                                                                                                                          ∗  “pick arm k” means “execute step algorithm ak” ing instances assuming priori information  alg                                              ideal candidate universal strategy luby sect                                                         invests equal portions runtime set exponentially  algorithm  algorithm selection using bps             spaced restart thresholds note restart thresholds    initialize bps                                  unsuccessful runs exploited form censored    each problem                                 samples simple scheme allow      set tk   rk                    formance bound universal strategy during training      repeat                                            use gathered data estimate fˆ corresponding        pick ∼                                      strategy tˆ exploited future problem instances        execute step rk ak                         limit cost learning allow exploitation model        update timer rk  rk  tk  tk  tkrk     start soon possible instead adopt online        problem solved                          lifelong learning approach use “bandit” al          observe reward xk  tk                     gorithm selection scheme described sect  upper                                                   level control allocation computational resources          observe reward xk                          case arms restart strategies each gen        end                                          erating sequence restart thresholds tkrk bound        update  bpsk xk                           algorithm “pull arm k” simply means “start      problem solved                              current problem threshold tkrk” problem    end                                             solved tkrktkrkandxk  ifitissolvedin                                                        time ts  tkrk tkrkts positive reward    note simple scheme falls partial informa observed  tion nonoblivious adversarial setting observe strategy gambler alg  starts solving ﬁrst  ward successful algorithms reward depends problem using universal strategy problem  previous choices example ak takes steps solve instance solved solution time successful run  problem reward ﬁrst −  pulls collected censored samples unsuccessful runs  adversary malevolent deceptive es used update estimate fˆ evaluate new tˆ  pecially tkr vary algorithms used problem model wewillusethe  bounds performance bps hold given def nonparametric productlimit estimator kaplan meier  inition reward expected performance alg   guarantees large sample convergence arbi  converge best solver set problems trary distribution account censored samples                                                        nonparametric form particularly appealing    mixing restart strategies                          does require apriori assumptions rtd                                                        fast update resulting cdf stepwise function  traditional statistical tests assess goodness model makes evaluation integral  inexpensive  measuring ﬁt corresponding probability density                                                          use exp bps need normalize  function pdf spectrum observed sam                                                        rewards setting lower upper bound                                             ˆ  ples case sole purpose model ts tmin ≤ ts ≤ tmax order limit impact  rtd set restart strategy order gain future choice convergence rate exp adopt log  performance quantity practical arithmic reward scheme solution problem during  loss performance induced mismatch model restart strategy rk  log tmax −log tklog tmax −                                  gagliolo schmidhuber studied impact  min                                                         log   total time spent strategy  increasingly censored sampling performance problem including unsuccesful runs way   estimated uniform restart strategy impact quite low maximum reward obtained strategy  fact  depends lower quantiles solved problem time tmin note use log  distribution rough model  obtained heavily arithm allows set tmin tmax respectively  censored sample lower portion time scale suf small large value knowledge  ﬁces evaluate nearoptimal strategy allowing reduce loose bound ts required  training cost                                          let analyze bound  combines    practical implementation technique requires optimal regret exp following ey expected  censoring strategy limit cost solving train                                                        value random variable tk represents solution time                                                       strategy single problem rk number restarts     consider situation solves problem step ∈  time  gets reward  solved performed pk   probability picking restart  problem  steps duration  each scoring reward  strategy indices ∗ tˆ label quantities related    censored sampling commonly used technique lifetime                                                             distribution estimation nelson  allows bound required hold set ts   reduce duration sequence experiments simply aborting maxtstmin exp able discriminate  runs exceeding time threshold information carried algorithms ts ≤ tmin work  runs used modeling parametric non restart algorithms exceeding tmax exp work                                                           ∗  parametric settings                                  tmax   cases xk ∈                                                      ijcai                                                     algorithm  gamblerm   restart strategy algorithm natural combination portfolio approach solution    set tmin tmax                                      adaptive simply consists restarting each algo    initialize expm pu                           rithm universal strategy literature meta    each problem                         learning algorithm selection giraudcarrier et al       set tk   rk                         rice  algorithm portfolios huberman et al       repeat                                            gomes selman  anytime algorithms boddy        pick ∼                                      dean  provides examples application        run cutoff tkrk                      performance modeling resource allocation works        update timer rk  rk  tk  tk mintstkrk ﬁeld adopt traditional ofﬂine approach        problem solved                          runtime samples collected during impractically                              log tmax−log tk          observe reward xk                           long initial training phase obtained model used                             log tmax−log tmin          tuning subsequently met problems dis                                                               cussed introduction implies stronger assump          observe reward xk                                                          tions representativeness training set does        end                                                        guarantee upper bounds execution time        update expk xk      problem solved                                max  karmed bandit problem variation    end                                             original formulation estimated reward                                                        updated arm obtaining maximum reward value                                                        set plays solvers game used cicirello  unknown optimal uniform universal estimated opti smith  streeter smith  maximize perfor  mal uniform strategies respectively identify mance quality single problem instance  novel strategy gambler interleaving execution allocation resources usually set each problem  given problem rtd al instance solution possibly updated afterward dynamic  gorithm restart strategy thresholds approaches feedback information algo  viewed sequence independent bernoulli processes rithms exploited adapt resource allocation during prob  success probabilities number restarts lem solution represent notable recent trend lagoudakis                                                                                       required solve problem distributed pdf littman  petrik   algorithm selection           r−   −                                      modeled reinforcement learning problem gen  pr      rf uniform strat  egy rt  prt  geometric ert  eral setting bandit problem proposed sect                                                         models performance conditional dynamic state af  deterministic strategy expected time solve                         er                           ter ﬁxed execution time used kautz et al   problem                  monotonic function                                     wu  implement dynamic contextsensitive restart  erforu  implies bound  trans policies sat solvers case main differ  lated bound ru  high probability ence approach online learning setting gagliolo  given simple reward scheme exp constant                                                                         schmidhuber present heuristic dynamic online    pu ptˆ during solution single problem consider learning approach resource allocation condi  worstcase setting tˆ tˆ tional model performance trained progressively ex  uniform restart tˆ solve problem ploited during training  spends ru restarts meantime rtˆ restarts                    ˆ                      −  wasted  ertˆeru  pu pu    experiments  exp  keeps pu ≥  γ expected perfor  mance gambler problem bounded experiments conducted using satzrand gomes et                                                        al aversionofsatzli anbulagan   etu  ru tˆ − γγ high probability upper                                                        random noise inﬂuences choice branching  bounds tu ru guarantee upper bound                                                        variable satz modiﬁed version complete dpll  tg                                                        procedure choice variable                                                        branch follows heuristic ordering based ﬁrst    originality related work                       second level unit propagation satzrand differs  restart strategies particularly effective rtd list formed variable branch randomly  controlled algorithm exhibits “heavy” tails  pareto picked fraction list present results  large small values time heuristic starting constrained vari  behavior observed backtracking search structured ables suggested li anbulagan  noise  underconstrained problems hogg williams  parameter set   gomes et al  problem domains benchmark satlib used consisting differ  networks van moorsel wolter  ent sets “morphed” graphcoloring problems gent et al  alternative solution run multiple copies  each graph composed set common edges  algorithm parallel algorithm portfolios huberman et al random graphs plus fractions −p   gomes selman  luby et al  maining edges each graph chosen form regular ring  luby presents parallel restart strategies represent lattices each  problem sets contains  instances                                                    ijcai                                                                                                                       generated logarithmic grid  different values                         −                                       parameter    henceforth refer      linst  labels    satzrand initialization cost  lset  depends size problem case    sets cost                                                                   bigger wethensettmin     tmax    minutes machine fair                                                                time  universal strategy multiplied shifted tu   tmin  tu   parameters exp                                                                determined α  γ     each experiment repeated  times different ran   dom seeds different random order instances                                                                                           comparison purposes repeated experiments running                    problem set  original algorithm restarts labeled sandthe  universal strategy compare ideal figure  experiments satzrand time solve each set              ∗                                                       formance  evaluated posteriori each run problems  ≈  min mixed strategy gambler  minimized cost problem set tlset performance initially universal restart strategy                                                                               ∗  cost each instance tlinstbasedontheactual run quickly converges near set lower bound perfor                                                        mance unknown optimal restart strategy set  time outcomes note different each run ∗  performances lower bounds performances inst instead lower bound performance distinct                ∗                                       optimal restart strategy each instance heterogeneous  optimal  evaluated unknown actual rtd                               respectively problem set each problem instance set obtained mixing sets    upper  conﬁdence bounds                                                        estimated  runs  difference bounds particularly interesting  indirect measure heterogeneity rtd  instances each set show effect  ×   practice “unlucky” runs  heterogeneous problem set run experiments long completion times pe   instances grouped single set labeled nalize performance dramatically gambler scores fairly  graph    α   γ  randomly   lower bounds   times  mixed ﬁgure  present each set total compu worst problem  heavy tail effect  tation time mixture comparison terms marked lower bounds diverge noticeably  upper  conﬁdence bounds given estimated  instances sets present heterogeneous rtds                                                                           ∗  runs plots rtd restart cost each set instanceoptimal inst vary  gagliolo schmidhuber           der magnitude  times worse                                                        gambler worst performance gambler compared    results quite impressive conﬁrm ∗  estimated restart strategy tˆ sensitive ﬁt set seen problem wheretg                                                         times tl∗ set  problems require small threshold  model fˆ typical run                                                                  require times larger sit  sample censored uncensored run              ∗                                                        uation threshold set varies visibly different  time each task ﬁt model visibly bad                                                                 ˆ  limited lower portion time scale near runs overestimates optimal threshold                                             ∗          results set problems better ex  tˆbuttˆ quickly converges close giving                             nearoptimal strategy                              pected tgtl∗set  performance com                                                        pared tl∗inst obviously worst  natural    problems  easy satzrand solves instances                ∗                                                        gamblerandt   set discriminate different  similar time larger optimal restart value                                    ×     restarts executed performance single restarts each problem completes set                                                             times worse gambler  copy ones optimal restarts   gambler quickly learns reach  value problem resulting ﬁve times worst perfor  conclusions future work  mance problems   effect heavytailed                                                        presented novel restart strategy gambler combin  rtd   solves each set times ×                                                     ing universal optimal strategies luby et al                                                         based bandit approach algorithm selection gam          ∗ min     ≥    min             tt        tt         tt     tl   bler takes best worlds preserves upper  cases                                                       bounds universal strategy worstcase setting     conducting experiments parallel portfolios proved effective realistic application  heterogeneous algorithms need common measure  time modiﬁed original code satzrand adding counter small overhead ideal lower bound save  incremented loop code resulting time orders magnitude computation time algorithms  measure consistent number backtracks results heavytailed rtd unlike universal strategy                                                reported loop cycles  ghz machine  cycles does worsen performance neg  minute                                ligible overhead compared controlled algorithm                                                    ijcai                                                     
