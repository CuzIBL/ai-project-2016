  generalization bounds weighted binary classiﬁcation applications                                        statistical veriﬁcation                                          vu ha      tariq samad                                              honeywell labs                                             technology dr                                          minneapolis mn                                      vuhatariqsamad  honeywellcom                                                                            abstract      approach statistically verify      ing complex controllers approach based      deriving practical vapnikchervonenkisstyle      vc generalization bounds binary classiﬁers      weighted loss important case deriv      ing bounds probability false positive      show existing methods derive bounds      classiﬁcation error extended derive      similar bounds probability false posi      tive bounds decisiontheoretic set      ting allows tradeoffs false negatives figure  organic air vehicle left complexity      false positives experiments  iterative equilibrium angleofattack computation determined                                                                                                   qs¯      ing bounds statistically verifying compu factors ﬂight path angle γ net lift force mg right      tational properties iterative controller computational complexity equated number      organic air vehicle oav                        iterations safe operating envelope consists inputs                                                        require iterations decision boundary empirically                                                        drawn using  random samples    introduction  computational requirements highperformance com  plex control algorithms vary considerably varia drastic consequences loss vehicle  tion arises computation depends number like obtain soe statisti  factors sensedestimated state cal guarantee safe classiﬁer prov  der control environmental disturbances operational ably small probability false positive  mode furthermore variation general criterion trivial classiﬁer clas  determined analytically hard realtime systems siﬁes negative current state  computation complete time command art candidate instead goal push  issued actuator sample instant boundaries soe far possible keeping  uncertainties pose signiﬁcant challenge rea cap probability false positive  son pid proportionalintegralderivative controllers example let consider computational property  deterministic execution time preferred highperformance controller oav figure   choice applications despite lesser performance oav ducted fan propulsion unit control provided    order bring practical acceptance highperformance movable vanes propwash vanes situated  complex control algorithms propose compromise propulsion airﬂow consequently interactions  makes use types controller algorithms high propulsion control surfaces highly  performance algorithms used safe opera nonlinear trim calculation oav iterative  tional envelope soe guaranteed complete algorithm computational time depends fac  allocated time outside soe lower perfor tors elgersma morton  interested  mance computationally simpler algorithms used ditions calculation reliably used  soe determined based simulation data approach based statistical learning theory slt  safe statistical guarantees        speciﬁcally derive statistical guarantees soes using    problem identifying soe binary classiﬁca vapnikchervonenkisstyle generalization bounds classi  tion problem false negative merely means conserv ﬁcation problems weighted loss classiﬁca  ative use low performance controller false positive tion problem false positive loss special case areinterested practical bounds–bounds asymptotically deﬁne               growth  competitive small preconstants slt lit function⊆ deﬁnedc ∩  ∩ ∈  erature contains vast collection vcstyle bounds ma                                                                  ∆k   max           jority bounds stated proved prob         ∩    ⊆                                                                                  ability misclassiﬁcation directly applica clearly ∆k    vc dimension deﬁned                                                                   ≤                        ble problem furthermore slt bounds                                                                                                       sup  ∆k      rived little emphasis obtaining optimal preconstants            ∈              addition emphasis ﬁnding small preconstants let  dh assume                                                                                        ∞  analysis unique aspects assume theorem  vapnik  equations   possible achieve small empirical loss true  case false positive loss second analysis                       en       uppertailoriented interested deriving lh δ lnh    ln     ln                                                                                                δ  bounds expected loss                                                                                                                      bound  obtained applying general    preliminaries                                      sult vapnik special setting learning binary clas                                                        siﬁers speciﬁc loss function lρ natural  notations use symbols denote ask bound improved slt results  probability expectation variance respectively σ  obtained address issue majority        σi denotes rademacher sequence–a sequence formulated proved  goals  dependent  symmetric valued random variables                     −                                  paper examine results extended    let nonempty sets                         ρ                                             ×          loss functions  ρ   turns  pair    denoted let µ sults fall categories∈ applicable  ﬁxed probability∈ measure× training set ﬁnite sample                                                  ρ     applicable ρ         zi      xi yi       drawn independently    ∈               ∈                            conclude preliminaries statement  according µ probability expectation respect oftenused sauer’s lemma  written pn en hypothesis space                                                                                                sauer’s lemma  sauer  ∆nh  end   set functions  loss function func                              ≤  tion       loss hypothesis  lh× →lhx expected loss of∈h  vc dimensionbased bounds  lh  elh empirical loss training set suppose hypothesis “small” empirical loss                                 lh          lh  assume lnh   ǫ  like bound probability                                                ≤    alls loss functions haves range   ﬁnite lh “large” lh  ǫ ǫ  ǫ amounts                                                                 classiﬁcation problem whenp   classiﬁcation bounding nq deﬁned                                binary let     deal binary              lnh ǫ lh  ǫ   classiﬁcation   −                                                  ∃        ≤              assume correct classiﬁcations incur zero loss major approaches classical ap        emerges loss function proach vapnik chervonenkis  approach  that− we− refer weighted classiﬁcation error deﬁned based abstract concentration inequalities developed ta                                                        lagrand                      y′      ρ      y′   ρ    y′   false negative   classical approach                          −                     y′     false positive                          −                            classical vc analysis begins observation                                                        pnq    prpr  satisﬁes pr   ρ number   idea false                                                         then≤ deﬁne based additional indepen  positives bad errors costly false negatives dent sample size ors equal  good errors ρ   difference                                  ′                                                              analysis approach let  zi  types errors loss called misclas independent sample size commonlys referred                                                                               ′          ′  siﬁcation error ρ   probability ghost sample let lnh  empiri                                                                                                   ′  classiﬁer making false positive error            cal loss ghost sample denote    let                                                                                              sn    given training set hypothesis loss   ǫ  ǫ  ǫ deﬁne  function wes bound expected∈ loss lh sta ≤    ≤                                                                      lnh ǫ l′  ǫ    tistical learning theory slt provides probabilistic answer        ∃       ≤            question typical slt result form pnlh                                                         upper bounding pnq   reduces upper bounding  ǫ  δ succinctly lh δ ǫ provides upper bound ǫ                                                        nr covering step lower bounding nr  lh conﬁdence  δ ǫ δ pos                                           itive reasonably small numbers the− upper bound ǫ typically symmetrization step                                                          covering step upper bounding pnr intuitively  depends δ sample size empirical loss lnh  complexity measure hypothesis space nr small small empirical loss  important complexity measure slt vc dimension let sample empirical loss ghost sample                                                        small change deﬁnition        set subsets note set  binaryc ⊆ classiﬁers example sets    l′ lnh  η η  ǫ ǫ                                                                   ∃      −                     −                                                                                                      step uses socalled permutation technique lemma  blumer et al      pnr  en   ln′ lnh  η                                                                   ∃       −                                                          en                                                          lnh   ln′  ǫ     exp  nǫ ln                                                                                  ≤           −          eneσ      σilh zin lh zi  nη                                                   ∃                 −                                                                                                symmetrization step lower bounding nr                                                                                                                                                         lower bound pnr ﬁx ignore condition  ﬁx bound inner expectation let                                                                                     lnh  ǫ bound following conditional probability                                                   hn   hn  hx     hxn             ≤                                 ∈   ⊆ −                         ′                                                                           n′   ln′  ǫ  lh  ǫ   mapping    hn manytoone denote lihn                    ∃            ∃                                                          equivalently  lh zi  →n inner expectation written              lnh  ǫ  lh  ǫ            ≤   ≤                                                          ∃            ∃                                                      let hypothesis lh  ǫ sufﬁces lower     eσ   hn  hn     σi linhn lihn  nη     bound              intuitively quantity large         ∃    ∈                    −                           nlnh  ǫ                                                                                                  cause empirical loss large  ǫ  union bound bounded              expected loss large  ǫ ǫ  ǫ                                                                         pnlnh  ǫ  plh  lnh  ǫ ǫ                                                                 ≤    ≤       −         −            eσ     σi linhn lihn  nη                                                                  −                                                nǫ  ǫ     hn hn                                                                                              ∈                                                               exp        −        ιn ǫ ǫ                                                                  ≤      − ǫ ǫ                                                                                            cardinality hn ∆nh                     −                                                      bernstein’s lefttail inequality coupled  end sauer’s lemma ﬁx hn hn sum  mand  written     ∈                 lemma  obtain following result                                                      corollary                       σ     σi linhn lihn  nη                                         −                             lnh ǫ lh  ǫ                                                             ≤                                                                                                                                                                                  en          nǫ  ǫ  hoeffding’s¨ righttail inequality bounded    ιn ǫ ǫ−      exp  −     −                                                                ≤   ∨   −                        ǫ  ǫ                                                                                                                     η                   nη                                      exp          −                 exp                     linhn lihn  ≤     −               inequality parameter ǫ unspeciﬁed                    −                                                                            minimize bound ǫ ǫ ǫ  havep arrived following result                                   ∈   ρ                                                           ρ    loss function binary nlnh  lemma  vapnik chervonenkis             binomial random variable parameters lh                                                      use lower bounds righttails                             en                    pn lnh  ǫ l′  ǫ       exp  nǫ  ǫ    binomial obtain           ≤           ≤           −      −                                                                                         nǫ    pn  lnh  ǫ  lh  ǫ       ǫ   ǫ possible improve lemma        ⇒              ∃  idea is≪ instead bounding probability ab nǫ   pn  lnh  ǫ  lh  ǫ                     ′                                               ⇒              ∃  solute discrepancy lnh lnh large bound prob                                 ′                      case      set      combine                       −         lnh−lnh                        ǫ             ǫ   ǫ  ability relative discrepancy ′  large lemma  obtain following result                               √lnhlnh  “weaken” deﬁnition                corollary  vapnik chervonenkis                                                           ρ            l′ lnh  η′                  ρ                 ∃     −                   ln′  lnh   ln′  lnh                                en           η′  ǫ  ǫ               η                              lh δ lnh   ln    ln                                                                                                δ             −      ǫ  ǫ        ǫ  ǫ                                                  proceed identically η             ′                                            case ǫ   set ǫ  ǫ combine   placed η  arrive following results   lemma  obtain following result  lemma  vapnik chervonenkis                                                         corollary  blumer et al   lρ ρ                                                                               en         nǫ  ǫ  pn lnh  ǫ ln′  ǫ      exp  −    −                                        en                ≤             ≤             ǫ  ǫ         lnh    lh δ      ln    ln                                                                        ⇒        ln           δ  corollary   vapnik and chervonenkis                                                                                                         shawetaylor et al  improve  using                                                                                     ′                                en                     argument uses second sample size ǫ  rǫ     pn lnh   l′  ǫ      exp  nǫ                                                              ≤          −                                                                                                      ǫk  ernd     considering  relative discrepancy man        −                      −                                                        theorem  shawetaylorp et al   lρ ρ   aged insert term ǫ  ǫ resulting tighter bound    lemma  tightening possible ǫ    instead using hoeffding’s inequality blumer et al  nǫ  pn lnh   lh  ǫ                                                                  ⇒  use combinatorial argument leads following im                                  en                                                                     exp √d   ǫn  ln ǫ  ln    provement corollary                                      ≤            −                                                                                                                  compared  sample complexity derived  simple algebra shows lh                                                                                             ∈          ≤   smaller factor  √ǫ    l˜  l˜  consequently upper bound l˜                                 ln  −     ≈                                                  typical values ǫ say                         translated upper bound lh unlike previous    point corollary   theorem  analysis appears use mcdiarmid’s bounded  previously stated proved loss function  difference inequality introduction term lh  analysis extends case  renders “difference” unbounded solution  shows hold   ρ   cov problem originates work talagrand abstractp  ering argument remains symmetrization concentration inequalities applications bounding                                       ρ  argument uses simple observation nln bino suprema empirical processes talagrand    mial random variable parameters lh regardless talagrand’s inequalities later improved using  ρ   ρ                              called entropy method following version provides                                                        best known bound    talagrand’s method                               lemma   bousquet  let countable set                                                                                              observe lh  lnh  suph∈h lh lnh   functions  let  supf∈f sup                  ≤                     −                                                         e−  supremum random function changing sin  supf∈f bn    supf∈f                                                                                                    −  gle element zi results change                                                           fzi α                                                      δbounded en sup ∈ lh lnh      ln                             −              δ                         lnδ        lnδ  mcdiarmid’s inequality mcdiarmid  bn δ   αenbn                                                                                                                      α       quantity bound expectation supremum                                    accomplished using concept rademacher aver   apply lemma  fz  lh lh  age let  class functions reals    obtain   rademacher average deﬁned rng                                                                             rg  σ  sup ∈          σigzi  analy                          lnδ        lnδ                                               l˜n δ   αenl˜n                          sis thes role played loss class associated                             α                                                                                                              lph    sym                                                                                ˜  metrization inequality eg→ bartlett et al∈  states proceed bound nln technique                                                        massart  referred peeling concept  en sup ∈ lh  lnh   ernlh   remains             −        ≤                             subroot functions function ψ       bound rademacher average technique                                   ∞   →     ∞  established based concepts covering number called subroot ψ nondecreasing ψr√r non                                                      increasing subroot function ψr known  denote nu lh  lµ  ucovering number lh              ∗        ∗     ∗                                                      unique ﬁxpoint ψr    bartlett et al   respect metric lµ                                                         suppose ψ subroot function ﬁxed point  lemma  dudley                              r∗                                                                                   eσrnlh            ln nu lh  lµ du        sup lh  lnh  lh   ψr                ≤  √n                                                  −           ≤    ≤      ∀                                                                         show α   l˜n δbounded     piece puzzle reveals bound                        ∀  nu  µn deﬁned based vc dimension                                                                                     ln    α ln                                                         α  √r        ln         δ          δ   lemma  haussler                          ∗                          αn                                                                             ∗                                                                                ∗                                                   ﬁnal step bound using            nu lh  lµ  ed                                     ≤                          koltchinskii panchenko  setting ψr                                                                                              dudley’s entropy integral end     combining   algebra obtain                                               δ                                                        bound lh od ln nn asymptotically compa  following result                                                        rable obtained using classical approach  theorem  dudleyhaussler                       theorem  albeit worse constants                               lnδ                  summary vc dimensionbased bounds        lh δ lnh                                                                        given sample  size hypothesis                                                                      bound  based analysis absolute empirical loss lnh say ex                                                        pected loss lh hs results section provide  discrepancy lh lnh deviation term dn  not− come surprise natural ask answers question general  approach used analyze form ofp relative form “if lnh small high probability lh  discrepancy expected loss lh empirical small” common assumption selected  loss lnh consider                                  hypothesis space ﬁnite vc dimension gen                                                        eral case assumed range   corol                                                                                            ρ                lh  lnh                             lary  useful   ρ         l˜n  sup     −        lh                          lh     ∈                        exploit binomial tail inequalities obtain                   psimpler bounds lnh   corollary  hyperrectangle contains sampled point  used lnh   theorem  provides bestknown algorithm does converge unsafe point elim  bound results based idea uniform inate hyperrectangle guarantees based  convergence relative discrepancies ucrd corol theorem  require zero false positives  lary  theorem  viewed based degener count number safe points lie outside hyper  ate cases ucrd specialpurpose combinatorial argu rectangle false negatives choose hyperrectangle  ments replacing generalpurpose hoeffding’s bound fewest number false negatives look  sults based uniform convergence absolute dis ing  random hyperrectangles able come  crepancies lemmas  theorem  contains  safe points unsafe  useful purpose cover situations points hypothesis       vapnik chervonenkis  refer pessimistic cases false negatives note number false negatives  simply pessimistic bounds loose need quite large use hyperrectangles  account hypotheses expected losses close  constitute simple hypothesis space does approxi  contrast need concern hypotheses mate decision surface soe  zero small empirical losses                  big improvement trivial soe    statisticalcomputational learning theory literature current state art advantage vc  contains vast collection generalization bounds gen dimension low small number samples  eral case function learning special case learn required obtain statistical guarantee reads  ing binary classiﬁers knowledge work explic “the hyperrectangle probability false positive  itly derived generalization bounds binary classiﬁers bounded   conﬁdence  weighted error penalties deﬁned paper bounds statement”  section  originate seminal work vapnik number alternatives proce  chervonenkis  contribution extensions dure example use corollary  instead  case ρ   corollary                  orem  requirement zero empirical loss    talagrand’s approach provides completely different way restrictive oav example  samples  arrive generalization bounds loss function leads hypothesis  false positives   lρ ρ   asymptotically equivalent clas false negatives reduction  maintaining  sical bounds∈ approach analyzes mean  conﬁdence probability false positive  dian panchenko  supremum  furthermore replace criterion “as  weighted discrepancies expected empiri false negatives possible” criteria example  cal losses using talagrand’s various concentration inequali prefers hypotheses large volumes finally  ties completed invariably symmetrization inequal willing make decisiontheoretic tradeoff  ity dudley’s entropy integral bound haussler’s packing false negatives false positives false positive  bound resulting bounds larger costly thousand false negatives set ρ    stants useful nonasymptotic purpose apply corollary       experiments                                           summary related work  applications bounds derived said divide slt practice  section  oav experiment described introduc grand canyon proportions vc bounds  tion identify factors affect computational loose useful practice paper  time iterative algorithm choose dimensional fers counterargument form sltbased approach  axisparallel hyperrectangles hypothesis space verifying complex controllers demonstrated ap  vc dimension hypothesis space  known proach problem signiﬁcant industrial military  vc dimension axisparallel hyperrectangles rm deriving safe operating envelope complex  δ   ǫ   need  samples control algorithm approach offers control engineers  ing theorem  using theorem  need  principled way increasingly replace lowperformance  samples  samples set δ   ǫ simple control algorithms highperformance complex  small  improvement generalization bound ones maintaining statistically high conﬁdence  ǫ fold samples complexity safety key making offer attractive lies deriv  fold                                              ing practical vcstyle generalization bounds weighted bi    search best hyperrectangle experiment nary classiﬁcation problem hitherto given  simple each sampled input determine attention vc analysis builds stan  iterative algorithm converges turns  dard vc analysis unweighted binary classiﬁcation shows  instances roughly  algorithm converges despite bounds possible signiﬁcantly  high empirical rate success current practice better general bound vapnik analysis precisely  loses pidlike controller ﬁxed deterministic pointed place false negative penalty  computation time randomly choose axisparallel effect symmetrization argument suc  dim hyperrectangle input ranges hypothesis cessfully applied veriﬁcation framework
