               hierarchical hidden markov models information extraction                marios skounakist                              mark craven                                soumya ray              marioscswiscedu                        cravenbiostatwiscedu                           sraycswiscedu                            department sciences                    department biostatistics  medical informatics                               university wisconsin                                    university wisconsin                             madison wisconsin                                     madison wisconsin                                 abstract                                                                              report identification integral membrane        information extraction defined task                     ubiquitinconjugating enzyme enzyme ubc local•       automatically extracting instances specified                    izes endoplasmic reticulum catalytic domain        classes relations text consider case                  facing cytosol        using machine learning methods induce mod•       els extracting relation instances biomedi•       cal articles propose evaluate approach        based using hierarchical hidden markov                        subcellularlocalizationubcendoplasmic reticulum        models represent grammatical structure        sentences processed approach                  figure  example information extraction task        uses shallow parser construct multilevel rep•              figure shows document wish extract        resentation each sentence processed                 instances subcellularlocalization relation        train hierarchical hmms capture regu•                    figure shows extracted tuple        larities parses positive negative        sentences evaluate method inducing        models extract binary relations three biomedi•               role contrast task consider extracting infor•       cal domains experiments indicate ap•                 mation abstracts biological articles hirschman et        proach results accurate models                 domain important learned models        baseline hmm approaches                                           able represent regularities grammatical structure                                                                           sentences                                                                              paper present approach based using hier•   introduction                                                          archical hidden markov models hhmms fine etal    application domains potential greatly          extract information scientific literature hierar•  increase utility online text sources using automated         chical hidden markov models multiple levels states   methods mapping selected parts unstructured text             input sequences different levels granu•  structured representation example curators          larity models level hmms represent   genome databases like tools accu•              sentences level phrases lower level   rately extract information scientific literature         hmms represent sentences level individual words   entities genes proteins cells diseases        approach involves computing shallow parse each   reason recent developing               sentence processed during training testing   methods task information extraction              hierarchical hmms manipulate twolevel description   defined automatically recognizing extracting in•          sentence parse instead just processing sentence words   stances specific classes entities relationships         directly evaluate approach extracting instances   entities text sources                                             three binary relations abstracts scientific articles     machine learning methods play key role sys•             experiments show approach results accurate   tems difficult costly manually encode              models baseline approaches using hmms   necessary extraction models hidden markov models                      example binary relation consider   hmms leek  bikel et al  freitag mccal               experiments subcellularlocalization relation   lum  related probabilistic sequence models mc              represents location particular protein cell     callum et  lafferty et aly              refer domains relation protein   accurate methods learning information extrac•              location refer instance relation tuple   tors work learning hmms information ex•             figure  provides illustration extraction task   traction focused tasks semistructured            figure shows sentences abstract   text sources english grammar does play key               figure shows instance target relation       information extraction                                                                                                                   subcellularlocalization like extract    second sentence tuple asserts protein ubc    subcellular compartment called endoplas•   mic reticulum order learn models perform task    use training examples consisting passages text an•   notated tuples extracted       earlier work ray craven  presented    approach incorporates grammatical information    singlelevel hmms approach described paper ex•   tends earlier work using hierarchical hmms provide    richer description information available sen•   tence parse       hierarchical hmms originally developed fine et    al  application models information    extraction novel approach incorporates ex•   tensions models tailor task bikel el    al  developed approach named enlily recognition    uses hmms multilevel representation similar    hierarchical hmm models level represents           figure  input representation sentence contains    classes person              subcellularlocalization tuple sentence segmented    level represents words sentence processed           typed phrases each phrase segmented words typed    approach differs theirs key respects in•       partofspeech tags phrase types labels arc shown column    representation sentences processed hierar•         word partofspeech tags labels arc shown column    chical ii models represent shallow phrase structure          words sentence shown column note group•   sentences iii focus learning extract relations           ing words phrases labels protein location                                                                            present training sentences    entities iv use null models represent sen•   tences relations    use discriminative training procedure miller el al            subcellularlocalization relation annotated segments    developed informationextraction approach uses lex           sentence segmented typed phrases each phrase    icalized probabilistic contextfree grammar lpcfg si•             segmented words typed partofspeech tags    multaneously syntactic parsing semantic information              example second phrase segment noun phrase    extraction genre text consider           npsegment contains protein ubc    quite different news story corpus avail•           protein label note types constants    able lpcfgs trained clear            predefined representation sundance parses   intriguing approach transfer task                     labels defined domains particu•                                                                           lar relation trying extract    sentence representation                                                                             hierarchical hmms information   previous work hmms natural language tasks   passages text processed represented                    extraction   sequences tokens hypothesis underlying work              schematic hierarchical hmms shown   incorporating sentence structure learned models            figure  figure shows positive model   provide better extraction accuracy approach based           trained represent sentences contain instances   using syntactic parses sentences processed              target relation figure shows null   particular use sundance riloff               model trained represent sentences   obtain shallow parse each given sentence                           contain relation instances offtopic sentences      representation use paper does incorpo•             coarse level hierarchical hmms represent sentences   rate information provided sundance parser             sequences phrases think level   instead representation provides partially flattened             hmm states emit phrases refer hmm   twolevel description each sundance parse tree               phrase hmm states phrase states fine   level represents each sentence sequence phrase seg•              level each phrase represented sequence words   ments lower level represents individual tokens               achieved embedding hmm each phrase state   partofspeech pos tags positive training ex•           refer embedded hmms word hmms   amples segment contains word words belong             states word states phrase states figure  de•  domain target tuple segment words in•             picted rounded rectangles word states depicted   terest annotated corresponding domain iefer             ovals explain sentence hmm fol•  annotations labels test instances contain            low transition start state phrase state qi   labels  labels predicted learned model         use word hmm qi emit phrase sen•     figure  shows sentence containing instance               tence transition phrase state qj emit                                                                                                              information extraction                                                                          figure  architectures word hmms subcellular                                                                           localization relation bold text states denotes domain labels                                                                            states implicit labels italicized text paren•                                                                           theses denotes position states emissions relative    figure  schematic architecture hierarchical hmm                                                                            domain words figure shows structure embedded    subcellularlocalization relation figure                                                                            hmms phrase states labels phrase states    shows positive model null model phrase                                                                            label phrase states labels    states depicted rounded rectangles word states ovals    types labels phrase states shown rectangles    right each state labels shown bold states  hmm shown figure explain phrase endo•   associated nonempty label sets depicted bold borders                                                                            plasmic reticulum  following transition start    labels word states abbreviated compactness                                                                            state state emitting word transition•                                                                           ing location state emitting words endoplas     phrase using word hmm moves                mic reticulum location label    end state phrase hmm note word             transitioning end state order phrase state    states direct emissions                                           emit phrase given input representation      like phrases input representation each phrase             sequences words shorter longer    state hmm type labels            phrase require embedded word hmm transi•   each phrase state constrained emit phrases             tion end state exactly emitted words    type agrees states type refer states         given phrase word hmms emit se•   labels associated extraction states            quences words constitute phrases transi•   used predict test sentences tuples             tions phrase states occur phrase boundaries   extracted                                                        standard dynamic programming algorithms      architectures word hmms shown fig•                  used learning inference hmms  forward back•  ure  use three different architectures depending             ward viterbi rabiner   need slightly mod•   labels associated phrase state word               ified hierarchical hmms particular need   hmm embedded word hmms phrase states                     handle multiplelevels input representation en•  label sets figure consist single emit•            forcing constraint word hmms emit sequences   ting state selftransition extraction states          words constitute phrases ii support use   phrase hmm word hmms specialized archi•                  typed phrase states enforcing agreement state   tecture different states domain instances          phrase types   words come domain                    forward algorithm hierarchical hmms de•  instances figures states word            fined recurrence relationships shown table    hmms emit words type partofspeech               three equations recurrence relation provide   untyped contrast typed phrase states            phraselevel description algorithm three   word states annotated label sets trained            equations provide wordlevel description notice   emit words identical label sets example word              equation describes linkage phrase level       information extraction                                                                                                                    tabic  left side table shows forwardalgorithm recurrence relation hierarchical hmms right side table    defines notation used recurrence relation      word level backward viterbi algorithms re•   null models shown figure  submodels    quire similar modifications show   shared start end states    space limitations                                              model trained use viterbi al•     illustrated figure  each training instance  gorithm predict tuples test sentences extract tuple    hmms consists sequence words segmented          given sentence viterbi path goes through states    phrases associated sequence labels test in• labels domains relation example   stance like trained model accurately pre•     subcellularlocalization relation viterbi path   dict sequence labels given observable sentence pass through state protein la•  sentence words phrases use discrimina•    bel state location label process   tive training algorithm krogh  tries model illustrated figure    parameters  maximize conditional likelihood   labels given observable sentences              hierarchical hmms context features                                                                 section extension hierarchical                                                                  hmms presented previous section enables                                                                  represent additional information structure sen•  sl sequence wordsphrases zth instance tences phrases refer extended hmms   sequence labels instance training context hierarchical hmms chhmms   algorithm converge local maximum objective    hierarchical hmms presented earlier partition sentence   function initialize parameters models  disjoint observations each word   doing standard generative training apply kroghs      chhmm represents sequence overlapping obser•  algorithm involves iterative updates hmm pa•      vations  each observation consists window   rameters avoid overfitting stop training ac•  three words centered  curacy heldaside tuning set maximized                ofspeech tags words formally vector      order algorithm able adjust param•                                              eters positive model response negative instances  partofspeech tag word ij note   viceversa join positive null models shown  share features   figure  combined model includes positive     located different positions vectors figure                                                                   shows vectors emitted phrase endoplasmic                                                                  reticulum word hmm chhmm                                                                    using features represent previous words                                                                  allows models capture regularities pairs triplets                                                                  words instance chhmm potentially able                                                                  learn word membrane  subcellular loca•                                                                 tion plasma membrane                                                                  membrane furthermore using features                                                                  represent partofspeech words models able   figure  architecture combined model positive null learn regularities groups words   models refer models figure                         speech addition regularities individual words                                                                                                information extraction                                 extracted tuples subcellularlocalizationmas mitochondria                                                             subcellularlocalizationmas mitochondria     figure  example procedure extracting tuples subcellularlocalization relation sentence fragment mas   mas mitochondria figure shows likely path explains sentence fragment bold   transitions states denote likely path dashed lines connect each state words emits table shows label   sets assigned phrases words sentence extracted tuples arc shown figure                                                                                ekoijkiqab probability word state qab                                                                            emitting observation ath feature oij                                                                              note features employed representation                                                                            equation  clearly conditionally independent con•                                                                           secutive words independent cer•                                                                           tainly partofspeech tag word independent                                                                            word argue discriminative   figure  generation phrase endoplasmic reticulum        training algorithm use krogh  compensate   word hmm chhmm bold arcs represent path             violation independence assumption     generates phrase vector observations oij emitted each   state shown rectangles model connected       empirical evaluation   dotted arcs emitting state word   emitted each state equivalent hhmm shown boldface       section present experiments testing hypothesis                                                                            hierarchical hmms able provide accurate                                                                            models hmms incorporate grammatical infor•                                                                           mation particular empirically compare types   advantages representation especially realized            hierarchical hmms three baseline hmms   dealing outofvocabulary word case                   • context hhmms hierarchical hmms context   partofspeech tags neighboring words quite in•                     features described previous section   formative meaning use outofvocabulary                                                                               • hhmms hierarchical hmms context features   word example outofvocabulary adjective rarely   protein proteins usually nouns                             • phrase hmms singlelevel hmms states                                                                                 typed phrase level hhmm emit      number possible observations given                    phrases hmms introduced ray   word state chhmm large possible vectors                     craven  unlike hierarchical hmms   representing sequences three words pos tags                 states phrase hmms embedded hmms   model probability observation chhmms                            emit words instead each state single multi•  assume features conditionally independent given                  nomial distribution represent emissions each   state assumption probability obser               emitted phrase treated bag words       information extraction                                                                                                                   
