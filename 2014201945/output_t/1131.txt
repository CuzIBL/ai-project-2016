              planning continuous resources stochastic domains                mausam                     emmanuel benazera∗                   eric hansen       dept science   ronen brafman† nicolas meuleau†       dept science            engineering            nasa ames research center                engineering        university washington              mail stop              mississippi state university         seattle wa          moffet field ca         mississippi state ms         mausamcswashingtonedu         ebenazer brafman nmeuleau       hansencsemsstateedu                                             emailarcnasagov                        abstract                          possible pedersen et al  result expected                                                        space scientists request large number potential      consider problem optimal planning       tasks future rovers perform feasible      stochastic domains resource constraints  presenting oversubscribed planning problem      resources continuous choice                                                          working application domain goal provide      action each step depend current                                                        planning algorithm generate reliable contingent plans      resource level principal contribution                                                        respond different events action outcomes      hao algorithm generalization ao algo                                                        plans optimize expected value experiments      rithm performs search hybrid state space                                                        conducted rover aware time energy      modeled using discrete continu                                                        memory constraints particular pay atten      ous state variables search algorithm leverages                                                        tion fact given initial state multiple      knowledge starting state focus computa                                                        locations rover reach experiments      tional effort relevant parts state space                                                        rover conduct combinations infea      claim approach especially effective                                                        sible resource constraints address problem      resource limitations contribute reachabil                                                        need faithful model rover’s domain al      ity constraints experimental results show ef                                                        gorithm generate optimal nearoptimal plans      fectiveness domain motivates                                                        domains general features problem include       search – automated planning planetary explo                                                        concrete starting state  continuous resources including      ration rovers                                                        time stochastic consumption  uncertain action ef                                                        fects  possible onetimerewards subset    introduction                                       achievable single run type problem                                                        general includes large class stochastic  control planetary exploration rovers presents im logistics problems  portant challenges research automated planning  cause difﬁculties inherent communicating devices past work dealt features problem  planets remote rovers operate autonomously related work mdps resource constraints includes  substantial periods time bresina et al  model constrained mdps developed community  planetary surfaces operate uncertain altman  constrained mdp solved linear  environments great deal uncertainty program includes constraints resource consumption  duration energy consumption outcome rover’s ac ﬁnds best feasible policy given initial state  tions currently instructions sent planetary rovers source allocation drawback constrained mdp model  form simple plan attaining single goal does include resources state space  photographing interesting rock rover attempts policy conditioned resource availability  carry remains idle fails does model stochastic resource consumption  early makes attempt recover possibly achieve area decisiontheoretic planning techniques  alternative goal impact mis proposed handle uncertain continuous variables  sions example estimated  mars feng et al  younes simmons  guestrin  pathﬁnder rover spent   time et al  smith  van den briel et al   ing plans did execute expected sider problem oversubscription planning plan  current mer rovers aka spirit opportunity require ning large set goals entirely achievable  average  days visit single rock future missions provide techniques selecting subset goals  multiple rock visits single communication cycle plan deal deterministic domains                                                        finally meuleau et al  present preliminary experiments    ∗ research institute advanced science scaling decisiontheoretic approaches planetary    † qss group                                    rover problems  contribution paper implemented algorithm notational convenience model discrete component  hybrid ao hao handles problems single variable  gether oversubscription planning uncertainty limited markov state ∈ pair ∈  continuous resources essential features discrete variable  xi vector continuous vari  algorithm ability handle hybrid statespaces ables domain each xi interval xi real line  utilize fact states unreachable  xi hypercube continuous  resource constraints                                 variables deﬁned assume explicit initial state    approach resources included state descrip noted absorbing terminal states  tion allows decisions based resource terminal state corresponds situation  availability allows stochastic resource consumption goals achieved model situations  model opposed constrained mdps resources exhausted action resulted  creases size state space assume value error condition requires executing safe sequence  functions represented compactly use work rover terminating plan execution  feng et al  piecewise constant linear ap actions executability constraints example  proximations dynamic programming dp imple action executed state does  mentation standard dp does exploit fact minimum resource requirements anx denotes set  reachable state space smaller com actions executable state  plete state space especially presence resource state transition probabilities given function  straints contribution show use prs   denotes state action  ward heuristic search algorithm called ao pearl   denotes state action called  hansen zilberstein  solve mdps resource arrival state following feng et al  probabili  constraints continuous resource variables unlike dp ties decomposed  forward search keeps track trajectory start                                                                                      state each reachable state check • discrete marginals prn                                                                       trajectory feasible violates resource constraint al n∈n prn    lows heuristic search prune infeasible trajectories                                                                                           • continuousr conditionals prx   dramatically reduce number states consid prxn ndx    ered ﬁnd optimal policy particularly important         x∈x  domain discrete state space huge expo transition results negative value contin  nential number goals portion reachable uous variable viewed transition terminal state  initial state relatively small resource reward transition function arrival  constraints wellknown heuristic search state complex dependencies possible  efﬁcient dp leverages search heuristic sufﬁcient goalbased domain models let  reachability constraints focus computation relevant rnx ≥  denote reward associated transition  parts state space show problems state  source constraints advantage greater application domain continuous variables model  usual resource constraints limit reachability nonreplenishable resources translates general    paper structured follows section  assumption value continuous variables non  basic action goal model section  explain increasing assume each action  planning algorithm hao initial experimental results minimum positive consumption resource  described section  conclude section  utilize assumption directly                                                        implications correctness approach    problem deﬁnition solution approach            pends  values continuous variables apriori                                                        bounded  number possible steps execu    problem formulation                              tion plan bounded refer saying  consider markov decision process mdp problem bounded horizon note actual num  continuous discrete state variables called hy ber steps termination vary depending actual  brid mdp  guestrin et al  generalized state resource consumption  mdp  younes simmons  each state corresponds given initial state objective ﬁnd  assignment set state variables variables policy maximizes expected cumulative reward  discrete continuous continuous variables typically application equal sum rewards  represent resources possible type resource goals achieved running resource note  time discrete variables model aspects state direct incentive save resources optimal solu  cluding application set goals achieved far tion save resources allows achieving  rover keeping track alreadyachieved goals ensures goals stay standard decisiontheoretic  markovian reward structure reward achievement framework problem solved solving bellman’s op  goal achieved past  models typically contain multiple discrete variables algorithm easily extended deal uncertain  plays role description algorithm starting state long probability distribution knowntimality equation takes following form    time remaining address problem ﬁnd                                                        optimal solution associate value estimate each                                                           markov states aggregate attach                                                      each search node value function function continu                               vn     max        prn              ous variables instead simple scalar value used stan              a∈a                                                  n∈n                             dard ao following approach feng et al                                            ¸          value function represented computed efﬁciently                      ¡            ¢          prx   rn   vn  dx     continuous nature states simplify                                                     ing assumptions transition functions using  note index represents iteration timestep value estimates associate different actions  dp does necessarily correspond time plan different markov states aggregate state correspond  ning problem duration actions biggest ing search node  sources uncertainty rover problems typically order select node fringe search  model time continuous resources xi     graph expand need associate scalar value                                                        each search node maintain search node    solution approach                                heuristic estimate value function used  feng et al dynamic programming dp algorithm make action selections heuristic estimate prior  solves bellman optimality equation particular ity used decide search node expand  show continuous integral com details given following section  puted exactly long transition function satisﬁes cer note lao generalization ao allows  tain conditions algorithm involved policies contain “loops” order specify behavior  treat blackbox algorithm fact inﬁnite horizon hansen zilberstein   replaced method carrying compu use similar ideas extend lao setting  tation simpliﬁes description algorithm need consider loops reasons  prob  section allows focus contribu lems bounded horizon  optimal policy  tion explain ideas assumptions contain intentional loop returning  algorithm feng et al section               discrete state fewer resources buy    difﬁculty address paper potentially current implementation assumes loop intentional  huge size state space makes dp infeasible discards actions create loop  reason size existence continuous vari  ables consider discrete compo  hybrid ao  nent state space size state space expo                                                        simple way understanding hao ao variant  nential number propositional variables comprising                                                        states identical discrete component expanded  discrete component address issue use                                                        unison hao works graphs  ward heuristic search form novel variant  ao algorithm recall ao algorithm search • explicit graph describes states  ing andor graphs pearl  hansen zilberstein   generated far andor edges connect   graphs arise problems choices nodes explicit graph stored  components each choice multiple lists open closed  sequences component case planning • greedy policy partial solution graph denoted  uncertainty ao effective solving greedy algorithms subgraph ex  planning problems large state space rea plicit graph describing current optimal policy  son ao considers states reach  able initial state reason given standard ao single action associated each  informative heuristic function ao focuses states node greedy graph described mul  reachable course executing good plan result tiple actions associated each node dif  ao ﬁnds optimal plan exploring small fraction ferent actions optimal different markov states rep  entire state space                            resented aggregate state    challenge face applying ao problem  data structures  challenge performing statespace search contin  uous state space solution search aggregate main data structure represents search node  state space represented search graph tains  node each distinct value discrete compo • value discrete state application  nent state words each node search discrete state variables set goals achieved  graph represents region continuous state space  discrete value approach dif • pointers parents children explicit  ferent actions optimal different markov states greedy policy graphs  aggregate state associated search node especially • pn· – probability distribution continuous vari  best action likely depend energy ables node each ∈ pnx estimate    probability density passing through state  create root node represents initial state        current greedy policy obtained pro  pn  initial distribution resources        gressing initial state forward through optimal ac  vn          tions greedy policy each pn maintain  gn        probability passing through greedy  open  greedy       policy                                           closed   ∅                                                          open ∩ greedy    ∅                  mpn        pnxdx                                                                                                                        x∈x                              arg maxn ∈open∩greedygn                                                              open closed    • hn· – heuristic function each ∈ hnx  ∈ × expanded      heuristic estimate optimal expected reward                                                              reachable pn      state                                     ∈ open ∪ closed                                                                                                        • vn· – value function leaf nodes ex  create data structure represent add                                                                                          plicit graph vn  hn nonleaf nodes         transition  explicit graph      explicit graph vn obtained backing func  hn       tions descendant leaves heuristic func  vn  hn                                                                          tion hn admissible leaf nodes  vnx  terminal      upper bound optimal reward come       add closed      reachable greedy policy                                                                      add open    • gn – heuristic estimate increase value             greedy policy expanding node   ancestor explicit graph                                                                     hn admissible gn represents upper bound                                                               add transition explicit graph      gain expected reward gain gn used                                                           pair expanded previous step       determine priority nodes open list gn        closed bound error greedy      solution each iteration algorithm           update vn expanded node                                                                ancestors explicit graph algorithm     note information redundant                                                           update pn gn using algorithm  nodes  theless convenient maintain algo children expanded node node  rithm easily access hao uses customary open    optimal decision changed previous  closed lists maintained ao encode ex     step  node ∈ closed  plicit graph current greedy policy closed contains changed open  expanded nodes open contains unexpanded nodes  nodes need reexpanded                                  algorithm  hybrid ao    hao algorithm                                                        arrival states result transi  algorithm  presents main procedure crucial steps tion prn    previously expanded  described                        open actions arrival  expanding node lines   each iteration hao nodes expanded considered line  check  expands open node highest priority gn node generated nec  greedy graph important distinction ao essary graph tree way  hao nodes partially each discrete state line  node terminal  expanded markov states associated dis action executable lack resources  crete node considered nodes closed application domain each goal pays  list open line  reason nodes goals problem achieved  markov state associated node terminal finally test line  prevents loops  previously considered unreachable reach explicit graph discussed earlier loops  able technically happens result ﬁnding suboptimal  new path node probability distribution updating value functions lines   stan               updated line  possibly increasing probability dard ao value newly expanded node  markov state  positive value process dated consists recomputing value function  illustrated figure  standard ao expands bellman’s equations eqn  based value functions  tip nodes hao expands nodes children explicit graph note backups  moved closed open “in middle of”  greedy policy subgraph                              assume performing action state                                                    hao considers possible successors  allowed error ends execution zero constant reward  given state distribution pn typically ex beneﬁcial use tree implementation ao  panded ﬁrst time enumerate actions possible problem graph tree duplicating nodes  ∈ anx  reachable pnx   represents discrete state reached through different paths                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         initial greedy graph actions multiple possible greedy graph expanded path      discrete effects possible effects optimal resource levels        curves represent current probability distribution pn changed consequence       value function values expanded  showing node reachable      fringe node                                         action doable                                          figure  node reexpansion    involve continuous states ∈ each node just expressed  reachable values consider actions                                                                                                          arrival nodes reachable according pn pnx           pn  prn                                                                                  value state updated new value propa                                                                            a∈Ωn  gated backward explicit graph backward propaga                                       tion stops nodes value function modiﬁed                    prx    ndx                                                                                                    andor root node process performed domain possible values  Ωn  applying algorithm  newly expanded node      set pairs greedy action                                                        reachable resource level                                                                                                                             Ωn   ∈ ×  ∃x ∈                                                                            ∗                                                                                   µ   prn           newly expanded node                                                                                          ∗     ∅                                   µnx ∈  greedy action clearly                          choose node ∈ descendant restrict attention stateaction pairs Ωn                     remove                                 note thisp operation induce loss total probability     update vn following eqn                      mass pn    pn  run resource     vn modiﬁed previous step     during transition end sink state                                 add parents explicit graph distribution pn node open list                                                       optimal decision changes   updated priority gn recomputed using following         pn                                 equation priority nodes closed maintained                                                            update greedy subgraph greedy                       necessary                                                                gn               pnxhnxdx                 mark use line  algorithm                             old                                                                       x∈spn−xn       algorithm  updating value functions vn     sp    support      sp                                                                                      old                                                        ∈    xn   contains ∈                                                          state expanded                                                           old                                                        xn    ∅ expanded techniques  updating state distributions line  pn’s represent used represent continuous probability distributions pn  state distribution greedy policy need compute continuous integrals discussed  updated recomputing greedy policy pre subsection algorithm  presents state distribution  cisely needs updated each descendant node updates applies set nodes greedy  optimal decision changed update node decision changed during value updates including newly  consider parents greedy policy graph expanded node hao – algorithm   actions lead parents  probability getting continuous component  handling continuous variables  sum possible values computationally challenging aspect hao  continuous component probability arriving handling continuous state variables particularly
