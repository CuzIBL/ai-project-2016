                                  extending dtgolog options                                         ferrein ch fritz lakemeyer                                             department science                                                       rwth aachen                                       ferrein fritz gerhardcsrwthaachende         introduction                                                briefly introduce mdps dtgolog section  describes   recently boutilier et al  proposed language dt     options map dtgolog   golog combines explicit agent programming           end discussion experimental results   decision theory motivation user   decision theory golog   idea solving particular problem                                                                  fully observable mdp represented tuple    time does want commit advance                                                                                finite set states finite set   exact course action instead certain choices left                                                                  actions pr probability distribution   agent running program determining optimal                                                                   prs  denotes probability agent end•  action selection policy involves solving markov decision                                                                                    ing state  performing action state    process mdp puterman  sense dtgolog                                                                                                                                           bounded reward function objective   program thought factored representation                                                                  construct policy maximizes   mdp example boutilier et al consider maildelivery                                                                  expected total reward horizon simple algorithm   scenario task delivering mail particular per•                                                                 constructing optimal policies value iteration cf put•  son handcoded fixed agent chooses or•                                                                 erman    der various people served according   reward function note approach allows solving     dtgolog boutilier et al  thought   problems larger solvable using   roughly programming language allows user   traditional dynamicprogramming approach mdps             combine predefined primitive actions complex pro•                                                                 grams using usual constructs like sequence ifthenelse      difficult examples                                                                  recursive procedures addition nonde  dtgolog shows poor computational behavior                                                                  terministic actions model agent choose   roughly complexity determined number                                                                  alternatives semantics based situation calcu•  choices agent needs consider computing pol•                                                                 lus particular socalled basic action theories de•  icy number actions uncertain outcomes na•                                                                 scribed reiter  define primitive ac•  tures choices need considered way                                                                  tions executable change change   better control potential blowup inspired                                                                  true world special form primitive ac•  work socalled macro actions options previously de•                                                                 tion dtgolog allows socalled stochastic actions   veloped mdp framework hauskrecht et al                                                                   probabilistic outcomes just mdps example   sutton et al  idea policies cer•                                                                 possible define action right   tain subtasks like leaving room precomputed                                                                  robot moved step right probabil•  simply used working global policy                                                                  ity  moved left probability     roughly propose given domain         mdpstyle reward cost functions incorporated   identify subtasks compute local policies using standard    given dtgolog program idea com•  mdp techniques like value iteration local          pute policy sense each nondeterministic choice   policies generate representations  deterministic   resolved decision theoretic manner choos•  dtgolog program directly encoding local policy          ing action maximal expected reward assuming   terms action local policy ap•     nondeterministic stochastic actions   plicable  socalled abstract stochastic actions   policy computed cases mdp repre•  address expected outcome local policy turns  sentation infeasible size state space   representations used compute   global policy use abstract stochastic actions     mapping options dtgolog   result exponential speedup— section                                                                  sutton et al  define options triples      work partly supported german science foun•          set initial states policy   dation dfg grant la  grant nrw min•  set terminating states illustrate options con•  istry education research mswf                        sider example figure  hauskrecht et al                                                                                                           poster papers                                                                east door primitive action final step                                                                  need replace abstract actions programs                                                                  derived                                                                   experimental results                                                                  using running example conducted number exper•          figure  mazc hauskrecht et al         iments goal different distances initial                                                                  position figl shows special case distance  re•  task optimal policy position   sults given figure  xaxis depicts initial dis•  position performing action cost  goal po•   tance goal yaxis running time compared                                                                  three different approaches calculating optimal policy   sition high positive reward agent perform                                                                  dtgolog nondeterministically choosing   stochastic basic primitive actions rlud succeeding  primitive actions using set procedures like   probability  probability  ad• previous section leaving each room certain   jacent position each room options arc defined leave neighboring room choosing primitive actions   room through certain door each roomdoor combi•     goal room using options form abstract   nation gray dots correspond termination positions stochastic actions choosing primitive actions   options left lower room figure  goal room     options oe leave through east north door   respectively              similarly order   mark state  success  failure   assigned positive negative reward respectively      applying standard value iteration techniques follow•  ing computed option       optimal policy appropriate action     ai each st       each st sj probability terminate   option outcome sj starting si                 figure  runtimes three test programs maze      results translated form suitable   dtgolog given  sn suppose each st            note yaxis diagram logarithmic scale   uniquely characterized logical formula     speedup shows benefit using dt•  example pi simply expresses coordinates location  golog constrain search space providing fixed pro•  policy translated straightfor•   grams certain subtasks interestingly using   ward fashion following dtgolog program               abstract actions clearly outperforms roughly be•                                                                 cause each abstract action outcomes                                                                  corresponding program provides finegrained view                                                                  huge number outcomes need considered                                                                    taking time calculation options account                                                                   seconds use options pays horizons   sensestateo socalled sens•                   greater  calculating options offline   ing action roughly executing sensestateo establishes       neglected case frequent reuse   truth values tested        remark method guarantees optimality   following conditions sensing actions    necessarily ana essentially   necessary account mdp assumption observ•    reasons hauskrecht et ai  certainly   ability program simply prescribes   case price worth computational   optimal action according  executed long   gain finally currently assume options given   agent initial states                hauskrecht et al  discuss ways automatically      given  generate each st generating options good solution qualities issue   abstract stochastic action osi example      intend investigate future   option oe state   define ab•          references   stract stochastic action natures choices   leaving through east north door respec•        boutilier et al  boutilier reiter soutchanski                                                                     thrun decisiontheoretic highlevel agent programming   tively probabilities actually occur•  situation calculus proc aaai    ring computed                                                                     hauskrecht et al  hauskrecht meuleau kael     just options mdp framework treated like       bling dean boutilier hierarchical solutions mdps                                                                     using macroactions proc uai    primitive actions mdps use options transla•  tion abstract stochastic actions dtgolog frame    puterman  puterman markov decision processes dis•  work effect write dt•           crete dynamic programming wiley new york    golog programs treating abstract stochastic actions just like  reiter  reiter knowledge action mit press    primitive actions global policy computed dt•   sutton et al  sutton precup singh   golog program usually mentions abstract actions       mdps semimdps framework temporal abstraction   readily executable leaving room through     reinforcement learning journal artificial intelligence        poster papers                                                                                                         
