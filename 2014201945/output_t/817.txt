        junction tree filters simultaneous localization mapping                                                       mark paskin                                               science division                                            university california berkeley                                                    berkeley ca                                                  paskincsberkeleyedu                               abstract                              grows time kalman filter used compute                                                                 filtered belief state observations timewhich        simultaneous localization mapping slam                                                                 case takes form multivariate gaussian distribu•       fundamental problem mobile robotics                                                                 tion regard meanas estimate        robot navigates unknown environment                                                                 map covariance matrix measure confidence        incrementally build map surround•                                                                   kalman filter solution elegant does scale        ings time localize                                                                 large slam problems explicitly repre•       map popular solution treat slam                                                                 sents correlations pairs variables size        estimation problem apply kalman fil•                                                                belief state grows each        ter approach elegant does scale                                                                 correlations updated landmark       size belief state time com•                                                                obscrved time complexity filter operation        plexity filter update grow quadratically                                                                          quadratic complexity renders kalman fil•       number landmarks map pa•                                                                ter inapplicable large slam problems gives rise        presents filtering technique maintains                                                                 need principled efficient approximations        tractable approximation belief state                                                                    unfortunately simplest approach—discarding correla•      junction tree junction tree grows filter                                                                 tions each variable estimated independently—presents        updates periodically thinned efficient                                                                 problems ignoring correlations robot state        maximum likelihood projections inference re•                                                                landmarks states leads overconfidence divergence        mains tractable applied slam prob•                                                                correlated observations treated con•       lem junction tree filters linear                                                                veyed independent information hebert et al  fur•       space belief state lineartime filtering opera•                                                                thermore correlations pairs landmark states        tion approximation yields filtering oper•                                                                required quick convergence robot closes loop        ation constanttime experiments                                                                 reobserves known landmarks extended pe•       suite slam problems validate approach                                                                 riod mapping unknown territory figure                                                                  robot closes loop reobserves landmarks po•   introduction                                                sitions known relative certainty helps robot                                                                 localize robotlandmark correlations translate im•  simultaneous localization mapping slam—where                                                                 proved localization estimate improved position estimates   robot navigating unknown environment incre•                                                                recentlyobserved landmarks interlandmark corre•  mentally build map surroundings localize                                                                 lations translate improved position estimates im•  map—has attracted significant attention                                                                 provements remaining landmarks tour    required applications mobile robotics thrun                                                                                                                   correlations kalman filter valuable property    typically environment idealized con•                                                                normally associated smoothing algorithms use   sists unknown number stationary landmarks                                                                 current observations improve estimates past   example given slam application landmarks                                                                   quadratically correlations necessary   lowlevel visual features structural features                                                                 close loops view challenge scalable slam filtering   walls corners slam viewed prob•                                                                estimating reasoning quadratically   lem incrementally estimating locations robot                                                                 correlations quadratic time space complexity   landmarks noisy incomplete observations                                                                 paper present novel general approximate filter•    popular approach treats slam filtering problem                                                                 ing method satisfying criterion point departure   smith et al  hidden state time    represented random variable includes xt                                                                     robotlandmark correlations decay time mo•  state robot time  locations            tion noise translate improved localization estimate   landmarks observed time size        improvements landmarks observed distant past con•  state vector linear number observed landmarks trast interlandmark correlations decay time       robotics                                                                                                                                                                           tree grow making inference expensive section                                                                   present novel thinning operation junction trees                                                                  called variable contraction prove each variable con•                                                                 traction maximum likelihood projection removes                                                                  set edges corresponding graphical model ap•                                                                 proximation error introduced variable contraction                                                                  computed efficiently allows choose edges                                                                  remove each time step minimize error                                                                     section  apply techniques slam prob•                                                                 lem obtain junction tree filter tjtf                                                                  space belief state representation time filter op•   figure  robot travelling counterclockwise      eration delaying incorporation recent evidence    square path dots represent landmarks true position   majority map improve filters time    robot shown square filter belief state vi• complexity present method evaluating signifi•   sualized using  confidence ellipses variable    cance evidence different portions map    marginals bold robot left accumulated noise   used adaptively interpolate constant    error led uncertain drifted estimates robot lineartime filter operations empirically   landmark positions right closing loop  adaptive filters choose constanttime updates mapping   position estimates improve confidences increase new territory closing loop use time lin•                                                                 ear length loop best time                                                                  complexity hope slam problem   section  view filtered belief state kalman linearlymany estimates improved constant time   filter gaussian graphical model cowell et al  section  presents results simulation experiments   evolves time allows express correlations   compare tjtf slam filters section  concludes   terms direct dependencies edges indirect dependen•    companion technical report contains proofs proposi•  cies paths analyzing evolution graphical model  tions additional background analysis experi•  reveals filter updates add edges graphical model  ments paskin    making inference expensive motivates approx•  imation scheme weak redundant edges period•     related work   ically removed improve complexity inference note    significant slam complexity problem led   edge removal different simply discarding cor• number approaches thrun  example   relations edges left intact paths—and submap approaches decompose prob•  correlations—persist each pair variables           lem set small mapping problems yielding block     graphical models valuable insight good     diagonal landmark covariance matrix techniques   approximate filters designed using rep•    achieve constant time complexity converge slowly be•  resent belief state presents problems variable     cause information pass submaps   marginals like robots current position im•     recently fastslam algorithm montemcrlo et al   mediately available kalman filter repre•    —a raoblackwellized particle filter—has attracted at•  sentation require inference obtain sec•     tention logarithmic time complexity   ond possible remove edges gaussian      experiments show fastslam susceptible divergence   graphical model using iterative proportional fitting algo• large noisy slam problems believe   rithm speed kiiveri  application context number particles required satisfactory solution   prohibitively slow finally choosing edges     grow exponentially time paskin  details   removal leaves distribution inference tractable   sparse extended information filters seif thrun et al   complicated process kjaerulff               viewed terms graphical model rep•     solution problems use different rep•   resentation described each timestep edges re•  resentation belief state exact inference graphical  moved constanttime filter operation guaran•  models implemented message passing junc•      teed avoid additional complexity inference seif   tion tree cowell et ah  view junction  employs approximate inference approximate model   tree algorithm inference engine use junction   seif paper provided valuable insight sparse   tree representation belief state repre• graphical models constitute efficient solution slam   sentation advantages belief state built  implementing insight avoiding additional approxi•  inference algorithm message passing gives    mation primary motivations work   immediate access marginal distribution vari•     each approaches described uses sublinear  able demonstrate gives efficient methods  time filter update improve   selecting edges prune pruning                     landmark estimates single update like kalman fil•    implement junction tree filter develop meth•   ter tjtf best worlds update step takes   ods updating junction tree reflect filtering updates constant time unless observation significant   section  updates cause width junction   warrant lineartime update                                                                                                                 robotics   outside slam literature works    change treatment landmark observed   especially relevant kjairulff  investigated edge re• state variable added belief state uninfor  moval means reducing complexity inference      mative infinite variance zero covariance prior measure•  graphical models approach somewhat simpler     ment update yields informed posterior estimate state   operates directly junction tree referring   underlying graphical model kjaerulffs analysis    gaussian graphical models   approximation error inspired results assumptions outlined filtered belief state   apply directly case                                                    multivariate gaussian distribution     junction tree filtering assumed density filtering kalman filter represents distribution using moment   adf algorithm periodically projects filters parameters—the mean vector covariance matrix   belief state tractable family distributions—in probability distribution   case family gaussian distributions characterized   junction trees makes work adf relevant                                                             especially boyen koller  be•  lief state dynamic bayesian network periodically pro• length contrast gaussian graphical   jected productofmarginals approximation fact  models usually based canonical parameters—   connection work stronger boyen koller   information vector matrix   extended earlier analysis filters belief state   represented junction tree structure evolves                                                          time algorithms presented knowl•  edge tjtf algorithm analysis applies                                                                 log nor•  apply tjtf gaussian graphical model noth•                                                                malization constant canonical moment parameters   ing prevents application discrete variable networks                                                                 related advantage   considered boyen koller                                                                 canonical parameterization multiplicationdivision                                                                 gaussians reduces additionsubtraction parameters    graphical model perspective slam                         let set random variables indexed   begin presenting slam model formulat•      elements finite set subset   ing slam filtering terms graphical models              family family                                                                 associated set random variables potential family    slam model                                                    nonnegative function let set   assume general slam model each time step        families let set potential   robot moves obtains odometry measurement       functions families defines distribution   motion makes observations landmarks   kalman filter context assume motion                                                             measurement models known linear  gaussian robot motion time  governed          normalizer finite                                                                markov graph associated vertex set                                                                 clique edges each   odometry measurement yt time governed      edge bound potential                                                              primary value markov graph representation comes                                                                 hammersleyclifford theorem states   yt typically noisy measurement robots velocities separates markov graph     landmark measurements typically assumed depend       iff provided words graph   state robot state observed separation markov graph encodes conditional inde•  landmark example observation consist      pendence properties conditional independence   range bearing landmark robots coordinate   properties translate efficient inference algorithms   frame zth landmark measurement time issued    junction tree markov graph gives good intuitions   landmark governed                                    design efficient approximations                                                                represent gaussian  markov graph                                                                 partition vector   simplicity assume correspondence each   measurement landmark issued   known question data association critically   important slam largely orthogonal issues   address particular standard technique choos•  ing maximum likelihood data association applies        models lineargaussian approx•  imated extended unscented kalman filter       robotics                                                                                                             normalization constant po•   tentials gaussian graphical model unary node    potentials binary edge potentials im•   portant interpretation    unity superfluous meaning edge    corresponding markov graph       filtering gaussian graphical models    filtering viewed threestep procedure estima•   tion incorporate current time steps mea•    figure  example evolution slam graphical model    surements prediction augment model     initial belief state robots state land•   state variables time step rollup marks states marginally independent    marginalize state variables past time     observing each landmark induces correlation    step measurement motion models linear      resulting new edge prediction update    gaussian prediction estimation steps reduce mul•  adds new robot state model joins    tiplying small gaussian potentials model up•  current robot state  rollup phase marginalizes    dates summarized                                           model adding clique edges                                                                  neighbors    proposition  ignoring irrelevant normalization constants    motion update equation  implemented mul•   tiplying potential                                         filtering slam graphical model                                                                  using results characterize structure                                                                  slam belief state evolves time figure                                                                   each observed landmark multiply measurement poten•   model odometry measurement update equation   tial graphical model adds edge be•    implemented multiplying potential        tween xt estimation phase robots                                                                  state connected states landmarks                                                                  observed prediction phase connects                                                                  finally rollup phase marginalizes places    landmark measurement update equation     potential markov blanket includes    implemented multiplying potential                   observed landmarks slam graphical                                                                  model takes form complete graph—ie belief                                                                  state conditional independencies induction                                                                  true time step      final step filtering rollup marginalizing  intuition graphical model dense    past state standard rule marginalization  time valuable robot measures landmark    canonical parameterization given cowell et al   landmarks state directly correlated                                                                  robot indirectly correlated covariates    fact   vi                                                                  robot state landmark states robots                                                                  state eliminated model during rollup indi•                                                             rect correlations expressed directly new edges                                                                    importantly indirect correlations                                                                  weaker direct ones slam                                                                  belief state true conditional independencies                                                          id     approximate conditional independencies                                                                  landmarks observed beginning end tour                                                                                                                            independent given observed middle    time complexity computing   quadratic   removing weak edges graphical model en•   dimension cubic dimension             force approximate conditional independencies      additive updates viewed multiply•  used speed inference    ing new potential    model markov blanket set zs                junction tree filtering    neighbors markov graph missing edges   markov graph correspond zeros infer  discussed introduction graphical model repre•   really potential marginal•       sentation valuable motivating approximate filter    izing ui model places clique edges   appropriate representation implementation   markov blanket                                           instead represent belief state filter using junc•                                                                 tion tree begin briefly summarizing relevant con•        model parameter indices omitted notational simplicity cepts cowell et al  details                                                                                                                 robotics  junction trees                                             update implemented multiplying small simple   let distribution form  families po• potentials probability distribution roll  tentials undirected                               phase implemented marginalizing variables   graph each vertex cluster subset     model section incrementally   junction tree following three properties hold maintain consistent junction tree updates                                                                    follows make use three nonstandard op•     singly connected property tree                                                                  erations restructure consistent junction tree      potential property family                                                                    • cloning clone cluster create copy attach        cluster                                                                       separator set      running intersection property present        clusters present clusters    • merging let neighboring clusters sep•       unique path                                  arator merge  update                                                                        update  swing edges incident   each edge associate separator                         remove            let set ts separators      given junction tree  perform inference      • pushing let neighboring clusters sep•  model passing messages clusters             arator push   begin associating set potential functions            update                                      each cluster         pass message update   separator charge defined                         extension push nonadjacent clus•                                                                      ter successive pushes unique path                                                                       nonmaximal clusters created pushing                                                                       subsequently merged subsuming neighbors                                                                  easy check operations preserve   initialize setting cluster separator potentials                                                                  three structural constraints charge consis•  unity multiplying each potential                                                                  tency junction tree       possible potential property mul•  tiplying arbitrary                                multiplying potentials      let adjacent clusters separator assume belief state represented consistent junc•  passing message updates separator potential  tion tree order update charge reflect      cluster potential follows                       multiplication potential                                                                  cluster multiplyinto restore consistency                                                              pass messages  twice                                                                  work needed simple consequence messagepassing                                                                  updates   need distribute evidence                                                                                                                            pass messages edges preorder                                                                  traversal   importantly updates leave charge  invariant                                                                    cluster covers family new po•            view reparameterizing                                                                  tential modify junction tree create   distribution messages passed edge                                                                  draper  presents techniques   directions appropriate schedule cluster                                                                  gaussian case problem somewhat simpler   separator potentials updated marginals                                                                  potentials bind variables multiplying     respective variables junction tree state                                                                  edge potential requires create cluster cover•  called consistent used obtain marginals                                                                  ing  closest pair clusters   set variables reside cluster                                                                  push  multiply      nonmaximal clusters                                                                           distribute evidence   number messages required inference bounded  •                                                                    worth noting cases conditional inde•       case gaussian graphical model cluster                                                                  pendencies obviate evidence distribution step   separator potentials gaussians represented                                                                  significant optimization message passing far   canonical parameters time complexity passing                                                                  expensive operation occurs example   message dominated cost marginalization                                                                  performing prediction step unob•   implemented                                                                    served directed leaf graphical model does   worst cubic size cluster sum inference                                                                  impact distributions nodes ob•  linear cubic width traditionally defined                                                                  serving landmark time uninformative   size largest cluster minus                                                                  prior certain types odometry updates    incremental junction tree maintenance                      marginalizing variables   adopt consistent junction trees belief state represen• assume consistent junction tree rep•  tation filter belief state represented resenting described section  marginalizing   charge  consistent junction tree recall sec• places potential markov blanket  be•  tion  prediction estimation phases filter cause junction tree cluster covers new       robotics                                                                                                             
