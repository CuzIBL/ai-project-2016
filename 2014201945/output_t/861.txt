              state abstraction discovery irrelevant state variables                         nicholas jong                                peter stone               department sciences              department sciences                 university texas austin                university texas austin                      austin texas                           austin texas                    nkjcsutexasedu                          pstonecsutexasedu                        abstract                          given user manually determines condi                                                        tions hold supplies corresponding state abstraction      abstraction powerful form domain knowl   rl algorithm      edge allows reinforcementlearning agents   propose alternative basis state abstraction      cope complex environments cases conducive automatic discovery intuitively      human supply knowledge ab    possible behave optimally ignoring certain aspect      sence prior knowledge given model   state representation agent reason ig      propose algorithm automatic discov nore aspect during learning recognizing discover      ery state abstraction policies learned ing structure tends slower learning optimal      domain use domains similar havior policy thrun schwartz  approach sug      structure end introduce novel condi gests knowledgetransfer framework analyze      tion state abstraction terms relevance policies learned domain discover abstractions      state features optimal behavior ex improve learning similar domains test      hibit statistical methods detect condition abstraction possible given region state space      robustly finally show apply temporal  statistical methods trade computational      abstraction beneﬁt safely partial state sample complexity      abstraction presence generalization error                                                          care apply discovered ab                                                        stractions criteria use discovery strictly    introduction                                       weaker given prior work safe state abstrac  humans cope unfathomably complex world tion transferring abstractions domain  ability focus pertinent information ignor introduce generalization error preserve conver  ing irrelevant contrast research gence optimal policy encapsulate state abstrac  artiﬁcial intelligence relies ﬁxed problem representations tions temporal abstractions construe sequences  typically researcher engineer feature space rich primitive actions constituting single abstract action sut  allow algorithm ﬁnd solution small ton et al  contrast previous work temporal  achieve reasonable efﬁciency paper abstraction discover abstract actions intended just sim  consider reinforcement learning rl problem plify state representation achieve certain goal  agent learn maximize rewards initially state rl agents equipped abstract actions  known stochastic environment sutton barto  learn apply state abstraction way learn  agent consider aspects each situation execute action  form choices spending resources worrying section  ﬁrst contribution alterna  minutiae practice complexity state represen tive condition state abstraction statistical mechanisms  tation key factor limiting application standard rl discovery section  second contribu  algorithms realworld problems                    tion approach discovering state abstractions    approach adjusting problem representation state encapsulating temporal abstractions sec  abstraction maps distinct states original tion  present empirical validation approach  formulation single abstract state agent treat section  discuss related work section   states exactly way agent conclude  learn optimal behavior markov decision process mdp  formalizes underlying domain obeys certain  policy irrelevance  ditions relevant states share local  havior abstract state space dean givan   deﬁning irrelevance  ravindran barto  prior research recapitulate standard mdp notation mdp  applies planning context mdp model hs ri comprises ﬁnite set states ﬁnite setof actions transition function  × × →     testing irrelevance    reward function  × →  exe    access transition reward functions  cuting action state yields expected immedi evaluate policy irrelevance candidate set  ate reward rs causes transition state state variables solving mdp using method  probability policy π  → speciﬁes policy iteration yield set optimal actions  action πs state induces value func π∗s ⊆ each state policy irrelevant        π  tion    →   thatp satisﬁes bellman equations action each sets each assignment   π                                     π                   ∗      rs πs  γ s∈s πs     π   ∅                                                          sx  γ ∈    discount factor future reward testing policy irrele  necessary make equations satisﬁable vance rl context trick                                    ∗                                                       mdp optimal policy π exists maximizes ier domain        value function state simultaneously denote optimal policy  unique optimal value function ∗ learning algo                                                                                             case domains  rithms converge optimal policies estimating opti tain structure symmetry                                 ∗  mal stateaction value function  × →   current rl algorithms focus   ∗                                  ∗                                                      rs  γ s∈s πs     ﬁnding single optimal action    loss generality assume state space each state optimal figure  domain  cartesian product domains state variables                          figure  learned                                                        actions example figure  values      xn state variables      ym shows values learned                                                                             × · · · × xn × × · · · × ym write sx run qlearning standard algorithm employs                                                                                     ∗  note projection  sx denote stochastic approximation learn watkins   agrees state variable  goal state variable actually policy irrelevant  determine safely abstract away work data conclude agent know  introduce novel approach state abstraction called pol value behave optimally   trial  icy irrelevance intuitively agent behave optimally allowed learning algorithm exploration ﬁnd  ignoring state variable abstract optimal policy converge accurate  state variable away formally say policy ir values stateaction pair argue phe  relevant optimal policy speciﬁes action nomenon quite common practical applications                       sx                      sufﬁcient exploration inherent stochasticity                                                        domain disguise state variable irrelevance pro                         ∗         ∗                                   ∃a∀s sx ∀a  ≥      pose methods detecting policy irrelevance manner                                                        robust variability  policy irrelevant policy irrelevant  entire domain                                statistical hypothesis testing    consider illustrative toy                       hypothesis testing method drawing inferences                                                      true distributions underlying sample data section  domain shown figure   just nonterminal states                    apply method problem  scribed state variables                     ferring policy irrelevance end interpret rl al                                                     deterministic                  gorithm’s learned value qs random variable  actions represented solid                   distribution depends learning algorithm  dashed arrows respectively               domain ideally directly test hypothe      actions ter                       sis  holds lack appropriate test statistic instead                                figure  domain assume reasonable rl algorithm means  minate episode determine nonterminal states                                                        distributions share relationships corre  ﬁnal reward indicated actions                                          ∗                                actions transition sponding true values qs ≥ qs  ≡ ≥  ﬁgure domain                          ∗      optimal policies absorbing state shown  test propositions form  express  solid arrow               qs ≥ qs              dashed arrow   say using standard procedure onesided paired ttest  policy irrelevant entire domain           wilcoxon signed ranks test degroot  tests    note simply aggregate output each hypothesis  signiﬁcance level psaa   states states mccallum pointed state qs  qs value uniformly random  distinctions sufﬁcient represent optimal policy number interval   psaa tend  necessarily sufﬁcient learn optimal policy mccallum  hypothesis  true  false   example observe treat   combine values straightforward way obtain  single abstract state   learn conﬁdence measure hypothesis   dashed arrow transitions abstract state                                                                      max  min  min  psaa                                                                                            solid arrow earns greater immediate reward              sx    demonstrate circumvent problem ben   eﬁtting abstraction section              discounting learning rate  boltzmann exploration                                                        starting temperature  cooling rate   episodes  figure  shows values toy domain ob value   cases actually relevant  tain data necessary run test ran  independent obtain value near  action optimal  trials qlearning used wilcoxon signedranks test obtain uniformly random number    unlike ttest does assume qs gaus action optimal achieves similar results  sian figure “random” looking values using data method incurs higher computational  accept policy irrelevant values cost need solve multiple mdps  figure values close  reject  hypothesis policy irrelevant value   abstraction discovery  work use  threshold rejecting hypoth  esis  exceeds  irrelevant  discovering irrelevance  entire domain practice number quite techniques described section  involve  conservative cases hypothesis false stages computation ﬁrst stage acquire sam  consistently values orders magnitude smaller ples stateaction values solving task repeat                                                        edly solving sampled mdps repeatedly second                                           stage use data test relevance arbitrary sets                                              state variables arbitrary states tests                                                        second stage cheap relative cost ﬁrst                                                                                    stage number possible tests astronomical                                        limit sets state variables test                                                states test                                                          consider sets state variables straightfor  figure  value each abstract states ward prove policy irrelevant  testing policy irrelevance    subset policy irrelevant corollary                                                        need test policy irrelevance     yk  monte carlo simulation                                    yk− yk policy irrelevant  hypothesis testing approach computationally efﬁcient observation suggests inductive procedure  requires large data explored alter ﬁrst tests each individual state variable policy irrelevance  native approach designed conserve experience data tests increasingly larger sets necessary  interaction domain expensive draw inductive process continue long ﬁnd  work bayesian mdp models dearden et al  creasingly powerful abstractions  reason directly distribution each qs afford test each state variable given state  technique regards successor state given state number variables relatively small contrast  action pair random variable unknown multino total number states quite large exponential  mial distribution each multinomial distribution number variables adopt heuristic approach  form bayesian estimation maintains probability dis tests policy irrelevance states visited  tribution multinomial parameters conditioning small number trajectories through task  state transition data run arbitrary rl algorithm states determine sets state variables  joint distribution parameters multinomi policy irrelevant described each set state  als gives distribution transition functions vari variables construct binary classiﬁcation prob  ance distribution goes  mean converges lem training set comprising visited states ap  true transition function data increases propriate classiﬁcation algorithm allows generalize    bayesian model domain ap region each set state variables policy ir  ply monte carlo simulation make probabilistic statements relevant note section  steps ensure  values sample mdps model classiﬁers’ generalization errors lead appli  solve obtain sample each value cation unsafe abstractions  estimate probability q∗s ≥ q∗s holds  fraction sample holds use  exploiting irrelevance  probability estimate way used signif section  describes represent learned classi  icance levels hypothesis testing approach obtain ﬁer region state space given set state  conﬁdence measure policy irrelevance variables policy irrelevant straightforward approach      max  min  min  prq∗s ≥ q∗s  state abstraction simply aggregate                              sx    method yield qualitatively similar results ameliorate cost somewhat initializing each mdp’s  hypothesis testing method obtain value function value function maximum likelihood                                                        mdp strens     possible build bayesian model reward func converse necessarily true suppose duplicate  tion domains studied use deterministic relevant state variable each copy state  wards                                                variable policy irrelevant given remainder state    use standard value iteration                   representation pair notstates region differ irrelevant vari state abstractions join set optimal actions each ap  ables approach prevent rl algorithm propriate state smaller state representation allow  learning correct value function op option policies converge quickly rl algorithms  timal policy section  gave simple example learn exploit optimal policy fragments instead  abstraction failure perfect knowledge policy covering optimal policy hard way illustrate  irrelevance generalizing learned classiﬁer visited process section  states domain unvisited states similar domain  introduces source error solution  problems encapsulate each learned state abstraction  results  side temporal abstraction particular apply each state use dietterich’s taxi  space aggregation inside option sutton et al  main dietterich  illus  abstract action persist multiple time trated figure  setting  steps original mdp                            work domain    formally set state variables policy irrele state variables ﬁrst  vant ⊆ construct option  hπ βi correspond taxi’s cur                                 comprising option policy π  → initiation set rent position grid world  ⊂ termination condition β  →   indicates passen  agent executes option state ex ger’s current location  ecutes primitive action πs each state terminating                                                        labeled positions red figure  taxi domain  probability βs set  βs   green blue yellow  ∈ βs   policy irrelevant side taxi fourth indicates labeled position  choose option policy π equal pro passenger like domain                jection optimal policy original mdp  ×  ×  ×    possible states each time step  agent augmented options behave optimally taxi north south east west  original mdp executing options tempt pick passenger attempt  possible                                        passenger actions taxi through wall    believe discovery structure grid effect action reward   interesting right utility appar illegal attempts pick passenger  ent consider transferring discovered options reward  agent receives reward   novel domains access achieving goal state passenger des  optimal policy transfer option new domain tination inside taxi paper consider  simply copy initiation set termination condition stochastic version domain taxi  straightforward approach sufﬁces domains share pre tempts resulting motion occurs random  cisely state space original domain pendicular direction probability  furthermore  state space changes representation β taxi picks passenger begins desti  learned classiﬁer gives hope reasonable generalization nation changes probability   copy option policy π expect opti domain’s representation requires state  mal behavior original domain remain optimal variables general affords opportunity ab  new domain                                       straction particular note passenger’s destination    paper assume policy irrelevance relevant agent picked passenger  remains relearn option policy applied methodology described sections    currently learning highlevel policy task discovering abstraction follows  chooses original primitive actions discov ran  independent trials qlearning obtain samples  ered options each option establish rl subproblem q∗ each trial set learning rate α    state space ix action space used ²greedy exploration ²   learning conver  option terminates state augment reward gence required  time steps each trial  environment pseudoreward equal current es data allows compute policy irrelevance state  timate optimal highlevel value function evaluated variable state example consider pas  think option learning achieve senger’s destination demonstrate typical behavior  subgoal learning behave ignoring certain state testing procedure show figure output ev  variables words option adopts goals ery location domain passenger waiting  highlevel agent learns reduced state space upper left corner red landmark using wilcoxon    each option just action highlevel signedranks test nonzero values state im  agent select rl algorithms learn disregard options ply passenger’s destination policy irrelevant  suboptimal states corresponding ab case note values extremely close   stractions unsafe options correspond safe agent optimal action upper                                                        left corner procedure identify conﬁdently    nonzero termination probability ∈ serves prob squares intermediate values precisely states  abilistic timeout escape bad abstractions    optimal action exists considerfigure shows output test relevant passenger taxi  passenger inside taxi values extremely close three rules classify state variables usually relevant   state middle narrow cases example rule holds red  layout domain agent destination upper half map   speciﬁes  behave optimally moving north                     taxi lower half obstacles                                                        particular map vertical rule example                                                        generalization holding passenger rightmost                column usually optimal just left unless passen                  ger wants green landmark upperright corner                                                          tested generalization performance learned                abstractions  ×  instances taxi domain                                                        randomly generated obstacles running horizontally                vertically placed landmark near each corner oth                                                        erwise gave domains dynamics origi                                                                       nal each abstraction implemented option dis                                                cussed section  locations landmarks                                                        moved simply transferred option policies  figure  results wilcoxon signedranks test deter original taxi domain experiments  mining policy irrelevance passenger’s destination used qlearning ²greedy exploration learn  taxi domain show result test each possible taxi option policies highlevel policy chose  location case passenger taxi apply each option each state abstraction im  case passenger inside taxi   prove learning efﬁciency added offpolicy training sut                                                        ton et al  follows primitive action    compute outcome test sub executed state updated qs high  set state variables state followed approach level agent option includes  described section  sampled  trajectories initiation set option terminated updated  domain using learned policies tested each indi qs state visited during execution  vidual state variable each state visited using hy each stateaction estimate received  pothesis testing approach created binary classiﬁcation exactly update each timestep action executed  problem each variable using visited states train state figure  compares learning performance  ing set positive examples took each state qlearner abstraction abstractions  hypothesis test returns value conservative allowed experimental qlearner converge faster  threshold  finally applied simple rulelearning optimal policy despite estimating strict superset  classiﬁer each problem incremental reduced error parameters baseline qlearner  pruning irep algorithm described cohen   typical set induced rules follows                    related work    taxi’s xcoordinate                              approach state abstraction discovery bears strong         ∧ passenger taxi ∧ destination red  semblance aspects mccallum’s utree algorithm mc          ⇒  policy irrelevant                          callum  uses statistical hypothesis testing       policy relevant                   determine features include state representation                                                        utree online instancebased algorithm adds state    taxi’s ycoordinate                                                        variable representation different values variable         ∧ passenger taxi ⇒ policy irrelevant predict different distributions expected future reward       policy relevant                   algorithm computes distributions values                                                        current representation resulting circularity pre    passenger’s destination                                                        vents guaranteeing convergence optimal state       passenger taxi ⇒ policy relevant          abstraction contrast seek explicitly preserve opti       policy irrelevant                 mality    passenger’s location destination                encapsulation partial state abstractions options                                                        inspired ravindran barto’s work mdp homo         ∧   ∨   ∧              morphisms ravindran barto  particular          ⇒  policy irrelevant                          discussion partial homomorphisms relativized       policy relevant                   options work focuses developing      sets state variables mentioned pos ²   α    itive training examples induced rule set general smdp qlearning necessary learn high  classiﬁes state variables relevant state rule set level policy actions time step   captures abstraction motivated analysis algorithm reduces standard qlearning ab  domain specifying passenger’s destination policy sence discounting taxi domain does require
