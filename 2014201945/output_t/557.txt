                    dynamic mixture models multiple time series                  xing wei                        jimeng sun                       xuerui wang     science department       science department       science department      univeristy massachusetts       carnegie mellon university        univeristy massachusetts          amherst ma                  pittsburgh pa               amherst ma           xweicsumassedu                 jimengcscmuedu                xueruicsumassedu                        abstract                          variance directions construction intu                                                        itive end users reiﬁcation mathemat      traditional probabilistic mixture models  ical properties svd techniques particular      latent dirichlet allocation imply data records streams negative coefﬁcients hidden variables      documents fully exchangeable  measurements nonnegative draw      data naturally collected time intrinsic gaussian distribution assumption      obey order time paper present input streams      dynamic mixture models dmms online pat      tern discovery multiple time series dmms     using mixture hidden variables data representation      noticeable drawback svdbased recently considerable text modeling      methods data streams negative values hid early example latent semantic indexing lsi      den variables produced non model based svd technique      negative inputs apply dmm models       recently hoffman presents probabilistic latent seman                                                                                                      realworld datasets achieve signiﬁcantly better tic indexing plsi technique hoffman   ap      results intuitive interpretation            proach uses latent variable model represents documents                                                        mixtures topics probabilistic framework latent                                                        dirichlet allocation lda blei et al  resem    introduction                                       bles generative process plsi overcomes  multiple coevolving time series data streams ubiqui drawbacks quickly popular  tous different realworld applications considering probabilistic text modeling techniques machine learning  sensor network multiple sensors continuously collect differ inspired series research works direction  ent measurements time chlorine concentration girolami kaban  teh et al  particular  different locations water distribution temperature lda shown effective textrelated tasks  light measurements different rooms host status dif document classiﬁcation blei et al  infor  ferent machines data center central site usually mation retrieval wei croft  effectiveness  analyzes measurements main trends anomaly continuous data timeseries remains unknown  detection success mining multiple streams topic models mentioned assume data  existing methods analyzing streams records documents fully exchangeable  signiﬁcant weaknesses                           true timeseries model realworld appli    existing monitoring softwares require considerable cations exactly topic modeling time  time expertise properly conﬁgured despite fact starts receive attentions recent dy  treat streams independent each data stream namic topic models blei lafferty  topic evolu  administrator intends monitor heshe make tions modeled through collections sliced certain peri  decisions proper thresholds data values ods time time dependency individual data  heshe deﬁne each data stream constitutes records documents inside collectionperiod consid  normal behaviors correlations streams usually ered topic time tot model wang mccal  captured                                         lum  continuous time stamps ldastyle    second stateoftheart algorithms automatically topic model observations drawing time stamps  summarizing multiple streams adopt matrix decomposi distribution tot model  tions like singular value decomposition svd vari dealing bursty data common data streams  ants example papadimitriou et al uses online svd paper present dynamic mixture model  tracking summarize multiple streams incrementally dmm latent variable model takes considera  through small number hidden variables papadimitriou tion time stamps data records dynamic streams  et al  hidden variables computed lin values streams represented mixtures hidden vari  ear combinations streams projecting maximum ables each time stamp mixture hidden vari                                                    ijcai                                                    ables streams dependent mixture pre α α      α       α  vious time stamp way capture shortterm                                                         θ       θ       θ       θ  ··· θ   θ    θ   ··· θ  dependencies compared svd techniques does                            t−           common drawback nonintuitive negative values                                 hidden variables interpretable expla  nation probabilistic framework compared                                                                                         nd      nd      nd         nt− nt   nt   nd  static ldastyle models advantage time            dependency multiple streams finally want point                                                         β       β       β                   φβ  topic models mainly designed discrete                               data apply ldastyle mixture models continuous  data standard way discretization show figure  graphical model representation dynamic topic  section  works dmm                          model left  slices dynamic mixture model right      related work                                        symbol      description  data streams containing interesting information       number hidden components  topics  static data extensively studied recent years number snapshots  documents  evidence shown temporal information plays cru        number parameters  words  cial role capturing meaningful interesting pat nt     sum parameter values tth snapshot   terns main objective temporal analysis efﬁciently    number words tth document  discover monitor patterns data streaming       snapshot index  time stamp documents  recent survey muthukrishnan  dis    θ       mixture distribution hidden components  cussed data streams algorithms svd           snapshot  document  pca popular tools summarizing multiple time    φ       mixture distribution parameters  words  series number hidden variables papadimitriou et         hidden component topic  al  assumes independency time         hidden component  topic  stamps results lack probabilistic argument     unit value parameter  word token  intuitive interpretation                              α       hyperparameter multinomial θ                                                             β                       φ    probabilistic mixture models hoffman  blei et al        hyperparameter   widely used summarize data based statistical  frameworks capture time evolution usage time         table  notation correspondence  probabilistic mixture models  example time taken care post  hoc way ﬁrst ﬁt timeunaware mixture model distribution tot appropriate data streams  order data time slice discrete sub usually bursty multimodal  sets examine mixture distributions each timeslice  grifﬁths steyvers  alternatively nonjoint mod  dynamic mixture models  eling predivide data discrete time slices ﬁt mixture models widely used modeling various  separate mixture model each slice possibly align complex data competitive results usage  mixture components slices wang et al  latent mixture model streaming data explored    systematic ways advantage temporal largely remains unknown present dynamic  formation mixture models categories mixture model dmm incorporating temporal information  dynamic behaviors driven state transitions data  kind models generally make markov assumption graphical model representation dmm shown  state time Δt independent figure  right mixture models discrete  history given state time instance blei data dmm regarded topic model  lafferty present dynamic topic models dtms presentation convenience explain model  alignment topics collections documents background text streaming data section   modeled kalman ﬁlter blei lafferty  dtms scribes streaming data table  summarizes  target topic evolution sets large amounts data notation correspondences text data streams  records local dependencies records particular each snapshot corresponds document topic  captured second type models does em models each stream corresponds word occurrence  ploy markov assumption time instead treats time time number occurrences word wi time  observed continuous variable topics represents single stream wi chlorine  time tot models wang mccallum  helps concentrations time light intensities time  capture longrange dependencies time avoids ith sensor  markov model’s risk inappropriately dividing mixture generative process dmm similar latent  component topic brief gap ap dirichlet allocation blei et al  mixture dis  pearance time stamps drawn tribution θ each document does dirichlet prior                                                    ijcai                                                      ﬁrst instead θt dependent  dirichlet dynamic mixture models                                         θt−    mixture distribution previous snapshot        described θt generated distribution      assume strong evolution dependencies θt− expectation strict requirements    continuous stream data evenly sampled time distribution just want build connections    successive snapshots reﬂect intrinsic dependencies tween successive snapshots mixture distribution    values time series                 current snapshot dependent previ      second time dependency complicated  ous snapshot setting continuous onemodal distri    changes hardly follow continuous time distribution bution desired gaussian distribution straightforward    tot model wang mccallum  neighboring selection θ represented natural parameters    dependency markov assumption appropriate   adopted blei lafferty  model depen      compared tot model dynamic mixture model dency consecutive α figure  left    constructed discrete time stamps assumes depen gaussian distribution conjugate multinomial dis    dencies consecutive snapshots tribution used generate hidden components     tot model captures shortterm longterm changes ics makes inference difﬁcult    treating time stamps observed random variables dmm best knowledge conjugate prior dirich    capable capture detailed changes model let distribution dirichlet distribution conjugate multino    dependency consecutive time shots mial distribution use dirichlet distribution model    especially appropriate streaming data dependency consecutive θ expectation    data equally sampled time                     make proposed distribution identiﬁable      compared dynamic topic models dtm figure  inﬁnitely dirichlet distribution having    left dmm captures evolution snapshots doc expectation introduce precision    uments instead snapshotgroups collections constraint parameters dirichlet distributions    text model dtm lda documents col sum ψ equal sum parameters α    lection words document fully exchangeable ﬁrst dirichlet distribution dirichlet distribution    dmm snapshots multiple time series correspond completely parameterized mean precision pa    documents text model strong temporal order rameters minka  notation convenience deﬁne    exchanges snapshots lead different results θ  αψ θtθt− ∼ dirψθt−    perspective dmm true online model note    svd treats time series vectors permutation inference    snapshots make difference              inference exactly complex graphical mod      model dependency setting expectation els dmms nonconjugacy dirichlet    distribution θt θt− θt generated dis distributions makes standard gibbs sampling methods ge    tribution θt− st order moment discuss man geman  harder approximate inference    concrete distribution section  process gen dmms use simple effective iterated sam    erating time series streams dmm follows     pling procedure considering streaming data dif                                                          ferent traditional data sets large text collections                                  φ      pick multinomial distribution each topic hidden ﬁrst massive amounts data arrive high rates                         dimension dirichlet distribution parame makes efﬁciency concern second users higher           β        ter                                              level applications require immediate responses      time shot  sample multinomial distribution afford postprocessing network intrusion detec        θt dirichlet distribution parameter α tion papadimitriou et al  summary algorithm                                                          expected efﬁcient incremental scalable possi                             each time shot    sample multinomial distri bly scaling linearly number streams shown              θt                              θt−        bution  distribution expectation     grifﬁths steyvers  deﬁne mzw num      sample hidden variable  topic ∈ ···k ber tokens word assigned topic posterior                                                                      φ        multinomial distribution parameter θt       distribution approximated                                                                                    β      add unit value parameter picked multi             φˆ          zw                                           φ                                zw   v        nomial distribution parameter zw  pick word                             β                                                 φ                                 zw           multinomial distribution parameter zw                                                                                             θ    likelihood generating data set multiple data estimate posterior distribution streaming data    streams                                           use technique commonly used meanﬁeld vari                                                          ational approximation assume each latent variable                                                                              k               dependent use iterative procedure   snapshot ··· snapshotdα β      pφzβ       update θ periodically deﬁne ntz num                                                      ber tokens document assigned topic          d          d nt k                            ×pθ α   pθ θ            θp φ  dθdφ                           ψθˆ               t−                     zi                      ˆ          tz    t−z                                                                      θtz                                      zi                                                      ˆ                                                                               zntz  ψθt−z                                                      ijcai                                                        each sample draw turn zti according prob                     θˆ   × φˆ                 θˆ  ability proportional tzti ztiwti  update  each iteration    streams special data require corresponding  algorithms highly efﬁcient order react timely  manner certainly possible derive variational  approximation scratch empiri  cally procedure effective efﬁcient  streaming data presented section  iter  ated sampling algirithm information ofﬂine  training run sampling coming snapshot  couple iterations parameter estimation updated  each new snapshot arrives                                                        figure  chlorine reconstruction reconstruction based     experiments                                        hidden variables close original value  section present case studies real realistic  datasets demonstrate effectiveness approach  discovering underlying correlations streams    chlorine concentrations  description chlorine dataset generated epa  net accurately simulates hydraulic chem  ical phenomena drinking water distribution systems  given network input epanet tracks ﬂow  water each pipe pressure each node height  water each tank concentration chemical  species network during simulation period  comprised multiple time stamps monitor chlo  rine concentration level  junctions network  shown figure   time stamps during  days figure  hidden variables chlorine data ﬁrst captures  time tick ﬁve minutes data generated daily cycles second reﬂects time shift different  ing input network demand patterns pressures sensors  ﬂows speciﬁed each node    data characteristics key features  clear constraint probability mass parameters  global periodic pattern daily cycle dominating residential equals unlike svd hidden variable lin  demand pattern chlorine concentrations reﬂect ear combination streams method interprets hid  exceptions  slight time shift different junc den variable generating process  streams  tions time takes fresh water ﬂow intuitive figure  notice  pipes reservoirs                   high probability mass stream  sensors    streams exhibit sinusoidallike pat highlighted figure  close reservoir  tern gradual phase shifts away main pipe water distribution network  reservoir    reconstruction method successfully summarize  light measurements  data using just numbers hidden variables time description dataset consists light intensity measure  tick figure  opposed original  numbers ments collected using berkeley mote sensors dif  figure  shows reconstruction sensor  ferent locations lab figure  period  reconstructions sensors achieve similarly month deshpande et al  simulate streams  sults note hidden variables good dataset cutting set parts process  reconstruction                                       snapshots second online man    hidden variables hidden variables figure  ner trained model ﬁrst results  reﬂect key dataset characteristics  ﬁrst hidden present dataset stream simulation running  variable captures global periodic pattern  second data characteristics main characteristics   follows similar periodic pattern clear global periodic pattern daily cycle  occasional big  slight phase shift                                   spikes sensors outliers reconstruction similar    interpretation hidden variables each hidden variable chlorine data reconstruction error light data  follows multinomial distribution  parameters small                                                          hidden variable ﬁrst hidden variable exhibits    httpwwwepagovordnrmrlwswrdepanethtml     daily periodicity shown figure  probability distri                                                    ijcai                                                                                                          figure  sensor map highlighted region receives  figure  distribution st hidden variable note sunshine  sensor   signiﬁcantly higher mass  main pipe water distribution                                                            figure  light reconstruction reconstruction based  hid                                                        den variables close original value  figure  water distribution network sensor  high  lighted close reservoir red arrow mixture models latent dirichlet allocation  main pipe                                        models order data ignored data                                                        records assumed fully exchangeable dmms                                                        contrary consideration temporal informa  bution figure  concentrates sensor  high tion order implied data  compared  lighted figure  receive sunshine stateoftheart svdbased methods data streams dmms  close window                                      naturally positive values hidden variables    comparison static mixture modeling show sults dmms intuitive interpretable  effectiveness modeling mixtures dynamically model believe probabilistic mixture modeling promis  ing dependency mixtures hidden variables ing direction streaming data especially dynamic mixture  compare streaming modeling based time series models shown effective model mul  nontime dependency latent variable modeling tech tiple time series application  niques lda model sums reconstruction er future work plan apply dmms larger datasets  rors time compared signiﬁcant pe −  improve performance efﬁciency  ttest error rate achieved dynamic mix line streaming data modeling dependencies using  ture model   iterations shown figure distributions dirichlet distribution                                                     teresting      conclusion future work                         acknowledgments  paper presented dynamic mixture models work supported center intelligent  dmms online pattern discovery multiple time series information retrieval national science foun  shown interesting results data sets generative dation grants iis iis int  process dmms similar traditional static probabilistic  sensor iis                                                    ijcai                                                    
