                                    distributed data mining                            aggregating models                                   mohamed aounallah        guy mineau                          science software engineering department                                   laval university quebec city canada                            mohamedaounallah guymineauiftulavalca                          abstract                          base db    ∪idbi used reference                                                        sess potential loss accuracy method      paper deal problem min    assumption stated process db      ing large distributed databases show downloadprocessing time constraints      aggregation models sets disjoint clas paper proceeds follows present sections       siﬁcation rules each built subdatabase survey known model aggregation techniques      quite aggregated model   section  present solution distributed data      predictive descriptive presents ex mining ddm model aggregation ddmma based      cellent prediction capability conceptu majority vote pondered conﬁdence coefﬁcient      ally simpler comparable techniques section  introduces conceptual comparison      results possible lifting dis method literature section       joint cover constraint aggregated model present experimental results prove viability      use conﬁdence coefﬁcient associated   method show bares comparable accuracy rates      each rule weighted majority vote       simpler ddm methods ﬁnally                                                        present conclusion future work    introduction  paper deals problem mining large  model aggregation existing techniques  geographically distributed databases dbi goal present paper technique developed dis  producing set classiﬁcation rules explains vari tributed data mining perspective ignore non  ous groupings observed data result relevant techniques ruler fayyad et al   mining predictive descriptive metaclassiﬁer fayyad et al  developed aggregation  words aim producing model decision trees build data set cen  capable predicting class new objects tralized distributed learning sikora  able explain choices predictions believe shaw  developed context information manage  kind models based classiﬁcation rules ment builds distributed learning  easy understand humans fragmentation approach w¨uthrich  uses  objectives application context probalistic rules ignore purely predictive tech  impossible gather databases site niques bagging breiman  boosting schapire  downloading time  stacking tsoumakas vlahavas andthear  difﬁculty aggregated database        biter combiner methods chan  prodromidis et    literature ﬁnd distributed data mining al   techniques predictive descriptive  majority try produce metaclassiﬁer form  mil algorithm  set rules disjoint cover object mil algorithm multiple induction learning ini  covered rule show pa tially proposed williams williams  order  constraint disjoint cover necessary solve conﬂicts conﬂictual rules expert systems  produce reliable metaclassiﬁer furthermore introduces authors hall et al took tech  unnecessary complexity mining technique nique williams williams  aggregate decision  propose simpler technique object cov trees built parallel transformed rules  ered rules lifting constraint process aggregation proposed authors  enables produce conceptually simple classiﬁer grouping rules accompanied process resolution  good prediction capability shown possible conﬂicts noted    performance metaclassiﬁer prediction resolution conﬂicts treats pair conﬂict  point view compared applied data rules time rules considered conﬂict                                                    ijcai                                                    premises consistent produce differ accompanied values necessary calculate measure  ent classes williams  called conﬂict type  associated each rule  conditions premises overlap partially hall et hall et al  authors show  al called conﬂict type ii rules extreme case property invariantpartitioning  number predicates different values satisﬁed prove precision  ditions classify objects class hall et aggregate rule set different precision  al called conﬂict type iiithe conﬂict resolution rules built training set authors show  consists specializing rules conﬂict conﬂicts rules solved described  conﬂicts type ii adjusting value condi hall et al williams   tion test boundary conﬂicts type ii iii addition hall et al  proposes new type  eventually combining rules conﬂict conﬂict ﬂict rules rule premise contains inter  type iii certain cases conﬂicts type ii new val overlaps interval contained premise  rules added based training sets recover cover second rule case general rule created  lost specialization process                   combining conﬂicting rules adjusting                                                        border values intervals    drl distributed rule learner  drl technique distributed rule learner provost  proposed model aggregation technique  hennessy  conceived based advantage proposed technique simple build parallel  invariantpartitioning property provost hennessy  each distributed dbi model set classiﬁcation  drl technique begins partitioning training data rules ri called base classiﬁer figure  shows example  nd disjoined subsets assigns each ei different rules  machine provides infrastructure communica                                                        adoptionofthebudgetresolution             tion different learners named rl rule                                                        physicianfeefreeze   satisﬁes evaluation criterion subset data                                                      class republican  eind ≥  evaluation function rule  constant candidate satisfy global                                                        adoptionofthebudgetresolution             evaluation criterion extended invariantpartitioning prop                                                        physicianfeefreeze   erty guarantees each rule satisfactory                                                        class democrat  data set acceptable subset  local learner discovers acceptable rule sends rule  machines update statistics figure  example rules contained base classiﬁer  remainder examples rule meets global evalu  ation criterion fr ≥ principal evaluation compute each rule conﬁdence coefﬁcient  function constant asserted satisfactory rule details finally centralized site base  opposite case local statistics replaced classiﬁers aggregated set rules  ∪iri  global statistics rule available special represents ﬁnal model called metaclassiﬁerthe  ized property invariantpartitioning guar global algorithm distributed data mining technique  antees each satisfactory rule data set described figure   rls                                                           parallel each database    dbi    combining rule sets generated parallel             apply  dbi  classification                                                              algorithm producing set     disjoint  work presented hall et al  mixture   cover rules    produced set  techniques presented williams   hall et al provost hennessy              ri  rik  ∈ ni  details associate each rule measurement                                                                 ni number   rules  ”quality” based prediction precision                   number type examples covers compute each   ik confidence                                                                coefficient   cr     technique suggested hall et al  based                    ik                                                           central site create  use provost hennessy  proposes                               §   small difference deletion                      ri  rule space rules consideration                   ind  rule classiﬁes data various distributed  nd  bases case measure fr lower     number distributed                                                            databases  certain threshold noted each rule  does ”travel” site                                                           figure  algorithm proposed ddm technique    rule evaluation function example laplace  precision estimator segal etzioni webb  different rule sets going merged                                                    ijcai                                                    issued different data subsets adaptation technique williams williams   each rule proper error rate er coverage nwe order treat distributed bases implies increase  compute each rule conﬁdence coefﬁcient cr conﬁ volume data exchanged various sites  dence coefﬁcient computed straightforward manner hand each rule travels accompanied index  lower bound error rate conﬁdence interval proposed covered objects hand event  langford wedeﬁneditasone minus worst   conﬂict objects covered rules  error rate  − δ time                    conﬂict downloaded training site site                                                        resolving conﬂict                cr −  bin−n nerδ                      def                                   drl  bin−n δ  minr  −  binn ≥ δ                               binn def ri − rn−i                signiﬁcant disadvantage drl                                               provost hennessy  execution time             set disjoint cover rules object rule considered acceptable given site  covered unique rule explain sites words ac  use metaclassiﬁer predictive descriptive model ceptable rule site classify data                                                      sites rule hand“travel” through    use   predictive model               sites hand classify data each  set represents aggregation base classiﬁers site rule considered satisfactory   ∪iri rule set used predictive model data set rule specialized process starts  descriptive predictive point view considered locally acceptable clear  predicted class new object class predicted process time consuming  majority vote rules cover rules  weighted conﬁdence coefﬁcients noted  combining rule sets generated parallel  contrarily identiﬁed literature §  restricted notion rules conﬂict combining rule sets generated parallel  cover object different classiﬁcation identical previous little difference  sults rules cover object predict rule generated given site cross  class consider conﬂict sites number rules traveling various    noted object covered sites signiﬁcant number rules drl  nd rules knowing nd number sites     consequently clear technique slower                                                        preceding    use descriptive model  classiﬁcation developed support  proposed technique  decisionmaking different rules covering object overcome problems mil technique proposed  proposed user judge ex based majority vote known ro  pertise relevance helped conﬁdence coefﬁcient bust model noisy data prediction process  presenting decision maker rule gives good results especially noisy bases §below  advantages provide larger com proposed technique § rules “travel”  plete view ”limits” each class bring mind way distributed database site central site  machine learning limit deﬁnes separation data minimal rule aug  various classes generally unique clear cut mented conﬁdence coefﬁcient  consequently rules producing class problem excess communication drl  represent ”hyperplanes” separating various classes successor avoided  providing various views data                   execution point view asymptotic analy                                                        sis conducted aounallah  aounallah    conceptual comparison                            mineau  technique presented §of    mil technique                                paper asymptotic analysis shows clearly                                                        worst case technique faster existing ones                                                   mil technique hall et al hall et al best case expect technique com  suffers problems process parable existing ones having conﬂicts  ﬂict resolution specializes rules based classi different base classiﬁers rare believe tech  ﬁcation rules data sets generated rules show poor nique faster existing ones technique does  classiﬁcation ability applied new objects es conduct conﬂict resolution step  pecially case noisy training data addition                                                          proposed technique sim    unlikely tie situation propose carry ple aggregation base classiﬁers consequently  simple majority vote rare cases simple majority doubt conceptually far simpler existing com  vote leads tie choose majority class different parable ones faster simpler implement  training sets                                        literature §                                                    ijcai                                                      empirical comparison                                                        table  comparison r original data sets  evaluate performance ddm technique                                                                                     lower  upper      cmp  ducted experiments order assess prediction ac                                                         bcw                        curacy rate compared algorithm built chess                    data set aggregation distributed data                                  r                     crx                     bases produces rule set  used iono                reference accuracy rate assumed intro mush                duction impossible gather bases pima            single site downloading time tictactoe      difﬁculty learn aggregated base vot                                           cause size rule set considered ideal wdbc              case theoretically possible perform better  model built data set    conducted experiments tested     table  comparison r  noise  data sets chess endgame kingrook versus kingpawn            r    lower  upper      cmp  crx housevotes ionosphere mushroom pimaindians bcw                      diabetes tictactoe wisconsin breast cancer bcwman chess                        gasarian wolberg  wisconsin diagnostic   crx                          breast cancer wdbc taken uci repository    iono                    blake merz  size data sets varies mush                     objects  objects objects missing val pima                   ues deleted furthermore order tictactoe          realistic data sets introduced noise aforemen vot               tioned databases reversing class attribute wdbc              successively     objects  each data set addition original set   noisy sets giving total number databases  produces decision tree directly trans    order simulate distributed environment data sets formed set rules conﬁdence coefﬁcient each  divided follows divided each database rule computed using program offered langford  test set proportion  data subset used langford downloaded  basis  conﬁ                                                                        δ   test set metaclassiﬁer  reference dence interval                                                        order assess prediction capability technique  classiﬁer remaining data subset proportion                                       divided randomly     data subsets order compared prediction rate   simulate distributed databases size bases aforementioned data sets table   results ob                                                        tained forth columns tables  chosen disparate way sig                                         niﬁcant difference smallest biggest data tain respectively lower upper bound error  subset example subdivision figure  rate conﬁdence interval computed  conﬁdence                                                        column contains              mushroomdata                               • “” technique outperforms r               objets                                                          • “” r outperforms metaclassier                                                          • blank techniques statistically compara    mushtesttest        mushtestdata                      ble   objets      objets                                                          tables metaclassier perfor                                                        mance comparable r  cases       mushdata obj  mushdata obj  mushdata obj                 mushtest obj  mushdata  mushdata                            r              mushtest obj  mushdata  mushdata table  comparison  noise                                                                                     mushtest obj  mushdata  mushdata             lower  upper      cmp                                                         bcw                                                                               chess                                                                              crx                     figure  example subdivision database iono                  uci                                                   mush                                                                               pima                      construction base classiﬁers used tictactoe         release  quinlan quinlan downloaded  vot                                                                                 wdbc                       note data sets binary class attribute                                                    ijcai                                                                                                          ﬁdence coefﬁcient used process uses rules                                  r     table  comparison    noise     weight reﬂects individual prediction power                                    lower upper       cmp       granularity majority vote   bcw                                 rule level instead classiﬁer level metaclassiﬁer   chess                                used descriptive model predictive class   crx                              object described rules covering   iono                               good results imagine tech   mush                                 nique applied large centralized databases   pima                     tictactoe                      divided smaller ones applying   vot                              technique applying data mining tool   wdbc                               centralized database propose explore                                                        near future                                                          furthermore propose hand test pro                                  r                  posed technique nary databases hand     table  comparison    noise     compare experimently exiting techniques               r    lower upper       cmp   bcw                       chess                              references   crx                             aounallah mineau  mohamed aounallah    iono                                guy mineau  le forage distribu´e des donn´ees  une   mush                                   m´ethode simple rapide et efﬁcace revue des nouvelles   pima                                technologies l’information extraction et gestion des   tictactoe                         connaissances rntie–    vot                          wdbc                              aounallah  mohamed aounallah le forage distribue´                                                           des donnees´  une approche basee´ sur l’agregation´ et le                                                           rafﬁnement modeles  phd thesis laval university feb   error rate statistically comparable  ruary                         cases worst  surprisingly meta                                                                    blake merz  cl blake    cj  merz  classiﬁer outperform  cases especially uci repository machine learning databases  case noise distributed data sets important httpwwwicsuciedu∼mlearnmlrepositoryhtml  cases easily advantage using noise ro   bust model weighted majority vote non disjoint                                                                      cover rule set instead using single model disjoint breiman  leo breiman bagging predictors machine  cover rule set                                          learning     results prove viability conﬁdence chan  philip kinwah chan extensible meta  efﬁcient proposed paper                         learning approach scalable accurate inductive                                                           learning phd thesis columbia university   conclusion                                           fayyad et al  um fayyad weir djorgov                                                           ski skicat machine learning automated cat  objective paper present simple dis                                                           aloging large scale sky surveys machine learning  tributed data mining technique ddm model aggregation                                                           proceedings tenth international conference pages  ma intention presented hand                                                           – san mateo ca  morgan kaufmann  rapid survey existing model aggregation techniques  comparable hand pre fayyad et al  usama fayyad george djorgov  sented description ddmma technique            ski nicholas weir advances knowledge discov    paper shown proposed ery data mining chapter automating analysis  ddm  technique conceptually far simpler existing cataloging sky surveys pages – aaai  comparable techniques consists simple aggre pressthe mit press menlo park california   gation distributed models base classiﬁers       hall et al lawrence hall nitesh chawla    experiments demonstrate technique pre kevin bowyer combining decision trees learned  diction point view performs better parallel working notes kdd   classiﬁer built data set r theoretically                                                                        ideal case built data meta hall et al lawrence hall nitesh chawla  classiﬁer outperform r weighted major kevin bowyer decision tree learning large  ity vote pondered conﬁdence coefﬁcient associated data sets ieee international conference systems  each rule conﬁdence coefﬁcient based lower man cybernetics  volume  pages –  bound error rate conﬁdence interval proposed lang oct   ford  words mojority vote imper hall et al  lawrence hall nitesh chawla  fect rules gives good predictive results kevin bowyer learning rules distributed                                                    ijcai                                                    
