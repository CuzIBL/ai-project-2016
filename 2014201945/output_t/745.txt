                   using learned policies heuristicsearch planning              sungwook yoon                        alan fern                       robert givan      science  engineering     science department     electrical  engineering          arizona state university          oregon state university             purdue university             tempe az                   corvallis             west lafayette           sungwookyoonasuedu                aferncsorstedu                givanpurdueedu                           abstract                          limit size training data occurs planning                                                         competitions policies capture use    current stateoftheart planners rely forward heuris ful imperfect constraints good courses action    tic search success search typically depends    heuristic distancetothegoal estimates derived main goal paper develop evaluate ap    plangraph estimates effective guiding search proach combining imperfect policies heuristics    domains remain domains order improve performance    current heuristics inadequate guide forward search ef techniques improve imperfect poli    fectively domains possible learn cies policy rollout bertsekas  tsitsiklis  lim    reactive policies example plans solve prob ited discrepancy search lds harvey  ginsberg     lems inductive nature learning representative techniques policy rollout uses    techniques policies faulty fail achieve online simulation determine each encountered state    high success rates work consider ef encountered action performs best    fectively integrate imperfect learned policies imperfect    heuristics order improve each propose follow learned base policy horizon    simple approach uses policy augment states policy rollout performs poorly base policy    expanded during each search step particular during each ﬂawed ﬁnd reward actions look equally attrac    search node expansion add neighbors tive occurs frequently goalbased domains    nodes trajectory followed policy policy rollout improve zerosuccessratio policy    node horizon empirical results show    proposed approach beneﬁts leveraged automated discrepancy search determines variable search horizon    techniques learning heuristic search outperforming counting number discrepancies base pol    stateoftheart benchmark planning domains icy searched path paths agree                                                         policy searched deeply search cost expo                                                         nential number discrepancies tolerated                    introduction                         result search prohibitively expensive unless base                                                         policy makes acceptableeffective choices  heuristic search successful dominant limitations quantity training data  approach suboptimal ai planning hoffmann  nebel  lack guarantee appropriate hypothesis space   alfonso gerevini  serina  vidal  machine learning produce low quality policies  success approach largely development policies impossible improve effectively  intelligent automated heuristic calculation techniques based policy rollout lds suggest using  relaxed plans rps plans ignore action effect learned policies during node expansions heuristic  delete lists rpbased heuristics provide effective gra search speciﬁcally propose adding neigh  dient search ai planning domains bors node expanded nodes oc  domains gradient provides inade cur trajectory given learned policy  quate guidance heuristic search planners fail scale current node policy makes bad decision  domains                                       nodes added search useful nodes    domains blocksworld                                                           contrast discrepancy search approach leverages  automated techniques martin  geffner  fern yoon                                                         heuristic function heavily contrast ordinary   givan  ﬁnd good policies through machine                                                         heuristic search approach ignore heuristic  learning enabling planners scale                                                         long trajectories suggested learned policy  duced policies lack deductive guarantees practice                                                         help planner critically escaping severe local minima  prone faulty—even resource constraints                                                         large plateaus heuristic function policy evalua  copyright c                                       tion typically cheaper heuristic function calculation  rights reserved                                   node expansion heuristic working                                                    ijcai                                                    approach faster                     each oi set speciﬁed corresponding cij —    tested proposed technique     ﬁring rule suggests action yoom ear  stripsadl domains international planning competi  liest rule list ﬁred current state  tions ipc   domains ﬁred action suggested rule  automated heuristic calculation effective pro tie broken lexicographically based arbitrary  duce essentially real distance used compe changing ordering domain objects  tition problems learning policies used rest syntax concepts following  problems testing approach note ap                  ∩   ¬     proach domainindependent empirical results show        anyobject                                                                                        ∗                  approach performed better using policies         pc  ci−    ci carityp  domains performed better stateoftheart plan  ners                                                  predicate symbol speciﬁed                                                         predicate symbol applied smaller class expres                      planning                           sions argument omitted indicated “”                                                         applications denote class objects make  deterministic planning domain deﬁnes set possible predicate true ﬁlled “” given ar                         actions set states terms set predicate guments provided match class expres  symbols  action types  objects each symbol sions given semantics constructs classes  σ deﬁned number arguments expects objects expected—please refer yoon fern   denoted arityσastates set state facts givan   mcallester  givan   state fact application predicate symbol arityp tailed speciﬁcation syntax semantics note  objects action application action syntax automatically derives predicate symbols  type arityy objects                     target planning domain    each action ∈ais associated three sets state evaluation semantics each class rela  facts preaaddaanddela representing precondi tive relational database constructed cur  tion add delete effects respectively usual action rent state goal information information  applicable state iff prea ⊆ applica deduced current state goal information  tion applicable action results new state case includes relaxed plan current state  ass  dela ∪ adda                            goal state form reasoning construct features geffner    given planning domain planning problem tuple  reﬂexive transitive closure p∗ each binary  gwherea ⊆ais set actions ∈sis initial predicate predicate symbols al  state set state facts representing goal lowed order represent goal information features  solution plan planning problem sequence actions relaxed plan described yoon fern   aal sequential application givan    sequence starting state leads goal state s                                                          example decision list policy consider simple  ⊆                                                 blocksworld problem goal clearing block                                                         following decisionlist policy optimal policy                 learning policies                       putdownxx   ∈ holding∗ unstackxx ∈                                                             ∗ ∩  ∗ ∗     reactive policy π planning domain function clear         ﬁrst rule says “putdown  maps each state action applicable block held” second rule says “unstack  state desire policy π iterative application block block clear”  π initial state problem ﬁnd                                                         learning reactive policies  goal ⊆ τπτπ τπs writing τπs  state π πs good reactive decision evaluation learn reactive policies solved  list policies represented learned ai small problems purpose deploy learner similar  planning domains khardon  martin  geffner  presented yoon fern  givan  given set  yoon fern  givan                         training problems solutions create train                                                         ing set learn classiﬁer taken learned  taxonomic decision list policies                       policy classiﬁer training set contains states labeled                                                         positive negative actions positive actions  represent reactive policy ordered list rules                                                         contained solution plan each state  rivest  each speciﬁes constraints ar                                                         actions state taken negative learn  guments action                                                         ing algorithm conducts beam search through candidate           dl   rulerulen                       class expressions greedily seeking constraints each ar          rulei  y  ∈ cixm ∈ cim            gument each action type match positive actions                                                         match negative actions note training set  arityy action type each cij noisy sense actions labeled negative  cept expression specifying set objects described actually bad optimalgood  low rule ﬁre tuple objects oom actions single state included                                                    ijcai                                                    training solutions noise reason learned resulting state policy rollout improve pol  policies imperfect                             icy optimal perform poorly                                                         policy commits errors leading inaccurate action            using nonoptimal policies                   costs multilevel policy rollout used xiang yan                                                          van roy  way improve  typically learned policies perfect bias procedure recursively applying rollout takes time  policy language variance learning proce exponential number recursive applications  dure domains imperfections catas approach work policy errors typically  trophic policies obtain high success rates restricted initial steps trajectories  domains ﬂaws lead extremely poor success policies learned planning domains distribution er  rates states learned policies rors does typically form multistep  poor success rates suggest good actions rollout ineffective improving weak policies  like methods exploit information effectively  existing techniques rollout dis  crepancy search utilize search improve im discrepancy search  perfect policy experiments show tech  niques work planning domains typi discrepancy search search step selected  cally quality learned policy low input policy heuristic function limited discrep  failures led propose new approach improving poli ancy search lds harvey  ginsberg  bounds  cies search discussed end section search depth given number discrepancies dis                                                         crepancy search limited disrepancies consider ev  policy rollout                                         ery search path deviates policy times                                                        select min                                                                   ai                                                                                                                                                                                                                                 follow policy                                                                                                                                                                                                   si     sn                                            length ai  length  length                                                    figure  example discrepancy search edges             heuristics   heuristicsi   heuristicsn                                                          ﬁgure represent discrepancies nodes labeled                                                         discrepancy depth  figure  policy rollout state each action roll  input policy π state horizon  deterministic domains length rollout trajec figure  shows example discrepancy search  tory plus heuristic end used qvalue lines choices favored heuristic function  action select best action               lines discrepancies each node shown                                                         rectangle labeled number discrepancies needed    policy rollout bertsekas  tsitsiklis  technique reach node consider depth goal search  improving performance nonoptimal “base” pol tree number discrepancies needed reach  icy using simulation policy rollout sequentially selects goal node root node ﬁgure  goal node  action best according onestep lookahead depth three discrepancy depth root node  policy evaluations base policy figure  shows plain dfs bfs search search nodes limit  onestep lookahead action selection each action disrepancy search dfs bfs need visit  nodes  procedure simulates action current state reaching goal discrepancy search limit  simulates execution base policy ﬁnd goal node search greedy heuristic  sulting state horizon goal each search tree needs backtrack times  action assigned cost equal length trajectory reaches goal node search space lds size  following plus value heuristic applied ﬁnal exponential discrepancy bound like policy  state zero goal states stochastic domains rollout lds effective policies heuristic  rollout tried times average functions high quality heuristics incorporated  rollout trials used qvalue estimate policy roll applying heuristic leaves discrepancy  executes action achieved smallest cost search tree selecting action lead best  current state repeats entire process heuristic value                                                    ijcai                                                    incorporating policies heuristic search           report number solved problems average solu  work consider alternative approach tion time solved problems average length  corporating imperfect policies search attempt solved problems separate columns results  advantage approaches automated heuristic cal used ﬁrst  problems training data each  culation automated policy learning main idea main remaining problems testing considered  use policies during node expansions bestﬁrst heuristic problem unsolved solved  min  search described node expansion function shown utes pr ph systems used horizon   figure  each node expansion bestﬁrst search experiments run linux machines   add search queue successors current best xeon processor gb ram  node usual add states encountered follow  ing policy current best node horizon blocksworld  approach similar marvin coles  smith  figure  shows results blocksworld track   macroff botea et al  yahsp vidal    ipc  domain len ph solved prob  unlike approaches just add ﬁ lems ff pr failed solve problems  nal state encountered policy macro add used  cpu secs learning policy learned policy  states trajectory                       πblocksworld solved  problems policy im    embedding policy node expansion during heuris provement approaches pr ph improved input  tic search yields primary advantages pure heuris policy ph solved problems im  tic search input policy correlates proved solution time len pr showing  heuristic values embedding reduce beneﬁt approach speedingup planning  search time typically direct policy calculation ph produced slightly longer plans ff  faster greedy heuristic search greedy heuristic  search needs compute heuristic value neigh               blocksworld ipc                                                                                 ↑      ↓       ↓  bor direct policy execution considers current    systems solved  time length  state selecting actions second like blocksworld       ff                   heuristic calculation frequently underestimates true dis     len                                                                                   pr                   tance node expansion lead heuristic search                           local minima                                                  ph                        nodeexpansionwithpolicy π     problem policy π horizon                            figure  blocksworld results       ←  neighborss                                                        ipc     ←                                     figure  summarizes results ipc domains        s ← πs                                       domains signiﬁcantly difﬁcult learn policies         ←  ∪s                                    used cpu time learning poli    return                                             cies depots driverlog freecell respectively al                                                         learning times substantial onetime                                                         cost learned policies resused problem  figure  node expansion heuristic search pol instance domain involved domains  icy add nodes occur trajectory input learned policies solve tested problems  policy neighbors                                                                                  pr ph able improve                                                         input policies ph solved problems overall                                                         ff ff slight advantage solution length                    experiments                          depots domain freecell domain deadlock states  evaluated approaches strips domains policy performs poorly probably leading  recent international planning competitions ipc deadlock states experiments show  ipc blocksworld show performance  approach attained better solution times demonstrating  each planning each row following ﬁg using policies search reduce search time  ures test effectiveness approach tested quickly ﬁnding lowheuristic states  base len proposed technique ph outperforms len decisively depots driver  ph len uses bestﬁrst search relaxed log showing clear beneﬁt using imperfect policies  plan–length rpl heuristic compare heuristic search domains  planner ff each ipc domain best  planner competition particular domain finally ipc  compare policy rollout pr limited discrep figure  shows results ipc domains used  ancy search techniques each domain      hours cpu time learning poli                                                    ijcai                                                                           ipc                                                   ipc        domain  systems solved ↑ time ↓ length ↓            domain   systems solved  ↑ time ↓ length ↓                  ff                                            ff                                 len                                            len                       depots   pr                                   pipesworld                                                                               pr                                ph                                                                              ff                                           ph                                len                                           ff                        driverlog pr                                                  len                                                                        pipesworld                                 ph                                 tankage   pr                                  ff                                                                          len                                         ph                       freecell pr                                          ff                                                                  psr      len                               ph                                middle                                                                                complied  pr                                 figure  ipc results                                                                                                                           ph                                                                                        ff                                                                                               len                       cies pipesworld pipesworldtankage psr philosopher  philosophers                   optical telegraph respectively evaluated              pr                   each domain denotes best                                domain ipc numbers                   ph                   downloaded ipc web site di                ff                        rectly compared systems results numbers             len                       idea performance comparison note      optical                    learned policies solve problems  telegraph  pr                  philosopher optical telegraph domains                                             ph generally outperforms ff solved problems render              ph                  ing solution length measures incomparable psr                                                                         figure  ipc results  approaches essentially tie    pipesworld pipesworldwithtankage domains  best performers domains competition  formed better ph ph figures   show heuristic traces len  performed better len ff pr showing ben ph freecell problem ph failed ph  eﬁts approach marvin macroff yahsp partic unable escape plateau making small jumps  ipated competition each macro action based heuristic value node expansions interestingly  planners solved  problems best planners trace len shows plateau higher heuris  each domain combined solved  problems ph solved  tic value ph followed rapid decrease  problems showing clear advantage technique heuristic value problem solved    overall results show typically able currently investigating reasons behavior  effectively combine learned imperfect policies point speculate learned policy manages lead  relaxedplan heuristic order improve each planner away useful exit  novel result policy rollout dis  crepancy search effective regard               conclusion  aware prior work successfully integrated im  perfect policies heuristic search planners       embedded learned policies heuristic search follow                                                         ing policy during node expansion generate trajectory  heuristic value trace search                        new child nodes empirical study indicates advan                                                         tages technique conjecture three  order view incorporating policies im sources policy correlates heuris  prove heuristic search plotted trace heuristic tic function embedding speed search sec  value each node expansion during search problem ondly embedding help escape local minima during   depots figure  shows long plateau heuristic val search finally heuristic search repair faulty ac  ues len figure  shows large jumps tion choices input policy approach easy  heuristic value ph using far fewer node ex implement effective knowledge represents  pansions indicating policies effectively helping ﬁrst demonstration effective incorporation imper  escape plateaus search space                fect policies heuristic search planning                                                    ijcai                                                    
