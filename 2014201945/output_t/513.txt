                    concurrent      hierarchical     reinforcement      learning        bhaskara   marthi  stuart russell david  latham             carlos  guestrin                    science division                 school   science                    university california                   carnegie mellon  university                      berkeley ca                           pittsburgh pa           bhaskararusselllathamcsberkeleyedu               guestrincscmuedu                        abstract       consider applying hierarchical reinforcement    forest      learning techniques problems agent                                         gold      effectors control simultaneously      argue kind prior knowledge typically                                                                                                peasants      problems best expressed using      multithreaded partial program present   base      current alisp language specifying par      tial programs algorithms learning      acting concurrent alisp efﬁ      cient exponentially joint      choices each decision point finally show figure  resourcegathering subgame stratagus peasants                                                        step directions remain stationary      sults applying methods complex com transitions deterministic pick wood gold      puter game domain                                forest goldmine drop resources base                                                        reward  received unit resource brought    introduction                                       base cost  paid timestep additional                                                        cost  paid peasants collide game ends  hierarchical reinforcement learning hrl emerging player’s reserves gold wood reach predeﬁned threshold  subdiscipline reinforcement learning methods  augmented prior knowledge highlevel struc  ture behaviour various formalisms expressing thread difﬁcult different peasants involved  prior knowledge exist including hams parr russell different activities different stages   maxq  dietterich  options precup sut activities thread complex bookkeeping  ton  alisp andre russell  idea essentially implements multiple control stacks second                                                                                prior knowledge enormously accelerate peasants  possible joint actions action  search good policies representing hierarchical selection combinatorially nontrivial exact value  structure behaviour methods expose ad functions given finally interactions  ditive structure value function dietterich  peasants’ actions—they collide each    hrl formalisms viewed express compete access mines forests  ing constraints behavior single agent sin handle issues introduce concurrent al  gle thread control example alisp extends lisp isp language hrl multieffector problems  language nondeterministic constructs create single language extends alisp allow multithreaded partial pro  threaded partial programming language section  ex grams each thread corresponds “task” threads  perience programming languages robotics suggests created destroyed time new tasks ini  behaviour complex physical agents tiated terminated each point time each effector  real domains best described terms concurrent activi controlled thread mapping change  ties agents effectors total time prior knowledge coordination threads  action space available agent cartesian product included partial program  action spaces each effector                  threads coordinate choices runtime maxi    consider example resource domain shown fig mize global utility function  ure  subproblem larger stratagus begin brieﬂy describing alisp section   main stratagussourceforgenet each peasant theoretical foundations semimarkov decision processes  source domain viewed effector multi smdps section  deﬁnes syntax concurrent al  body agent controlling peasants single control isp semantics—ie combination concurrent alisp program physical environment deﬁnes                                                         defun  singlepeasanttop     smdp obtain result analogous alisp op loop  timal solution smdp optimal policy orig choose  inal environment consistent partial program ’call  getgold  getwood  section  describes methods used handle  large smdps face use linear function approxi defun getwood   mators combined relational feature templates guestrin nav choose forests  et al  action selection use coordination action ’getwood  graphs guestrin et al  deﬁne suitable  nav homebaseloc  smdp  qlearning method section  show experi   action  ’dropoff  mentally suitable concurrent alisp program allows                                                         defun  getgold    agent control large number peasants resource nav choose goldmines  gathering domain larger stratagus action ’getgold  subgame complex concurrent hierarchical ac nav homebaseloc  tivities learned effectively finally section  show action ’dropoff  additive reward decomposition results russell  zimdars  used recover threepart defun nav  decomposition results singlethreaded alisp loop atpos   complex concurrent setting                           action  choose  ’n  rest      background                                         figure  alisp program resource domain single peasant  alisp andre russell  language writing  partial programs superset common lisp alisp  programs use standard lisp constructs defun multipeasanttop                                                            loop   including loops arrays hash tables alisp adds loop myeffectors  following new operations                                     choose  ’    • choose   choices picks forms     setf   myeffectors      list choices executes choices does choose                                                       spawn  ’getgold   list     • action   performs action environment        spawn  ’getwood   list     •  args calls subroutine argument list      args                                             figure  new level concurrent alisp program resource    • getstate   returns current environment state domain number peasants rest program    figure  simple alisp program resource identical figure   main figure  singlepeasant case agent ex  ecutes program acts environment let ω  incorporate prior knowledge developed  θ joint state consists environment concurrent extension alisp allows multithreaded par  state machine state θ turn consists tial programs multiplepeasant resource domain  program counter stack global memory concurrent alisp partial program identical  partial program partial program reaches choice figure  toplevel function replaced  state joint state program counter version figure  new level waits unassigned  choose  statement agent pick choices peasant chooses gold wood  execute learning task ﬁnd optimal choice spawns thread chosen task peasant  each choice point function ω formally al complete assigned task thread die  isp partial program coupled environment results sent level reassignment overall  semimarkov decision process markov decision process program longer earlier alisp program  actions random time joint doesn’t mention coordination peasants  choice states ﬁnding optimal policy smdp theless executing partial program peasants  equivalent ﬁnding optimal completion partial automatically coordinate decisions qfunction  program original mdp andre             refer joint choices peasants example                                                        learn collide each navigat    concurrent    alisp                                ing joint decisions efﬁciently despite  consider resource domain peas exponentially large number joint choices  ant suppose prior knowledge want incorpo happens  rate each individual peasant behaves single  peasant case picks resource location navi  syntax  gates location gets resource returns base concurrent alisp includes standard lisp choose  drops repeats process             statements syntax alispthreads effectors referred using unique ids allegro lisp uses scheduler seman  point during execution set threads tics seen independent choice  each thread assigned possibly subset scheduler long scheduler fair nonholding  effectors action statement looks like thread eventually chosen let ψ thread chosen  action      en means each effector scheduler statement ψ does use  ei ai special case thread exactly concurrent alisp operations effect stan  effector assigned ei omitted legal dard lisp statement does function  program singlethreaded alisp legal concur dates stack spawn reassign myeffectors  rent alisp                                           wait  notify  operations work described sec    following operations deal threads effectors tion  executing statement increment ψ’s    • spawn   threadid fn args effectorlist creates program counter reached end func      new thread given id starts calling fn tion pop stack repeatedly      arguments args given effectors    case stack initial function    • reassign   effectorlist threadid reassigns thread terminated ψ removed Ψ effectors      given effectors currently assigned reassigned thread spawned      calling thread threadid                       example suppose partial program figure     • myeffectors     returns set effectors ing run three peasants corresponding threads ψ      signed calling thread                     ψ ψ let ψ initial thread consider sit                                                        uation ψ dummy choose statement ψ    interthread communication through condition vari                                                        line getwood ψ ﬁrst  ables using following operations                                                         line getgold ψ line getgold    • wait  cvname  waits given condition vari internal state set nonholding threads      able created doesn’t exist      ψ ψ suppose scheduler picks ψ state    • notify   cvname  wakes threads waiting ment executed stack appropriately updated      condition variable                          ψ nav subroutine    concurrent alisp program contain desig second case known choice state  nated level function execution begin threads holding exists thread ef  domains set effectors changes time exam fectors assigned action statement  ple stratagus existing units destroyed new agent pick joint choice choice threads  ones trained program domain include given environment machine state program coun  function assigneffectors   called ters choice threads updated based joint  beginning each timestep assign new effectors threads choice choices agent case                                                        viewed completion partial program formally    semantics                                        completion mapping choice states joint choices  state space initial state transition example suppose ψ dummy choose state  function obtain running concurrent alisp pro ment ψ ψ choose nav  gram environment used construct ψ dropoff action getwood choice  smdp analogous alisp case actions smdp state choice threads ψ ψ ψ suppose agent  correspond joint choices partial program learnt completion partial program makes    joint state space Ω  ω  θ choose  rest program counter ψ  environment state θ machine state machine state loop program counters  consists global memory state µ list Ψ threads ψ ψ action statement nav  each ψ ∈ Ψ unique identiﬁer ι set effectors case known action state  assigned program counter ρ stack σ threads holding thread effectors  say thread holding choose action signed action statement  wait statement thread called choice thread joint action determined action performed  choose statement note statements nested environment action threads stepped  example line figure  thread ward effectors added new envi  execute choose case choice ronment state assignneweffectors function  thread ﬁnished making choice called decide threads assign contin  execute action case uing left example suppose ψ executes    initial machine state global memory myeffectors statement gets choose  single thread starting level function statement action state joint action  effectors assigned                     dropoff rest  environment ψ    three cases transition distribution ψ return loop ψ die  ﬁrst case known internal state thread releasing effector  holding need pick nonholding thread execute  assume external scheduler makes choice threads program deadlocked  choice example current implementation built leave programmer write programs avoid deadlock  let Γ partial program scheduler consider choices result collision peasants  following random process given choice state ω   joint choice repeatedly step partial program forward peasants  collision fea  described reaching choice state ω tures each corresponding weight learn intuitively  let number joint actions happen doing collision peasants effect  ω random variables environment guestrin et al  proposed using rela                         stochastic let pΓsω  nω joint distribution tional valuefunction approximation weights  given ω                                        features tied single value                                                  say Γ good pΓs fair wcoll mathematically equivalent having single  case deﬁne corresponding smdp Ωc set “feature template” sum individual colli  choice states set “actions” possible smdp sion features keeping features separate exposes  ω set joint choices ω transition distribution structure qfunction critical efﬁcient  pΓs fair reward function rω ex execution shown section  pected discounted reward received choice state  theorem shows acting smdp equiva  choice selection  lent following partial program original mdp suppose partial program set features                                                        optimal set weights  theorem  given good partial program Γ bijec                                                        run partial program reach choice  tive correspondence completions Γ policies                                                        state ω need pick maximizing qω  smdp preserves value function particu                                                        multieffector case maximization straightforward  lar optimal completion Γ corresponds optimal                                                        example resource domain peasants  policy smdp                                                        navigation choices joint choices    design decisions implicit se advantage using linear approximation  mantics threads correspond tasks function maximization efﬁ  multiple effectors assigned helps incor ciently reach choice state ω form coordi  porating prior knowledge tasks require multiple nation graph guestrin et al  graph contain  effectors work allows “coordina ing node each choosing thread ω feature  tion threads” don’t directly control effectors sec clique graph choosing threads  ond threads wait each action statements depends maximizing joint choice  effectors act simultaneously prevents joint time exponential treewidth graph using  haviour depending speed execution cost network dynamic programming dechter   different threads choices jointly naive implementation treewidth coordina  sequentially each thread’s choice depending tion graph large example resource  threads chose based intuition domain collision feature each pair peasants  qfunction joint choice qu     treewidth equal typical situa  easier represent learn set qfunctions tion pairs peasants far apart  qu quu     qnunu    un− chance colliding make use kind “context  making joint choices presents computational difﬁculties speciﬁcity” implement feature template function  address following section       takes joint state ω returns component                                                        features active ω—the inactive features equal    implementation                                      regardless value example collision  section function approximation ar feature template fcollω return collision feature  chitecture algorithms making joint choices learn each pair peasants sufﬁciently close each  ing qfunction use linear function approximation chance colliding step  turns crucial efﬁciency algorithms signiﬁcantly reduces treewidth    linear function approximation                      learning                                                        thanks theorem  learning optimal completion  represent smdp policy partial program implic                                                        partial program equivalent learning qfunction  itly using qfunction ω represents total                                                      smdp  use qlearning algorithm run  expected discounted reward taking joint choice ω                                                        partial program environment reach  acting optimally use linear approximation                                                      choice state pick joint choice according glie ex  qˆω   wkfkω each fk feature              pk                                        ploration policy track accumulated reward  maps ω pairs real numbers resource number environment steps place  main feature fgoldω returns each pair choice states results stream samples  gold reserves state ω form ω ω maintain running estimate  set features fcollij ω returns  navigation receiving sample perform update         analogous requirement standard multi ← wα  γn max qω − qω fω  threaded program contain race conditions                                                                                                                            −                                                                 −        −            −                                                               −          −                                                                 −                                                              total  reward policy          total  reward policy −            −                                                               −          −                                   concurrent hierarchical q−learning                     concurrent hierarchical q−learning                                 flat concurrent q−learning                             flat concurrent q−learning        −                                                   −                                                                                         steps learning                            steps learning    figure  learning curves resource domain  peas figure  learning curves resource domain   ants goal collect  resources curves averaged peasants goal collect  resources shap   learning runs policies evaluated running  ing function section  used  steps termination shaping used                                                                               shared variables    shaping                                                               resource allocation                                                           allocate gold    potentialbased shaping ng et al  way modi allocate barracks peasants train peasants town hall  fying reward function mdp speed learning footmen  affecting ﬁnal learned policy given potential                 root  function Φ state space use new reward func                                                         train footmen barracks allocate peasants  tion r˜s   rs   γΦs  − Φs pointed                                      new                                                                                  send idle peasants gather peasants  andre russell  shaping extends naturally                    gold build barracks  hierarchical rl potential function depend     tactical decision                                                          new   machine state partial program footmen decide use                                                                     new footmen attack                                                                                  build barracks gather gold   source domain example let Φω sum                     peasant peasant    distances each peasant current navigation goal attack footmen                                                                                           legend  term depending gold wood      controls set footmen                                                                 jointly attacking thread spawns  gathered                                    enemy                 effector    experiments                                                         figure  structure partial program strategic domain   learning experiments ﬁrst  source domain complex strategic domain  details domains partial programs  function approximation architectures used presented makes discovery reaches nearoptimal policy  forthcoming technical report                      test world  peasants                                                        learning curves shown figure  large map    running  example                                 size used shaping function section  hi  begin peasant resource domain erarchical learning ﬂat learning shaping function  world quite small  states  used destination peasant  joint actions each state tabular learning infeasi environment state write shaping function  ble compared ﬂat coordinated qlearning guestrin et programmer ﬁgure advance  al  hierarchical qlearning algorithm sec destination each peasant allocated function  tion  figure  shows learning curves ﬁrst environment state similar reasons difﬁcult   steps hierarchical learning usually reaches “reason ﬁnd good set features function approximation  able” policy moves peasants current ﬂat case able ﬂat learning learn  goal avoiding collisions remaining gains kind reasonable policy hierarchical learning learnt  improved allocation peasants mines forests reasonable policy sense deﬁned earlier ﬁnal learnt  minimize congestion “human expert” domain allocation peasants resource areas optimal  average total reward − believe learnt average reward − human ex  hierarchical policy nearoptimal despite constraints pert able total reward − working  partial program  steps ﬂat learning usually implement techniques section  believe  learns avoid collisions learnt pick speed convergence situations large num  ing resources good idea  steps ber effectors
