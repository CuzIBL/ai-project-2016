                  reinforcement learning pomdps resets               eyal evendar                   sham kakade                    yishay mansour      school science     information science      school science           telaviv university           university pennsylvania           telaviv university         telaviv israel            philadelphia pa              telaviv israel           evendposttauacil          skakadelinccisupennedu          mansourposttauacil                        abstract                          contain asymptotic results general pomdps                                                        guarantee average reward agent near      consider realistic reinforcement learn optimal limit      ing setting agent starts unknown      environment pomdp follow         technical difﬁculty currently      continuous uninterrupted chain experience  general results belief state tracking approximate      access “resets” “ofﬂine” simula    model showing divergence belief state does      tion provide algorithms general connected eventually occur crudely issue belief state      pomdps obtain near optimal average reward   tracked approximate manner impor      algorithm present convergence rate   tant show approximation quality does contin      depends exponentially certain horizon  ually degrade time — agent eventu      time optimal policy dependence  ally loose track belief state inﬁnite horizon      number unobservable states main  course issue mdp current state      building block algorithms implemen  observable boyen koller  address issue      tation approximate reset strategy approximate belief state tracking setting      show exists pomdp   model known perfectly goal compact      ing aspect algorithms use representation belief state note approximate      strategy balancing exploration exploita lief state tracking simpler agent acting      tion                                             ﬁxed ﬁnite horizon bound error                                                        accumulation function horizon                                                          present new algorithms learning pomdps    introduction                                       guarantee agent obtain optimal average  address problem lifelong learning partially ob ward limit furthermore provide ﬁnite time  servable markov decision process pomdp consider vergence rates algorithms expo  general setting agent begins unknown nential dependence certain horizon time optimal  pomdp desires obtain near optimal reward strategy dependence number states  setting agent forced obey dynamics envi pomdp result reminiscent trajectory tree  ronment general permit resets     algorithm kearns et al  similar depen    problem lifelong learning studied dencies assumed access generative  observable mdps kearns singh  provide model allowed simulation pomdp given  algorithm ﬁnite polynomial time guarantees plethora complexity results literature planning  agent obtains near optimal reward unfortunately pomdps lusena et al  feel depen  algorithm applicable challenging dencies best hope general  pomdp setting fact guarantees litera setting  ture learning limit mdps apply pomdps central algorithms implementation ap  reasons essentially partially observabil proximate reset strategy homing strategy idea  ity                                                  reset strategy new literature — homing sequences    pomdps problem balancing exploitation used learning deterministic ﬁnite automata  exploration received little attention literature rivest schapire  sequence pro  — typically results pomdps planning vided exact resets agent follows homing strat  example sondik  lovejoy hauskrecht egy order approximately reset show   cassandra  existing learning strategy exists ﬁnite convergence  algorithms parr russell  peshkin et al rates depend characteristic time takes approx   assume goal state assume reset button imately reset note existence strat  fact best knowledge literature does egy does imply strategy usefulthe reason agent actions reset  homing strategies  better spent exploring exploiting  turns algorithms use homing strategy clearly having action resets agent des  exploring exploiting fact use homing ignated state useful allow test  strategies inﬁnitely unfortunately detracts compare performance various policies starting  exploiting able show ratio start state general action  time homing strategies used compared time disposal  spent exploiting decreasing sufﬁciently rapidly instead algorithms utilize approximate reset  near optimal average reward obtained          show exists subtle points                                                        designing reset select actions    preliminaries                                      achieve approximate reset approximate reset  partially observable markov decision process pomdp through use homing strategy                                                        homing agent exploring exploiting sec  deﬁned ﬁnite set states initial state set  actions set observations output model ond moving ﬁxed state homing strategy  qo rs probability observing hope ﬁxed unknown belief                                                        state shall pomdp  reward performing action state assume                                         ∈   set transitions probabilities  periodic stopping time random variable   sa transition probability implement randomized stopping time introduce ﬁc  forming action deﬁne rs expected reward titious ’stay’ actions agent does ac  q·s performing action state tion period mean homing strategy     history   sequence  actions rewards   decides ’stay’ action time —  observations ﬁnite length        possible true pomdp does permit ’stay’ actions                                                        —  agent just ignores ’stay’ action obtains   rt ot strategy policy  pomdp    deﬁned mapping  histories  action homing strategy execute  actions deﬁne  belief state distribu agent taken homing actions  tion states given initial belief state let real ’stay’ actions agent taken − real actions                                                       pomdp stay actions deﬁne approx  prhb     prr     rt ota      probability observing sequence rewardobservations imate reset strategy     rt ot performing actions       each  strategy π   deﬁne thorizon ex   deﬁnition   kapproximate reset homing                                            π           strategy belief states  pected reward  belief state rt               pt                                          kheb  − hebk  ≤   heb expected  teh∼π  rsi aib  tmarkov strategy  strategy depends observations belief state reached homing actions  optimal tmarkov strategy’s expected return initial real actions taken hb random                        ∗                               variable heb  ehb∼hbhb  lief state deﬁned rt    assumption make pomdp                                                          deﬁnition states approximately  nected states exists strategy π                                                        reset approximation quality poor  reach positive probability starting                                                        show amplify accuracy approximate hom  make ergodicity assumptions strategies                                                        ing strategy show approximate homing  deﬁnition nonstationary note pomdp discon                                                        strategy exists  nected best statement hope obtain  optimal average reward connected compo                                                        lemma   suppose  approximate reset  nents                                                                                             connectivity implies exists strategy π∗   approximate reset consec  maximizes average reward formally exists utively implements  times furthermore implies     ∗                                π∗                exists unique belief state bh hebh    π limt→∞ rt exists  does depend denote r∗ ii bh             ∗                π  π  ≥ limt→∞  sup rt     exist τ ≥ τ         proof proof standard contraction argument                                                        use induction   claim follows deﬁnition                     ∗    π∗                                              −         −          −                   − rt  ≤                     assume khe  −   bk ≤     let                                                          l−                 l−                                                                              qs −  refer τ horizon time optimal strategy −                      essentially τ timescale optimal strategy qs ≤   arbitrary state  using fact  achieves close average reward    say restart tmarkov strategy π randomizing stopping times allowing  belief state mean speciﬁcally reset ’stay’ actions state transition matrix periodic  tory  ∅ run π starting state distributed stationary distribution exist states deterministically  according                                       alternate states  linear operator                                                          input    kh  approximate reset strategy      kh  −                                   ∞                                                      exploration phase           kh   −                                                                                                                                       logt Πt                                                                     t            qs − qshesk                                                             foreach policy π Πt                                                                                                                                                                                      qs − qshes                              run π steps                                                                  repeatedly run logt times                           − sh − sk               end                                                         let vπ average return π                                                                                                                             trials               qs − qs                           end                                                           exploitation phase                                                                  ∗               π       ≤                                                    let πˆt  arg maxπ∈Πt                                                                                                                                  kt    current time  ﬁrst term  distributions          t                                                                                                      qs − qs   −      vector hs               time  th exploration phase   constant sum used fact                khs − hsk ≤                                                            deﬁnition                               ∗     show  random walk strategy includ        run πˆt steps  ing ’stay’ actions approximate reset strategy repeatedly run logt times  pomdp including periodic ones prior knowl  end                                                          end  edge better approximate reset strategies  disposal                                                         algorithm  policy search  lemma   pomdps random walk strategy                                                                                             ∗  ing ’stay’ actions constitutes  approximate reset homing sequence run πˆt  asymptotically                                   strategy ≥                      stop homing nonetheless able show                                                        exists algorithm obtains near optimal reward    proof connectivity assumption states                                                        pomdp ratio time spent exploiting vs  exists strategy reaches pos                                                        homing decreases sufﬁciently fast  itive probability implies random walk  strategy positive probability moving theorem  exists algorithm  state markov chain irreducible fur connected pomdp obtains optimal average reward  thermore performs ’stay’ actions markov limit probability   chain aperiodic exists unique stationary dis later provide algorithm better convergence  tribution choose time error rate theorem  start simpler pol  convergence  starting states icy search algorithm establishes theorem  linearity expectation error   belief states steps                              policy search                                                          algorithm  takes input  kh approximate reset    reinforcement learning homing                 strategy random walk strategy   provide algorithms demonstrate  crude reset algorithm works phases interleav  nearoptimal average reward obtained different ing exploration phases exploitation phases let start  rates convergence key success algo describing exploration phases let Πt set  rithms use homing sequences exploration tmarkov strategies estimate value policy  exploitation exploration idea each π ∈ Πt ﬁrst resetting running π  time attempt exploration trajectory steps exploration phase consists obtaining estimate                                                         π  implementing reset strategy — information return each policy π ∈ Πt each estimate  approximately grounded respect belief state bh consists average trials followed approximate  recall hebh   bh  idea exploration ﬁnd resets                         ∗  good tmarkov strategy πˆt bh  during exploitation estimates bias variance variance  goal use tmarkov strategy unfortunately just stochastic nature pomdp bias                         ∗  guaranteed πˆt performs starting bh fact exactly reset bh   steps each time exploit run log t times t error pa   ∗  πˆt  run homing sequence close bh rameter tth phase ﬁxed               ∗  rerun πˆt  gradually increase process lemma  expected belief states approach                                                                log t    problem homing wasting time   t close bh  following lemma  exploiting exploring furthermore use shows accurate estimates obtained                                                                                            previous time spent mdp time plus  lemma   phase   logt Πt each                                                      time spent exploration phase  reset consists using homing sequence log  times                                                      factor accounts case time lies  policies π ∈ Πt estimated thorizon reward   π           π        π                               exploration phase immediately   satisﬁes rt bh  −  ≤ t probability greater   −                                              bound average reward obtained exploita                                                    tion phase let show taverage reward                                                                     ∗                     ∗    proof let deal bias expected belief    πˆt         ∗         πˆt                                                        policy used rt  satisﬁes rt bh  − rt bh  ≤ t  states results using homing sequence log t                                                                      probability  −   lemma  each policy                                  log t                                    times satisfy kb − bh ≤   ≤ t                  π         π                                                        π ∈ Πt rt bh  −  ≤ t probability  straightforward bh belief states                ∗                                                         −      πˆt toptimal probabil  kb −  ≤  strategy π rπb −       Πtt                                                       −                             πˆ∗  rπb   ≤  let belief states time bt ity  observed average return                                                                              r∗b    bt  result following π starting exploitation period close probability                                                      −   observed average return exploitation   respectively linearity expectation follows                                                                                      πˆ∗                                                   π        good used ﬁnd    kb − bh ≤  directly implies rt −                                             π  rt bh  ≤                                           average return during exploitation phase                                                                                           ∗    variance hoeffding bound choice observed average return πˆt   imply average return each policy t close set each exploitation steps number steps  expectation expectation initial state kh logt resets exploitation pe  policy trajectory probability  −  riod change average reward fraction                                                   ∗                                                        during exploitation algorithm uses policy πˆt kh logt  highest return exploration phase  previous lemma close policy largest  model based algorithm  turn note guaranteed large return                                                        previous algorithm simplest way demonstrate  executing πˆ∗ steps like                                                    theorem  inefﬁcient testing  exploit longer period time key                                                        tmarkov policies — doubly exponential  reset log  times each time run πˆ∗                                                    polices provide efﬁcient model based  resets close t close bh  unfortunately means                                         ∗              algorithm resembles algorithms given kearns  spend kh log t steps each run πˆ                                                        et al  mcallester singh  exponen  average return kh log t                                                       tial horizon time dependence  like kh log t fraction time spend number states pomdp  resetting note fraction large desire state convergence rate terms τ   t small thought guarantee ac horizon time optimal policy section   curate resets                                       terms homing time recall time exists    exploit reset run exploita                                                                           pomdp using random walk policy  tion phase long time overall  average reward comparable average reward theorem  exists algorithm  exploitation phase                              connected pomdp probability greater  − δ  lemma   time phase average reward achieves average reward  close optimal  time  time satisﬁes  pt ≥ r∗b   −    average reward number steps pomdp                                            polynomial aok logδ exponential  o    log                      −                                            probability    τ furthermore computational runtime algorithm    proving lemma let state corollary polynomial ao logδ exponential τ  theorem  follows                                                          provide algorithm page explo                                pt          ∗  corollary  let t  ri ≥ rt bh  − ration phase algorithm builds approximate model    kh logt                                          transition probabilities history occurred        probability  −                                                         starting bh  tth phase builds model    importantly note loss term goes  goes inﬁn respect set tlength histories denote  ity furthermore large phase know   ∗                                          ∗         ht exploitation phase uses best markov strat  rt bh  approach optimal average reward   ∗                                                    egy respect model use homing strategies   independent starting theorem  follows similar previous algorithm  essentially home inﬁnitely ra let  ao note lt ≥  ex  tio time spent homing time spent using tstep                                                                log                    ploration phase algorithm takes actions uniformly ran  exploitation policies going  goes  dom steps resets running homing strategy    proof  let show average reward                  ∗                                              ri ≥ rt bh  t average number histories length exponential  reward obtained tth exploitation phase number tmarkov polices exponential number tlength                                set time exploitation phase t times greater histories                                                      prˆ oh    input   kh  approximate reset strategy     let  ·                                                                                                                         prha ob  prˆ ha ob        ∞                                                −                                                                                           prhb prˆ prˆ hb prˆ             logt ht                                                                      t                                                                                  t                        times                                              prha ob  lt prha ob                                                          ≤                            −                         run random  steps                                               t                                                              prˆ   prhb −      prhb                       loglt                                                          run            steps                                   t                          t                                                                            lt            prha ob lt         end                                                                                                                                                               ˆ               t                       t       ∈ ht ∈ ∈                         pra  prhb − lt prhbprhb − lt             ˆ          proh                                               ˆ              t                            ≤                 prha ob ≥ lt                          lt                            ˆ              ˆ             prhaob             proh  ˆ   ˆ                                                                                              prhbpra               ﬁrst inequality holds probability  −                                                                                                       t           end                                           inequality used fact prhb ≥ lt        end                                                exploitation policy using dynamic pro                ∗      ˆ       compute πˆt using prob                gramming model note pomdp equiva                      current time                       lent mdp histories states exploita               t                                                                                        ∗                                                       tion phase algorithm uses best tmarkov policy πˆt               time  th exploration phase   respect approximate model interleaving                                                      kh  logt homing steps       times          run πˆ∗ steps                                                                    ∗                                                      lemma   phase exploitation policy πˆt  satisﬁes                                                                     ∗          run kh logt steps                   ∗         πˆt                       t                                                        rt bh  − rt bh  ≤ tt  lt   t  lt        end                                                                      end                                                 probability  −               algorithm  model based                    proof sketch observe ignoring histories                                                                                              ˆ         t                                                        view nodes tree prhb ≤ lt                                                         return optimal strategy empirical model                                                                     t  t   logl t times times using decreased lt  fact true                                                                                    t    t  empirical frequencies trajectories algorithm history probability bounded lt  lt  return                ˆ                                       each node bounded total number nodes  forms estimates proh bh  just empirical          probability observing conditioned history followed bounded  prove return opti                                                                                                       taking action histories unlikely mal policy empirical model loses t  lt   empirical estimates bad shall tree approximation nodes                                                        histories using backward induction show pol  need accurate estimates prˆ oh bh                                                             ∗                           histories let ha history followed icy πˆt return t  lt  comparison                                                true optimal value starting −  length                                                        tories base case leaves tlength histories                                                        holds reward encoded through obser                                                                                                                                                                        vations t  lt  ﬁrst error  lemma   phase    logt ht                                   t                   imperfect reset second marginal distribu                                                 each reset consists using homing sequence logl t                                                                          tion error bounded lt lemma  assume                 ˆ                     t  times  prhb − prhb ≤ lt   induction assumption holds −  sources                                           t  ha ∈ ht prha ob ≥ lt  error ﬁrst current estimation error   ˆ                                               marginal distribution immediate reward  proh − proh ≤ lt  probability                                                       bounded   second errors   −                                                       lt                                                                                                                                                          previous levels bounded −  t  lt                                                        induction assumption summing terms completes    proof ﬁrst note probability  −  induction step                                                          ˆ                    t  history ∈ ht prhb − prhb ≤ lt similarly subsection  exploit long  using hoeffding bound error  proh − overall average reward essentially average                                                        ward exploitation period                                                        lemma   time phase average reward    use algorithm approximate homing strategy                      pt          ∗                                                        time  time satisﬁes ri ≥ rt bh  −  simply random policy reset  exploration use policy algorithm ott  lt  tkh logt probability                                                              slightly simplify                                     − 
