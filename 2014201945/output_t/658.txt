      ranking cases decision trees geometric method preserves                                              intelligibility                       isabelle alvarez                        stephan bernard                      lip paris vi university                      cemagref lisc              place jussieu paris france          aubiere cedex france                    isabellealvarezlipfr                  stephanbernardcemagreffr                        abstract                          conditional probability estimate leaf corrected                                                        ratio shifts probability prior probability      paper proposes new method rank cases class raw conditional probability estimate      classiﬁed decision tree method applies                                                                                 leaf deﬁned   number      posteriori modiﬁcation tree  training cases class label classiﬁed leaf      doesn’t use additional training cases consists total number training cases classiﬁed leaf      computing distance cases deci cases classiﬁed leaf      sion boundary induced decision tree general type correction generally used      rank according geometric score mestimate pm equation  uses prior prob      data numeric easy implement ability class parameter laplace cor      efﬁcient distancebased score global rection pl particular case mcorrection      assess contrary methods evaluate classes priors cestnik       score level leaf distancebased zadrozny elkan       score gives good results pruned tree      tree intelligible property preserved      improved ranking ability main reason                 pcm                                                                              pmc                plc                efﬁcacity geometric method                                     cases classiﬁer sufﬁciently accu      rate errors located near decision boundary main smoothing methods don’t                                                        modify structure tree order improve                                                        probability estimate methods applied    introduction                                       pruned trees provost domingos   decision trees dt popular classiﬁcation tool telligibility model reduced  cause easy build provide intelligible main decision trees compared  model data contrary learning methods classiﬁers like naive bayes neural networks instance  need intelligibility important artiﬁcial intelli ensemble methods like bagging used successfully  gence applications fully automatic rank cases margin priori estimate  interaction enduser expert class membership ensemble methods loose  reason dt algorithm widely used classiﬁca intelligibility model  tion purpose murthy  examples real world method propose aims ﬁrstly preserving  applications applications knowing class intelligibility model objective improve  each case sufﬁcient make decision needs ranking modifying tree method  compare cases order select based computation distance cases  promising examples case marketing decision boundary boundary inverse image  applications allocation resources grants different classes input space possible  zadrozny elkan  description chari deﬁne metric input space distance case  table donation problem traditional idea case decision boundary deﬁnes score speciﬁc  look probability each case belong pre each case unlike methods score  dicted class just class cases ﬁned level leaf shared cases  ranked according probabilitybased score unfortu classiﬁed leaf geometric methods like  nately methods highly suitable probability esti support vector machine svm proved  mate produces generally unintelligible models distance decision boundary used estimate  reason recent works aim improving decision tree posterior probabilities platt  details  probability estimate smoothing methods particularly twoclass problem additional database needed order  teresting purpose consist replacing raw calibrate probabilities applications                 size   ∆n                       estimator leaf modiﬁcation tree struc       database    dataset  utpt      ut           ture method improves signiﬁcantly class probability                                                        estimates practical use kernel density estimator       bupa            ±  ±       glass            ±   ±        limited low dimension setting parameters       ionosphere       ±  ±         easy kohavi  builds naive bayes classiﬁers       iris             ±   ±        level leaf using induction algorithm       letter        ±  ±        objective tree partition separate classes       newthyroid       ±   ±        segment data conditional independence       optdigits       ±  ±         sumption better veriﬁed size tree limited       pendigits      ±  ±         cover each leaf data experiment size       pima             ±  ±        naive bayes trees nbt comparable size       sat             ± ±        pruned trees segmentation space com                               ±          ±       segmentati                    pletely different different objectives structures       sonar            ±  ±       vehicle          ±  ±        interpretation dt nbt compare easily       vowel            ±   ±          methods try correct probability estimate       wdbc             ±  ±        each nodes propagating case through different pos       wine             ±   ±        sible path each node methods like fuzzy trees                                                        umano  et al  fuzzy split quinlan   table  comparison size pruned pt uncollapsed recently ling yan  deal different issue  unpruned ut trees mean standard deviation difference managing uncertainty input case train  number leaves  resamples         ing database generally computation probability                                                        estimate complex cases difﬁcult  don’t need exact posterior probability generally stand lot nodes involved nonconvex  possible use directly score induced distance area input space corresponding class di  rank select interesting cases        vided arbitrarily leaves point view    paper organized follow section  examines intelligibility methods totally convincing  intelligibility viewpoint methods applied decision propose structure pruned tree  trees rank cases estimate posterior probabilities sec rank cases accordingly distance  tion  presents method obtaining distancebased decision boundary deﬁned tree  score explains interesting theoreti  cal point view section  presents experimental  distance ranking methods decision trees  sults drawn numerical databases consider axisparallel dt adt operating nu  uci repository comparison results obtained merical data each test tree involves unique attribute  smoothing methods applied databases note Γ decision boundary induced tree Γ  make comments geometric score hybrid sists pieces hyperplanes normal  method concluding section                     axes                                                          consider multiclass problem class    decision tree methods ranking             positive class let case cx class label     intelligibility viewpoint                          assigned tree  dx Γ distance                                                        decision boundary Γ use distance example  success decision trees classiﬁcation method decision boundary deﬁne geometric score  good intelligibility model produced  algorithms pruning methods breiman et al   global local geometric ranking  bradley lovell  esposito et al  produce deﬁnition  geometric score  shorter trees performance longer geometric score gx distance  trees generalization performance enhanced decision boundary cx  opposite  produce shorter tree purpose seeking com  promise accuracy performance criteria       dx Γ   cx positive class                                                             gx                                        size tree table  shows unpruned trees         dx Γ  large compared pruned trees similar accuracy       −  mean absolute difference databases  theorem  global geometric ranking   size problem geometric score induce quasiorder ex  desirable improve probability estimate given dt amples classiﬁed tree   order allow compromise size ranking ability                                                                            gx   gy                 smoothing methods probability estimate                   ⇔      ≥  examples classiﬁed leaf order cases ranked decreasing order relatively geomet  produce speciﬁc probability estimates methods ric score promising cases highest geomet  learn directly probability class membership leaf ric score means predicted class positive  instance smyth et al  use kernelbased density far decision boundary  geometric score examples ranked individu positive examples plotted ratio neg  ally leaf leaf                               ative examples score varies methods    geometric score speciﬁc each example constant probability estimates leaf points plot  possible ﬁrst rank leaves smoothing method ted leaf afﬁne interpolation  equivalent method ranks leaves cases consecutive points assumes examples selected ran  rank cases inside leaf             domly inside leaf set leaves score                                                        use roc curve visualize ranking geometric rank  theorem  local geometric ranking                                                        ing good beginning curve seen    geometric score induce quasiorder ex                                                      figure   amples classiﬁed leaf                             pc  pc                                                ⇔     pc  pc gx gy                                     ≥    local geometric score leaves ranked ac    cording probability estimate inside each leaf                    ranking method  inside each group leaves output probability  estimate examples ranked according geometric                  mestimate  score                                                                       local geometric                                                                               global geometric    distance case decision boundary com positive  ratio  puted algorithm described alvarez   sists projecting leaves class label  differs cx nearest projection gives distance  algorithm  distancefromxdt             ∞   gather set leaves class cf  cx               each                                                          negative ratio              ∈           compute pf  projectionontoleafxf     compute  dx                     figure  roc curves different ranking method wd breast                                                    cancer sample local geometric ranking curve intersects     df   df                  basic mestimate curve leaf points   return  dx Γ              algorithm  projectionontoleafxf  tii           dt algorithms generally designed maximize                                 ∈              accuracy unreasonable hypothesize errors lie     sizei                           near decision boundary ideal case                             doesn’t verify test ti yu       possible demonstrate hypothesis true                                                 ti involves attribute threshold value dt builds partition input space class   return                                           problem possible associate dt unique function                                                              gx predicted class  projection leaf straightforward case several→ decision   trees associated function  adt area classiﬁed leaf hyperrectangle consider ideal case statistical decision  deﬁned tests complexity algorithm joint distribution observations uniform  onn  worst case number tests graph indicator function set    tree number different attributes tree suppose size maximal hypercubes                                                         lower bound ν   prevent pathological    theoretical viewpoint                            situation boundary case decision trees  expect geometric ranking interesting results built samples drawn approach closely  errors occur near decision                       wanted size sample grow indeﬁnitely    property veriﬁed positive cases cases class particular case function errors located near  class recognized negative decision boundary  geometric score small absolute value false pos theorem  proximity errors dt associated  itive negative cases classiﬁed positive function close errors near decision  small positive geometric score contrary true boundary ∂g  positive higher geometric score true negative note area   set errors a˙  negative geometric score high absolute value inde interior consider− ǫ small  pendently score estimated leaf geometric ǫ  νn  score tends bring side side false negative false pos  itive repel true positive true negative                                                                                                    seen receiver operating characteristic roc curve     ǫ  a˙    dx ∂g  αn √ǫ    way described adams hand  ratio ze  −         ∈   ⇒                   ˙                    proof let consider bx   maximal case geometric score good expect  hypercube centered connected component geometric score better accurate trees                                                  volume included                                 −             ǫ  νn size smallerr ν  √ǫ  experimental results  boundary ∂b encounters decision boundary  experimental design  meeting points different hyperplanes−                                                        studied geometric ranking database  maximal constant ∂f                                                        uci repository blake merz  numerical  ∂g cross necessarily dx ∂g  √n                                                       attributes missing values directly  constant size smaller ν                                                        concerned study problem prevalence  meeting point lies ∂g size                                                        positive class method doesn’t build decision  smaller lower bound maximal balls                                                        tree applies grown tree didn’t pay par  dx ∂g √n  √n √n ǫ                      ≤             •                 ticular attention relative frequency classes    real conditions far ideal case datasets chose positive class class  ﬁrst place generally doesn’t exist test lowest frequency database class grouped  hypothesis proximity errors generally veriﬁed table classes logical   shows mean difference mean distance classes equiprobable particular meaning  correctly classiﬁed cases hits errors decision chose randomly lot work  boundary computed each sample λ inverse analysis multiclass problem simplicity  coefﬁcient variation difference means treated multiclass problem class problem class  deﬁned  dh σh mean standard examples modiﬁed growing trees  deviation distance correctly classiﬁed examples each database divided  bootstrap samples  decision boundary σe magnitude separate training test sets proportion    error examples                                       specting prior classes estimated frequency                          dh                         total database best way build                    λ      −                                              σh  σe                      accurate trees unbalanced dataset different error costs                                                        interested building accurate  table  shows percentagep samples λ                                                         efﬁcient tree just want study effect geometric   conﬁdence coefﬁcient normal≥                                                        ranking pruned trees reason grow trees  assumption test unilateral errors                                                        default options weka’s witten frank  closer decision boundary majority databases                                                         implementation cases dif  datasets property veriﬁed generally                                                        ferent options build better trees unpruned trees  low mean accuracy  bupa  sonar                                                        disabled collapsing function  consider samples accuracy better                                                          used laplace correction mestimate smoothing   proportion shifts   respectively                                                        methods correct raw probability estimate leaf                                                        reducederror pruned tree normal pruned tree                  ∆               samples    value chosen pc   pc                                λ            λ       database      means                   ≥        prior probability class of× suggested    bupa        ±  ±                zadrozny elkan     glass       ±  ±                  used different metrics order compute dis    ionosphere  ±  ±                tance decision boundary minmax mm metric    iris        ±  ±                standard metric metrics deﬁned    newthyroid  ±  ±                basic information available data estimate    optdigits   ± ±                                                                 range each attribute estimate mean ei    pendigits   ±  ±               standard deviation  new coordinate    pima        ±  ±                                          sat         ± ±               deﬁned     segment    ±  ±                      mm      xi   mini            xi   ei    sonar       ±  ±                    yi        −            yi     −                                                                          maxi    mini                si    vehicle     ±  ±                                 −                                                        parameters metric estimated each sample    vowel       ± ±             wdbc        ± ±               choice metric limited effect geo    wine        ±  ±              metric score measure difference area                                                        roc curve auc  each database                                                                              table  comparison mean distance errors hits  −  −  thyroid vehicle                                                                      ±                                   decision boundary test bases  samples database databases −  glass database −   mean difference estimated each sample bad  sults bold                                         comparison distancebased ranking                                                             smoothing methods    corollary theorem  tree accurate er geometric score used rank examples  rors lie near decision boundary changing tree structure used estimate          rederror  normal                                     reducederror  normal     dataset  pruning  pruning   pruning  nbtree             dataset     pruning    pruning   unpruned     bupa   ± ± ±  ±       bupa     ±    ±  ±     glass  ± ± ± ±       glass    ±    ±  ±     iono ±  ± ± ±        iono     ±    ±  ±     iris   ± ± ± ±          iris     ±    ±  ±     letter ± ± ± ±         letter   ±    ±  ±     thyroid ± ± ± ±        thyroid  ±    ±  ±     optdig ± ± ± ±        optd    ±    ±  ±     pendig ± ± ± ±         pend    ±    ±  ±     pima   ± ± ± ±        pima     ±    ±  ±     sat    ± ± ± ±          sat      ±    ±  ±     segment ± ± ± ±        segment ±    ±  ±     sonar  ± ± ± ±         sonar    ±    ±  ±     vehicle ± ± ± ±        vehicle  ±    ±  ±     vowel  ± ± ± ±          vowel    ±     ±  ±     wdbc   ± ± ± ±          wdbc     ±    ±  ±     wine   ± ± ± ±         wine     ±     ±  ±    table  absolute difference auc global geometric table  absolute difference auc local geometric  ranking standard metric smoothing methods leaf ranking standard metric best smoothing method  column shows difference global geometric ranking mean values standard deviations × insigniﬁcant values  rederror pruning tree nbtree mean values stan italic bad value  dard deviations × insigniﬁcant values italic bad results  bold                                                        experiment partially conﬁrms theoretical viewpoint                                                        cerning fact geometric score gives interesting results  posterior probability example appropriate mea misclassiﬁed examples near decision bound  sure performance case auc table  shows ary particularly true bupa liverdisorder  difference global geometric ranking laplace ionosphere databases table  shows datasets  mestimate correction leaf                     doesn’t verify hypothesis proximity errors ma    apart cases global geometric ranking gives jority samples actually global geometric score  better values laplace mestimate correction bad results datasets   conﬁdence coefﬁcent differences rel concerning improvement geometric ranking  atively small   ab accuracy tree better experiment  solute values improvement important conclusive compute table  table  subset  shown difference auc global geo samples best quartile tree accuracy global  metric ranking reducederror pruned tree nbtree geometric ranking improved results signiﬁ    table  shows difference local geometric rank local geometric ranking gives better results  ing smoothing correction leaf local geometric rank total sample glass ionosphere  ing better  conﬁdence coefﬁcent database hypothesis proximity errors  smoothing method case sig improved subset samples  niﬁcant like global ranking improvement  vary lot absolute value      said theoretical viewpoint section expect  conclusion  geometric ranking outperform smoothing method presented article geometric method rank  ginning roc curve measure relative behavior cases classiﬁed decision tree applies  roc curves increasing value negative ratio axisparallel tree classiﬁes examples numerical  computed aucx         integral func tributes concerned problem  tion roc curve a≤≤step value global growing tree problem unbalanced datasets dif  geometric score smoothing correction ta ferent misclassiﬁcation costs lead preprocessing  ble  shows normal pruned trees theshows maximum data new pruning methods geometric method  absisse value aucgy aucsy doesn’t depend type splitting pruning criteria  ﬁdence coefﬁcient  the≥ normal assumption used build tree depends shape   smaller values negative ratio cision boundary induced tree consists ranking  global geometric≤ ranking outperforms method case according distance decision boundary  term auc                                         taking account class class    table  bases global predicted decision tree theoretical arguments suggest  geometric ranking methods efﬁcient begin method interesting misclassiﬁed exam  ning roc curve total range ples lie near decision boundary partially  forms badly like pima database table  ﬁrmed experimentation combination geomet
