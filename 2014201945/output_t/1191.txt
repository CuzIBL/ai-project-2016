            graphbased semisupervised learning generative model                           jingrui               jaime carbonell               yan liu                                      carnegie mellon university                                     school science                                  forbes avenue pittsburgh                     jingruihcscmuedu           jgccscmuedu          yanliucscmuedu                         abstract                      labeled data  finally classification labels                                                    obtained comparing function value prespecified       paper proposes develops new                                                    threshold  example gaussian random fields       graphbased semisupervised learning method                                                     harmonic function method learning problem formu      different previous graphbased methods                                                    lated terms gaussian random field graph       based discriminative models method                                                    mean field serves function zhu et al         essentially generative model class                                                    example local global consistency method       conditional probabilities estimated graph                                                    function each point iteratively determined       propagation class priors estimated                                                    information propagated neighbors       linear regression  experimental results various                                                    initial label zhou et al   example       datasets show proposed method superior                                                    graph mincut method function corresponds parti      existing graphbased semisupervised learning                                                    tioning graph way roughly minimizes number       methods especially labeled subset                                                    similar pairs examples given different labels       proves insufficient estimate meaningful class                                                    blum chawla   mincut method func      priors                                                    tion binary values                                                      till graphbased semisupervised learning meth    introduction                                  ods generally approached discriminative   real world classification tasks number labeled spective zhu  function graph cor   instances prohibitive cost manually responds posterior probabilities way     labeling single data point number unla discriminative setting use unlabeled data    beled data large easy obtain  does necessarily guarantee better decision boundaries     traditional classification algorithms known supervised addition clear explanation function    learning make use labeled data prove graph correspond posterior probabilities    insufficient situations  address problem statistics point view   semisupervised learning developed makes  paper propose new graphbased   use unlabeled data boost performance supervised semisupervised learning method generative model   learning  particular graphbased semisupervised learn perspective  specifically class conditional probabilities    ing algorithms proved effective applica class priors estimated weighted graph     tions handwritten digit classification zhu et al potential advantages involve aspects     zhu et al  medical image segmentation grady theoretically justified ideal cases   funkalea  word sense disambiguation niu ji classes separable output functions terms certain   tan  image retrieval et al  eigenvectors graph converge class conditional     compared semisupervised learning methods probabilities number training data goes infinity    tsvm joachims  finds hyperplane nonideal cases functions provide good estimate   separates labeled unlabeled data class conditional probabilities  finally estimated   maximum margin graphbased semisupervised learning class priors make use labeled unlabeled data   methods make better use data distribution revealed compensate lack label information   unlabeled data  graphbased semisupervised learning practical situations  experimental results show ap  weighted graph constructed labeled proach leads better performance existing   unlabeled data represented vertices  graphbased methods variety datasets    methods viewed estimating function claim stronger theoretical justification better em  graph zhu   based assumption nearby pirical results   points feature space likely label   rest paper organized follows  section    function defined locally smooth consistent section  introduce estimate class condi                                                ijcai                                                tional probabilities class priors respectively  section                                                                                                   deals outofsample problem followed            line algorithm section   experimental                                                                            results shown section   finally conclusion     represent submatrices corresponding    hint future work section            positive negative samples respectively                                                     represents zero matrix  total number positive        estimating class conditional probabilities   negative samples training set                                                           nn  nn square matrix  let        notation                                    diagonal matrices diagonal elements      binary classification problem suppose given row sums     written                                                                                                          set  training samples xx   nl  sam   dwd                                                                                                    ples labeled including  positive  yi                                                     il                                    dwd   nnn              yin             lll negative   ill     samples    following theorem connects class conditional     remaining nnnul samples unlabeled  goal probabilities diagonal elements       predict class labels nu  points computing theorem  xij xxxv      posterior probability  pyxii                    vn  positive parameters function  satisfies     bayes rule                         following conditions        udu                         px py            py        ii                                                       ii                                sup     limuu         limv   lim nv                         px  py                                                                    ii                                                                    yi                                                            number examples  goes infinity dn                                                                                         ii yi   yi  predicted  iff pyii   genera                                                     verges pxyii   tive model order calculate pyii  need esti                                                     proof theorem straightforward            pxy       py  mate   ii       section focus appendix  notice theorem similar                                                    result kernel density estimation  difference   estimating class conditional probability pxyii                                                    kernel density estimation labeled data    estimation py discussed section single class situation labeled     form affinity matrix   nn   unlabeled data estimate class condi   wxx          nonnegative function tional distributions classes time     ij            ij                             suppose labeled data noisefree  according   measuring direct similarity xi                                                       theorem  use dii  approximate class condi  define     diagonal matrix                                                    tional probability xi  given observed label yi            dwin          sdwd    finally define     iij   ij                                      unlabeled points know dii  corre              dimensional vectors  element sponds pxyii   pxyii    address prob      set  iff corresponding point positive lem make use eigenvectors     negative labeled                                                      easy show largest eigenvalue                                                         form connected graph respectively      ideal case                                                                                                                                vd   start let consider ideal case corresponding eigenvectors       classes far apart  case following                                                    vd      chung   based      equation                                                    construct eigenvectors  eigenvalue         px  py   pxy     py    pxy                                 tt                                                               tt                                                              vv                                pyxx pxy                                                     zero vector  notice square                                                                             yx  observed class label data point                                                     elements   add      based assumption xi   dif                                                                                         vvd         ferent classes corresponding wij                                                               obviously   correspond  pxyii     knew labels samples sam                                                   pxy  ples class affinity matrix  ii   respectively nonzero elements      symmetric matrix  blockdiagonal  spe equal dii     cific let                                                   ijcai                                                    perform fsf         sponds eigenvalue   iterate fsf                                                   fsf  convergence     fsf convergence  initial value                                                    verge eigenvector  hand op                                       orthogonal    elements     eration sf fsf seen     nonnegative  converge   similarly  labeled data gradually spreading information nearby                                                 points  iteration steps unlimited data point                                         converge    convergence    equally influenced positive negative la  proportion class conditional probability beled data leading value                                                     solve problem algorithm designed   positive negative class  normalizing                                                  stopping criterion iteration process stopped   sums  empirical estimation criterion satisfied  specific esti    pxyii    pxyii   converges true value mating class conditional probabilities positive class    goes infinity                         estimate pxyii   each iteration step                                                                       figure  gives example density estimation normalizing  sums   summing   ideal case  figure shows training data  moons represent classes each class labeled probability negative labeled samples   example marked star  figure show es average likelihood samples positive class                                                         nl  timated class conditional distributions classes lpxynii    stop iteration                                                                                                           second derivative  respect iteration steps                                                  crosses   criterion justified follows                                                  initial iteration steps negative data positive                                                 score nearby positive labeled points rate                                                  increases low iteration proceeds                                             negative data accumulated high scores                                               propagate majority negative points rate                                                   gradually increases finally  begins converge                                                   value each data point stable rate decreases                                                   reaches   plot curve  respect                                                   number iteration steps shape convex                                                   concave convergence figure  notice                                                   initial iteration steps positive points                                                   far away positive labeled points connected                                                   kind manifold positive scores                                                                                  algorithm stops stage fully explore   figure  density estimation ideal case training data                                                   data distribution cause misclassification certain   class conditional distributions                                                   clusters data  choose transition point     general case                           convex concave stopping point order                                                   trade prematurity excessive propagation    general cases classes far apart                                                   stopping criterion negative class derived   following theorem                                                                   nl                                                  similarly pxyii      key point   theorem  xij  satisfies conditions theorem                                                       algorithm estimation class conditional   number samples  goes infinity dnii                                                   probabilities classes independent   verges pxy py     pxy    py               ii             ii                     numbers iteration steps stopping criterions    proof theorem quite similar theorem   satisfied necessarily   omit details  seen easily theorem    figure  gives example density estimation   special case theorem  classes far general case showing effectiveness criterion    apart                                     example quite similar shown figure            limdnii pxy  py            classes far apart  figure shows                                              value  upper curve  lower curve        limdnii  pxy  py                                                     each iteration step  arrows point positions                                                   curves criterions satisfied  figure     equation  fact limnn py     leads theorem                            show estimated class conditional distributions     general cases  tends form connected graph classes  small gaps middle   instead  eigenvector corre distributions moon structure recovered fairly                                                  ijcai                                                                                                                                                      prediction new testing data                                                                                                                                                  classify data point  present during                                                                           training stage calculate class conditional prob                                                                            abilities kernel regression                                                                                                                                           xpxy                                                                        ii                                                                          pxy                                                                                            xx                                                                                                                                      using class conditional probabilities class                                                                                           priors obtained during training stage calculate                                                    posterior probability make prediction                                                       algorithm                                                      procedures estimating pxyii py  sum                                                   marized table  table  respectively                                                                                      nn                                                     form affinity matrix                                                                                              wxx        calculate      figure  density estimation generation case training ij   data   each iteration class conditional  initialize    element    set    distributions                                       corresponding point positive negative      note stopping criterion discussed based labeled     simple heuristics  currently trying design  update fsf fsf    stopping criterion principled manner                                                                                                                                               assign pxyii    pxyii                                                                                          estimating class priors                                                      pxy                                                           normalize          ii            section focus estimating class prior py                                                            pxy       existing graphbased semisupervised learning methods  ii   use labeled set estimate class priors explic  calculate average likelihood negative positive    itly zhu et al  implicitly zhou et al   ob labeled points positive negative class                                                                                   viously real applications proportion positive pxy  lpxynl                                                               ii           ii       negative labeled data far true class priors                   algorithm use labeled unlabeled step  unless following conditions    data estimate class priors  according theorem  satisfied    estimated class conditional probability    remains     converged                                                             pxyii feed following equations    does remain  second   form linear regression problem solution rivative    respect iteration steps    equal squares estimator py          crosses                                                                                                          output pxyii   pxyii        dii pxy pˆˆ pxy      number labeled data small                                                          table  description estimation pxyii    estimated class conditional probabilities     accurate pˆ  reliable  solve  solve following linear regression problem   problem use beta distribution prior distribution squares estimator pˆ  py     py    parameters pˆ   pˆ      ppxyˆˆ pxy  ni    estimate py   based labeled set           ii             ii      ii                                                                      pnˆ                                  calculate class priors smoothed proportion          pyl    py       py                   positive negative samples labeled set                                                                                   pnˆ   equivalent smoothing proportion posi pyl py      py                                                                           tive negative samples labeled set               number labeled data small unlabeled data fully table  description estimation py    exploited compensate proportion labeled set    class priors number  experimental results    labeled data large labeled data dominate estima                                                   section present comparative experimental    tion class priors                                                    results datasets cedar buffalo binary digits database                                                    hull  document genreclassification dataset                                                    liu et al   algorithm compared                                                   ijcai                                                graphbased semisupervised learning methods gaussian case performance algorithm comparable    random fields zhu et al  local global balanced case  class mass   consistency method zhou et al   did compare malization procedure adopted gaussian random fields    supervised learning methods nearest depends labeled set estimate class priors    neighbor proved effective algorithm makes use labeled    gaussian random fields based experimental results unlabeled set estimate class priors     zhu et al                              robust perturbation proportion      designed kinds experiments balanced positive negative data labeled set    unbalanced  balanced case ratio labeled points    each class class priors  genre dataset    unbalanced case explained fix total genre classification classify documents based    number nl  labeled points perturb number writing styles political articles movie reviews                             positive labeled points   gaussian distri genre dataset use consists documents                                        bution mean  standard deviation   each genres including biographies interview scripts    experiment gradually increase number labeled data movie reviews mr product reviews pr product press    perform  trials each labeled data volume average releases ppr product descriptions store websites pd    accuracy each volume point                                                    political articles newspapers pa editorial papers      cedar buffalo binary digits database       politics ep news search results multiple                                                    search engines using  queries sr  randomly select    perform experiments cedar buffalo binary digits  documents each category compose    database hull  including classification tasks dataset  documents  each document processed    classifying digits “” vs “”  images each class “tfidf” vector generated based     odd vs digits  images each class  frequent words dataset stemming    images each digit  data use header stop words removed     used zhu et al                                                      xxexp         xx    bor                                                  ij                 xxij       exp                                                         rowed zhu et al  roughly measures    average distance each data point  nearest similarity documents  difference    neighbors                                               edges instead keeping edges                                                    nearest neighbors  perform experiments compare                                                   three algorithms  results provided figure                                                     figure  respectively                                 algorithm                                                                                                           algorithm         algorithm                                                          gaussian random fields  algorithm     gaussian random fields gaussian random fields     accuracy               accuracy                     local global consistency local global consistency         local global consistency  gaussian random fields                                                                                                     local global consistency                                                                                                                                                                                  accuracy           labeled set size       labeled set size accuracy                                                                figure  balanced classification  vs  odd vs                                                                                                        algorithm                                                                                         labeled set size      labeled set size                              gaussian random fields                                                    local global consistency                                                                                                                            figure  classification random partitions balanced            algorithm           gaussian random fields                  unbalanced                                                                              accuracy local global consistency accuracy      algorithm         algorithm                                                     gaussian random fields  gaussian random fields                                                       local global consistency local global consistency                                                                                                                                                                                                         labeled set size        labeled set size                                                                                                                                        accuracy              accuracy    figure  unbalanced classification  vs  odd vs        figure show results classifi                                                                                   cation tasks balanced case  performance                                                                            labeled set size       labeled set size   algorithm comparable gaussian random fields                        better local global                                                   figure  unbalanced classification pa vs vs    sistency method  figure show results                                                      figure  randomly partition  categories    unbalanced case  situation performance                                                    classes pa pr sr vs mr ppr pd ep     gaussian random fields worse balanced                                                    figure correspond balanced unbal                                                  ijcai                                               
