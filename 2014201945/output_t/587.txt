                       hybridized planner stochastic domains                  mausam                      piergiorgio bertoli                 daniel weld    dept sc engg              itcirst              dept sc engg        university washington             sommarive               university washington           seattle wa               povo trento italy            seattle wa   mausamcswashingtonedu                 bertoliitcit              weldcswashingtonedu                        abstract                          aimed different objectives example algorithm portfo                                                        lios run multiple algorithms parallel reduce      markov decision processes powerful frame   total time obtain solution bound bound uses solu      work planning uncertainty current al tion algorithm bound algorithm      gorithms difﬁculties scaling large problems paper employ tighter notion hybridization      present novel probabilistic planner based explicitly incorporate solutions subproblems      notion hybridizing algorithms partic algorithm partial solution      ular hybridize gpt exact mdp solver                                                          present novel algorithm hybplan hy      mbp planner plans using qualitative non                                                        bridizes planners gpt mbp gpt general plan      deterministic model uncertainty ex  ning tool bonet geffner  exact mdp      act mdp solvers produce optimal solutions quali solver using labeled real time dynamic programming rtdp      tative planners sacriﬁce optimality achieve speed                                                        mbp  model based planner bertoli et al  non      high scalability hybridized planner hyb deterministic planner exploiting binary decision diagrams      plan able obtain best techniques bdds hybridized planner enjoys beneﬁts al      —  speed quality scalability hyb gorithms scalability quality best      plan  excellent anytime properties makes  knowledge ﬁrst attempt bridge efﬁciency      effective use available time memory       expressiveness gap planners dealing quali                                                        tative vs probabilistic representations uncertainty    introduction                                         hybplan   excellent anytime properties — produces                                                        legal solution fast successively improves  realworld domains involve uncertain actions            execution stochastically lead different outcomes solution  hybplan effectively han  problems frequently modeled indeﬁnite hori dle interleaved planning execution online setting  zon markov decision process mdp known stochas makes use available time memory efﬁciently  tic shortest path problem bertsekas  mdps able converge desired optimality bound  general framework popular optimal algorithms duces optimal planning given indeﬁnite time mem  lao hansen zilberstein  labeled rtdp bonet ory experiments demonstrate hybplan competi  geffner  scale large problems   tive stateoftheart planners solving problems    researchers argued planning quali international planning competition planners  tative nondeterministic model uncertainty contrast solve  numeric probabilities contingent planners mbp  bertoli et al  make use quantitative likeli  background  hood cost information solve sim following bonet geffner  deﬁne  pler problem planners able scale larger deﬁnite horizon markov decision process tuple  problems probabilistic planners stripping mdp s pr gs  probabilities costs use qualitative contin •sis ﬁnite set discrete states use factored mdps  gent planner quickly generate policy quality compactly represented terms set state  resulting solution likely poor natural variables  ask “can develop algorithm beneﬁts •a  frameworks”                                                ﬁnite set actions applicability function                                                            ap   s→pa denotes set actions    using algorithms obtain beneﬁts                       generic idea forms basis proposed applied given state  represents power set  algorithms bound bound martello toth  rtdp popular anytime algorithm prob  algorithm portfolios gomes selman  vari lems absorbing goals return legal policy  ous schemes hybridize multiple algorithms differently  min                                                    ijcai                                                      •pr    s×a×s      →  transition function rtdp conceptually lazy version value iteration      prss denotes probability arriving state s states updated proportion frequency      executing action state              visited repeated executions    •c  a→    cost model                     greedy policy speciﬁcally rtdp simulates greedy pol                                                        icy single trace execution updates values    •g⊆s            set absorbing goal states process states visits using bellman backups rtdp trial      ends states reached         path starting ending goal reached    • start state                              number updates exceeds threshold rtdp repeats                                                        trials convergence note common states    assume observability seek ﬁnd opti                                                        updated frequently rtdp wastes time states  mal stationary policy function π s→a mini                                                        unreachable given current policy complete  mizes expected cost indeﬁnite horizon incurred                                                        vergence state slow likely po  reach goal state policy π execution starting                                                        tentially important states updated infrequently  start state induces execution structure execπ                                                        rtdp guaranteed terminate  directed graph nodes states transitions                                                          labeled rtdp ﬁxes problems clever labeling  labeled actions performed π denote                                                        scheme focuses attention states value func  graph free absorbing cycles nongoal                                                        tion converged bonet geffner  specif  node exists path reach goal prob                                                        ically states gradually labeled solved signifying  ability reach goal greater zero policy                                                        value function converged algorithm  free absorbing cycles known proper policy                                                        propagates information starting goal states    note cost function s→ mapping states                                                        algorithm terminates start state gets labeled  expected cost reaching goal state deﬁnes greedy                                                        labeled rtdp guaranteed terminate guaranteed  policy follows                                                      converge optimal value function states reach                                                       able using optimal policy initial value function                                              πj  argmin ca     prs ajs        admissible algorithm implemented gpt general            a∈aps        s∈s                         purpose tool bonet geffner                                                    ∗      labeled rtdp popular quickly producing rela    optimal policy derives value function                                                         tively good policy discounted rewardmaximization prob  satisﬁes following pair bellman equations                                                        lems range inﬁnite horizon total rewards ﬁnite                                                        problems planning competition undis      ∗      ∈g                                  counted cost minimization problems absorbing goals                                                                                         intermediate rtdp policies problems contain       ∗                                  ∗          min    ca     prs aj     absorbing cycles implies expected cost reach             a∈aps                            s∈s                        goal inﬁnite clearly rtdp anytime algo                                                        rithm problems ensuring trajectories    labeled rtdp                                     reach goal takes long time  value iteration canonical algorithm solve mdp  dynamic programming approach optimal value func  modelbased planner  tion solution equations  calculated limit mdp cost probability information trans  series approximations each considering increasingly long lates planning problem qualitative uncertainty  action sequences jns value state iteration strongcyclic solution problem — admits  value state iteration calculated loops free absorbing cycles — used le  process called bellman backup follows    gal solution original mdp highly                                                        suboptimal solving easier                                                                                                      problem algorithms solving relaxed problem                                                      highly scalable algorithm implemented     jns   min    ca     prs ajns               a∈aps                                   mbp                             s∈s                                                                                     modelbased planner mbp bertoli et al     value iteration terminates ∀s ∈s  jns −    lies effective bddbased representation techniques im  jn−s≤ termination guaranteed  plement set sound complete plan search veri  furthermore sequence ji guaranteed converge ﬁcation algorithms algorithms mbp  optimal value function ∗ regardless initial val signed deal qualitatively nondeterministic planning  ues unfortunately value iteration tends quite slow domains deﬁned moore machines using mbp’s input lan  explicitly updates state exponential guage smv planner general capable ac  number domain features optimization restricts commodating domains various state observability  search state space reachable initial fully observable conformant different kinds planning  state algorithms exploiting reachability analy goals reachability goals temporal goals  sis lao hansen zilberstein  focus mbp variants strongcyclic algorithm denoted  rtdp barto et al                             “global” “local” share representation                                                    ijcai                                                    policy π binary decision diagram fact algorithm  hybplanhybtime threshold  constructed backward chaining goal general  deadends ←∅  idea iteratively regress set solution states  repeat  current policy π ﬁrst admitting kind loop introduced  run gpt hybtime  backward step removing “bad” loops  open ←∅ closed ←∅  chance goal achievement exists  openinserts  “local” variant algorithm prioritizes solutions  open nonempty                                                             remove open  loops order retrieve strong solutions           possible search ends π covers initial  closedinsert                                                              labeled solved inside gpt  state ﬁxed point reached details                                                               πhybs ← πgpts  cimatti et al                                                                                          visitsthreshold                                                                    π  ← π   hybplan hybridized planner                                  hyb      gpt                                                                novel planner hybplan hybridizes gpt mbpon                                                                 assignmbpsolutions  hand gpt produces costoptimal solutions                                                                                             st prs πhybs   ∈ closed  original mdp mbp ignores probability      s ∈ deadends  cost information produces solution extremely quickly   assignmbpsolutions  hybplan   combines produce highquality solu      tions intermediate running times                           openinserts    high level hybplan invokes gpt maximum    remove absorbing cycles execπhyb                                                                    π                 time say hybtime gpt preempts run  evaluate hyb computing hyb                                                                π  ning time passes control hyb  hyb best policy far cache  plan point gpt performed rtdp tri  resources exhaust desired error bound achieved  als labeled states solved  cost function jn converged start state function  assignmbpsolutions                                                      solved despite current greedy partial mbps succeeds    policy given equation  contains useful infor πhybs ← πmbps                                            π                                            mation hybplan  combines partial policy  gpt st prs πhybs   ∈ closed                                                                          policy mbp πmbp construct hybridized policy openinserts   πhyb deﬁned states reachable following πhyb  guaranteed lead goal evaluate                                                               problem unsolvable exit  πhyb computing jhybs denoting expected cost                                                             reach goal following policy case dissatisﬁed           π   run rtdp trials repeat       closedremove        hyb                                                    deadendsinserts  process pseudocode planner                                                                             st πhybs  leads directly                                                                                      algorithm                                                   assignmbpsolutions      construction hybridized policy starts  start state uses popular combination open  closed lists denoting states action needs return failure implying solution exists  assigned assigned respectively additionally current state clearly choice action pre  maintain deadends list memoizes states starting vious step faulty step led deadend  reach goal deadend         procedure assignmbpsolution   recursively looks                                                        policy assignments previous levels debugs π  deciding gpt mbp lines                                              hyb                                                        assigning solutions π  additionally memoize  state shybplan decides assign action using                  mbp                                                        deadend states reduce future computation  πgpt πmbpifs labeled solved cer  tain gpt computed optimal policy starting cleaning evaluating caching π lines                                                                                       hyb   lines  need assess conﬁdence formulate evaluation π following  π                                                                               hyb   gpt  estimate conﬁdence gpt’s greedy policy linear equations  keeping count number times updated  inside labeled rtdp intuitively smaller number visits                                                              ∈gelse  state corresponds low conﬁdence quality  hyb                 π  prefer use π instead user                                               gpt                          mbp                           scπs       prs πsj    deﬁned threshold decides quickly start trusting   hyb                               hyb                                                                               s∈s  gpt                                                          equations tricky solve possi  mbp returning failure function  mbp                                                        ble absorbing cycle execπhyb rank    partial states reachable greedy policy coefﬁcient matrix number equations  explored                       absorbing cycle convert matrix                                                    ijcai                                                    rowechelon form using row transformations  implementation hybplan  parallel perform transformations identity ma address different efﬁciency issues implement  trix ﬁnd row zeros nonzero entries ing hybplan instead precomputing mbp pol  row transformed identity matrix reveals icy state space computation  states original form absorbing cycles mand modify mbp efﬁciently solve  pick states assign mbp action subproblems repeating computation mod  peat computation note equations ify mbp’s “local” strong cyclic planning algorithm fol  small fraction overall state space lowing ways —  cache policy table π  produced         π                                                                                     mbp  exec hyb step expensive ﬁnd previous planning episodes  each planning episode  intermediate hybridized policies cache best analyze cached result π  input state                                                                               mbp  minimum hyb  policy far line  solved search skipped  perform search                                                        taking π  starting point goal  termination line  terminate differently dif  mbp                                                          second implementation evaluate π  ferent situations given ﬁxed time stop                                           hyb                                                        solving linear equations instead ap  gpt available time expire follow                                                        proximate averaging repeated simulations pol  hybridized policy computation terminate given                                                        icy start state simulation exceeds maxi  ﬁxed memory mem                                                        mum trajectory length guess hybrid policy  ory need terminate desired fraction                                                        absorbing cycle try break cycle recur  optimal repeat hybridized policy computation reg                                                        sively assigning action π state cycle  ular intervals ﬁnd policy error bound                      mbp                                                        modiﬁcation speeds overall algorithm  desired limits terminate                                                        theory takes away guarantee reaching goal  error bound develop simple procedure bound probability  low probabil                                                        ity trajectory explored simulation contain  error πhyb labeled rtdp started  admissible heuristic gpt’s current cost function jn remains absorbing cycle practice modiﬁcation sufﬁcient                                   ∗  lower bound optimal jn ≤  hy planning competition relies policy simulation  bridized policy clearly worse optimal evaluating planners experiments hybridized          ∗       s−jns    ≥         hyb                         π         policies reach goal probability    hyb           bounds error hyb                                                       ﬁnally remark gpt takes input planning  properties hybplan    hybplan   uses current  problem probabilistic pddl format mbp’s input  greedy policy gpt combines solutions mbp main smv format translation pddl smv  states fully explored gpt ensures systematic semiautomated implementing  ﬁnal hybridized policy proper free absorbing cy fully automated procedure  cles hybplan excellent anytime properties                                                           experiments  πmbps returned success hybplan capa  ble improving quality solution time avail evaluate hybplan speed planning quality  able algorithm increases inﬁnite time resources solutions returned anytime behavior scalability large  available algorithm algorithm reduces problems perform sensitivity experiment testing  gpt available resources extremely limited algorithm sensitivity parameters  reduces mbp cases hybridized plan  ner demonstrates intermediate behavior               methodology                                                        compare hybplan   gpt mbp graphs    views hybridized planner              plot expected cost cached policy hyb                                                        plan  gpt function time ﬁrst value  hybridized planner understood ways hybplan curve mbp initially each state    ﬁrst view mbpcentric run hybplan visits   πhyb  πmbp plot                         π  gpt computation mbp outputted current jns value labeled rtdp admissible  lution legal possibly low quality hybplan value increases error bound solution reduces  successively improves quality basic solution run experiments three large probabilistic pddl  mbp plugging additional information gpt   domains ﬁrst domains probabilistic variants    alternative view gptcentric draw rovers machineshop domains  aips    tuition gpt partial greedy policy πgpt improves planning competition elevators domain  gradually eventually gets deﬁned relevant states  icaps planning competition largest  accurately convergence current greedy pol problem attempted elevators domain  icy deﬁned states  state variables  accurate explored experiments terminate labeled  hybplan  uses partial policy reasonable rtdp terminates memory goes bound  completes adding solutions mbp making experiments initialize hybplan hybtime  ﬁnal policy consistent useful essence views   sec threshold   perform experiments  useful each algorithm patches other’s weakness analyze sensitivity parameters                                                    ijcai                                                                   rover domain deadends               machineshop domain deadends                                                                                       exp cost gpt                             exp cost hybplan                              jvalue gpt                                            jvalue hybplan                                                                                                                                                                                                                                                                            best  expected cost                                best  expected cost                                                  jvalue  start state        exp cost gpt     jvalue  start state                                                                               exp cost hybplan                                                                               jvalue gpt                                                                                jvalue hybplan                                                                                                                                             time sec                                      time sec    figure  anytime properties hybplan yaxis show expected cost cached policy  jns values jn converges curves meet ﬁnd hybplan’s policy superior gpt’s greedy policy  ﬁrst time gpt’s policy noninﬁnite expected cost occurs later algorithm      anytime property                                                      large rover domain deadends  ﬁrst evaluate anytime properties hybplan                                                                              exp cost gpt  moderate sized planning problems show results          exp cost hybplan                                                                        jvalue gpt  problems rovers domain          jvalue hybplan  machineshop domain figures prob  lems  state variables  actions each          observe πhyb consistently better expected cost  did πgpt difference algorithms     substantial example figure ﬁrst time                                                             best  expected cost  π                                                                                                     jvalue  start state   gpt noninﬁnite expected cost simulated paths    reach goal  seconds hybplan al  ways constructs valid policy                                    figure domain deadends                          domain figure ’bad’ actions                     time sec  agent recover hybplan  obtains greater beneﬁts domains deadends figure  plot expected cost problem large  domains anytime nature hybplan superior gpt converge  gpt notice jns values figure  hyb  plan  takes marginally longer converge  cause overheads hybrid policy construction clearly notice elevator problems table    overhead insigniﬁcant                             variables domain problems largest test                                                        problems elevators domain planning competi    recall ﬁrst expected cost πhyb expected                                                        tion  planners solve hybplan’s  cost following mbp policy clearly mbp policy  high quality substantially improved time performance encouraging  progresses initial policy computed quickly sensitivity parameters  scaling large problems                             hybplan   controlled parameters hybtime  desire run algorithm convergence hyb threshold evaluate sensitive hybplan  plan better gpt large problems parameters ﬁnd increasing hybtime reduces total  running convergence practical option lim algorithm time difference marginal implying  ited resources example figure  show experiments overhead hybrid policy construction signiﬁcant  larger problem rovers domain mem smaller values result repeated policy construc  ory requirements exceed machine’s  gb typically tion helps ﬁnding good quality solution early  memory ﬁlls algorithm explores  small values hybtime overall effective  states cases hybridization provides bene varying threshold does affect overall algorithm  ﬁts table  large domains gpt fact time does marginally affect ﬁrst time good  able output single policy ﬁnite expected cost lution observed increasing threshold implies mbp  use mbp directly problems hybridiz policy used state sufﬁciently explored gpt  ing algorithms able consistently higher rovers domain translates extra time  quality solutions                                    good policy observed machineshop observe                                                    ijcai                                                    
