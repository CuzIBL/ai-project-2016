                    ambiguous partofspeech tagging improving                    accuracy domain portability syntactic parsers              kazuhiro yoshida∗      yoshimasa tsuruoka†      yusuke miyao∗      jun’ichi tsujii∗†‡                          ∗department science university tokyo                              †school informatics university manchester                                      ‡national center text mining                            kyoshidatsuruokayusuketsujiiissutokyoacjp                        abstract                          clude ability adapt domains pos taggers new                                                        domain easier develop parsers      aim improve performance syntac    training corpora pos taggers easier construct com      tic parser uses partofspeech pos tagger pared parsers require annotation      preprocessor pipelined parsers consisting nested phrase structures      pos taggers syntactic parsers ad      vantages capability domain adapta  independence assumption taggers parsers      tion performance systems  degrade overall accuracy parsers wat                                                                      raw texts tends disappointing af son  reported using automatic pos tagger      fected errors automatic pos tagging caused score grammatical relations output      attempt compensate decrease accu   parser drop  points attempted weaken      racy caused automatic taggers allowing  independence assumption letting taggers output multi      taggers output multiple answers tags  ple tags each word weighted probability values      determined reliably empir   approach improved score  points      ically verify effectiveness method   paper verify watson’s results ambiguous      ing hpsg parser trained penn treebank  pos tagging using hpsg pollard sag  parser      results show ambiguous pos tagging im   developed trained penn treebank marcus et al      proves parsing outputs taggers weighted  time investigate effectiveness      probability values results support pre ambiguous pos tagging domain adaptation parsers      vious studies similar intentions ex ing genia corpus ohta et al  test set ex      amine effectiveness method adapting perimental results show multiple output prob      parser genia corpus show  ability values improve parser suggest      use ambiguous pos taggers help develop    importance probability distribution multiple tags ob      ment portable parsers keeping accuracy   tained pos taggers additionally show pos      high                                             itive effect ambiguous pos tagging maintained                                                        mains unfamiliar parser    introduction                                                           background  parsers use pos taggers preprocessors  use integrated models achieve tagging parsing section pos tagger syntactic  simultaneously approach general parser used experiments taggers parsers  successfully used stateoftheart parsers combined make pipelined syntactic parser raw  charniak’s charniak johnson asanatural texts using strategy described section  consequence pursuit accuracy integrated tagging parsing models based  models tagging parsing tend complex com loglinear classiﬁers ﬁrst brieﬂy introduce loglinear  putationally expensive terms training runtime models tagger parser  costs    hand pipelined systems pos taggers  loglinear models  parsers built independently developed taggers  parsers models easily make use taggers loglinear models widely used machine  use stateoftheart sequence labeling techniques learning techniques nlp literature use models  difﬁcult incorporated syntactic disam pos taggers syntactic parsers conditional log  biguation models advantages pipelined parsers linear model calculates probability event given                                                    ijcai                                                    input sentence                                                hpsg grammar combination loglinear mod  output tag sequence                               els  algorithm                                              ﬁrst loglinear model selecting lexical entries     foreach ti ti  null                 null                                 words postagged sentence estimates proba     foreach                                       bility         compute probability distribution         titi−ti−titis                                  liwiti                          loglinear models     let easiest place tag                 wi tiandli represent ith word pos tag lex                                                                                                               ti  probable tag ti                 ical entry sentence respectively information     repeat                               ti used combination features loglinear       ti  null each                            model                                                          second model selecting probable parse            figure  algorithm pos tagging         hpsg parse forest constructed lex                                                        ical entries weighted ﬁrst model    condition follows                                          lnwntn                                              exp   λifiec                  ∈  overall disambiguation model           ec                                              zc                                                                          argmax wntn                                                                            ∈f  realvalued functions fi used indicate use                                                              λ  ful features predicting  parameters optimized argmax lnwntn liwiti  ﬁt model training data zc normal ∈f                          ization constant criteria estimating param  eters loglinear models exist used map estimation parsing algorithm disambiguation model  gaussian prior distribution chen rosenfeld  cyklike chart parsing iterative beam search ninomiya  commonly successfully applied various et al   nlp tasks                                                           combining pos taggers parsers    pos tagger                                                        pipelined parsers pos taggers syntactic parsers  employ bidirectional inference model proposed developed separately freely change taggers  tsuruoka et al  pos taggers model parsers special care algorithms make  sists  loglinear models each provides pipelined systems ambiguous pos taggers parsers  probability                                           work way restrict interface ambiguous                                                        taggers formalize condition syntactic disam             titi−ti−titis                                                                biguation models applied output ambigu  given sentence ti pos tag ith ous taggers modiﬁcation  word algorithm used pos taggers shown fig concentrate following situation each word  ure  key idea algorithm does work sentence pos taggers output set candidate pos tags  usual lefttoright manner instead iteratively tries obtained tags used restrict parses parsing  tag words tagged easily “easiness” tag model  ging measured highest probability value experiments compared following three types  probability distribution tags calculated eq taggers   step  algorithm each ti−ti−titi  null prepared  classiﬁers cover                                 null                   single single tagger outputs probable single  pattern appearances  each token tag each word  training corpus used training data  classiﬁers  features used classiﬁers surface strings  words base forms words obtained dictionary preﬁxes multi multi tagger outputs set candidate pos tags  sufﬁxes poss tagged words        each word    algorithm described totally determin  istic slight modiﬁcation beam search strategy prob prob tagger similar multi each pos tag  make output approximation kbest tag sequences weighted probability provides probability  probability values                                   distribution tij swheres input sentence tij                                                        represents jth candidate pos tag ith word    grammar parser                                 assume parsing model consists indepen  parser used based headdriven phrase struc dent models terminal nonterminalaparset given  ture grammar hpsg developed using penn sentence consists terminal structure non  treebank miyao et al  disambiguation model terminal structure nt terminal nonterminal models                                                    ijcai                                                    used estimate probability distributions ts         lplr        ntt ts best parse given                                                          nt                  argmax                   ntt                                 table  accuracy section  correct pos tags                       nt                  argmax sp            ntt                                                          control ambiguity tagger introduce pa  assume terminal structure fur rameter θ used threshold candidate tags  ther decomposed sequence terminal labels liln marginalized probability candidate tag smaller  label li corresponds ith word sentence probability best tag word multiplied θ  terminal labels independent ter candidate discarded  minal model                                            figure  illustrates example outputs each tagger                                        ts                           best analysis sentence “mr meador                                                 executive vice president balcor” thresholding                                                        marginalization probability distribution output pos  overall disambiguation model              taggers loses information sequential dependency                                                   nt                                   tags harm performance parser ex        argmax llns  lis         ample let consider sentence “time ﬂies like arrow”          nt                                                 candidate pos tag sequences “nn vbz                                                        dt nn” “vb nns dt nn” output multi  assume tractable estimation method disam                               biguation algorithm applicable eq  exist        tagger “ nn vb vbz nns   dt nn”                                                        induce tag sequence “vb vbz dt nn”    let rewrite distribution lis make  pendent outputs pos taggers incorporate included original candidates  information pos tags parsing model                                                          experiments          lis   tij sp litij                                                          ﬁrst experiment veriﬁed effect ambiguous pos                                                        tagging using penn treebank training test  tij jth element set pos tags assigned ing second experiment examined parser’s capability  ith word sentence tij probability domain adaptation using penn treebank training  tag calculated prob tagger multi single tag parser biomedical texts training tagger  gers integrated model similarly assuming genia corpus testing  tags assigned word taggers equally experiments used hpsg grammar  probable replacing lis appears eq  parser grammar developed using penn treebank  eq  apply disambiguation algorithm sections  disambiguation model trained  eq  combined ambiguous pos tagger data accuracy parser evaluated  parsing model                                penn treebank section  using correct pos tags    strategy applicable wide range grammars shown table  labeled precision lp labeled  disambiguation models including pcfg terminal lr precision recall dependencies  model used assign pos tags words lexicalized predicateargument labels harmonic mean  grammars hpsg terminal model assigns lp lr each dependency pair words  lexical entries words hpsg parser described sec counted event measures used  tion  straightforwardly adapted model taking following experiments ﬁgures table  consid  eq  terminal model                            ered practical upper bounds affected    problem strategy increased ambiguity incorrect predictions pos taggers refer  introduced multiple tags reported watson  results using correct pos tags gold follow  increase computational costs suppressed ap ing comparisons systems performed using  plying appropriate parsing algorithms parsing algorithm scores  used ninomiya et al  suitable purposes  show experimentally disambiguation  parsing penn treebank  model performed efﬁciency similar                                                        pos taggers described section trained  models single taggers                                                        penn treebank sections  performance    implementation prob tagger                    tagger development set  described                                                        section  taggers multi prob hyper  various methods implementing prob tag parameter θ controlling number alternative answers  ger described implementation outputs approxi  mation tag probabilities marginalizing probability following convention literature penn treebank pars  kbest tag sequences obtained algorithm described ing used section  development section  ﬁnal  section                                        test                                                    ijcai                                                            input         mr        meador        executive  vice president  balcor        best sequences nnp         nnp      vbd   vbn       jj      nn     nn        nnp            probability    nn          nn      vbd   vbn       jj      nn     nn        nnp                            nnp         nnp      vbd   vbn       nn      nn     nn        nnp               single       nnp         nnp      vbd   vbn       jj      nn     nn        nnp             multi      nnp nnnnp nn     vbd   vbn    jj nn   nn     nn        nnp             prob       nnp nnp  vbd   vbn     jj nn     nn        nnp          probability nn   nn               nn                                         figure  example tagging results                               lplr                         oftrain  tagging                                                                                      lplr               multi θ                         ing data   strategy          multi θ                                     gold                multi θ                                 single              multi θ                                   prob                prob θ                                       gold                prob θ                                  single              prob θ                                     prob                prob θ                                    gold                                                                             single                                                                               prob                   table  accuracy different θ                                                                           gold                                                                            single      included output smaller θ means larger num            prob        ber alternative answers comparing ambiguous tag              gold        gers multi prob single ﬁrst determined         single      multi prob value θ perform best         prob          performance parser various θ shown  table  parsers multi tagger outperformed table  accuracy different sizes training data  prob cases used prob tagger  following experiments performance parser  prob tagger peak θ  accuracy                                 θ  slowly decreases smaller reason      problem noted paragraph section   used best performing θ  following experi     ments                                                            interesting contribution                                                                biguous tagging changed according performance         syntactic disambiguation model ambiguous                                                                  pos tagging did work poorer disambiguation  model ambiguous tagging considered help                              gold                                                                                         single  performance domains unfamiliar parser                           prob  performance disambiguation model likely       lower observed change performance parser                       number sentences used developing gram                   sentences  mar training syntactic disambiguation model  changed table  shows performance parser trained  various numbers sentences selected penn    figure  different sizes training data  treebank sections  figure  shows scores  systems trained    sentences                                     mean                                                                                           parsing    ambiguous pos tagging prob tagger                  lplr                                                                                                 time  contended contribute performance parser                                                                gold        msec  cause performance parser prob tagger                                                                single      msec  consistently better single tagger                                                                prob        msec    actually multi negative effect compared results  single shown ’all’ row table  score table  accuracy test set                                                      ijcai                                                                                         mean                                                        signiﬁcant       pos tagger    lplr          parsing                                       time               summary showed adaptation pos taggers       gold             msec         signiﬁcantly improve performance parsers new       singlepenn      msec         domains use ambiguous pos tagging extend       probpenn        msec         improvement       singlebio       msec       probbio         msec            related work                                                        charniak et al  investigated use pos taggers               table  accuracy genia               output multiple tags parsing concluded single tag                                                        gers preferable accuracy tag sequences                                                        selected parsers did improve signiﬁcantly  experiments evaluated measure crease computational cost considerable watson   entire training data used development parser revisited task observed terms ac  contribution ambiguous pos tagging  curacy parsing multiple taggers improve performance  table  summarizes performance systems signiﬁcantly results show taggers pass  evaluated using section  penn treebank overall parser multiple answers probability val  tendency observed development set remained ues each candidate watson’s results imply ap  contribution ambiguous tagging considerably bigger propriate parsing strategies make increase compu  development set seen table tational cost problematic charniak et al suggested  prob slowest difference big results agree watson’s point  ninomiya’s method hpsg parsing  clark et al  introduced supertaggers output  outputs ﬁrst answer worked tolerantly multiple supertag candidates taggers used  crease ambiguity                                  end ccg parser training su                                                        pertaggers require corpora deeply annotated pos    parsing genia corpus                             tagged ones method difﬁculties ex  crucial advantages separating pos tagging ploit taggers domain adaptation  syntactic parsing domain adaptation adaptation general parsers biomedical domain  systems partly achieved just changing pos tag attracted researchers biggest dif  gers section verify effect ambiguous ference general english biomedical literature  pos tagging remains domains parsing vocabularies use lexical resources tools  models familiar                              natural approach lease et al  modiﬁed charniak’s    test data used genia treebank tateisi et parser  make conform genia’s tagging  al  annotates penn treebankstyle tree struc vention use biomedical dictionaries restrict  tures sentences genia corpus genia corpus parser output obtained impressive error reduc  consists abstracts biomedical papers adopted tion  expected improvement pos  medline   campbell et al  trained pos tag sible using namedentity recognizers  ger mixture wsj penn treebank performance shown section   penn bioie corpus kulick et al andthegenia signiﬁcantly lower  suggests  corpus consist    sentences changing pos taggers partially achieve domain  respectively following hara et al  adopted  adaptation consider training syntactic disam  abstracts  sentences evaluation performance biguation models target domain desire  single tagger sentences       improvement hara et al  explored    result shown table  singlepenn probpenn results suggested small number indomain data  single prob taggers trained penn treebank bring considerable improvements performance  singlebio probbio trained biomedical outofdomain data large penn treebank  domain penn bioie corpus genia corpus cause evaluation pos tagged sentences  compared results gold pos tags use auto case task parsing raw texts  matic taggers trained different domain degraded open question  formance   points measure drop  recovered using welltuned taggers remarkably conclusion  results probbio  points lower  bound difference processing time paper demonstrated accuracy parsers                                                        improved allowing pos taggers output mul    ﬁgure signiﬁcant tiple answers words performance  show results experiment using test set parser evaluated terms precisionrecall predicate  signiﬁcant improvements                              argument relations performance parser    improvement prob single signiﬁcant    −    according stratiﬁed shufﬂing tests cohen  niﬁcant p− according stratiﬁed shufﬂing tests    improvement terms recall prob single sig improvement precision signiﬁcant                                                    ijcai                                                    
