 real boosting      la carte   application boosting oblique decision trees              claudia henry             richard nock                   frank nielsen               antillesguyane  ceregmia                       sony   cs labs              schoelcher martinique france                tokyo  japan            chenryrnockmartiniqueunivagfr                 nielsencslsonycojp                          abstract                          simple assumption corresponding boosting algorithm                                                        using recent result nock nielsen      past years boosting major algorithm uses realvalued weak hypotheses does      ﬁeld machine learning classiﬁcation  face repetitive minimization exponential loss faced      paper brings contributions theory algo previous real boosting algorithms friedman et al       rithms ﬁrst unify wellknown topdown   schapire singer  classiﬁer oblique                                            cision tree induction algorithm kearns decision tree algorithm obtained three key features                                               mansour   discrete adaboost freund      fast simple ﬁrst provable boosting algorithm                             schapire   versions    fully exploits class important prob      higherlevel boosting algorithm used   lem inducing oblique decision trees longstanding      basic building block devise simple prov tory algorithms available far time      able boosting algorithms complex classiﬁers  suming complex handle formally breiman et al       provide example ﬁrst boosting al     cant´upaz kamath  heath et al  murthy et      gorithm oblique decision trees algorithm  al  shawetaylor cristianini  follow      turns simpler faster signiﬁ ing section presents deﬁnitions followed section      cantly accurate previous approaches    theoretical results section discusses                                                        gives boosting algorithm oblique decision trees    introduction                                       sections discuss experiments conclude sake  loosely speaking boosting algorithm repeatedly trains clarity proofs postponed appendix  moderately accurate learners gets weak hy  potheses combines output strong classiﬁer  deﬁnitions problem statement  boosts accuracy arbitrary high levels kearns boldfaced variables represent vectors  valiant  pioneering paper schapire  unless stated sets represented calligraphic  proved existence boosting algorithms uppercase alphabets  unless explicitely stated  drew roots popular representative ad enumerated following lowercase xi    aboost builds linear combination predictions    vector sets xi     sets  weak hypotheses freund schapire  consider following supervised learning setting let  paper kearns mansour  proved denote domain observations dimension nsuchasrn  popular topdown decision tree induction algorithms   suppose set  examples                                    boosting algorithms disguise breiman et al   cardinal notation deﬁned  si  xiyi∈  quinlan  kinds algorithms outwardly x×−      discrete dis  different each adaboost repeatedly modi tribution known “” called positive class  ﬁes weights training examples directly minimizes “” negative class primary objective related  aconvexexponential loss topdown decision tree induction building accurate classiﬁer hypothesis  x→r  algorithms modify weights examples goodness ﬁt evaluated quan  minimize expectation concave socalled permissible tities  function kearns mansour                                                                           explains starting point paper quite εh  ew      signhxy    surprising result prove kinds algo                        −                                                                    εexph ew      exp yh         rithms fact algorithm induction                                                                             ≥      −  formed different classiﬁer “graphs” apparent signaiff      ceptual differences algorithms stem different indicator variable expectation   structural properties classiﬁers suggests generic ventional empirical risk  upperbound expo  induction scheme gives classiﬁers meet nential loss schapire singer  interested                                                    ijcai                                                        input                           input                  input                                                     ht ← trees wt                       let  ∈ leavesht− containing let  ∈ leavesht− containing          ←                    −            αt    arg minα∈r ewt exp yαht          examples classes       examples classes                                ht ←  stumps            ht ← stumplss            wti ← wti exp−αtyihtxizt    ←  ht                         ← ht     adaboost  trees                        topdown  dt cart       topdown  odt oc sadt    figure  abstraction  core greedy procedures popular induction algorithms text references details    stagewise greedy ﬁtting scratch large figure  gives abstract view pop  number classes classiﬁers popular induction algo ular induction algorithms core procedure  rithms core procedure follows scheme including induces large classiﬁer adaboost trees freund  decision trees dt breiman et al  quinlan  schapire  schapire singer cquin  branching programs bp mansour mcallester  lan cartbreiman et al ocmurthy et  linear separators ls freund schapire  al  sadt heath et al  ﬁrst algorithm  nock nielsen  virtually procedures share induction odt dates mideighties  commonpoint components used grow descendant cart breiman et al   classiﬁers dts adaboost duction algorithms dt odt integrate pruning stage  trees internal node splits dts bps akin interested greedy induction scheme  decision stumps                                wl various forms induces decision tree    given ﬁxed class classiﬁers ﬁt sample distribution repeatedly modiﬁed  problem statement weak learner wl adaboost induces decision tree single internal  assumed available queried sam node stump subset reaches leaf  noted                                       ple discrete distribution  assump s distribution modiﬁed cart  tion existence called weak learning assump replaces ordinary axisparallel stump linear  tion wla returns polynomial time classiﬁer separator stump called stump sake simplicity  following assumed εh w ≤ oc sadt choice split dt odt   − γ γ     schapire singer  lies repetitive minimization “impurity” criterion  given   possible build polynomial   expectation leaves current tree  time εh ≤   having queried socalled permissible function kearns mansour  times wl classiﬁers hh  ht forsomet     provided additional assumptions generaliza                                                                                                                                  tion abilities section “discussion” algorithm                            s                                                           εimph  fe∼leavesh                   called boosting algorithm freund schapire                                   ws  kearns valiant                                                                                                                                                                                                                                               ws total weight      unifying boosting properties                       weight restricted positive class expec  sake clarity plug subscript tation weight leaf  ws  permissible func                                                      tion   →   concave symmetric  h element ls classiﬁer ht ht                     ∈                                     ex    αtht whereαt    leveraging coefﬁcient amples permissible functions fzz −  interpreted conﬁdence ht element dt gini index breiman et al  fz−z log −  −  rooted directed tree each internal node supports log − entropy quinlan  log base  single boolean test observation variable each                                                                                 −                           leaf labeled real classiﬁcation obser fz optimal choice kearns                                                        mansour  remark εimph wf ≥  vation proceeds following root path             −             tests satisﬁes reaches leaf gives class εimph  min εh                                                         missible  minimizing  minimizing  figure  left presents example dt                                                               empirical risk reason  vv boolean description variables finally oblique  cision trees odt generalizes dt linear combinations better choice focusing directly empirical risk                                                                                                      variables authorized internal nodes allows comes concavity permissible function kearns                                                                          splits oblique instead just axisparallel breiman et mansour   seemingly different  al                                            each adaboost dt induction algorithms                                                        cartfamily independently proven boosting al    sake simplicity polynomial time means polynomial gorithms freund schapire  kearns mansour  relevant parameters paper       schapire singer  sofarnosuchformal                                                    ijcai                                                                                                                                                      input                                                                                                                                                                                                                       compute                                                                                      ht ← wlst  wt                                                                 m          −                                                           −                                                                                       μtyiht iht  ∈s                                                                                    −       si                                                                    ←w    ×        μt                                            αh−                        ti   ti                     ∈ss                          αh αh                                     si                              αh αh −                                                                       ←                      −                                                              αt    ht  ln  μt μt  figure  dt  leaves  internal nodes left genericgreedy  equivalent representation ﬁts right                                                          figure  generic greedy induction ht  compu  boosting algorithm proof exist odt tation st depends decision graph ht  advantage structure combining oblique stumps    decision trees objective let shift                                                                                            we extend weight notation let wt sp   general standpoint problem address following                                                                                            st            ∈pand  question kind classiﬁers used solve  si∈sp       si∈st                                                           ≤   ≤                                    ∈  consider following assumption represents     deﬁne ht maxsi∈st  ht                                    ht                                                   kind local linear classiﬁer                          maximal absolute value ht tafternock                                                        nielsen  deﬁne notions marginstheﬁrstis      assumption ∀t denote ht  hh  ht                                                         normalized margin ht      set outputs wl exists function                                                                                            maps observation nonempty subset    μ                    ∈ −                 x→     ht ∅  ∀x ∈x                                        ti         ght                      ght                wtst                                                                          si∈st      quired computable polynomially size      ht  describes classiﬁcation function ht help deﬁnition figure  presents generic      following way                                greedy induction ht  remark ht linear                                                       separator adaboostr boosting algorithm nock          ht              αthtx  ∀x ∈x         nielsen  second margin deﬁnition mar                        ∈                                                x                        ht ght                          gin ht example  nock nielsen                                                                νt xy    tanhyht ∈ −     means words classiﬁcation ht obtained                                      summing outputs elements  classi margin comes mind relationships  ﬁers satisfy decisition trees decision lists linear boosting logistic prediction ht  logpry   separators                                      xpry  −x friedman et al  case                                                                       x                    −  deﬁnition  decision graph ht isanoriented  νt  ypry         pry                                                        −   graph ht   eanarchtht  ∈eiff tt      expectation normalized margin             ∈x                 ∈                                         −  exists      htht  ght   ht   bayesian prediction    called gentle lo                                                                                           tt   ght                            gistic approximation friedman et al   following                                                        nock nielsen wedeﬁnethemargin error ht  remark acyclic maps each observation                                                      ∀θ ∈ −   path gwealsodeﬁnep   representing                                  set paths mapped ght                    νwht θ    ew νt s≤θ                    ⊆h     ∃x ∈x                                            ≤                           ght             εht       νwht  minimizing                                                                                                                                                    νwht  minimize εht     simplest case obtained                                                      following theorem shows weak  ht  ∀x ∈x ht  linear separator asinglepath                                                        conditions show νw minimized   examples classes classiﬁers different                          θ                                                     values θ ∈ −  θ   linear separators ﬁt include dt odt  case tree weak hypotheses fact theorem  ≥  queries wl classiﬁer ob                                                        tained  satisﬁes  stant predictions tree nodes figure  right displays            way represent dt meet remark                                                                                                         ≤  θ    ×                  −     exists possible equivalent transformations νw                sp         μ                                                                   θ    − θ                     dt way induce ht shall favor single                   p∈p        ht∈p  follows deﬁne types subsets                                                        θ ∈ −   ∈pand   ≤ ≤                                                         proof appendix read theorem  consider follow                x   ∈s                               iyi      ght          ing wla realvalued weak hypotheses borrowed                ∪                      p∈pht∈p                       nock nielsen                                                     ijcai                                                                 ≥                  ∀ ≥                                   −                            wla  μt   γforsomeγ                    arg minr ew exp yht  figure  matches                                                        exactly discrete adaboost freund schapire   wla theorem     states νw    ≤                                            θ         decision trees linear separators ex   exp− min ∈p pγ constant θ    θ                          θ                      tremal classiﬁers respect finally  constant ∈ −  words provided                                                                                                 linear separator restriction weak hypotheses  duction performed paths roughly                                                           specializes adaboostr nock nielsen   equal size breadthﬁrst induction decision  graph margin error guaranteed decrease exponen  boosting algorithms  tially fast ﬁts treeshaped decision graphs  consider following assumption                    original boosting setting examples drawn                             ∈h                         dependently according unknown ﬁxed distribu      assumption each ht constant ii tion  goal minimize true risk      rooted tree                                                        εht high probability basically wish  assumption basically restricts ht tree arity εht ≤  probability ≥  − δ sampling  kind classiﬁer used split freund schapire  kearns valiant                                       −  internal nodes  use notation index’ sufﬁcient conditions polynomial time induction  weight class “” “” respectively          algorithm satisfy constraint return ht                                                        εh    ii prove structural parameters  theorem  suppose hold                                                                      class classiﬁers ht belongs satisfy particular                                                       bounds freund schapire  shawetaylor cris                               wst                ht       ln −                 tianini  theorem  prove holds                                                                                                                              st                   fairly general conditions algorithm figure                                                         provided wla holds example γlog  ∀x ∈xand     leaf furthermore                                                                                                         γ  simpliﬁes                                         erations ls  dt                                                                ≤                                                      εht      theorem  fixing mini wi easily                                                        yields bound ls adaboost dis                                                               ×     st   −   st       crete real schapire singer  dt   εexpht        wst                                                    wst      wst                                                          ht leaf                             improves exponent constant kearns man                                                       sour  finally ii immediate follows                                   −                  εimpht             mild assumptions wl simple matter fact                                                        ii hold inducing odt  proof appendix                                                          recursive boosting oblique decision    discussion                                              trees    kearns mansour’s algorithm adaboost         preceeding subsections suggest used                                                              build ht  core procedure wl  similarity   fz − example comes theorem  adaboost  trees                                                                                                 immediate quite surprising shows identity pruning figure  equivalent growing lin                                                                                       tween convex loss expectation concave loss ear separators wl growing decision tree  coincidence theorem  shows wl returns constant weak hypothesis  surprising result choice weak hypotheses general case obtain recursivecomposite design                                                        “master” gviag recursion  does impact ht  aandb                                                        end reach wl afford exhaustive search  hold way modify ht through deci  sion graph choice splits tree simple classiﬁers axisparallel stumps constant classi                                                                                                       simple way choose ﬁers instead calling howeverg  thing popular ls boosting algorithms friedman used build decision graph ht inthesame  et al  schapire singer  repeatedly mini recursive fashion consider example class odt  mize exponential loss  theorem  internal nodes’ splits local classiﬁers ls decide  amounts minimization impurity criterion  path based sign output equivalently                     −                                  class suggest build tree  fz      zthisisexactly dt induc                                                              linear separators internal nodes ht  tion algorithm proposed kearns mansour   meets representation optimal bound               use   split leaf  linear separator uses ordi                                                        nary decision stumps tree shape    hand  ht  linear separator  inﬂuence decision graph induc boostodt odt induction algorithm turns  tion  single path    brings boosting algorithm takes advantage                                                   odt structure time assume  way modify ht  through choice  weak hypotheses suppose each weak hypothe  wla  level deeper stumps linear  sis output restricted set classes −  separators splits oblique decision tree  case αt  ln − εht wtεht wt  theorem  boostodt boosting algorithm                                                    ijcai                                                                                             xd              xd             xd         domain               boostodt       oc   adaboost                                                                                                                                     adultstrech                                                                         breastcancer                                                                    breastcancerw                                                                        bupa                                                       colic                                                                             colicorig                                                                             credita                        figure  margin distributions boostodt creditg                        domain xd  left  center  class diabetes                       noise right stairshaped bold plain curves theoret hepatitis             ical margins logistic model text        ionosphere                                                                                 krvskp                                                                                   labor                           proof omitted lack space builds ledeven                   orem  plus lemma  mansour mcallester  ledeven                  structural arguments freund schapire  monks                           shawetaylor cristianini  proof emphasizes monks                          relative importance sizes suppose each linear sep monks                                                                            mushroom                            arator contains number weak hypotheses  tree complete internal nodes having parity                                                                          sick                                                                          log    Ωγ  log             sonar                                                                                  vot                               εht  ≤  experimental xd                               standpoint suggests build trees  yellowsmall                                                                               wins                         experiments                                         wins                      performed comparisons testbed  domains  classes uci table  results  domains each domain bold  repository ml databases blake et al  comparisons faces indicate lowest errors each row “wins  performed tenfold stratiﬁed crossvalidation  z” bold faces denote number times  oc adaboost weak learner corresponding algorithm column best three                                 ∈         pruned boostodt ran     weak   columns boostodtt    oc adaboostc  hypotheses linear separators decision stumps ∈  furthermore numbers parenthe   make fair comparisons ran adaboost  ses each row number signiﬁcant wins student  total number boosting iterations brings paired ttest   boostodt vs oc boostodt vs  fair comparisons observation classiﬁed  nodes adaboostc oc vs boostodt adaboostc vs  including leaves boostodt  unpruned trees ad boostodt left right  aboost looking results boostodt proven  faster oc practice times                                                         margin error curves domain xd variable class  faster oc’s time complexity onm log murthy                                                                                           et al  guarantee result boos noise nock nielsen  description                                                        domain averaged test folds nock nielsen  todt’s onm wla guarantee                                                              reach empirical consistency case complexity reduc  themargin curve obtained compared                                                                                               tions order magnitude respect logistic prediction friedman et al    svm based induction odt shawetaylor cristianini computed exactly approximation logistic model   table  summarizes results obtained rejec boostodt quite remarkable margin curves  tion probabilities ranging  display single stairshape theoretical logistic model                                                        domain xd  additional class noise uni   hypothesis boostodt does perform  better sign tests comparing runs boostodt formly distributed odt leaves  opponents display better performances  conﬁrmed student paired ttests conclusion  tell simulated domains boostodt performs main contribution paper show  better domain gets harder case formal boosting reach using uniﬁed algo  monks domains ledeven domains boostodt   rithm wide variety formalisms restricted  beaten opponents ledeven beats popular included paper decision lists  ledeven ledeven  irrelevant variables rivest  simple rules nock      looking simulated domains drilled contribution quite surprising show boosting al  results boostodt using  plotted figure gorithm follows immediately complex combinations                                                    ijcai                                                     
