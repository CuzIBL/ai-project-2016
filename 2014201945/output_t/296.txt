          semisupervised learning explicit misclassification modeling                                      massihreza amini patrick gallinari                                      university pierre marie curie                                    science laboratory paris                                  rue du capitaine scott paris france                                        amini gallinari poleialipfr                          abstract                          mixture component unlabeled data belong                                                          components using expectation maximization em       paper investigates new approach training                                                          algorithm dempster et al  proposed approaches usu•      discriminant classifiers small set la•                                                         ally attempt optimize likelihood labeled      beled data available large set                                                          unlabeled data starting initial labeling ap•      unlabeled data algorithm optimizes clas•                                                         proaches proceed computing each estep tentative       sification maximum likelihood set labeled                                                         labels unlabeled data using current parameters       unlabeled data using variant form clas•                                                         model update mstep parameters us•      sification expectation maximization cem algo•                                                         ing estimated labels departure point       rithm originality makes use un•                                                         work amini gallinaria  proposed semi      labeled data probabilistic misclassification                                                          supervised discriminant algorithm using variant form       model data parameters label                                                         cem algorithm celeux govaert  discrimina•      error model learned classifier                                                          tive approaches attempt estimate directly posterior       parameters demonstrate effectiveness                                                          class probabilities considered superior generative       approach datasets show advan•                                                         models compute posteriors learning class       tages method previously developed                                                          conditional densities tests different datasets amini       semisupervised algorithm does consider                                                          gallinari  led conclusion semi      imperfections labeling process                                                          supervised learning like methods each step                                                          algorithm model computes tentative labels unla•   introduction                                         beled data extend incorporating                                                          model takes account label errors provides   reallife applications labeling training data learn•                                                         unifying framework semisupervised learning learn•  ing costly realistic prone error                                                          ing label noise knowledge form model   example rapidly evolving data bases available                                                          studied algorithm case   web time label data differ•                                                         logistic classifier convergence proof   ent information needs cases like medical diagnosis                                                          general case show experimentally modeling   biological data analysis labeling data require                                                          stochastic labeling noise increases notably perfor•  expensive tests small labeled data sets                                                          mance especially small labeled datasets avail•  available cases like object identification images                                                          able paper organized follows make brief   noise inherent labeling process                                                          review work semisupervised learning learning   statistician pattern recognition communities                                                          presence label noise section  section  present   consider problem forming discriminant rules us•                                                         formal framework model section    ing partially classified random misclassified training                                                          semisupervised approach propose finally present   data order cope type situation                                                          series experiments data sets   recently idea motivated ma•  chine learning community papers deal   subject use partially classified data training  related work   known semisupervised learning subject   intense studies  recently  learning labeled unlabeled data   resurgence training misclassified data idea using partially labeled data learning started   called learning presence label noise            statistician community end seminal   consider semisupervised learning classification paper day  presents iterative emlike approach   approaches problem make use mixture  learning parameters mixture density as•  density model mixture components identified sumption multivariate normal components common   classes labeled data known belong exactly covariance matrix groupconditional distributions       learning                                                                                                 iterative algorithms building maximum likelihood     semisupervised probabilistic model    classifiers labeled unlabeled data based     mislabeling    type assumption followed oneill  mclachlan    ganesalingam  authors suggested        consider problem semisupervised learn­   updating procedures nonormal group conditional densi­    ing start logisticcem algorithm described    ties using example kernel methods modeling mixture    amini gallinaria  generic scheme    components imurray titterington     sense used discriminant classifier esti­   considerably fewer work discriminative approaches      mating posterior class probabilities classifier    fundamental paper logistic regression anderson sug­   trained labeled data alternates steps con­   gests modifying logistic regression classifier incorporate vergence local maximum classification maximum    unlabeled data order maximize likelihood function   likelihood cml criterion symons  unlabeled data    anderson                                              labeled using output current classifier clas­                                                                 sifier parameters learned maximizing cml   semisupervised paradigm recently rediscov­                                                                 function computed using known labels labeled data   ered machine learning community papers pro­                                                                 current estimated labels unlabeled data algo­   pose mixture density models combining labeled unla­                                                                 rithm each iteration labels computed unlabeled data   beled data classification miller uyar  consider                                                                  considered desired outputs   mixture density model each class described sev­  eral component densities roth steinhage  pro­      suppose labels labeled dataset cor­  pose kernel discriminant analysis extension classical rect each step algorithm labels computed   linear discriminant analysis framework used  unlabeled data subject error propose model   semisupervised learning fnigam et al  propose   imperfection labels using probabilistic for­  semisupervised em algorithm essentially similar   malism learn semisupervised classifier taking   mclachlan  makes use naive bayes      account labeling errors according error model   estimator modeling different densities present   parameters error model classifier   empirical evaluation text classification tasks au­   learned simultaneously new algorithm compared   thors make use discriminant classifiers instead model­   baseline algorithm amini gallinaria  ex­  ing conditional densities example joachims  pro­   plicitly account fact current classifier   pose transductive support vector machine finds pa­     optimally trained each step data labels   rameters linear separator using labeled data missing estimated   training set current test data class un­  following present general framework   known iblum mitchell  introduce cotraining     learning criterion model   paradigm each sample supposed described                                                                   general framework   modalities classifiers used each   modality operating alternatively teacher student suppose each example belongs   framework used unsupervised semisupervised     class available set labeled exam­  learning based multimodal framework introduced        ples set unlabeled examples du discrim­  blum mitchell  muslea et al  propose    inant classifier trained basis  ra   combine active semisupervised learning     ddimensional feature vectors each labeled ex­  different authors proposed semisupervised learn­   ample xt  let ct respectively   ing schemes usually follow ideas       class label indicator vector class associated xt       learning imperfectly labeled data                     suppose each unlabeled example xi du                                                                  exists perfect imperfect label respectively denoted ci                                                                  ci propose model imperfections labels   practical applications pattern recognition like image   following probabilities   classification problems motivated early   work problem learning presence mislabeled                                                                                                                            data fully supervised learning chittineni  ob­                                                                 subject constraint   tained error bounds performance bayes   nearest neighbor classifiers imperfect labels krish                                                              nan  considered class classification problem   group multivariate normal mixture training samples   subject random misclassification derived likeli­ order simplify presentation consider fol­  hood estimation parameters titterington  proposed   lowing twoclass classification problem   logisticnormal distribution model worked em   algorithm estimation parameters recently                                                                  restrictive easily extend analysis   lawrence scholkopf  proposed algorithm                                                                  multiclass cases   constructing kernel fisher discriminant training ex­  amples presence label noise                            components discrete                                                                                                                  learning   classification maximum likelihhod estimation          presence label noise    baseline semisupervised discriminant algorithm    conceived extension classification problems    cem algorithm proposed fceleux govaert     clustering cem proposed learning    parameters gaussian mixtures using training criterion    classification maximum likelihood criterion initially de­   scribed symons     mclachlan extended cml cem case    labeled unlabeled data used learning    mclachlan  page  cml criterion    case completedata likelihood writes        algorithm proposed mclachlan  optimizes    criterion mixture normal densities algorithm    training discriminant classifiers labeled unlabeled    data amini gallinaria  make use following    form logclassificationlikelihood                                                                     updating discriminant function                                                                      basis labeled imperfect labeled data                                                                  present iterative discriminant cem algorithm                                                                  learning classifier semisupervised learning in­   writing makes apparent posterior probabilities corporates mislabeling error model training criterion    directly estimated algorithm instead condi­  simplification consider simple logistic classi­   tional densities mclachlan  assumptions      fier anderson  algorithm easily adapted    distributional nature data maximizing lc  training discriminant classifier consider logistic    equivalent maximization lc mclachlan   classifier parameters output    page                                                                   input                                                                                       learned respectively used                                                                  estimatebe                                                                                                                             current partition unlabeled data param­                                                                 eters misclassification model logistic classifier                                                                  iteration algorithm learning criterion                                                                   function  iterative approach adopted    let introduce misclassifiction model                                                                  maximization algorithm  parameters ini­   express second summation                                                                  tialized training classifier labeled dataset      function mislabeling probabilities                                                                                                                       steps iterated convergence criterion    posterior probability correct labels  consider                                                                  lc step classifier considered imper­                                                                 fect supervisor unlabeled data outputs gx                                                                    gx used compute posterior imperfect class                                                                  probabilities pc —  each du                         decomposed regard      labeled according maximum imperfect output   conditional class probabilities                               second step parameters error model                                                                  classifier updated using imperfect labels obtained                                                                previous step labeled data adopted                                                                  step gradient algorithm maximize  advantage   following chittineni  make assumption                                                                  method requires order deriva­  density example given true label does depend                                                                  tives each iteration following lemma provide   imperfect label                                                                  proof convergence algorithm local maximum                                                                  likelihood function semisupervised training       learning                                                                                                                                                                           adopt textspan extraction paradigm amounts                                                                selecting subset representative document sentences                                                                classification problem sentences classi­                                                               fied relevant non relevant extracted summary                                                                problems twoclass classification tasks email                                                                spam cmplg continuous attributes credit                                                                vectors mixed continuous qualitative mushroom                                                                attributes qualitative                                                                each dataset ran  algorithms  semisupervised                                                                learning algorithm using label imperfections baseline                                                                semisupervised algorithm amini gallinario                                                                 fully supervised logistic classifier trained                                                                available labeled data compared performance                                                                algorithms  runs arbitrary training test                                                                splits varying proportion labeledunlabeled data                                                                training set                                                                text summarization represent each sentence using                                                                continuous version features proposed kupiec                                                                et al  characterization given good re­                                                               sults previous work amini gallinarib                                                                 each sentence length li characterized       finally finite number partitions example   classes increasing sequence   takes finite number values converges sta­  evaluation measures   tionary value ■                                             three uci datasets approximately   following section present results     proportion examples each class table  used   datasets using baseline logistic classifier trained                                                                performance criterion percentage good classification   updating scheme presented                                                                pgc defined    experiments    data sets   experiments used spambase credit              text summarization followed summac evalua­  screening mushroom collections                  tion using  compression ratio each doc­  uci repository   computation              ument test set formed summary select­                                                              ing  sentences having higher score respect   language cmplg collection tipster summac   text summarization table  summarizes characteris­  output classifier evaluation compared   tics datasets removed  samples missing   extractive sentences desired summary each   attributes credit data set summarization   document desired extractive summaries generated                                                                abstract each article using textspan align­       ftpftpicssuciedupubmachineleamingdatabases    ment method described banko et al        httpwwwitl nistgoviauirelatedprojcctstipstersummac collection balanced positive negative                                                                                                               learning figure  performance curves  datasets showing classical logistic classifier trained fully supervised scheme   square baseline semisupervised logisticcem algorithm circle semisupervised algorithm label imper•  fections star each point represents mean performance  arbitrary runs error bars show standard deviations   estimated performance     examples pgc meaningless used average preci•        sion ap measure evaluation let number      figures exhibit behavior datasets   sentences extracted arc target semisupervised algorithms baseline classifier   summaries total number sentences extracted    trained labeled training data example   precision defined ratio         consider text summarization using  labeled                                                                sentences algorithm allows increase performance    results                                                   compared fully supervised algorithm trained   each dataset each cross validation  examples    labeled data needed reach   held aside test set vary percentage labeled performance baseline method model uni•  unlabeled data remaining  training set collec• formly better reference models used ex•  tions figure  shows performance test sets periments provides important performance increase es•  datasets function proportion labeled data pecially labeled data available   training set xaxis  means  data interesting situation semisupervised learning   training set labeled training  remaining credit card problem dataset small   used unlabeled training data each experiment   semisupervised learning allows smaller increase per•  carried paired trials randomly selected training formance datasets lot un•  test splits yaxis each point represents mean per• labeled data available semisupervised learning allows   formance  runs error bars correspond reach optimal performance  labeled data   standard deviation estimated performance tibshirani  mushroom set       learning                                                                                                           
