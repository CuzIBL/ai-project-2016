                                    kernel matrix evaluation                                      canh hao nguyen tu bao ho                                       school knowledge science                            japan advanced institute science technology                               asahidai nomi ishikawa  japan                                         canhhaobaojaistacjp                        abstract                          goodness kernel function seen                                                        goodness kernel matrix measuring      study problem evaluating goodness goodness primary various contexts      kernel matrix classiﬁcation task kernel      matrix evaluation usually used expensive ways measure goodness ker      procedures like feature model selections nel matrix kernel function classiﬁcation task differ      goodness measure calculated efﬁciently   ently reﬂecting expected quality kernel function      previous approaches efﬁcient commonly used measures regularized risk sch¨olkopf      kernel target alignment kta cal smola  negative logposterior fine schein      culated time complexity kta    berg  hyperkernels ong et al  mea      widely used show  sures speciﬁc value assert certain cri      drawbacks propose efﬁcient surrogate mea  teria form regularities certain spaces example      sure evaluate goodness kernel matrix  rkhs hyperrkhs kernel measures require      based data distributions classes fea optimization procedure evaluation      ture space measure overcomes    prohibitively expensive incorporated ex      limitations kta possesses prop pensive procedures like model feature selections      erties like invariance efﬁciency error bound techniques like cross validation leaveoneout estimators      guarantee comparative experiments show  radiusmargin bound considered expected qual      measure good indication goodness ity functionals like previously mentioned works      kernel matrix                                    require learning process                                                          used feature model selection processes good                                                        ness measures kernel matrices efﬁciently calcu    introduction                                       lated best knowledge efﬁcient kernel  kernel methods support vector machines gaussian goodness measure kernel target alignment cristianini et  processes delivered extremely high performance al  simplicity efﬁciency kta  wide variety supervised nonsupervised learning used methods feature model selections  tasks sch¨olkopf smola  key success notable methods learning conic combinations kernels  kernel methods modularized modules lanckriet et al  learning idealized kernels kwok  ﬁrst map data usually higher dimensional feature tsang  feature selection neumann et al   space powerful framework different data kernel design using boosting crammer et al   types second use linear algorithm fea work ﬁrst analyze kta show having  ture space efﬁcient theoretical guarantees high kta sufﬁcient condition good ker  shawetaylor cristianini  linear al nel matrix necessary condition possible  gorithms kernelized major effort kernel meth kernel matrix good performance  ods designing good mappings deﬁned kernel kta low propose efﬁcient surrogate          φ  functions                                            measure based data distributions feature space                      φ  →                     relax strict conditions imposed kta measure  original data space feature space invariant linear operators feature space  reproducing kernel hilbert space rkhs tains properties kta efﬁciency er  manydimensional space algorithm takes ad ror bound guarantee measure closely related  vantage kernel trick operate solely kernel ma works worstcase error bounds distance metric  trix induced data                               learning show experimentally new measure                                                        closely correlated goodness kernel matrix              φxiφxj injn      conclude ﬁnally future works                                                    ijcai                                                       kernel target alignment                            svms varying data feature space separat  kernel target alignment used measure kernel ing hyperplane affect margin data varies  matrix aligns target kernel cristianini et al perpendicular direction separating hyperplane   alignment target deﬁned normalized margin changed kta does  frobenius inner product kernel matrix account variances different directions  covariance matrix target vector interpreted second condition strict applicable  sine distance bidimensional vectors cases requires mapped vectors differ  ﬁrst introduce notations                        ent classes additive inverses each ideally    training example set xiin ⊂ correspond kernel function maps examples class vector                                                    feature space kernel matrix evaluated  ing target vector  yiin ⊂−   suppose                                                        optimal reason having high kta     yn  yn     ynn−  −  examples belong class  n− examples belong class − sufﬁcient condition necessary condition use   n−   feature map φ kernel matrix examples show limitation quantitatively                                                                                                 φ  deﬁned                                              example   best case kernel function maps                                                        examples class  vector φ ∈ examples              φx φx                          ij            injn      class − vector φ− ∈ assume φφ                                                         φ  φ       φ  φ   α  −   α           α    frobenius inner product deﬁned                −   −        −                forany                                                          kernel matrix evaluated optimal                           n n               k k∗           k∗                 duced optimal feature map kta value                                ij ij                                                                                   −   α                                                                ak y         −       −                                                                                                               target matrix deﬁned yy  kernel target align                                                                                                         n− nn−α    ∗  ment deﬁned normalized frobenius inner prod  uct                                                                                             α                                                      kta  values kernel matrices change varies                           k yy f                           −           ak y                                        value range kta                                                                     k kf yy yy f             value kernel matrix optimal feature function                                                                                                                                                                 lim    ak   n−n−                  n−n−    advantage makes kta popular feature α→                  kta ranges    model selection methods satisﬁes prop  case  erties efﬁcient computational time onitis example  worst case kernel function φ maps half  highly concentrated expected value giving high examples each class vector φ ∈  probability value close true alignment value half vector φ ∈ assume φφ  φφ   gives error bounds meaning kta  φφ  α −  α  foranyαthekernelma  high expect error rate using kernel trix evaluated low induced  limited certain                       worst kernel function fuses classes    claim having high value aky generalization ability kta value  sufﬁcient condition necessary condition               −   α  good kernel matrix given task speciﬁed target                     −                                                                          ak                                                                                                              α  yas    ak   whenak bi                             ∗                             yyt  dimensional vectors    linear constant                                                                                                                     n−n−  optimal alignment happens ij   jthisis    shown limα→ ak        equivalent conditions                                                        limit best case kta                                                                          n−n−    examples class mapped case ranges  similar caution      vector feature space kij xixj meila  best worst cases cover      class                                           range kta values case coincide    mapped vectors different classes feature kta mistake case                                                        best worst      space additive inverse kij  − xixj differ      ent classes                                       example  popular case consider case ker                                                        nel function maps examples orthant feature    sufﬁcient condition having high space inducing kernel matrix nonnegative entries  expect good performance classes collapsed point kij   case  feature space violating conditions                      mean kta penalized necessary                                                                                                                 n−  condition analyze                                       ak                            ﬁrst condition implies withinclass variance                         penalizes kta gap condi                                                        proof derived using fact kij    tion concept margin support vector machines cauchyschwarz inequality gradshteyn ryzhik                                                                                     ak  √    cristianini et al itis−  ak inequality special case  −      comes fact positive semideﬁnite kta kernel matrix ranges  andthe                                                    ijcai                                                                                                           calculate withinclass variance classes di                                                                                           φ−−φ                                                        rection class centers denote φ−−φ                                                        unit vector direction total withinclass variance                                                        classes direction                                                                                                                                                                                                                                               iφxi − φe                                                                    var                                                                                                         −                                                                                                                                                                                                                                                                    φxi − φ−e                                                                                                                                                                                                                                                                  n− −    figure  data variance direction class centers calculate ﬁrst term equation  total                                                        withinclass variance class  direction                                                        φ φ− denoted var  higher better example means types ker                                                                          n  nel functions bounded kta values matter                                good kernel functions drawback kta − var    φxi − φe  unfortunately type kernel function extensively used                                                                      n  practice gaussian kernels shawetaylor cristianini    φx  − φ φ  − φ                                                                         −                                                                                                       kernels deﬁned discrete structures       φ− − φ  gartner et al  distributions jebara et al n                                                                  φx φ   φ − φx φ  − φ φ    fall category                                         −                −                                                                                            examples show kta mistake ker                   φ− − φ                                                                                                                                                                          pn                                                                                                                      φxj   nel matrix best worst kta bound                 φxj         jn                                                          substitute φ          φ−   numbers popular kernel matrices pos                                    n−  itive entries gaussian kernels discrete equation  deﬁne auxiliary vari  structures distributions kta disadvantage use ables follows                                                                           pn             pn  kernels                                                              φxiφxj  kij                                                          • ai  φxiφ                                                                                                                                                                                      pn                 pn                                                                                   φxiφxj         kij    feature spacebased kernel matrix                                       jn             jn                                                          • bi  φxiφ−                                   evaluation measure                                                           n−              n−                                                            section introduce new goodness measure                  pn             pn                                                                             φxiφxj  kij                                                          • ci  φxiφ                 kernel matrix given binary classiﬁcation task                                                                                                                               named feature spacebased kernel matrix evaluation mea                                                                                                                       φxiφxj         kij                                                                             jn             jn  sure fsm measure computed ker • di  φxiφ−                                                                                                             −             −  nel matrix efﬁciently overcome limitations kta         pn         pn                                                                                                       ci  idea use data distributions feature space                                                                      denote                         n−                                                                 pn  speciﬁcally factors taken account          di                                                                                                                                          φφ      φφ−  withinclass variance direction class cen     n−                                                                                                                     ters  relative positions class centersthese  φ−φ− φ− − φ   − −  factors depicted figure  ﬁrst factor improves                                                        plugging equation   ﬁrst condition kta allowing data vary cer                 −  −                                                              − var                  tain directions second factor solves problem im                    − −            posed second condition kta let var total  withinclass variance classes direction use similar calculation second term equa                                                        tion   tween class centers denote center classpn mean                                                         φx                                                                                                                             ci − di  −                                  φ                                       class data feature space            − var                                     pn                                                   −        −                                                φxi                                                            − −           φ−                evaluation measure fsm             n−                                           proposed kernel matrix evaluation measure fsm  ﬁned ratio total withinclass variance deﬁned formula  calculated  direction class centers distance                     var  var−                                                                  fsmk y√                           class centers                                                               − −                                var                     fsmk     smaller fsmk better              fsmk                                                     φ− − φ                                                                            easily ai bi ci di    fsm calculation                                  calculated time complexity pass  show evaluation measure calculated using through kernel matrix evaluation measure  kernel matrix efﬁciently simple formulas ﬁrst efﬁciently calculated time complexity                                                    ijcai                                                       invariance                                                                            fsmk                               properties         regarding linear                                  −kta  operations make closer svms kta                                    fsmerr                                                                                      error    theorem  fsmk   invariant translation  rotation scale feature space                           sketch proof fsmk scale invariant owing                               −  −   fsmk  normalization factor                                     rotation translation invariant built building  blocks ai − bi di − ci                                     performance svms kernel methods           unchanged rotation translation scale feature                                                                   space reasonable ask measures  properties kta rotation invariant     translation invariant                                                                                                                              error bound                                                                                                  β  show kernel matrix possible obtain figure  results synthetic data different values  training error rate bounded propor  tionate fsmk means low fsmk  value guarantee low training error rate case scale relaxes strict conditions kta considering  expect low generalization error rate           relative positions classes feature space ex    theorem   exists separating hyperplane amples section  fsm values best cases                                                                                        ∞  training error bounded                  worst cases  pop                                                       ular case bounds measure                           fsmk                    similarities works             fsmerr                                                    fsmk                     minimax probability machine mpm lanckriet et al                                                         controls misclassiﬁcation probability worstcase    proof use onetailed version chebyshev’s setting worstcase misclassiﬁcation probability min                     equality feller   consider data distribution imized using semideﬁnite programming worst case  direction class centers each class data dis determined making assumption mean  tribution mean φ φ− correspond             var    var                                 covariance matrix each class mpm similar  ing variance     − following inequalities approach measure shows worstcase mis  derived                                           classiﬁcation error difference mpm considers                                                       data variance directions using covariance ma    prφ − φ− −e  kvar−φ ∈ class−                                                    trix measure takes variance direc                                                       tion class centers reason    prφ − φ  e  kvar φ ∈ class                                                       measure lightweight version mpm calcu                                                    lated efﬁciently special case mpm direction                                                        class centers contains eigenvectors covari    separating hyperplane takes norm vec ance matrices objective function mpm optimizes  tor intersects line segment φ φ− equivalent measure              h−φ  var  point h−φ− var−  formula          context nearest neighbor classiﬁcation                                                        criteria learning good distance function transform                       var−φ   varφ−         fxex  −                          data examples class collapsed                          var  var−                   point globerson roweis  measure                                                        shows quantitatively class collapses  error rate hyperplane each class bounded                                          φ −φ       key difference method objective  ing inequalities equation   −                                          varvar−      class collapses point measure shows        fsmky  total training error rate class collapses hyperplane differ  classes bounded                           ence explains method applied nearest neighbor                                                        classiﬁcation measure kernel methods                        fsmk                                                         fsmk                                                                        experiments    discussion                                       purpose measures predict efﬁciently  measure interpreted ratio withinclass good kernel matrix given task using kernel methods  variance betweenclass variance indicates compare measures facto standard cross  classes separated advantageous kta validation error rates svms mimicked model se  cause takes account withinclass variances ﬁner lection process choosing different kernels monitored                                                    ijcai                                                                   australian            breast−cancer             diabetes                fourclass                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           lin  poly rbf   tanh  lin  poly  rbf   tanh  lin  poly  rbf   tanh  lin  poly  rbf   tanh                   german                  heart                 ionosphere               vehicle                                                                                                                                                                                                                                                                −kta                                                                            fsmerr                                                                                        error                                                                                                                                                                                                                                                     lin  poly rbf   tanh  lin  poly  rbf   tanh  lin  poly  rbf   tanh  lin  poly  rbf   tanh                                      figure  results different uci data sets    error rates baselines kta fsm different β expected fsmerr stable  reﬂect baselines used times fold stratiﬁed varying similarly error kta changes dramatically  cross validations estimate error rates          concluded kta    facilitate visual inspection results instead sensitive absolute positions data feature space  showed following quantities −ktabfsmerr conﬁrms claim limitations kta  deﬁned error bound based fsm measure formula shows measure robust   error cross validation error rates reason  quantities range   relate  real data  expected error rates kernel function sense showed advantage proposed measure kta  smaller values better kernel ﬁrst real situations selecting popular data sets  quantities expected correlated uci collection experiments data names experi                                                        mental results displayed figure  data normalized    synthetic data                                   −  classes grouped make binary classiﬁcation  generated synthetic data used linear kernels problems chose types kernel model selection  simulate different data distributions different kernels linear kernels polynomial kernels degree  scale   feature space each data set parameterized angle rbf gaussian kernels default tanhyperbolic kernels                                                                 β used gaussian distributions ﬁxed variance default  described ran cross validations                                                     directions classes centered φ   ∈ displayed results different kernels                                                  class  φ− cosβsinβ   ∈       following conclusions observed  class − each class contains  training examples graphs  variances gaussian distributions determined • rbf kernels approximately lowest cross vali                   var  var−     φ− −φ  ensures β dation error rates   data sets shows  data set images linear operation rbf kernels usually good choice kta  data sets linear kernels equivalent makes rbf kernels worst values kta  data set different kernel functions β highest ones   cases measure  problems level error rates agrees error rates cases making  measures using linear kernels linear kernels reliable kta  evaluated level goodness run •                         β                                  similarly tanh kernels perform worst cases  experiments different values      kta values best ones   degrees turn results kta fsmerr  error shown figure                          default parameter values set libsvm environment    figure  observe error stable httpwwwcsientuedutw˜ cjlinlibsvm                                                    ijcai                                                     
