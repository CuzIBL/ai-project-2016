                       fusion stacking dynamic integration                                         niall rooney david patterson                             northern ireland knowledge engineering laboratory                                 faculty engineering university ulster                               jordanstown newtownabbey bt oqb uk                                   nfrooney wdpattersonulsteracuk                           abstract                          important base learning models sufficiently ac                                                        curate diverse predictions krogh  vedelsby                                                          diversity terms regression usually                                                         measured ambiguity variance predictions         paper present novel method    achieve terms homogeneous learning methods         fuses ensemble metatechniques stack    exist manipulate each model training         ing dynamic integration di regres    data through instance feature sampling methods ma        sion problems adding major         nipulate learning parameters learning algorithm                                                         brown et al provide recent survey ensem        computational overhead intention                                                         ble generation methods         technique benefit varying                                                        having generated appropriate set diverse suffi        formance stacking di different       ciently accurate models ensemble integration method         data sets order provide robust    required create successful ensemble well–known         technique empirical analysis   approach ensemble integration combine models         technique referred weighted meta –    using learning model process referred         combiner wmetacomb compare       stacking wolpert  forms metamodel created         formance stacking di technique    metadata set metadata set formed pre        dynamic weighting selection em       dictions base models output variable each         pirical analysis consisted sets ex  training instance consequence each training        periments each experiment recorded        stance xy   belonging training set  meta                                                                   ˆˆ                             ˆ        crossfold evaluation each technique instance mifxfxy  formed         large number diverse data sets      prediction base model ii nwhere size         each base model created using random fea    ensemble ensure predictions unbiased         ture selection base learning al   training data divided fold crossvalidation         gorithm each experiment differed terms   process subsets each iteration cross        base learning algorithm used     validation process subsets removed         demonstrate each evaluation wmeta    training data models retrained remaining         comb able outperform di stack       data predictions each base model         ing each experiment fuses   held subset  metamodel trained accumu        underlying mechanisms successfully         lated metadata completion cross validation                                                         process each base models trained                                                         training data breimanb showed successful appli                                                        cation method regression problems using linear    introduction                                        regression method metamodel coeffi                                                        cients linear regression model constrained   objective ensemble learning integrate number nonnegative specific requirement   base learning models ensemble generali use linear regression metamodel hastie et al    zation performance ensemble better linear algorithms used particularly   individual base learning models base learning models area classification seewald  džeroski    created using learning algorithm ensemble Ženko    learning schema homogeneous heterogene technique appears similar scope   ous clearly case important base stacking seen variant thereof pro  learning models identical theoretically posed tsymbal et al referred dynamic inte  shown ensemble effective gration technique runs jfold cross validation                                                       ijcai                                                    process each base model process  methodology   creates uses metadata different each meta  instance consists following format section process wmeta                                                      comb training phase wmetacomb shown fig    xerr   errn    erri   predic  tion error each base model each training ure     stance  forming metamodel based   derived metadata dynamic integration uses algorithm  wmetacomb training phase  nearest neighbour knn lazy model nearest input                                                                ˆ sr ˆ di ˆ  neighbours based original instance feature space output   fii  subspace metadata given test  step       stance determines total training error base partition  j–fold partitions dij  models recorded metanearest neighbour set initialise metadata set mdsr ∅  level error used allow dynamic selection                                                            initialise metadata set md di ∅  base models appropriate weighted combination    select number base models make predic     dd  tion given test instance tsymbal et al    train                                                               dd  scribes three dynamic integration techniques classifica  test                                                                               ˆ  tion adapted regression problems        build learning models fii  using dtrain  rooney et al  paper consider variant                                                                           each instance dtest  techniques referred dynamic weighting                                                           form metainstance mi     sr  ffyˆˆxx   selection dws applies weighting combination                                                                                                                       di   base models based localized error make   form metainstance mi    xxerr   errn    prediction excludes base models   add mi      sr mdsr  deemed inaccurate added weighted combina   add mi     di md di  tion                                                     endfor  method injecting diversity homogeneous ensem endfor  ble use random subspacing ho ran build sr metamodel fˆ sr using metadata set mdsr  dom subspacing based process each base                                                            build  di metamodel fˆ di using metadata set md di  model built data randomly subspaced features   tsymbal et al  revised mechanism vari  step                                                              ij   able length features randomly subspaced shown                                                                method effective creating ensembles test    integrated using di methods classification          determine totalerrsr totalerrdi using dtest  shown empirically stacking dynamic   step     integration based integrating models generated using                                                                   random subspacing sufficiently different each   dtraindd                                                                 varying generalization perform ddtest  ance data sets rooney et al  main                                                             build learning models fˆ  using  computational requirement stacking dynamic                      ii train                                                                          integration use crossvalidation base models each instance dtest  form metadata requires little additional computa  form metainstance mi sr  ffyˆˆxx   tional overhead propose ensemble metalearner                                                                                      form metainstance mi di  xxerr   err    does cross validation ensemble base mod                                els consequence able create stacking  add mi sr mdsr  metamodel dynamic integration metamodel  add mi     di md di  required knowledge ensemble metalearner use endfor                                                                    ˆ  models form predictions test instances focus retrain fii   paper formulation evaluation build sr metamodel fˆ sr using metadata set mdsr  ensemble metalearner referred weighted meta                                                                           ˆ di                 di  combiner wmetacomb investigated wmeta     build di metamodel using metadata set md  comb average outperform stacking regres figure  training phase wmetacomb   sion sr dws range data sets range   base learning algorithms used generate homogeneous step training phase wmetacomb consists   randomly subspaced models                            building sr di metamodels folds                                                         training data step  second step involves testing                                                         metamodels using remaining training fold indica                                                        tor potential generalization performance  im                                                        portant models created                                                       ijcai                                                    using complete metadata tested using data normerror− normerror                                                                         trained order reliable estimate                                                                                              test performance performance metamodels   recorded total absolute error totalerror                                                  sr      wmetacomb forms prediction test instance   totalerrordi step  metamodels  built using entire metadata completed step  following combination predictions meta  computational overhead wmetacomb comparison  models   sr centres cost training addi    tional metamodel additional metamodel    based lazy nearest neighbour learner cost trivial     ˆˆsr             di                                                           norm mw   norm  mw    prediction wmetacomb simple     weighted combination metamodels based   total error performance determined step  denote   totalerr totalerrorsr totalerror di   individual   experimental evaluation analysis   weight each metamodel determined finding experiments investigated performance   normalized error each metamodel                 wmetacomb comparison sr dws work                                                         carried based appropriate extensions weka               normerrorii totalerror  totalerror                 environment implementation dws en                                                 semble sets consisted  base homogeneous models   normalized weighting  each metamodel given data each base model randomly sub                                                        spaced tsymbal et al  note random subspacing            − normerrori                                 pseudorandom process feature subset gen     mw                                                                                erated each model ensemble set                                                         ensemble sets created using different base learning algo                                                        rithms data set metalearner sr                   mw                                    stacking component wmetacomb based      norm mw                                     model tree quinlan  larger hy                 mw                                                      pothesis space linear regression number near                                                                        est neighbours dws knn metalearner set                                                          performance dws affected size   equation  influenced weighting mechanism                                                          particular data sets particular choice   wolpert  macready  proposed combining base                                                         data sets previous empirical analysis shown   models generated stochastic process used                                                         value  gave overall relatively strong performance   bagging  breimana                                                         value during training phase each meta  weighting parameter determines relative                                                         technique set    influence each metamodel based normal                                                        order evaluate performance different ensem  ized error suppose          normerror                                                          ble techniques chose  data sets available               normerror    table  gives normalized   weka environment witten  frank  data   weight values different values  indicates sets represent number synthetic realworld prob  low value  large imbalance weighting lems variety different domains varying   high value  equally weights meta range attribute sizes contain mixture   learners regardless difference normalized errors discrete continuous attributes data sets                                                          instances sampled replacement                                                         reduce size data set maximum                  norm mw   norm  mw                                                          stances order make evaluation computationally                                                                                  tractable missing values data set replaced                                              using mean modal technique characteristics data                                           sets shown table                                                          carried set experiments each ex  table  effect weighting parameter        periment differed purely base learning algorithm                                                        ployed generate base models ensemble each ex  based considerations applied following periment calculated relative root mean squared error   heuristic function setting  increases rrmse setiono et al  each ensemble technique   imbalance weighting dependent great each data set based  fold cross validation meas  normalized errors differ                             ure                                                                                                                ijcai                                                                                                          data sets technique outperformed technique      data set   original  data set number            significantly losses count number data sets                 data set size  input                 technique outperformed technique significantly                 size    experi attributes                         ments     dis  total      ties count significant difference                                 tinuous crete           significance gain difference number      abalone                                  wins number losses zero gain      autohorse                              shown gain  indicative      autompg                                   slight improvement      autoprice                        auto                                    determined average significance difference asd     bankfm                                  rrmse techniques averaged      banknh                                wins losses data sets significant difference      bodyfat                                 shown technique outperformed shown      breasttumor                               significance ratio asd gave percentage measure      calhousing                         cloud                                     degree outperformed technique      cpu                                       outperformed asd value negative      cpuact                                asd course direct indication     cpusmall                              formance techniques supplementary      echomonths                                measure large asd does indicate technique      elevators                      fishcatch                                 performed considerably better difference      friedman                             number wins losses small conversely      housing                                 number wins larger number losses      housel                                asd small reduces impact result      househ                             considerably section divided discussion      lowbwt                                    summary results each individual base learning algo     ma                               chinecpu                                          rithm discussion overall results       pollution                              pyrim                                      base learner knn      sensory                           servo           sleep                                       table  shows summary results evaluation      strike                                    base learning algorithm nn clearly      veteran                                   ensemble techniques strongly outperformed single nn                                                         terms significance wmetacomb shown   table  data sets’ characteristics               largest improvement gain  sr                                                         showed fewer data sets significant improvement   rrmse percentage measure way compari greater asd wmetacomb wmetacomb outper  son ensemble approaches general deter formed dws gain  showed small im  mined rrmse single model built provement sr gain  sr showed signifi  unsampled data using learning algorithm used cance gain  dws   ensemble base models repeated evaluation   times each time using different learning algorithm technique nn sr     dws wmetacomb   generate base models choice learning algorithms   nn                   based choice simple efficient methods                            order make study computationally tractable sr                representative range methods different learn                                                                               dws                  ing hypothesis spaces order determine                              wmetacomb robust method independent base     wmetacomb         learning algorithm deployed  base learning algorithms                  used experiments  nearest neighbours   nn linear regression lr locally weighted regression   atkeson et al  model tree learner table  comparison methods base learning algorithm   results evaluation assessed performing nn    tailed paired test cross fold rrmse   each technique comparison each each data   set results comparison technique   summarised form   winslossesties asd  wins count number                                                       ijcai                                                     base learner lr                                   base learner   table  shows ensembles techniques outperformed   single lr wmetacomb showing largest             technique       sr     dws wmetacomb   improvement gain  asd                               wmetacomb outperformed sr dws larger                                    improvement shown comparison dws sr    sr                                                                                                      terms significance asd sr outperformed dws     dws                  significance gain  asd                                                                                                wmetacomb                                                                                          technique lr      sr      dws wmetacomb       lr                                                   table  comparison methods base learning algorithm       sr                                                              dws                          table  shows reduced performance ensem                                     bles comparison single model base learn      wmetacomb                     ing algorithm fact sr showed improvement                                      wmetacomb dws im                                                        proved terms significance did show asd                                                         large previous learning algorithms reduced   table  comparison methods base learning algorithm                                                         performance indicative case certain   lr                                                         learning algorithms random subspacing able                                                         generate sufficiently accurate base models needs    base learner lwr                                 enhanced using search approaches improve qual  table  shows ensembles techniques outperformed ity ensemble members considered tsymbal et   single lwr wmetacomb showing largest        al   wmetacomb showed improvements sr   improvement significance gain  wmetacomb lesser degree terms significance asd   outperformed sr dws larger improvement     values   shown dws sr terms significance   overall seen wmetacomb gave strongest   asd  sr outperformed dws significance gain  performance ensembles    base learning    asd                                   algorithms nn lr comparison single                                                         model terms significance gain tied compari                                                        son dws lwr   learning algo     technique lwr sr           dws wmetacomb           rithms lr lwr wmetacomb highest      lwr                            asd compared single model wmetacomb outper                                formed sr learning algorithms largest signifi     sr                               improvement sr recorded                                    significance gain  largest asd comparison      dws                                                  sr recorded lwr  lowest signifi     wmetacomb                       improvement sr recorded nn lr                                     lwr significant gain  lowest asd                                                                                                                  wmetacomb outperformed dws base learning   table  comparison methods base learning algorithm algorithms degree improvement larger com  lwr                                               parison dws sr largest significant improve  table  shows ensembles techniques outperformed ment dws shown lr significance   single lwr wmetacomb showing largest        gain  largest asd  lwr low  improvement significance gain  wmetacomb est improvement shown terms gain   outperformed sr dws larger improvement     asd   respectively effect   shown dws sr terms significance   wmetacomb able fuse effectively overlapping   asd  sr outperformed dws significance gain  expertise metatechniques seen   asd                                       case independent learning algorithm employed                                                         addition wmetacomb provided benefit overall                                                         given experiment sr stronger dws                                                       ijcai                                                    
