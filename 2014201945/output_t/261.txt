                   analogical learningin turnbased strategy game                                 thomas hinrichs kenneth forbus                           qualitative reasoning group northwestern university                                              sheridan road                                             evanston il                                    thinrichs forbus northwesternedu                         abstract                            paper integration analogy                                                         experimentation qualitative model city manage      key problem playing strategy games learn ment support planning learning game       ing allocate resources effectively                                                          freeciv freeciv   rest section     difficult task machine learning                                                         line context work providing brief synopsis       connections actions goal outputs  freeciv domain htn planner analogical rea      indirect complex  show combina                                                        soning experimentation        tion structural analogy experimentation                                                         using qualitative model city economics support credit       qualitative modeling used improve  assignment avoid local maxima        formance optimizing food production strat                                                        experiments demonstrate ability transfer       egy game  experimentation bootstraps case li                                                        adapt prior training different cities close dis      brary drives variation analogical reason cussing related work future plans       ing supports retrieval transfer  qualitative       model serves partial domain theory support  freeciv domain      adaptation credit assignment                                                          freeciv opensource turnbased strategy game modeled       techniques enable learn effects                                 ™      actions ranges quantities apply sid meiers series civilization  games freeciv       training city structurally different   objective game start civilization       cities  experiments demonstrating initial settlers stone age expand      transfer learning                             velop conquer world win space                                                         race escape alpha centauri  case game                                                         characterized race build civilization      introduction                                      technological sophistication faster opponents    novice learns play strategy game ini way competing demands lim  tial focus attention usually figuring things ited resources investment development example   work  long building repertoire competitive players improve terrain irrigation roads   strategies trying basic actions seeing effects avoiding famine military defeat em  discovering traps avoid  believe analogy plays phasis military preparedness example make citi  key role discovery process especially mapping zens unhappy productive  money   structurally different situations  analogy  self account active process learners   through exploring world constructing explana  tions  paper show experimentation strate  gies qualitative reasoning support planning   analogy learning resource allocation tasks turnbased   strategy game      games sort interesting properties     involve incomplete knowledge world    entail complex interrelationships actions ob  servable quantities  goals like optimization   problems states achieved  planning   executing tightly interleaved  qualitative representa  tions serve partial domain theory guide planning   learning different levels expertise                            figure  freeciv                                                      ijcai                                                     allocated research new technologies iron defer decision execution time implementing   making democracy enable players create new kind conditional plan  domainindependent primi  improvements cities new types units adopt new tives include bookkeeping methods updating facts   types governments each tradeoffs   cases docallback suspends plan     current set experiments focus subtask condition true time instantiates plan   managing cities optimize use resources build new bindings resumes execution    infrastructure improve terrain research technologies  play important role evaluating delayed effects ac  planner direct exploration city management tions   tasks offer clearer evaluation metrics  currently   ignore military operations focusing instead make  analogical learning  rich productive civilization                      highlevel goal research demonstrate                                                         analogy qualitative reasoning support machine                                                         learning increasingly distant transfer precedents     htn planning                                      using structure mapping engine sme   support performing learning strategy game falkenhainer et al  macfac retrieval mecha  implemented hierarchical task network htn nism forbus et al  seql generalization   planner using shop algorithm nau et al   kuehne et al  core components    htn planner complex tasks decomposed primitive analogical mechanisms tightly integrated underly  executable tasks  primitives freeciv correspond ing reasoning engine provide mechanisms  packets sent game server representing actions trieval comparison transfer  structure mapping   sending unit particular location telling engine particular assesses similarity   city build  complex tasks level figur precedent current situation projects pre  ing unit particular turn decid vious solution new case translating entities   ing ameliorate crisis city famine mapped equivalents form candidate infer  revolt  planner generates plans each unit city ences analogy provides level adaptation   turn integrates combined plan automatically   ningexecution environment  planning invoked partly   unit comparison retrieval case   eventdriven manner reified events approach cases entire games lessons   game trigger certain decisions  example planning certainly gleaned granularity   agent does recompute global strategy turn entire cities  instead case individual decision   checks acquired new technologies context particular city particular moment time   turn does reevaluate strategy  given game   example cases capture decision     critical aspect game requires planning improvements build tiles work   incomplete uncertain information  terrain broader game level technologies research     known explored  outcomes actions each type decision set queries repre  stochastic example village huts contain bar sented knowledge base designated possibly   barians kill explorer contain gold relevant making decision  set que  new technologies  vastly information ries relevant capturing assessing case   game considered planning state  solution  decision acted game snap  consequently planner plan agents actions shot case constructed execution   starting complete initial state  reify informa stored game context  case snapshot used   tion demand querying game state  analyzing effects actions supporting later   time planner project effects actions analogical transfer   planned state deviates game state  reconcile   retrieval purposes cases organized case li  competing demands maintain contexts cf braries macfac extract struc  lenat  game context reflects incom turally similar precedent  planner attempts learn   plete correct current state game planning decision tasks creates populates libraries each   context states projected forward  query type task  executing plan current relevant   information directly answered facts queried stored temporal subabstraction   planned state proceeds query game state  game context   result action evalu  turning gamestate information checked ated respect performance goal case added   sistency plan state ensure example taskspecific library successful failed cases   unit believed places time  appropriate  case considered successful im  simple heuristic checks explicit negations proves goal quantity quantity meets thresh  inconsistent values functional predicates old requirements  essentially “rewarding” actions     addition reifying demand way ac improve goal quantity viewed type   commodate incomplete information through secondorder learning method temporal differences sutton   planning primitives  doplan primitive allows plan   threshold provides additional crite                                                      ijcai                                                     rion helps prevent accumulation lowvalues cases tures game like freeciv complexity quanti  tend leave stuck local maxima  tative relationships simulation engine  understanding   words action improved precedent “terrible” relationships critical factor playing game   merely “bad” probably won’t help new situa figure  shows small fraction model cities   tion  maximizing goals initial threshold simply freeciv     “greater zero” minimizing balance goals   primary way qualitative model currently used   undefined                                      determine changes incurred action corre    complication mapping prior solutions spond goals changes signify success   new problem particular choices clear failure  model allows trace local   correspondence new problem  happens tasks global goals assign credit blame   mapping process produces analogy skolem denoting   second role model identifying leaf quanti  unmapped entity  resolve skolems collecting ties affect outputs spawning learning goals   facts mention entity base case treat described ‘overcoming local maxima’   ing constraints problem  pick ran   greatest potential role qualitative models   domly choices satisfy constraints   synthesizing higherlevel strategies proposing primi  important insight necessary tive actions influence goal quantities   ongoing   solve skolems previous plan corre work outside scope paper   sponding current choice  example previ  ous plan worker tile   current problem allocate available worker   necessary resolve worker’s prior location     experimentation  analogy powerful technique learning   cases retrieve compare  boot  strap case library ensure variety cases guide   empirical learning explicit learning goals ram   leake    learning goals typically provided   beginning game goal learn effect   action allocating workers  learning goals   posted byproduct explaining failures    learning ranges quantities  goals persist   games     goal learning effects actions determines   decision task solved insuffi        figure  portion city model  cient analogical precedents canned plans  experi  mentation strategy uses makes decisions randomly  overcoming local maxima  der generate requisite variation provide cases   better cover decision space  strategy typically difficulties applying analogical learning   accompanied additional learning goals control parame optimization problems type easy fall   ters suppressing decisions order try learn local maxima performance stops   effect time   trial error approach tends improving keeps adopting precedents   work best simple lowlevel decisions              ways overcome     context single game poor choices fails improve goal attempts ex  revisited blindly recorded nogoods  farming plain  traversing qualitative model collects   desert typically results tile labeled nogood  leaf quantities ultimately affect goal    problem difficult cities posts learning goal learn maximum minimum   run choices point revisit values quantities food produced indi  nogoods having tried order vidual tiles  iterations provides simple   performance choose best remains      expectation attainable  learner     later successful cases accumulate possi uses information estimate improvement   ble solve problems analogically  analogical possible moving worker pro  transfer fails runs successful precedents falls ductive tile maximally productive tile  depending   experimentation                              current tolerance risk sets minimally accept                                                        able threshold goal quantity  raising bar                                                         way forces learner experiment      exploiting qualitative model                    better solution  tolerance risk mentioned   qualitative model partial domain theory captures function penalty incurred bad deci  influences quantities   outstanding fea sion  example city grows granary starts                                                       ijcai                                                      moving worker productive location   philadelphia             new york   lead catastrophic famine granary   easily corrected granary     second method implemented explicitly   recognizing lack improvement  way   person look learning graph recognize local   maxima  recent trend   games flat declining serve spur ex  periment satisfied existing cases        transfer learning experiments    order measure generality learning mecha               figure  freeciv cities   nism performed number transfer learning experi  ments transfer learning involves training performance   order simplify evaluation controlled city   set tasks conditions measuring production queues produce coinage constrained   effect learning different related set tasks technology research agenda work therepub  conditions  three types improvement possible lic  necessary control   higher initial performance intercept faster learn game’s unfortunate tendency allocate workers   ing rate andor higher final asymptotic performance   city grows shrinks  did intercept  experiments learned ing changes lowlevel packet handler routines   allocate workers productive tiles holding undoing game’s interference learning   potentially conflating decisions constant            agent sees  experiments ran  training      understand worker allocation task better nec games city “philadelphia” followed  games   essary understand cities freeciv benefit structurally different city “new york” figure     working  tiles immediate region each game stopped  turns approximately   shown figure   tiles productive long show variation performance   virtue terrain type randomly placed   figure  shows learning curves trials   resource “specials”   city founded prior training  case trials   “worker” citizen available work land produce prior training flatlined essentially   food  food produced consumed ceiling  trials prior training start  food   city’s granary slowly fills number turns  points baseline performance rise     city grows producing worker games  viewed  improvement  assigned tile  hand food intercept   consumed produced granary   ran experiment cities reversed   city experiences famine loses worker  task order determine sensitive order pres  set learner learn tiles entation  improvement   food   improve food production  performance objective points games prior training remain fixed    maximize total food production end  turns   points trial games training   note strongly affected fast city start oscillate   points  going   grows turn depends making good decisions   looked saved game cases saw   early avoiding famine                            successful cases transferred applied                                                                                                                                                                                                                                                                                                                             prior training                                    philadelphia                                         prior training                                                                                          new york                                   prior training                                                                                          prior training                                                                                                               figure  food production  turns new york figure  food production  turns philadelphia                                                       ijcai                                                                                                          ment learning successful actions rewarded                                                         preferred future  successful actions                                                        rewarded adding library successful cases                                                          emphasis clearly different process                                                        described knowledge intensive modeldriven                                                         instance based   derek bridge  explored                                                        relation reinforcement learning cbr                                                        context recommender systems                                prior training     qualitative models used planning previ                               philadelphia                                                        ously notably hogge’s  tplan compiled                            prior training  qualitative domain theory planning operators forbus’                                                         actionaugmented envisionments integrated                                              actions envisioner drabble’s                 figure  learning failure          excalibur planning execution used qp        average food production new york  turns theory hierarchical partialorder planner   strat                                                        egy game domain complex domains   city cities  cases tackled previous efforts  use htns inspired   prior training kept making mistakes muñozavila  aha  used htn planning    specifically happening realtime strategy game   gained experience transferred successful cases early   prodigyanalogy tightly integrated analogy problem   game reaching population    aren’t solving veloso   prodigy’s core meansends analysis   typically   uniquely mappable highly pro strategy amenable inferential tasks   ductive tiles city  size fell kinds  optimization goals face  liu stone   random choice  terrible  apply structure mapping transfer learning   facility learning prior failed domain robocup soccer   cases continue try farming desert   researchers used freeciv learning   suffer famine  case negative transfer domain  ashok goel’s group georgia tech applied   modified experimentation strategy retrieve modelbased selfadaptation conjunction reinforce  similar failed cases map choices ment learning ulam et al   believe analogy   nogoods current case  ran  sequences  better support distant transfer learning qualita  games each condition plot average learning tive models ultimately permit strategic reasoning   curves figure   results show  improvement ways tkml models    initial yintercept performance pvalue     confidence interval    summary  statistically significant  analyzed                                                         paper presented initial results integrating anal  saved cases determined incidence famine                                                         ogy experimentation qualitative modeling   dropped half   avoiding poor random decisions                                                         planning executing learning strategy games    effect converging rapidly asymptote                                                        suggest qualitative models provide intermediate level   ducing variability performance  suggests                                                         domain knowledge comparable novice human   future learning experiments require complex                                                         player start  described use analogy   openended tasks                                                         compare snapshots order extract                                                         effects actions  basis experimental results      related work                                      implemented minor change plans enable learning   work governmentfunded program   failures  led pronounced improvement   transfer learning  groups pursuing similar goals behavior   somewhat different ways  icarus langley et al    clearly learning resource allocation decisions    soar nason laird  step learning transferring abstractions high  applied learning realtime game environments using level strategies   investigating role qualitative   markov logic networks bayesian techniques respec models composing new plans  intend  tively  icarus extended include htn velop elaborate strategies pursuing learning goals   planner generate hierarchical skills executed   does currently construct explicit gener  architecture  unlike icarus does alizations  consequently viewed kind   learn hierarchical skills instead employs analogy transfer reoperationalization krulwich et al    transfer instances decisions                      spawning learning goals response failures     type learning doing viewed way explanatory transfer  step   kind reinforcement learning kaelbling et al  employ seql construct generalizations   far involves unsupervised online learning rules conceptual distinctions sufficiently cap  requires exploration space actions  reinforce tured case base                                                       ijcai                                                     
